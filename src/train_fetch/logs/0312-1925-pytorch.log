epoch: 0, step: 0
	action: tensor([[ 0.8262,  1.6650, -2.0908,  0.0042, -1.8560, -1.5299,  0.4407]],
       dtype=torch.float64)
	q_value: tensor([[-0.0161]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 1
	action: tensor([[ 1.4102,  2.1608, -1.5834, -0.5008,  0.7638, -0.6583, -1.9561]],
       dtype=torch.float64)
	q_value: tensor([[-0.0019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 2
	action: tensor([[ 0.2172, -1.2115, -0.0556, -1.1976,  1.1492,  0.8161, -0.2165]],
       dtype=torch.float64)
	q_value: tensor([[-0.0019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6822731042492474, distance: 1.4842427767866646 entropy 1.8700141906738281
epoch: 0, step: 3
	action: tensor([[-1.5911, -3.3458,  1.0311, -0.4892,  1.4769, -0.2827, -1.9672]],
       dtype=torch.float64)
	q_value: tensor([[0.0282]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 4
	action: tensor([[-0.5488,  0.0947, -3.6994, -1.0296, -2.3126,  2.6503,  1.0374]],
       dtype=torch.float64)
	q_value: tensor([[-0.0019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 5
	action: tensor([[ 0.2953,  1.6334, -2.1185, -0.0277, -0.0777,  0.2000,  0.1521]],
       dtype=torch.float64)
	q_value: tensor([[-0.0019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 6
	action: tensor([[ 0.5188, -0.1916, -0.6119, -1.4698,  2.5792, -2.1341, -1.4476]],
       dtype=torch.float64)
	q_value: tensor([[-0.0019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4156295615008333, distance: 0.8747839622853649 entropy 1.8700141906738281
epoch: 0, step: 7
	action: tensor([[ 0.4186, -1.4652, -0.1776, -2.6446, -0.0942, -1.0815,  0.9839]],
       dtype=torch.float64)
	q_value: tensor([[0.0730]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 8
	action: tensor([[-1.4539,  0.2257, -0.8241, -0.4883,  1.7010,  1.6112,  1.6169]],
       dtype=torch.float64)
	q_value: tensor([[-0.0019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2899470741664967, distance: 1.2996985642093806 entropy 1.8700141906738281
epoch: 0, step: 9
	action: tensor([[ 0.0598, -0.6401,  0.9741, -1.7592, -0.8918, -1.3090, -1.2400]],
       dtype=torch.float64)
	q_value: tensor([[0.0083]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3253946868550066, distance: 1.31743533020495 entropy 1.8700141906738281
epoch: 0, step: 10
	action: tensor([[ 0.0859,  0.0108,  1.8727, -2.2195,  1.8833, -2.7332, -4.0546]],
       dtype=torch.float64)
	q_value: tensor([[0.0404]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 11
	action: tensor([[-1.2187,  1.7946,  0.5662, -0.1596,  0.8473,  2.6356, -2.5836]],
       dtype=torch.float64)
	q_value: tensor([[-0.0019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 12
	action: tensor([[-0.8085, -0.0809,  1.3669,  1.3246,  0.0751,  1.2489,  3.0557]],
       dtype=torch.float64)
	q_value: tensor([[-0.0019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 13
	action: tensor([[ 0.2551, -0.7979,  0.8395, -2.6186,  0.4604, -0.4044, -2.7466]],
       dtype=torch.float64)
	q_value: tensor([[-0.0019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 14
	action: tensor([[-2.2714, -0.8910, -1.7994,  3.4999, -0.1083, -1.2216, -0.2581]],
       dtype=torch.float64)
	q_value: tensor([[-0.0019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 15
	action: tensor([[-1.5009,  1.1986, -0.2003,  0.0902,  1.5048, -0.2053,  1.9558]],
       dtype=torch.float64)
	q_value: tensor([[-0.0019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 16
	action: tensor([[-4.0431, -0.7018, -2.2257,  0.8276,  0.1457, -0.8338, -0.5704]],
       dtype=torch.float64)
	q_value: tensor([[-0.0019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 17
	action: tensor([[ 1.0451, -1.6104,  0.1918, -2.6636, -3.0533, -0.1884, -1.5389]],
       dtype=torch.float64)
	q_value: tensor([[-0.0019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 18
	action: tensor([[-1.2347, -0.5952, -2.4458, -1.7440, -1.4356,  1.0849,  2.4427]],
       dtype=torch.float64)
	q_value: tensor([[-0.0019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.210362075333012, distance: 1.0168823104754616 entropy 1.8700141906738281
epoch: 0, step: 19
	action: tensor([[-1.5669,  0.1912, -0.3578,  0.1113, -1.1516,  0.5110, -2.0385]],
       dtype=torch.float64)
	q_value: tensor([[-0.0726]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.4428372920985786, distance: 1.7885618536431753 entropy 1.8700141906738281
epoch: 0, step: 20
	action: tensor([[-1.6697,  2.8152,  0.5258, -4.1447, -1.7157,  4.5581,  2.3752]],
       dtype=torch.float64)
	q_value: tensor([[0.0573]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 21
	action: tensor([[-1.4182, -1.5201, -1.3489, -0.1726, -0.8951, -0.4268, -1.6491]],
       dtype=torch.float64)
	q_value: tensor([[-0.0019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2879633207415737, distance: 1.7309368437940442 entropy 1.8700141906738281
epoch: 0, step: 22
	action: tensor([[ 1.2296,  1.6997,  2.1129,  0.3216, -0.8068,  1.1979, -0.7330]],
       dtype=torch.float64)
	q_value: tensor([[0.0731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 23
	action: tensor([[-1.6700, -0.2331,  1.9443, -0.8457, -1.7025, -0.3874,  1.4030]],
       dtype=torch.float64)
	q_value: tensor([[-0.0019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 24
	action: tensor([[-1.9198,  2.1296, -0.7136,  0.3760, -1.0923, -0.1905,  0.5890]],
       dtype=torch.float64)
	q_value: tensor([[-0.0019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 25
	action: tensor([[-0.4479,  0.9049,  1.8769,  1.7101,  3.5376,  1.0220, -0.1319]],
       dtype=torch.float64)
	q_value: tensor([[-0.0019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 26
	action: tensor([[ 0.2781, -0.3791,  0.9991, -1.6554,  1.4238,  0.8984,  2.5398]],
       dtype=torch.float64)
	q_value: tensor([[-0.0019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.41321082323159497, distance: 1.3603797879242958 entropy 1.8700141906738281
epoch: 0, step: 27
	action: tensor([[-0.4282,  1.3921, -1.6878, -0.5934, -3.2903, -1.7783,  0.4064]],
       dtype=torch.float64)
	q_value: tensor([[-0.0463]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 28
	action: tensor([[ 0.2700, -0.2717, -3.3677, -0.6286, -1.6750,  0.9223,  1.1071]],
       dtype=torch.float64)
	q_value: tensor([[-0.0019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.5215908156645079 entropy 1.8700141906738281
epoch: 0, step: 29
	action: tensor([[ 1.6716,  0.0797,  4.1661,  2.1368,  1.1335, -1.7089,  0.2326]],
       dtype=torch.float64)
	q_value: tensor([[-0.0019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 30
	action: tensor([[-0.2573, -2.0253,  1.2275,  1.2897, -0.6992,  1.1901,  1.3199]],
       dtype=torch.float64)
	q_value: tensor([[-0.0019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 31
	action: tensor([[-0.1474,  0.9866, -2.6430, -1.1251,  1.7971,  2.4341,  0.1724]],
       dtype=torch.float64)
	q_value: tensor([[-0.0019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 32
	action: tensor([[ 0.5167,  1.7324,  0.4337, -1.5968, -0.5270,  0.2912,  3.8391]],
       dtype=torch.float64)
	q_value: tensor([[-0.0019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 33
	action: tensor([[ 2.5809,  1.5619,  0.1105,  3.4100, -1.6428, -0.6375,  0.6314]],
       dtype=torch.float64)
	q_value: tensor([[-0.0019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 34
	action: tensor([[-0.0884,  1.0525,  0.9078, -0.8571, -1.0462,  2.9933,  1.9289]],
       dtype=torch.float64)
	q_value: tensor([[-0.0019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 35
	action: tensor([[ 0.9190,  0.5601,  1.0868,  1.3984, -0.0626, -0.6462,  2.9780]],
       dtype=torch.float64)
	q_value: tensor([[-0.0019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 36
	action: tensor([[-3.5534, -2.1479, -1.8315,  0.7937, -2.0623,  5.0496, -1.8198]],
       dtype=torch.float64)
	q_value: tensor([[-0.0019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 37
	action: tensor([[-0.1717, -0.7429, -1.9178,  1.1330, -0.1207, -0.1352,  0.1314]],
       dtype=torch.float64)
	q_value: tensor([[-0.0019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.001971988893553, distance: 1.6191448081191488 entropy 1.8700141906738281
epoch: 0, step: 38
	action: tensor([[ 1.8206,  2.6095, -0.1551,  0.3849, -0.3955, -1.9150, -1.3678]],
       dtype=torch.float64)
	q_value: tensor([[0.0211]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 39
	action: tensor([[ 1.1495,  0.9892, -1.1621,  2.0809, -1.9892, -0.9176, -0.2487]],
       dtype=torch.float64)
	q_value: tensor([[-0.0019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 40
	action: tensor([[-1.2334, -1.0019, -1.1618, -0.3473, -0.9256,  1.1013,  1.1187]],
       dtype=torch.float64)
	q_value: tensor([[-0.0019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.3334726122016258, distance: 1.7480668912264647 entropy 1.8700141906738281
epoch: 0, step: 41
	action: tensor([[ 1.3768,  1.7595, -1.5869,  4.6306, -1.2601, -0.7717, -1.6572]],
       dtype=torch.float64)
	q_value: tensor([[0.0008]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 42
	action: tensor([[ 1.1817,  3.4552,  0.2062,  0.1166,  1.2268,  0.7089, -0.2478]],
       dtype=torch.float64)
	q_value: tensor([[-0.0019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 43
	action: tensor([[-1.0777,  0.1583,  0.0453, -0.3619, -1.3611, -0.2581, -0.0119]],
       dtype=torch.float64)
	q_value: tensor([[-0.0019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0634809925737225, distance: 1.6438301044054622 entropy 1.8700141906738281
epoch: 0, step: 44
	action: tensor([[-1.0457, -2.8240,  1.1317, -1.3212,  1.1145, -0.1939, -0.7754]],
       dtype=torch.float64)
	q_value: tensor([[0.0175]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 45
	action: tensor([[-1.7563, -1.3214, -0.0660, -3.3755, -1.0068,  1.2755, -0.6903]],
       dtype=torch.float64)
	q_value: tensor([[-0.0019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 46
	action: tensor([[-0.0146,  1.5426,  3.0191,  1.6912,  0.1733,  1.5715,  1.0189]],
       dtype=torch.float64)
	q_value: tensor([[-0.0019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 47
	action: tensor([[-1.4306, -1.7858, -0.4800, -1.6456,  0.0917,  0.1131,  1.6198]],
       dtype=torch.float64)
	q_value: tensor([[-0.0019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 48
	action: tensor([[ 2.2487, -0.4925, -0.5594,  1.1307,  0.8479,  1.3805, -0.2491]],
       dtype=torch.float64)
	q_value: tensor([[-0.0019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7471118094457034, distance: 0.5754677202968447 entropy 1.8700141906738281
epoch: 0, step: 49
	action: tensor([[ 3.0395, -0.4378,  0.8689,  0.0135, -0.0179,  0.8201, -1.1140]],
       dtype=torch.float64)
	q_value: tensor([[0.0326]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 50
	action: tensor([[ 0.4195,  0.5930, -0.3819,  1.0772, -0.5486,  0.6721,  1.0368]],
       dtype=torch.float64)
	q_value: tensor([[-0.0019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 51
	action: tensor([[ 1.9477,  1.1684, -1.7994, -1.9084, -1.2829, -4.6569, -3.1634]],
       dtype=torch.float64)
	q_value: tensor([[-0.0019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 52
	action: tensor([[-1.2049,  0.0639, -3.3473,  1.6558, -0.9106,  0.1821,  0.6420]],
       dtype=torch.float64)
	q_value: tensor([[-0.0019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4552661597359557 entropy 1.8700141906738281
epoch: 0, step: 53
	action: tensor([[-1.9561, -1.7673, -1.0876,  0.5177, -1.9985,  0.7466, -1.0814]],
       dtype=torch.float64)
	q_value: tensor([[-0.0019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 54
	action: tensor([[ 0.4277,  1.6982, -1.0989,  0.6516,  2.0849, -0.4819, -0.0984]],
       dtype=torch.float64)
	q_value: tensor([[-0.0019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 55
	action: tensor([[-0.6083, -1.2038, -0.8222,  1.0062, -1.2278,  2.5294,  0.8474]],
       dtype=torch.float64)
	q_value: tensor([[-0.0019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5055330111578451, distance: 1.4041122701316653 entropy 1.8700141906738281
epoch: 0, step: 56
	action: tensor([[ 1.9330,  0.8930, -1.8654, -2.2054,  0.9594, -0.8648, -0.7465]],
       dtype=torch.float64)
	q_value: tensor([[-0.0110]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1318717860593257, distance: 1.2174617079477845 entropy 1.8700141906738281
epoch: 0, step: 57
	action: tensor([[ 1.0452,  2.2401,  1.3744, -0.4490,  0.4489, -0.5997, -0.1279]],
       dtype=torch.float64)
	q_value: tensor([[0.0012]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 58
	action: tensor([[-4.0969,  1.0707,  0.3047,  0.5620, -0.9650, -0.1602, -3.7428]],
       dtype=torch.float64)
	q_value: tensor([[-0.0019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 59
	action: tensor([[ 1.7056, -1.3097, -0.5625,  0.6271,  0.1385, -1.6202,  1.2539]],
       dtype=torch.float64)
	q_value: tensor([[-0.0019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5084611502347662, distance: 1.405477048856459 entropy 1.8700141906738281
epoch: 0, step: 60
	action: tensor([[-0.2365, -2.8835,  3.1173,  0.4202, -0.1441, -0.0697,  2.2179]],
       dtype=torch.float64)
	q_value: tensor([[0.0522]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 61
	action: tensor([[ 1.4077, -2.2271, -2.0570,  2.3944, -0.1983, -1.5162, -0.4486]],
       dtype=torch.float64)
	q_value: tensor([[-0.0019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 62
	action: tensor([[ 0.0607, -1.8303, -2.0384,  0.2310, -2.0447, -2.0811,  2.2710]],
       dtype=torch.float64)
	q_value: tensor([[-0.0019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 63
	action: tensor([[-0.4935, -0.1067, -0.0908, -1.7932,  1.5737,  1.7472,  0.8890]],
       dtype=torch.float64)
	q_value: tensor([[-0.0019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36703684381015156, distance: 0.9104286351039991 entropy 1.8700141906738281
epoch: 0, step: 64
	action: tensor([[ 0.8012,  1.3609, -0.7052,  0.6585,  0.7270,  0.5345,  0.1765]],
       dtype=torch.float64)
	q_value: tensor([[-0.0041]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 65
	action: tensor([[-0.7496, -2.5737, -2.8413,  1.5787, -1.4212, -0.1689,  0.1084]],
       dtype=torch.float64)
	q_value: tensor([[-0.0019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 66
	action: tensor([[ 0.8720,  1.5541,  0.0210,  1.0178,  4.1572, -2.2614, -0.1409]],
       dtype=torch.float64)
	q_value: tensor([[-0.0019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 67
	action: tensor([[-0.9445, -1.1234,  0.9326,  1.6481, -0.5708, -1.1149,  1.5885]],
       dtype=torch.float64)
	q_value: tensor([[-0.0019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06336450402132798, distance: 1.180042836492968 entropy 1.8700141906738281
epoch: 0, step: 68
	action: tensor([[ 0.6808,  2.7395, -0.4997,  1.3837,  1.5573, -3.0670,  1.4485]],
       dtype=torch.float64)
	q_value: tensor([[0.0891]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 69
	action: tensor([[-2.7140, -1.9771, -0.6930,  0.8533,  0.3384,  2.6153,  0.8710]],
       dtype=torch.float64)
	q_value: tensor([[-0.0019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 70
	action: tensor([[ 0.1973, -0.4076, -0.1362, -2.2057, -1.5689,  2.8519,  0.9122]],
       dtype=torch.float64)
	q_value: tensor([[-0.0019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 71
	action: tensor([[-0.5890, -0.2549,  2.2822, -3.2538,  0.6163, -1.9364,  0.6461]],
       dtype=torch.float64)
	q_value: tensor([[-0.0019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 72
	action: tensor([[-1.2115,  0.5864, -0.9516,  1.4812,  1.6720, -1.6151,  1.8050]],
       dtype=torch.float64)
	q_value: tensor([[-0.0019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6240753162273995, distance: 1.4583433181424001 entropy 1.8700141906738281
epoch: 0, step: 73
	action: tensor([[-1.5227, -2.4172,  1.2690, -0.6188, -1.4794, -2.4092, -0.6237]],
       dtype=torch.float64)
	q_value: tensor([[0.0752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 74
	action: tensor([[ 1.8548,  0.5896, -2.6235,  1.0212, -0.4643, -1.7462, -1.2079]],
       dtype=torch.float64)
	q_value: tensor([[-0.0019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 75
	action: tensor([[ 1.3314,  0.6982, -2.1895,  0.6362, -0.8901,  1.1977,  0.3552]],
       dtype=torch.float64)
	q_value: tensor([[-0.0019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 76
	action: tensor([[ 2.9042, -1.7024,  2.1117, -2.3426, -0.6676, -1.6765,  1.4195]],
       dtype=torch.float64)
	q_value: tensor([[-0.0019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 77
	action: tensor([[-1.6126, -2.5523,  0.7765,  0.7450, -0.6408,  0.8423, -0.9019]],
       dtype=torch.float64)
	q_value: tensor([[-0.0019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 78
	action: tensor([[ 2.0288, -2.0898,  3.8040,  1.3016,  0.3113, -3.7585,  0.6373]],
       dtype=torch.float64)
	q_value: tensor([[-0.0019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 79
	action: tensor([[-1.0621,  1.1139, -0.3456,  2.6529, -1.5778, -2.0701, -1.4786]],
       dtype=torch.float64)
	q_value: tensor([[-0.0019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 80
	action: tensor([[-2.2011,  0.3052, -0.8717, -1.2131,  3.0474,  1.0307, -0.0361]],
       dtype=torch.float64)
	q_value: tensor([[-0.0019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 81
	action: tensor([[-1.5062,  1.5455, -0.5735, -1.6568, -0.7944,  2.2016,  0.8650]],
       dtype=torch.float64)
	q_value: tensor([[-0.0019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.00857859945400019, distance: 1.1492422075330349 entropy 1.8700141906738281
epoch: 0, step: 82
	action: tensor([[-0.6721,  1.7871,  0.1611,  0.5559,  1.0322,  0.8173, -0.0112]],
       dtype=torch.float64)
	q_value: tensor([[-0.0434]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 83
	action: tensor([[ 0.7566,  1.8090,  2.4105, -0.5511,  2.0059, -0.1242,  0.3637]],
       dtype=torch.float64)
	q_value: tensor([[-0.0019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 84
	action: tensor([[-0.9119,  1.4190,  1.0493, -2.1521,  1.7164, -2.2929,  0.9958]],
       dtype=torch.float64)
	q_value: tensor([[-0.0019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07121481717465161, distance: 1.1028446289836065 entropy 1.8700141906738281
epoch: 0, step: 85
	action: tensor([[-0.1112,  1.6864, -0.4259, -0.4874, -0.4478, -2.7359, -1.0512]],
       dtype=torch.float64)
	q_value: tensor([[0.0278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 86
	action: tensor([[-0.0587,  1.3823, -0.4691, -2.4628, -2.1552,  0.4240, -0.2307]],
       dtype=torch.float64)
	q_value: tensor([[-0.0019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 87
	action: tensor([[-2.0542,  4.0068, -0.8416,  2.4976,  0.3650,  0.0187, -0.4404]],
       dtype=torch.float64)
	q_value: tensor([[-0.0019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 88
	action: tensor([[-2.0016,  1.4705,  3.0048,  0.5227, -2.3422,  0.5329,  2.4019]],
       dtype=torch.float64)
	q_value: tensor([[-0.0019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 89
	action: tensor([[-0.4687,  0.8831, -1.4547,  0.1926,  1.4809, -0.1580,  0.2547]],
       dtype=torch.float64)
	q_value: tensor([[-0.0019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 90
	action: tensor([[-3.7822, -0.9919,  1.9528, -1.8014,  0.2384,  0.7094,  3.0396]],
       dtype=torch.float64)
	q_value: tensor([[-0.0019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 91
	action: tensor([[ 0.2822,  0.1909, -0.5926,  1.9919,  0.7142, -3.0695,  0.3496]],
       dtype=torch.float64)
	q_value: tensor([[-0.0019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 92
	action: tensor([[-2.1644, -2.1697,  1.6329, -1.4162,  0.5786, -2.4639,  2.3220]],
       dtype=torch.float64)
	q_value: tensor([[-0.0019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 93
	action: tensor([[ 1.2602, -0.1586, -1.5510, -2.6149, -0.3608, -0.8513,  0.7483]],
       dtype=torch.float64)
	q_value: tensor([[-0.0019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 94
	action: tensor([[ 1.8000,  2.0766,  0.8018, -2.2917,  0.4294, -0.1406, -1.6193]],
       dtype=torch.float64)
	q_value: tensor([[-0.0019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 95
	action: tensor([[ 0.5066, -0.8309,  1.8484, -2.1359, -0.3264,  1.4269, -0.3121]],
       dtype=torch.float64)
	q_value: tensor([[-0.0019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 96
	action: tensor([[-0.2351,  2.3716, -1.6083, -2.5969,  0.4973,  0.0388,  0.7689]],
       dtype=torch.float64)
	q_value: tensor([[-0.0019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 97
	action: tensor([[ 0.3349, -1.8542,  0.7012,  0.0659,  1.0211, -0.0603,  1.7872]],
       dtype=torch.float64)
	q_value: tensor([[-0.0019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 98
	action: tensor([[-1.8123,  1.0874,  0.9044,  3.3313,  0.2581, -2.9542, -2.9521]],
       dtype=torch.float64)
	q_value: tensor([[-0.0019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 99
	action: tensor([[-2.1102, -1.2823, -1.8707, -2.5600,  1.2709,  0.3285,  1.6747]],
       dtype=torch.float64)
	q_value: tensor([[-0.0019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 100
	action: tensor([[ 8.8119e-01,  1.0357e+00, -1.5465e-03, -2.8458e+00,  2.8226e+00,
         -1.6233e+00,  1.6125e+00]], dtype=torch.float64)
	q_value: tensor([[-0.0019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 101
	action: tensor([[-2.8542,  1.5450, -0.2010, -1.1973,  1.8443,  0.3360, -0.7762]],
       dtype=torch.float64)
	q_value: tensor([[-0.0019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 102
	action: tensor([[-0.8894, -1.2317, -0.1714, -0.8945,  1.6918, -1.4890, -1.6947]],
       dtype=torch.float64)
	q_value: tensor([[-0.0019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9361189174938369, distance: 1.5922919792626786 entropy 1.8700141906738281
epoch: 0, step: 103
	action: tensor([[-1.1512,  0.6408,  0.3805,  1.0080, -1.0659, -1.1966,  3.0706]],
       dtype=torch.float64)
	q_value: tensor([[0.0518]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2232031783652475, distance: 1.0085801424194456 entropy 1.8700141906738281
epoch: 0, step: 104
	action: tensor([[ 1.3235, -0.2291, -0.7284, -1.5653,  1.7289,  1.8224, -0.9312]],
       dtype=torch.float64)
	q_value: tensor([[0.0504]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 105
	action: tensor([[2.9635, 1.1523, 0.5798, 0.0074, 1.2606, 2.4155, 4.2928]],
       dtype=torch.float64)
	q_value: tensor([[-0.0019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 106
	action: tensor([[ 1.0134, -0.3078,  0.2601,  1.4084,  1.3147, -2.7350,  1.4337]],
       dtype=torch.float64)
	q_value: tensor([[-0.0019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 107
	action: tensor([[-0.7889,  2.2780, -1.2209, -0.6746, -2.4228, -0.2023, -0.1234]],
       dtype=torch.float64)
	q_value: tensor([[-0.0019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 108
	action: tensor([[ 2.2258, -1.1109, -2.0458,  3.3164, -1.7291,  1.6419,  2.3935]],
       dtype=torch.float64)
	q_value: tensor([[-0.0019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 109
	action: tensor([[ 1.0255,  0.9500, -0.9132,  4.1998,  2.0738,  1.4478,  0.6614]],
       dtype=torch.float64)
	q_value: tensor([[-0.0019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 110
	action: tensor([[-0.4326,  0.0061,  0.6319, -2.2807, -2.2392,  0.1652,  0.5917]],
       dtype=torch.float64)
	q_value: tensor([[-0.0019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4911264469440706, distance: 1.3973780908645927 entropy 1.8700141906738281
epoch: 0, step: 111
	action: tensor([[-0.8062, -2.5379, -1.2647,  0.4179, -0.4170,  0.2764,  1.2954]],
       dtype=torch.float64)
	q_value: tensor([[-0.0008]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 112
	action: tensor([[ 0.0932,  1.7827, -1.6544, -1.9721, -1.5682,  0.0701,  0.3157]],
       dtype=torch.float64)
	q_value: tensor([[-0.0019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 113
	action: tensor([[ 0.5211, -0.1594, -2.4129,  0.0319,  0.2489, -0.4763, -0.3261]],
       dtype=torch.float64)
	q_value: tensor([[-0.0019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 114
	action: tensor([[-0.2385,  0.1533, -1.6088,  0.5318, -0.9691, -1.8588, -1.7531]],
       dtype=torch.float64)
	q_value: tensor([[-0.0019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 115
	action: tensor([[ 2.6059, -0.2677,  2.4766,  0.0091,  0.6393,  1.1180,  0.9577]],
       dtype=torch.float64)
	q_value: tensor([[-0.0019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 116
	action: tensor([[-1.4074, -1.7606, -1.4757, -0.3910, -0.9334, -1.7921, -1.7157]],
       dtype=torch.float64)
	q_value: tensor([[-0.0019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 117
	action: tensor([[ 0.2046, -1.9173, -1.0736,  0.7084,  3.2460, -1.4867,  1.3358]],
       dtype=torch.float64)
	q_value: tensor([[-0.0019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 118
	action: tensor([[-1.6034, -1.1427,  1.5665,  0.6125,  0.0302, -1.1705,  1.8365]],
       dtype=torch.float64)
	q_value: tensor([[-0.0019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.4027630515025082, distance: 1.7738306947438989 entropy 1.8700141906738281
epoch: 0, step: 119
	action: tensor([[-0.8998,  0.3157,  0.8484,  1.2929, -1.0081,  1.1139,  0.6419]],
       dtype=torch.float64)
	q_value: tensor([[0.0947]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 120
	action: tensor([[-0.7749,  1.5258, -0.2033,  3.2745,  1.3783, -0.1673, -1.9866]],
       dtype=torch.float64)
	q_value: tensor([[-0.0019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 121
	action: tensor([[ 0.0762, -3.9345, -1.0516,  0.9151,  1.9097, -1.9751, -0.8392]],
       dtype=torch.float64)
	q_value: tensor([[-0.0019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 122
	action: tensor([[-2.0817, -0.7356, -0.1458, -1.3541,  2.5447, -1.7278, -0.4252]],
       dtype=torch.float64)
	q_value: tensor([[-0.0019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 123
	action: tensor([[-1.0886,  1.8997,  0.0216,  1.3510,  1.3770,  1.8031,  0.8394]],
       dtype=torch.float64)
	q_value: tensor([[-0.0019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 124
	action: tensor([[ 4.2784,  2.9272,  0.5735, -1.5316, -0.3826,  2.2067,  0.7390]],
       dtype=torch.float64)
	q_value: tensor([[-0.0019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 125
	action: tensor([[ 0.9665, -0.4880, -1.4631, -2.1812, -1.2907, -2.0377, -1.5000]],
       dtype=torch.float64)
	q_value: tensor([[-0.0019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 126
	action: tensor([[-0.3050, -0.4266,  1.3955, -2.2356, -0.0216, -1.6437, -1.1983]],
       dtype=torch.float64)
	q_value: tensor([[-0.0019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25689805036555, distance: 0.9864632115873846 entropy 1.8700141906738281
epoch: 0, step: 127
	action: tensor([[ 1.2389,  2.7564,  1.7739,  1.4550, -0.5369, -0.6877,  0.1855]],
       dtype=torch.float64)
	q_value: tensor([[0.0536]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
LOSS epoch 0 actor 819.1570182269608 critic 2444.621949405996 
epoch: 1, step: 0
	action: tensor([[ 0.6485,  0.3139, -1.5348, -2.6723, -0.0606,  0.8937, -1.4123]],
       dtype=torch.float64)
	q_value: tensor([[-0.2991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 1
	action: tensor([[-0.5198,  1.8691,  0.6595,  0.9901,  0.1721, -0.5814,  0.8770]],
       dtype=torch.float64)
	q_value: tensor([[-0.2991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 2
	action: tensor([[ 1.9684,  1.4667,  1.7451, -0.2106,  1.6192, -1.1560,  1.5223]],
       dtype=torch.float64)
	q_value: tensor([[-0.2991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 3
	action: tensor([[ 0.1209,  1.2774, -2.4220, -2.5477, -0.1045, -2.3515, -2.7600]],
       dtype=torch.float64)
	q_value: tensor([[-0.2991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 4
	action: tensor([[ 2.2985,  1.8801,  0.9114,  0.2745, -0.9129,  0.8559, -1.4226]],
       dtype=torch.float64)
	q_value: tensor([[-0.2991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 5
	action: tensor([[-2.1146,  1.3938, -0.4238,  0.4293, -1.0272,  2.1540,  2.1851]],
       dtype=torch.float64)
	q_value: tensor([[-0.2991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 6
	action: tensor([[-3.6114,  1.6595, -1.5461, -2.9958,  0.6050, -1.4390,  2.8804]],
       dtype=torch.float64)
	q_value: tensor([[-0.2991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 7
	action: tensor([[ 0.0205, -2.1311,  1.1090, -1.6254,  1.1859,  0.5957,  2.1622]],
       dtype=torch.float64)
	q_value: tensor([[-0.2991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 8
	action: tensor([[1.2389, 1.8857, 0.5340, 0.8013, 3.2776, 0.4684, 0.9958]],
       dtype=torch.float64)
	q_value: tensor([[-0.2991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 9
	action: tensor([[ 1.3231,  0.3363,  0.5200, -0.7021, -0.9926,  1.1491,  1.6614]],
       dtype=torch.float64)
	q_value: tensor([[-0.2991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9238956900534891, distance: 0.31569035937283685 entropy 1.8700141906738281
epoch: 1, step: 10
	action: tensor([[ 0.8769,  1.2170,  0.6440, -0.7898, -1.8897,  0.1195,  1.6786]],
       dtype=torch.float64)
	q_value: tensor([[-0.3843]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9187384391898945, distance: 0.32621150440052954 entropy 1.8700141906738281
epoch: 1, step: 11
	action: tensor([[-0.5585,  0.3899,  3.9063, -0.6686,  2.0159, -1.3977,  1.4613]],
       dtype=torch.float64)
	q_value: tensor([[-0.4014]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 12
	action: tensor([[-0.0849, -0.2094,  1.5609,  1.9199,  0.6351, -0.3936, -1.8484]],
       dtype=torch.float64)
	q_value: tensor([[-0.2991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 13
	action: tensor([[-1.8307, -0.2192, -2.2656, -1.3135, -1.0871,  0.8865, -1.2740]],
       dtype=torch.float64)
	q_value: tensor([[-0.2991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 14
	action: tensor([[-2.2200, -3.4081,  0.3024,  0.5354, -1.4716,  0.8252,  1.2641]],
       dtype=torch.float64)
	q_value: tensor([[-0.2991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 15
	action: tensor([[-1.3773,  1.0155,  0.6205,  1.8068,  0.1857, -1.1089,  1.8356]],
       dtype=torch.float64)
	q_value: tensor([[-0.2991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 16
	action: tensor([[ 0.0050, -1.2317,  1.1658,  0.3386,  1.0189,  1.3985, -0.7184]],
       dtype=torch.float64)
	q_value: tensor([[-0.2991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13702940547569598, distance: 1.2202323694895425 entropy 1.8700141906738281
epoch: 1, step: 17
	action: tensor([[-1.1094,  1.4909,  1.1636, -0.1780,  0.7410,  0.4626, -2.6825]],
       dtype=torch.float64)
	q_value: tensor([[-0.2169]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 18
	action: tensor([[-0.3713, -1.8881, -2.5387, -0.1101, -1.1052, -1.2319, -0.0497]],
       dtype=torch.float64)
	q_value: tensor([[-0.2991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 19
	action: tensor([[-1.5639,  0.1217, -3.3027, -0.9476, -1.4292, -0.3167, -1.9037]],
       dtype=torch.float64)
	q_value: tensor([[-0.2991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.5256098687881658 entropy 1.8700141906738281
epoch: 1, step: 20
	action: tensor([[-2.9591,  0.5175,  1.7283,  0.3008,  2.9361, -2.0934, -3.5917]],
       dtype=torch.float64)
	q_value: tensor([[-0.2991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 21
	action: tensor([[ 0.9090,  0.0124,  1.8756, -1.2667,  1.4028, -1.1029,  0.2246]],
       dtype=torch.float64)
	q_value: tensor([[-0.2991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 22
	action: tensor([[ 0.3536,  1.6559,  0.4396,  0.2551, -0.9050,  0.5772,  0.6855]],
       dtype=torch.float64)
	q_value: tensor([[-0.2991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 23
	action: tensor([[ 0.7720, -1.9950, -0.5449, -3.2035, -2.1580, -3.4573,  2.3672]],
       dtype=torch.float64)
	q_value: tensor([[-0.2991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 24
	action: tensor([[-2.6709,  0.1305,  2.8069,  0.0121, -2.7421, -2.0989,  0.4979]],
       dtype=torch.float64)
	q_value: tensor([[-0.2991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 25
	action: tensor([[ 0.0987,  0.6741,  0.0954,  0.0459,  0.0049, -1.1047, -2.0505]],
       dtype=torch.float64)
	q_value: tensor([[-0.2991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 26
	action: tensor([[-0.6400,  0.8002,  2.0200, -2.8450,  0.7739,  0.1620,  0.3333]],
       dtype=torch.float64)
	q_value: tensor([[-0.2991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 27
	action: tensor([[ 2.2081, -0.5852,  0.4352, -0.0691,  2.5364, -0.2581,  1.7122]],
       dtype=torch.float64)
	q_value: tensor([[-0.2991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1272088493856265, distance: 1.214951350424851 entropy 1.8700141906738281
epoch: 1, step: 28
	action: tensor([[-1.6740, -0.9532,  3.3352,  0.3426,  1.0412, -2.9287,  3.3566]],
       dtype=torch.float64)
	q_value: tensor([[-0.3255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 29
	action: tensor([[-0.3949,  1.4433,  0.4457, -2.6243, -0.6473,  1.6700, -1.7296]],
       dtype=torch.float64)
	q_value: tensor([[-0.2991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 30
	action: tensor([[ 0.6133,  1.9844,  0.6653,  0.6980,  0.7913, -1.7512, -1.4768]],
       dtype=torch.float64)
	q_value: tensor([[-0.2991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 31
	action: tensor([[-1.9064,  1.0388,  1.2171, -2.3569,  0.7128,  0.6562,  0.8717]],
       dtype=torch.float64)
	q_value: tensor([[-0.2991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 32
	action: tensor([[ 0.5060,  1.5974, -1.6738, -2.9141,  2.8242, -0.2137,  2.1679]],
       dtype=torch.float64)
	q_value: tensor([[-0.2991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 33
	action: tensor([[ 0.2779, -0.0179,  1.3375, -0.4143, -1.5223,  0.9325,  2.2850]],
       dtype=torch.float64)
	q_value: tensor([[-0.2991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.49699372423595434, distance: 0.8116025353635854 entropy 1.8700141906738281
epoch: 1, step: 34
	action: tensor([[-3.5727, -2.8329, -3.1134, -3.2112, -0.4511, -1.8467,  1.0700]],
       dtype=torch.float64)
	q_value: tensor([[-0.4096]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 35
	action: tensor([[-0.9129, -1.3318,  0.9584,  1.9255,  3.0802, -0.5351, -1.3244]],
       dtype=torch.float64)
	q_value: tensor([[-0.2991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5219839414856465, distance: 0.7911848025974144 entropy 1.8700141906738281
epoch: 1, step: 36
	action: tensor([[-0.5038,  0.8747,  0.8072, -2.3156, -0.9738, -0.3393,  0.5048]],
       dtype=torch.float64)
	q_value: tensor([[-0.3201]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.33049637404447463, distance: 1.3199684200473971 entropy 1.8700141906738281
epoch: 1, step: 37
	action: tensor([[-1.7477, -0.3487,  2.8043,  0.0598, -3.1165, -0.4664, -0.4684]],
       dtype=torch.float64)
	q_value: tensor([[-0.3170]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.774516541111156, distance: 1.9061205012949083 entropy 1.8700141906738281
epoch: 1, step: 38
	action: tensor([[-1.5121,  1.6378, -2.4040, -1.6583,  0.5520,  3.1559,  0.2145]],
       dtype=torch.float64)
	q_value: tensor([[-0.3764]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 39
	action: tensor([[ 1.6477, -0.6679,  0.0460, -0.4615, -0.5758, -1.4288,  0.1233]],
       dtype=torch.float64)
	q_value: tensor([[-0.2991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.48003048561369677, distance: 1.3921692079976478 entropy 1.8700141906738281
epoch: 1, step: 40
	action: tensor([[-1.4636,  0.5834,  0.7950, -1.3561, -1.1246, -0.8185, -0.0236]],
       dtype=torch.float64)
	q_value: tensor([[-0.2292]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1368376108933833, distance: 1.6727939644941527 entropy 1.8700141906738281
epoch: 1, step: 41
	action: tensor([[-0.2786, -1.1500, -1.8618, -0.9649, -1.5487, -4.5564,  0.9647]],
       dtype=torch.float64)
	q_value: tensor([[-0.2513]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 42
	action: tensor([[ 1.1369,  0.6654,  1.0885, -1.4676,  0.4429, -1.6531, -0.5553]],
       dtype=torch.float64)
	q_value: tensor([[-0.2991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 43
	action: tensor([[ 1.0493,  1.9519, -3.4854,  1.2009,  1.7689, -1.8755,  2.3631]],
       dtype=torch.float64)
	q_value: tensor([[-0.2991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 44
	action: tensor([[ 1.6631, -1.2210, -1.1148,  1.0373, -0.1534, -1.3492, -1.2716]],
       dtype=torch.float64)
	q_value: tensor([[-0.2991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2505743900201949, distance: 1.279709690501784 entropy 1.8700141906738281
epoch: 1, step: 45
	action: tensor([[-0.1830,  1.9556,  1.0058, -1.9280, -1.4342,  1.2530,  1.6869]],
       dtype=torch.float64)
	q_value: tensor([[-0.2273]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 46
	action: tensor([[-0.9618,  0.6471, -3.1770, -0.3567, -1.3552,  0.2586,  0.9673]],
       dtype=torch.float64)
	q_value: tensor([[-0.2991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 47
	action: tensor([[-3.0207,  0.1858, -2.0265, -1.3066, -0.2797, -0.1157,  0.6350]],
       dtype=torch.float64)
	q_value: tensor([[-0.2991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 48
	action: tensor([[ 4.2526, -1.3912, -1.0054, -1.3490, -0.6995, -0.9541,  1.1304]],
       dtype=torch.float64)
	q_value: tensor([[-0.2991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 49
	action: tensor([[ 1.7559,  0.7316, -0.6508, -0.8254,  1.5889, -0.4166,  0.2126]],
       dtype=torch.float64)
	q_value: tensor([[-0.2991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 50
	action: tensor([[ 0.6779,  0.9949,  1.3530,  0.2214,  1.5555, -3.3861,  0.2440]],
       dtype=torch.float64)
	q_value: tensor([[-0.2991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 51
	action: tensor([[-0.7634, -0.9192, -1.0649, -0.3144, -0.5941,  1.1529, -1.1606]],
       dtype=torch.float64)
	q_value: tensor([[-0.2991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1215330275802429, distance: 1.6667927081626297 entropy 1.8700141906738281
epoch: 1, step: 52
	action: tensor([[-1.9287, -2.4798,  0.9551,  1.7658, -0.4330,  0.8675,  1.7561]],
       dtype=torch.float64)
	q_value: tensor([[-0.2359]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 53
	action: tensor([[ 0.0307, -0.7984, -1.1801, -0.1450, -0.9095,  0.8080,  0.9196]],
       dtype=torch.float64)
	q_value: tensor([[-0.2991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5643044977557898, distance: 1.4312560658143145 entropy 1.8700141906738281
epoch: 1, step: 54
	action: tensor([[ 2.1940,  1.1555, -0.1342, -1.5423, -1.3173, -1.9710,  0.9374]],
       dtype=torch.float64)
	q_value: tensor([[-0.2931]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 55
	action: tensor([[ 0.2667, -1.5756,  3.7615,  0.3504, -1.5254, -1.1012,  1.9307]],
       dtype=torch.float64)
	q_value: tensor([[-0.2991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 56
	action: tensor([[ 1.2569, -0.3422,  1.7185, -1.5524,  0.2524, -1.2907, -0.9887]],
       dtype=torch.float64)
	q_value: tensor([[-0.2991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 57
	action: tensor([[ 1.9944e+00, -1.9379e+00,  2.7375e+00, -1.4633e-03, -8.0725e-01,
         -2.5539e-01, -2.4589e-01]], dtype=torch.float64)
	q_value: tensor([[-0.2991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 58
	action: tensor([[-1.2381e-03,  8.9887e-01,  2.1131e+00,  6.5646e-01,  1.2347e+00,
          1.5036e-01, -8.3452e-01]], dtype=torch.float64)
	q_value: tensor([[-0.2991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 59
	action: tensor([[-1.9055,  0.5074,  0.5881,  0.3047,  0.0322, -1.8821, -2.6341]],
       dtype=torch.float64)
	q_value: tensor([[-0.2991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 60
	action: tensor([[-0.3387,  0.2739,  1.5952, -0.3345,  0.4342, -0.9170,  0.3026]],
       dtype=torch.float64)
	q_value: tensor([[-0.2991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 61
	action: tensor([[ 0.7344,  0.4873, -2.6682,  1.6546,  0.5163,  0.6608, -0.3457]],
       dtype=torch.float64)
	q_value: tensor([[-0.2991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 62
	action: tensor([[-0.1655,  0.7073, -1.5888,  0.2310, -1.8948, -1.8191, -0.5118]],
       dtype=torch.float64)
	q_value: tensor([[-0.2991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 63
	action: tensor([[ 0.6958, -2.0746,  0.6573, -0.4993,  0.0621, -0.0486,  1.2400]],
       dtype=torch.float64)
	q_value: tensor([[-0.2991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 64
	action: tensor([[ 0.2509, -2.1847,  0.3522,  1.2201, -3.1225, -0.4666,  0.4097]],
       dtype=torch.float64)
	q_value: tensor([[-0.2991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 65
	action: tensor([[ 1.3218, -0.6916,  0.4716,  2.9358,  2.3010,  0.5396,  0.7750]],
       dtype=torch.float64)
	q_value: tensor([[-0.2991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 66
	action: tensor([[-0.1775,  2.2523,  0.1396, -0.1767, -1.0841,  1.2159, -1.1748]],
       dtype=torch.float64)
	q_value: tensor([[-0.2991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 67
	action: tensor([[-0.8446,  1.8218, -1.8126,  0.5636, -1.3390, -0.3871,  0.8389]],
       dtype=torch.float64)
	q_value: tensor([[-0.2991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 68
	action: tensor([[ 1.1987,  1.0852, -0.8045, -2.2505, -1.5486,  0.7206,  0.6359]],
       dtype=torch.float64)
	q_value: tensor([[-0.2991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 69
	action: tensor([[ 0.7208,  0.9808,  0.3209, -2.0901,  2.2248,  0.3266, -3.0303]],
       dtype=torch.float64)
	q_value: tensor([[-0.2991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 70
	action: tensor([[-2.8083, -1.3812, -1.1324, -1.6071,  0.0833, -2.5417,  1.2906]],
       dtype=torch.float64)
	q_value: tensor([[-0.2991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 71
	action: tensor([[ 0.5350, -0.8224, -0.3936,  1.7933, -0.9757,  1.4787, -0.9786]],
       dtype=torch.float64)
	q_value: tensor([[-0.2991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4190477405308667, distance: 0.8722217573392987 entropy 1.8700141906738281
epoch: 1, step: 72
	action: tensor([[ 0.3763,  0.2685, -1.9611, -1.1819,  1.5638, -1.7023, -0.4426]],
       dtype=torch.float64)
	q_value: tensor([[-0.2283]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7522368153229498, distance: 0.5696066889332337 entropy 1.8700141906738281
epoch: 1, step: 73
	action: tensor([[-0.9440, -1.7970, -1.8954, -0.1256, -2.0100,  0.0029,  0.6467]],
       dtype=torch.float64)
	q_value: tensor([[-0.3095]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 74
	action: tensor([[-1.2717, -0.2115,  0.3611,  1.2386,  1.3377,  0.3032,  0.8215]],
       dtype=torch.float64)
	q_value: tensor([[-0.2991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21274220755279205, distance: 1.2602042491950352 entropy 1.8700141906738281
epoch: 1, step: 75
	action: tensor([[-0.2040, -0.3091, -0.3635,  4.5734,  2.4857,  4.1078, -0.0286]],
       dtype=torch.float64)
	q_value: tensor([[-0.2085]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 76
	action: tensor([[-3.2435, -2.0740, -2.2815, -1.4712, -1.3532,  0.1274,  0.5043]],
       dtype=torch.float64)
	q_value: tensor([[-0.2991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 77
	action: tensor([[ 1.8968, -0.2916, -0.4136,  1.4739,  2.3190,  0.2035,  0.3852]],
       dtype=torch.float64)
	q_value: tensor([[-0.2991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 78
	action: tensor([[-0.8956,  0.2926, -0.7317, -1.0698,  1.2449, -2.0992,  1.8327]],
       dtype=torch.float64)
	q_value: tensor([[-0.2991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3049439380741754, distance: 1.3072318492124582 entropy 1.8700141906738281
epoch: 1, step: 79
	action: tensor([[ 1.9923, -1.7550, -1.5239, -1.1016, -0.5134, -1.6505, -1.7667]],
       dtype=torch.float64)
	q_value: tensor([[-0.3260]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 80
	action: tensor([[ 0.3896, -2.0988, -0.8102, -1.7410, -3.7969,  1.1379, -0.7507]],
       dtype=torch.float64)
	q_value: tensor([[-0.2991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 81
	action: tensor([[-1.2018, -1.9675,  0.8056,  1.3547,  0.2439,  2.0947, -0.5504]],
       dtype=torch.float64)
	q_value: tensor([[-0.2991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 82
	action: tensor([[-0.5377,  0.8125, -0.3616,  1.6080, -1.6733, -0.8316,  0.0610]],
       dtype=torch.float64)
	q_value: tensor([[-0.2991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 83
	action: tensor([[ 1.4662,  1.2886,  2.7655,  1.6440,  0.9513, -0.4904, -0.2693]],
       dtype=torch.float64)
	q_value: tensor([[-0.2991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 84
	action: tensor([[-2.1207, -3.5710, -3.4775,  1.0032,  1.4508, -1.5349,  3.5028]],
       dtype=torch.float64)
	q_value: tensor([[-0.2991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 85
	action: tensor([[ 0.0681,  1.2099,  1.2788, -0.3128,  0.2155, -0.5314, -1.1015]],
       dtype=torch.float64)
	q_value: tensor([[-0.2991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 86
	action: tensor([[-2.2471, -2.3055, -0.3477, -1.5359,  0.5568, -0.3824, -0.0386]],
       dtype=torch.float64)
	q_value: tensor([[-0.2991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 87
	action: tensor([[ 0.7884, -2.0368, -0.9287, -1.6397,  1.7246,  0.3607,  1.1865]],
       dtype=torch.float64)
	q_value: tensor([[-0.2991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 88
	action: tensor([[ 0.7602, -0.1155,  3.7876,  1.0355,  1.9867,  1.9992, -0.1238]],
       dtype=torch.float64)
	q_value: tensor([[-0.2991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 89
	action: tensor([[-0.6977, -1.2634, -1.9084,  2.2888, -0.3090,  0.3029,  0.5173]],
       dtype=torch.float64)
	q_value: tensor([[-0.2991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.285015425810488, distance: 1.2972117202107951 entropy 1.8700141906738281
epoch: 1, step: 90
	action: tensor([[ 1.1814, -0.9607,  0.1375,  1.9652,  1.3244,  0.1394,  2.3386]],
       dtype=torch.float64)
	q_value: tensor([[-0.2499]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8385363442381071, distance: 0.45982659283667 entropy 1.8700141906738281
epoch: 1, step: 91
	action: tensor([[-3.1243,  1.3785,  1.9402,  0.8973,  0.7730, -3.5665,  0.7248]],
       dtype=torch.float64)
	q_value: tensor([[-0.2740]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 92
	action: tensor([[ 0.1939,  2.5010, -0.8825,  0.5129,  2.5130, -0.1723,  0.3903]],
       dtype=torch.float64)
	q_value: tensor([[-0.2991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 93
	action: tensor([[ 1.9422,  1.3804, -0.5795, -1.9716, -4.2326,  2.3270,  0.0755]],
       dtype=torch.float64)
	q_value: tensor([[-0.2991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 94
	action: tensor([[-0.2673, -0.1030, -2.3106, -0.7490,  2.3342, -0.1842,  0.3821]],
       dtype=torch.float64)
	q_value: tensor([[-0.2991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 95
	action: tensor([[ 0.1556,  0.6755, -0.9945,  1.1461,  1.0231,  3.1857,  0.1402]],
       dtype=torch.float64)
	q_value: tensor([[-0.2991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 96
	action: tensor([[ 1.4454,  1.0091, -0.1129, -0.1062,  0.2469,  0.9803,  1.4047]],
       dtype=torch.float64)
	q_value: tensor([[-0.2991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 97
	action: tensor([[-4.2059, -0.0461, -1.2245, -0.3024, -1.3718, -0.2432, -0.9084]],
       dtype=torch.float64)
	q_value: tensor([[-0.2991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 98
	action: tensor([[-1.2584, -1.6304,  1.6379, -2.9384, -2.3200, -2.6304,  0.2772]],
       dtype=torch.float64)
	q_value: tensor([[-0.2991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 99
	action: tensor([[ 0.4154,  0.3323, -0.7252,  1.3285, -0.1973, -1.2076, -3.6247]],
       dtype=torch.float64)
	q_value: tensor([[-0.2991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 100
	action: tensor([[ 3.1568, -1.7307,  0.5202,  1.5598, -0.8964, -0.3760,  0.7465]],
       dtype=torch.float64)
	q_value: tensor([[-0.2991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 101
	action: tensor([[ 0.7960, -1.1677, -1.8685,  1.9593, -1.1198, -4.9634,  1.8132]],
       dtype=torch.float64)
	q_value: tensor([[-0.2991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 102
	action: tensor([[ 0.0330,  0.0446,  0.4518,  1.3859, -3.5768,  0.0506,  2.0575]],
       dtype=torch.float64)
	q_value: tensor([[-0.2991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 103
	action: tensor([[-0.1573, -0.3932,  0.6248, -1.4391, -1.2623, -2.6036,  1.3390]],
       dtype=torch.float64)
	q_value: tensor([[-0.2991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1753458439029697, distance: 1.03918440173513 entropy 1.8700141906738281
epoch: 1, step: 104
	action: tensor([[ 1.5513, -1.8939, -2.1137, -1.5179,  3.8317,  0.1102, -0.7511]],
       dtype=torch.float64)
	q_value: tensor([[-0.3171]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 105
	action: tensor([[-1.8036, -0.7361,  1.7800,  0.0337, -0.8384, -2.4806, -1.1838]],
       dtype=torch.float64)
	q_value: tensor([[-0.2991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 106
	action: tensor([[-1.0923, -0.5971,  1.2373, -2.3550,  0.3204, -1.2299,  0.3449]],
       dtype=torch.float64)
	q_value: tensor([[-0.2991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1313722038604248, distance: 1.0665311752461137 entropy 1.8700141906738281
epoch: 1, step: 107
	action: tensor([[-1.9689,  1.3519,  1.0412, -2.0234, -1.3219,  1.6326,  0.8223]],
       dtype=torch.float64)
	q_value: tensor([[-0.2728]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 108
	action: tensor([[ 0.7040, -0.4478,  0.2443,  0.0670, -0.0381,  1.9512,  0.9476]],
       dtype=torch.float64)
	q_value: tensor([[-0.2991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8247936679032546, distance: 0.47899567516475344 entropy 1.8700141906738281
epoch: 1, step: 109
	action: tensor([[ 0.1207, -0.1990, -0.5606,  0.7722,  1.3618,  1.1170, -1.6672]],
       dtype=torch.float64)
	q_value: tensor([[-0.3016]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 110
	action: tensor([[ 0.3931, -3.3003, -0.4269,  1.5214, -1.4850,  1.5743, -0.6721]],
       dtype=torch.float64)
	q_value: tensor([[-0.2991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 111
	action: tensor([[ 1.6674,  1.6710, -0.2124, -0.4862,  0.5157,  0.1018, -1.2453]],
       dtype=torch.float64)
	q_value: tensor([[-0.2991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 112
	action: tensor([[ 0.9086,  1.1893,  0.5695, -0.7514, -1.4159, -0.6636,  1.5961]],
       dtype=torch.float64)
	q_value: tensor([[-0.2991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 113
	action: tensor([[-1.5055,  0.2800, -0.8902, -0.1502,  0.6219,  0.3465, -0.6106]],
       dtype=torch.float64)
	q_value: tensor([[-0.2991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1613527815944868, distance: 1.6823622815694401 entropy 1.8700141906738281
epoch: 1, step: 114
	action: tensor([[ 2.2682,  0.9073, -0.4880, -0.9625,  2.6777, -1.4012, -1.1905]],
       dtype=torch.float64)
	q_value: tensor([[-0.2097]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 115
	action: tensor([[-2.2199,  1.1501, -2.5548, -2.0276,  0.5083, -2.6263,  0.3029]],
       dtype=torch.float64)
	q_value: tensor([[-0.2991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 116
	action: tensor([[ 2.2683, -2.2608, -0.7549,  0.1178, -0.1257, -0.1344, -0.2270]],
       dtype=torch.float64)
	q_value: tensor([[-0.2991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 117
	action: tensor([[-0.9476,  1.2749, -0.6345, -1.1433,  1.6876,  0.7483, -0.0186]],
       dtype=torch.float64)
	q_value: tensor([[-0.2991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 118
	action: tensor([[ 0.5207, -2.5270, -0.5079, -0.1747,  0.2004,  1.0862, -1.0329]],
       dtype=torch.float64)
	q_value: tensor([[-0.2991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 119
	action: tensor([[0.7549, 0.3468, 1.8855, 4.5649, 3.3094, 2.4425, 2.4130]],
       dtype=torch.float64)
	q_value: tensor([[-0.2991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 120
	action: tensor([[-0.6993, -1.5563,  1.4597,  1.1296,  0.9663,  0.0985,  0.0903]],
       dtype=torch.float64)
	q_value: tensor([[-0.2991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04295612984191599, distance: 1.1194961806527939 entropy 1.8700141906738281
epoch: 1, step: 121
	action: tensor([[-0.7510, -0.3341, -1.3118, -0.4447,  2.2965,  1.5936,  1.4440]],
       dtype=torch.float64)
	q_value: tensor([[-0.1770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.207491572546189, distance: 1.018728926159399 entropy 1.8700141906738281
epoch: 1, step: 122
	action: tensor([[ 0.6611, -0.2227, -2.0748,  1.8466,  0.3145,  0.4948, -0.5768]],
       dtype=torch.float64)
	q_value: tensor([[-0.3708]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03370698087066959, distance: 1.1634706117405553 entropy 1.8700141906738281
epoch: 1, step: 123
	action: tensor([[-3.3798, -4.1935,  1.2640,  1.1231, -1.9354, -2.4988,  2.1379]],
       dtype=torch.float64)
	q_value: tensor([[-0.2105]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 124
	action: tensor([[-0.5493, -0.8166, -2.6036,  0.8611, -1.5303,  2.5182, -0.9296]],
       dtype=torch.float64)
	q_value: tensor([[-0.2991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.062435572986158316, distance: 1.1795272945911763 entropy 1.8700141906738281
epoch: 1, step: 125
	action: tensor([[ 0.0242,  0.0516,  1.4006, -1.5613,  1.9570, -2.8814, -0.0297]],
       dtype=torch.float64)
	q_value: tensor([[-0.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.499517588322563, distance: 0.8095638426898398 entropy 1.8700141906738281
epoch: 1, step: 126
	action: tensor([[ 0.3641, -0.5707, -4.4182,  0.5482, -1.8844, -1.8485, -0.0878]],
       dtype=torch.float64)
	q_value: tensor([[-0.3017]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1268715908103886 entropy 1.8700141906738281
epoch: 1, step: 127
	action: tensor([[ 2.4401,  0.8239,  0.3753, -0.1733, -1.4822, -1.1750,  1.0640]],
       dtype=torch.float64)
	q_value: tensor([[-0.2991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
LOSS epoch 1 actor 778.084011034207 critic 2388.908428364147 
epoch: 2, step: 0
	action: tensor([[ 1.1160, -2.7357,  1.6272, -3.1754,  1.3277, -2.8020,  1.0918]],
       dtype=torch.float64)
	q_value: tensor([[-0.6675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 2, step: 1
	action: tensor([[ 0.4185,  1.2630, -1.0721,  0.3261,  2.2034,  3.8740,  0.7991]],
       dtype=torch.float64)
	q_value: tensor([[-0.6675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 2, step: 2
	action: tensor([[-0.6168,  3.0629,  1.7717,  2.4958,  4.1351, -0.9187,  2.6100]],
       dtype=torch.float64)
	q_value: tensor([[-0.6675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 2, step: 3
	action: tensor([[ 0.1144,  0.9189, -1.4821,  2.2480, -1.9673, -1.7699, -0.4425]],
       dtype=torch.float64)
	q_value: tensor([[-0.6675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 2, step: 4
	action: tensor([[-1.4233,  2.2702, -1.3393,  2.0687,  0.1564,  1.0623, -0.1850]],
       dtype=torch.float64)
	q_value: tensor([[-0.6675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 2, step: 5
	action: tensor([[ 0.5641,  0.5973,  0.5414,  0.5030, -0.8974,  2.2628, -2.1581]],
       dtype=torch.float64)
	q_value: tensor([[-0.6675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 2, step: 6
	action: tensor([[-0.5639, -2.3365, -0.7542, -1.1752,  2.4499,  0.5879, -1.5770]],
       dtype=torch.float64)
	q_value: tensor([[-0.6675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 2, step: 7
	action: tensor([[ 0.6164, -0.6731, -1.0902, -0.0247, -2.3158, -1.7207, -0.0716]],
       dtype=torch.float64)
	q_value: tensor([[-0.6675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 2, step: 8
	action: tensor([[ 2.6702, -0.2942, -0.8320, -0.7244,  0.2975, -0.5909,  0.5904]],
       dtype=torch.float64)
	q_value: tensor([[-0.6675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 2, step: 9
	action: tensor([[ 2.1193, -0.8795, -2.6580,  0.8188,  1.0127, -0.1217, -2.5060]],
       dtype=torch.float64)
	q_value: tensor([[-0.6675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 2, step: 10
	action: tensor([[ 1.5964, -0.7741, -0.3003, -1.1903, -1.0532, -0.8702,  0.9671]],
       dtype=torch.float64)
	q_value: tensor([[-0.6675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 2, step: 11
	action: tensor([[ 1.5270, -1.2417, -0.0574, -0.8047, -3.7733, -0.1451, -0.6874]],
       dtype=torch.float64)
	q_value: tensor([[-0.6675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 2, step: 12
	action: tensor([[ 2.0164,  0.9398,  1.8359, -3.6770,  0.1614, -3.8496, -2.2315]],
       dtype=torch.float64)
	q_value: tensor([[-0.6675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 2, step: 13
	action: tensor([[-0.1407, -1.2764,  0.6977,  1.7257,  2.5523,  0.5179,  0.3477]],
       dtype=torch.float64)
	q_value: tensor([[-0.6675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5396841095112083, distance: 0.7763984808519466 entropy 1.8700141906738281
epoch: 2, step: 14
	action: tensor([[-1.2588,  0.9350, -3.1080, -0.7215, -0.1839, -0.2669, -2.4704]],
       dtype=torch.float64)
	q_value: tensor([[-0.6294]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 2, step: 15
	action: tensor([[ 2.3521, -0.9417,  0.5996, -0.9812, -2.3277,  1.0474,  0.7459]],
       dtype=torch.float64)
	q_value: tensor([[-0.6675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.010901965515880407, distance: 1.1505651457712902 entropy 1.8700141906738281
epoch: 2, step: 16
	action: tensor([[-1.8422,  1.3006, -1.1610, -0.1949, -0.5192, -2.3024,  2.1712]],
       dtype=torch.float64)
	q_value: tensor([[-0.8282]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 2, step: 17
	action: tensor([[ 3.1150, -2.1275,  3.1071, -0.3638,  0.1698, -0.9437, -1.4838]],
       dtype=torch.float64)
	q_value: tensor([[-0.6675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 2, step: 18
	action: tensor([[ 0.7888,  0.2691,  1.8417,  1.5217, -1.0017, -1.3249, -2.5756]],
       dtype=torch.float64)
	q_value: tensor([[-0.6675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 2, step: 19
	action: tensor([[-2.7471,  0.7198,  0.4594,  0.5554, -0.7655, -4.8656,  1.7752]],
       dtype=torch.float64)
	q_value: tensor([[-0.6675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 2, step: 20
	action: tensor([[ 0.6990, -2.1989, -0.2396,  0.0981, -0.5914, -0.5613, -0.8425]],
       dtype=torch.float64)
	q_value: tensor([[-0.6675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 2, step: 21
	action: tensor([[ 0.4967, -0.2361, -1.0668, -0.9339,  1.1570, -1.3441, -0.2208]],
       dtype=torch.float64)
	q_value: tensor([[-0.6675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1411835650829234, distance: 1.0604907057672046 entropy 1.8700141906738281
epoch: 2, step: 22
	action: tensor([[ 0.3923, -0.5692, -0.9928,  0.9283, -2.1169, -1.4314,  0.0060]],
       dtype=torch.float64)
	q_value: tensor([[-0.5691]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6659946132396334, distance: 0.6613531536373989 entropy 1.8700141906738281
epoch: 2, step: 23
	action: tensor([[-0.5742, -1.7911,  0.3935, -1.5936, -1.0154,  2.6550,  0.4878]],
       dtype=torch.float64)
	q_value: tensor([[-0.6080]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 2, step: 24
	action: tensor([[ 0.5738,  3.3818,  2.4685, -3.9521,  0.3634,  1.5161,  0.5912]],
       dtype=torch.float64)
	q_value: tensor([[-0.6675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 2, step: 25
	action: tensor([[-1.0741, -0.4130, -0.0111, -2.6021,  1.2804,  0.2130,  0.9925]],
       dtype=torch.float64)
	q_value: tensor([[-0.6675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 2, step: 26
	action: tensor([[-0.2113, -0.2643, -2.7378,  1.5891,  0.3904,  2.2412,  1.4940]],
       dtype=torch.float64)
	q_value: tensor([[-0.6675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08805547548608028, distance: 1.0928005459774999 entropy 1.8700141906738281
epoch: 2, step: 27
	action: tensor([[ 1.8279,  1.0549, -1.3404,  1.3820,  2.5009, -1.7664, -3.1664]],
       dtype=torch.float64)
	q_value: tensor([[-0.8585]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 2, step: 28
	action: tensor([[-1.4475, -1.6781,  0.5059,  0.2683, -1.7983,  2.6781, -1.3189]],
       dtype=torch.float64)
	q_value: tensor([[-0.6675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8676708880551289, distance: 1.5638923957711395 entropy 1.8700141906738281
epoch: 2, step: 29
	action: tensor([[ 1.4757, -0.8248,  0.0346, -0.2064,  1.6950, -0.2852, -3.7410]],
       dtype=torch.float64)
	q_value: tensor([[-0.8106]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3422878251886086 entropy 1.8700141906738281
epoch: 2, step: 30
	action: tensor([[-0.6260, -0.3709, -2.2347, -0.0407,  2.5055,  0.5244,  0.9081]],
       dtype=torch.float64)
	q_value: tensor([[-0.6675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4771407298476418, distance: 1.390809440515432 entropy 1.8700141906738281
epoch: 2, step: 31
	action: tensor([[-1.7257,  0.6535, -0.8276, -1.1486,  1.1555, -1.1406, -1.6838]],
       dtype=torch.float64)
	q_value: tensor([[-0.7366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8835750511916185, distance: 1.570536948067899 entropy 1.8700141906738281
epoch: 2, step: 32
	action: tensor([[-1.5367,  0.1905,  1.2256, -0.9908,  1.4266, -0.2432, -1.0949]],
       dtype=torch.float64)
	q_value: tensor([[-0.6354]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.554590014309833, distance: 1.8290151313182679 entropy 1.8700141906738281
epoch: 2, step: 33
	action: tensor([[ 1.3187, -0.2822,  1.7791,  0.5359, -0.2474,  2.1067,  0.6743]],
       dtype=torch.float64)
	q_value: tensor([[-0.6152]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2822194370998129, distance: 0.9695105517503728 entropy 1.8700141906738281
epoch: 2, step: 34
	action: tensor([[-0.4058,  0.1331,  0.8104,  1.7936, -0.2266, -0.4280, -0.4757]],
       dtype=torch.float64)
	q_value: tensor([[-0.7518]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 2, step: 35
	action: tensor([[ 1.9674, -0.0688, -1.0703, -2.4261,  0.2159, -0.9401,  0.3834]],
       dtype=torch.float64)
	q_value: tensor([[-0.6675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 2, step: 36
	action: tensor([[ 0.1472, -1.3299,  1.1218,  0.5125,  1.5425, -3.1683,  2.0114]],
       dtype=torch.float64)
	q_value: tensor([[-0.6675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 2, step: 37
	action: tensor([[ 0.7766,  1.0273,  0.2930, -2.7032,  0.3955,  1.0097,  0.7133]],
       dtype=torch.float64)
	q_value: tensor([[-0.6675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 2, step: 38
	action: tensor([[ 2.9482,  0.8915,  0.5212, -0.8663,  0.9218, -0.3220,  1.3388]],
       dtype=torch.float64)
	q_value: tensor([[-0.6675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 2, step: 39
	action: tensor([[ 0.7180, -1.5144, -0.4236,  0.5961, -0.5068,  1.0741,  1.7366]],
       dtype=torch.float64)
	q_value: tensor([[-0.6675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16502468852965535, distance: 1.0456672602324224 entropy 1.8700141906738281
epoch: 2, step: 40
	action: tensor([[ 4.3277, -0.0100,  1.9119, -0.4995,  2.6631,  0.9572, -0.6657]],
       dtype=torch.float64)
	q_value: tensor([[-0.6711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 2, step: 41
	action: tensor([[-0.2432, -0.4633,  0.1639, -1.9576,  0.5409, -4.2027, -0.6893]],
       dtype=torch.float64)
	q_value: tensor([[-0.6675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 2, step: 42
	action: tensor([[ 0.3061, -1.5875,  1.1275, -2.3572,  0.6173, -2.5568,  2.4755]],
       dtype=torch.float64)
	q_value: tensor([[-0.6675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4841083298760691, distance: 0.8219321174100204 entropy 1.8700141906738281
epoch: 2, step: 43
	action: tensor([[ 0.1294, -1.2704, -2.5799,  2.4604,  0.5930, -1.8900,  1.7413]],
       dtype=torch.float64)
	q_value: tensor([[-0.9367]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.35312191194721687, distance: 1.3311443609363791 entropy 1.8700141906738281
epoch: 2, step: 44
	action: tensor([[ 0.4008, -2.5054,  0.9461, -2.2121,  0.8386, -0.4722,  0.1847]],
       dtype=torch.float64)
	q_value: tensor([[-0.7548]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 2, step: 45
	action: tensor([[-3.1698,  2.0778, -0.0893,  2.1826,  1.5877, -1.2298, -1.5227]],
       dtype=torch.float64)
	q_value: tensor([[-0.6675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 2, step: 46
	action: tensor([[ 1.4005, -1.5214,  0.3351, -0.2855,  0.2030,  0.2835, -1.1565]],
       dtype=torch.float64)
	q_value: tensor([[-0.6675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.790170296707615, distance: 1.5311010936131224 entropy 1.8700141906738281
epoch: 2, step: 47
	action: tensor([[-2.0419, -2.6504, -1.5683, -0.7890, -3.6718, -1.0260, -0.2034]],
       dtype=torch.float64)
	q_value: tensor([[-0.4984]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 2, step: 48
	action: tensor([[ 1.5790, -0.4656, -0.3534, -1.4210,  0.0376,  2.3249, -2.5375]],
       dtype=torch.float64)
	q_value: tensor([[-0.6675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18528474716405063, distance: 1.0329031855527204 entropy 1.8700141906738281
epoch: 2, step: 49
	action: tensor([[-1.8886,  0.1881, -0.8948,  0.7500, -0.9288, -0.9181,  0.2903]],
       dtype=torch.float64)
	q_value: tensor([[-0.7977]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9875892307614196, distance: 1.6133181167962785 entropy 1.8700141906738281
epoch: 2, step: 50
	action: tensor([[ 0.4404,  0.7170, -2.0230, -0.1841, -0.2522, -1.9415, -0.1901]],
       dtype=torch.float64)
	q_value: tensor([[-0.5431]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 2, step: 51
	action: tensor([[ 1.6590,  0.0075, -2.1869, -0.0741,  0.4447,  0.7990, -0.4228]],
       dtype=torch.float64)
	q_value: tensor([[-0.6675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 2, step: 52
	action: tensor([[ 3.0224,  1.3075,  1.1634, -1.1821, -0.3645, -0.6890,  0.7134]],
       dtype=torch.float64)
	q_value: tensor([[-0.6675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 2, step: 53
	action: tensor([[-3.2187,  0.8199, -0.8887, -0.7806, -0.5941, -2.5267, -3.3643]],
       dtype=torch.float64)
	q_value: tensor([[-0.6675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 2, step: 54
	action: tensor([[ 0.5330,  1.8558,  2.6365,  0.1700, -2.5492,  1.4556,  0.0813]],
       dtype=torch.float64)
	q_value: tensor([[-0.6675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 2, step: 55
	action: tensor([[ 0.6916, -0.1706,  3.8343, -1.0194,  0.8223,  0.6263,  1.1009]],
       dtype=torch.float64)
	q_value: tensor([[-0.6675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 2, step: 56
	action: tensor([[-0.2809, -1.4130,  0.2124, -0.2011, -0.0684, -0.5620, -0.5878]],
       dtype=torch.float64)
	q_value: tensor([[-0.6675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0338817890955703, distance: 1.6319977180047611 entropy 1.8700141906738281
epoch: 2, step: 57
	action: tensor([[-0.2975, -1.1992,  3.1677,  1.7295,  2.6152,  1.0921,  2.8426]],
       dtype=torch.float64)
	q_value: tensor([[-0.3773]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.5326769077829152 entropy 1.8700141906738281
epoch: 2, step: 58
	action: tensor([[-1.4172,  0.9868, -1.5247,  1.6425,  0.0750, -3.1906, -0.9664]],
       dtype=torch.float64)
	q_value: tensor([[-0.6675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 2, step: 59
	action: tensor([[-0.0867, -1.1507, -0.3100,  2.5437,  0.6767, -0.5510, -0.9458]],
       dtype=torch.float64)
	q_value: tensor([[-0.6675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2505335578285558, distance: 0.9906786169525724 entropy 1.8700141906738281
epoch: 2, step: 60
	action: tensor([[ 0.6636, -0.2585,  0.5053,  0.9392,  3.0772,  2.2283, -0.1960]],
       dtype=torch.float64)
	q_value: tensor([[-0.5231]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 2, step: 61
	action: tensor([[ 0.2222, -0.5073,  1.4812,  1.4394, -0.7303,  1.5103, -1.0970]],
       dtype=torch.float64)
	q_value: tensor([[-0.6675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 2, step: 62
	action: tensor([[-2.0574, -0.4871, -1.9062, -1.8157, -1.4783, -1.1055,  2.3321]],
       dtype=torch.float64)
	q_value: tensor([[-0.6675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 2, step: 63
	action: tensor([[-0.4580, -0.5489, -2.1741, -0.3476,  0.3538,  2.5116, -2.0294]],
       dtype=torch.float64)
	q_value: tensor([[-0.6675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.37659465923902924, distance: 1.3426404694547172 entropy 1.8700141906738281
epoch: 2, step: 64
	action: tensor([[ 1.8764,  1.7987, -3.4956, -0.7747, -0.9327,  1.6306,  0.3818]],
       dtype=torch.float64)
	q_value: tensor([[-0.7313]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 2, step: 65
	action: tensor([[-0.6772,  0.5401,  0.7950, -1.1786,  1.5205,  0.8208, -0.2543]],
       dtype=torch.float64)
	q_value: tensor([[-0.6675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5827841917880592, distance: 1.439685217177207 entropy 1.8700141906738281
epoch: 2, step: 66
	action: tensor([[ 1.9265, -0.7083,  1.2739, -1.0989,  0.3223,  0.2727,  0.9624]],
       dtype=torch.float64)
	q_value: tensor([[-0.6180]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08614324835789111, distance: 1.0939456751456647 entropy 1.8700141906738281
epoch: 2, step: 67
	action: tensor([[-1.5074, -1.4994, -1.5829, -0.5061,  1.5545, -2.3421,  0.2206]],
       dtype=torch.float64)
	q_value: tensor([[-0.6625]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 2, step: 68
	action: tensor([[-0.7766,  2.1412, -3.0081, -1.6228,  0.9713, -0.3706, -0.6017]],
       dtype=torch.float64)
	q_value: tensor([[-0.6675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 2, step: 69
	action: tensor([[-0.3604,  1.9680, -0.0541, -0.8757,  2.8174,  2.3269, -1.8798]],
       dtype=torch.float64)
	q_value: tensor([[-0.6675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 2, step: 70
	action: tensor([[ 0.6278, -1.0326, -1.1468, -2.4895,  2.5285, -1.7322, -0.4702]],
       dtype=torch.float64)
	q_value: tensor([[-0.6675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1653661753541209, distance: 1.235343963970359 entropy 1.8700141906738281
epoch: 2, step: 71
	action: tensor([[ 0.4142,  1.8840, -3.7208, -1.1655,  2.5079, -3.2377, -1.4595]],
       dtype=torch.float64)
	q_value: tensor([[-0.8586]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 2, step: 72
	action: tensor([[ 1.5897, -1.8158,  1.2919, -1.6803, -1.4289,  0.5934, -0.3909]],
       dtype=torch.float64)
	q_value: tensor([[-0.6675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 2, step: 73
	action: tensor([[-0.6718,  0.8697,  1.0122,  0.5088, -1.0091, -0.0203, -3.7393]],
       dtype=torch.float64)
	q_value: tensor([[-0.6675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 2, step: 74
	action: tensor([[ 0.4761, -1.2994,  1.4536,  2.2276,  0.2517,  1.9130, -0.0359]],
       dtype=torch.float64)
	q_value: tensor([[-0.6675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 2, step: 75
	action: tensor([[-0.5474, -0.0198, -3.0741, -2.4143, -0.6577,  2.0929,  0.3718]],
       dtype=torch.float64)
	q_value: tensor([[-0.6675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 2, step: 76
	action: tensor([[ 2.1783,  1.1029, -1.6927,  2.3387, -1.4238,  0.3995, -2.4158]],
       dtype=torch.float64)
	q_value: tensor([[-0.6675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 2, step: 77
	action: tensor([[-1.9414, -1.4575,  0.4456, -1.4186, -1.4244,  1.6466, -0.5576]],
       dtype=torch.float64)
	q_value: tensor([[-0.6675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 2, step: 78
	action: tensor([[ 2.7679,  0.2001,  1.1072, -2.7140,  1.1353, -1.1316,  2.0938]],
       dtype=torch.float64)
	q_value: tensor([[-0.6675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 2, step: 79
	action: tensor([[-1.7750,  1.1712,  1.4679,  1.3334, -1.2615, -1.5455, -1.1381]],
       dtype=torch.float64)
	q_value: tensor([[-0.6675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 2, step: 80
	action: tensor([[-1.9513, -2.8091, -1.4473, -0.7200, -0.2716,  1.9820, -0.0587]],
       dtype=torch.float64)
	q_value: tensor([[-0.6675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 2, step: 81
	action: tensor([[-2.3461, -0.1623, -1.6508, -0.7243, -1.7656,  1.6181,  1.3745]],
       dtype=torch.float64)
	q_value: tensor([[-0.6675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 2, step: 82
	action: tensor([[ 1.6280, -0.3579, -0.2151,  2.3614, -0.2481, -1.8654, -1.0032]],
       dtype=torch.float64)
	q_value: tensor([[-0.6675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 2, step: 83
	action: tensor([[ 1.4150,  0.3997,  2.3273,  0.2363, -0.8049,  0.4979,  2.3272]],
       dtype=torch.float64)
	q_value: tensor([[-0.6675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 2, step: 84
	action: tensor([[-1.7847, -0.0424,  0.8490, -1.8883,  1.0014,  0.6021,  0.1768]],
       dtype=torch.float64)
	q_value: tensor([[-0.6675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 2, step: 85
	action: tensor([[-1.8527, -1.8736, -0.2699,  3.9661, -1.5491, -0.3833, -1.3285]],
       dtype=torch.float64)
	q_value: tensor([[-0.6675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 2, step: 86
	action: tensor([[-1.5915,  0.6948, -0.1820,  1.7733,  1.4761, -0.3711, -0.3662]],
       dtype=torch.float64)
	q_value: tensor([[-0.6675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 2, step: 87
	action: tensor([[-1.2409,  0.0826,  0.3573, -0.2525,  0.4914, -2.6158,  0.2635]],
       dtype=torch.float64)
	q_value: tensor([[-0.6675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6712253362858542, distance: 1.4793611140189913 entropy 1.8700141906738281
epoch: 2, step: 88
	action: tensor([[-0.5171, -0.2244, -2.7179,  1.1863, -0.5392,  2.2899,  1.8134]],
       dtype=torch.float64)
	q_value: tensor([[-0.5484]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.045291821540391464, distance: 1.1181292656458661 entropy 1.8700141906738281
epoch: 2, step: 89
	action: tensor([[-1.6767,  1.6770, -1.9163, -2.2488,  0.3638, -0.5849,  0.0777]],
       dtype=torch.float64)
	q_value: tensor([[-0.9776]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 2, step: 90
	action: tensor([[-0.9236,  2.4387, -1.8224, -1.6717, -1.4878, -1.0433,  0.3295]],
       dtype=torch.float64)
	q_value: tensor([[-0.6675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 2, step: 91
	action: tensor([[ 1.6122, -0.3713,  0.4892,  0.5267,  0.4201, -1.8676,  1.1546]],
       dtype=torch.float64)
	q_value: tensor([[-0.6675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36272376036044873, distance: 0.9135252513882043 entropy 1.8700141906738281
epoch: 2, step: 92
	action: tensor([[-1.8755,  0.8303, -0.5359,  2.3839, -2.2728, -0.4480, -0.2459]],
       dtype=torch.float64)
	q_value: tensor([[-0.6144]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 2, step: 93
	action: tensor([[0.8802, 1.3453, 0.6352, 0.6010, 1.2625, 0.6047, 1.9086]],
       dtype=torch.float64)
	q_value: tensor([[-0.6675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 2, step: 94
	action: tensor([[ 0.7430,  0.7496, -0.4619, -1.7189,  0.4098,  0.2186, -0.8481]],
       dtype=torch.float64)
	q_value: tensor([[-0.6675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 2, step: 95
	action: tensor([[-1.0006,  0.2397, -0.1516,  3.6168,  0.5023,  0.1652,  1.2687]],
       dtype=torch.float64)
	q_value: tensor([[-0.6675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 2, step: 96
	action: tensor([[ 2.7116, -0.0323, -0.2720,  1.0491, -1.6220,  1.6041,  0.3998]],
       dtype=torch.float64)
	q_value: tensor([[-0.6675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 2, step: 97
	action: tensor([[-0.4407, -0.2385, -0.8617, -1.4100, -2.4488,  1.6618,  2.8617]],
       dtype=torch.float64)
	q_value: tensor([[-0.6675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09871000112784278, distance: 1.086398029584481 entropy 1.8700141906738281
epoch: 2, step: 98
	action: tensor([[-1.7573, -0.5848, -1.8187,  0.3919, -3.3112, -0.3869,  0.8722]],
       dtype=torch.float64)
	q_value: tensor([[-1.3526]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 2, step: 99
	action: tensor([[ 1.0118,  1.3558,  1.2013,  2.4255, -0.5958, -0.9037,  0.1585]],
       dtype=torch.float64)
	q_value: tensor([[-0.6675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 2, step: 100
	action: tensor([[-1.0600, -1.0058,  1.5302, -2.1118, -0.8907,  0.6674, -2.2061]],
       dtype=torch.float64)
	q_value: tensor([[-0.6675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8344079911429845, distance: 1.5499035038769255 entropy 1.8700141906738281
epoch: 2, step: 101
	action: tensor([[ 0.4097, -1.7240, -0.1952,  4.2834,  0.1906, -1.0380,  0.8049]],
       dtype=torch.float64)
	q_value: tensor([[-0.7932]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 2, step: 102
	action: tensor([[-2.3978,  0.6811,  2.7994, -1.6881, -1.0907, -0.7227,  0.4684]],
       dtype=torch.float64)
	q_value: tensor([[-0.6675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 2, step: 103
	action: tensor([[-0.4091, -2.6159, -0.2238,  1.2640, -1.2278,  1.8417,  0.5062]],
       dtype=torch.float64)
	q_value: tensor([[-0.6675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 2, step: 104
	action: tensor([[ 1.1597, -1.0233, -0.2940,  1.8250, -1.1844,  0.3359,  1.2207]],
       dtype=torch.float64)
	q_value: tensor([[-0.6675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8801405858715319, distance: 0.39618020149776156 entropy 1.8700141906738281
epoch: 2, step: 105
	action: tensor([[ 0.4234,  2.1704, -1.3570,  1.5535, -0.1324,  1.0092,  0.4287]],
       dtype=torch.float64)
	q_value: tensor([[-0.6122]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 2, step: 106
	action: tensor([[ 1.9491,  0.8951, -1.6928,  2.6056,  0.4682, -1.1978,  0.8957]],
       dtype=torch.float64)
	q_value: tensor([[-0.6675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 2, step: 107
	action: tensor([[ 2.8042, -0.5246,  0.2563, -0.2590, -1.7083,  0.8045, -2.1089]],
       dtype=torch.float64)
	q_value: tensor([[-0.6675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 2, step: 108
	action: tensor([[ 1.8592,  1.3981, -0.7646,  0.7750,  1.6471,  2.5748,  1.7816]],
       dtype=torch.float64)
	q_value: tensor([[-0.6675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 2, step: 109
	action: tensor([[ 2.0992, -1.5879, -0.5793, -0.1581, -1.2726,  0.7043, -2.1958]],
       dtype=torch.float64)
	q_value: tensor([[-0.6675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6768256677348563, distance: 1.4818377349875962 entropy 1.8700141906738281
epoch: 2, step: 110
	action: tensor([[ 1.7102,  0.7914,  0.1587, -0.8144, -0.1715,  0.1212, -0.1368]],
       dtype=torch.float64)
	q_value: tensor([[-0.6106]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 2, step: 111
	action: tensor([[-0.5567, -1.0788,  1.4117,  2.0004,  1.0930, -1.5733,  1.9211]],
       dtype=torch.float64)
	q_value: tensor([[-0.6675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32309093899878105, distance: 0.9415033226893316 entropy 1.8700141906738281
epoch: 2, step: 112
	action: tensor([[ 1.6018, -1.5082, -2.6202, -0.4102, -1.5094,  0.6194,  0.4514]],
       dtype=torch.float64)
	q_value: tensor([[-0.6470]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 2, step: 113
	action: tensor([[ 0.0795, -1.0440, -1.5000, -1.4114,  1.3953,  2.4232,  1.4072]],
       dtype=torch.float64)
	q_value: tensor([[-0.6675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.46454037271967497, distance: 0.8373751314012717 entropy 1.8700141906738281
epoch: 2, step: 114
	action: tensor([[-0.6239,  0.6431,  0.8793,  0.1922, -0.2599, -3.7934, -0.5960]],
       dtype=torch.float64)
	q_value: tensor([[-1.0186]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 2, step: 115
	action: tensor([[-2.0864,  2.0434,  1.8421, -0.9313,  1.9701,  2.1115, -0.0342]],
       dtype=torch.float64)
	q_value: tensor([[-0.6675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 2, step: 116
	action: tensor([[ 1.7222, -0.9719, -0.2210, -0.3084,  3.0252, -0.7650, -1.7650]],
       dtype=torch.float64)
	q_value: tensor([[-0.6675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3631339871077617, distance: 1.3360600136517555 entropy 1.8700141906738281
epoch: 2, step: 117
	action: tensor([[-2.4492, -0.7457, -1.0507,  1.7355,  0.7240, -0.3886,  0.2246]],
       dtype=torch.float64)
	q_value: tensor([[-0.8000]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 2, step: 118
	action: tensor([[-1.8991,  0.5962,  0.6636,  0.0080, -1.7849, -0.0736,  0.6364]],
       dtype=torch.float64)
	q_value: tensor([[-0.6675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 2, step: 119
	action: tensor([[-0.6164,  0.3861, -1.6710,  1.9389, -2.0955, -1.4713,  1.2960]],
       dtype=torch.float64)
	q_value: tensor([[-0.6675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.022218103470407202, distance: 1.156986994878758 entropy 1.8700141906738281
epoch: 2, step: 120
	action: tensor([[ 0.2784,  0.3011, -0.5718, -0.8480, -2.4149, -2.2589,  1.9119]],
       dtype=torch.float64)
	q_value: tensor([[-0.8616]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 2, step: 121
	action: tensor([[-1.1504,  0.7443, -0.3214,  0.6958,  0.3455, -0.9095, -1.3873]],
       dtype=torch.float64)
	q_value: tensor([[-0.6675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.37777930600747567, distance: 1.3432180587387466 entropy 1.8700141906738281
epoch: 2, step: 122
	action: tensor([[ 0.9071, -0.4236, -2.4299, -0.3505, -1.6461,  3.0290, -1.9060]],
       dtype=torch.float64)
	q_value: tensor([[-0.5180]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 2, step: 123
	action: tensor([[ 2.5499,  1.1259, -0.2468,  2.5106, -1.2899, -0.5792,  0.1597]],
       dtype=torch.float64)
	q_value: tensor([[-0.6675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 2, step: 124
	action: tensor([[ 0.0477, -1.2087,  1.5505,  1.2088, -0.2443,  0.0065,  2.0320]],
       dtype=torch.float64)
	q_value: tensor([[-0.6675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11264192682867391, distance: 1.077968687297903 entropy 1.8700141906738281
epoch: 2, step: 125
	action: tensor([[ 1.7912, -1.8987, -0.0499,  4.4746,  0.8692, -1.2991, -2.3054]],
       dtype=torch.float64)
	q_value: tensor([[-0.6602]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 2, step: 126
	action: tensor([[-2.4311, -1.7034,  3.1972, -0.3167,  0.5768,  2.4547, -0.0470]],
       dtype=torch.float64)
	q_value: tensor([[-0.6675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 2, step: 127
	action: tensor([[-1.6280,  1.0369,  2.7154, -1.4135,  0.6019,  2.0164, -1.2175]],
       dtype=torch.float64)
	q_value: tensor([[-0.6675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
LOSS epoch 2 actor 696.6403994088051 critic 2341.283426166175 
epoch: 3, step: 0
	action: tensor([[-3.5983, -0.3110,  0.6346,  0.8905,  0.2117, -0.5878,  0.6616]],
       dtype=torch.float64)
	q_value: tensor([[-1.3353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 1
	action: tensor([[ 0.3127, -1.0586,  2.4171, -1.3776,  3.5071, -1.7248, -0.3933]],
       dtype=torch.float64)
	q_value: tensor([[-1.3353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2203917958095953 entropy 1.7646539211273193
epoch: 3, step: 2
	action: tensor([[-0.5299, -0.4031, -0.9003,  0.0915,  0.1600, -0.9671,  1.1381]],
       dtype=torch.float64)
	q_value: tensor([[-1.3353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4190449170753594, distance: 1.3631848928034205 entropy 1.7646539211273193
epoch: 3, step: 3
	action: tensor([[-1.1345,  1.0933,  0.6726, -0.0887, -0.3705,  0.0651,  0.3939]],
       dtype=torch.float64)
	q_value: tensor([[-0.9768]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 4
	action: tensor([[ 2.0731, -0.8186, -0.2025, -0.0695,  1.3991,  3.5740,  0.2555]],
       dtype=torch.float64)
	q_value: tensor([[-1.3353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 5
	action: tensor([[ 1.4723,  1.2768, -0.3461,  1.9845, -0.9404,  0.9516,  1.0826]],
       dtype=torch.float64)
	q_value: tensor([[-1.3353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 6
	action: tensor([[-0.1052, -1.0005,  0.2015, -0.7567, -0.3164, -0.9658,  4.0278]],
       dtype=torch.float64)
	q_value: tensor([[-1.3353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.5173542232559 entropy 1.7646539211273193
epoch: 3, step: 7
	action: tensor([[-0.9797, -2.0497, -2.1271,  0.4206, -1.1003, -0.7146, -0.3683]],
       dtype=torch.float64)
	q_value: tensor([[-1.3353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 8
	action: tensor([[-0.8292, -0.4933,  1.1757, -0.2785, -0.0285, -1.9953,  1.1125]],
       dtype=torch.float64)
	q_value: tensor([[-1.3353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7461318181717262, distance: 1.5121511579047546 entropy 1.7646539211273193
epoch: 3, step: 9
	action: tensor([[-0.5048, -1.1044,  1.5737, -1.8480,  0.0814, -0.5159,  0.6756]],
       dtype=torch.float64)
	q_value: tensor([[-1.1139]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.526017333241479, distance: 1.4136321918551567 entropy 1.7646539211273193
epoch: 3, step: 10
	action: tensor([[ 0.5335, -2.4382, -0.5804,  1.3202,  1.5439,  1.3629,  1.0785]],
       dtype=torch.float64)
	q_value: tensor([[-1.2565]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 11
	action: tensor([[-1.0458,  0.7382,  0.1820, -2.5886, -0.7661,  2.1017, -0.5186]],
       dtype=torch.float64)
	q_value: tensor([[-1.3353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 12
	action: tensor([[-0.0110, -1.6157,  0.0220, -0.4616, -1.2790,  0.1874,  2.5275]],
       dtype=torch.float64)
	q_value: tensor([[-1.3353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0481676227258245, distance: 1.6377192037432993 entropy 1.7646539211273193
epoch: 3, step: 13
	action: tensor([[ 0.5102, -1.2244, -1.9772,  0.2254,  1.5576,  1.5452,  0.0569]],
       dtype=torch.float64)
	q_value: tensor([[-1.7266]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.014588111911602697, distance: 1.1526609436407298 entropy 1.7646539211273193
epoch: 3, step: 14
	action: tensor([[-3.0126, -0.9965,  0.6365, -2.1376,  0.6142,  0.6468, -2.7071]],
       dtype=torch.float64)
	q_value: tensor([[-1.3924]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 15
	action: tensor([[-0.6549, -0.0364,  4.1655, -0.3949, -1.1594, -0.4413, -0.4325]],
       dtype=torch.float64)
	q_value: tensor([[-1.3353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1510829788343646 entropy 1.7646539211273193
epoch: 3, step: 16
	action: tensor([[-0.0619,  1.0147,  0.2962,  1.3954,  2.6754, -0.1408, -2.1658]],
       dtype=torch.float64)
	q_value: tensor([[-1.3353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 17
	action: tensor([[-1.1990, -0.6681, -1.2361,  3.8491,  1.3266, -1.0528, -3.3625]],
       dtype=torch.float64)
	q_value: tensor([[-1.3353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 18
	action: tensor([[-0.1306, -0.9487, -1.7647,  0.0270, -4.2740,  0.8633,  0.1938]],
       dtype=torch.float64)
	q_value: tensor([[-1.3353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3024719030728398 entropy 1.7646539211273193
epoch: 3, step: 19
	action: tensor([[ 1.3839,  0.1372, -0.2707, -0.0760, -0.4019,  1.1368, -0.5502]],
       dtype=torch.float64)
	q_value: tensor([[-1.3353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 20
	action: tensor([[-1.3067, -0.6651,  1.7958, -1.1936, -1.6398, -2.6242, -1.3406]],
       dtype=torch.float64)
	q_value: tensor([[-1.3353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7081609025747078, distance: 1.4956193726167737 entropy 1.7646539211273193
epoch: 3, step: 21
	action: tensor([[ 2.6710, -1.5054, -0.8736, -1.9291,  0.1329,  1.8791,  2.2369]],
       dtype=torch.float64)
	q_value: tensor([[-1.5045]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 22
	action: tensor([[ 0.8534,  0.8799, -0.1922, -0.2841, -1.2112, -2.5036, -1.5524]],
       dtype=torch.float64)
	q_value: tensor([[-1.3353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 23
	action: tensor([[ 1.4706, -0.8915, -0.6088,  0.8013, -1.2049, -0.5283, -0.2941]],
       dtype=torch.float64)
	q_value: tensor([[-1.3353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29697799747385034, distance: 0.9594915446787374 entropy 1.7646539211273193
epoch: 3, step: 24
	action: tensor([[ 0.7077,  1.9002, -1.7050, -0.6252, -0.5518, -0.1966,  1.4456]],
       dtype=torch.float64)
	q_value: tensor([[-0.9303]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 25
	action: tensor([[-1.0015, -0.7272, -2.8295, -0.9788,  1.4577, -0.2005,  0.6280]],
       dtype=torch.float64)
	q_value: tensor([[-1.3353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5152719546483995, distance: 1.408646387545881 entropy 1.7646539211273193
epoch: 3, step: 26
	action: tensor([[ 1.3296, -1.4848,  0.4640, -1.0586,  0.1773, -0.9525,  3.4423]],
       dtype=torch.float64)
	q_value: tensor([[-1.5747]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 27
	action: tensor([[ 0.7061, -3.6690, -1.7894, -1.4257, -1.2219, -0.5086,  0.0973]],
       dtype=torch.float64)
	q_value: tensor([[-1.3353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 28
	action: tensor([[-1.1559, -2.9577,  1.0282, -0.1915,  0.8850, -0.9075,  2.6060]],
       dtype=torch.float64)
	q_value: tensor([[-1.3353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 29
	action: tensor([[ 0.4352,  1.1695,  0.1378, -2.8546, -0.4436,  0.8306, -1.5832]],
       dtype=torch.float64)
	q_value: tensor([[-1.3353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 30
	action: tensor([[-2.3861, -0.0999,  0.2594,  2.3289,  0.5816, -0.2718, -0.5556]],
       dtype=torch.float64)
	q_value: tensor([[-1.3353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 31
	action: tensor([[ 0.1097, -1.1606,  0.1574, -1.8690,  0.0551, -0.1756, -1.2078]],
       dtype=torch.float64)
	q_value: tensor([[-1.3353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5746903004529027, distance: 1.4359994364378903 entropy 1.7646539211273193
epoch: 3, step: 32
	action: tensor([[ 1.0482,  1.2873,  1.6246,  3.1174,  0.5042, -1.4857, -1.2480]],
       dtype=torch.float64)
	q_value: tensor([[-1.1948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34475636728302417, distance: 0.9263137229219804 entropy 1.7646539211273193
epoch: 3, step: 33
	action: tensor([[-1.4671, -1.1989,  0.5136, -1.0486,  0.3813, -0.4810,  1.7295]],
       dtype=torch.float64)
	q_value: tensor([[-1.6196]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 34
	action: tensor([[-0.3528,  0.3314,  0.3503, -0.1037, -2.1721, -1.1626, -1.9006]],
       dtype=torch.float64)
	q_value: tensor([[-1.3353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 35
	action: tensor([[ 1.3741, -2.6944, -1.0006, -1.9116, -2.8761, -1.1732, -0.8681]],
       dtype=torch.float64)
	q_value: tensor([[-1.3353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 36
	action: tensor([[ 0.7858,  1.7750,  0.5978,  1.5609,  1.1473, -1.1416,  1.4604]],
       dtype=torch.float64)
	q_value: tensor([[-1.3353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 37
	action: tensor([[ 0.0132, -2.4709, -0.5447,  1.6821, -0.5120, -3.2315,  0.2783]],
       dtype=torch.float64)
	q_value: tensor([[-1.3353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 38
	action: tensor([[-2.5147, -0.5444,  0.1316, -1.1529,  0.9707,  1.3293, -0.6285]],
       dtype=torch.float64)
	q_value: tensor([[-1.3353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 39
	action: tensor([[ 0.8308, -1.1261, -0.5538,  1.9806, -1.2493,  0.4641, -0.4428]],
       dtype=torch.float64)
	q_value: tensor([[-1.3353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6796224988983921, distance: 0.6477205829665178 entropy 1.7646539211273193
epoch: 3, step: 40
	action: tensor([[ 0.3262,  1.5839,  1.2477,  1.4561,  1.0702, -0.9530, -1.0512]],
       dtype=torch.float64)
	q_value: tensor([[-1.0470]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 41
	action: tensor([[-0.9189, -0.3795,  0.6993,  1.0590,  2.6707,  2.7234,  0.5660]],
       dtype=torch.float64)
	q_value: tensor([[-1.3353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 42
	action: tensor([[ 2.2758, -1.7714,  0.2721, -0.6463,  1.3050,  0.0394,  0.6866]],
       dtype=torch.float64)
	q_value: tensor([[-1.3353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 43
	action: tensor([[-1.8703, -2.1859,  0.0222, -0.0066,  0.0756, -1.2992, -1.2457]],
       dtype=torch.float64)
	q_value: tensor([[-1.3353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 44
	action: tensor([[-1.3081, -2.3314, -1.5458,  1.6914, -0.4052,  0.6471, -1.9128]],
       dtype=torch.float64)
	q_value: tensor([[-1.3353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 45
	action: tensor([[-2.4709, -1.2773,  0.2900,  1.0211, -0.3402,  0.7000,  0.7075]],
       dtype=torch.float64)
	q_value: tensor([[-1.3353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 46
	action: tensor([[-2.0640, -2.1502, -2.4646, -0.4126,  1.8748, -0.8119,  1.5900]],
       dtype=torch.float64)
	q_value: tensor([[-1.3353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 47
	action: tensor([[ 1.0284, -2.4882, -0.1873, -0.7874,  1.1743, -0.6280, -0.0293]],
       dtype=torch.float64)
	q_value: tensor([[-1.3353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 48
	action: tensor([[ 1.5205,  0.8594,  1.0988, -0.2082,  1.1979, -0.6840,  0.1238]],
       dtype=torch.float64)
	q_value: tensor([[-1.3353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 49
	action: tensor([[-1.2004,  2.3089, -1.6295,  0.0040,  0.0211,  1.2454, -0.6930]],
       dtype=torch.float64)
	q_value: tensor([[-1.3353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 50
	action: tensor([[-0.6734, -0.3850, -0.5457, -0.3179, -0.7827, -1.1916,  0.7222]],
       dtype=torch.float64)
	q_value: tensor([[-1.3353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11227713911328818, distance: 1.2068775225125208 entropy 1.7646539211273193
epoch: 3, step: 51
	action: tensor([[ 1.6822,  0.2017, -0.6757, -0.4737, -1.0850,  0.9540, -0.9732]],
       dtype=torch.float64)
	q_value: tensor([[-0.9974]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4936910714548427, distance: 0.8142625975292085 entropy 1.7646539211273193
epoch: 3, step: 52
	action: tensor([[-0.9309,  0.1188, -1.4013, -0.6507, -0.0395,  0.4148, -3.6519]],
       dtype=torch.float64)
	q_value: tensor([[-1.0217]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3603141653304793 entropy 1.7646539211273193
epoch: 3, step: 53
	action: tensor([[-0.4396,  1.9328,  0.5093,  0.2010,  0.4508,  1.7065,  0.5659]],
       dtype=torch.float64)
	q_value: tensor([[-1.3353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 54
	action: tensor([[ 2.3542,  1.4353,  0.9647,  0.4916, -0.0044,  1.1271,  0.0026]],
       dtype=torch.float64)
	q_value: tensor([[-1.3353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 55
	action: tensor([[ 0.3757, -2.7577,  1.0769, -0.9218,  0.4452,  0.6892, -1.1060]],
       dtype=torch.float64)
	q_value: tensor([[-1.3353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 56
	action: tensor([[-0.1984,  0.0376, -0.9219, -2.8640, -0.1918, -2.0685, -1.1194]],
       dtype=torch.float64)
	q_value: tensor([[-1.3353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 57
	action: tensor([[ 0.5174, -3.0250, -1.0874, -0.4555, -0.5502, -1.4994,  2.3464]],
       dtype=torch.float64)
	q_value: tensor([[-1.3353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 58
	action: tensor([[ 0.5565, -0.8052,  0.3388, -0.2726,  0.2446, -1.8228, -0.7772]],
       dtype=torch.float64)
	q_value: tensor([[-1.3353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.254759980638561, distance: 1.2818494538663152 entropy 1.7646539211273193
epoch: 3, step: 59
	action: tensor([[-0.7992, -2.0733, -0.8810, -1.3144,  0.9176, -0.7581,  1.4113]],
       dtype=torch.float64)
	q_value: tensor([[-0.9248]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 60
	action: tensor([[-2.5680, -2.9581,  2.0994,  2.6718,  1.8117, -0.4921,  0.6239]],
       dtype=torch.float64)
	q_value: tensor([[-1.3353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 61
	action: tensor([[ 0.0810, -0.8383,  1.0927, -1.8456, -1.0691,  1.0327,  0.6910]],
       dtype=torch.float64)
	q_value: tensor([[-1.3353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.47920943889624157, distance: 1.391783001585608 entropy 1.7646539211273193
epoch: 3, step: 62
	action: tensor([[-1.3948, -1.1915, -0.3819,  0.4167, -0.8727, -0.7610, -2.1034]],
       dtype=torch.float64)
	q_value: tensor([[-1.5265]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.3336828122373015, distance: 1.748145622694425 entropy 1.7646539211273193
epoch: 3, step: 63
	action: tensor([[ 0.5177, -3.5715,  0.4117, -0.7883, -0.0539, -0.7036,  1.9541]],
       dtype=torch.float64)
	q_value: tensor([[-1.1760]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 64
	action: tensor([[ 1.0901, -0.2030, -0.7795, -1.0650, -1.8212,  0.4934, -3.4308]],
       dtype=torch.float64)
	q_value: tensor([[-1.3353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1929281300143877 entropy 1.7646539211273193
epoch: 3, step: 65
	action: tensor([[-1.4957, -2.7222,  0.6829,  0.8632, -3.2745,  0.3109, -1.0852]],
       dtype=torch.float64)
	q_value: tensor([[-1.3353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 66
	action: tensor([[-0.5550,  0.6086, -1.3094,  1.2650, -0.2462,  2.3443,  1.7704]],
       dtype=torch.float64)
	q_value: tensor([[-1.3353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.022517830314455534, distance: 1.1313868204905726 entropy 1.7646539211273193
epoch: 3, step: 67
	action: tensor([[ 1.5036,  0.7609, -0.9324,  1.0635,  0.2780, -1.4233, -1.4181]],
       dtype=torch.float64)
	q_value: tensor([[-1.7303]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 68
	action: tensor([[ 0.6884, -0.8344,  0.0234,  1.7391, -1.7924, -0.7966, -2.0369]],
       dtype=torch.float64)
	q_value: tensor([[-1.3353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9048751876164667, distance: 0.35294220927965286 entropy 1.7646539211273193
epoch: 3, step: 69
	action: tensor([[-0.2567, -1.1188, -2.3582, -1.9190,  0.4273,  1.5026,  1.5816]],
       dtype=torch.float64)
	q_value: tensor([[-1.3055]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24461367382288957, distance: 0.9945835062567487 entropy 1.7646539211273193
epoch: 3, step: 70
	action: tensor([[-0.7747, -0.1636, -1.3793, -0.8830,  3.0529,  0.9866, -2.7623]],
       dtype=torch.float64)
	q_value: tensor([[-2.1637]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3662816124809166, distance: 0.9109716203022289 entropy 1.7646539211273193
epoch: 3, step: 71
	action: tensor([[ 0.5817, -1.0446,  2.0680, -1.0205,  0.5418, -1.3657, -2.5644]],
       dtype=torch.float64)
	q_value: tensor([[-1.7431]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.46458572479852933, distance: 0.8373396688703713 entropy 1.7646539211273193
epoch: 3, step: 72
	action: tensor([[-0.3712,  1.1592, -2.8182, -0.1903,  0.8098,  0.5135,  0.9215]],
       dtype=torch.float64)
	q_value: tensor([[-1.4569]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 73
	action: tensor([[-0.3315, -1.4721,  1.3231, -0.4195, -0.3780, -1.7710,  1.9585]],
       dtype=torch.float64)
	q_value: tensor([[-1.3353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.708251408931569, distance: 1.495658994548018 entropy 1.7646539211273193
epoch: 3, step: 74
	action: tensor([[-2.2685,  1.1186,  0.4568, -0.5573, -1.0403,  0.4958, -0.7145]],
       dtype=torch.float64)
	q_value: tensor([[-1.3164]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 75
	action: tensor([[ 0.2385,  0.7450, -0.1639, -0.4696, -0.9022,  0.0683,  0.6980]],
       dtype=torch.float64)
	q_value: tensor([[-1.3353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 76
	action: tensor([[-0.4487, -2.0196,  1.0485, -1.4763, -0.8902, -0.4008,  2.7564]],
       dtype=torch.float64)
	q_value: tensor([[-1.3353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 77
	action: tensor([[-2.5512, -0.7206, -0.6395,  1.1167, -0.5883,  0.7225,  1.4871]],
       dtype=torch.float64)
	q_value: tensor([[-1.3353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 78
	action: tensor([[-0.5420, -1.8684,  0.4289, -1.5150,  0.3415, -2.7450, -1.3739]],
       dtype=torch.float64)
	q_value: tensor([[-1.3353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 79
	action: tensor([[-0.9682,  0.0954,  0.1974,  0.1276, -1.2576, -0.6911,  0.7902]],
       dtype=torch.float64)
	q_value: tensor([[-1.3353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5259498493629218, distance: 1.41360093453069 entropy 1.7646539211273193
epoch: 3, step: 80
	action: tensor([[-0.7581, -2.2330, -1.2557,  1.0243, -1.0215,  0.2411, -2.0220]],
       dtype=torch.float64)
	q_value: tensor([[-1.1031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 81
	action: tensor([[ 1.9005, -2.1086, -1.0646,  0.9322,  0.6329,  1.1054,  1.4087]],
       dtype=torch.float64)
	q_value: tensor([[-1.3353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 82
	action: tensor([[-0.2019,  0.0390,  0.4153, -0.0660,  3.2104, -4.0661, -2.5565]],
       dtype=torch.float64)
	q_value: tensor([[-1.3353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 83
	action: tensor([[-0.2854, -2.0579, -0.6682,  2.3770, -1.1594, -1.2147,  0.8173]],
       dtype=torch.float64)
	q_value: tensor([[-1.3353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 84
	action: tensor([[-0.5426, -1.7843, -1.2900, -1.2744,  1.3943,  1.1437,  0.3521]],
       dtype=torch.float64)
	q_value: tensor([[-1.3353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 85
	action: tensor([[ 0.5349, -2.2325,  0.2792,  1.3606,  0.3451,  2.8937, -2.2922]],
       dtype=torch.float64)
	q_value: tensor([[-1.3353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 86
	action: tensor([[ 1.5731, -0.4463, -0.6667, -1.2182, -1.0995, -0.0549, -0.2071]],
       dtype=torch.float64)
	q_value: tensor([[-1.3353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 87
	action: tensor([[ 0.0710, -0.5511, -1.4115,  1.3053,  0.9983, -1.3941, -1.4641]],
       dtype=torch.float64)
	q_value: tensor([[-1.3353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.41886590941697155, distance: 1.363098909537934 entropy 1.7646539211273193
epoch: 3, step: 88
	action: tensor([[-0.9790,  0.2464, -2.2880, -1.9193,  1.0145, -0.3782, -1.1514]],
       dtype=torch.float64)
	q_value: tensor([[-1.0774]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 89
	action: tensor([[ 2.1031, -3.2013, -0.7442,  0.4031, -1.2343,  1.1884, -0.7490]],
       dtype=torch.float64)
	q_value: tensor([[-1.3353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 90
	action: tensor([[ 1.4190,  0.0452,  0.1651, -0.0035,  0.7738,  0.9600, -1.2998]],
       dtype=torch.float64)
	q_value: tensor([[-1.3353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5439957223182352, distance: 0.7727538040050032 entropy 1.7646539211273193
epoch: 3, step: 91
	action: tensor([[ 1.0232,  2.5638, -1.6202, -1.6860,  0.1071, -0.0752, -0.3847]],
       dtype=torch.float64)
	q_value: tensor([[-0.9419]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 92
	action: tensor([[-1.2155, -0.7143,  0.2190,  2.0802, -0.1615,  1.6880,  0.3263]],
       dtype=torch.float64)
	q_value: tensor([[-1.3353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28483150962268977, distance: 0.9677448728323482 entropy 1.7646539211273193
epoch: 3, step: 93
	action: tensor([[-0.9410, -1.9505, -0.4968,  1.2057,  0.1374,  0.4696,  0.3303]],
       dtype=torch.float64)
	q_value: tensor([[-1.2690]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 94
	action: tensor([[-1.1579e+00, -2.0030e-01,  6.1160e-01, -1.7817e+00,  5.4294e-01,
          1.0742e-03, -8.0185e-02]], dtype=torch.float64)
	q_value: tensor([[-1.3353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9458407888198312, distance: 1.596284676625913 entropy 1.7646539211273193
epoch: 3, step: 95
	action: tensor([[-0.8498,  0.4327, -1.1797,  0.6881,  1.8742,  0.7950, -1.7951]],
       dtype=torch.float64)
	q_value: tensor([[-1.1604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3181364851024244, distance: 1.3138230708652114 entropy 1.7646539211273193
epoch: 3, step: 96
	action: tensor([[-1.5374, -0.9515,  4.0481, -2.4674, -1.9367,  0.7025,  1.3874]],
       dtype=torch.float64)
	q_value: tensor([[-1.1374]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 97
	action: tensor([[ 1.2528, -0.7323, -0.4281, -1.8198,  0.0169,  0.6305, -0.1791]],
       dtype=torch.float64)
	q_value: tensor([[-1.3353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6317397846728905, distance: 1.4617804339903842 entropy 1.7646539211273193
epoch: 3, step: 98
	action: tensor([[-0.9160, -1.2162,  1.2283,  1.2009, -2.0646,  1.6475, -0.7654]],
       dtype=torch.float64)
	q_value: tensor([[-1.3273]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.49211413711902363, distance: 1.3978408108906795 entropy 1.7646539211273193
epoch: 3, step: 99
	action: tensor([[-1.8605,  0.0776, -0.5671, -0.5392,  0.9225, -0.5870,  0.7019]],
       dtype=torch.float64)
	q_value: tensor([[-1.6103]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 100
	action: tensor([[-1.4333,  0.2436, -0.3216, -0.6502, -0.6593, -1.9975,  0.2006]],
       dtype=torch.float64)
	q_value: tensor([[-1.3353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23189369814682825, distance: 1.2701157749925356 entropy 1.7646539211273193
epoch: 3, step: 101
	action: tensor([[-4.5469, -1.3587,  0.8553, -0.5175, -0.0632, -0.7866, -0.8859]],
       dtype=torch.float64)
	q_value: tensor([[-1.1730]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 102
	action: tensor([[ 1.4179,  0.3096,  0.2442, -1.8306, -1.0415, -0.1452,  1.5518]],
       dtype=torch.float64)
	q_value: tensor([[-1.3353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11152921878441358, distance: 1.0786443382457647 entropy 1.7646539211273193
epoch: 3, step: 103
	action: tensor([[ 0.3057,  0.0550, -2.7103, -0.3478, -0.0856,  0.9380, -1.1268]],
       dtype=torch.float64)
	q_value: tensor([[-1.5919]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5963370301740982, distance: 0.7270531306080928 entropy 1.7646539211273193
epoch: 3, step: 104
	action: tensor([[ 0.5683, -0.9050, -0.2381,  0.0264,  1.5093, -4.7652,  3.6578]],
       dtype=torch.float64)
	q_value: tensor([[-1.2370]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 105
	action: tensor([[ 1.9984,  0.8796,  1.9151,  0.5688,  1.3928, -0.6326, -0.7503]],
       dtype=torch.float64)
	q_value: tensor([[-1.3353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 106
	action: tensor([[ 0.5305, -2.4118, -0.4713, -1.1365, -0.7048,  0.8772, -1.0398]],
       dtype=torch.float64)
	q_value: tensor([[-1.3353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 107
	action: tensor([[-1.8782,  1.0706, -1.1136, -0.8062, -0.1108,  1.4480,  0.3034]],
       dtype=torch.float64)
	q_value: tensor([[-1.3353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 108
	action: tensor([[ 1.9120,  0.8698, -1.1599,  0.4130,  0.3840, -1.7336,  1.7818]],
       dtype=torch.float64)
	q_value: tensor([[-1.3353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 109
	action: tensor([[ 0.3652, -2.3490, -0.1005,  2.3870,  0.1544,  1.6720,  0.3040]],
       dtype=torch.float64)
	q_value: tensor([[-1.3353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 110
	action: tensor([[ 2.0555, -0.2572, -0.4567,  2.1044, -0.7893, -0.9699,  1.4168]],
       dtype=torch.float64)
	q_value: tensor([[-1.3353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 111
	action: tensor([[-0.2582, -1.2964, -1.1449,  0.7269,  1.9778, -1.5516,  2.6869]],
       dtype=torch.float64)
	q_value: tensor([[-1.3353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8043898076283498, distance: 1.5371699146273388 entropy 1.7646539211273193
epoch: 3, step: 112
	action: tensor([[ 0.0139,  0.2158,  0.3126,  2.3979, -0.0970,  0.2720,  1.3222]],
       dtype=torch.float64)
	q_value: tensor([[-1.5584]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 113
	action: tensor([[ 2.5216,  0.3537,  1.1769,  0.6763, -0.8706, -0.0931, -2.1032]],
       dtype=torch.float64)
	q_value: tensor([[-1.3353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 114
	action: tensor([[ 2.2703, -1.7247, -0.7399, -2.1220,  1.8555,  1.8045,  0.1205]],
       dtype=torch.float64)
	q_value: tensor([[-1.3353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 115
	action: tensor([[ 3.0664,  0.3181, -1.0759, -0.4525,  0.0290,  0.7750,  1.7057]],
       dtype=torch.float64)
	q_value: tensor([[-1.3353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 116
	action: tensor([[ 1.3280,  0.6001,  0.6465,  2.3541, -2.2867,  0.7800, -1.8587]],
       dtype=torch.float64)
	q_value: tensor([[-1.3353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 117
	action: tensor([[ 1.1650, -0.6818, -1.1752, -0.0975, -1.9160, -0.2814, -2.2713]],
       dtype=torch.float64)
	q_value: tensor([[-1.3353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 118
	action: tensor([[-1.3571, -2.4284,  1.1439,  0.5136, -0.0146, -0.5009,  1.7685]],
       dtype=torch.float64)
	q_value: tensor([[-1.3353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 119
	action: tensor([[ 0.1176, -1.1809, -0.0497,  0.6256,  0.0312,  0.3535,  2.7149]],
       dtype=torch.float64)
	q_value: tensor([[-1.3353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15907567068021855, distance: 1.2320053343682276 entropy 1.7646539211273193
epoch: 3, step: 120
	action: tensor([[-2.8297,  1.4476, -2.0179,  0.0970, -2.8905,  1.5238,  1.0098]],
       dtype=torch.float64)
	q_value: tensor([[-1.5087]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 121
	action: tensor([[-2.1172, -1.5716, -2.3882,  0.5239, -2.5040,  1.1898,  0.8335]],
       dtype=torch.float64)
	q_value: tensor([[-1.3353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 122
	action: tensor([[ 0.2810,  0.9372,  1.8974,  0.5982,  0.3482, -1.3219, -0.8394]],
       dtype=torch.float64)
	q_value: tensor([[-1.3353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 123
	action: tensor([[ 0.0123,  1.0910,  2.3473,  0.5906,  0.6638, -0.1377,  0.7540]],
       dtype=torch.float64)
	q_value: tensor([[-1.3353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 124
	action: tensor([[-1.6198, -0.4579, -1.2741, -1.1795, -0.8900, -3.5387,  1.2111]],
       dtype=torch.float64)
	q_value: tensor([[-1.3353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 125
	action: tensor([[ 0.2953, -0.1761,  1.0393, -0.9416, -1.6390, -1.6453,  2.3779]],
       dtype=torch.float64)
	q_value: tensor([[-1.3353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10384196543525648, distance: 1.0833006274479673 entropy 1.7646539211273193
epoch: 3, step: 126
	action: tensor([[ 1.9637,  0.7107, -0.0222, -1.7476,  2.5288, -1.2069,  0.0873]],
       dtype=torch.float64)
	q_value: tensor([[-1.6858]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 127
	action: tensor([[-2.6640, -1.4837,  2.9057, -2.4438, -1.8325, -0.5910,  2.6410]],
       dtype=torch.float64)
	q_value: tensor([[-1.3353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
LOSS epoch 3 actor 675.3582992417812 critic 2269.6794307838354 
epoch: 4, step: 0
	action: tensor([[-0.1567, -0.7846, -1.2965, -1.7491,  0.0957,  0.1857,  0.4659]],
       dtype=torch.float64)
	q_value: tensor([[-2.4993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.46586229846711213, distance: 0.8363408500717008 entropy 1.7646539211273193
epoch: 4, step: 1
	action: tensor([[ 1.7365, -0.9768, -0.0699, -0.1983,  0.2288,  0.9384, -2.5302]],
       dtype=torch.float64)
	q_value: tensor([[-2.5211]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3343010370098194, distance: 1.3218543514770098 entropy 1.7646539211273193
epoch: 4, step: 2
	action: tensor([[ 0.4788, -3.7789,  1.2533, -1.0707,  0.4138, -0.2566, -2.2805]],
       dtype=torch.float64)
	q_value: tensor([[-2.2101]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 4, step: 3
	action: tensor([[-0.1665, -4.4927,  2.4700, -0.4401,  0.8836, -0.2029, -1.5364]],
       dtype=torch.float64)
	q_value: tensor([[-2.4993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 4, step: 4
	action: tensor([[ 3.5853, -0.7081,  1.2523,  0.7346,  0.0360,  2.5541, -0.4174]],
       dtype=torch.float64)
	q_value: tensor([[-2.4993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 4, step: 5
	action: tensor([[-1.6595, -2.7925, -1.6998,  0.2604, -2.6322, -0.3612, -1.2606]],
       dtype=torch.float64)
	q_value: tensor([[-2.4993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 4, step: 6
	action: tensor([[ 0.0289,  0.1244, -1.8097, -1.3111,  1.2934,  2.1003, -0.3997]],
       dtype=torch.float64)
	q_value: tensor([[-2.4993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 4, step: 7
	action: tensor([[-0.1867, -0.6936, -1.7978,  1.6180,  0.7412,  0.6663,  1.4144]],
       dtype=torch.float64)
	q_value: tensor([[-2.4993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5721147554074737, distance: 1.4348246039108616 entropy 1.7646539211273193
epoch: 4, step: 8
	action: tensor([[ 0.0318, -3.0575,  0.7505, -0.7899,  1.4806,  0.4706,  0.0475]],
       dtype=torch.float64)
	q_value: tensor([[-2.2936]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 4, step: 9
	action: tensor([[ 0.0870, -0.3999, -0.2521,  0.1650,  1.3747, -1.6988, -1.3406]],
       dtype=torch.float64)
	q_value: tensor([[-2.4993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08524453082239947, distance: 1.1921214330675345 entropy 1.7646539211273193
epoch: 4, step: 10
	action: tensor([[ 1.3847, -1.2113, -1.1123, -0.6659,  1.5805,  0.3995,  1.8800]],
       dtype=torch.float64)
	q_value: tensor([[-1.9965]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8942306386690908, distance: 1.5749730316260064 entropy 1.7646539211273193
epoch: 4, step: 11
	action: tensor([[ 0.9850, -2.1574,  0.4395,  3.4157,  0.5414, -0.0141,  2.4791]],
       dtype=torch.float64)
	q_value: tensor([[-2.9522]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 4, step: 12
	action: tensor([[-0.0125,  0.8560, -0.3554, -2.1046, -1.4247, -1.0733,  0.5338]],
       dtype=torch.float64)
	q_value: tensor([[-2.4993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 4, step: 13
	action: tensor([[ 1.3742, -1.9400, -1.8313,  0.5238,  1.3962, -0.6369,  0.2553]],
       dtype=torch.float64)
	q_value: tensor([[-2.4993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 4, step: 14
	action: tensor([[-0.1314, -1.8311,  0.0616, -0.2787,  0.2555,  1.9855,  1.1761]],
       dtype=torch.float64)
	q_value: tensor([[-2.4993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 4, step: 15
	action: tensor([[-3.4634e+00, -1.0579e+00, -1.2655e+00,  1.3060e+00, -2.4261e-03,
         -1.2067e+00,  1.1448e+00]], dtype=torch.float64)
	q_value: tensor([[-2.4993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 4, step: 16
	action: tensor([[-0.7165, -1.7278, -0.4291,  2.7732,  0.8810,  1.6277, -0.7386]],
       dtype=torch.float64)
	q_value: tensor([[-2.4993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4716140333667749, distance: 0.8318256932310062 entropy 1.7646539211273193
epoch: 4, step: 17
	action: tensor([[-2.7352, -4.5237, -0.5974, -0.3744, -1.8571, -2.1607, -1.1897]],
       dtype=torch.float64)
	q_value: tensor([[-2.1084]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 4, step: 18
	action: tensor([[-0.3422, -1.3566, -0.1162, -1.5134, -1.2819,  2.0784,  0.8246]],
       dtype=torch.float64)
	q_value: tensor([[-2.4993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5263457213196914, distance: 1.4137842854707234 entropy 1.7646539211273193
epoch: 4, step: 19
	action: tensor([[-3.6450, -1.2222, -0.9794,  0.1583,  0.6260,  0.3325, -0.1720]],
       dtype=torch.float64)
	q_value: tensor([[-3.3930]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 4, step: 20
	action: tensor([[-1.0765, -0.1340, -0.4315,  0.3240, -0.5973, -0.5612, -3.6750]],
       dtype=torch.float64)
	q_value: tensor([[-2.4993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.602063250912712 entropy 1.7646539211273193
epoch: 4, step: 21
	action: tensor([[-1.3385, -2.5726, -0.9163, -1.5259,  1.2671,  0.2650,  1.8839]],
       dtype=torch.float64)
	q_value: tensor([[-2.4993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 4, step: 22
	action: tensor([[ 0.1793, -1.0022,  2.0929, -0.8053,  0.4032,  2.4714,  0.6861]],
       dtype=torch.float64)
	q_value: tensor([[-2.4993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11024126149413416, distance: 1.205772501016457 entropy 1.7646539211273193
epoch: 4, step: 23
	action: tensor([[-0.9725, -0.0315,  0.3196, -1.4761, -2.3422, -0.4552,  0.3429]],
       dtype=torch.float64)
	q_value: tensor([[-3.0453]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9810408990426127, distance: 1.6106582970737708 entropy 1.7646539211273193
epoch: 4, step: 24
	action: tensor([[ 0.4698, -1.5451, -0.4808,  1.4438,  3.3069,  1.7045,  1.7000]],
       dtype=torch.float64)
	q_value: tensor([[-2.9165]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 4, step: 25
	action: tensor([[-1.1385, -1.2405, -1.4256, -2.4396,  0.4415, -0.0863,  0.1689]],
       dtype=torch.float64)
	q_value: tensor([[-2.4993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.684106306250223, distance: 0.643172061958312 entropy 1.7646539211273193
epoch: 4, step: 26
	action: tensor([[ 0.0783, -0.6700,  0.6760, -0.6256,  0.2933, -0.1975,  0.3479]],
       dtype=torch.float64)
	q_value: tensor([[-2.9874]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7276454352835966, distance: 1.5041252496080981 entropy 1.7646539211273193
epoch: 4, step: 27
	action: tensor([[ 0.9453, -2.0668,  0.8328,  1.1750,  0.0369, -0.2461, -0.4703]],
       dtype=torch.float64)
	q_value: tensor([[-1.3730]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 4, step: 28
	action: tensor([[ 0.5042,  0.3206, -0.2486,  0.0414, -0.9423,  2.1705, -1.7816]],
       dtype=torch.float64)
	q_value: tensor([[-2.4993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 4, step: 29
	action: tensor([[-0.8914, -0.4630, -3.0560, -0.7329,  1.2927,  0.2626, -0.5267]],
       dtype=torch.float64)
	q_value: tensor([[-2.4993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.768142497177033, distance: 1.5216519417201093 entropy 1.7646539211273193
epoch: 4, step: 30
	action: tensor([[-2.5775, -1.5635, -1.8477,  0.1673, -0.2008,  0.2092,  0.9066]],
       dtype=torch.float64)
	q_value: tensor([[-2.7693]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 4, step: 31
	action: tensor([[ 0.5247, -0.4317,  2.3804,  0.1097, -0.6227, -1.8971,  1.1357]],
       dtype=torch.float64)
	q_value: tensor([[-2.4993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 4, step: 32
	action: tensor([[-1.2399, -0.7727, -0.0932,  1.1269,  1.6430,  0.0042, -0.6187]],
       dtype=torch.float64)
	q_value: tensor([[-2.4993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1039061765507143, distance: 1.659853954883196 entropy 1.7646539211273193
epoch: 4, step: 33
	action: tensor([[-0.4415,  1.0838, -0.6728, -2.9742,  1.0498,  0.0567,  1.4209]],
       dtype=torch.float64)
	q_value: tensor([[-1.8517]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44042920103987016, distance: 0.856020597396257 entropy 1.7646539211273193
epoch: 4, step: 34
	action: tensor([[ 1.3510, -2.0578, -0.7199, -1.7038,  1.0402,  1.9565, -0.4145]],
       dtype=torch.float64)
	q_value: tensor([[-3.1886]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 4, step: 35
	action: tensor([[ 0.5157, -0.5677, -0.1356,  1.8094, -2.0390, -0.5669,  0.8902]],
       dtype=torch.float64)
	q_value: tensor([[-2.4993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 4, step: 36
	action: tensor([[-1.9745, -6.0606, -0.3311,  1.6228, -0.4599, -1.2709, -0.1261]],
       dtype=torch.float64)
	q_value: tensor([[-2.4993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 4, step: 37
	action: tensor([[-0.9640, -2.0161, -2.6129, -0.4319, -1.6610, -2.0986, -2.2055]],
       dtype=torch.float64)
	q_value: tensor([[-2.4993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 4, step: 38
	action: tensor([[ 1.2425, -1.5086,  0.4299, -2.3077, -0.3258,  0.3755,  1.8692]],
       dtype=torch.float64)
	q_value: tensor([[-2.4993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.231068429147109, distance: 1.2696902663688263 entropy 1.7646539211273193
epoch: 4, step: 39
	action: tensor([[-1.6307,  0.1240, -0.5257,  1.0498,  2.3405,  0.6686, -0.2435]],
       dtype=torch.float64)
	q_value: tensor([[-3.2423]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.871554523507712, distance: 1.5655175305618345 entropy 1.7646539211273193
epoch: 4, step: 40
	action: tensor([[ 1.2740, -1.0531, -1.8668,  0.5857, -0.9281, -1.4323,  0.7927]],
       dtype=torch.float64)
	q_value: tensor([[-2.3176]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16075158116693133, distance: 1.0483395226701773 entropy 1.7646539211273193
epoch: 4, step: 41
	action: tensor([[-1.1307,  1.0031, -0.5543, -0.1820, -0.8726,  3.3790, -0.6119]],
       dtype=torch.float64)
	q_value: tensor([[-2.4065]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 4, step: 42
	action: tensor([[ 0.5448, -2.5445,  0.7217,  0.9267,  1.5592,  0.4225, -0.1788]],
       dtype=torch.float64)
	q_value: tensor([[-2.4993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 4, step: 43
	action: tensor([[-0.1442, -1.0012, -0.2043,  2.6061, -0.1957, -1.0202,  1.2225]],
       dtype=torch.float64)
	q_value: tensor([[-2.4993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37646019610032666, distance: 0.9036261371743256 entropy 1.7646539211273193
epoch: 4, step: 44
	action: tensor([[-1.2623, -0.7794,  2.6358, -1.3063, -1.3710,  1.8920, -3.3865]],
       dtype=torch.float64)
	q_value: tensor([[-2.1930]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3553563576619854 entropy 1.7646539211273193
epoch: 4, step: 45
	action: tensor([[-1.1233, -0.2109, -0.7291, -3.5938, -1.4860, -1.2375, -2.4483]],
       dtype=torch.float64)
	q_value: tensor([[-2.4993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 4, step: 46
	action: tensor([[-0.0778, -2.3003, -0.5042, -2.4987,  0.9360,  1.4025,  2.2291]],
       dtype=torch.float64)
	q_value: tensor([[-2.4993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 4, step: 47
	action: tensor([[ 0.7419, -1.7788, -1.7386, -0.9848, -0.9401,  0.0750, -1.0052]],
       dtype=torch.float64)
	q_value: tensor([[-2.4993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 4, step: 48
	action: tensor([[-0.8167,  0.3682, -0.3709,  0.4717, -1.3564,  0.3750, -0.2004]],
       dtype=torch.float64)
	q_value: tensor([[-2.4993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.43117913389577645, distance: 1.369000766013051 entropy 1.7646539211273193
epoch: 4, step: 49
	action: tensor([[-0.5195,  0.0751, -0.3802,  0.8713, -1.7639,  0.7895,  0.1849]],
       dtype=torch.float64)
	q_value: tensor([[-1.9309]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.31838990926547583, distance: 1.3139493622295817 entropy 1.7646539211273193
epoch: 4, step: 50
	action: tensor([[-0.9258, -0.4255, -0.1388,  0.9899,  1.2886,  1.5647, -0.3266]],
       dtype=torch.float64)
	q_value: tensor([[-2.2456]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14303345198346373, distance: 1.0593479438538966 entropy 1.7646539211273193
epoch: 4, step: 51
	action: tensor([[-2.2334, -2.3033,  3.3193, -1.1504, -0.9321,  0.2471,  1.7908]],
       dtype=torch.float64)
	q_value: tensor([[-2.0099]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 4, step: 52
	action: tensor([[ 0.8975,  1.2880, -1.8295,  0.3143,  1.0027, -2.4597,  0.0537]],
       dtype=torch.float64)
	q_value: tensor([[-2.4993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 4, step: 53
	action: tensor([[ 1.2336, -3.4224, -1.6577, -1.7211,  1.7667,  1.5999,  1.6694]],
       dtype=torch.float64)
	q_value: tensor([[-2.4993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 4, step: 54
	action: tensor([[ 0.1112, -0.4615, -1.0277,  0.4711, -1.3953, -0.9896, -0.1251]],
       dtype=torch.float64)
	q_value: tensor([[-2.4993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.388677570016021, distance: 0.8947297101441855 entropy 1.7646539211273193
epoch: 4, step: 55
	action: tensor([[-1.9330, -2.3906,  2.9907, -3.3939, -1.1604,  1.6485, -0.5674]],
       dtype=torch.float64)
	q_value: tensor([[-1.7469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 4, step: 56
	action: tensor([[-0.7545, -1.0513, -0.8649, -3.3367,  1.1067,  0.4139,  0.9980]],
       dtype=torch.float64)
	q_value: tensor([[-2.4993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 4, step: 57
	action: tensor([[-0.0588, -2.5197,  1.6318,  1.3168, -1.4725,  0.2842,  1.0315]],
       dtype=torch.float64)
	q_value: tensor([[-2.4993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 4, step: 58
	action: tensor([[-0.6461, -2.5997, -3.1437,  1.7319,  0.7456, -0.0066,  3.1530]],
       dtype=torch.float64)
	q_value: tensor([[-2.4993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 4, step: 59
	action: tensor([[-0.1244, -5.0294,  0.2657, -2.7737,  0.3673, -1.1002, -0.9213]],
       dtype=torch.float64)
	q_value: tensor([[-2.4993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 4, step: 60
	action: tensor([[-1.1975,  1.1258, -2.7695,  3.7775,  1.4562,  0.2082,  3.1101]],
       dtype=torch.float64)
	q_value: tensor([[-2.4993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 4, step: 61
	action: tensor([[ 1.5279, -1.5426, -0.7708,  1.3188, -2.2242, -1.7201, -1.6097]],
       dtype=torch.float64)
	q_value: tensor([[-2.4993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5722939797560505, distance: 0.7483924109668334 entropy 1.7646539211273193
epoch: 4, step: 62
	action: tensor([[ 0.1622, -0.2825,  0.1004,  0.9415,  1.6394,  0.7599,  0.3176]],
       dtype=torch.float64)
	q_value: tensor([[-2.6596]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8561689698820965, distance: 0.4339932638211625 entropy 1.7646539211273193
epoch: 4, step: 63
	action: tensor([[-1.1564,  1.7018, -0.3129, -4.1227, -1.0307,  1.8995,  1.5211]],
       dtype=torch.float64)
	q_value: tensor([[-1.7879]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 4, step: 64
	action: tensor([[-0.6193, -0.5920, -2.5642, -3.0165, -0.6322, -1.5450, -0.1830]],
       dtype=torch.float64)
	q_value: tensor([[-2.4993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 4, step: 65
	action: tensor([[-0.0454, -0.0825, -0.1703, -1.5634, -1.0776,  0.1175,  0.1591]],
       dtype=torch.float64)
	q_value: tensor([[-2.4993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23237237059084803, distance: 1.2703625131548764 entropy 1.7646539211273193
epoch: 4, step: 66
	action: tensor([[-0.0957,  0.4839, -1.0055, -4.2324,  1.0909, -2.5400,  3.5476]],
       dtype=torch.float64)
	q_value: tensor([[-2.1068]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 4, step: 67
	action: tensor([[ 1.2291, -1.9254,  2.6577, -1.0082,  0.6447, -0.8176, -0.6478]],
       dtype=torch.float64)
	q_value: tensor([[-2.4993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 4, step: 68
	action: tensor([[-3.6058e+00,  6.2402e-01, -1.5741e-01, -2.3952e+00,  3.2780e-03,
          2.1211e+00, -2.3131e+00]], dtype=torch.float64)
	q_value: tensor([[-2.4993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 4, step: 69
	action: tensor([[-1.7346, -2.7301,  0.7324,  2.5243,  0.4341,  0.4793,  0.6709]],
       dtype=torch.float64)
	q_value: tensor([[-2.4993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 4, step: 70
	action: tensor([[-1.3389, -1.3665,  0.8275, -0.4442,  1.3982,  0.4414,  1.0182]],
       dtype=torch.float64)
	q_value: tensor([[-2.4993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2952517466718665, distance: 1.733691646232804 entropy 1.7646539211273193
epoch: 4, step: 71
	action: tensor([[-1.7135, -1.3154,  1.4630,  0.4418, -0.9373,  1.2756, -2.2689]],
       dtype=torch.float64)
	q_value: tensor([[-2.2355]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 4, step: 72
	action: tensor([[ 2.1490,  0.2363,  2.2453,  0.4253,  2.1291, -1.9054, -0.5151]],
       dtype=torch.float64)
	q_value: tensor([[-2.4993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 4, step: 73
	action: tensor([[ 0.0122,  0.3276,  0.9517, -0.2534, -1.0817,  0.8415, -0.7374]],
       dtype=torch.float64)
	q_value: tensor([[-2.4993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 4, step: 74
	action: tensor([[ 0.9177, -1.8833, -0.0861,  1.0601, -0.1485,  0.1259,  3.2645]],
       dtype=torch.float64)
	q_value: tensor([[-2.4993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 4, step: 75
	action: tensor([[ 0.8904, -2.6987, -1.9677, -0.6648,  1.2020,  0.0544,  0.0451]],
       dtype=torch.float64)
	q_value: tensor([[-2.4993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 4, step: 76
	action: tensor([[-0.1517, -1.6494, -0.3990, -1.3862, -0.1933, -0.4078,  0.5993]],
       dtype=torch.float64)
	q_value: tensor([[-2.4993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.28892003927411225, distance: 1.2991810617469841 entropy 1.7646539211273193
epoch: 4, step: 77
	action: tensor([[-0.2417, -3.9016, -0.7632,  1.6858,  0.9358, -1.7852,  2.7530]],
       dtype=torch.float64)
	q_value: tensor([[-2.0697]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 4, step: 78
	action: tensor([[ 0.8613, -0.8652, -1.8918, -0.9194,  1.0988, -2.8807,  0.3188]],
       dtype=torch.float64)
	q_value: tensor([[-2.4993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 4, step: 79
	action: tensor([[-0.2214, -0.4149, -0.4360, -0.2147,  2.8846,  1.5512,  3.9573]],
       dtype=torch.float64)
	q_value: tensor([[-2.4993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1471905449497573 entropy 1.7646539211273193
epoch: 4, step: 80
	action: tensor([[ 1.5832,  0.6452,  0.6561,  0.4135,  0.1648, -2.8220, -0.2772]],
       dtype=torch.float64)
	q_value: tensor([[-2.4993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 4, step: 81
	action: tensor([[-1.2731,  0.4503,  2.3208, -3.1326,  2.4590,  0.6227, -0.6991]],
       dtype=torch.float64)
	q_value: tensor([[-2.4993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 4, step: 82
	action: tensor([[ 0.3398, -1.1859,  1.3980,  0.9866, -1.2397,  1.3932,  0.9881]],
       dtype=torch.float64)
	q_value: tensor([[-2.4993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.48310469917178556, distance: 0.8227312343015485 entropy 1.7646539211273193
epoch: 4, step: 83
	action: tensor([[-0.9568, -1.9407, -0.7206,  0.2394,  0.6821, -1.8622,  0.6245]],
       dtype=torch.float64)
	q_value: tensor([[-2.6081]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 4, step: 84
	action: tensor([[ 1.6200, -0.6624,  0.1369,  0.0044, -0.6328,  0.4786,  1.3599]],
       dtype=torch.float64)
	q_value: tensor([[-2.4993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.004625199763539922, distance: 1.1416947764721408 entropy 1.7646539211273193
epoch: 4, step: 85
	action: tensor([[-0.6403, -0.9845, -1.2690,  1.7954, -2.3074, -2.4198,  2.6260]],
       dtype=torch.float64)
	q_value: tensor([[-2.2244]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1920409864869138, distance: 1.028611459553424 entropy 1.7646539211273193
epoch: 4, step: 86
	action: tensor([[-1.3207,  1.2524,  1.0435,  0.6461, -1.1169, -3.0005,  0.7019]],
       dtype=torch.float64)
	q_value: tensor([[-3.8895]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 4, step: 87
	action: tensor([[ 0.0030, -0.9008,  0.4215, -0.0272, -0.9686,  1.0605,  0.2764]],
       dtype=torch.float64)
	q_value: tensor([[-2.4993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.27757785269838897, distance: 1.2934521901711236 entropy 1.7646539211273193
epoch: 4, step: 88
	action: tensor([[ 1.8414, -0.5717, -0.0961, -0.6955, -1.2673,  2.0808, -1.1791]],
       dtype=torch.float64)
	q_value: tensor([[-1.9348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35932341879728, distance: 0.915959176509862 entropy 1.7646539211273193
epoch: 4, step: 89
	action: tensor([[-0.6485,  0.8032, -1.2909, -0.8416,  0.3651,  1.6419,  0.1983]],
       dtype=torch.float64)
	q_value: tensor([[-2.5087]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 4, step: 90
	action: tensor([[-3.4765, -2.7191,  0.0478,  0.5810,  0.9048,  0.0669, -1.0249]],
       dtype=torch.float64)
	q_value: tensor([[-2.4993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 4, step: 91
	action: tensor([[-0.3466, -2.0010, -2.3468,  0.1762, -0.7012, -0.9037,  0.4211]],
       dtype=torch.float64)
	q_value: tensor([[-2.4993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 4, step: 92
	action: tensor([[-0.2703, -3.7437,  0.2089, -1.9213, -2.6252, -0.5730, -0.5141]],
       dtype=torch.float64)
	q_value: tensor([[-2.4993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 4, step: 93
	action: tensor([[ 0.8182, -2.4667,  0.2601,  0.0832,  0.2445, -0.2175, -1.7808]],
       dtype=torch.float64)
	q_value: tensor([[-2.4993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 4, step: 94
	action: tensor([[ 0.2701, -1.6022, -2.0454,  1.9462, -0.1256, -0.8357, -1.0711]],
       dtype=torch.float64)
	q_value: tensor([[-2.4993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6567851860212957, distance: 1.4729560705036955 entropy 1.7646539211273193
epoch: 4, step: 95
	action: tensor([[ 0.8527, -3.2900,  3.5829, -2.8768,  0.2560, -2.0583, -0.7166]],
       dtype=torch.float64)
	q_value: tensor([[-2.0049]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 4, step: 96
	action: tensor([[ 1.3394,  0.5526,  1.5408, -1.5913,  0.3688,  1.9763, -0.7542]],
       dtype=torch.float64)
	q_value: tensor([[-2.4993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 4, step: 97
	action: tensor([[-1.6711, -2.6185,  0.0318, -0.9301,  1.9067,  0.4518, -0.0673]],
       dtype=torch.float64)
	q_value: tensor([[-2.4993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 4, step: 98
	action: tensor([[-0.2079, -0.2825, -3.1878, -2.8333, -0.1393,  2.7595, -0.1573]],
       dtype=torch.float64)
	q_value: tensor([[-2.4993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 4, step: 99
	action: tensor([[-2.0983, -2.2544,  1.4288,  2.0894, -0.9295, -0.4787,  2.6798]],
       dtype=torch.float64)
	q_value: tensor([[-2.4993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 4, step: 100
	action: tensor([[ 0.6201, -1.9355, -0.7921, -0.6651,  1.2747, -3.1777, -0.9077]],
       dtype=torch.float64)
	q_value: tensor([[-2.4993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 4, step: 101
	action: tensor([[ 1.0472,  0.0429, -0.2939,  0.9972,  1.0995,  2.1746,  0.5043]],
       dtype=torch.float64)
	q_value: tensor([[-2.4993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 4, step: 102
	action: tensor([[-1.8051, -1.5106, -2.0837, -0.2166,  1.8939,  1.2954,  0.4337]],
       dtype=torch.float64)
	q_value: tensor([[-2.4993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 4, step: 103
	action: tensor([[-2.4700,  1.3451,  1.1053,  0.4168, -0.8076,  1.1295, -0.6840]],
       dtype=torch.float64)
	q_value: tensor([[-2.4993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 4, step: 104
	action: tensor([[ 0.2923, -2.1732,  0.8952,  0.8293,  1.5379,  0.5333, -1.4370]],
       dtype=torch.float64)
	q_value: tensor([[-2.4993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 4, step: 105
	action: tensor([[ 1.3598, -4.0261,  2.3455, -0.2012, -1.1576,  0.9430, -2.7405]],
       dtype=torch.float64)
	q_value: tensor([[-2.4993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 4, step: 106
	action: tensor([[-1.3306, -0.7405, -0.0194,  2.8389,  2.0960,  1.6514, -0.8640]],
       dtype=torch.float64)
	q_value: tensor([[-2.4993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.025985052175456103, distance: 1.1591168255160802 entropy 1.7646539211273193
epoch: 4, step: 107
	action: tensor([[-0.5990, -0.5080, -0.6290,  1.8197,  2.6165,  2.3774,  0.8551]],
       dtype=torch.float64)
	q_value: tensor([[-2.5575]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 4, step: 108
	action: tensor([[ 1.9449,  1.0713,  0.2542, -1.8656, -2.0659, -0.8569,  0.1719]],
       dtype=torch.float64)
	q_value: tensor([[-2.4993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 4, step: 109
	action: tensor([[ 0.0565, -1.5668, -1.5451, -0.6230, -1.4148,  1.6691,  1.4134]],
       dtype=torch.float64)
	q_value: tensor([[-2.4993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4513328580856444, distance: 1.3786061360151014 entropy 1.7646539211273193
epoch: 4, step: 110
	action: tensor([[ 1.0667,  0.8738,  0.0539,  1.3104, -0.7480, -0.1695, -2.3574]],
       dtype=torch.float64)
	q_value: tensor([[-3.2947]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 4, step: 111
	action: tensor([[ 2.5022, -3.2179, -0.5490,  0.3345,  2.8591, -0.3005, -0.0581]],
       dtype=torch.float64)
	q_value: tensor([[-2.4993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 4, step: 112
	action: tensor([[-0.0087,  1.1884, -0.5733, -0.8140, -3.2157,  0.3451,  0.5983]],
       dtype=torch.float64)
	q_value: tensor([[-2.4993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 4, step: 113
	action: tensor([[-1.0357, -1.8573, -1.1174, -0.1706, -1.9018, -1.6306,  0.0209]],
       dtype=torch.float64)
	q_value: tensor([[-2.4993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 4, step: 114
	action: tensor([[-1.1107,  1.6891,  0.0584, -0.9706,  0.8017, -1.2947,  0.0027]],
       dtype=torch.float64)
	q_value: tensor([[-2.4993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 4, step: 115
	action: tensor([[-1.2016,  0.0722, -1.0411, -0.4569,  0.1712,  1.3040,  0.2071]],
       dtype=torch.float64)
	q_value: tensor([[-2.4993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8378888924666286, distance: 1.5513733252702573 entropy 1.7646539211273193
epoch: 4, step: 116
	action: tensor([[ 0.0688, -0.1418, -0.5695,  0.1493,  2.3943,  0.6399, -0.5747]],
       dtype=torch.float64)
	q_value: tensor([[-2.3240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4304065560496957, distance: 0.8636527977354184 entropy 1.7646539211273193
epoch: 4, step: 117
	action: tensor([[ 1.9418, -0.9213, -0.4859, -0.2306, -0.5690, -0.3485,  0.2158]],
       dtype=torch.float64)
	q_value: tensor([[-2.2147]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7850263697259803, distance: 1.528899756099513 entropy 1.7646539211273193
epoch: 4, step: 118
	action: tensor([[-0.0902, -4.1264,  1.1421, -1.9489, -2.6807, -2.1912,  1.7744]],
       dtype=torch.float64)
	q_value: tensor([[-1.6982]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 4, step: 119
	action: tensor([[-0.3063, -1.7033, -1.4858,  2.2654, -1.3687, -1.3079, -1.0722]],
       dtype=torch.float64)
	q_value: tensor([[-2.4993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10063418419475956, distance: 1.2005443049332867 entropy 1.7646539211273193
epoch: 4, step: 120
	action: tensor([[ 0.3222, -3.7694, -0.4323,  0.4126, -2.3039,  0.0488, -0.3095]],
       dtype=torch.float64)
	q_value: tensor([[-2.4248]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 4, step: 121
	action: tensor([[-0.3008, -1.7159, -1.3194,  0.2649,  1.2689, -2.5946,  1.8633]],
       dtype=torch.float64)
	q_value: tensor([[-2.4993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3711341273674298, distance: 1.3399749004835133 entropy 1.7646539211273193
epoch: 4, step: 122
	action: tensor([[-0.0593, -0.8097, -0.2656, -3.2243, -0.6589, -0.7436,  1.9357]],
       dtype=torch.float64)
	q_value: tensor([[-2.5886]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 4, step: 123
	action: tensor([[-2.3871, -3.2237, -3.9801, -1.0380, -0.9855, -0.6626, -0.3745]],
       dtype=torch.float64)
	q_value: tensor([[-2.4993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 4, step: 124
	action: tensor([[-3.4792, -1.4667, -2.5657,  1.0790,  1.8571, -0.4117,  0.1662]],
       dtype=torch.float64)
	q_value: tensor([[-2.4993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 4, step: 125
	action: tensor([[ 0.2846, -1.1622,  1.5633,  0.0406, -3.7784,  0.1292,  2.8157]],
       dtype=torch.float64)
	q_value: tensor([[-2.4993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4877767201458052 entropy 1.7646539211273193
epoch: 4, step: 126
	action: tensor([[-0.5396, -0.8850,  1.2882,  1.2698,  1.0482, -1.7729, -0.7069]],
       dtype=torch.float64)
	q_value: tensor([[-2.4993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.004747328698436304, distance: 1.1470573270091238 entropy 1.7646539211273193
epoch: 4, step: 127
	action: tensor([[ 0.7320, -3.2207, -0.1355,  2.1340,  1.1036,  0.3457, -0.2278]],
       dtype=torch.float64)
	q_value: tensor([[-2.2999]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
LOSS epoch 4 actor 591.957682692009 critic 2150.9434120611986 
epoch: 5, step: 0
	action: tensor([[-1.3368, -1.6985, -0.8105, -1.4125, -0.6768,  3.8397, -0.4303]],
       dtype=torch.float64)
	q_value: tensor([[-4.5102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 5, step: 1
	action: tensor([[-4.3923,  0.1426, -1.6009, -0.1997,  2.2371, -0.7748, -0.8509]],
       dtype=torch.float64)
	q_value: tensor([[-4.5102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 5, step: 2
	action: tensor([[-1.6045, -0.3495,  2.0130, -2.0518, -0.1469,  0.5873,  0.5498]],
       dtype=torch.float64)
	q_value: tensor([[-4.5102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7525937980167536, distance: 1.5149466131094387 entropy 1.7646539211273193
epoch: 5, step: 3
	action: tensor([[ 0.3385, -0.5196,  0.9446, -2.1340, -0.0357, -0.4133,  1.4046]],
       dtype=torch.float64)
	q_value: tensor([[-5.3964]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2602098690322312, distance: 1.2846302116880917 entropy 1.7646539211273193
epoch: 5, step: 4
	action: tensor([[ 0.6237, -0.0289,  1.1683,  1.4470,  2.0796, -0.3865,  0.9184]],
       dtype=torch.float64)
	q_value: tensor([[-4.5687]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8317626679867371, distance: 0.46937275757386837 entropy 1.7646539211273193
epoch: 5, step: 5
	action: tensor([[ 0.4033, -1.6914, -0.8951, -0.0731, -1.0168, -0.5835,  3.5144]],
       dtype=torch.float64)
	q_value: tensor([[-4.1563]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 5, step: 6
	action: tensor([[ 0.6827,  1.7518, -0.8871, -0.8155,  0.6243, -1.6919, -1.8062]],
       dtype=torch.float64)
	q_value: tensor([[-4.5102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 5, step: 7
	action: tensor([[ 0.8134,  1.6256, -0.2935, -1.5700,  1.0568, -2.0747,  0.7208]],
       dtype=torch.float64)
	q_value: tensor([[-4.5102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 5, step: 8
	action: tensor([[-1.5177, -0.4635, -1.7888,  0.3249,  0.3541,  0.2044,  1.1713]],
       dtype=torch.float64)
	q_value: tensor([[-4.5102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.9210721245440152, distance: 1.95581527395406 entropy 1.7646539211273193
epoch: 5, step: 9
	action: tensor([[ 1.5889,  1.7225, -0.5794, -0.6200,  1.3809, -3.8696, -0.0312]],
       dtype=torch.float64)
	q_value: tensor([[-4.2921]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 5, step: 10
	action: tensor([[-1.2198,  0.9944,  0.3207,  0.8119, -1.6842,  2.9025,  0.5422]],
       dtype=torch.float64)
	q_value: tensor([[-4.5102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 5, step: 11
	action: tensor([[-0.2422, -1.7289, -0.2871, -0.8331,  0.9587,  0.4103,  3.1951]],
       dtype=torch.float64)
	q_value: tensor([[-4.5102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 5, step: 12
	action: tensor([[ 0.6467, -1.4111,  2.2013, -0.5846,  1.7509, -1.2179, -0.9923]],
       dtype=torch.float64)
	q_value: tensor([[-4.5102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 5, step: 13
	action: tensor([[ 1.8920,  1.3716, -0.4150,  1.6725,  0.8854,  1.2849, -2.0495]],
       dtype=torch.float64)
	q_value: tensor([[-4.5102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 5, step: 14
	action: tensor([[-0.2681, -1.4111, -0.0645, -0.2507, -1.0242,  1.0609,  0.8028]],
       dtype=torch.float64)
	q_value: tensor([[-4.5102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9707466189269165, distance: 1.606468034356401 entropy 1.7646539211273193
epoch: 5, step: 15
	action: tensor([[ 0.7239,  0.6866,  0.5941,  0.1934, -1.4620, -0.8674,  0.0843]],
       dtype=torch.float64)
	q_value: tensor([[-4.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9891736028705899, distance: 0.11906899009576201 entropy 1.7646539211273193
epoch: 5, step: 16
	action: tensor([[-0.7198, -0.1730, -0.4048, -0.8528, -0.9987, -0.9725,  2.1829]],
       dtype=torch.float64)
	q_value: tensor([[-3.5962]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 5, step: 17
	action: tensor([[ 0.9170, -2.1628, -2.2435, -2.5828, -1.9663, -0.3152,  0.7582]],
       dtype=torch.float64)
	q_value: tensor([[-4.5102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 5, step: 18
	action: tensor([[ 0.7022, -3.2519, -0.5106,  0.7610, -0.2673,  0.7911,  0.0309]],
       dtype=torch.float64)
	q_value: tensor([[-4.5102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 5, step: 19
	action: tensor([[-0.3774,  1.8401,  1.6224, -1.4058, -0.9910,  2.1176,  0.4909]],
       dtype=torch.float64)
	q_value: tensor([[-4.5102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 5, step: 20
	action: tensor([[ 1.0249,  0.0083,  2.5012,  1.1343, -0.7665, -1.1254,  0.5971]],
       dtype=torch.float64)
	q_value: tensor([[-4.5102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 5, step: 21
	action: tensor([[-1.8187, -1.2643, -2.0263, -0.8211,  0.5782,  1.3913,  0.7323]],
       dtype=torch.float64)
	q_value: tensor([[-4.5102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 5, step: 22
	action: tensor([[-1.3086,  0.8125, -1.3193, -2.1559, -0.0730,  2.1686, -0.1740]],
       dtype=torch.float64)
	q_value: tensor([[-4.5102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16938543318338606, distance: 1.0429331332045064 entropy 1.7646539211273193
epoch: 5, step: 23
	action: tensor([[ 0.1733,  0.4667, -2.2114, -2.3591, -0.7295, -2.0947, -0.3960]],
       dtype=torch.float64)
	q_value: tensor([[-6.7388]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 5, step: 24
	action: tensor([[ 0.8628, -0.8726, -0.6411,  1.6743,  0.4941,  2.8697,  0.5655]],
       dtype=torch.float64)
	q_value: tensor([[-4.5102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 5, step: 25
	action: tensor([[ 0.1880, -0.6230, -0.7927, -0.9036,  1.2430, -0.6514, -0.9666]],
       dtype=torch.float64)
	q_value: tensor([[-4.5102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3030645457396386, distance: 1.306290166332529 entropy 1.7646539211273193
epoch: 5, step: 26
	action: tensor([[ 1.2937, -1.2991, -2.8776, -0.3923, -2.0324,  0.9060,  1.6248]],
       dtype=torch.float64)
	q_value: tensor([[-3.4217]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3527423841590318, distance: 1.3309576661156755 entropy 1.7646539211273193
epoch: 5, step: 27
	action: tensor([[-0.3070,  1.0541,  0.0176, -0.3563, -2.1264,  1.1833, -0.6708]],
       dtype=torch.float64)
	q_value: tensor([[-6.5682]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 5, step: 28
	action: tensor([[-2.6010,  1.3216, -0.3976, -0.2100,  0.7447,  0.1919, -1.6426]],
       dtype=torch.float64)
	q_value: tensor([[-4.5102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 5, step: 29
	action: tensor([[ 3.1137,  0.4417,  1.6466, -0.9422,  2.9216,  2.7565, -1.1495]],
       dtype=torch.float64)
	q_value: tensor([[-4.5102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 5, step: 30
	action: tensor([[-0.7166,  2.0478, -2.5567, -1.0060, -0.8917,  0.6275,  0.5165]],
       dtype=torch.float64)
	q_value: tensor([[-4.5102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 5, step: 31
	action: tensor([[-0.5766, -0.4488, -0.3322,  0.0536,  0.6222, -0.5982, -1.5475]],
       dtype=torch.float64)
	q_value: tensor([[-4.5102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8248505565618542, distance: 1.5458606611043149 entropy 1.7646539211273193
epoch: 5, step: 32
	action: tensor([[-0.0437, -3.7295, -1.8295, -0.5842,  2.0648, -1.9567, -0.5655]],
       dtype=torch.float64)
	q_value: tensor([[-2.9319]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 5, step: 33
	action: tensor([[ 0.3668, -0.0917, -0.4643,  2.3246, -2.5700, -0.6318, -0.5037]],
       dtype=torch.float64)
	q_value: tensor([[-4.5102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 5, step: 34
	action: tensor([[ 1.1332, -2.9073,  0.3913,  2.4730,  0.0390, -1.4612,  1.1971]],
       dtype=torch.float64)
	q_value: tensor([[-4.5102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 5, step: 35
	action: tensor([[ 1.1255, -0.7707,  1.2476,  2.3518, -1.1143,  1.2879, -2.0895]],
       dtype=torch.float64)
	q_value: tensor([[-4.5102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 5, step: 36
	action: tensor([[ 0.1914, -1.9916, -0.0281, -0.8434, -0.1759,  1.9964,  1.1313]],
       dtype=torch.float64)
	q_value: tensor([[-4.5102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 5, step: 37
	action: tensor([[ 1.1614,  0.6484,  0.1620, -1.0308,  1.3286, -0.6249, -1.9383]],
       dtype=torch.float64)
	q_value: tensor([[-4.5102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 5, step: 38
	action: tensor([[-0.2388, -1.4216,  1.1406,  2.1288, -0.0251,  1.0831,  0.8068]],
       dtype=torch.float64)
	q_value: tensor([[-4.5102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6959988345487951, distance: 0.630949088890343 entropy 1.7646539211273193
epoch: 5, step: 39
	action: tensor([[ 1.4759,  0.9192, -0.1665, -1.5830, -0.9558, -0.5791, -1.9023]],
       dtype=torch.float64)
	q_value: tensor([[-4.2995]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3073701180081674, distance: 0.9523735062633336 entropy 1.7646539211273193
epoch: 5, step: 40
	action: tensor([[ 0.2651, -1.2580, -1.7867,  0.2460,  0.4506,  0.0029,  0.6041]],
       dtype=torch.float64)
	q_value: tensor([[-3.8605]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8167346867143204, distance: 1.5424192876960536 entropy 1.7646539211273193
epoch: 5, step: 41
	action: tensor([[-0.4372,  1.1420, -2.5024, -1.8160, -0.8209, -1.6717, -0.1713]],
       dtype=torch.float64)
	q_value: tensor([[-3.4222]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 5, step: 42
	action: tensor([[-0.0802, -1.0400, -2.3350,  0.4072, -0.5248,  2.3388,  2.9250]],
       dtype=torch.float64)
	q_value: tensor([[-4.5102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.29537571395484985, distance: 1.3024305319895448 entropy 1.7646539211273193
epoch: 5, step: 43
	action: tensor([[2.7651, 0.8674, 0.7555, 1.5311, 1.5642, 0.6615, 0.9304]],
       dtype=torch.float64)
	q_value: tensor([[-7.4070]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 5, step: 44
	action: tensor([[-1.3646,  0.0153, -1.6776,  1.3285, -1.4772, -0.1957, -1.6871]],
       dtype=torch.float64)
	q_value: tensor([[-4.5102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.3004896021078598, distance: 1.7356686954667866 entropy 1.7646539211273193
epoch: 5, step: 45
	action: tensor([[-0.4621,  0.5273, -2.6890, -0.5001,  0.4787,  0.9584,  2.4585]],
       dtype=torch.float64)
	q_value: tensor([[-4.1930]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 5, step: 46
	action: tensor([[-0.0998,  0.1445,  0.0133,  2.1015,  1.6683, -0.8777, -2.9397]],
       dtype=torch.float64)
	q_value: tensor([[-4.5102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31090967674549275, distance: 0.949936923761042 entropy 1.7646539211273193
epoch: 5, step: 47
	action: tensor([[ 0.3070, -0.3425, -0.0585, -1.4641,  0.3594, -0.1090,  0.4453]],
       dtype=torch.float64)
	q_value: tensor([[-4.9200]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 5, step: 48
	action: tensor([[ 0.8580, -2.0203,  0.9583, -0.6996, -0.5067, -1.3520,  2.4051]],
       dtype=torch.float64)
	q_value: tensor([[-4.5102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 5, step: 49
	action: tensor([[ 0.2539,  0.1472,  0.9641,  1.7342, -0.5490,  1.0100, -1.0611]],
       dtype=torch.float64)
	q_value: tensor([[-4.5102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 5, step: 50
	action: tensor([[-0.0957, -2.2891, -1.6402,  0.7376,  2.6428, -0.7236, -0.3872]],
       dtype=torch.float64)
	q_value: tensor([[-4.5102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 5, step: 51
	action: tensor([[ 0.7463,  1.6413, -1.3678, -1.0834,  0.9021, -0.6049,  0.1415]],
       dtype=torch.float64)
	q_value: tensor([[-4.5102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 5, step: 52
	action: tensor([[-1.3401,  0.1645, -3.3282, -1.8504,  0.0131,  0.0899,  1.0159]],
       dtype=torch.float64)
	q_value: tensor([[-4.5102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 5, step: 53
	action: tensor([[-2.8315, -1.9835, -1.0821, -2.4836,  1.3408,  0.6050, -1.2239]],
       dtype=torch.float64)
	q_value: tensor([[-4.5102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 5, step: 54
	action: tensor([[-1.7444,  0.1857,  1.1371, -0.3512,  1.3847,  2.4843,  0.3615]],
       dtype=torch.float64)
	q_value: tensor([[-4.5102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 5, step: 55
	action: tensor([[ 0.3750, -1.3863,  2.0929, -1.1788, -0.3889, -0.6012,  0.4367]],
       dtype=torch.float64)
	q_value: tensor([[-4.5102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 5, step: 56
	action: tensor([[ 0.0560,  0.0991,  1.9183,  0.3160, -0.3873,  1.2807, -0.7092]],
       dtype=torch.float64)
	q_value: tensor([[-4.5102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 5, step: 57
	action: tensor([[-1.2996, -2.8566, -1.7157, -1.2794,  0.1231,  0.1024, -1.7131]],
       dtype=torch.float64)
	q_value: tensor([[-4.5102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 5, step: 58
	action: tensor([[-0.2294, -0.6441,  0.8563, -1.5152,  2.1655, -0.7704,  1.3186]],
       dtype=torch.float64)
	q_value: tensor([[-4.5102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9058433403561854, distance: 1.5797933913210591 entropy 1.7646539211273193
epoch: 5, step: 59
	action: tensor([[ 1.3367, -1.6652,  3.2192,  0.9025, -0.7510,  0.5122, -1.8620]],
       dtype=torch.float64)
	q_value: tensor([[-4.5854]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 5, step: 60
	action: tensor([[-0.0075, -2.0021, -0.3398, -0.6344, -1.6564, -3.6034,  0.3897]],
       dtype=torch.float64)
	q_value: tensor([[-4.5102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 5, step: 61
	action: tensor([[-0.4645, -1.7375, -0.0099,  2.2231,  1.7983, -0.5840,  0.3084]],
       dtype=torch.float64)
	q_value: tensor([[-4.5102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14215386333024982, distance: 1.0598914606685663 entropy 1.7646539211273193
epoch: 5, step: 62
	action: tensor([[-0.6159, -1.1362, -1.5386, -2.3656,  0.7099,  1.6352, -0.5131]],
       dtype=torch.float64)
	q_value: tensor([[-3.7238]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3640893356130328, distance: 0.9125459614006457 entropy 1.7646539211273193
epoch: 5, step: 63
	action: tensor([[ 0.2853, -0.7655,  0.9806, -4.0748, -1.0090, -1.3192,  1.8599]],
       dtype=torch.float64)
	q_value: tensor([[-6.1335]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 5, step: 64
	action: tensor([[0.1791, 0.4772, 0.3226, 1.8724, 1.0233, 0.2231, 0.0566]],
       dtype=torch.float64)
	q_value: tensor([[-4.5102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 5, step: 65
	action: tensor([[-2.4016, -2.1593, -1.8397, -0.6384,  1.5075,  0.3000,  1.3459]],
       dtype=torch.float64)
	q_value: tensor([[-4.5102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 5, step: 66
	action: tensor([[-0.2418,  1.1275,  1.3767, -1.5016, -0.0974, -1.4483,  0.4402]],
       dtype=torch.float64)
	q_value: tensor([[-4.5102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06339624804746702, distance: 1.1800604499411065 entropy 1.7646539211273193
epoch: 5, step: 67
	action: tensor([[ 0.3272, -1.4725,  1.3488, -1.1919, -2.8174,  1.4288, -0.3719]],
       dtype=torch.float64)
	q_value: tensor([[-3.9946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11562064345986744, distance: 1.2086900979028758 entropy 1.7646539211273193
epoch: 5, step: 68
	action: tensor([[ 1.6468,  0.7579, -0.3673, -0.6424,  0.6695,  0.7261,  1.0576]],
       dtype=torch.float64)
	q_value: tensor([[-6.1478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.49398627184099986, distance: 0.8140251874554847 entropy 1.7646539211273193
epoch: 5, step: 69
	action: tensor([[ 0.0835, -0.0149, -1.0696,  2.3453,  0.6060,  0.8967, -0.8678]],
       dtype=torch.float64)
	q_value: tensor([[-3.9175]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 5, step: 70
	action: tensor([[ 0.2284, -2.8209,  0.3318, -0.2512,  1.9060,  1.1522, -0.6149]],
       dtype=torch.float64)
	q_value: tensor([[-4.5102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 5, step: 71
	action: tensor([[ 0.5076,  0.1686, -0.4203, -0.8606,  1.1563,  1.1622, -1.2176]],
       dtype=torch.float64)
	q_value: tensor([[-4.5102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 5, step: 72
	action: tensor([[-0.8992, -1.7279,  1.2462,  0.3838, -1.4749, -1.3419,  1.7111]],
       dtype=torch.float64)
	q_value: tensor([[-4.5102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5623161306584339, distance: 1.4303461510940245 entropy 1.7646539211273193
epoch: 5, step: 73
	action: tensor([[-2.3977,  0.3545, -2.2195, -0.2630, -2.1692, -2.2701,  1.8945]],
       dtype=torch.float64)
	q_value: tensor([[-4.7893]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 5, step: 74
	action: tensor([[-0.7976, -0.4930,  0.7391,  0.0438,  0.2399, -0.7307, -1.8483]],
       dtype=torch.float64)
	q_value: tensor([[-4.5102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1496552314002755, distance: 1.677803512452514 entropy 1.7646539211273193
epoch: 5, step: 75
	action: tensor([[ 0.7521,  1.2529, -0.1677,  1.1938, -3.8365, -0.8747,  0.4235]],
       dtype=torch.float64)
	q_value: tensor([[-3.6627]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 5, step: 76
	action: tensor([[-1.1150,  1.0726, -0.5170,  1.4504, -0.4580,  0.0103, -0.2127]],
       dtype=torch.float64)
	q_value: tensor([[-4.5102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 5, step: 77
	action: tensor([[ 2.1283,  0.4265, -1.4903, -1.8753,  0.6611, -0.4127, -2.6167]],
       dtype=torch.float64)
	q_value: tensor([[-4.5102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 5, step: 78
	action: tensor([[-0.0033, -2.0628,  0.2131,  2.9394, -0.1714,  0.0401, -0.6354]],
       dtype=torch.float64)
	q_value: tensor([[-4.5102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 5, step: 79
	action: tensor([[ 0.0256,  2.3625, -0.0280, -0.5039, -0.4850,  1.0350, -0.8598]],
       dtype=torch.float64)
	q_value: tensor([[-4.5102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 5, step: 80
	action: tensor([[ 1.8768, -0.7977, -0.3519,  1.9536, -0.5328,  0.5229,  2.6223]],
       dtype=torch.float64)
	q_value: tensor([[-4.5102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8754113188129293, distance: 0.4039205858762201 entropy 1.7646539211273193
epoch: 5, step: 81
	action: tensor([[-2.2891, -0.9447, -0.6689,  0.0929,  0.4229,  2.1634, -0.8862]],
       dtype=torch.float64)
	q_value: tensor([[-5.4332]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8093031491776808, distance: 1.539261343629654 entropy 1.7646539211273193
epoch: 5, step: 82
	action: tensor([[-0.9722, -1.6028, -0.0250, -1.0257, -0.6449, -0.4611,  1.1676]],
       dtype=torch.float64)
	q_value: tensor([[-4.6763]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 5, step: 83
	action: tensor([[-1.2477,  1.4567, -0.3338, -1.2754, -1.1982,  0.4034,  1.5289]],
       dtype=torch.float64)
	q_value: tensor([[-4.5102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 5, step: 84
	action: tensor([[ 0.5703, -2.7938, -0.8431,  1.2542, -0.3286,  1.7361,  1.8725]],
       dtype=torch.float64)
	q_value: tensor([[-4.5102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 5, step: 85
	action: tensor([[-2.7424, -1.2715,  0.7664, -0.0302,  0.4366,  1.9448, -0.3313]],
       dtype=torch.float64)
	q_value: tensor([[-4.5102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 5, step: 86
	action: tensor([[ 1.8002,  1.2454, -1.1085, -0.9792, -0.8884,  1.8307,  1.8611]],
       dtype=torch.float64)
	q_value: tensor([[-4.5102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 5, step: 87
	action: tensor([[-0.3006, -1.7630,  0.9061, -0.9460,  0.3691, -0.8332, -1.0095]],
       dtype=torch.float64)
	q_value: tensor([[-4.5102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 5, step: 88
	action: tensor([[ 0.7514,  1.0534, -1.7676, -0.5754, -2.3353,  1.5671, -0.2037]],
       dtype=torch.float64)
	q_value: tensor([[-4.5102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 5, step: 89
	action: tensor([[-3.9588,  1.6675,  0.2180,  0.3434, -1.4862, -3.0885, -0.7911]],
       dtype=torch.float64)
	q_value: tensor([[-4.5102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 5, step: 90
	action: tensor([[ 1.5300,  0.1566, -0.2457,  2.0845, -1.2855, -0.9616, -0.9108]],
       dtype=torch.float64)
	q_value: tensor([[-4.5102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 5, step: 91
	action: tensor([[-0.5330,  0.6990, -2.3760,  0.4419,  2.0553, -2.2798,  0.5274]],
       dtype=torch.float64)
	q_value: tensor([[-4.5102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2598419605145965, distance: 0.9845072612700636 entropy 1.7646539211273193
epoch: 5, step: 92
	action: tensor([[ 1.7015, -0.0652,  0.6650, -0.6902,  0.8317, -1.5179,  0.5049]],
       dtype=torch.float64)
	q_value: tensor([[-5.0673]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 5, step: 93
	action: tensor([[ 3.2511,  1.1308, -0.0472, -1.6443, -1.9714,  0.6203,  2.0117]],
       dtype=torch.float64)
	q_value: tensor([[-4.5102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 5, step: 94
	action: tensor([[ 1.1252,  0.5209, -1.9663, -0.6674, -1.4687, -0.1096, -1.5033]],
       dtype=torch.float64)
	q_value: tensor([[-4.5102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 5, step: 95
	action: tensor([[-1.3119, -1.1359, -0.6056, -3.0375, -1.6453, -0.1151,  1.8110]],
       dtype=torch.float64)
	q_value: tensor([[-4.5102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 5, step: 96
	action: tensor([[-1.1949, -0.0941, -0.7638,  1.0631,  0.1430, -0.1417, -0.5427]],
       dtype=torch.float64)
	q_value: tensor([[-4.5102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0088723755874054, distance: 1.6219328377181532 entropy 1.7646539211273193
epoch: 5, step: 97
	action: tensor([[ 1.1168,  2.1804,  2.4930,  1.8508, -0.7067, -0.4454, -0.0621]],
       dtype=torch.float64)
	q_value: tensor([[-2.6685]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 5, step: 98
	action: tensor([[-1.8966, -1.1290, -0.6540, -0.9437, -1.4341,  0.3280,  0.6623]],
       dtype=torch.float64)
	q_value: tensor([[-4.5102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 5, step: 99
	action: tensor([[-1.5315, -3.3982, -0.3661, -2.5290,  0.7665,  0.9473, -0.7299]],
       dtype=torch.float64)
	q_value: tensor([[-4.5102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 5, step: 100
	action: tensor([[-1.2963,  2.9976, -1.9841,  3.8681,  0.7096, -0.3564, -2.4766]],
       dtype=torch.float64)
	q_value: tensor([[-4.5102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 5, step: 101
	action: tensor([[-0.6371, -0.2647, -0.7456,  0.3285, -0.6297,  0.6892,  0.8990]],
       dtype=torch.float64)
	q_value: tensor([[-4.5102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7684809718816288, distance: 1.521797579316104 entropy 1.7646539211273193
epoch: 5, step: 102
	action: tensor([[-2.6495,  0.4342, -0.2664, -1.6698,  0.4608, -1.4833, -0.1621]],
       dtype=torch.float64)
	q_value: tensor([[-3.4176]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 5, step: 103
	action: tensor([[ 1.8290,  1.6254, -0.9064, -2.9655, -0.3015, -3.1163, -0.3174]],
       dtype=torch.float64)
	q_value: tensor([[-4.5102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 5, step: 104
	action: tensor([[-0.0227, -1.8378, -0.4132, -0.8970, -2.0016,  3.1054, -0.9617]],
       dtype=torch.float64)
	q_value: tensor([[-4.5102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 5, step: 105
	action: tensor([[ 1.8860, -1.9009,  0.6447,  1.5978, -1.7401, -1.3484,  0.2607]],
       dtype=torch.float64)
	q_value: tensor([[-4.5102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 5, step: 106
	action: tensor([[-2.6816, -1.3628, -2.4575,  2.5376,  0.7711, -0.6362,  2.3895]],
       dtype=torch.float64)
	q_value: tensor([[-4.5102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 5, step: 107
	action: tensor([[-0.6732, -0.5761, -1.1707,  1.1096, -1.2102,  0.7455,  1.5674]],
       dtype=torch.float64)
	q_value: tensor([[-4.5102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1017359762254846, distance: 1.6589976560038378 entropy 1.7646539211273193
epoch: 5, step: 108
	action: tensor([[ 0.5905,  0.2192, -0.7768,  0.3870,  0.4677, -1.8742, -0.5272]],
       dtype=torch.float64)
	q_value: tensor([[-4.6430]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7521259358570569, distance: 0.569734130427393 entropy 1.7646539211273193
epoch: 5, step: 109
	action: tensor([[-0.5082,  1.5701, -1.7481,  0.3857,  0.8254,  0.8741,  1.2790]],
       dtype=torch.float64)
	q_value: tensor([[-3.0558]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 5, step: 110
	action: tensor([[-2.4668, -1.0223,  1.6753,  0.1930, -2.1440, -1.6561, -0.6109]],
       dtype=torch.float64)
	q_value: tensor([[-4.5102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 5, step: 111
	action: tensor([[ 0.6288, -1.8266, -2.2364,  0.4374, -0.7921, -0.5704,  0.6089]],
       dtype=torch.float64)
	q_value: tensor([[-4.5102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 5, step: 112
	action: tensor([[ 2.5017,  0.7945,  0.3907,  2.5161,  0.2023,  0.3245, -0.2528]],
       dtype=torch.float64)
	q_value: tensor([[-4.5102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 5, step: 113
	action: tensor([[ 3.0870, -1.7026, -1.1052,  0.7962, -1.4442, -1.1897, -0.3018]],
       dtype=torch.float64)
	q_value: tensor([[-4.5102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 5, step: 114
	action: tensor([[ 1.4458,  1.7062,  1.4024,  0.3876,  2.2666,  1.6044, -1.1934]],
       dtype=torch.float64)
	q_value: tensor([[-4.5102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 5, step: 115
	action: tensor([[ 0.5565,  1.2675,  0.7320,  1.1847, -0.8451,  0.6896, -1.3317]],
       dtype=torch.float64)
	q_value: tensor([[-4.5102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 5, step: 116
	action: tensor([[-1.9143,  2.2192,  1.3637, -0.4376,  0.2964, -0.0085, -2.1726]],
       dtype=torch.float64)
	q_value: tensor([[-4.5102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 5, step: 117
	action: tensor([[-1.5869, -2.0567, -0.7602,  1.2537,  1.5317,  0.1403, -1.7605]],
       dtype=torch.float64)
	q_value: tensor([[-4.5102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 5, step: 118
	action: tensor([[-0.4611, -0.1778,  2.0222, -1.5808,  2.1832, -0.5954, -1.4861]],
       dtype=torch.float64)
	q_value: tensor([[-4.5102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 5, step: 119
	action: tensor([[ 0.4414, -0.9352,  1.7855,  0.7272, -1.4170,  0.4325, -0.0945]],
       dtype=torch.float64)
	q_value: tensor([[-4.5102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.018413437617653905, distance: 1.1337596470965388 entropy 1.7646539211273193
epoch: 5, step: 120
	action: tensor([[ 2.5883, -0.3164, -0.5274, -0.7286, -0.7986, -1.2744, -1.8231]],
       dtype=torch.float64)
	q_value: tensor([[-4.1194]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 5, step: 121
	action: tensor([[-0.4313, -2.9205, -1.4227, -1.1087,  1.8390, -2.2420, -0.0643]],
       dtype=torch.float64)
	q_value: tensor([[-4.5102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 5, step: 122
	action: tensor([[ 0.2853, -1.2196,  0.3384,  0.3089, -1.1085,  2.4254,  1.6207]],
       dtype=torch.float64)
	q_value: tensor([[-4.5102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14256770202069102, distance: 1.0596357755358115 entropy 1.7646539211273193
epoch: 5, step: 123
	action: tensor([[ 1.3790, -0.4457, -0.9058,  1.2519, -0.6525, -1.2148,  1.4416]],
       dtype=torch.float64)
	q_value: tensor([[-5.4655]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7521565768712836, distance: 0.5696989154258063 entropy 1.7646539211273193
epoch: 5, step: 124
	action: tensor([[-0.0718,  1.0068, -1.6646, -0.9280, -1.3906,  0.0679,  1.8720]],
       dtype=torch.float64)
	q_value: tensor([[-4.2077]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 5, step: 125
	action: tensor([[ 0.6514, -0.7488,  1.1654, -1.1659,  0.2276, -1.4061,  1.0176]],
       dtype=torch.float64)
	q_value: tensor([[-4.5102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.052229425733867685, distance: 1.1140592879573417 entropy 1.7646539211273193
epoch: 5, step: 126
	action: tensor([[-0.4749, -0.1669, -1.2251, -0.4018,  0.2333,  0.3650,  1.9653]],
       dtype=torch.float64)
	q_value: tensor([[-3.4607]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21961731184926836, distance: 1.2637712855568919 entropy 1.7646539211273193
epoch: 5, step: 127
	action: tensor([[-0.7347,  0.0301, -1.0901,  0.2753,  1.5649,  1.9293, -1.1955]],
       dtype=torch.float64)
	q_value: tensor([[-4.6949]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19597691099565495, distance: 1.0261029909404602 entropy 1.7646539211273193
LOSS epoch 5 actor 562.3173533665067 critic 1935.3400221107408 
epoch: 6, step: 0
	action: tensor([[ 1.6536,  1.8217,  1.9990,  0.3309, -1.4352,  0.7511, -0.3501]],
       dtype=torch.float64)
	q_value: tensor([[-7.1805]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 6, step: 1
	action: tensor([[-1.7013, -0.3315,  3.1030, -1.0988,  1.0240,  1.6656, -0.8342]],
       dtype=torch.float64)
	q_value: tensor([[-7.8255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 6, step: 2
	action: tensor([[ 2.1145, -2.4205, -0.1399, -1.2336,  1.5314, -1.9270,  0.1353]],
       dtype=torch.float64)
	q_value: tensor([[-7.8255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 6, step: 3
	action: tensor([[ 0.3146, -1.3554, -0.2272, -0.6181, -0.2821, -0.0300,  2.8362]],
       dtype=torch.float64)
	q_value: tensor([[-7.8255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9191714804652944, distance: 1.5853077542126082 entropy 1.6592931747436523
epoch: 6, step: 4
	action: tensor([[ 2.3805,  1.5472, -1.5648, -1.0786, -0.7836,  1.1534, -0.9813]],
       dtype=torch.float64)
	q_value: tensor([[-9.3128]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 6, step: 5
	action: tensor([[ 0.2814,  1.6033, -0.9654, -0.9146,  0.4034, -2.4666, -0.2648]],
       dtype=torch.float64)
	q_value: tensor([[-7.8255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 6, step: 6
	action: tensor([[-1.2179, -0.8137,  1.3300,  0.8876, -2.8660, -0.5102,  0.7528]],
       dtype=torch.float64)
	q_value: tensor([[-7.8255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3137535906268014, distance: 1.3116369760849138 entropy 1.6592931747436523
epoch: 6, step: 7
	action: tensor([[-0.0709, -1.7929,  1.0139, -1.0603, -0.3468, -0.0823, -0.2242]],
       dtype=torch.float64)
	q_value: tensor([[-10.1278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 6, step: 8
	action: tensor([[-0.5973, -0.2691,  0.9147,  0.8305,  2.6035,  0.5126, -2.3566]],
       dtype=torch.float64)
	q_value: tensor([[-7.8255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0020869682785449317, distance: 1.1431495252552728 entropy 1.6592931747436523
epoch: 6, step: 9
	action: tensor([[ 1.9996, -0.4759, -1.0892, -0.7902, -0.5153,  0.7608, -0.2492]],
       dtype=torch.float64)
	q_value: tensor([[-8.7728]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3474965538225696, distance: 1.328374483895403 entropy 1.6592931747436523
epoch: 6, step: 10
	action: tensor([[-0.3192, -0.8629,  1.2905,  1.0449, -0.3456,  0.7153, -1.1904]],
       dtype=torch.float64)
	q_value: tensor([[-5.6761]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5417059161064847, distance: 0.77469154976093 entropy 1.6592931747436523
epoch: 6, step: 11
	action: tensor([[ 1.1727,  1.5779, -1.1505,  0.8981,  2.8228, -2.1462,  3.1956]],
       dtype=torch.float64)
	q_value: tensor([[-6.1390]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 6, step: 12
	action: tensor([[ 0.2168,  0.3167, -0.0780,  0.9674,  1.5426, -1.9350,  4.1874]],
       dtype=torch.float64)
	q_value: tensor([[-7.8255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 6, step: 13
	action: tensor([[ 0.1615,  0.5228, -3.0989,  1.0277,  1.5538, -0.2647,  0.1271]],
       dtype=torch.float64)
	q_value: tensor([[-7.8255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 6, step: 14
	action: tensor([[-0.5071, -0.8180, -1.9251, -0.7226, -1.6354, -2.0389,  0.7942]],
       dtype=torch.float64)
	q_value: tensor([[-7.8255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.030082608157707913, distance: 1.127000390934573 entropy 1.6592931747436523
epoch: 6, step: 15
	action: tensor([[-2.0566, -1.5922, -0.6534,  0.3266, -0.1509, -1.3139,  0.5554]],
       dtype=torch.float64)
	q_value: tensor([[-8.7954]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 6, step: 16
	action: tensor([[ 0.9258, -0.5601, -1.0258, -1.3506,  1.3797,  0.3066, -0.8189]],
       dtype=torch.float64)
	q_value: tensor([[-7.8255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12955793796163828, distance: 1.2162166629475186 entropy 1.6592931747436523
epoch: 6, step: 17
	action: tensor([[-0.0968,  0.7654, -1.1998, -0.4568,  1.2947,  1.6321,  1.8254]],
       dtype=torch.float64)
	q_value: tensor([[-6.6984]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 6, step: 18
	action: tensor([[-0.1252, -2.0812,  0.5227,  0.9146,  1.2356, -0.9573, -0.0711]],
       dtype=torch.float64)
	q_value: tensor([[-7.8255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 6, step: 19
	action: tensor([[-0.3273, -2.0817,  0.2184, -0.6809, -0.5299, -0.0714,  1.3715]],
       dtype=torch.float64)
	q_value: tensor([[-7.8255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 6, step: 20
	action: tensor([[-0.3764, -0.0640,  0.3217, -1.4348, -0.4376, -1.5934, -0.6390]],
       dtype=torch.float64)
	q_value: tensor([[-7.8255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.033383833657128825, distance: 1.1632887412100505 entropy 1.6592931747436523
epoch: 6, step: 21
	action: tensor([[-0.3314, -0.3197, -0.5495,  0.0177,  0.9957, -1.4194, -2.7505]],
       dtype=torch.float64)
	q_value: tensor([[-5.5437]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4348119093507874, distance: 1.3707371386081906 entropy 1.6592931747436523
epoch: 6, step: 22
	action: tensor([[ 0.1121,  3.0308, -3.5784, -0.5720,  1.5108,  1.7853,  1.0001]],
       dtype=torch.float64)
	q_value: tensor([[-7.3999]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 6, step: 23
	action: tensor([[ 0.8281, -1.8097,  1.0171,  1.2691, -1.0034,  2.1199,  1.7509]],
       dtype=torch.float64)
	q_value: tensor([[-7.8255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 6, step: 24
	action: tensor([[ 0.1111, -1.3271, -0.7140,  1.0698,  0.5674,  3.2130,  0.1912]],
       dtype=torch.float64)
	q_value: tensor([[-7.8255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 6, step: 25
	action: tensor([[ 1.0792, -0.5453, -0.2309, -0.3695, -2.7631,  1.3239, -2.8927]],
       dtype=torch.float64)
	q_value: tensor([[-7.8255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17094272851780268, distance: 1.2382961431942612 entropy 1.6592931747436523
epoch: 6, step: 26
	action: tensor([[ 0.3976,  0.4575, -0.2104,  0.5699, -1.0116,  2.5590,  0.1133]],
       dtype=torch.float64)
	q_value: tensor([[-8.7861]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 6, step: 27
	action: tensor([[ 0.4144, -0.8035, -0.2272, -0.1834, -1.5123,  0.8641, -1.4893]],
       dtype=torch.float64)
	q_value: tensor([[-7.8255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.34972163987653637, distance: 1.329470786646342 entropy 1.6592931747436523
epoch: 6, step: 28
	action: tensor([[ 0.4848,  0.1120,  0.1222, -1.8322, -0.4034, -1.6518,  0.8012]],
       dtype=torch.float64)
	q_value: tensor([[-5.7786]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3061534791966929, distance: 0.9532095849693653 entropy 1.6592931747436523
epoch: 6, step: 29
	action: tensor([[ 0.5557,  1.3235,  1.4957,  0.3760, -2.0736,  0.2903,  0.5793]],
       dtype=torch.float64)
	q_value: tensor([[-6.5312]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 6, step: 30
	action: tensor([[ 1.7721, -0.5687,  1.5366, -0.2567, -0.3742, -1.2926,  0.7978]],
       dtype=torch.float64)
	q_value: tensor([[-7.8255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2517530919865498, distance: 0.9898722711121944 entropy 1.6592931747436523
epoch: 6, step: 31
	action: tensor([[-1.8607,  1.4143, -0.4228,  0.8954,  0.6800, -0.2073,  1.5800]],
       dtype=torch.float64)
	q_value: tensor([[-5.9117]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 6, step: 32
	action: tensor([[ 0.4625, -2.4741,  1.2367,  0.3920,  0.6931,  0.9173, -0.5952]],
       dtype=torch.float64)
	q_value: tensor([[-7.8255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 6, step: 33
	action: tensor([[-0.1124,  2.5261, -0.8807, -2.1830, -1.8999, -2.4560, -1.0700]],
       dtype=torch.float64)
	q_value: tensor([[-7.8255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 6, step: 34
	action: tensor([[ 0.3075,  0.5870,  0.3325, -1.3128,  0.1990, -1.1277, -1.1839]],
       dtype=torch.float64)
	q_value: tensor([[-7.8255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22451408604377854, distance: 1.0077287526489147 entropy 1.6592931747436523
epoch: 6, step: 35
	action: tensor([[ 0.2375, -1.0413, -0.7856,  1.3777, -1.4108,  0.8936, -0.5246]],
       dtype=torch.float64)
	q_value: tensor([[-5.2174]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.22240276773189271, distance: 1.2652136115657837 entropy 1.6592931747436523
epoch: 6, step: 36
	action: tensor([[-2.4780,  1.2430, -0.8041, -2.6060, -1.4401,  1.1188,  1.3869]],
       dtype=torch.float64)
	q_value: tensor([[-5.8496]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 6, step: 37
	action: tensor([[-0.8345, -2.4779, -0.4699,  2.9156, -0.2359, -1.6470, -0.1883]],
       dtype=torch.float64)
	q_value: tensor([[-7.8255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 6, step: 38
	action: tensor([[ 3.9142, -1.2452,  1.4513, -0.2515, -1.2131,  0.7519, -0.4307]],
       dtype=torch.float64)
	q_value: tensor([[-7.8255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 6, step: 39
	action: tensor([[-0.5727,  0.7443, -1.2084,  0.0518,  0.7528,  0.8864, -0.5778]],
       dtype=torch.float64)
	q_value: tensor([[-7.8255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10646888019013478, distance: 1.081711718579176 entropy 1.6592931747436523
epoch: 6, step: 40
	action: tensor([[ 1.2217, -2.7372, -1.1043,  2.4208, -1.6299, -0.8449, -0.6499]],
       dtype=torch.float64)
	q_value: tensor([[-5.5489]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 6, step: 41
	action: tensor([[ 2.1425, -0.1136,  2.3236, -0.3591,  1.2046, -2.2101,  0.0309]],
       dtype=torch.float64)
	q_value: tensor([[-7.8255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 6, step: 42
	action: tensor([[-2.0528,  0.4433,  0.0552,  1.1739, -0.0967, -0.7742,  1.7020]],
       dtype=torch.float64)
	q_value: tensor([[-7.8255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 6, step: 43
	action: tensor([[ 0.9303,  0.3827,  0.9511, -0.5574,  0.1336,  1.9959,  1.1257]],
       dtype=torch.float64)
	q_value: tensor([[-7.8255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 6, step: 44
	action: tensor([[-0.2118, -0.4623, -0.1260, -2.5616, -0.9343,  0.3993,  1.0171]],
       dtype=torch.float64)
	q_value: tensor([[-7.8255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.30755100622721665, distance: 1.308537017174591 entropy 1.6592931747436523
epoch: 6, step: 45
	action: tensor([[ 0.9111, -0.9191,  0.5249, -1.1334, -4.5643,  2.1274, -0.1979]],
       dtype=torch.float64)
	q_value: tensor([[-8.6043]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2020098040282878 entropy 1.6592931747436523
epoch: 6, step: 46
	action: tensor([[ 0.5356, -2.4041,  0.2759, -0.0666, -0.9286,  1.2281,  0.0734]],
       dtype=torch.float64)
	q_value: tensor([[-7.8255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 6, step: 47
	action: tensor([[ 0.2734, -1.3788,  1.7634,  0.5656, -0.0043,  1.6029, -0.5410]],
       dtype=torch.float64)
	q_value: tensor([[-7.8255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14146951839124433, distance: 1.060314139471785 entropy 1.6592931747436523
epoch: 6, step: 48
	action: tensor([[-1.4667,  1.4397, -0.8950, -1.2055,  0.0407, -1.4769,  1.1048]],
       dtype=torch.float64)
	q_value: tensor([[-6.9386]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17006445857697117, distance: 1.0425067483912545 entropy 1.6592931747436523
epoch: 6, step: 49
	action: tensor([[ 1.6919, -0.9982,  0.2840,  0.8099, -1.5530,  0.4090, -0.4549]],
       dtype=torch.float64)
	q_value: tensor([[-8.8140]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 6, step: 50
	action: tensor([[ 0.1696,  0.2828,  0.9731,  0.1183, -0.6135,  1.2231, -1.9000]],
       dtype=torch.float64)
	q_value: tensor([[-7.8255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 6, step: 51
	action: tensor([[ 0.5499,  0.2152,  0.8667,  0.6774,  1.0154,  1.1733, -0.7761]],
       dtype=torch.float64)
	q_value: tensor([[-7.8255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 6, step: 52
	action: tensor([[-0.4399, -2.0812,  1.6863, -0.4430, -1.0422, -0.3396, -0.2719]],
       dtype=torch.float64)
	q_value: tensor([[-7.8255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 6, step: 53
	action: tensor([[-1.4470, -0.7204, -0.8976, -1.3990, -0.2257, -2.3989, -0.1375]],
       dtype=torch.float64)
	q_value: tensor([[-7.8255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1648456931129585, distance: 1.0457793351731348 entropy 1.6592931747436523
epoch: 6, step: 54
	action: tensor([[-0.5590, -1.7078, -2.6038,  1.0648,  0.6172, -1.4046, -0.2175]],
       dtype=torch.float64)
	q_value: tensor([[-7.6497]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 6, step: 55
	action: tensor([[-0.3897, -1.3312,  0.1067, -1.2372, -3.7263,  1.6186, -1.6162]],
       dtype=torch.float64)
	q_value: tensor([[-7.8255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 6, step: 56
	action: tensor([[-0.5147,  0.4963,  1.5042, -2.2299,  0.7367,  0.6068,  0.3802]],
       dtype=torch.float64)
	q_value: tensor([[-7.8255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5881493801156457, distance: 1.4421232146432579 entropy 1.6592931747436523
epoch: 6, step: 57
	action: tensor([[-0.7618,  0.3170,  1.4061,  1.3873, -1.8769, -1.7774, -0.0800]],
       dtype=torch.float64)
	q_value: tensor([[-8.1969]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28627425321415356, distance: 0.9667682410298044 entropy 1.6592931747436523
epoch: 6, step: 58
	action: tensor([[ 2.1816, -1.0831, -1.1200,  0.8145, -1.1971, -0.4448, -0.5465]],
       dtype=torch.float64)
	q_value: tensor([[-8.8326]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.050015945894584246, distance: 1.1153594494832852 entropy 1.6592931747436523
epoch: 6, step: 59
	action: tensor([[ 2.3621, -0.3765,  0.4919,  2.0613, -2.5280,  0.5012,  0.0920]],
       dtype=torch.float64)
	q_value: tensor([[-5.4430]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 6, step: 60
	action: tensor([[ 1.8610, -0.0161,  2.8860,  0.3979,  0.5215, -3.0977,  0.0617]],
       dtype=torch.float64)
	q_value: tensor([[-7.8255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 6, step: 61
	action: tensor([[-0.1067, -0.1567, -0.4440, -0.4658, -0.8457, -0.4747,  0.7251]],
       dtype=torch.float64)
	q_value: tensor([[-7.8255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14693714339423047, distance: 1.0569323958739478 entropy 1.6592931747436523
epoch: 6, step: 62
	action: tensor([[ 0.3611,  0.0374, -1.4262, -1.1736,  0.8347,  1.0423, -1.5514]],
       dtype=torch.float64)
	q_value: tensor([[-5.0760]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.900070850679263, distance: 0.3617452093912902 entropy 1.6592931747436523
epoch: 6, step: 63
	action: tensor([[ 3.1716, -1.5791, -1.1422, -1.0948, -1.1609, -0.6744, -2.4664]],
       dtype=torch.float64)
	q_value: tensor([[-6.8408]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 6, step: 64
	action: tensor([[-1.9178, -1.0297,  0.0289,  0.1699, -1.6108, -0.2839, -0.2625]],
       dtype=torch.float64)
	q_value: tensor([[-7.8255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 6, step: 65
	action: tensor([[1.0959, 0.8776, 0.5495, 0.6912, 2.1906, 1.5785, 0.5423]],
       dtype=torch.float64)
	q_value: tensor([[-7.8255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 6, step: 66
	action: tensor([[-1.5670, -2.6152, -0.6690,  0.8596, -0.8674, -2.0209, -0.4642]],
       dtype=torch.float64)
	q_value: tensor([[-7.8255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 6, step: 67
	action: tensor([[ 2.9861, -0.7488,  0.8206, -1.0574,  0.2130, -0.8016,  1.2985]],
       dtype=torch.float64)
	q_value: tensor([[-7.8255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 6, step: 68
	action: tensor([[ 0.4217, -0.5335,  1.4147,  1.0515, -1.7082,  0.4714, -0.0401]],
       dtype=torch.float64)
	q_value: tensor([[-7.8255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6355424786459951, distance: 0.69084425739408 entropy 1.6592931747436523
epoch: 6, step: 69
	action: tensor([[-0.8235,  0.6936,  0.4033,  2.9160, -0.0416, -1.0984, -0.3233]],
       dtype=torch.float64)
	q_value: tensor([[-6.9802]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 6, step: 70
	action: tensor([[ 1.5966,  0.7368, -0.8036, -2.3598,  1.1656, -0.0207, -0.9289]],
       dtype=torch.float64)
	q_value: tensor([[-7.8255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 6, step: 71
	action: tensor([[-0.5890,  1.5432, -0.0485, -0.1105, -0.5708,  0.5074, -0.5721]],
       dtype=torch.float64)
	q_value: tensor([[-7.8255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 6, step: 72
	action: tensor([[-1.7677, -0.5676, -0.0394,  0.9773, -1.7182,  0.2729,  1.3649]],
       dtype=torch.float64)
	q_value: tensor([[-7.8255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 6, step: 73
	action: tensor([[ 1.0517,  1.2963, -0.1507,  1.6183,  0.2482, -0.0707, -0.3516]],
       dtype=torch.float64)
	q_value: tensor([[-7.8255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 6, step: 74
	action: tensor([[-1.6385, -1.3895, -0.2723, -1.4581, -1.3554, -0.5602,  0.9455]],
       dtype=torch.float64)
	q_value: tensor([[-7.8255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.02017158744586045, distance: 1.132743836174043 entropy 1.6592931747436523
epoch: 6, step: 75
	action: tensor([[-3.0677,  1.5914,  2.5338, -0.7924,  0.4239,  0.9764,  0.8557]],
       dtype=torch.float64)
	q_value: tensor([[-8.9115]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 6, step: 76
	action: tensor([[ 2.9250, -0.2045,  0.2921,  1.0947, -0.0887, -0.5970,  0.6730]],
       dtype=torch.float64)
	q_value: tensor([[-7.8255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 6, step: 77
	action: tensor([[ 0.9719,  1.1983,  0.6017,  2.5759,  0.7878, -0.1357,  0.6097]],
       dtype=torch.float64)
	q_value: tensor([[-7.8255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 6, step: 78
	action: tensor([[ 0.8469,  0.1042, -0.6365, -0.1648, -1.0408, -2.5006, -0.2012]],
       dtype=torch.float64)
	q_value: tensor([[-7.8255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 6, step: 79
	action: tensor([[-0.4187, -1.1566, -0.4122,  0.4312, -0.2554, -0.5145,  0.2627]],
       dtype=torch.float64)
	q_value: tensor([[-7.8255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.856745005456993, distance: 1.5593112975163548 entropy 1.6592931747436523
epoch: 6, step: 80
	action: tensor([[ 0.3733, -1.2887, -1.2269,  0.1639, -1.1055, -2.0337,  0.6859]],
       dtype=torch.float64)
	q_value: tensor([[-4.2607]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 6, step: 81
	action: tensor([[-0.1855,  1.1831,  0.6165, -2.9371,  0.3565, -0.8029, -0.9692]],
       dtype=torch.float64)
	q_value: tensor([[-7.8255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 6, step: 82
	action: tensor([[ 1.0615,  1.0380, -0.4068,  1.4411,  0.8410, -0.4678,  1.0483]],
       dtype=torch.float64)
	q_value: tensor([[-7.8255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 6, step: 83
	action: tensor([[ 0.2182, -3.0484,  0.1297,  1.5315, -1.0679,  0.7091, -0.6347]],
       dtype=torch.float64)
	q_value: tensor([[-7.8255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 6, step: 84
	action: tensor([[ 2.2159,  0.9423, -1.9088, -0.0044, -0.6203, -1.2103,  1.3707]],
       dtype=torch.float64)
	q_value: tensor([[-7.8255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 6, step: 85
	action: tensor([[-1.2890,  0.1048, -0.9726,  0.3801,  0.7361,  2.0623,  0.1488]],
       dtype=torch.float64)
	q_value: tensor([[-7.8255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2064164852818311, distance: 1.2569133088651987 entropy 1.6592931747436523
epoch: 6, step: 86
	action: tensor([[-1.0355, -0.0536,  0.4915, -2.1191,  1.0922, -0.7613, -1.1410]],
       dtype=torch.float64)
	q_value: tensor([[-7.7341]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5294433615334533, distance: 1.4152181596276754 entropy 1.6592931747436523
epoch: 6, step: 87
	action: tensor([[ 1.9859, -0.4422, -2.2784,  0.6649,  1.4063, -0.8901, -2.0867]],
       dtype=torch.float64)
	q_value: tensor([[-7.6272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.25571185135544683, distance: 1.2823355721918825 entropy 1.6592931747436523
epoch: 6, step: 88
	action: tensor([[-0.6989, -1.8719, -1.0338, -0.0824, -0.9009, -0.7596,  1.3480]],
       dtype=torch.float64)
	q_value: tensor([[-6.8911]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 6, step: 89
	action: tensor([[ 1.9012, -0.1253, -0.2637, -0.1742, -1.8953,  0.4997, -0.6303]],
       dtype=torch.float64)
	q_value: tensor([[-7.8255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 6, step: 90
	action: tensor([[-2.8567, -0.4378, -1.0622,  0.4238, -0.8268,  1.6734, -1.3670]],
       dtype=torch.float64)
	q_value: tensor([[-7.8255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 6, step: 91
	action: tensor([[ 0.7031,  0.9265, -1.6849,  0.3216, -0.5244,  0.6024, -0.7259]],
       dtype=torch.float64)
	q_value: tensor([[-7.8255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 6, step: 92
	action: tensor([[-0.0067, -1.0504,  0.2315, -1.0719, -0.5447,  0.1848,  1.0151]],
       dtype=torch.float64)
	q_value: tensor([[-7.8255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0189406246119592, distance: 1.6259922329166074 entropy 1.6592931747436523
epoch: 6, step: 93
	action: tensor([[ 2.6177, -1.3897,  0.2550, -0.5004, -0.8522,  1.6448, -0.2857]],
       dtype=torch.float64)
	q_value: tensor([[-6.3818]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 6, step: 94
	action: tensor([[ 1.7593, -1.1101,  0.3990, -1.0857, -0.1952, -2.1427,  0.1317]],
       dtype=torch.float64)
	q_value: tensor([[-7.8255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.017692158505620448, distance: 1.1341761192723465 entropy 1.6592931747436523
epoch: 6, step: 95
	action: tensor([[ 0.2972, -1.5443, -0.4860,  0.0871,  1.4214,  0.0631, -1.6705]],
       dtype=torch.float64)
	q_value: tensor([[-6.3408]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 6, step: 96
	action: tensor([[-1.1903,  0.5663,  0.1373, -1.7825, -2.6528, -2.4722,  0.3210]],
       dtype=torch.float64)
	q_value: tensor([[-7.8255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24672200130676036, distance: 1.2777371002542908 entropy 1.6592931747436523
epoch: 6, step: 97
	action: tensor([[ 1.8288, -1.5191,  0.4271, -0.3655,  0.3319,  2.0506,  0.2603]],
       dtype=torch.float64)
	q_value: tensor([[-10.4914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1700163890778199, distance: 1.237806234728399 entropy 1.6592931747436523
epoch: 6, step: 98
	action: tensor([[-0.4268,  2.2953,  2.2157,  0.6639,  0.7008, -1.3794,  0.5905]],
       dtype=torch.float64)
	q_value: tensor([[-7.3825]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 6, step: 99
	action: tensor([[-1.1494, -0.3689,  1.9495,  0.0136, -1.8660, -0.4092, -1.6631]],
       dtype=torch.float64)
	q_value: tensor([[-7.8255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.4015823086882513, distance: 1.773394801738218 entropy 1.6592931747436523
epoch: 6, step: 100
	action: tensor([[-1.4461, -0.3248, -1.3621, -2.2625,  0.5771, -0.0560,  2.4336]],
       dtype=torch.float64)
	q_value: tensor([[-8.7050]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 6, step: 101
	action: tensor([[-0.4905, -0.9723,  0.7454, -1.1683,  1.2566, -1.1985, -0.1373]],
       dtype=torch.float64)
	q_value: tensor([[-7.8255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.922109392366564, distance: 1.5865207029977224 entropy 1.6592931747436523
epoch: 6, step: 102
	action: tensor([[ 0.6727,  1.8518, -0.1470,  1.1838,  0.9672, -1.8672,  1.0768]],
       dtype=torch.float64)
	q_value: tensor([[-5.7944]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 6, step: 103
	action: tensor([[-2.0237, -2.1994,  0.6381, -0.4208,  0.5468,  0.0105, -0.5143]],
       dtype=torch.float64)
	q_value: tensor([[-7.8255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 6, step: 104
	action: tensor([[-1.1354, -1.4361, -1.2509, -2.2649,  0.8079, -0.9206,  1.4160]],
       dtype=torch.float64)
	q_value: tensor([[-7.8255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5999853209997995, distance: 0.7237601337216863 entropy 1.6592931747436523
epoch: 6, step: 105
	action: tensor([[ 1.2673,  2.1781, -0.0690, -1.2019, -0.6211, -0.7703,  2.2033]],
       dtype=torch.float64)
	q_value: tensor([[-9.6092]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7936637162119842, distance: 0.5198098388612625 entropy 1.6592931747436523
epoch: 6, step: 106
	action: tensor([[-0.1592, -1.6000, -0.1296, -2.4653, -0.4024,  1.0926,  1.2619]],
       dtype=torch.float64)
	q_value: tensor([[-9.1674]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4446218798632928, distance: 1.3754150983412998 entropy 1.6592931747436523
epoch: 6, step: 107
	action: tensor([[-0.1164,  1.2774,  0.6561,  0.3412, -1.8472, -0.5544,  0.4804]],
       dtype=torch.float64)
	q_value: tensor([[-9.6769]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 6, step: 108
	action: tensor([[ 0.9523,  1.7156, -0.5503,  1.6763,  1.8696, -0.3069, -1.6860]],
       dtype=torch.float64)
	q_value: tensor([[-7.8255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 6, step: 109
	action: tensor([[-0.7475, -0.6317, -2.3756, -1.0755, -0.3750, -0.7545, -1.7351]],
       dtype=torch.float64)
	q_value: tensor([[-7.8255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2617564529151498, distance: 0.9832331738627867 entropy 1.6592931747436523
epoch: 6, step: 110
	action: tensor([[ 1.5161, -0.0932, -1.1949,  0.4806,  3.3219,  0.5600, -0.7148]],
       dtype=torch.float64)
	q_value: tensor([[-7.9891]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7886601645624101 entropy 1.6592931747436523
epoch: 6, step: 111
	action: tensor([[-0.0668,  1.4717,  1.2348,  1.3283,  0.4330, -1.6044, -1.0462]],
       dtype=torch.float64)
	q_value: tensor([[-7.8255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 6, step: 112
	action: tensor([[ 0.7875,  0.5195,  0.4331,  1.6550,  3.1464, -0.6804, -0.6378]],
       dtype=torch.float64)
	q_value: tensor([[-7.8255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 6, step: 113
	action: tensor([[-0.2678,  0.8078, -1.3557, -1.4605, -0.0600,  0.4103,  0.3567]],
       dtype=torch.float64)
	q_value: tensor([[-7.8255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 6, step: 114
	action: tensor([[ 1.6843,  0.7446,  0.2450, -1.7333,  2.0726,  1.0326, -1.4628]],
       dtype=torch.float64)
	q_value: tensor([[-7.8255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 6, step: 115
	action: tensor([[ 0.0402, -0.1383, -1.6621,  0.0893,  0.3980,  0.5958, -2.0265]],
       dtype=torch.float64)
	q_value: tensor([[-7.8255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08539745106122099, distance: 1.0943919679249914 entropy 1.6592931747436523
epoch: 6, step: 116
	action: tensor([[-2.5074,  0.1860, -0.0333, -2.5165,  0.1838,  0.7427,  1.0693]],
       dtype=torch.float64)
	q_value: tensor([[-5.6085]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 6, step: 117
	action: tensor([[-1.7132,  2.3358, -1.2129, -0.2808, -0.7974, -0.1900,  0.3436]],
       dtype=torch.float64)
	q_value: tensor([[-7.8255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 6, step: 118
	action: tensor([[ 1.9268, -0.8380,  1.2510,  0.6119,  0.3854, -2.4435,  0.4205]],
       dtype=torch.float64)
	q_value: tensor([[-7.8255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20128128968062964, distance: 1.0227126370760666 entropy 1.6592931747436523
epoch: 6, step: 119
	action: tensor([[-1.3637,  1.8314,  1.3800,  1.3964, -1.5025, -1.3779,  2.4205]],
       dtype=torch.float64)
	q_value: tensor([[-6.3829]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 6, step: 120
	action: tensor([[ 0.4833,  1.3447, -0.2865,  1.9057, -0.4407, -2.3321,  0.4746]],
       dtype=torch.float64)
	q_value: tensor([[-7.8255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 6, step: 121
	action: tensor([[-1.9138, -0.3711, -1.3834, -1.7692, -0.7231,  0.0970, -0.4289]],
       dtype=torch.float64)
	q_value: tensor([[-7.8255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 6, step: 122
	action: tensor([[ 1.1609, -1.2541,  0.2355,  0.5758, -0.0745,  0.0283,  2.6743]],
       dtype=torch.float64)
	q_value: tensor([[-7.8255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1937052718857737, distance: 1.2502741418959467 entropy 1.6592931747436523
epoch: 6, step: 123
	action: tensor([[-0.8606,  2.9734,  2.5763,  0.5674, -1.0218, -0.4246, -0.6200]],
       dtype=torch.float64)
	q_value: tensor([[-8.3364]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 6, step: 124
	action: tensor([[ 0.0468,  0.5723, -0.8156, -0.1878, -0.5099,  1.1570,  0.4679]],
       dtype=torch.float64)
	q_value: tensor([[-7.8255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 6, step: 125
	action: tensor([[ 0.2626,  0.2646,  0.7287, -0.0908,  1.1342,  0.6790, -1.8884]],
       dtype=torch.float64)
	q_value: tensor([[-7.8255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7154938131693852, distance: 0.6103831705083558 entropy 1.6592931747436523
epoch: 6, step: 126
	action: tensor([[-1.0049,  0.1640, -0.9722, -1.6102, -0.9566, -0.1201,  1.9319]],
       dtype=torch.float64)
	q_value: tensor([[-6.0512]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07430019274819355, distance: 1.1010113092152722 entropy 1.6592931747436523
epoch: 6, step: 127
	action: tensor([[-2.2992, -0.5056,  1.8244,  0.9823, -0.0342, -0.2433, -1.5253]],
       dtype=torch.float64)
	q_value: tensor([[-9.8225]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
LOSS epoch 6 actor 419.79616712510597 critic 1691.843935400575 
epoch: 7, step: 0
	action: tensor([[ 2.3972, -0.6353, -0.6561,  0.4967,  0.0603, -0.8050,  1.5848]],
       dtype=torch.float64)
	q_value: tensor([[-13.1610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 7, step: 1
	action: tensor([[ 0.5036, -1.2566, -1.2903, -2.4221, -0.0039, -0.9225, -0.8145]],
       dtype=torch.float64)
	q_value: tensor([[-13.1610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.46414794203986665, distance: 0.8376819253112938 entropy 1.6592931747436523
epoch: 7, step: 2
	action: tensor([[-0.1757, -0.1544,  0.9012,  2.3027, -1.7274,  0.2474, -1.8991]],
       dtype=torch.float64)
	q_value: tensor([[-12.5490]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 7, step: 3
	action: tensor([[-0.9714, -0.5019,  1.4069,  0.5170, -2.9198,  0.8212, -0.3425]],
       dtype=torch.float64)
	q_value: tensor([[-13.1610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9952382862761999, distance: 1.6164194894922739 entropy 1.6592931747436523
epoch: 7, step: 4
	action: tensor([[ 0.4303, -0.8994,  1.5421, -1.8072,  1.0873,  0.4770,  0.3488]],
       dtype=torch.float64)
	q_value: tensor([[-16.7319]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.36787932799213996, distance: 1.3383835387406204 entropy 1.6592931747436523
epoch: 7, step: 5
	action: tensor([[-1.9475,  0.8235, -2.4632, -0.0989, -1.1195,  1.8047,  1.8176]],
       dtype=torch.float64)
	q_value: tensor([[-11.1656]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5509817270005815, distance: 1.4251482171766943 entropy 1.6592931747436523
epoch: 7, step: 6
	action: tensor([[-1.8496,  1.0005,  0.0639, -0.4876,  0.8602,  0.1416, -0.3889]],
       dtype=torch.float64)
	q_value: tensor([[-20.3450]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 7, step: 7
	action: tensor([[-0.7915,  0.3308,  0.7083, -0.5982, -0.6764, -0.8835, -0.9453]],
       dtype=torch.float64)
	q_value: tensor([[-13.1610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7997355847348311, distance: 1.5351861551348553 entropy 1.6592931747436523
epoch: 7, step: 8
	action: tensor([[-0.3114,  0.4735, -2.1516, -0.7973, -0.1594,  1.3399, -0.9875]],
       dtype=torch.float64)
	q_value: tensor([[-8.8312]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 7, step: 9
	action: tensor([[-0.4813, -0.6390,  0.0678,  1.9360,  0.0596,  0.2266,  1.0529]],
       dtype=torch.float64)
	q_value: tensor([[-13.1610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 7, step: 10
	action: tensor([[ 0.5036, -0.1356,  0.4629, -0.3752,  0.8273, -1.6118,  0.3475]],
       dtype=torch.float64)
	q_value: tensor([[-13.1610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2700964115901242, distance: 0.9776635924717277 entropy 1.6592931747436523
epoch: 7, step: 11
	action: tensor([[ 2.8323, -2.4995,  0.9032, -0.4092, -0.7807,  0.2503,  0.3646]],
       dtype=torch.float64)
	q_value: tensor([[-7.5867]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 7, step: 12
	action: tensor([[ 2.0997, -0.8701, -0.3975,  0.6235,  0.3936, -1.4713, -1.2895]],
       dtype=torch.float64)
	q_value: tensor([[-13.1610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 7, step: 13
	action: tensor([[ 2.2235, -2.5898, -2.5428,  1.2004, -0.5742,  0.3408, -0.1701]],
       dtype=torch.float64)
	q_value: tensor([[-13.1610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 7, step: 14
	action: tensor([[-2.6632, -0.5677, -1.1007,  0.9816,  0.5271,  1.0563,  1.2498]],
       dtype=torch.float64)
	q_value: tensor([[-13.1610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 7, step: 15
	action: tensor([[-0.8878, -0.7878,  0.8293, -0.2567,  2.0440,  0.4768, -1.3706]],
       dtype=torch.float64)
	q_value: tensor([[-13.1610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.3625504163059001, distance: 1.7589246521396134 entropy 1.6592931747436523
epoch: 7, step: 16
	action: tensor([[-0.5572,  1.0446,  0.3511,  1.5069,  1.2268, -1.1770, -2.0918]],
       dtype=torch.float64)
	q_value: tensor([[-12.1663]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 7, step: 17
	action: tensor([[-1.5200,  0.1999, -0.6712, -0.3879, -0.3867,  2.9491, -1.0463]],
       dtype=torch.float64)
	q_value: tensor([[-13.1610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 7, step: 18
	action: tensor([[-1.1310,  0.2805, -2.2956,  0.5949,  2.3057, -0.9720, -0.4251]],
       dtype=torch.float64)
	q_value: tensor([[-13.1610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1391422038774284, distance: 1.6736957808827384 entropy 1.6592931747436523
epoch: 7, step: 19
	action: tensor([[ 2.4477,  0.8287, -0.3303, -3.5807, -1.6953,  0.8868,  0.6699]],
       dtype=torch.float64)
	q_value: tensor([[-13.1535]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 7, step: 20
	action: tensor([[-0.5316, -0.7419, -1.2637, -0.9828,  0.1134,  0.0860,  0.3558]],
       dtype=torch.float64)
	q_value: tensor([[-13.1610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0730171097744916, distance: 1.1853866089384621 entropy 1.6592931747436523
epoch: 7, step: 21
	action: tensor([[-0.8303, -1.5055, -1.2893,  1.0452,  0.6170,  0.6033,  0.0882]],
       dtype=torch.float64)
	q_value: tensor([[-10.0822]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 7, step: 22
	action: tensor([[ 3.1047, -2.2507, -1.1192,  1.0525,  2.5500, -2.2756,  0.9294]],
       dtype=torch.float64)
	q_value: tensor([[-13.1610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 7, step: 23
	action: tensor([[ 2.9108,  0.7059, -1.0141, -2.6242,  1.0410,  1.0726, -0.1182]],
       dtype=torch.float64)
	q_value: tensor([[-13.1610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 7, step: 24
	action: tensor([[-0.0316, -0.4281,  1.6306,  2.2619,  0.3958, -2.3420,  2.0398]],
       dtype=torch.float64)
	q_value: tensor([[-13.1610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6157988581624458, distance: 0.7093098958398047 entropy 1.6592931747436523
epoch: 7, step: 25
	action: tensor([[-0.5916, -1.1584, -0.8000,  1.2410,  0.6958,  3.0200, -0.0580]],
       dtype=torch.float64)
	q_value: tensor([[-14.7414]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10682979479891286, distance: 1.0814932343069832 entropy 1.6592931747436523
epoch: 7, step: 26
	action: tensor([[-2.6359,  1.3520, -0.2836,  0.0343, -0.4326, -0.0789, -0.4133]],
       dtype=torch.float64)
	q_value: tensor([[-11.0943]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 7, step: 27
	action: tensor([[ 1.9260,  0.8396,  0.7232, -0.3875,  0.4715, -1.2982, -0.9795]],
       dtype=torch.float64)
	q_value: tensor([[-13.1610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 7, step: 28
	action: tensor([[ 2.9363, -0.7828, -2.8446,  0.7636, -1.7948,  1.4222,  0.2611]],
       dtype=torch.float64)
	q_value: tensor([[-13.1610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 7, step: 29
	action: tensor([[ 1.4196,  2.0631,  2.7900, -0.7939, -2.1287,  4.8257,  0.0418]],
       dtype=torch.float64)
	q_value: tensor([[-13.1610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 7, step: 30
	action: tensor([[-2.5102, -0.5929,  2.1569, -0.7122, -1.4670,  0.1112,  0.0136]],
       dtype=torch.float64)
	q_value: tensor([[-13.1610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 7, step: 31
	action: tensor([[-1.0149,  0.3701,  0.5152,  1.6454, -0.1441, -2.8146, -1.9547]],
       dtype=torch.float64)
	q_value: tensor([[-13.1610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 7, step: 32
	action: tensor([[ 0.4939, -2.0514, -1.0770,  0.6163, -0.2206,  1.8800,  1.9911]],
       dtype=torch.float64)
	q_value: tensor([[-13.1610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 7, step: 33
	action: tensor([[-0.7536, -0.2957, -0.5614,  0.5733,  1.8478,  0.5545,  2.0689]],
       dtype=torch.float64)
	q_value: tensor([[-13.1610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4771200283930741, distance: 1.3907996947008525 entropy 1.6592931747436523
epoch: 7, step: 34
	action: tensor([[-2.1125,  1.1637, -0.3189,  0.7303,  0.2992, -2.2263,  2.7600]],
       dtype=torch.float64)
	q_value: tensor([[-12.7497]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 7, step: 35
	action: tensor([[ 0.5051, -0.9381,  2.5361, -0.6180, -0.2386, -1.1249,  0.1930]],
       dtype=torch.float64)
	q_value: tensor([[-13.1610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 7, step: 36
	action: tensor([[ 0.2520, -1.6332,  2.2379,  0.8905,  0.8861,  1.4898, -0.8954]],
       dtype=torch.float64)
	q_value: tensor([[-13.1610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.38675149552750465, distance: 1.3475845237988708 entropy 1.6592931747436523
epoch: 7, step: 37
	action: tensor([[-0.5209,  0.0046, -0.9488, -1.5426, -0.9717, -0.6481, -0.0127]],
       dtype=torch.float64)
	q_value: tensor([[-11.9963]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 7, step: 38
	action: tensor([[-0.5566,  0.7406,  0.0107,  2.0129,  2.5495,  0.3842,  1.6081]],
       dtype=torch.float64)
	q_value: tensor([[-13.1610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 7, step: 39
	action: tensor([[ 1.8200, -0.8877, -0.3650, -0.0989,  0.3851, -0.7061,  2.0751]],
       dtype=torch.float64)
	q_value: tensor([[-13.1610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 7, step: 40
	action: tensor([[-0.1691, -1.0468,  0.7202,  1.9737, -2.3236, -1.3826, -2.1138]],
       dtype=torch.float64)
	q_value: tensor([[-13.1610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7058323811841728, distance: 0.6206605269330181 entropy 1.6592931747436523
epoch: 7, step: 41
	action: tensor([[-0.4816, -1.2399, -1.0004, -1.7242,  1.7277,  0.5929,  3.1682]],
       dtype=torch.float64)
	q_value: tensor([[-15.4555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8120924744662512 entropy 1.6592931747436523
epoch: 7, step: 42
	action: tensor([[ 1.5975, -1.7778,  0.4366,  0.0634,  0.9074, -0.3474, -1.1018]],
       dtype=torch.float64)
	q_value: tensor([[-13.1610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 7, step: 43
	action: tensor([[ 0.2027,  1.0637,  0.4556, -0.0188, -1.3378, -0.1536,  2.0875]],
       dtype=torch.float64)
	q_value: tensor([[-13.1610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 7, step: 44
	action: tensor([[ 0.0937,  0.7846, -1.7294,  1.3648,  0.6710, -0.0705,  0.7447]],
       dtype=torch.float64)
	q_value: tensor([[-13.1610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 7, step: 45
	action: tensor([[ 2.8044,  1.8059,  0.1463, -0.7141,  0.4421, -0.8464, -0.1231]],
       dtype=torch.float64)
	q_value: tensor([[-13.1610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 7, step: 46
	action: tensor([[-0.5506, -0.7593, -1.6071, -0.6067, -1.2114,  2.3192, -0.1816]],
       dtype=torch.float64)
	q_value: tensor([[-13.1610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.33369254538635196, distance: 1.321552909373256 entropy 1.6592931747436523
epoch: 7, step: 47
	action: tensor([[ 1.4234,  1.1640,  0.7287,  2.6117,  2.1812, -0.3825,  1.1737]],
       dtype=torch.float64)
	q_value: tensor([[-14.8089]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 7, step: 48
	action: tensor([[-0.0968, -0.7833, -1.5009,  0.2794,  0.1096,  1.1473, -1.2123]],
       dtype=torch.float64)
	q_value: tensor([[-13.1610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5184034527594574, distance: 1.4101012078441064 entropy 1.6592931747436523
epoch: 7, step: 49
	action: tensor([[ 1.6635, -1.1013, -0.5156, -1.2416, -0.1856, -0.7799, -0.0391]],
       dtype=torch.float64)
	q_value: tensor([[-8.7439]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8090205213280297, distance: 1.5391411163713729 entropy 1.6592931747436523
epoch: 7, step: 50
	action: tensor([[-0.6107,  1.8005, -0.7206,  0.3256,  1.5603, -0.0654, -0.6012]],
       dtype=torch.float64)
	q_value: tensor([[-8.8720]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 7, step: 51
	action: tensor([[-0.9283, -0.2971,  0.1145, -1.7915,  0.1030, -0.4523, -1.5462]],
       dtype=torch.float64)
	q_value: tensor([[-13.1610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4925206335647847, distance: 1.3980312047126606 entropy 1.6592931747436523
epoch: 7, step: 52
	action: tensor([[-0.6242,  1.7571,  1.1240,  0.6983,  0.8119,  1.0987,  1.2337]],
       dtype=torch.float64)
	q_value: tensor([[-11.3800]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 7, step: 53
	action: tensor([[-2.1141,  0.4086, -0.2463, -0.0755, -1.9314,  0.4555,  0.4732]],
       dtype=torch.float64)
	q_value: tensor([[-13.1610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 7, step: 54
	action: tensor([[ 2.3925,  0.3695, -0.1318, -0.3072, -0.8065,  0.3629,  0.9876]],
       dtype=torch.float64)
	q_value: tensor([[-13.1610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 7, step: 55
	action: tensor([[-0.9046, -0.6691, -0.5154,  0.5505, -0.2892, -0.8254, -0.1581]],
       dtype=torch.float64)
	q_value: tensor([[-13.1610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0180177526026664, distance: 1.6256205641756516 entropy 1.6592931747436523
epoch: 7, step: 56
	action: tensor([[-1.2855, -1.0847,  1.5471,  0.1196, -0.1907, -0.1129,  1.5430]],
       dtype=torch.float64)
	q_value: tensor([[-7.3072]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.523217554166063, distance: 1.8177495339725354 entropy 1.6592931747436523
epoch: 7, step: 57
	action: tensor([[2.0406, 2.0168, 1.0920, 0.5005, 0.0193, 0.2790, 2.1270]],
       dtype=torch.float64)
	q_value: tensor([[-11.2996]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 7, step: 58
	action: tensor([[-2.2962, -0.7319,  0.2853, -0.8607,  0.4898,  2.4499,  1.8765]],
       dtype=torch.float64)
	q_value: tensor([[-13.1610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 7, step: 59
	action: tensor([[ 2.3534,  1.2701, -0.8320, -0.4789,  1.1758, -1.8773,  1.3390]],
       dtype=torch.float64)
	q_value: tensor([[-13.1610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 7, step: 60
	action: tensor([[ 1.0559,  0.5057, -0.3238, -0.0665,  1.5568,  0.0619, -0.4078]],
       dtype=torch.float64)
	q_value: tensor([[-13.1610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 7, step: 61
	action: tensor([[-1.1795, -0.6734,  2.3939,  0.6208, -0.9207,  0.7718, -0.4919]],
       dtype=torch.float64)
	q_value: tensor([[-13.1610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6671939100015931, distance: 1.4775757365084756 entropy 1.6592931747436523
epoch: 7, step: 62
	action: tensor([[-0.1448,  0.8732, -1.8486, -0.4333,  1.0261, -1.1696,  1.2824]],
       dtype=torch.float64)
	q_value: tensor([[-13.1224]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 7, step: 63
	action: tensor([[-1.8565, -0.3891, -2.1464,  0.1479, -0.1766,  0.2385,  3.5173]],
       dtype=torch.float64)
	q_value: tensor([[-13.1610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 7, step: 64
	action: tensor([[ 2.3883e-01, -8.3961e-01, -5.6721e-01, -4.2434e-04, -1.9762e+00,
          5.1447e-01, -1.7087e-01]], dtype=torch.float64)
	q_value: tensor([[-13.1610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4861392456206983, distance: 1.3950393077405918 entropy 1.6592931747436523
epoch: 7, step: 65
	action: tensor([[-0.0236,  1.0587, -1.5437,  1.3975,  2.2766, -0.1681,  0.5366]],
       dtype=torch.float64)
	q_value: tensor([[-10.6879]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 7, step: 66
	action: tensor([[-0.2918,  0.1273, -1.5930, -0.4581,  0.6591,  1.3945,  1.9657]],
       dtype=torch.float64)
	q_value: tensor([[-13.1610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 7, step: 67
	action: tensor([[-2.6538, -1.3289,  0.0848,  1.3341, -0.8533, -0.6988, -1.6943]],
       dtype=torch.float64)
	q_value: tensor([[-13.1610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 7, step: 68
	action: tensor([[ 0.1656, -0.8107, -0.1294,  2.3130,  0.2102,  2.2180,  0.4812]],
       dtype=torch.float64)
	q_value: tensor([[-13.1610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4386540099982955, distance: 0.8573773486830811 entropy 1.6592931747436523
epoch: 7, step: 69
	action: tensor([[-1.5765,  0.7932, -0.5904, -1.8350,  0.1205,  0.0133,  0.0246]],
       dtype=torch.float64)
	q_value: tensor([[-11.3340]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 7, step: 70
	action: tensor([[ 1.9560,  0.8145,  0.3843,  0.2173,  0.0918,  1.2001, -0.3095]],
       dtype=torch.float64)
	q_value: tensor([[-13.1610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 7, step: 71
	action: tensor([[ 0.6451, -0.7874, -1.7333,  0.4400, -1.4344,  0.6315,  0.3954]],
       dtype=torch.float64)
	q_value: tensor([[-13.1610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.36866878760447075, distance: 1.3387697012186153 entropy 1.6592931747436523
epoch: 7, step: 72
	action: tensor([[ 1.5643, -1.0464,  0.1789, -0.7878,  1.2962,  1.0182,  0.5109]],
       dtype=torch.float64)
	q_value: tensor([[-10.6446]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8252355745334774, distance: 1.5460237300042412 entropy 1.6592931747436523
epoch: 7, step: 73
	action: tensor([[ 1.3868,  1.3484,  0.0535, -0.0936, -1.8609,  0.5498,  0.8239]],
       dtype=torch.float64)
	q_value: tensor([[-10.5178]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 7, step: 74
	action: tensor([[-0.5490, -1.4857, -0.6453,  0.1865,  0.3141,  1.0640, -0.7510]],
       dtype=torch.float64)
	q_value: tensor([[-13.1610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8065066916414803, distance: 1.538071343069777 entropy 1.6592931747436523
epoch: 7, step: 75
	action: tensor([[ 2.6465, -0.3833, -0.9319, -2.0506, -2.1703, -1.7937,  0.8550]],
       dtype=torch.float64)
	q_value: tensor([[-8.7271]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 7, step: 76
	action: tensor([[-0.7800, -1.0657,  1.0676,  2.3708, -0.7868,  1.6549,  1.3687]],
       dtype=torch.float64)
	q_value: tensor([[-13.1610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2923815447086703, distance: 0.9626230770510948 entropy 1.6592931747436523
epoch: 7, step: 77
	action: tensor([[-1.1333,  0.7418,  0.9236,  0.9547, -0.7010, -0.8126, -2.7655]],
       dtype=torch.float64)
	q_value: tensor([[-14.8897]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 7, step: 78
	action: tensor([[ 0.8422, -0.2684, -1.2617,  0.0649,  1.1239, -0.0489,  1.5979]],
       dtype=torch.float64)
	q_value: tensor([[-13.1610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 7, step: 79
	action: tensor([[ 0.3671, -0.5196,  2.4984,  0.6779, -0.0198, -0.9297, -1.1407]],
       dtype=torch.float64)
	q_value: tensor([[-13.1610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13357308468492846, distance: 1.065179159584702 entropy 1.6592931747436523
epoch: 7, step: 80
	action: tensor([[-1.6984, -2.9016,  2.8537, -1.9447,  1.0367, -0.1049,  0.1237]],
       dtype=torch.float64)
	q_value: tensor([[-11.8849]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 7, step: 81
	action: tensor([[ 1.2142, -0.0643,  0.2262, -1.1249,  2.0662,  1.0390,  0.2482]],
       dtype=torch.float64)
	q_value: tensor([[-13.1610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 7, step: 82
	action: tensor([[ 1.0683,  1.2330, -0.5544, -0.7200,  0.3622, -0.0633,  1.7564]],
       dtype=torch.float64)
	q_value: tensor([[-13.1610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 7, step: 83
	action: tensor([[ 1.4411,  0.0303,  0.3869,  1.7090,  1.6817, -0.0836, -0.9997]],
       dtype=torch.float64)
	q_value: tensor([[-13.1610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 7, step: 84
	action: tensor([[-0.7328, -3.0372, -1.4629, -1.6281,  1.9646,  0.9150, -3.6587]],
       dtype=torch.float64)
	q_value: tensor([[-13.1610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 7, step: 85
	action: tensor([[-0.2705,  0.8054,  1.5060,  1.1301, -1.5312,  0.8622,  0.2598]],
       dtype=torch.float64)
	q_value: tensor([[-13.1610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 7, step: 86
	action: tensor([[ 0.1887, -0.6531,  1.7626,  1.3042, -0.1106,  0.1153, -0.4666]],
       dtype=torch.float64)
	q_value: tensor([[-13.1610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3478621361215025, distance: 0.9241158126286458 entropy 1.6592931747436523
epoch: 7, step: 87
	action: tensor([[-1.1099,  1.1954, -1.6871,  0.9434,  1.1924,  0.9566, -0.0684]],
       dtype=torch.float64)
	q_value: tensor([[-9.8542]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.36267876484299477, distance: 1.3358369046040113 entropy 1.6592931747436523
epoch: 7, step: 88
	action: tensor([[-2.9028, -2.1151, -1.9832, -0.3626, -0.3032, -0.5026,  1.1747]],
       dtype=torch.float64)
	q_value: tensor([[-11.6667]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 7, step: 89
	action: tensor([[-0.5380,  1.3677, -2.7707, -1.0666,  0.3147,  0.4601,  0.1703]],
       dtype=torch.float64)
	q_value: tensor([[-13.1610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 7, step: 90
	action: tensor([[-2.5332,  0.6417, -0.9474, -0.0723, -0.2248, -0.7577,  0.7765]],
       dtype=torch.float64)
	q_value: tensor([[-13.1610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 7, step: 91
	action: tensor([[ 1.9102, -2.1182,  0.4368,  0.5080, -0.7699, -1.8874,  0.0110]],
       dtype=torch.float64)
	q_value: tensor([[-13.1610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 7, step: 92
	action: tensor([[ 0.4418, -0.1998, -0.5295,  1.3810,  0.4992,  0.3196,  1.5781]],
       dtype=torch.float64)
	q_value: tensor([[-13.1610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 7, step: 93
	action: tensor([[-0.1282, -1.2864,  0.7957, -0.8141, -0.2131, -1.0096, -0.9674]],
       dtype=torch.float64)
	q_value: tensor([[-13.1610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.956226225740139, distance: 1.600538892172553 entropy 1.6592931747436523
epoch: 7, step: 94
	action: tensor([[-0.3789, -0.2490,  0.0490,  1.9089, -1.1692,  1.2991,  0.2477]],
       dtype=torch.float64)
	q_value: tensor([[-8.7694]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 7, step: 95
	action: tensor([[ 2.3803, -0.3149, -0.6609,  1.1523,  0.3239,  1.3337,  0.4729]],
       dtype=torch.float64)
	q_value: tensor([[-13.1610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 7, step: 96
	action: tensor([[ 2.1586,  0.8184, -0.1520,  0.2120, -0.8992,  1.0074, -0.5971]],
       dtype=torch.float64)
	q_value: tensor([[-13.1610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 7, step: 97
	action: tensor([[-0.1163,  2.2056,  1.3160, -0.7251,  2.1187,  0.0897,  0.9800]],
       dtype=torch.float64)
	q_value: tensor([[-13.1610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 7, step: 98
	action: tensor([[-1.5525, -1.3838, -0.0414, -1.0443,  0.5996,  0.8834,  2.7151]],
       dtype=torch.float64)
	q_value: tensor([[-13.1610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8100517445363828, distance: 1.5395797437645589 entropy 1.6592931747436523
epoch: 7, step: 99
	action: tensor([[ 1.0966, -0.5552, -0.1323,  0.7851,  0.2747, -1.5233, -0.6828]],
       dtype=torch.float64)
	q_value: tensor([[-17.3582]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36701465110418663, distance: 0.91044459550999 entropy 1.6592931747436523
epoch: 7, step: 100
	action: tensor([[ 0.6069, -0.6532, -0.2050, -0.6982, -0.1275, -0.8885,  0.3538]],
       dtype=torch.float64)
	q_value: tensor([[-7.3578]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4123242881207696, distance: 1.359953024405958 entropy 1.6592931747436523
epoch: 7, step: 101
	action: tensor([[-0.6687,  1.8669, -1.3033, -1.5856,  0.8533, -0.1549,  0.9463]],
       dtype=torch.float64)
	q_value: tensor([[-6.9423]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 7, step: 102
	action: tensor([[ 0.6174, -0.1630, -1.0844,  2.2771,  0.3149,  2.3728, -0.5019]],
       dtype=torch.float64)
	q_value: tensor([[-13.1610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5432036952516621, distance: 0.7734246051656753 entropy 1.6592931747436523
epoch: 7, step: 103
	action: tensor([[ 3.2033, -1.4966, -1.2349, -0.7286, -1.7799, -0.3712,  1.5623]],
       dtype=torch.float64)
	q_value: tensor([[-9.9892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 7, step: 104
	action: tensor([[ 0.8423, -2.0184,  0.5346,  0.9243,  0.5087,  2.8580, -0.5374]],
       dtype=torch.float64)
	q_value: tensor([[-13.1610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 7, step: 105
	action: tensor([[-1.8816, -1.6878,  0.3416, -0.3564, -1.2959,  1.1932, -0.2363]],
       dtype=torch.float64)
	q_value: tensor([[-13.1610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 7, step: 106
	action: tensor([[ 2.6011, -0.4066,  1.0166, -0.3287, -1.6906, -0.3987, -0.3414]],
       dtype=torch.float64)
	q_value: tensor([[-13.1610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 7, step: 107
	action: tensor([[ 1.1010, -0.5863, -2.1104, -0.5602,  0.9894, -0.4222,  2.2576]],
       dtype=torch.float64)
	q_value: tensor([[-13.1610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 7, step: 108
	action: tensor([[-2.1675,  0.8938,  1.1792,  1.1057, -1.9049,  0.1088,  0.5138]],
       dtype=torch.float64)
	q_value: tensor([[-13.1610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 7, step: 109
	action: tensor([[-1.5650, -2.2545, -1.1546, -3.5586, -0.9624,  0.6350, -0.9723]],
       dtype=torch.float64)
	q_value: tensor([[-13.1610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 7, step: 110
	action: tensor([[-0.3115, -2.0230,  0.8216,  0.6745, -0.9058, -0.3305,  1.4263]],
       dtype=torch.float64)
	q_value: tensor([[-13.1610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 7, step: 111
	action: tensor([[-0.3921, -0.8261, -0.9899, -1.4674,  1.4415,  0.6493,  0.0337]],
       dtype=torch.float64)
	q_value: tensor([[-13.1610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2897608531398307, distance: 0.9644039850492686 entropy 1.6592931747436523
epoch: 7, step: 112
	action: tensor([[-0.6050,  1.9395, -1.0159,  0.0481,  0.1428,  4.3995, -0.5582]],
       dtype=torch.float64)
	q_value: tensor([[-12.6889]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 7, step: 113
	action: tensor([[-0.4603, -0.9094, -0.7294, -1.5502, -0.6448,  0.1526, -0.2690]],
       dtype=torch.float64)
	q_value: tensor([[-13.1610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10631284236103755, distance: 1.203637389734814 entropy 1.6592931747436523
epoch: 7, step: 114
	action: tensor([[-1.2609,  0.2505, -0.0167,  0.9890, -2.9705,  0.2293,  0.7670]],
       dtype=torch.float64)
	q_value: tensor([[-10.8130]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4029797145238925, distance: 1.3554465269164067 entropy 1.6592931747436523
epoch: 7, step: 115
	action: tensor([[ 0.6633, -1.0602,  0.3056,  0.5776, -3.2874,  1.8701,  1.9726]],
       dtype=torch.float64)
	q_value: tensor([[-17.4397]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2723235305842702 entropy 1.6592931747436523
epoch: 7, step: 116
	action: tensor([[-8.6865e-01, -8.3008e-01,  1.9608e+00, -1.0055e-03,  1.8218e+00,
          1.7035e+00,  3.8808e+00]], dtype=torch.float64)
	q_value: tensor([[-13.1610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.597927671671993 entropy 1.6592931747436523
epoch: 7, step: 117
	action: tensor([[ 0.8335,  1.6021,  1.2437,  1.2674, -0.3017, -0.1238,  1.8833]],
       dtype=torch.float64)
	q_value: tensor([[-13.1610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 7, step: 118
	action: tensor([[-1.8425, -0.4814,  0.5569,  0.7172,  0.6117,  1.9600, -2.2138]],
       dtype=torch.float64)
	q_value: tensor([[-13.1610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 7, step: 119
	action: tensor([[-0.7727, -1.6893, -1.3207,  0.8127,  1.5856, -1.1853, -0.3515]],
       dtype=torch.float64)
	q_value: tensor([[-13.1610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1818628951987566, distance: 1.6903258052138432 entropy 1.6592931747436523
epoch: 7, step: 120
	action: tensor([[-1.3793, -1.3890,  0.4360, -2.4674,  1.1937,  1.6341, -0.5134]],
       dtype=torch.float64)
	q_value: tensor([[-10.6130]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 7, step: 121
	action: tensor([[-1.2193, -0.9925, -0.6605,  0.0274,  1.6062, -1.3487, -2.1161]],
       dtype=torch.float64)
	q_value: tensor([[-13.1610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.3254596813963966, distance: 1.7450629595610643 entropy 1.6592931747436523
epoch: 7, step: 122
	action: tensor([[ 1.1178, -0.3220, -2.9603,  1.8029,  1.0576,  0.7639,  0.0976]],
       dtype=torch.float64)
	q_value: tensor([[-12.9082]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09724198428306008, distance: 1.1986928137288326 entropy 1.6592931747436523
epoch: 7, step: 123
	action: tensor([[-0.1242, -1.3217, -1.0708,  2.1722, -0.1126, -1.4077,  1.3943]],
       dtype=torch.float64)
	q_value: tensor([[-10.9436]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3405774121447096, distance: 1.3249596178592455 entropy 1.6592931747436523
epoch: 7, step: 124
	action: tensor([[-1.0456, -0.6459,  1.0327, -2.1442, -0.6222,  1.1605, -0.8750]],
       dtype=torch.float64)
	q_value: tensor([[-11.8195]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9427588405486271, distance: 1.5950200263026537 entropy 1.6592931747436523
epoch: 7, step: 125
	action: tensor([[-1.5028, -0.1580, -1.8157, -0.7723, -0.7153, -0.1137,  0.2924]],
       dtype=torch.float64)
	q_value: tensor([[-14.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8436743449043163, distance: 1.553813174695424 entropy 1.6592931747436523
epoch: 7, step: 126
	action: tensor([[-0.5706, -0.3809,  0.3951,  1.3785, -0.3602,  0.1816, -2.6625]],
       dtype=torch.float64)
	q_value: tensor([[-13.2137]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 7, step: 127
	action: tensor([[-2.6986, -0.1885, -0.6879,  0.4421, -0.0887, -0.7257,  0.0467]],
       dtype=torch.float64)
	q_value: tensor([[-13.1610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
LOSS epoch 7 actor 401.8475980251195 critic 1326.823776232053 
epoch: 8, step: 0
	action: tensor([[-0.9391, -1.0281, -0.8371, -1.4988,  0.0278, -0.9000,  2.5460]],
       dtype=torch.float64)
	q_value: tensor([[-21.2157]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.365307776962727, distance: 0.911671296990767 entropy 1.6592931747436523
epoch: 8, step: 1
	action: tensor([[-0.0539, -1.4958,  2.4364,  0.6530, -0.3264, -1.8240, -1.3952]],
       dtype=torch.float64)
	q_value: tensor([[-24.9487]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 8, step: 2
	action: tensor([[-0.8647,  0.6535,  0.0694, -1.2127,  0.0726,  1.5922, -0.6945]],
       dtype=torch.float64)
	q_value: tensor([[-21.2157]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 8, step: 3
	action: tensor([[ 0.4226,  0.7547,  0.7771,  0.4471, -0.4539, -0.8971,  1.5937]],
       dtype=torch.float64)
	q_value: tensor([[-21.2157]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 8, step: 4
	action: tensor([[-1.9024,  0.4811, -0.4669,  0.3516,  0.7679,  0.2993,  0.2879]],
       dtype=torch.float64)
	q_value: tensor([[-21.2157]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 8, step: 5
	action: tensor([[ 3.1224, -1.4579,  0.3534,  0.2039, -0.3484,  0.7630,  2.0661]],
       dtype=torch.float64)
	q_value: tensor([[-21.2157]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 8, step: 6
	action: tensor([[ 1.9286, -3.6685, -0.5921,  3.3554,  1.8098, -1.5039,  0.9173]],
       dtype=torch.float64)
	q_value: tensor([[-21.2157]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 8, step: 7
	action: tensor([[ 2.5486, -3.7503,  0.1867, -0.5196,  0.0757, -1.8435, -0.1013]],
       dtype=torch.float64)
	q_value: tensor([[-21.2157]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 8, step: 8
	action: tensor([[ 1.2085, -2.0446,  0.6600, -1.3080, -0.7706,  0.6069, -0.8211]],
       dtype=torch.float64)
	q_value: tensor([[-21.2157]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 8, step: 9
	action: tensor([[ 0.2426, -0.5232, -2.2100,  0.0764, -0.0050, -1.0444, -0.3337]],
       dtype=torch.float64)
	q_value: tensor([[-21.2157]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 8, step: 10
	action: tensor([[-1.3972,  0.3932,  1.6438, -1.5868, -5.2126, -3.3699,  0.1789]],
       dtype=torch.float64)
	q_value: tensor([[-21.2157]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 8, step: 11
	action: tensor([[ 0.4845,  0.3368, -0.1079, -0.8802,  2.2388,  0.9961, -0.1139]],
       dtype=torch.float64)
	q_value: tensor([[-21.2157]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 8, step: 12
	action: tensor([[ 1.9257,  1.5240,  2.1977, -0.9082, -1.4961,  0.5160, -1.7471]],
       dtype=torch.float64)
	q_value: tensor([[-21.2157]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 8, step: 13
	action: tensor([[ 1.8528, -0.3371,  1.1524,  0.9760,  1.0457, -1.2250,  0.1407]],
       dtype=torch.float64)
	q_value: tensor([[-21.2157]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 8, step: 14
	action: tensor([[ 1.1474,  0.6797, -1.1713, -1.6751, -0.1022,  0.5214, -1.5410]],
       dtype=torch.float64)
	q_value: tensor([[-21.2157]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 8, step: 15
	action: tensor([[ 0.3646, -2.2732, -1.1469,  0.2188, -1.9939, -0.2624, -2.1573]],
       dtype=torch.float64)
	q_value: tensor([[-21.2157]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 8, step: 16
	action: tensor([[ 0.8381, -0.2523,  0.6782,  0.3107, -1.3439,  2.4729,  0.2273]],
       dtype=torch.float64)
	q_value: tensor([[-21.2157]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8598826971558905, distance: 0.42835375438490014 entropy 1.6592931747436523
epoch: 8, step: 17
	action: tensor([[-1.0314, -0.6029,  1.4640,  0.3088, -0.9446,  1.5403,  0.3891]],
       dtype=torch.float64)
	q_value: tensor([[-19.4154]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.31117602858927507, distance: 1.3103496397038847 entropy 1.6592931747436523
epoch: 8, step: 18
	action: tensor([[-0.4196, -0.7188, -1.8824,  0.3019,  0.3822, -0.4082, -0.5267]],
       dtype=torch.float64)
	q_value: tensor([[-20.5778]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8109297189937983, distance: 1.5399530888416837 entropy 1.6592931747436523
epoch: 8, step: 19
	action: tensor([[-0.7420, -2.4187, -0.7343, -0.4723, -0.1682, -1.5893, -0.7190]],
       dtype=torch.float64)
	q_value: tensor([[-13.3694]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 8, step: 20
	action: tensor([[-1.5726, -0.6067, -1.1071,  0.2720, -2.1284,  0.7449, -1.4222]],
       dtype=torch.float64)
	q_value: tensor([[-21.2157]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.7900443681396911, distance: 1.9114469451368217 entropy 1.6592931747436523
epoch: 8, step: 21
	action: tensor([[ 1.4988,  0.0491, -1.4358, -1.4264, -1.7746, -0.2471, -0.7987]],
       dtype=torch.float64)
	q_value: tensor([[-22.7914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14429448182579008, distance: 1.2241245139827308 entropy 1.6592931747436523
epoch: 8, step: 22
	action: tensor([[ 1.5024, -1.2133, -0.4283, -1.4939,  1.0082, -1.6964,  1.0782]],
       dtype=torch.float64)
	q_value: tensor([[-18.1805]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1013531685230038, distance: 1.200936365998231 entropy 1.6592931747436523
epoch: 8, step: 23
	action: tensor([[ 0.0130,  1.8929, -0.1558,  0.4868, -0.9572, -0.1137,  1.1203]],
       dtype=torch.float64)
	q_value: tensor([[-17.9447]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 8, step: 24
	action: tensor([[ 0.3916, -1.7176,  0.1313,  0.1214, -0.1607,  0.4772,  0.3569]],
       dtype=torch.float64)
	q_value: tensor([[-21.2157]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3825304564116807, distance: 1.3455320500526695 entropy 1.6592931747436523
epoch: 8, step: 25
	action: tensor([[ 0.4455, -1.5079,  0.0868, -0.3528, -0.0038, -3.2003,  0.6196]],
       dtype=torch.float64)
	q_value: tensor([[-10.8116]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 8, step: 26
	action: tensor([[-1.0575, -0.0553, -0.6549,  0.0583, -1.0056, -1.3599,  2.1593]],
       dtype=torch.float64)
	q_value: tensor([[-21.2157]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.35633983017500537, distance: 1.332726247234321 entropy 1.6592931747436523
epoch: 8, step: 27
	action: tensor([[ 0.8121, -0.6468, -0.4631,  0.9827,  2.8037,  1.2695,  1.9083]],
       dtype=torch.float64)
	q_value: tensor([[-22.4329]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5004717761059455, distance: 0.8087917431612233 entropy 1.6592931747436523
epoch: 8, step: 28
	action: tensor([[-0.5219,  0.6979,  1.1170, -0.3555,  1.5910, -0.7450, -0.6767]],
       dtype=torch.float64)
	q_value: tensor([[-21.1147]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 8, step: 29
	action: tensor([[-0.4898, -0.0414,  0.2086, -1.6997,  0.0561,  1.4347,  1.0976]],
       dtype=torch.float64)
	q_value: tensor([[-21.2157]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4017550070130216, distance: 1.354854790643975 entropy 1.6592931747436523
epoch: 8, step: 30
	action: tensor([[-0.8306, -0.2541,  0.9393,  0.4807,  0.1757,  0.1171,  0.7074]],
       dtype=torch.float64)
	q_value: tensor([[-22.9726]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3889562021574675, distance: 1.3486553171673288 entropy 1.6592931747436523
epoch: 8, step: 31
	action: tensor([[-0.2212,  0.6879, -0.1575, -1.7751, -0.7989,  1.3553, -0.7142]],
       dtype=torch.float64)
	q_value: tensor([[-12.1487]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13787435643474755, distance: 1.0625318933607604 entropy 1.6592931747436523
epoch: 8, step: 32
	action: tensor([[ 2.1215,  0.8028, -0.9642,  1.1075, -0.1452,  0.8456,  0.5582]],
       dtype=torch.float64)
	q_value: tensor([[-20.3014]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 8, step: 33
	action: tensor([[-2.9325, -0.8589, -0.9943, -1.1290,  1.2934,  0.9153,  0.9684]],
       dtype=torch.float64)
	q_value: tensor([[-21.2157]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 8, step: 34
	action: tensor([[-1.1420,  0.5014,  1.0843,  0.9566,  0.5030,  0.7286,  2.9300]],
       dtype=torch.float64)
	q_value: tensor([[-21.2157]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 8, step: 35
	action: tensor([[ 0.8036, -1.7076,  2.0441,  0.9689, -0.3659,  0.0690,  0.0544]],
       dtype=torch.float64)
	q_value: tensor([[-21.2157]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6176291292056837, distance: 1.4554462540503048 entropy 1.6592931747436523
epoch: 8, step: 36
	action: tensor([[-1.3880, -0.6291, -0.6221,  0.1379, -0.2820, -0.1714,  1.6410]],
       dtype=torch.float64)
	q_value: tensor([[-16.2713]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.6555555235041872, distance: 1.8648091283022046 entropy 1.6592931747436523
epoch: 8, step: 37
	action: tensor([[ 0.3286, -0.8209, -0.8378,  0.7534, -0.3608,  2.3674, -1.6228]],
       dtype=torch.float64)
	q_value: tensor([[-18.3774]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2933011293757919, distance: 0.9619973859058379 entropy 1.6592931747436523
epoch: 8, step: 38
	action: tensor([[ 0.6836, -0.2332,  0.2416, -1.1773,  2.0853, -2.3234,  0.2879]],
       dtype=torch.float64)
	q_value: tensor([[-17.1078]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5276476861691635, distance: 0.7864836829594213 entropy 1.6592931747436523
epoch: 8, step: 39
	action: tensor([[-0.2910, -0.3981, -1.9614,  1.8668, -1.8106, -2.3002,  0.9193]],
       dtype=torch.float64)
	q_value: tensor([[-18.2229]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 8, step: 40
	action: tensor([[-0.8215,  0.7424, -0.2077,  0.0782,  0.9616, -0.7781,  0.1433]],
       dtype=torch.float64)
	q_value: tensor([[-21.2157]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3615281742649501, distance: 1.3352728223058485 entropy 1.6592931747436523
epoch: 8, step: 41
	action: tensor([[ 1.4822, -0.2690, -1.4751, -0.6730,  1.7246,  1.3054, -0.7871]],
       dtype=torch.float64)
	q_value: tensor([[-12.9243]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 8, step: 42
	action: tensor([[ 1.4908, -1.6797,  1.7351, -0.7515,  1.4284,  2.0636,  0.7361]],
       dtype=torch.float64)
	q_value: tensor([[-21.2157]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 8, step: 43
	action: tensor([[ 0.4480, -1.5311,  0.3658, -0.5752,  0.2304, -0.1919, -1.3235]],
       dtype=torch.float64)
	q_value: tensor([[-21.2157]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0132210316824999, distance: 1.6236874079082382 entropy 1.6592931747436523
epoch: 8, step: 44
	action: tensor([[ 1.4446, -0.5467, -2.2879, -1.4789,  1.1061,  0.0836, -0.0665]],
       dtype=torch.float64)
	q_value: tensor([[-12.6696]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2579552685560166, distance: 0.9857612365390753 entropy 1.6592931747436523
epoch: 8, step: 45
	action: tensor([[-0.2603, -1.2913, -2.8574,  0.9078, -0.1270, -1.3795,  0.0326]],
       dtype=torch.float64)
	q_value: tensor([[-20.1811]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7464637910944687, distance: 1.5122948954441953 entropy 1.6592931747436523
epoch: 8, step: 46
	action: tensor([[ 0.3022, -2.1636,  0.8800,  3.4526, -1.0051, -1.6324,  1.3206]],
       dtype=torch.float64)
	q_value: tensor([[-18.7651]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 8, step: 47
	action: tensor([[ 0.8744,  1.6221,  0.2397, -2.2770,  0.9805, -1.2169, -0.4834]],
       dtype=torch.float64)
	q_value: tensor([[-21.2157]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 8, step: 48
	action: tensor([[ 0.3214,  0.2141, -1.7582, -1.3704,  1.3653,  0.0636,  1.3281]],
       dtype=torch.float64)
	q_value: tensor([[-21.2157]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 8, step: 49
	action: tensor([[-1.8389,  1.3910,  0.9506,  1.0478, -0.9613, -1.5353, -1.3623]],
       dtype=torch.float64)
	q_value: tensor([[-21.2157]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 8, step: 50
	action: tensor([[-0.7928, -0.6989,  2.4390,  0.5696,  1.5463, -0.4474,  1.0079]],
       dtype=torch.float64)
	q_value: tensor([[-21.2157]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6222940837262352, distance: 1.4575433671995672 entropy 1.6592931747436523
epoch: 8, step: 51
	action: tensor([[-2.7981, -1.7264, -0.7941,  2.3360,  0.7534, -0.5824,  4.3894]],
       dtype=torch.float64)
	q_value: tensor([[-18.7993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 8, step: 52
	action: tensor([[-1.1626, -1.0078, -0.8318, -0.0699, -0.7720, -3.7425, -0.2628]],
       dtype=torch.float64)
	q_value: tensor([[-21.2157]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 8, step: 53
	action: tensor([[ 1.4825, -0.3787, -1.8318,  0.9921,  0.0907, -0.0964,  1.7365]],
       dtype=torch.float64)
	q_value: tensor([[-21.2157]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 8, step: 54
	action: tensor([[-1.4406, -0.1612, -1.4799,  0.5193,  1.7491,  0.0071, -1.0780]],
       dtype=torch.float64)
	q_value: tensor([[-21.2157]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.7176465316428575, distance: 1.886484226322427 entropy 1.6592931747436523
epoch: 8, step: 55
	action: tensor([[ 0.1370, -1.3317, -0.1581, -0.2786, -0.7717,  1.4592, -0.2676]],
       dtype=torch.float64)
	q_value: tensor([[-17.6273]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5234603885014912, distance: 1.4124473774018529 entropy 1.6592931747436523
epoch: 8, step: 56
	action: tensor([[ 0.6034,  0.5788, -1.2136, -0.4295, -1.9857,  0.5726,  0.0973]],
       dtype=torch.float64)
	q_value: tensor([[-15.7376]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9311914161623983, distance: 0.3001774079270344 entropy 1.6592931747436523
epoch: 8, step: 57
	action: tensor([[ 0.3200,  0.2577,  0.7783,  1.0723, -0.2150, -1.0138, -0.6469]],
       dtype=torch.float64)
	q_value: tensor([[-18.2370]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 8, step: 58
	action: tensor([[ 1.2657,  0.2148,  1.1953, -2.2500, -1.1871,  1.9708, -1.1228]],
       dtype=torch.float64)
	q_value: tensor([[-21.2157]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8334637319047318, distance: 0.4669937921587027 entropy 1.6592931747436523
epoch: 8, step: 59
	action: tensor([[-0.9051, -1.1252, -1.9432,  0.3606, -0.7254, -0.3669,  0.7260]],
       dtype=torch.float64)
	q_value: tensor([[-23.2967]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.4367126910254808, distance: 1.7863183349029195 entropy 1.6592931747436523
epoch: 8, step: 60
	action: tensor([[-2.7954, -2.3120,  1.1245,  1.6090,  1.0472,  0.7941,  0.3742]],
       dtype=torch.float64)
	q_value: tensor([[-18.0521]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 8, step: 61
	action: tensor([[ 0.3137, -0.6165,  0.3221,  0.4892, -2.1027,  0.9614, -1.4777]],
       dtype=torch.float64)
	q_value: tensor([[-21.2157]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17831835855605505, distance: 1.0373098101385954 entropy 1.6592931747436523
epoch: 8, step: 62
	action: tensor([[-0.1443, -0.3421,  0.8376, -0.4243, -2.0696, -0.8231,  0.9516]],
       dtype=torch.float64)
	q_value: tensor([[-17.9196]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1581762869881751, distance: 1.2315272549104188 entropy 1.6592931747436523
epoch: 8, step: 63
	action: tensor([[-0.8750,  0.3354, -1.4495, -0.0983, -1.4046, -0.3021, -0.0214]],
       dtype=torch.float64)
	q_value: tensor([[-19.5136]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.29146654678110817, distance: 1.3004638185344297 entropy 1.6592931747436523
epoch: 8, step: 64
	action: tensor([[-2.0468, -0.3458,  1.1196,  1.1079, -1.8783, -0.1835,  1.6008]],
       dtype=torch.float64)
	q_value: tensor([[-17.5959]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 8, step: 65
	action: tensor([[-3.0379,  1.7999,  0.8943, -1.0592, -1.5431,  0.0417,  0.5694]],
       dtype=torch.float64)
	q_value: tensor([[-21.2157]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 8, step: 66
	action: tensor([[ 0.8582,  2.3457, -0.4076, -1.3611,  1.6950, -1.0508,  0.3966]],
       dtype=torch.float64)
	q_value: tensor([[-21.2157]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 8, step: 67
	action: tensor([[ 2.9991, -0.8973,  2.8133,  0.5207, -2.3327,  1.8763, -0.3938]],
       dtype=torch.float64)
	q_value: tensor([[-21.2157]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 8, step: 68
	action: tensor([[ 0.7696, -0.0528,  0.0720,  0.9877, -2.3335,  0.6320, -0.3269]],
       dtype=torch.float64)
	q_value: tensor([[-21.2157]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 8, step: 69
	action: tensor([[-0.8303, -0.9571, -1.8503,  0.9767,  0.3495,  0.7044, -0.7076]],
       dtype=torch.float64)
	q_value: tensor([[-21.2157]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2503657728208863, distance: 1.7166558986719835 entropy 1.6592931747436523
epoch: 8, step: 70
	action: tensor([[ 2.5617,  1.3936,  1.7898,  1.6811, -0.5549, -0.5944,  0.3479]],
       dtype=torch.float64)
	q_value: tensor([[-14.2564]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 8, step: 71
	action: tensor([[-0.3140, -0.0637, -2.9262, -0.8054,  0.1322,  2.2834,  0.5624]],
       dtype=torch.float64)
	q_value: tensor([[-21.2157]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 8, step: 72
	action: tensor([[ 0.8399, -0.2443, -2.6811, -1.6317,  0.4248,  1.0534,  0.9542]],
       dtype=torch.float64)
	q_value: tensor([[-21.2157]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 8, step: 73
	action: tensor([[ 1.8466, -0.7009, -0.9922,  0.5322,  0.4510,  0.8943, -1.3440]],
       dtype=torch.float64)
	q_value: tensor([[-21.2157]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5240258146128911, distance: 0.7894931984901855 entropy 1.6592931747436523
epoch: 8, step: 74
	action: tensor([[-1.2570, -0.4812, -0.1609,  1.9320,  1.6153,  0.4022, -1.2106]],
       dtype=torch.float64)
	q_value: tensor([[-12.1369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 8, step: 75
	action: tensor([[-0.8663,  1.4184, -3.0419,  0.8299,  1.8585,  1.4036, -0.0435]],
       dtype=torch.float64)
	q_value: tensor([[-21.2157]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 8, step: 76
	action: tensor([[-0.6853, -0.6897, -0.4666, -2.6014,  0.4751, -0.6467,  0.8409]],
       dtype=torch.float64)
	q_value: tensor([[-21.2157]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 8, step: 77
	action: tensor([[-2.0062,  2.9682, -3.0575, -0.4601, -1.9401, -0.1091,  1.5050]],
       dtype=torch.float64)
	q_value: tensor([[-21.2157]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 8, step: 78
	action: tensor([[ 0.6703, -0.2815,  1.1380, -1.3146,  0.6603, -0.3951, -0.7371]],
       dtype=torch.float64)
	q_value: tensor([[-21.2157]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.006948874071458944, distance: 1.1403613707417266 entropy 1.6592931747436523
epoch: 8, step: 79
	action: tensor([[-1.1618, -0.0017, -1.4874, -0.8730, -1.1057, -0.7863, -0.1502]],
       dtype=torch.float64)
	q_value: tensor([[-12.5427]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0011382733318590077, distance: 1.1449953570427855 entropy 1.6592931747436523
epoch: 8, step: 80
	action: tensor([[-0.2974,  1.4807,  1.4861,  0.3093,  0.5325,  2.0583, -1.2995]],
       dtype=torch.float64)
	q_value: tensor([[-19.2044]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 8, step: 81
	action: tensor([[ 0.8031, -0.8114,  1.0323,  0.2504,  1.8265, -0.4851,  0.8361]],
       dtype=torch.float64)
	q_value: tensor([[-21.2157]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2148629374795038, distance: 1.013980102065176 entropy 1.6592931747436523
epoch: 8, step: 82
	action: tensor([[ 0.1856, -1.2066,  0.7099,  0.1656, -0.1340,  2.5560, -0.1063]],
       dtype=torch.float64)
	q_value: tensor([[-14.2633]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3455528592763982, distance: 0.9257505539152093 entropy 1.6592931747436523
epoch: 8, step: 83
	action: tensor([[ 0.0758,  0.4327,  0.7621,  0.9882,  3.1561, -0.3312,  2.8757]],
       dtype=torch.float64)
	q_value: tensor([[-18.2238]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 8, step: 84
	action: tensor([[ 1.5433, -0.9886, -0.7724,  0.4673, -2.7000, -0.4986, -1.5204]],
       dtype=torch.float64)
	q_value: tensor([[-21.2157]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09798229094379085, distance: 1.0868365251821177 entropy 1.6592931747436523
epoch: 8, step: 85
	action: tensor([[ 0.9245, -0.7339, -1.4366, -0.6202,  0.9718,  0.6545,  2.6104]],
       dtype=torch.float64)
	q_value: tensor([[-19.3869]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19776678938686854, distance: 1.0249602234795643 entropy 1.6592931747436523
epoch: 8, step: 86
	action: tensor([[-0.4844, -1.2374, -1.3070, -0.3234, -0.7384,  0.5467, -3.3689]],
       dtype=torch.float64)
	q_value: tensor([[-24.0942]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.6225860214184473 entropy 1.6592931747436523
epoch: 8, step: 87
	action: tensor([[-0.4513,  0.4568, -0.1082,  0.2495,  0.8406, -1.1533,  1.1727]],
       dtype=torch.float64)
	q_value: tensor([[-21.2157]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 8, step: 88
	action: tensor([[-2.7072, -0.6198, -0.9620, -0.4239,  0.4074,  1.3552,  1.1293]],
       dtype=torch.float64)
	q_value: tensor([[-21.2157]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 8, step: 89
	action: tensor([[ 0.0043,  0.2378, -0.5220,  0.5806,  2.0335,  2.0893,  1.0839]],
       dtype=torch.float64)
	q_value: tensor([[-21.2157]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 8, step: 90
	action: tensor([[-1.3080,  0.5032, -1.5862,  0.5597, -1.5359,  1.8705, -1.0786]],
       dtype=torch.float64)
	q_value: tensor([[-21.2157]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.781758107089054, distance: 1.5274994587842112 entropy 1.6592931747436523
epoch: 8, step: 91
	action: tensor([[ 2.2875,  0.8147, -1.7008, -2.0192,  1.1687,  2.3416,  1.2205]],
       dtype=torch.float64)
	q_value: tensor([[-21.6103]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 8, step: 92
	action: tensor([[ 0.8749, -0.1943, -1.1920,  0.0723, -0.1204, -1.3348, -0.9359]],
       dtype=torch.float64)
	q_value: tensor([[-21.2157]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 8, step: 93
	action: tensor([[ 0.2880, -1.0284, -1.4256,  1.1179,  0.5119,  0.8881, -1.2645]],
       dtype=torch.float64)
	q_value: tensor([[-21.2157]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08466947430715255, distance: 1.19180554664015 entropy 1.6592931747436523
epoch: 8, step: 94
	action: tensor([[-0.5814,  0.5514, -1.1346, -0.7591,  1.2159,  0.7461, -0.0117]],
       dtype=torch.float64)
	q_value: tensor([[-12.6889]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 8, step: 95
	action: tensor([[ 0.5961, -1.6136, -1.4989, -1.3973,  1.1861, -2.0629,  1.9253]],
       dtype=torch.float64)
	q_value: tensor([[-21.2157]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08141021968045448, distance: 1.1900136089938902 entropy 1.6592931747436523
epoch: 8, step: 96
	action: tensor([[-0.6738,  0.6005,  1.2063,  2.7577, -1.3295,  0.1744, -0.5307]],
       dtype=torch.float64)
	q_value: tensor([[-22.3213]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 8, step: 97
	action: tensor([[-0.7060,  0.5661, -0.1221, -2.2822,  0.1219,  0.9619, -0.5708]],
       dtype=torch.float64)
	q_value: tensor([[-21.2157]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13369873521126818, distance: 1.2184438615148523 entropy 1.6592931747436523
epoch: 8, step: 98
	action: tensor([[-1.1673, -0.1121,  0.9085,  0.1170, -0.3713, -0.3879, -0.8506]],
       dtype=torch.float64)
	q_value: tensor([[-22.1978]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2714331528877936, distance: 1.7246726383431685 entropy 1.6592931747436523
epoch: 8, step: 99
	action: tensor([[-0.0149,  0.2071, -0.0436, -0.3253,  1.2829,  0.2643,  4.3626]],
       dtype=torch.float64)
	q_value: tensor([[-13.9852]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9098388768097047 entropy 1.6592931747436523
epoch: 8, step: 100
	action: tensor([[-1.5953, -1.2467, -0.4920,  1.5845,  2.0014, -3.0422,  0.8195]],
       dtype=torch.float64)
	q_value: tensor([[-21.2157]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 8, step: 101
	action: tensor([[-0.4506, -0.8097, -0.4508,  0.4198,  0.6816,  0.0624,  2.0676]],
       dtype=torch.float64)
	q_value: tensor([[-21.2157]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7435583917005055, distance: 1.5110364526423576 entropy 1.6592931747436523
epoch: 8, step: 102
	action: tensor([[ 0.1702,  0.9259,  1.5216,  0.1518,  0.4903, -0.5448, -1.0302]],
       dtype=torch.float64)
	q_value: tensor([[-17.4962]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 8, step: 103
	action: tensor([[ 0.0499,  0.7640, -2.6048, -1.4516,  1.2732, -0.7464,  1.3904]],
       dtype=torch.float64)
	q_value: tensor([[-21.2157]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 8, step: 104
	action: tensor([[-3.4746,  1.0883,  0.1663, -2.0004, -1.7781, -0.7428, -0.5958]],
       dtype=torch.float64)
	q_value: tensor([[-21.2157]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 8, step: 105
	action: tensor([[ 1.6079, -0.8079, -1.0849, -1.3549, -2.6585, -1.6155, -1.2670]],
       dtype=torch.float64)
	q_value: tensor([[-21.2157]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 8, step: 106
	action: tensor([[-1.2492,  0.8583, -1.2385, -0.4028, -1.0521, -0.4726, -0.1228]],
       dtype=torch.float64)
	q_value: tensor([[-21.2157]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0179934021209478, distance: 1.1545936772186507 entropy 1.6592931747436523
epoch: 8, step: 107
	action: tensor([[-0.6578, -0.8249,  0.1161,  0.4311, -0.2979,  1.3476, -1.3392]],
       dtype=torch.float64)
	q_value: tensor([[-18.2532]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3550588638970673, distance: 1.3320967661080851 entropy 1.6592931747436523
epoch: 8, step: 108
	action: tensor([[ 0.4072, -1.1308,  0.5558,  0.6832,  0.3867, -0.6769, -0.6882]],
       dtype=torch.float64)
	q_value: tensor([[-15.4384]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.19977553941463122, distance: 1.2534490773553737 entropy 1.6592931747436523
epoch: 8, step: 109
	action: tensor([[ 1.0290, -0.7922, -1.1271, -1.5330, -0.9625,  0.5787,  0.3991]],
       dtype=torch.float64)
	q_value: tensor([[-11.0100]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4828103953307763, distance: 1.393476035514924 entropy 1.6592931747436523
epoch: 8, step: 110
	action: tensor([[-0.6047,  1.7587, -1.5645, -0.2215,  0.6531,  1.5356,  0.1715]],
       dtype=torch.float64)
	q_value: tensor([[-18.2474]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 8, step: 111
	action: tensor([[ 0.5801, -0.1754, -0.5276, -0.2942,  1.6946, -0.6796,  1.6179]],
       dtype=torch.float64)
	q_value: tensor([[-21.2157]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33478168531995933, distance: 0.9333376647382174 entropy 1.6592931747436523
epoch: 8, step: 112
	action: tensor([[-2.1709, -2.1913, -0.4219,  2.3871,  0.4052, -0.2457, -0.4773]],
       dtype=torch.float64)
	q_value: tensor([[-15.8021]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 8, step: 113
	action: tensor([[-2.3173,  1.1529,  0.6091, -0.5333, -2.6021,  0.2360, -1.0978]],
       dtype=torch.float64)
	q_value: tensor([[-21.2157]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 8, step: 114
	action: tensor([[ 0.5342,  0.9997,  0.3112, -1.0661, -1.0886,  0.5174,  2.1419]],
       dtype=torch.float64)
	q_value: tensor([[-21.2157]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 8, step: 115
	action: tensor([[-0.8529,  0.1287,  0.2586, -1.5682, -0.8651, -2.1491,  3.3237]],
       dtype=torch.float64)
	q_value: tensor([[-21.2157]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0365542115892654 entropy 1.6592931747436523
epoch: 8, step: 116
	action: tensor([[-1.1257,  0.1707,  0.1517, -1.9104, -1.1992,  0.7522, -0.5248]],
       dtype=torch.float64)
	q_value: tensor([[-21.2157]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.949669143482887, distance: 1.5978542143350636 entropy 1.6592931747436523
epoch: 8, step: 117
	action: tensor([[ 0.4560,  2.2118,  1.4529,  0.3633, -0.6316, -2.2684, -0.8989]],
       dtype=torch.float64)
	q_value: tensor([[-21.8930]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 8, step: 118
	action: tensor([[ 1.7048, -0.1625, -1.0752,  1.2768, -0.7493,  0.9505,  0.4240]],
       dtype=torch.float64)
	q_value: tensor([[-21.2157]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 8, step: 119
	action: tensor([[ 0.2381,  0.1331, -2.9588, -1.0775, -2.1378, -0.9204, -0.2286]],
       dtype=torch.float64)
	q_value: tensor([[-21.2157]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 8, step: 120
	action: tensor([[-0.3866,  0.2219, -1.0115,  0.0971, -0.9437,  0.1673,  1.0804]],
       dtype=torch.float64)
	q_value: tensor([[-21.2157]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1359573352154626, distance: 1.219656973931357 entropy 1.6592931747436523
epoch: 8, step: 121
	action: tensor([[-0.5079,  0.8464,  0.7512,  0.7130,  0.3043,  0.1012,  0.7569]],
       dtype=torch.float64)
	q_value: tensor([[-16.5311]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 8, step: 122
	action: tensor([[-1.9182, -0.9538, -0.1981, -1.1892,  1.2134, -0.3579, -2.3031]],
       dtype=torch.float64)
	q_value: tensor([[-21.2157]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 8, step: 123
	action: tensor([[ 1.3962, -0.0774, -2.2443,  0.4449,  1.4484, -0.1922,  0.5795]],
       dtype=torch.float64)
	q_value: tensor([[-21.2157]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 8, step: 124
	action: tensor([[-0.1720, -0.2398, -0.0346,  2.4117,  1.6832,  2.3223,  0.7744]],
       dtype=torch.float64)
	q_value: tensor([[-21.2157]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 8, step: 125
	action: tensor([[-0.4322,  0.6650,  1.5594, -0.3243,  2.1294,  2.5072, -1.3773]],
       dtype=torch.float64)
	q_value: tensor([[-21.2157]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 8, step: 126
	action: tensor([[ 0.7489, -0.9668, -0.4666,  1.5128,  1.9423, -1.5075,  0.3001]],
       dtype=torch.float64)
	q_value: tensor([[-21.2157]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39528721048339055, distance: 0.8898796395583692 entropy 1.6592931747436523
epoch: 8, step: 127
	action: tensor([[ 1.0685,  1.9050, -0.2249, -0.4442,  0.4691,  0.2849,  0.5151]],
       dtype=torch.float64)
	q_value: tensor([[-15.1737]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
LOSS epoch 8 actor 266.5841526187397 critic 848.065673146331 
epoch: 9, step: 0
	action: tensor([[ 0.1194, -0.7353, -0.1375, -0.2775,  0.1591,  0.3803, -0.0981]],
       dtype=torch.float64)
	q_value: tensor([[-32.9770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.32441207313236764, distance: 1.3169468832280473 entropy 1.553932785987854
epoch: 9, step: 1
	action: tensor([[-0.6897,  0.7275, -0.3255, -0.0644, -0.1337,  0.9820,  1.6573]],
       dtype=torch.float64)
	q_value: tensor([[-13.9120]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.013422692600976727, distance: 1.1366382171219278 entropy 1.553932785987854
epoch: 9, step: 2
	action: tensor([[ 1.3327, -0.0030, -0.2797,  1.3299, -0.0290, -2.0250, -0.4727]],
       dtype=torch.float64)
	q_value: tensor([[-29.9625]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 9, step: 3
	action: tensor([[ 0.4871,  0.8584,  0.3728, -0.3137,  0.5435,  0.4809,  0.1430]],
       dtype=torch.float64)
	q_value: tensor([[-32.9770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 9, step: 4
	action: tensor([[-1.8998,  1.5391,  0.0142, -1.2687, -0.0608,  0.0555,  0.1638]],
       dtype=torch.float64)
	q_value: tensor([[-32.9770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 9, step: 5
	action: tensor([[-2.4057, -0.0474,  1.4046, -3.2730,  0.1413, -0.0888, -0.2351]],
       dtype=torch.float64)
	q_value: tensor([[-32.9770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 9, step: 6
	action: tensor([[ 2.0236, -0.8689, -1.4767, -0.9817,  0.4378,  0.3969,  0.0475]],
       dtype=torch.float64)
	q_value: tensor([[-32.9770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 9, step: 7
	action: tensor([[-2.9922, -1.9183,  0.4651,  0.0387,  0.0905,  0.2839,  1.1358]],
       dtype=torch.float64)
	q_value: tensor([[-32.9770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 9, step: 8
	action: tensor([[-1.5764, -3.0346,  1.9001, -2.3566, -3.6084, -1.6051,  1.9267]],
       dtype=torch.float64)
	q_value: tensor([[-32.9770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 9, step: 9
	action: tensor([[ 2.2136, -0.1046,  0.1389,  2.0837, -0.0935, -2.1343,  1.6217]],
       dtype=torch.float64)
	q_value: tensor([[-32.9770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6305615005543039, distance: 0.6955490616711227 entropy 1.553932785987854
epoch: 9, step: 10
	action: tensor([[-1.6168, -1.6144,  0.1281,  0.8950, -0.4822, -0.3486, -0.0430]],
       dtype=torch.float64)
	q_value: tensor([[-30.6559]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 9, step: 11
	action: tensor([[-1.2069, -2.7842, -0.5328, -0.7219,  0.8524, -0.7498,  2.5855]],
       dtype=torch.float64)
	q_value: tensor([[-32.9770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 9, step: 12
	action: tensor([[ 1.1273,  0.2324, -0.0200, -0.5765,  0.7457, -0.1640,  0.1670]],
       dtype=torch.float64)
	q_value: tensor([[-32.9770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 9, step: 13
	action: tensor([[ 1.3302, -0.2697,  0.0539, -0.4038, -1.2316,  0.1012,  2.3194]],
       dtype=torch.float64)
	q_value: tensor([[-32.9770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 9, step: 14
	action: tensor([[ 1.1457, -0.0023,  0.7590, -1.8037, -0.2462,  1.6499, -0.3343]],
       dtype=torch.float64)
	q_value: tensor([[-32.9770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6146716986278649, distance: 0.7103496114880791 entropy 1.553932785987854
epoch: 9, step: 15
	action: tensor([[-0.5217, -1.3316,  1.1823, -0.2406, -0.8904, -0.5702,  0.9463]],
       dtype=torch.float64)
	q_value: tensor([[-27.8616]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.252671706604426, distance: 1.717535196015133 entropy 1.553932785987854
epoch: 9, step: 16
	action: tensor([[-0.5513, -0.4658, -1.1465, -0.3447, -0.6550,  1.0027,  0.8038]],
       dtype=torch.float64)
	q_value: tensor([[-22.9753]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7563139446901872, distance: 1.5165536129905508 entropy 1.553932785987854
epoch: 9, step: 17
	action: tensor([[ 1.0661, -1.1786, -0.4715,  0.9416, -0.3972,  0.2453, -1.0709]],
       dtype=torch.float64)
	q_value: tensor([[-26.9294]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3014638615368601, distance: 0.9564254694867919 entropy 1.553932785987854
epoch: 9, step: 18
	action: tensor([[-0.3364,  0.9394, -1.7343, -0.4861,  0.3702,  0.1290,  0.8079]],
       dtype=torch.float64)
	q_value: tensor([[-17.0180]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 9, step: 19
	action: tensor([[0.2763, 0.8088, 0.0745, 2.8716, 2.6909, 1.7382, 0.8786]],
       dtype=torch.float64)
	q_value: tensor([[-32.9770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 9, step: 20
	action: tensor([[-2.3788, -1.8024, -1.3231,  0.6506, -0.8949, -0.0281,  0.1119]],
       dtype=torch.float64)
	q_value: tensor([[-32.9770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 9, step: 21
	action: tensor([[-0.6636,  1.7377, -0.7019, -1.4749,  1.2600, -0.0948, -0.4540]],
       dtype=torch.float64)
	q_value: tensor([[-32.9770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 9, step: 22
	action: tensor([[ 0.1914, -2.1070,  0.8406, -0.7619, -0.2874,  0.2394, -0.1116]],
       dtype=torch.float64)
	q_value: tensor([[-32.9770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 9, step: 23
	action: tensor([[-0.1675,  0.0547,  0.1530, -0.3588,  2.1731, -1.5136,  3.2900]],
       dtype=torch.float64)
	q_value: tensor([[-32.9770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0898046541476882 entropy 1.553932785987854
epoch: 9, step: 24
	action: tensor([[ 2.0283,  0.4050, -1.1175, -2.6086, -0.7559,  1.4266,  1.2068]],
       dtype=torch.float64)
	q_value: tensor([[-32.9770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 9, step: 25
	action: tensor([[-0.4896, -1.0538,  1.4998, -0.3140, -2.1963,  1.5192,  0.9953]],
       dtype=torch.float64)
	q_value: tensor([[-32.9770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6808119038214124, distance: 1.4835980398167068 entropy 1.553932785987854
epoch: 9, step: 26
	action: tensor([[-0.5749,  1.8945,  1.4172,  0.6814, -1.5823, -0.2317, -0.8273]],
       dtype=torch.float64)
	q_value: tensor([[-38.9703]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 9, step: 27
	action: tensor([[ 0.6261,  0.2664, -0.8148, -0.1624, -0.5263,  1.4680, -0.9277]],
       dtype=torch.float64)
	q_value: tensor([[-32.9770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 9, step: 28
	action: tensor([[ 2.0827, -3.2910, -1.4455,  0.2807,  0.0505, -0.5690, -0.4955]],
       dtype=torch.float64)
	q_value: tensor([[-32.9770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 9, step: 29
	action: tensor([[ 0.5322, -0.3026,  0.1285, -1.1484,  0.6009, -0.5920, -0.1541]],
       dtype=torch.float64)
	q_value: tensor([[-32.9770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.28967347893618656, distance: 1.299560725149087 entropy 1.553932785987854
epoch: 9, step: 30
	action: tensor([[-0.6255, -0.4040,  0.9173,  0.8061, -0.7301,  1.2153,  1.3545]],
       dtype=torch.float64)
	q_value: tensor([[-17.0969]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3743830866624791, distance: 0.9051299464550931 entropy 1.553932785987854
epoch: 9, step: 31
	action: tensor([[ 0.3676,  1.5887,  1.9872,  0.9628, -0.3063, -1.5271, -2.3676]],
       dtype=torch.float64)
	q_value: tensor([[-28.1263]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 9, step: 32
	action: tensor([[ 0.2684,  0.5820, -0.7002,  0.0511, -0.1065,  0.9361,  0.8435]],
       dtype=torch.float64)
	q_value: tensor([[-32.9770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 9, step: 33
	action: tensor([[ 1.8400,  0.4527, -1.2811, -2.0045,  0.4695,  1.2322, -0.1389]],
       dtype=torch.float64)
	q_value: tensor([[-32.9770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 9, step: 34
	action: tensor([[ 0.0118, -1.0655,  0.4896,  1.0393, -0.2192, -0.5054,  1.2259]],
       dtype=torch.float64)
	q_value: tensor([[-32.9770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10208206084173543, distance: 1.084363816405838 entropy 1.553932785987854
epoch: 9, step: 35
	action: tensor([[-0.0991, -1.5707, -1.1632,  1.1873,  1.0586, -0.7004,  0.8632]],
       dtype=torch.float64)
	q_value: tensor([[-19.8133]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 9, step: 36
	action: tensor([[0.0620, 1.1537, 1.1218, 0.0604, 0.8005, 0.9863, 1.5317]],
       dtype=torch.float64)
	q_value: tensor([[-32.9770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 9, step: 37
	action: tensor([[-0.9556, -0.1816,  0.6172, -2.1265,  0.7653,  1.2557, -1.7801]],
       dtype=torch.float64)
	q_value: tensor([[-32.9770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6361921072820431, distance: 1.46377336343196 entropy 1.553932785987854
epoch: 9, step: 38
	action: tensor([[-0.3744,  0.5627, -1.3264,  1.4104, -0.9886, -0.6329,  2.1623]],
       dtype=torch.float64)
	q_value: tensor([[-34.9133]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18690395441407004, distance: 1.2467072402877206 entropy 1.553932785987854
epoch: 9, step: 39
	action: tensor([[-0.2207, -1.1120,  0.2303, -0.1677, -1.6920, -1.7522, -1.2252]],
       dtype=torch.float64)
	q_value: tensor([[-34.0640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 9, step: 40
	action: tensor([[ 0.4956, -1.3416, -0.3602,  1.1057, -1.0910, -0.2885,  0.2342]],
       dtype=torch.float64)
	q_value: tensor([[-32.9770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22407542546727433, distance: 1.0080137277676138 entropy 1.553932785987854
epoch: 9, step: 41
	action: tensor([[ 0.6719, -0.9030,  1.9954,  0.8308,  0.2207,  1.4380,  0.8413]],
       dtype=torch.float64)
	q_value: tensor([[-19.7306]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.023823287881092003, distance: 1.1578950442342661 entropy 1.553932785987854
epoch: 9, step: 42
	action: tensor([[ 1.9057, -0.7254, -0.3918,  0.6499,  0.2747, -0.3191,  0.1179]],
       dtype=torch.float64)
	q_value: tensor([[-26.7243]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 9, step: 43
	action: tensor([[ 2.1918,  0.0317, -0.8376, -0.8245,  0.5929,  0.3449,  1.4172]],
       dtype=torch.float64)
	q_value: tensor([[-32.9770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 9, step: 44
	action: tensor([[ 1.4384, -0.1880, -1.2786, -0.4750,  0.1968,  1.0275, -0.0711]],
       dtype=torch.float64)
	q_value: tensor([[-32.9770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 9, step: 45
	action: tensor([[ 0.1076,  0.8466, -0.2356, -0.1922, -1.7294, -0.1433, -1.2944]],
       dtype=torch.float64)
	q_value: tensor([[-32.9770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 9, step: 46
	action: tensor([[ 2.2231, -0.7764,  0.3396, -1.5729, -0.7315, -0.5241,  1.6991]],
       dtype=torch.float64)
	q_value: tensor([[-32.9770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.43173197072244185, distance: 1.3692651497466402 entropy 1.553932785987854
epoch: 9, step: 47
	action: tensor([[ 0.8635, -0.2331, -0.1577, -1.9276, -0.5221,  0.8365, -1.6615]],
       dtype=torch.float64)
	q_value: tensor([[-28.8648]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1476928205410004, distance: 1.2259408758440133 entropy 1.553932785987854
epoch: 9, step: 48
	action: tensor([[-1.4124, -0.4016,  0.4928,  2.1878, -0.1428,  0.0531,  0.0067]],
       dtype=torch.float64)
	q_value: tensor([[-26.6717]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39669603904076645, distance: 0.8888424373388414 entropy 1.553932785987854
epoch: 9, step: 49
	action: tensor([[-0.6704,  0.1926,  0.9595, -0.7100,  0.3481, -0.0468, -0.1226]],
       dtype=torch.float64)
	q_value: tensor([[-25.1324]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8948737932942414, distance: 1.5752403869313734 entropy 1.553932785987854
epoch: 9, step: 50
	action: tensor([[ 1.6158,  0.7548,  0.0918,  0.6589, -0.7587, -0.2125,  0.1537]],
       dtype=torch.float64)
	q_value: tensor([[-18.4843]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 9, step: 51
	action: tensor([[-0.3624,  0.3524, -0.0202,  0.3310,  0.4660, -0.1988, -0.1682]],
       dtype=torch.float64)
	q_value: tensor([[-32.9770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 9, step: 52
	action: tensor([[ 2.5995,  0.3271, -1.5875, -0.7067, -0.9862,  0.5012,  0.2050]],
       dtype=torch.float64)
	q_value: tensor([[-32.9770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 9, step: 53
	action: tensor([[-1.6237,  1.0129, -1.0083,  0.4929,  0.4638, -1.8870,  0.0208]],
       dtype=torch.float64)
	q_value: tensor([[-32.9770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3307704733408503, distance: 1.320104378246971 entropy 1.553932785987854
epoch: 9, step: 54
	action: tensor([[-0.2889, -0.2918,  0.5130, -0.0435,  0.7057,  1.1587,  0.7546]],
       dtype=torch.float64)
	q_value: tensor([[-28.6239]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2767301265481069, distance: 0.97321071336726 entropy 1.553932785987854
epoch: 9, step: 55
	action: tensor([[ 0.5795, -1.9382, -0.2035, -0.6123, -1.5898,  0.0667, -0.6560]],
       dtype=torch.float64)
	q_value: tensor([[-21.2671]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 9, step: 56
	action: tensor([[-0.1514,  0.1532,  1.1229,  0.3995, -1.9857,  0.2628,  0.6708]],
       dtype=torch.float64)
	q_value: tensor([[-32.9770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 9, step: 57
	action: tensor([[ 0.9598, -0.2313,  0.5001,  2.8146,  0.9797,  0.6576, -1.4585]],
       dtype=torch.float64)
	q_value: tensor([[-32.9770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 9, step: 58
	action: tensor([[-0.4359, -1.1308,  0.4980, -1.7716, -0.4985, -0.4839,  0.2566]],
       dtype=torch.float64)
	q_value: tensor([[-32.9770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4641692693291015, distance: 1.3846893023074291 entropy 1.553932785987854
epoch: 9, step: 59
	action: tensor([[ 0.2921, -1.2452, -1.7616,  1.1599, -0.1562,  0.2014, -0.2934]],
       dtype=torch.float64)
	q_value: tensor([[-25.2143]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8157376673517909, distance: 1.5419959917265549 entropy 1.553932785987854
epoch: 9, step: 60
	action: tensor([[ 2.2573, -0.3780,  1.5124, -0.8104,  1.0144,  0.2948,  0.7325]],
       dtype=torch.float64)
	q_value: tensor([[-18.4622]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 9, step: 61
	action: tensor([[ 0.3571, -0.1294, -0.0166, -1.4147, -0.4984, -1.7149,  1.2600]],
       dtype=torch.float64)
	q_value: tensor([[-32.9770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20501645847846806, distance: 1.0203185020880194 entropy 1.553932785987854
epoch: 9, step: 62
	action: tensor([[-0.2827,  1.6938,  0.6172, -0.6782,  0.2246, -0.7610,  1.0734]],
       dtype=torch.float64)
	q_value: tensor([[-25.6146]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 9, step: 63
	action: tensor([[-0.3729, -1.4232,  2.3562, -0.7746, -1.0057,  1.0593,  0.2699]],
       dtype=torch.float64)
	q_value: tensor([[-32.9770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5616786115862862, distance: 1.430054287630237 entropy 1.553932785987854
epoch: 9, step: 64
	action: tensor([[ 0.3441,  0.1792,  1.2070, -0.4348,  0.2104,  1.7777,  0.4063]],
       dtype=torch.float64)
	q_value: tensor([[-31.2234]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8361432718590454, distance: 0.46322163231812874 entropy 1.553932785987854
epoch: 9, step: 65
	action: tensor([[ 0.0658, -1.0300, -1.4771, -1.1850,  2.1813,  0.1387,  1.7281]],
       dtype=torch.float64)
	q_value: tensor([[-26.0120]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23269295087476471, distance: 1.002400529226688 entropy 1.553932785987854
epoch: 9, step: 66
	action: tensor([[ 0.3747, -1.3274,  0.7789, -1.2132, -0.0652,  0.4545, -2.4965]],
       dtype=torch.float64)
	q_value: tensor([[-35.8823]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 9, step: 67
	action: tensor([[ 1.5196, -2.7762,  0.8772, -1.4345,  1.2062, -0.5265, -0.5590]],
       dtype=torch.float64)
	q_value: tensor([[-32.9770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 9, step: 68
	action: tensor([[ 0.7219, -1.8341, -0.2421, -0.0726,  2.1545, -0.0742,  1.7170]],
       dtype=torch.float64)
	q_value: tensor([[-32.9770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 9, step: 69
	action: tensor([[ 1.2095,  0.4344,  1.3038,  0.1148, -0.0073,  1.1899,  1.4931]],
       dtype=torch.float64)
	q_value: tensor([[-32.9770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 9, step: 70
	action: tensor([[ 0.6195,  1.7736, -0.8233,  0.2217,  1.4118,  0.8331,  0.7936]],
       dtype=torch.float64)
	q_value: tensor([[-32.9770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 9, step: 71
	action: tensor([[-1.6907, -1.8783, -0.7659,  1.1036,  1.3625, -0.3452, -0.9749]],
       dtype=torch.float64)
	q_value: tensor([[-32.9770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 9, step: 72
	action: tensor([[-0.0892,  1.5881, -2.4843,  1.0244, -1.1800, -0.0954, -0.0905]],
       dtype=torch.float64)
	q_value: tensor([[-32.9770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 9, step: 73
	action: tensor([[ 0.8046,  1.2257, -0.2320,  0.3559,  1.2930,  1.2899, -1.8237]],
       dtype=torch.float64)
	q_value: tensor([[-32.9770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 9, step: 74
	action: tensor([[-0.6895,  1.7442,  0.1170, -0.9751, -0.1316,  1.9495, -0.6656]],
       dtype=torch.float64)
	q_value: tensor([[-32.9770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 9, step: 75
	action: tensor([[ 0.2297,  0.2592, -1.0888,  1.0735, -1.6140, -0.3007,  0.6606]],
       dtype=torch.float64)
	q_value: tensor([[-32.9770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 9, step: 76
	action: tensor([[-0.1545,  0.6315, -0.7799,  0.4312,  1.4172,  1.1373,  1.7585]],
       dtype=torch.float64)
	q_value: tensor([[-32.9770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 9, step: 77
	action: tensor([[-0.3005,  1.7857, -0.9901,  0.6561, -0.0217,  1.7015,  2.3105]],
       dtype=torch.float64)
	q_value: tensor([[-32.9770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 9, step: 78
	action: tensor([[ 0.2389, -0.3440,  1.0851, -0.4173, -0.5114, -2.5617, -0.7955]],
       dtype=torch.float64)
	q_value: tensor([[-32.9770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10993600778519896, distance: 1.0796110207414473 entropy 1.553932785987854
epoch: 9, step: 79
	action: tensor([[-0.8387,  0.0245,  0.1192, -0.2802,  3.5285,  1.4337, -1.3155]],
       dtype=torch.float64)
	q_value: tensor([[-22.4614]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.5146256396002884 entropy 1.553932785987854
epoch: 9, step: 80
	action: tensor([[-0.4496,  0.7538, -0.5550, -0.7245, -1.3890,  0.3969,  1.5060]],
       dtype=torch.float64)
	q_value: tensor([[-32.9770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 9, step: 81
	action: tensor([[ 0.1367, -2.0557, -0.9753, -0.0170,  1.4670,  2.3539,  1.2644]],
       dtype=torch.float64)
	q_value: tensor([[-32.9770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 9, step: 82
	action: tensor([[ 0.9773, -0.3855, -1.1688, -0.0369,  0.1765,  1.1816,  0.0186]],
       dtype=torch.float64)
	q_value: tensor([[-32.9770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6636087648239948, distance: 0.6637110207326647 entropy 1.553932785987854
epoch: 9, step: 83
	action: tensor([[-2.5819, -0.0317,  1.9699,  1.7714, -0.8965, -0.3770,  1.3399]],
       dtype=torch.float64)
	q_value: tensor([[-19.1150]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 9, step: 84
	action: tensor([[ 0.4601, -0.2357, -0.0563,  2.0810,  1.5116,  0.7071,  0.7510]],
       dtype=torch.float64)
	q_value: tensor([[-32.9770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 9, step: 85
	action: tensor([[-2.5087, -1.0137, -0.7174, -1.4747, -2.5935,  1.0100, -1.8697]],
       dtype=torch.float64)
	q_value: tensor([[-32.9770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 9, step: 86
	action: tensor([[ 0.9984, -1.5581, -0.4165,  1.5741,  0.3593,  1.1934,  0.0065]],
       dtype=torch.float64)
	q_value: tensor([[-32.9770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8098028169409625, distance: 0.4990668617721617 entropy 1.553932785987854
epoch: 9, step: 87
	action: tensor([[-0.5956, -1.0248,  1.9720, -1.3895, -0.9317, -2.1644,  0.8909]],
       dtype=torch.float64)
	q_value: tensor([[-18.8643]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.288173616766942, distance: 1.2988048249006383 entropy 1.553932785987854
epoch: 9, step: 88
	action: tensor([[ 0.7616,  0.8681, -0.9078, -0.5445, -3.0340,  0.3495,  2.4390]],
       dtype=torch.float64)
	q_value: tensor([[-30.2061]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9563639149030927, distance: 0.23904495547219082 entropy 1.553932785987854
epoch: 9, step: 89
	action: tensor([[-0.0584, -3.5114, -0.4025,  1.4416, -1.7398,  1.3557, -0.1374]],
       dtype=torch.float64)
	q_value: tensor([[-46.9447]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 9, step: 90
	action: tensor([[-0.5745,  1.2205,  0.6035, -0.5866,  0.9575,  0.9701,  0.1762]],
       dtype=torch.float64)
	q_value: tensor([[-32.9770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 9, step: 91
	action: tensor([[ 0.1536,  0.9198, -0.2819, -0.5477,  0.7702,  1.0843,  0.5651]],
       dtype=torch.float64)
	q_value: tensor([[-32.9770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 9, step: 92
	action: tensor([[-0.2448, -2.3577,  0.8137,  0.0374,  0.9194,  1.3241, -0.4140]],
       dtype=torch.float64)
	q_value: tensor([[-32.9770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 9, step: 93
	action: tensor([[-0.0319,  0.1016,  0.1566,  1.0858, -1.0798,  0.2922,  0.4424]],
       dtype=torch.float64)
	q_value: tensor([[-32.9770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 9, step: 94
	action: tensor([[-0.4327, -1.3737, -0.2550, -0.9633, -0.2545,  0.8742, -0.2172]],
       dtype=torch.float64)
	q_value: tensor([[-32.9770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8957302264024807, distance: 1.5755963303250555 entropy 1.553932785987854
epoch: 9, step: 95
	action: tensor([[-0.9082,  0.3228,  0.7565,  1.9381, -0.2275, -3.1546,  1.9123]],
       dtype=torch.float64)
	q_value: tensor([[-23.1304]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 9, step: 96
	action: tensor([[-0.2412, -1.1889, -1.0663,  1.5326,  0.3401,  0.4694,  0.2338]],
       dtype=torch.float64)
	q_value: tensor([[-32.9770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.42240639871452945, distance: 1.3647985170280537 entropy 1.553932785987854
epoch: 9, step: 97
	action: tensor([[ 0.4047,  2.4535,  0.8711,  0.9736, -1.3904,  0.1398,  0.9375]],
       dtype=torch.float64)
	q_value: tensor([[-18.1521]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 9, step: 98
	action: tensor([[ 0.4949,  0.3311, -1.0377,  0.9366, -0.2990, -0.1994, -2.9933]],
       dtype=torch.float64)
	q_value: tensor([[-32.9770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 9, step: 99
	action: tensor([[ 0.1838, -1.8239, -0.7708, -2.3303, -1.2635,  0.4596, -1.2712]],
       dtype=torch.float64)
	q_value: tensor([[-32.9770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 9, step: 100
	action: tensor([[-0.8508, -0.3473, -1.0076, -0.3536, -0.1041, -0.9291,  0.1501]],
       dtype=torch.float64)
	q_value: tensor([[-32.9770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3344602330812614, distance: 1.321933204650906 entropy 1.553932785987854
epoch: 9, step: 101
	action: tensor([[-2.3541, -0.8869,  1.8491, -0.1288,  2.3419,  0.8439, -0.6854]],
       dtype=torch.float64)
	q_value: tensor([[-21.0361]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 9, step: 102
	action: tensor([[-0.8933,  0.2761,  0.2243, -0.1716,  0.8583,  0.0814, -1.0735]],
       dtype=torch.float64)
	q_value: tensor([[-32.9770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7372030749545868, distance: 1.508280054518647 entropy 1.553932785987854
epoch: 9, step: 103
	action: tensor([[-0.3231, -0.3798,  2.0870,  0.1618,  0.1208,  1.2178, -0.0482]],
       dtype=torch.float64)
	q_value: tensor([[-20.6549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0059540005581397715, distance: 1.1409324547910094 entropy 1.553932785987854
epoch: 9, step: 104
	action: tensor([[-0.7428,  1.6030, -1.3938, -0.2294,  0.2970,  0.3057,  0.9186]],
       dtype=torch.float64)
	q_value: tensor([[-24.6635]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 9, step: 105
	action: tensor([[-0.2507,  0.5921, -1.0108,  0.1369, -1.1937,  0.8144,  0.8071]],
       dtype=torch.float64)
	q_value: tensor([[-32.9770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 9, step: 106
	action: tensor([[ 0.3451,  0.5929,  0.6726, -0.6639, -1.4269,  0.5142,  2.3918]],
       dtype=torch.float64)
	q_value: tensor([[-32.9770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 9, step: 107
	action: tensor([[ 0.4914, -0.2529,  0.8967, -0.2028, -1.2243,  2.3426,  2.2205]],
       dtype=torch.float64)
	q_value: tensor([[-32.9770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7409644185582209, distance: 0.5824201675805097 entropy 1.553932785987854
epoch: 9, step: 108
	action: tensor([[ 0.1440, -1.0340,  0.8403, -1.8259, -0.2343,  0.1312, -0.6522]],
       dtype=torch.float64)
	q_value: tensor([[-40.2482]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7461868991763381, distance: 1.5121750078074945 entropy 1.553932785987854
epoch: 9, step: 109
	action: tensor([[ 0.7716,  1.4912,  0.8161,  0.7787,  0.2854, -0.5689,  1.6460]],
       dtype=torch.float64)
	q_value: tensor([[-23.8370]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 9, step: 110
	action: tensor([[-2.1664, -1.4373,  0.3282, -1.6674, -1.0638, -0.2876, -0.8006]],
       dtype=torch.float64)
	q_value: tensor([[-32.9770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 9, step: 111
	action: tensor([[-0.5547, -1.9128,  0.3603, -0.4289, -0.1530,  1.2866,  2.7550]],
       dtype=torch.float64)
	q_value: tensor([[-32.9770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 9, step: 112
	action: tensor([[ 1.5465, -1.8216,  0.8533, -0.3026, -1.1684,  0.6597,  0.5785]],
       dtype=torch.float64)
	q_value: tensor([[-32.9770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 9, step: 113
	action: tensor([[-0.3257, -2.0010,  0.1127, -0.4850, -0.4723, -1.6659,  1.2067]],
       dtype=torch.float64)
	q_value: tensor([[-32.9770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 9, step: 114
	action: tensor([[ 0.9248, -1.7800, -0.9927,  0.5319, -0.7408, -0.2954, -0.0575]],
       dtype=torch.float64)
	q_value: tensor([[-32.9770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 9, step: 115
	action: tensor([[-0.8431, -0.6146, -0.3646, -0.1014, -0.2577, -0.3499,  0.8128]],
       dtype=torch.float64)
	q_value: tensor([[-32.9770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.010143897516587, distance: 1.6224460602007438 entropy 1.553932785987854
epoch: 9, step: 116
	action: tensor([[ 0.3564,  0.2218,  2.0838,  1.6852, -1.4045,  1.5679,  0.5941]],
       dtype=torch.float64)
	q_value: tensor([[-19.5774]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 9, step: 117
	action: tensor([[-1.8932, -0.2772, -0.6801,  0.5127, -0.6099, -0.2528, -0.5819]],
       dtype=torch.float64)
	q_value: tensor([[-32.9770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 9, step: 118
	action: tensor([[ 0.1992, -2.3578, -0.3881,  0.2197, -0.4145,  0.7615,  1.0089]],
       dtype=torch.float64)
	q_value: tensor([[-32.9770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 9, step: 119
	action: tensor([[-1.8041, -0.1725, -0.8592,  1.4971, -1.2531, -0.6720, -2.2399]],
       dtype=torch.float64)
	q_value: tensor([[-32.9770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 9, step: 120
	action: tensor([[ 0.3737, -0.4504, -1.1277,  0.2354, -1.4694, -1.0422,  1.9838]],
       dtype=torch.float64)
	q_value: tensor([[-32.9770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5599402945849187, distance: 0.7591236033690332 entropy 1.553932785987854
epoch: 9, step: 121
	action: tensor([[ 0.3225, -1.0223,  0.4599, -0.1677,  2.2738,  1.2420, -2.3670]],
       dtype=torch.float64)
	q_value: tensor([[-32.3335]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5718016264651484, distance: 1.4346817048371359 entropy 1.553932785987854
epoch: 9, step: 122
	action: tensor([[-1.0730, -0.8632,  0.0190,  0.8491, -0.4659, -0.2628,  0.4341]],
       dtype=torch.float64)
	q_value: tensor([[-33.5347]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0708691523494887, distance: 1.6467702884752573 entropy 1.553932785987854
epoch: 9, step: 123
	action: tensor([[ 0.2252, -0.9154, -0.3179, -0.1840, -1.1054, -0.5989,  0.0720]],
       dtype=torch.float64)
	q_value: tensor([[-19.5748]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3289844850362458, distance: 1.3192182440089397 entropy 1.553932785987854
epoch: 9, step: 124
	action: tensor([[ 0.1413, -0.6281, -1.1794, -0.9628, -2.3546, -1.5444,  1.2465]],
       dtype=torch.float64)
	q_value: tensor([[-17.6926]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41302295076013607, distance: 0.8767327981774705 entropy 1.553932785987854
epoch: 9, step: 125
	action: tensor([[ 0.3928, -0.4897,  0.5632, -2.9444, -1.2311, -1.1872,  1.0128]],
       dtype=torch.float64)
	q_value: tensor([[-35.9121]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 9, step: 126
	action: tensor([[ 1.5611, -3.2269, -0.5836,  0.5490,  0.1771,  1.0692,  0.9057]],
       dtype=torch.float64)
	q_value: tensor([[-32.9770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 9, step: 127
	action: tensor([[ 0.3168,  0.3963, -2.2887,  0.6844,  0.2030,  2.0077,  2.2134]],
       dtype=torch.float64)
	q_value: tensor([[-32.9770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
LOSS epoch 9 actor 204.24183825301594 critic 374.45161975595414 
epoch: 10, step: 0
	action: tensor([[ 0.0309,  0.4358, -0.3777,  0.2299, -0.9396, -1.1397, -0.2812]],
       dtype=torch.float64)
	q_value: tensor([[-48.4427]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 10, step: 1
	action: tensor([[-0.2762,  1.2372, -0.4127,  1.1905,  1.9107,  0.9724,  0.3495]],
       dtype=torch.float64)
	q_value: tensor([[-48.4427]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 10, step: 2
	action: tensor([[-1.3413, -1.1836,  0.8051, -0.0032, -0.2336, -1.4789, -0.4797]],
       dtype=torch.float64)
	q_value: tensor([[-48.4427]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9895121739067512, distance: 1.6140983507034068 entropy 1.553932785987854
epoch: 10, step: 3
	action: tensor([[-1.6705,  0.5152, -0.9247, -0.2881,  0.8627, -1.2702, -0.0767]],
       dtype=torch.float64)
	q_value: tensor([[-34.0836]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0858417920552381, distance: 1.6527127429242214 entropy 1.553932785987854
epoch: 10, step: 4
	action: tensor([[ 0.1277, -0.4288, -1.8627, -0.7455,  1.4334,  1.5564,  0.5946]],
       dtype=torch.float64)
	q_value: tensor([[-38.5707]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6365763501579631, distance: 0.6898636884578074 entropy 1.553932785987854
epoch: 10, step: 5
	action: tensor([[-0.1912, -2.2723,  0.3850, -0.6207, -1.3999,  0.6685,  0.7692]],
       dtype=torch.float64)
	q_value: tensor([[-47.5660]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 10, step: 6
	action: tensor([[-0.3795,  0.7451, -1.1444,  2.9285, -0.0952,  0.7300,  0.7648]],
       dtype=torch.float64)
	q_value: tensor([[-48.4427]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 10, step: 7
	action: tensor([[-0.8580,  0.2146,  1.1286, -0.8281,  0.4484, -0.9123,  0.4905]],
       dtype=torch.float64)
	q_value: tensor([[-48.4427]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.977479541969065, distance: 1.6092098894132452 entropy 1.553932785987854
epoch: 10, step: 8
	action: tensor([[-0.4425, -1.6793, -1.5375, -0.7723, -1.2098, -0.7214,  0.5175]],
       dtype=torch.float64)
	q_value: tensor([[-29.6252]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 10, step: 9
	action: tensor([[ 0.0846, -0.8635, -0.3180, -1.5613,  0.3421, -0.3175, -1.2325]],
       dtype=torch.float64)
	q_value: tensor([[-48.4427]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4571377782280477, distance: 1.3813604016249958 entropy 1.553932785987854
epoch: 10, step: 10
	action: tensor([[ 0.3312, -0.0995, -0.2829, -0.4578,  1.2584, -0.4296,  0.2903]],
       dtype=torch.float64)
	q_value: tensor([[-33.3275]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16898814285945385, distance: 1.0431825254763472 entropy 1.553932785987854
epoch: 10, step: 11
	action: tensor([[-1.1332, -1.3743, -0.7376, -1.8234,  0.2219,  0.1715,  0.4557]],
       dtype=torch.float64)
	q_value: tensor([[-25.0171]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2994056486052782, distance: 0.957833470622154 entropy 1.553932785987854
epoch: 10, step: 12
	action: tensor([[-0.3312, -1.0108, -1.8623, -1.9096, -1.3714,  1.8308, -0.2797]],
       dtype=torch.float64)
	q_value: tensor([[-45.7797]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2167250700152874, distance: 1.0127769451179807 entropy 1.553932785987854
epoch: 10, step: 13
	action: tensor([[-0.8627, -0.7419,  0.9530,  1.7990, -1.1247, -2.2799, -1.6834]],
       dtype=torch.float64)
	q_value: tensor([[-58.4932]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0675497284357729, distance: 1.1050184598036379 entropy 1.553932785987854
epoch: 10, step: 14
	action: tensor([[-1.0302, -0.2098, -2.1564,  0.9152, -0.8602,  1.1400,  2.1704]],
       dtype=torch.float64)
	q_value: tensor([[-47.4307]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0242301793424078, distance: 1.628120861350953 entropy 1.553932785987854
epoch: 10, step: 15
	action: tensor([[-1.7249, -0.1719,  0.4502, -0.3122,  0.9354, -0.6929, -1.7267]],
       dtype=torch.float64)
	q_value: tensor([[-58.8650]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.830082125457213, distance: 1.9251129366985849 entropy 1.553932785987854
epoch: 10, step: 16
	action: tensor([[-0.3015, -1.5656, -0.3861,  0.4025, -0.8316,  1.5612, -0.5385]],
       dtype=torch.float64)
	q_value: tensor([[-39.9545]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 10, step: 17
	action: tensor([[-1.6393,  0.0455,  1.8349,  0.3754,  1.8894,  0.0396, -2.9908]],
       dtype=torch.float64)
	q_value: tensor([[-48.4427]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.445527273018028, distance: 1.7895463386754897 entropy 1.553932785987854
epoch: 10, step: 18
	action: tensor([[ 0.7113, -0.4921, -0.3670, -0.9331,  0.3490, -0.4314,  0.4199]],
       dtype=torch.float64)
	q_value: tensor([[-55.6904]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.33179338625915356, distance: 1.32061163793512 entropy 1.553932785987854
epoch: 10, step: 19
	action: tensor([[ 0.2566,  0.8718,  2.6678,  0.5122,  1.7388, -0.3890, -0.1802]],
       dtype=torch.float64)
	q_value: tensor([[-24.2467]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 10, step: 20
	action: tensor([[-0.5877, -0.2791, -0.2125,  1.1026,  1.1122,  0.6659,  0.5302]],
       dtype=torch.float64)
	q_value: tensor([[-48.4427]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21970154241066708, distance: 1.0108508194610426 entropy 1.553932785987854
epoch: 10, step: 21
	action: tensor([[ 0.8543, -1.4531, -0.4364, -0.0436, -1.2289, -1.1082,  0.5282]],
       dtype=torch.float64)
	q_value: tensor([[-26.8237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4243751340304074, distance: 1.365742690911516 entropy 1.553932785987854
epoch: 10, step: 22
	action: tensor([[ 0.4241,  1.0485, -0.1139,  0.8710, -0.0992,  1.8351,  1.1518]],
       dtype=torch.float64)
	q_value: tensor([[-31.0200]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 10, step: 23
	action: tensor([[-0.4336, -2.3902,  0.6505, -0.1631, -2.0098, -0.7635,  1.1185]],
       dtype=torch.float64)
	q_value: tensor([[-48.4427]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 10, step: 24
	action: tensor([[ 1.6969, -0.3981,  1.2448, -0.8706, -1.2815,  0.0947,  0.1002]],
       dtype=torch.float64)
	q_value: tensor([[-48.4427]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3131946107951076, distance: 0.9483606822670051 entropy 1.553932785987854
epoch: 10, step: 25
	action: tensor([[-1.2055,  1.8560, -0.1835,  1.5480, -0.2260, -0.6902,  1.5726]],
       dtype=torch.float64)
	q_value: tensor([[-30.7836]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 10, step: 26
	action: tensor([[ 1.0302, -0.5527,  0.3589,  1.3231,  1.0333, -1.2994, -0.0282]],
       dtype=torch.float64)
	q_value: tensor([[-48.4427]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8341393286525126, distance: 0.46604558995176476 entropy 1.553932785987854
epoch: 10, step: 27
	action: tensor([[ 0.1382, -3.5361, -2.3174,  0.5887, -0.4715,  2.1648,  4.0359]],
       dtype=torch.float64)
	q_value: tensor([[-25.5302]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 10, step: 28
	action: tensor([[-0.7096, -0.8843,  0.3065, -0.5192,  1.5239, -0.2186,  2.3144]],
       dtype=torch.float64)
	q_value: tensor([[-48.4427]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2729442116690333, distance: 1.725246207601686 entropy 1.553932785987854
epoch: 10, step: 29
	action: tensor([[ 0.0793,  1.4994,  1.2324, -1.8004,  0.4800,  1.2944, -1.4099]],
       dtype=torch.float64)
	q_value: tensor([[-42.9172]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27460688585319837, distance: 0.9746381517136273 entropy 1.553932785987854
epoch: 10, step: 30
	action: tensor([[ 0.7975, -0.7363, -0.5306,  2.1914, -0.1965, -0.4015,  1.8318]],
       dtype=torch.float64)
	q_value: tensor([[-43.9984]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 10, step: 31
	action: tensor([[-1.0066, -0.0654, -0.7482, -1.8533, -0.0381, -1.3913, -1.1730]],
       dtype=torch.float64)
	q_value: tensor([[-48.4427]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31352300928672716, distance: 0.9481339241049153 entropy 1.553932785987854
epoch: 10, step: 32
	action: tensor([[ 0.6414, -0.8886, -1.1537,  0.8883, -0.5864, -0.0783, -0.0818]],
       dtype=torch.float64)
	q_value: tensor([[-42.7648]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.017103048317747227, distance: 1.1540886534416688 entropy 1.553932785987854
epoch: 10, step: 33
	action: tensor([[ 0.4870,  0.8410, -0.7750,  3.7167, -0.3747, -1.1230, -0.2568]],
       dtype=torch.float64)
	q_value: tensor([[-23.0534]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 10, step: 34
	action: tensor([[ 0.4299, -0.6345, -2.9701,  0.2258, -1.3599, -2.1452, -0.1759]],
       dtype=torch.float64)
	q_value: tensor([[-48.4427]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 10, step: 35
	action: tensor([[-1.5870, -0.8809, -0.7045, -1.5697, -0.1844,  0.1996,  0.0477]],
       dtype=torch.float64)
	q_value: tensor([[-48.4427]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2747076704403051, distance: 1.2919984506037578 entropy 1.553932785987854
epoch: 10, step: 36
	action: tensor([[ 1.7089,  0.4254, -0.2857,  0.6346, -0.0279,  0.6351, -0.4376]],
       dtype=torch.float64)
	q_value: tensor([[-46.1088]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8321441182154599, distance: 0.46884034319827766 entropy 1.553932785987854
epoch: 10, step: 37
	action: tensor([[ 0.6874,  0.3635, -0.0420,  0.9447,  1.0958,  0.2046, -0.6123]],
       dtype=torch.float64)
	q_value: tensor([[-19.5462]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9101680253187558, distance: 0.34298266180741765 entropy 1.553932785987854
epoch: 10, step: 38
	action: tensor([[-1.4736, -0.4172,  1.9652,  0.7033, -0.4008, -1.0776,  1.2468]],
       dtype=torch.float64)
	q_value: tensor([[-21.4712]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 10, step: 39
	action: tensor([[ 0.2364,  1.3689, -0.8426,  2.7453,  1.0181, -0.6322,  1.1489]],
       dtype=torch.float64)
	q_value: tensor([[-48.4427]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 10, step: 40
	action: tensor([[ 0.4057, -0.4551, -0.9880, -0.6337, -0.6982,  0.3492, -0.1923]],
       dtype=torch.float64)
	q_value: tensor([[-48.4427]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09836757180006683, distance: 1.0866043889244512 entropy 1.553932785987854
epoch: 10, step: 41
	action: tensor([[-0.8559,  0.1482,  0.4434,  0.1781,  0.0457,  1.1319,  1.3595]],
       dtype=torch.float64)
	q_value: tensor([[-25.9887]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.006576847657031926, distance: 1.140574957340958 entropy 1.553932785987854
epoch: 10, step: 42
	action: tensor([[-0.0952,  2.6459, -0.0340, -1.9260, -1.3757,  0.2330,  0.4497]],
       dtype=torch.float64)
	q_value: tensor([[-38.2205]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 10, step: 43
	action: tensor([[-0.8330, -0.4095, -0.1463,  2.0182, -0.1912, -0.7434, -0.0894]],
       dtype=torch.float64)
	q_value: tensor([[-48.4427]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03166767002511861, distance: 1.1260791290898475 entropy 1.553932785987854
epoch: 10, step: 44
	action: tensor([[-0.6872, -0.4141,  1.6451,  3.0526,  0.9828, -1.2761,  2.4325]],
       dtype=torch.float64)
	q_value: tensor([[-30.4376]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 10, step: 45
	action: tensor([[-1.3377, -0.6580,  0.1566,  1.7762, -0.7800,  0.6530,  1.1981]],
       dtype=torch.float64)
	q_value: tensor([[-48.4427]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15120113835357607, distance: 1.2278132010364209 entropy 1.553932785987854
epoch: 10, step: 46
	action: tensor([[-1.2890, -0.0714,  0.6149, -0.8128, -0.2906,  1.4986,  1.7091]],
       dtype=torch.float64)
	q_value: tensor([[-40.8780]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0249906774105773, distance: 1.6284266730389616 entropy 1.553932785987854
epoch: 10, step: 47
	action: tensor([[-1.3395,  0.0388, -0.1963, -0.0228, -1.5904,  0.5021,  0.6987]],
       dtype=torch.float64)
	q_value: tensor([[-52.3697]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.47308193948161, distance: 1.7995998413595038 entropy 1.553932785987854
epoch: 10, step: 48
	action: tensor([[ 1.5557, -1.1105,  0.7351,  0.9190, -1.3272, -0.7487,  1.8813]],
       dtype=torch.float64)
	q_value: tensor([[-43.5040]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5563978940054548, distance: 1.4276344211198073 entropy 1.553932785987854
epoch: 10, step: 49
	action: tensor([[ 0.1062, -0.4543, -0.8137, -0.7577, -1.2809,  0.2908,  0.4339]],
       dtype=torch.float64)
	q_value: tensor([[-41.7704]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09336285699571256, distance: 1.1965720422477946 entropy 1.553932785987854
epoch: 10, step: 50
	action: tensor([[ 1.6155, -0.2345, -2.5978, -1.2257, -2.6434, -1.8158,  1.3839]],
       dtype=torch.float64)
	q_value: tensor([[-34.1503]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.587206830092117, distance: 0.7352295347538894 entropy 1.553932785987854
epoch: 10, step: 51
	action: tensor([[ 0.0670,  0.2591, -2.0401,  0.5331, -1.8456,  0.2459,  1.8855]],
       dtype=torch.float64)
	q_value: tensor([[-62.4954]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19777055131927845, distance: 1.0249578202908451 entropy 1.553932785987854
epoch: 10, step: 52
	action: tensor([[ 0.5796, -0.3759,  0.1824,  0.5458,  0.3361, -0.1258, -1.1703]],
       dtype=torch.float64)
	q_value: tensor([[-53.9740]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6588229131820648, distance: 0.6684156682295491 entropy 1.553932785987854
epoch: 10, step: 53
	action: tensor([[ 0.0289, -0.0598, -0.0028,  0.7389, -0.3669, -0.0078, -1.6684]],
       dtype=torch.float64)
	q_value: tensor([[-19.9335]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 10, step: 54
	action: tensor([[-0.5769, -1.2898, -2.5583, -0.3133,  0.5066,  0.7257, -1.9497]],
       dtype=torch.float64)
	q_value: tensor([[-48.4427]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0951447424043717, distance: 1.6563942300228205 entropy 1.553932785987854
epoch: 10, step: 55
	action: tensor([[ 2.4354,  1.3303,  0.3015,  0.3318, -0.5732, -0.6985, -0.7519]],
       dtype=torch.float64)
	q_value: tensor([[-44.6545]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 10, step: 56
	action: tensor([[ 1.9308, -0.2252, -1.0549, -0.5262,  1.5085, -0.6522, -0.0345]],
       dtype=torch.float64)
	q_value: tensor([[-48.4427]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 10, step: 57
	action: tensor([[-0.0391,  1.1620,  0.6443, -0.6471, -3.2429,  0.2668,  0.4135]],
       dtype=torch.float64)
	q_value: tensor([[-48.4427]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 10, step: 58
	action: tensor([[ 1.8432,  0.8345, -0.3021,  0.8290, -2.5378, -3.2306, -1.2687]],
       dtype=torch.float64)
	q_value: tensor([[-48.4427]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 10, step: 59
	action: tensor([[-0.4035, -1.5437, -0.3556,  1.2153,  0.6564,  2.6734,  0.5441]],
       dtype=torch.float64)
	q_value: tensor([[-48.4427]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30858070081577704, distance: 0.9515408601148901 entropy 1.553932785987854
epoch: 10, step: 60
	action: tensor([[-0.5463, -0.7938, -1.3874, -0.5685, -0.0983,  1.0118,  1.6935]],
       dtype=torch.float64)
	q_value: tensor([[-38.4010]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7259250949402678, distance: 1.5033761804464256 entropy 1.553932785987854
epoch: 10, step: 61
	action: tensor([[ 1.1517,  2.0262,  0.0184, -0.2788,  0.1899,  0.5963, -0.3367]],
       dtype=torch.float64)
	q_value: tensor([[-47.3441]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 10, step: 62
	action: tensor([[ 2.0260, -0.2660, -0.7135, -0.5784,  1.9514, -0.7788,  0.4394]],
       dtype=torch.float64)
	q_value: tensor([[-48.4427]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 10, step: 63
	action: tensor([[-2.1334,  0.2179, -0.1981,  2.1109,  2.1417,  0.9739, -0.4694]],
       dtype=torch.float64)
	q_value: tensor([[-48.4427]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 10, step: 64
	action: tensor([[ 0.7385,  0.0491, -0.8090, -1.8469, -0.6185, -2.4425,  1.3803]],
       dtype=torch.float64)
	q_value: tensor([[-48.4427]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 10, step: 65
	action: tensor([[ 0.7886,  0.3091,  1.3494, -0.9588, -0.6317, -0.8699, -0.6426]],
       dtype=torch.float64)
	q_value: tensor([[-48.4427]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5166084168664082, distance: 0.7956209959114711 entropy 1.553932785987854
epoch: 10, step: 66
	action: tensor([[-1.3759, -0.0232,  0.4758, -1.2868,  0.7556,  0.5922,  1.4481]],
       dtype=torch.float64)
	q_value: tensor([[-26.5769]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.414583341634235, distance: 1.7781884839721211 entropy 1.553932785987854
epoch: 10, step: 67
	action: tensor([[ 0.9180,  0.4541, -1.0763, -1.4218, -1.8559,  0.6394,  1.5488]],
       dtype=torch.float64)
	q_value: tensor([[-46.7888]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4172320262896829, distance: 0.8735837194782372 entropy 1.553932785987854
epoch: 10, step: 68
	action: tensor([[-2.1118, -3.5705, -2.2061, -1.6950, -0.5370, -0.3040, -0.7386]],
       dtype=torch.float64)
	q_value: tensor([[-52.5263]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 10, step: 69
	action: tensor([[-1.5630, -0.0675, -1.5881,  2.0494, -0.3604,  1.4721, -1.7354]],
       dtype=torch.float64)
	q_value: tensor([[-48.4427]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 10, step: 70
	action: tensor([[ 1.2151, -1.0495, -1.4808,  1.6314, -1.0049,  1.3965, -0.8852]],
       dtype=torch.float64)
	q_value: tensor([[-48.4427]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12372509852102931, distance: 1.071215577742503 entropy 1.553932785987854
epoch: 10, step: 71
	action: tensor([[-0.2715, -2.2626,  0.9332, -1.0647,  1.3915, -1.6505,  1.0093]],
       dtype=torch.float64)
	q_value: tensor([[-32.4780]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 10, step: 72
	action: tensor([[ 0.0523,  0.3398,  0.5493, -0.8359, -0.1505, -0.1986, -1.0066]],
       dtype=torch.float64)
	q_value: tensor([[-48.4427]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05193813030062977, distance: 1.1142304767768745 entropy 1.553932785987854
epoch: 10, step: 73
	action: tensor([[-0.1590,  1.1971, -0.5539, -0.4059,  0.3342,  2.1488, -3.2322]],
       dtype=torch.float64)
	q_value: tensor([[-23.5312]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 10, step: 74
	action: tensor([[ 0.8005, -0.4398, -1.0476, -1.7571, -0.2177,  0.5266, -0.0230]],
       dtype=torch.float64)
	q_value: tensor([[-48.4427]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 10, step: 75
	action: tensor([[ 1.4703, -1.1065, -0.7503, -0.6337, -1.5894,  0.2672, -0.8061]],
       dtype=torch.float64)
	q_value: tensor([[-48.4427]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9749705174562928, distance: 1.6081886832527548 entropy 1.553932785987854
epoch: 10, step: 76
	action: tensor([[-2.8969, -0.8974,  0.4189, -0.6466,  1.4105, -0.8319,  1.7542]],
       dtype=torch.float64)
	q_value: tensor([[-32.6541]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 10, step: 77
	action: tensor([[ 1.3205, -0.9635,  1.7735, -1.2868,  0.2038, -0.9490,  0.4228]],
       dtype=torch.float64)
	q_value: tensor([[-48.4427]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 10, step: 78
	action: tensor([[ 0.7119, -1.3741, -0.3281, -0.5788, -1.5960,  0.4125, -0.6070]],
       dtype=torch.float64)
	q_value: tensor([[-48.4427]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9213820302185776, distance: 1.5862204900381887 entropy 1.553932785987854
epoch: 10, step: 79
	action: tensor([[ 0.0748, -0.6628, -1.6312, -0.4018,  1.6936,  0.6030,  1.5262]],
       dtype=torch.float64)
	q_value: tensor([[-32.5795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24211716786287207, distance: 0.9962256696244626 entropy 1.553932785987854
epoch: 10, step: 80
	action: tensor([[-0.9265,  0.3053,  0.4124, -0.6610,  3.0587,  1.7455,  0.0450]],
       dtype=torch.float64)
	q_value: tensor([[-44.6157]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.40820001765561753, distance: 1.3579659047177985 entropy 1.553932785987854
epoch: 10, step: 81
	action: tensor([[ 1.0420,  0.0279,  0.3908,  0.8409,  0.4142, -1.2765,  2.1158]],
       dtype=torch.float64)
	q_value: tensor([[-53.8624]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8824542701307646, distance: 0.39233777223359423 entropy 1.553932785987854
epoch: 10, step: 82
	action: tensor([[-1.5187, -0.1050, -0.0736, -1.5900, -1.0490,  1.5019,  2.8993]],
       dtype=torch.float64)
	q_value: tensor([[-36.5828]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0871384460687596, distance: 1.6532263637743114 entropy 1.553932785987854
epoch: 10, step: 83
	action: tensor([[ 1.3021,  0.6965, -0.3357, -2.0008, -0.2998,  1.6001,  0.6734]],
       dtype=torch.float64)
	q_value: tensor([[-74.2073]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6655103776631225, distance: 0.6618323895252173 entropy 1.553932785987854
epoch: 10, step: 84
	action: tensor([[ 1.9727, -0.9925, -1.7493, -0.0035,  0.6369,  1.1419,  0.9095]],
       dtype=torch.float64)
	q_value: tensor([[-46.8982]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 10, step: 85
	action: tensor([[-0.6971, -0.5526, -2.5418,  1.1331, -0.4293,  0.1995, -2.7353]],
       dtype=torch.float64)
	q_value: tensor([[-48.4427]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.205534386206753, distance: 1.6994704198536383 entropy 1.553932785987854
epoch: 10, step: 86
	action: tensor([[-1.6271, -0.2389, -1.6868, -2.1552, -0.8799,  1.1397,  0.9227]],
       dtype=torch.float64)
	q_value: tensor([[-41.7020]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11528222634129248, distance: 1.2085067593964678 entropy 1.553932785987854
epoch: 10, step: 87
	action: tensor([[-0.8661, -0.3372,  0.9491,  0.1786, -0.1219, -0.9055,  0.3262]],
       dtype=torch.float64)
	q_value: tensor([[-67.3336]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1166732819399394, distance: 1.6648825722986897 entropy 1.553932785987854
epoch: 10, step: 88
	action: tensor([[ 0.1891, -1.3780, -0.1063, -1.2793,  0.9273,  0.4215,  0.3757]],
       dtype=torch.float64)
	q_value: tensor([[-25.4583]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7783469706506114, distance: 1.5260365763413184 entropy 1.553932785987854
epoch: 10, step: 89
	action: tensor([[-0.0117,  0.3234,  0.5225, -0.7535, -0.2023, -0.1590,  0.3825]],
       dtype=torch.float64)
	q_value: tensor([[-33.7282]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.051068495420747206, distance: 1.1732004007838297 entropy 1.553932785987854
epoch: 10, step: 90
	action: tensor([[-0.8949, -0.2135,  0.7888,  0.3644,  1.1223, -0.9092, -1.1321]],
       dtype=torch.float64)
	q_value: tensor([[-23.4732]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8635447181425411, distance: 1.5621639184043206 entropy 1.553932785987854
epoch: 10, step: 91
	action: tensor([[ 0.0449, -1.3288, -1.5422,  0.2358, -0.2371,  0.6282,  0.7885]],
       dtype=torch.float64)
	q_value: tensor([[-31.5123]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9878542694620966, distance: 1.613425678629802 entropy 1.553932785987854
epoch: 10, step: 92
	action: tensor([[-1.1405,  0.1080, -0.8056,  2.4513, -0.7127,  1.5887, -1.9046]],
       dtype=torch.float64)
	q_value: tensor([[-33.2925]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.060912365564153914, distance: 1.1786814507564338 entropy 1.553932785987854
epoch: 10, step: 93
	action: tensor([[-0.5716, -0.5749, -0.4663,  0.1982,  0.1174, -1.4443,  0.4054]],
       dtype=torch.float64)
	q_value: tensor([[-42.3876]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4970723892418174, distance: 1.40016137698917 entropy 1.553932785987854
epoch: 10, step: 94
	action: tensor([[-1.3622, -0.6447, -0.5819, -0.3899,  0.2156, -0.3903, -0.7663]],
       dtype=torch.float64)
	q_value: tensor([[-27.0804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.3448076302699654, distance: 1.752307430736649 entropy 1.553932785987854
epoch: 10, step: 95
	action: tensor([[-0.5560, -1.8925, -1.3588, -1.0062, -0.3684, -0.3759,  1.1555]],
       dtype=torch.float64)
	q_value: tensor([[-32.1811]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 10, step: 96
	action: tensor([[ 0.7652,  0.2760, -1.0372, -0.5220,  0.9784,  1.6850,  1.2886]],
       dtype=torch.float64)
	q_value: tensor([[-48.4427]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 10, step: 97
	action: tensor([[-0.2518,  0.8766,  1.4495, -0.1783, -0.3588,  0.9780, -0.4347]],
       dtype=torch.float64)
	q_value: tensor([[-48.4427]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 10, step: 98
	action: tensor([[ 1.4957, -1.4912, -0.4298,  3.1463, -1.4531,  1.1123,  1.4481]],
       dtype=torch.float64)
	q_value: tensor([[-48.4427]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 10, step: 99
	action: tensor([[-1.5048, -0.0845, -2.6139, -1.4239, -2.2880, -1.9440, -2.2245]],
       dtype=torch.float64)
	q_value: tensor([[-48.4427]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.634584936629019, distance: 1.4630542817399697 entropy 1.553932785987854
epoch: 10, step: 100
	action: tensor([[ 0.1049, -0.7992, -2.0277, -0.3123,  0.6766, -0.7949, -1.1710]],
       dtype=torch.float64)
	q_value: tensor([[-66.3070]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10269754920610585, distance: 1.083992107759973 entropy 1.553932785987854
epoch: 10, step: 101
	action: tensor([[ 0.6966, -0.9494,  0.0264, -0.8308,  0.3272, -0.8136,  0.4639]],
       dtype=torch.float64)
	q_value: tensor([[-33.5555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.717947750961524, distance: 1.4998978026381624 entropy 1.553932785987854
epoch: 10, step: 102
	action: tensor([[ 0.3953, -1.8056,  1.6924,  0.2484,  1.3550,  0.4816, -1.2052]],
       dtype=torch.float64)
	q_value: tensor([[-23.9821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 10, step: 103
	action: tensor([[-0.3667,  1.3091,  1.1333, -1.8213, -0.5225, -0.2795, -0.8959]],
       dtype=torch.float64)
	q_value: tensor([[-48.4427]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3910664029826727, distance: 1.349679414774967 entropy 1.553932785987854
epoch: 10, step: 104
	action: tensor([[-1.3210, -0.4293, -0.8811,  1.2427, -1.4371,  0.9293,  0.4047]],
       dtype=torch.float64)
	q_value: tensor([[-38.1239]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2156602830188206, distance: 1.703367197800377 entropy 1.553932785987854
epoch: 10, step: 105
	action: tensor([[ 1.0584, -0.8247, -1.6660,  0.7634, -0.7912,  0.3574,  0.4379]],
       dtype=torch.float64)
	q_value: tensor([[-41.7207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.052266732188561926, distance: 1.1738689449554718 entropy 1.553932785987854
epoch: 10, step: 106
	action: tensor([[-0.7864, -1.5243,  2.5389, -0.0789, -2.2251, -0.6166, -0.0606]],
       dtype=torch.float64)
	q_value: tensor([[-29.5643]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 10, step: 107
	action: tensor([[ 1.0074,  1.0106, -0.0416,  0.6193,  0.5254, -0.5133, -1.5326]],
       dtype=torch.float64)
	q_value: tensor([[-48.4427]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 10, step: 108
	action: tensor([[ 0.4104,  0.4270, -1.1398, -0.5350,  2.9703, -0.4471, -1.1729]],
       dtype=torch.float64)
	q_value: tensor([[-48.4427]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8867368798559689, distance: 0.3851243283425629 entropy 1.553932785987854
epoch: 10, step: 109
	action: tensor([[ 0.0456, -3.0770,  0.9951, -0.1017,  0.7320, -2.6221,  0.3594]],
       dtype=torch.float64)
	q_value: tensor([[-46.3436]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 10, step: 110
	action: tensor([[ 0.6862,  0.3940, -0.4834, -0.6146,  1.8604, -0.1265,  0.5848]],
       dtype=torch.float64)
	q_value: tensor([[-48.4427]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 10, step: 111
	action: tensor([[ 0.3632, -1.5974,  2.4797, -0.4353, -0.0730,  4.3393, -2.2405]],
       dtype=torch.float64)
	q_value: tensor([[-48.4427]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 10, step: 112
	action: tensor([[ 1.8213, -0.1114, -1.1714, -1.1798, -0.6956, -2.9039, -1.2067]],
       dtype=torch.float64)
	q_value: tensor([[-48.4427]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 10, step: 113
	action: tensor([[-0.8990, -1.7052, -0.7053,  1.0193,  0.7051,  0.9425,  0.0258]],
       dtype=torch.float64)
	q_value: tensor([[-48.4427]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8393899314888584, distance: 1.5520067141098004 entropy 1.553932785987854
epoch: 10, step: 114
	action: tensor([[ 1.2962, -2.2995,  0.3805,  0.4939, -0.1329,  1.7288, -2.2886]],
       dtype=torch.float64)
	q_value: tensor([[-29.3600]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 10, step: 115
	action: tensor([[-0.7424,  0.1429, -0.6718,  0.5579,  0.8426,  0.9454,  0.1112]],
       dtype=torch.float64)
	q_value: tensor([[-48.4427]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06003662683945743, distance: 1.1781948742375075 entropy 1.553932785987854
epoch: 10, step: 116
	action: tensor([[-1.6027,  1.6669,  0.3201, -0.1670,  1.9798, -0.8180, -0.5206]],
       dtype=torch.float64)
	q_value: tensor([[-28.8301]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 10, step: 117
	action: tensor([[-0.7798, -0.6389, -1.4488, -1.6885,  1.3104,  1.4869,  1.6941]],
       dtype=torch.float64)
	q_value: tensor([[-48.4427]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3672416494741354, distance: 0.9102813310904342 entropy 1.553932785987854
epoch: 10, step: 118
	action: tensor([[ 1.2191, -0.7918,  1.2246, -0.2085, -0.8321,  1.4156,  1.3751]],
       dtype=torch.float64)
	q_value: tensor([[-61.7030]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3153519735001551, distance: 0.9468700364482873 entropy 1.553932785987854
epoch: 10, step: 119
	action: tensor([[-0.9866,  1.6299, -0.4819,  0.7682,  2.6025,  0.2653, -2.0839]],
       dtype=torch.float64)
	q_value: tensor([[-40.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 10, step: 120
	action: tensor([[-0.4980, -0.0881,  1.5830,  0.5029,  0.2332, -0.0339, -0.0848]],
       dtype=torch.float64)
	q_value: tensor([[-48.4427]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.02046094355895567, distance: 1.13257656680761 entropy 1.553932785987854
epoch: 10, step: 121
	action: tensor([[-0.4702, -0.9677, -1.8949,  1.7643, -1.3003,  2.3274, -0.1108]],
       dtype=torch.float64)
	q_value: tensor([[-26.6934]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08510133272721321, distance: 1.1920427802177609 entropy 1.553932785987854
epoch: 10, step: 122
	action: tensor([[ 0.1765,  0.5246, -0.3597,  0.3617, -1.0465,  0.7041,  0.3219]],
       dtype=torch.float64)
	q_value: tensor([[-44.9045]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 10, step: 123
	action: tensor([[-0.9559,  0.4862,  1.0836,  0.0812, -1.8151,  1.2758, -0.3549]],
       dtype=torch.float64)
	q_value: tensor([[-48.4427]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3046915626949456, distance: 1.3071054341700745 entropy 1.553932785987854
epoch: 10, step: 124
	action: tensor([[-1.1895, -1.0281, -0.7791, -0.8890, -0.3232,  1.3776, -0.4828]],
       dtype=torch.float64)
	q_value: tensor([[-44.0420]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0713781405500415, distance: 1.6469726516105243 entropy 1.553932785987854
epoch: 10, step: 125
	action: tensor([[-0.8389, -0.0900, -1.0476, -0.0960,  0.8548,  0.1617, -0.6268]],
       dtype=torch.float64)
	q_value: tensor([[-43.2141]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7835904358394017, distance: 1.5282846837687474 entropy 1.553932785987854
epoch: 10, step: 126
	action: tensor([[-0.7622, -0.6546,  0.3801,  0.9866,  1.0308, -1.2937,  0.0796]],
       dtype=torch.float64)
	q_value: tensor([[-28.8467]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.661207613550399, distance: 1.4749206282412302 entropy 1.553932785987854
epoch: 10, step: 127
	action: tensor([[-1.6439, -0.6558, -0.3959,  2.8304, -4.3586,  0.2262, -0.9409]],
       dtype=torch.float64)
	q_value: tensor([[-28.5629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
LOSS epoch 10 actor 495.7204929537159 critic 156.0302340019097 
epoch: 11, step: 0
	action: tensor([[-0.7881,  0.4227, -2.1402, -0.3263, -0.4689,  1.4258, -0.0700]],
       dtype=torch.float64)
	q_value: tensor([[-58.7059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3665799894922117, distance: 1.3377477273831464 entropy 1.553932785987854
epoch: 11, step: 1
	action: tensor([[ 0.7235,  1.1778, -0.4264, -0.3223, -0.5190,  1.8835, -0.3076]],
       dtype=torch.float64)
	q_value: tensor([[-53.6231]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 11, step: 2
	action: tensor([[-0.5392,  0.5122, -0.4911, -2.8090, -1.2587,  0.3830,  1.9338]],
       dtype=torch.float64)
	q_value: tensor([[-58.7059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 11, step: 3
	action: tensor([[-0.7455, -0.1959, -1.3185,  1.1072, -1.5315,  1.2980,  1.4079]],
       dtype=torch.float64)
	q_value: tensor([[-58.7059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9624650696442105, distance: 1.6030890991323652 entropy 1.553932785987854
epoch: 11, step: 4
	action: tensor([[ 0.1965, -1.0745, -0.8264,  0.0212,  0.9280, -1.8429, -0.7690]],
       dtype=torch.float64)
	q_value: tensor([[-57.2578]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4476361879413393, distance: 1.3768493021491222 entropy 1.553932785987854
epoch: 11, step: 5
	action: tensor([[-0.8287, -1.0663,  0.9660, -0.2448,  0.3642, -0.0788, -0.0087]],
       dtype=torch.float64)
	q_value: tensor([[-38.9399]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.4070612315340942, distance: 1.7754165433897113 entropy 1.553932785987854
epoch: 11, step: 6
	action: tensor([[ 0.5776,  1.1555, -1.0076,  1.5701,  1.4674,  0.3450, -2.5595]],
       dtype=torch.float64)
	q_value: tensor([[-31.3838]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 11, step: 7
	action: tensor([[ 1.4175,  1.2601, -0.8153, -0.5045,  2.0556, -0.4338, -1.1389]],
       dtype=torch.float64)
	q_value: tensor([[-58.7059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 11, step: 8
	action: tensor([[-1.0218, -0.2823,  0.7553,  0.3275, -1.9678, -0.9346, -1.7450]],
       dtype=torch.float64)
	q_value: tensor([[-58.7059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5247906589366889, distance: 1.4130639103335008 entropy 1.553932785987854
epoch: 11, step: 9
	action: tensor([[ 1.6410, -0.3486, -0.5314,  0.6159, -1.4833, -0.7976,  0.0965]],
       dtype=torch.float64)
	q_value: tensor([[-51.1973]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4782976553087658, distance: 0.8265480156079257 entropy 1.553932785987854
epoch: 11, step: 10
	action: tensor([[-0.6530,  1.5636, -2.8041, -2.0346, -1.7248,  0.8135,  0.0908]],
       dtype=torch.float64)
	q_value: tensor([[-37.2298]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6565004930927894, distance: 0.6706867896786458 entropy 1.553932785987854
epoch: 11, step: 11
	action: tensor([[ 0.0837, -2.2587,  0.9335, -0.6724,  0.1879, -0.8859,  0.9899]],
       dtype=torch.float64)
	q_value: tensor([[-82.9857]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 11, step: 12
	action: tensor([[ 0.9619, -0.0598,  3.5875,  0.0088,  1.5489,  0.5378, -1.0135]],
       dtype=torch.float64)
	q_value: tensor([[-58.7059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.5395379988248591 entropy 1.553932785987854
epoch: 11, step: 13
	action: tensor([[-2.4306,  0.7023, -2.9043, -0.5721,  0.4427,  1.6608,  2.1997]],
       dtype=torch.float64)
	q_value: tensor([[-58.7059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 11, step: 14
	action: tensor([[-1.6439, -0.9745,  1.5362,  1.2347, -1.2812,  0.8837, -0.1036]],
       dtype=torch.float64)
	q_value: tensor([[-58.7059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5433020136841142, distance: 1.4216155154522059 entropy 1.553932785987854
epoch: 11, step: 15
	action: tensor([[ 1.3546,  0.5594, -1.2130, -1.9558,  0.0910, -0.2301,  1.2443]],
       dtype=torch.float64)
	q_value: tensor([[-55.2594]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08637344941298974, distance: 1.1927413201988277 entropy 1.553932785987854
epoch: 11, step: 16
	action: tensor([[ 0.3199, -0.9767,  0.6995, -0.6882, -1.0727,  2.1045, -0.6539]],
       dtype=torch.float64)
	q_value: tensor([[-52.1307]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12739725298038374, distance: 1.0689686807575152 entropy 1.553932785987854
epoch: 11, step: 17
	action: tensor([[ 1.0646e+00,  6.5289e-04,  9.6972e-01,  8.6855e-01, -2.1869e+00,
         -6.3520e-01,  3.7928e-01]], dtype=torch.float64)
	q_value: tensor([[-50.7810]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5620284514228999, distance: 0.7573203774979838 entropy 1.553932785987854
epoch: 11, step: 18
	action: tensor([[-1.1685,  1.0779,  2.4637,  1.0088,  0.0280,  0.8317,  0.2536]],
       dtype=torch.float64)
	q_value: tensor([[-48.8686]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 11, step: 19
	action: tensor([[-0.1811,  1.1401, -0.8745,  0.1046,  1.7051, -0.3826, -1.3529]],
       dtype=torch.float64)
	q_value: tensor([[-58.7059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 11, step: 20
	action: tensor([[ 2.4315, -0.1396,  0.4127,  2.0194,  0.5197, -1.2770, -1.6742]],
       dtype=torch.float64)
	q_value: tensor([[-58.7059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 11, step: 21
	action: tensor([[-0.2559,  0.7557,  1.7838, -0.5964, -0.5866, -0.7572,  1.6927]],
       dtype=torch.float64)
	q_value: tensor([[-58.7059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 11, step: 22
	action: tensor([[-0.7140, -1.1272, -1.3611,  1.1107, -0.7967, -1.4000,  0.8868]],
       dtype=torch.float64)
	q_value: tensor([[-58.7059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8485612025614881, distance: 1.5558710866032157 entropy 1.553932785987854
epoch: 11, step: 23
	action: tensor([[-0.9459,  1.7531, -1.1274,  0.1145,  1.8421, -1.6960, -0.8715]],
       dtype=torch.float64)
	q_value: tensor([[-45.5702]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03067211275897619, distance: 1.161761435255887 entropy 1.553932785987854
epoch: 11, step: 24
	action: tensor([[ 1.2344, -2.5247, -0.7058,  0.4606,  1.1889, -0.5182,  0.8019]],
       dtype=torch.float64)
	q_value: tensor([[-52.7151]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 11, step: 25
	action: tensor([[ 0.1408, -0.4231,  0.4351, -1.3389, -1.2613,  1.1984,  1.2909]],
       dtype=torch.float64)
	q_value: tensor([[-58.7059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.40072594551070884, distance: 1.3543573838655543 entropy 1.553932785987854
epoch: 11, step: 26
	action: tensor([[ 0.2873,  0.9523,  1.5024,  1.3736, -1.2714, -2.1060, -0.5093]],
       dtype=torch.float64)
	q_value: tensor([[-56.3567]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 11, step: 27
	action: tensor([[-1.2746, -0.1521, -0.7628, -0.7480, -0.7542,  0.4341,  0.8640]],
       dtype=torch.float64)
	q_value: tensor([[-58.7059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2126804668880613, distance: 1.7022213929110088 entropy 1.553932785987854
epoch: 11, step: 28
	action: tensor([[ 0.3873,  0.4320, -1.8329, -0.8610,  0.6505, -2.4813, -1.2655]],
       dtype=torch.float64)
	q_value: tensor([[-51.8848]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7724940186880962, distance: 0.5458245970304145 entropy 1.553932785987854
epoch: 11, step: 29
	action: tensor([[ 0.1557,  1.2983,  2.3746, -0.3468,  1.1090,  0.0138, -0.6090]],
       dtype=torch.float64)
	q_value: tensor([[-49.4205]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 11, step: 30
	action: tensor([[-0.6833, -0.3141, -1.4343,  0.2057,  0.3277, -0.2453, -0.8358]],
       dtype=torch.float64)
	q_value: tensor([[-58.7059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9089234173113274, distance: 1.5810694460563717 entropy 1.553932785987854
epoch: 11, step: 31
	action: tensor([[-1.4126,  0.9733,  0.5051,  0.9159, -0.7197,  0.9166, -1.6935]],
       dtype=torch.float64)
	q_value: tensor([[-32.4786]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 11, step: 32
	action: tensor([[-0.5643,  0.8686,  0.9682,  1.2766, -2.8526,  0.5801,  1.0187]],
       dtype=torch.float64)
	q_value: tensor([[-58.7059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 11, step: 33
	action: tensor([[ 1.4780, -0.2760, -3.1473,  1.9592,  0.4724, -0.4518, -0.8775]],
       dtype=torch.float64)
	q_value: tensor([[-58.7059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3450052285921603 entropy 1.553932785987854
epoch: 11, step: 34
	action: tensor([[-1.1609, -1.1916, -1.7273, -0.7112,  1.1799,  0.3175, -1.4518]],
       dtype=torch.float64)
	q_value: tensor([[-58.7059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8957595317771758, distance: 1.5756085085510874 entropy 1.553932785987854
epoch: 11, step: 35
	action: tensor([[ 0.4316,  0.5199, -1.6895,  1.4841, -0.4947, -0.2507, -1.0425]],
       dtype=torch.float64)
	q_value: tensor([[-54.5437]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26129018598476916, distance: 0.9835436247638245 entropy 1.553932785987854
epoch: 11, step: 36
	action: tensor([[ 0.0172,  1.2427,  0.4612, -0.2916,  1.0925,  0.3378, -0.0392]],
       dtype=torch.float64)
	q_value: tensor([[-32.0833]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 11, step: 37
	action: tensor([[-1.5751, -2.4267,  0.8971,  2.3603,  0.3095, -1.6223, -1.1439]],
       dtype=torch.float64)
	q_value: tensor([[-58.7059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 11, step: 38
	action: tensor([[-0.3002, -0.6550, -0.6262, -0.4211,  1.6867,  1.4274, -0.1799]],
       dtype=torch.float64)
	q_value: tensor([[-58.7059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19476805611612547, distance: 1.0268740780729606 entropy 1.553932785987854
epoch: 11, step: 39
	action: tensor([[ 1.3265,  0.0846, -1.7195, -1.3322, -0.9054, -1.0827,  0.4000]],
       dtype=torch.float64)
	q_value: tensor([[-45.7911]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3290716185265238, distance: 0.9373348732564981 entropy 1.553932785987854
epoch: 11, step: 40
	action: tensor([[-0.6475,  0.0879, -0.1894, -2.5094, -1.0694,  1.2673,  0.2401]],
       dtype=torch.float64)
	q_value: tensor([[-45.9331]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 11, step: 41
	action: tensor([[ 0.0705,  0.9639,  0.0067,  1.8165,  1.3978, -0.0994, -1.4871]],
       dtype=torch.float64)
	q_value: tensor([[-58.7059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 11, step: 42
	action: tensor([[-1.0388,  0.1539, -1.6229, -1.0702,  0.3189,  1.1733, -2.7319]],
       dtype=torch.float64)
	q_value: tensor([[-58.7059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21436505219212254, distance: 1.261047145558857 entropy 1.553932785987854
epoch: 11, step: 43
	action: tensor([[-1.4693, -2.1976,  0.8412,  1.9749, -0.5280,  2.8399, -0.8147]],
       dtype=torch.float64)
	q_value: tensor([[-61.6740]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 11, step: 44
	action: tensor([[-0.7610, -1.9941, -0.7498, -1.0508,  2.3297,  1.1123,  1.3639]],
       dtype=torch.float64)
	q_value: tensor([[-58.7059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 11, step: 45
	action: tensor([[-0.9467, -0.0150,  2.1899, -0.6377,  1.1054, -1.9579,  1.1719]],
       dtype=torch.float64)
	q_value: tensor([[-58.7059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.011510692100691333, distance: 1.1377390943132215 entropy 1.553932785987854
epoch: 11, step: 46
	action: tensor([[-1.0474, -1.3617, -0.6461, -0.1240,  1.4266, -1.5744,  1.0397]],
       dtype=torch.float64)
	q_value: tensor([[-50.0508]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8962870739713258, distance: 1.5758277194104617 entropy 1.553932785987854
epoch: 11, step: 47
	action: tensor([[-0.3928,  1.3401, -1.5489,  0.3814,  2.0062, -0.5222,  0.5307]],
       dtype=torch.float64)
	q_value: tensor([[-45.9011]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 11, step: 48
	action: tensor([[-1.0268, -2.5606, -0.7327,  1.0538,  1.1541,  0.3442,  0.2112]],
       dtype=torch.float64)
	q_value: tensor([[-58.7059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 11, step: 49
	action: tensor([[-0.6462, -0.1862, -1.4719,  0.4249, -0.8442,  1.8187, -2.2932]],
       dtype=torch.float64)
	q_value: tensor([[-58.7059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6315476789427268, distance: 1.4616943833020613 entropy 1.553932785987854
epoch: 11, step: 50
	action: tensor([[ 1.0680,  0.2124,  0.5643, -1.0770,  0.0718,  0.4796, -2.1405]],
       dtype=torch.float64)
	q_value: tensor([[-51.0973]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6062761392972789, distance: 0.7180464853065383 entropy 1.553932785987854
epoch: 11, step: 51
	action: tensor([[-1.1525,  0.3119,  1.5236,  0.2195, -2.0109,  1.3920,  2.0508]],
       dtype=torch.float64)
	q_value: tensor([[-37.7670]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 11, step: 52
	action: tensor([[-0.7536, -0.0726,  2.9235,  0.2534, -0.9182,  1.2685, -0.3741]],
       dtype=torch.float64)
	q_value: tensor([[-58.7059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17835145981298317, distance: 1.2422074094122462 entropy 1.553932785987854
epoch: 11, step: 53
	action: tensor([[ 1.1268,  1.2298, -0.1176, -0.6413,  1.4764, -0.5312,  2.3836]],
       dtype=torch.float64)
	q_value: tensor([[-55.3737]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 11, step: 54
	action: tensor([[ 1.4849, -0.4136, -2.6818, -1.0924, -0.0406, -0.7409, -0.7911]],
       dtype=torch.float64)
	q_value: tensor([[-58.7059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 11, step: 55
	action: tensor([[ 0.6569, -2.1361, -1.2685, -0.7892,  0.8213, -0.1250, -0.2232]],
       dtype=torch.float64)
	q_value: tensor([[-58.7059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 11, step: 56
	action: tensor([[-0.5464, -0.6518,  0.9481,  1.9909, -0.8684, -0.8506,  0.1517]],
       dtype=torch.float64)
	q_value: tensor([[-58.7059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6625636658444736, distance: 0.6647412285990838 entropy 1.553932785987854
epoch: 11, step: 57
	action: tensor([[-1.8208, -1.1635,  1.3104,  2.3842, -0.4938,  0.8010,  2.7635]],
       dtype=torch.float64)
	q_value: tensor([[-43.9587]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 11, step: 58
	action: tensor([[ 1.5792,  1.4601,  2.3482, -2.1486, -3.6265,  1.9202,  2.0693]],
       dtype=torch.float64)
	q_value: tensor([[-58.7059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 11, step: 59
	action: tensor([[ 0.1326, -1.3931,  0.2759,  0.8211,  0.8277, -2.7953, -1.9407]],
       dtype=torch.float64)
	q_value: tensor([[-58.7059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 11, step: 60
	action: tensor([[-0.4396,  0.0894,  2.4263,  1.7227,  0.0823, -1.2271, -2.0370]],
       dtype=torch.float64)
	q_value: tensor([[-58.7059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 11, step: 61
	action: tensor([[ 1.5098, -0.6861, -1.6811, -0.5037,  1.1668,  1.7659, -0.0978]],
       dtype=torch.float64)
	q_value: tensor([[-58.7059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 11, step: 62
	action: tensor([[ 0.4943,  0.5854, -0.1337, -1.1333,  0.9664, -0.5808,  0.8441]],
       dtype=torch.float64)
	q_value: tensor([[-58.7059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 11, step: 63
	action: tensor([[ 1.1473,  1.1414, -0.7781,  1.9541,  0.0300,  0.0656,  1.1465]],
       dtype=torch.float64)
	q_value: tensor([[-58.7059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 11, step: 64
	action: tensor([[ 2.7975, -0.5226, -1.5606, -1.4571,  0.3840,  1.0690, -1.3495]],
       dtype=torch.float64)
	q_value: tensor([[-58.7059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 11, step: 65
	action: tensor([[-0.2876, -1.4709,  1.7008, -1.4338,  1.1982,  0.0940, -2.0074]],
       dtype=torch.float64)
	q_value: tensor([[-58.7059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9848239498857949, distance: 1.6121954425382596 entropy 1.553932785987854
epoch: 11, step: 66
	action: tensor([[ 0.6479, -2.8722,  0.4125, -0.5239, -0.8257, -0.6595,  0.6374]],
       dtype=torch.float64)
	q_value: tensor([[-52.1568]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 11, step: 67
	action: tensor([[ 0.1174, -0.1997, -0.0335,  1.1781, -1.0855,  1.5997, -0.3303]],
       dtype=torch.float64)
	q_value: tensor([[-58.7059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 11, step: 68
	action: tensor([[-1.3047,  0.7942, -1.0315,  0.5614,  0.7470,  0.4162,  0.0414]],
       dtype=torch.float64)
	q_value: tensor([[-58.7059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 11, step: 69
	action: tensor([[ 1.0343e+00, -2.9499e-04, -1.0686e+00, -5.5771e-01, -3.7199e-02,
         -9.0393e-01,  5.6802e-01]], dtype=torch.float64)
	q_value: tensor([[-58.7059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34042462192445244, distance: 0.9293705594613051 entropy 1.553932785987854
epoch: 11, step: 70
	action: tensor([[ 2.6157, -1.1341,  0.6799, -0.9353,  0.2248, -1.2954, -1.4072]],
       dtype=torch.float64)
	q_value: tensor([[-31.9180]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 11, step: 71
	action: tensor([[-0.2877, -0.4986, -0.3945, -0.3506, -2.6470, -1.2186,  0.8965]],
       dtype=torch.float64)
	q_value: tensor([[-58.7059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.027290021450472768, distance: 1.159853742280206 entropy 1.553932785987854
epoch: 11, step: 72
	action: tensor([[-0.8062,  0.9346,  3.1302, -0.8903,  3.8525,  0.1977, -0.4841]],
       dtype=torch.float64)
	q_value: tensor([[-59.9851]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 11, step: 73
	action: tensor([[ 0.2548, -1.7504, -0.4715, -1.1256, -0.0821, -0.0417, -0.7431]],
       dtype=torch.float64)
	q_value: tensor([[-58.7059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7357678057316879, distance: 1.507656858735651 entropy 1.553932785987854
epoch: 11, step: 74
	action: tensor([[ 1.1943,  0.5802, -0.1855,  0.7971, -0.8354,  0.4755,  0.8151]],
       dtype=torch.float64)
	q_value: tensor([[-34.5858]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 11, step: 75
	action: tensor([[ 0.1466,  0.1838, -0.4272,  0.6446,  1.4164,  1.3670, -0.8440]],
       dtype=torch.float64)
	q_value: tensor([[-58.7059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 11, step: 76
	action: tensor([[-0.9739,  1.0064,  1.5004, -0.7390, -0.6102,  0.7892, -0.5115]],
       dtype=torch.float64)
	q_value: tensor([[-58.7059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 11, step: 77
	action: tensor([[ 0.4518, -0.6148, -0.3527, -1.1627,  0.3451, -0.1278,  1.0465]],
       dtype=torch.float64)
	q_value: tensor([[-58.7059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.45229502262490473, distance: 1.3790630353957358 entropy 1.553932785987854
epoch: 11, step: 78
	action: tensor([[-0.7003, -0.7676,  2.0906, -0.6514, -0.3211,  0.2712,  1.2923]],
       dtype=torch.float64)
	q_value: tensor([[-36.8788]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2024181181481968, distance: 1.6982693781776639 entropy 1.553932785987854
epoch: 11, step: 79
	action: tensor([[ 1.1487,  2.7231, -0.0318,  1.7755, -1.1191,  1.4103,  0.5400]],
       dtype=torch.float64)
	q_value: tensor([[-47.0348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 11, step: 80
	action: tensor([[-0.9083,  1.5742, -0.3380, -0.8185, -1.2501,  0.3532,  1.3138]],
       dtype=torch.float64)
	q_value: tensor([[-58.7059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 11, step: 81
	action: tensor([[ 1.6422,  0.2334, -0.4444,  0.5724, -1.1214,  0.3700,  0.0141]],
       dtype=torch.float64)
	q_value: tensor([[-58.7059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 11, step: 82
	action: tensor([[ 1.8084,  0.0941, -2.0845,  0.7403, -0.6187, -0.2257, -0.2871]],
       dtype=torch.float64)
	q_value: tensor([[-58.7059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 11, step: 83
	action: tensor([[-1.5093, -0.2997,  0.5356,  1.5696,  0.3061,  0.4615, -0.7796]],
       dtype=torch.float64)
	q_value: tensor([[-58.7059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08994747249253465, distance: 1.091666349317801 entropy 1.553932785987854
epoch: 11, step: 84
	action: tensor([[-0.8885,  0.1094,  0.4120,  0.2501,  0.5956, -0.6184, -0.7448]],
       dtype=torch.float64)
	q_value: tensor([[-40.8463]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7491906079253807, distance: 1.5134750352244397 entropy 1.553932785987854
epoch: 11, step: 85
	action: tensor([[-0.4210,  0.3484, -0.5383, -0.3660, -1.5559, -0.4050,  0.4412]],
       dtype=torch.float64)
	q_value: tensor([[-31.4051]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2247625291427645, distance: 1.0075673162480558 entropy 1.553932785987854
epoch: 11, step: 86
	action: tensor([[ 1.2509,  2.0013,  0.8676,  0.1257, -1.0294,  1.5478,  0.1091]],
       dtype=torch.float64)
	q_value: tensor([[-42.4861]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 11, step: 87
	action: tensor([[-0.8463, -0.0166, -1.2379, -0.5707,  0.9439, -1.2363,  0.8421]],
       dtype=torch.float64)
	q_value: tensor([[-58.7059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.28790513395456263, distance: 1.2986694685562157 entropy 1.553932785987854
epoch: 11, step: 88
	action: tensor([[ 0.7035,  0.6676,  0.4570,  0.5594,  0.9360, -0.5595,  0.3595]],
       dtype=torch.float64)
	q_value: tensor([[-45.8235]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 11, step: 89
	action: tensor([[-0.5911,  1.1000, -1.9796, -0.7597, -0.7099, -0.0036,  0.3355]],
       dtype=torch.float64)
	q_value: tensor([[-58.7059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 11, step: 90
	action: tensor([[-0.5079,  0.1012, -0.8483, -0.9012, -1.1881, -1.8138,  1.1220]],
       dtype=torch.float64)
	q_value: tensor([[-58.7059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6363460652490514, distance: 0.690082221344012 entropy 1.553932785987854
epoch: 11, step: 91
	action: tensor([[ 0.5155, -0.8141, -1.6518,  1.0922, -0.3350,  0.5404, -2.2675]],
       dtype=torch.float64)
	q_value: tensor([[-52.8622]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.29579595331009245, distance: 1.302641778880152 entropy 1.553932785987854
epoch: 11, step: 92
	action: tensor([[-0.1665, -1.4950,  0.5551, -1.1267, -0.5650,  0.6077, -1.3712]],
       dtype=torch.float64)
	q_value: tensor([[-37.9596]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 11, step: 93
	action: tensor([[-0.4355, -1.4417, -0.8120, -1.5319, -0.4102,  1.0041,  1.6622]],
       dtype=torch.float64)
	q_value: tensor([[-58.7059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3516035448276069, distance: 1.3303972984847348 entropy 1.553932785987854
epoch: 11, step: 94
	action: tensor([[ 0.2964, -0.2178,  1.4922,  0.8769,  1.2398, -0.1072,  0.5091]],
       dtype=torch.float64)
	q_value: tensor([[-61.5493]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6949608094342825, distance: 0.6320253723819883 entropy 1.553932785987854
epoch: 11, step: 95
	action: tensor([[-0.3550, -0.8194,  0.9878,  0.3035,  0.2568,  1.8889,  0.1227]],
       dtype=torch.float64)
	q_value: tensor([[-35.7850]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3491347137436155, distance: 0.9232137155084186 entropy 1.553932785987854
epoch: 11, step: 96
	action: tensor([[-0.6554, -0.1695, -1.2335, -1.1599, -1.5156, -0.0272,  0.1044]],
       dtype=torch.float64)
	q_value: tensor([[-44.4526]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1670077033110272, distance: 1.044424824545173 entropy 1.553932785987854
epoch: 11, step: 97
	action: tensor([[ 0.8613,  0.5328, -1.2361,  0.3639,  0.3781, -0.2405,  0.8833]],
       dtype=torch.float64)
	q_value: tensor([[-52.0348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9736940932104767, distance: 0.18560229060016226 entropy 1.553932785987854
epoch: 11, step: 98
	action: tensor([[-1.4159, -1.9089, -2.9977,  0.4265,  0.2199,  1.2090,  0.5874]],
       dtype=torch.float64)
	q_value: tensor([[-32.4020]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 11, step: 99
	action: tensor([[-1.3508, -0.9264, -1.4486,  0.2000,  0.4218,  0.3182,  0.3468]],
       dtype=torch.float64)
	q_value: tensor([[-58.7059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.7783997649583614, distance: 1.9074539416183438 entropy 1.553932785987854
epoch: 11, step: 100
	action: tensor([[-0.9318,  1.0576, -0.2714, -1.2845, -0.1568,  0.4463, -1.2154]],
       dtype=torch.float64)
	q_value: tensor([[-44.0648]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2275288522286807, distance: 1.2678636410889894 entropy 1.553932785987854
epoch: 11, step: 101
	action: tensor([[ 1.1084, -1.5410, -2.0222, -0.1510,  1.4442, -0.0447,  0.8875]],
       dtype=torch.float64)
	q_value: tensor([[-49.1044]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6495722632401206, distance: 1.4697462678260391 entropy 1.553932785987854
epoch: 11, step: 102
	action: tensor([[ 1.1925,  0.2740,  0.3494, -1.5619,  1.2963,  0.4559,  0.9178]],
       dtype=torch.float64)
	q_value: tensor([[-47.0967]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2468664276978747, distance: 0.9930993485885335 entropy 1.553932785987854
epoch: 11, step: 103
	action: tensor([[ 0.3576,  1.9450, -2.6265, -0.5831, -0.3234,  1.7841, -1.5372]],
       dtype=torch.float64)
	q_value: tensor([[-42.5083]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 11, step: 104
	action: tensor([[-0.6645, -0.1110,  1.7878, -1.5386,  1.2741,  0.8081,  1.1034]],
       dtype=torch.float64)
	q_value: tensor([[-58.7059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9403142109359497, distance: 1.5940161805062727 entropy 1.553932785987854
epoch: 11, step: 105
	action: tensor([[-0.1460, -0.0275, -0.8654, -1.3225, -0.3906, -0.9430,  2.3625]],
       dtype=torch.float64)
	q_value: tensor([[-52.8171]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4841266836535312, distance: 0.821917496420555 entropy 1.553932785987854
epoch: 11, step: 106
	action: tensor([[-0.7820, -0.3892, -1.1039,  0.6454,  0.9057, -0.7823, -1.1207]],
       dtype=torch.float64)
	q_value: tensor([[-59.8083]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.150838268639994, distance: 1.6782651285736014 entropy 1.553932785987854
epoch: 11, step: 107
	action: tensor([[-0.4654,  1.2397,  1.1171, -1.9803,  0.3918,  0.0842, -0.3946]],
       dtype=torch.float64)
	q_value: tensor([[-36.4818]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.488141868496095, distance: 1.3959789226210966 entropy 1.553932785987854
epoch: 11, step: 108
	action: tensor([[ 8.6056e-01, -2.1685e-01, -4.4898e-01,  1.4030e+00, -1.7581e+00,
         -5.2709e-04,  9.1862e-01]], dtype=torch.float64)
	q_value: tensor([[-49.2828]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 11, step: 109
	action: tensor([[-0.4661, -1.1367,  0.0639, -0.1777, -0.4513, -0.3398,  0.9825]],
       dtype=torch.float64)
	q_value: tensor([[-58.7059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9824390902839508, distance: 1.611226586982862 entropy 1.553932785987854
epoch: 11, step: 110
	action: tensor([[-0.1321,  0.9094,  0.4784, -1.1720,  2.2022, -1.1984, -1.6032]],
       dtype=torch.float64)
	q_value: tensor([[-35.0549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 11, step: 111
	action: tensor([[ 0.8158,  1.8503, -1.9353, -0.0911, -0.7236,  1.4559,  0.4113]],
       dtype=torch.float64)
	q_value: tensor([[-58.7059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 11, step: 112
	action: tensor([[ 0.2620,  0.6053, -0.6735,  0.8293,  1.7197, -1.0364,  0.5639]],
       dtype=torch.float64)
	q_value: tensor([[-58.7059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 11, step: 113
	action: tensor([[ 0.4952, -0.0570,  0.1711, -1.1269,  1.1072, -1.2551,  0.3773]],
       dtype=torch.float64)
	q_value: tensor([[-58.7059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06665659285645986, distance: 1.1055475470459117 entropy 1.553932785987854
epoch: 11, step: 114
	action: tensor([[ 1.3276,  0.2391,  1.9580,  0.2790, -0.2785, -1.4720, -0.8430]],
       dtype=torch.float64)
	q_value: tensor([[-34.9174]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8785032886776577, distance: 0.39887696305435105 entropy 1.553932785987854
epoch: 11, step: 115
	action: tensor([[ 0.4516,  0.6525,  0.2970,  0.1524, -1.5633, -1.0659,  1.1260]],
       dtype=torch.float64)
	q_value: tensor([[-40.9639]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 11, step: 116
	action: tensor([[-1.7365, -1.2594, -0.2250, -0.7354, -1.4799,  2.9995, -0.6758]],
       dtype=torch.float64)
	q_value: tensor([[-58.7059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 11, step: 117
	action: tensor([[ 2.2705,  1.9324, -0.0978,  1.1644, -0.2001,  0.2880, -0.2420]],
       dtype=torch.float64)
	q_value: tensor([[-58.7059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 11, step: 118
	action: tensor([[ 0.3597, -0.3486,  0.8920, -0.0065, -0.3433, -1.7528,  0.0501]],
       dtype=torch.float64)
	q_value: tensor([[-58.7059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08002257416423786, distance: 1.0976029831068683 entropy 1.553932785987854
epoch: 11, step: 119
	action: tensor([[-0.8773, -1.4049, -1.0840, -1.1052, -1.2540,  0.3971,  0.0308]],
       dtype=torch.float64)
	q_value: tensor([[-32.8477]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2599423922530546, distance: 1.2844938744817322 entropy 1.553932785987854
epoch: 11, step: 120
	action: tensor([[ 0.0384, -1.2151, -1.6797,  0.0533,  0.2473, -2.3637,  0.3162]],
       dtype=torch.float64)
	q_value: tensor([[-52.3906]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1137454621561611, distance: 1.0772981876205632 entropy 1.553932785987854
epoch: 11, step: 121
	action: tensor([[-0.5756, -0.0645, -0.4769,  1.1945, -0.1081,  3.7130,  0.3402]],
       dtype=torch.float64)
	q_value: tensor([[-46.6696]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 11, step: 122
	action: tensor([[ 0.7779,  1.1963,  1.9102,  0.5570,  0.3009,  1.4132, -0.6711]],
       dtype=torch.float64)
	q_value: tensor([[-58.7059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 11, step: 123
	action: tensor([[ 1.0526, -1.0945,  0.6165, -1.5044,  2.6769,  3.2267, -1.3668]],
       dtype=torch.float64)
	q_value: tensor([[-58.7059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 11, step: 124
	action: tensor([[ 0.7939,  0.4874,  0.0467, -1.1448, -1.8217, -1.6436,  1.6441]],
       dtype=torch.float64)
	q_value: tensor([[-58.7059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 11, step: 125
	action: tensor([[-1.6412, -0.1831,  1.6847, -2.0986,  1.1574,  0.6879, -0.2071]],
       dtype=torch.float64)
	q_value: tensor([[-58.7059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7566293239087813, distance: 1.5166897697480952 entropy 1.553932785987854
epoch: 11, step: 126
	action: tensor([[-1.3512,  2.2740,  0.3307, -0.0419, -1.0533,  1.9717, -0.2419]],
       dtype=torch.float64)
	q_value: tensor([[-61.8957]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 11, step: 127
	action: tensor([[ 0.8136,  3.2664,  0.8979,  0.0837, -0.7735,  0.6544,  1.2651]],
       dtype=torch.float64)
	q_value: tensor([[-58.7059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
LOSS epoch 11 actor 536.2174609194719 critic 94.9288046611828 
epoch: 12, step: 0
	action: tensor([[ 0.2570, -1.4291, -0.6510, -0.6993,  2.5201,  0.1100, -0.5820]],
       dtype=torch.float64)
	q_value: tensor([[-55.3523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7307080888535291, distance: 1.5054578652897657 entropy 1.448572039604187
epoch: 12, step: 1
	action: tensor([[ 0.0758, -0.4199, -0.5959,  1.0691, -0.7827,  1.6462,  0.6861]],
       dtype=torch.float64)
	q_value: tensor([[-52.4752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15188854789191053, distance: 1.0538605731096564 entropy 1.448572039604187
epoch: 12, step: 2
	action: tensor([[-0.3399,  0.0637, -1.2349,  0.5898, -0.3281, -0.5931,  0.1142]],
       dtype=torch.float64)
	q_value: tensor([[-39.0340]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1057771512349841, distance: 1.203345945978878 entropy 1.448572039604187
epoch: 12, step: 3
	action: tensor([[-0.5184, -1.1609, -1.5299, -0.3620,  0.0627,  0.3290,  0.2766]],
       dtype=torch.float64)
	q_value: tensor([[-29.3844]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8457086883862437, distance: 1.5546701910715288 entropy 1.448572039604187
epoch: 12, step: 4
	action: tensor([[ 2.6239, -0.1407, -1.4612, -1.2648,  0.0497, -0.0648,  0.4772]],
       dtype=torch.float64)
	q_value: tensor([[-39.7231]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 12, step: 5
	action: tensor([[ 0.0848, -1.0705,  1.7125, -0.4272,  0.0383,  1.4461, -0.4420]],
       dtype=torch.float64)
	q_value: tensor([[-55.3523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.282840065496335, distance: 1.2961132516138145 entropy 1.448572039604187
epoch: 12, step: 6
	action: tensor([[-1.8319, -0.5875,  0.3583, -0.5581,  0.5959, -1.3513, -0.6351]],
       dtype=torch.float64)
	q_value: tensor([[-41.8990]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.194272843240666, distance: 1.6951260866784539 entropy 1.448572039604187
epoch: 12, step: 7
	action: tensor([[-0.2139,  0.9179, -0.9153,  1.0382, -1.6802,  0.0762,  0.6981]],
       dtype=torch.float64)
	q_value: tensor([[-42.5065]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 12, step: 8
	action: tensor([[ 0.0142,  0.5291, -0.6076,  0.3203, -1.8049, -1.2504, -1.1722]],
       dtype=torch.float64)
	q_value: tensor([[-55.3523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 12, step: 9
	action: tensor([[-1.6660, -0.3433, -2.5538, -0.4455,  0.6175,  0.0279,  0.8673]],
       dtype=torch.float64)
	q_value: tensor([[-55.3523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 12, step: 10
	action: tensor([[-0.9261, -0.5218, -1.0346, -1.1998,  0.5345,  0.1835,  1.1300]],
       dtype=torch.float64)
	q_value: tensor([[-55.3523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2614859249844772, distance: 1.2852804388090675 entropy 1.448572039604187
epoch: 12, step: 11
	action: tensor([[-0.5356, -1.3335,  0.0739, -2.2937,  0.4762,  0.6155, -0.1312]],
       dtype=torch.float64)
	q_value: tensor([[-48.2639]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11986728170583416, distance: 1.2109883676159774 entropy 1.448572039604187
epoch: 12, step: 12
	action: tensor([[ 0.7771, -1.1149, -0.8356,  0.0371, -0.0118, -0.2509,  0.6578]],
       dtype=torch.float64)
	q_value: tensor([[-50.8774]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.526574767026182, distance: 1.4138903587765959 entropy 1.448572039604187
epoch: 12, step: 13
	action: tensor([[-1.1264, -0.7468,  1.2476,  1.4029,  1.3996,  1.4160,  1.0885]],
       dtype=torch.float64)
	q_value: tensor([[-29.5684]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07922260008717252, distance: 1.0980800943445461 entropy 1.448572039604187
epoch: 12, step: 14
	action: tensor([[-0.5573, -0.7638,  0.4192,  1.7417, -2.0846, -0.5070,  1.3867]],
       dtype=torch.float64)
	q_value: tensor([[-49.3315]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.619274458952876, distance: 0.706094290077746 entropy 1.448572039604187
epoch: 12, step: 15
	action: tensor([[-1.1677, -0.8730, -2.0511, -0.7069,  1.3672,  0.7897, -0.0106]],
       dtype=torch.float64)
	q_value: tensor([[-56.8951]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.948367325961935, distance: 1.5973206720249598 entropy 1.448572039604187
epoch: 12, step: 16
	action: tensor([[ 0.1889, -1.4747, -0.2515,  0.5042, -1.6674, -1.5616,  1.2815]],
       dtype=torch.float64)
	q_value: tensor([[-55.0876]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 12, step: 17
	action: tensor([[-0.5990,  0.5035, -0.9202,  0.2723, -0.3763, -0.6686,  0.4700]],
       dtype=torch.float64)
	q_value: tensor([[-55.3523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09737060845316203, distance: 1.0872049692823007 entropy 1.448572039604187
epoch: 12, step: 18
	action: tensor([[ 0.3745,  0.6194,  1.2491, -1.0836,  1.8602, -1.0808, -1.7590]],
       dtype=torch.float64)
	q_value: tensor([[-32.7436]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 12, step: 19
	action: tensor([[-0.7988, -0.8606, -1.4768, -1.9225,  0.7449,  0.1160, -0.3913]],
       dtype=torch.float64)
	q_value: tensor([[-55.3523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.613471300865331, distance: 0.7114552128888746 entropy 1.448572039604187
epoch: 12, step: 20
	action: tensor([[ 0.6767,  1.4458, -0.3760,  0.5841, -1.3499,  0.6348,  0.7622]],
       dtype=torch.float64)
	q_value: tensor([[-54.5918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 12, step: 21
	action: tensor([[ 0.7988, -1.3360,  0.2436,  0.2906,  0.7547,  0.6996, -0.2555]],
       dtype=torch.float64)
	q_value: tensor([[-55.3523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12791716081962656, distance: 1.215333013852472 entropy 1.448572039604187
epoch: 12, step: 22
	action: tensor([[ 0.2489, -0.0316, -0.9361,  0.0750,  1.4993, -2.0111,  0.3446]],
       dtype=torch.float64)
	q_value: tensor([[-29.8012]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29398227473132277, distance: 0.9615336678736945 entropy 1.448572039604187
epoch: 12, step: 23
	action: tensor([[-0.8537, -0.7023,  1.9135, -0.5254, -0.8286,  0.7802, -1.7328]],
       dtype=torch.float64)
	q_value: tensor([[-40.6739]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0843335546575306, distance: 1.652115110365647 entropy 1.448572039604187
epoch: 12, step: 24
	action: tensor([[ 1.3766, -1.7989,  0.6298,  0.0893, -0.3752,  0.0968, -1.3425]],
       dtype=torch.float64)
	q_value: tensor([[-49.8227]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 12, step: 25
	action: tensor([[-0.2458, -0.9264, -1.0584,  0.3645, -0.9888,  1.4963,  0.1102]],
       dtype=torch.float64)
	q_value: tensor([[-55.3523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.722052440520764, distance: 1.5016885851973134 entropy 1.448572039604187
epoch: 12, step: 26
	action: tensor([[-0.6938, -1.0577,  0.2670,  0.0811, -0.9765,  0.2767, -0.6671]],
       dtype=torch.float64)
	q_value: tensor([[-40.9709]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1156864159942694, distance: 1.6644944142677687 entropy 1.448572039604187
epoch: 12, step: 27
	action: tensor([[ 0.6974,  1.0732,  0.2079, -0.0464, -0.1287,  1.1718,  0.1547]],
       dtype=torch.float64)
	q_value: tensor([[-35.4374]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 12, step: 28
	action: tensor([[ 0.2953,  1.5957, -0.0127, -2.3583,  1.0896, -0.8774, -1.3487]],
       dtype=torch.float64)
	q_value: tensor([[-55.3523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 12, step: 29
	action: tensor([[ 1.3661,  0.3223,  0.7833, -1.2087, -0.8121,  1.8794,  1.5970]],
       dtype=torch.float64)
	q_value: tensor([[-55.3523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 12, step: 30
	action: tensor([[ 0.1151,  0.9172, -1.1662, -0.5184,  0.0967,  0.4406,  0.8112]],
       dtype=torch.float64)
	q_value: tensor([[-55.3523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 12, step: 31
	action: tensor([[-0.0996, -1.1785,  0.4918,  0.8840,  0.7574,  1.4538,  0.1505]],
       dtype=torch.float64)
	q_value: tensor([[-55.3523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.499430535572047, distance: 0.8096342464575773 entropy 1.448572039604187
epoch: 12, step: 32
	action: tensor([[ 0.9653,  1.1593,  0.4916,  2.0637,  1.4515, -0.1828, -0.7441]],
       dtype=torch.float64)
	q_value: tensor([[-36.4185]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0920283426795181, distance: 1.1958415756563983 entropy 1.448572039604187
epoch: 12, step: 33
	action: tensor([[-0.5380, -0.8992,  0.3106,  1.3494, -0.5894, -0.8078, -0.3040]],
       dtype=torch.float64)
	q_value: tensor([[-40.3533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 12, step: 34
	action: tensor([[-1.6212,  0.6679,  0.9623, -1.3168, -1.1119,  1.3347,  1.1473]],
       dtype=torch.float64)
	q_value: tensor([[-55.3523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8327900284913308, distance: 1.54921983939529 entropy 1.448572039604187
epoch: 12, step: 35
	action: tensor([[-1.4211,  0.8269, -1.4647, -0.2715, -1.0424,  1.0627, -0.7928]],
       dtype=torch.float64)
	q_value: tensor([[-63.0053]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7575503557053216, distance: 1.5170873312669109 entropy 1.448572039604187
epoch: 12, step: 36
	action: tensor([[ 2.4556,  0.1975,  1.3522,  0.1769,  0.2561,  0.9159, -1.2901]],
       dtype=torch.float64)
	q_value: tensor([[-50.8598]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 12, step: 37
	action: tensor([[ 0.0505, -0.7712,  2.0779, -0.5808,  0.1302, -1.0299, -1.1525]],
       dtype=torch.float64)
	q_value: tensor([[-55.3523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1288787437029275, distance: 1.2158509572327685 entropy 1.448572039604187
epoch: 12, step: 38
	action: tensor([[ 0.0233, -0.2548, -0.7403,  1.1128, -0.2782, -0.1841,  1.1570]],
       dtype=torch.float64)
	q_value: tensor([[-40.9779]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 12, step: 39
	action: tensor([[-0.2648,  0.6176, -0.7043, -1.6114,  0.5121, -1.2899, -0.4693]],
       dtype=torch.float64)
	q_value: tensor([[-55.3523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 12, step: 40
	action: tensor([[-0.3815, -0.8873, -0.6352,  0.6756,  0.8605,  0.5268,  0.7080]],
       dtype=torch.float64)
	q_value: tensor([[-55.3523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.47809306172047283, distance: 1.391257704759558 entropy 1.448572039604187
epoch: 12, step: 41
	action: tensor([[-1.4604, -0.2294,  0.3731,  0.7996, -2.0011,  0.3233,  1.5505]],
       dtype=torch.float64)
	q_value: tensor([[-32.0494]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0696341821580204, distance: 1.6462791865846629 entropy 1.448572039604187
epoch: 12, step: 42
	action: tensor([[-1.2164,  0.1139, -1.0412, -0.0597,  0.1504, -0.9623, -0.0801]],
       dtype=torch.float64)
	q_value: tensor([[-58.2372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.807767986994419, distance: 1.5386081868429755 entropy 1.448572039604187
epoch: 12, step: 43
	action: tensor([[-0.3465,  1.4010, -0.5003, -0.2728,  0.1240,  2.0116, -1.7598]],
       dtype=torch.float64)
	q_value: tensor([[-37.9931]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 12, step: 44
	action: tensor([[-1.1064, -0.6881,  2.2713, -0.9050, -0.1882,  1.3925, -0.0249]],
       dtype=torch.float64)
	q_value: tensor([[-55.3523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1196593311450576, distance: 1.6660565062330144 entropy 1.448572039604187
epoch: 12, step: 45
	action: tensor([[ 0.3794,  0.5707, -1.2459, -1.6485, -0.2316,  0.4144,  1.4861]],
       dtype=torch.float64)
	q_value: tensor([[-51.5857]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 12, step: 46
	action: tensor([[ 0.4483, -1.4421,  0.7253, -0.8345, -0.1865, -1.0076,  0.4910]],
       dtype=torch.float64)
	q_value: tensor([[-55.3523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7984775637921897, distance: 1.5346495113178453 entropy 1.448572039604187
epoch: 12, step: 47
	action: tensor([[ 0.3473, -1.8365, -2.0456, -0.5491, -1.6240, -0.5129,  0.1443]],
       dtype=torch.float64)
	q_value: tensor([[-31.9116]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 12, step: 48
	action: tensor([[-0.0014,  0.7993,  1.3340,  0.5645,  0.3146,  0.8565, -0.8994]],
       dtype=torch.float64)
	q_value: tensor([[-55.3523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 12, step: 49
	action: tensor([[ 0.5161, -1.2611, -0.0269, -0.6266, -1.0590,  0.9762,  0.4077]],
       dtype=torch.float64)
	q_value: tensor([[-55.3523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7782390771377015, distance: 1.5259902828037828 entropy 1.448572039604187
epoch: 12, step: 50
	action: tensor([[-0.6216, -0.9376,  0.4448,  0.5680, -0.9366,  1.6182,  0.5474]],
       dtype=torch.float64)
	q_value: tensor([[-38.9495]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2555337045483388, distance: 1.2822446070192415 entropy 1.448572039604187
epoch: 12, step: 51
	action: tensor([[ 1.0409,  0.4592, -0.7631,  1.0948, -0.4664, -0.3301,  0.1330]],
       dtype=torch.float64)
	q_value: tensor([[-45.0018]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 12, step: 52
	action: tensor([[-0.9259,  0.8011,  0.6598, -0.4885,  1.0214,  1.5375, -1.2954]],
       dtype=torch.float64)
	q_value: tensor([[-55.3523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07982128807085531, distance: 1.0977230513427718 entropy 1.448572039604187
epoch: 12, step: 53
	action: tensor([[-0.2694, -1.2503, -2.3340, -0.4009, -0.5878,  1.7582, -0.5781]],
       dtype=torch.float64)
	q_value: tensor([[-48.8034]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.556110974383482, distance: 1.4275028239203444 entropy 1.448572039604187
epoch: 12, step: 54
	action: tensor([[ 0.4675, -0.9942,  1.2110,  0.2347,  0.6418,  2.0404,  1.1420]],
       dtype=torch.float64)
	q_value: tensor([[-53.4580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1706179188464957, distance: 1.04215908145623 entropy 1.448572039604187
epoch: 12, step: 55
	action: tensor([[ 0.8835, -1.0750, -0.1092, -1.2904,  0.5614,  0.3129, -0.1293]],
       dtype=torch.float64)
	q_value: tensor([[-48.2611]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9122558705208526, distance: 1.582448899632727 entropy 1.448572039604187
epoch: 12, step: 56
	action: tensor([[ 0.6039, -0.6971, -0.1170,  0.8679, -0.0128, -2.0776,  0.9053]],
       dtype=torch.float64)
	q_value: tensor([[-34.1828]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23962368925043775, distance: 0.997863144091325 entropy 1.448572039604187
epoch: 12, step: 57
	action: tensor([[-0.5315, -1.3066,  0.0373,  1.1019,  1.0206, -0.7835,  1.8194]],
       dtype=torch.float64)
	q_value: tensor([[-40.3999]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8010041618307604, distance: 1.5357271120811298 entropy 1.448572039604187
epoch: 12, step: 58
	action: tensor([[-2.0706,  0.3814,  0.7775, -0.5897,  0.7719,  0.5926,  0.5549]],
       dtype=torch.float64)
	q_value: tensor([[-42.6138]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 12, step: 59
	action: tensor([[ 0.9737,  1.8115, -1.9913, -0.5951,  0.2901,  0.6372, -0.1175]],
       dtype=torch.float64)
	q_value: tensor([[-55.3523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 12, step: 60
	action: tensor([[-1.6475,  0.2162, -0.5096,  0.8783, -1.0125,  2.0489,  1.0925]],
       dtype=torch.float64)
	q_value: tensor([[-55.3523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.49131165407991273, distance: 1.3974648696748841 entropy 1.448572039604187
epoch: 12, step: 61
	action: tensor([[ 1.9349, -0.5232, -0.5715,  2.2780,  0.0865,  0.2376, -2.0828]],
       dtype=torch.float64)
	q_value: tensor([[-57.8967]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8129328291369479, distance: 0.49494333730526824 entropy 1.448572039604187
epoch: 12, step: 62
	action: tensor([[-1.0659,  1.5094,  0.0103,  1.5143,  1.1804,  0.2814, -0.6136]],
       dtype=torch.float64)
	q_value: tensor([[-41.4572]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5601471020898019, distance: 0.7589452060907839 entropy 1.448572039604187
epoch: 12, step: 63
	action: tensor([[ 1.0536,  0.1983,  0.1025,  0.5302,  1.2389, -0.0197, -1.2998]],
       dtype=torch.float64)
	q_value: tensor([[-43.3532]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 12, step: 64
	action: tensor([[ 0.1621, -0.0470,  0.1510, -0.3104,  1.3959,  0.5287,  0.1872]],
       dtype=torch.float64)
	q_value: tensor([[-55.3523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.40149086915241383, distance: 0.8853033007970574 entropy 1.448572039604187
epoch: 12, step: 65
	action: tensor([[ 0.0811,  0.7589, -0.7190,  0.2788, -0.9779,  0.7071,  0.3188]],
       dtype=torch.float64)
	q_value: tensor([[-31.4523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 12, step: 66
	action: tensor([[ 1.0835,  1.0308, -0.9630,  0.2417,  0.9753,  0.0075, -0.9743]],
       dtype=torch.float64)
	q_value: tensor([[-55.3523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 12, step: 67
	action: tensor([[ 1.0770,  0.2503, -0.1923, -1.2799, -0.1846, -1.5526, -0.6514]],
       dtype=torch.float64)
	q_value: tensor([[-55.3523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 12, step: 68
	action: tensor([[-0.7886, -0.9445, -0.8198,  0.1444,  1.8879,  0.0221, -0.5054]],
       dtype=torch.float64)
	q_value: tensor([[-55.3523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2284623368329135, distance: 1.7082811256751538 entropy 1.448572039604187
epoch: 12, step: 69
	action: tensor([[-0.4249, -1.6509,  0.7149, -0.8275, -0.4485,  1.0939,  0.2247]],
       dtype=torch.float64)
	q_value: tensor([[-42.9202]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 12, step: 70
	action: tensor([[-0.4362, -0.8795,  0.5435,  0.0076, -0.9022,  0.7301, -1.1616]],
       dtype=torch.float64)
	q_value: tensor([[-55.3523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6602008574700462, distance: 1.474473631026346 entropy 1.448572039604187
epoch: 12, step: 71
	action: tensor([[-0.3428,  0.0826,  0.6063,  0.0607,  0.8279,  0.5384, -0.2175]],
       dtype=torch.float64)
	q_value: tensor([[-36.5645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2400975296922193, distance: 0.9975521785918451 entropy 1.448572039604187
epoch: 12, step: 72
	action: tensor([[-0.2461, -0.5524, -1.8714, -1.6332,  1.6374, -0.6034, -1.5349]],
       dtype=torch.float64)
	q_value: tensor([[-28.5911]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6148846883150729, distance: 0.710153261967265 entropy 1.448572039604187
epoch: 12, step: 73
	action: tensor([[-0.7847,  2.1932,  1.0321,  1.8953, -0.9543,  0.2740,  0.0888]],
       dtype=torch.float64)
	q_value: tensor([[-57.8216]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 12, step: 74
	action: tensor([[-0.9016, -0.0281,  0.2432,  0.2261,  0.9319,  0.5574,  1.3155]],
       dtype=torch.float64)
	q_value: tensor([[-55.3523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.44788277646965935, distance: 1.3769665625454306 entropy 1.448572039604187
epoch: 12, step: 75
	action: tensor([[ 1.0111,  0.4802, -1.4780, -1.1224,  0.6028,  0.1976,  2.4339]],
       dtype=torch.float64)
	q_value: tensor([[-38.7242]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6038679525522509, distance: 0.7202390803408895 entropy 1.448572039604187
epoch: 12, step: 76
	action: tensor([[ 0.2668, -1.5239,  1.3257,  0.1301, -0.4785,  0.9258,  0.6678]],
       dtype=torch.float64)
	q_value: tensor([[-58.2261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13266014715647123, distance: 1.2178856218530962 entropy 1.448572039604187
epoch: 12, step: 77
	action: tensor([[ 0.4676,  0.0436,  1.3883, -2.5136,  0.2097, -1.0138,  0.0140]],
       dtype=torch.float64)
	q_value: tensor([[-37.8750]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15082837246449543, distance: 1.0545190526867432 entropy 1.448572039604187
epoch: 12, step: 78
	action: tensor([[-1.6954, -0.0146,  0.9560, -0.2337, -0.0018, -0.9381,  1.3011]],
       dtype=torch.float64)
	q_value: tensor([[-40.7303]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.7068897887789953, distance: 1.8827470689599903 entropy 1.448572039604187
epoch: 12, step: 79
	action: tensor([[ 0.1128,  0.3990,  0.3527, -0.2397, -1.4124, -1.5005, -1.7679]],
       dtype=torch.float64)
	q_value: tensor([[-41.9415]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7219747195468322, distance: 0.6033910124261419 entropy 1.448572039604187
epoch: 12, step: 80
	action: tensor([[-0.0521, -0.1957,  1.4695, -1.4865,  1.4801,  1.4367, -0.3757]],
       dtype=torch.float64)
	q_value: tensor([[-41.0080]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.38591085347399434, distance: 1.3471760122677727 entropy 1.448572039604187
epoch: 12, step: 81
	action: tensor([[ 0.3744, -1.7561,  0.0859, -0.1108, -1.1637, -0.4212,  0.0431]],
       dtype=torch.float64)
	q_value: tensor([[-47.3155]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 12, step: 82
	action: tensor([[-0.2626,  1.2762,  0.4782, -0.8510, -0.6800,  0.1871,  1.2898]],
       dtype=torch.float64)
	q_value: tensor([[-55.3523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 12, step: 83
	action: tensor([[ 0.9869,  0.0896,  0.7722,  1.4128,  0.2472, -1.4038,  0.7863]],
       dtype=torch.float64)
	q_value: tensor([[-55.3523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 12, step: 84
	action: tensor([[-1.5110,  0.5240,  1.5642, -0.5336,  1.0990,  0.1891,  0.2411]],
       dtype=torch.float64)
	q_value: tensor([[-55.3523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2970482491215596, distance: 1.73436999711167 entropy 1.448572039604187
epoch: 12, step: 85
	action: tensor([[-0.5277,  0.1245, -0.8781,  0.6310, -0.1659, -1.2942,  0.1009]],
       dtype=torch.float64)
	q_value: tensor([[-45.8236]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05615452546487787, distance: 1.1760354831575104 entropy 1.448572039604187
epoch: 12, step: 86
	action: tensor([[-0.1650, -0.2170,  0.4971, -0.4294, -0.7805,  0.6041, -1.1509]],
       dtype=torch.float64)
	q_value: tensor([[-32.8643]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2132634517448604, distance: 1.2604750419383866 entropy 1.448572039604187
epoch: 12, step: 87
	action: tensor([[-0.8548, -0.1728, -0.6321,  1.5564, -0.0139, -0.8811, -0.4936]],
       dtype=torch.float64)
	q_value: tensor([[-32.0844]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5110731991067772, distance: 1.4066933834183286 entropy 1.448572039604187
epoch: 12, step: 88
	action: tensor([[-1.1347, -1.7002,  1.8989, -0.5901,  0.8915,  0.7473, -0.7417]],
       dtype=torch.float64)
	q_value: tensor([[-34.5640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 12, step: 89
	action: tensor([[-0.7292, -0.3763, -1.7153, -2.4794,  0.4610, -0.2679, -0.3237]],
       dtype=torch.float64)
	q_value: tensor([[-55.3523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7392404141365705, distance: 0.5843550944221434 entropy 1.448572039604187
epoch: 12, step: 90
	action: tensor([[ 1.0275, -0.7814, -1.3080,  3.0148, -0.7280,  0.3077, -1.6012]],
       dtype=torch.float64)
	q_value: tensor([[-57.8262]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4632253980853728, distance: 0.838402708268556 entropy 1.448572039604187
epoch: 12, step: 91
	action: tensor([[-0.6864, -0.2042,  1.2097, -0.5303,  1.8901,  1.6933,  0.2902]],
       dtype=torch.float64)
	q_value: tensor([[-40.4532]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 12, step: 92
	action: tensor([[ 0.8431,  0.0698, -1.2340,  1.3152, -0.2940, -0.1073,  1.2302]],
       dtype=torch.float64)
	q_value: tensor([[-55.3523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 12, step: 93
	action: tensor([[ 1.1315, -0.3907, -0.9305,  1.0719,  0.5913,  2.4952, -1.7005]],
       dtype=torch.float64)
	q_value: tensor([[-55.3523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 12, step: 94
	action: tensor([[-0.7206,  0.9624, -1.6690, -1.1103,  0.9465, -0.2289,  2.2053]],
       dtype=torch.float64)
	q_value: tensor([[-55.3523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 12, step: 95
	action: tensor([[-0.2855, -0.6342, -0.2360,  0.1080, -0.9773,  0.6797, -1.4612]],
       dtype=torch.float64)
	q_value: tensor([[-55.3523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.548913144550474, distance: 1.4241975224770789 entropy 1.448572039604187
epoch: 12, step: 96
	action: tensor([[-1.5076, -0.1025, -0.8774,  0.9692,  1.8807, -0.3067, -0.6066]],
       dtype=torch.float64)
	q_value: tensor([[-34.4213]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.3806280085393454, distance: 1.7656412344155827 entropy 1.448572039604187
epoch: 12, step: 97
	action: tensor([[ 0.0086, -0.2065, -0.5985,  0.2515, -0.9970,  0.3187,  1.0204]],
       dtype=torch.float64)
	q_value: tensor([[-44.6591]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0618005917771427, distance: 1.1084197885403428 entropy 1.448572039604187
epoch: 12, step: 98
	action: tensor([[ 0.1094,  0.9637, -0.7544, -0.6661,  0.3845,  0.2489, -0.7739]],
       dtype=torch.float64)
	q_value: tensor([[-35.4322]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 12, step: 99
	action: tensor([[ 0.3210, -2.2136, -0.9212,  0.9812, -0.3752,  0.1126, -0.1486]],
       dtype=torch.float64)
	q_value: tensor([[-55.3523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 12, step: 100
	action: tensor([[ 0.0197,  1.0226, -0.5089, -1.7065, -1.1712, -0.7304,  0.0752]],
       dtype=torch.float64)
	q_value: tensor([[-55.3523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 12, step: 101
	action: tensor([[-8.0055e-01,  8.9966e-04,  1.6990e-01, -1.6826e+00, -3.1692e-01,
         -5.6770e-01,  1.1769e+00]], dtype=torch.float64)
	q_value: tensor([[-55.3523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.44605445342233496, distance: 1.3760969013392295 entropy 1.448572039604187
epoch: 12, step: 102
	action: tensor([[-0.3287, -2.5249, -0.5726,  0.2385,  0.7750, -1.0232, -1.8160]],
       dtype=torch.float64)
	q_value: tensor([[-46.1082]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 12, step: 103
	action: tensor([[ 0.7460, -0.1272,  1.2338, -0.9350, -0.0035,  1.9991, -0.8407]],
       dtype=torch.float64)
	q_value: tensor([[-55.3523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7424914202963686, distance: 0.5807009613639661 entropy 1.448572039604187
epoch: 12, step: 104
	action: tensor([[ 2.8157, -1.2942, -0.5264,  2.4816,  0.2092, -0.0991, -1.0828]],
       dtype=torch.float64)
	q_value: tensor([[-43.6764]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 12, step: 105
	action: tensor([[ 1.7236, -0.2211,  0.3187,  0.1768, -1.2889, -0.2363,  0.7804]],
       dtype=torch.float64)
	q_value: tensor([[-55.3523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 12, step: 106
	action: tensor([[-0.2091, -0.4499, -0.9174, -1.4799, -0.9394,  1.5183,  0.8899]],
       dtype=torch.float64)
	q_value: tensor([[-55.3523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1896216924580132, distance: 1.2481337610847327 entropy 1.448572039604187
epoch: 12, step: 107
	action: tensor([[0.7355, 2.1867, 0.4388, 0.8409, 0.1582, 0.3852, 0.7959]],
       dtype=torch.float64)
	q_value: tensor([[-55.3586]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 12, step: 108
	action: tensor([[ 0.8948,  0.0215,  0.5539, -0.3174,  0.4528,  0.5317,  0.1921]],
       dtype=torch.float64)
	q_value: tensor([[-55.3523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7251187032831543, distance: 0.5999696595963911 entropy 1.448572039604187
epoch: 12, step: 109
	action: tensor([[-0.4878, -1.6755, -1.0944,  0.2262, -0.1039, -1.4186, -0.5862]],
       dtype=torch.float64)
	q_value: tensor([[-24.4192]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 12, step: 110
	action: tensor([[ 1.7005,  0.2235,  0.8530,  0.1402, -0.4585,  0.0081,  1.1402]],
       dtype=torch.float64)
	q_value: tensor([[-55.3523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 12, step: 111
	action: tensor([[ 0.3725,  2.0210, -0.8785,  0.3302,  0.1563, -0.6078, -0.5475]],
       dtype=torch.float64)
	q_value: tensor([[-55.3523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 12, step: 112
	action: tensor([[ 0.6032, -0.0466, -0.5304,  2.9354,  1.9989, -0.1895, -0.3182]],
       dtype=torch.float64)
	q_value: tensor([[-55.3523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 12, step: 113
	action: tensor([[-1.3839, -0.6229, -1.8489, -0.1060, -0.8612, -0.0408,  0.0934]],
       dtype=torch.float64)
	q_value: tensor([[-55.3523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.6570325053634702, distance: 1.8653276462472415 entropy 1.448572039604187
epoch: 12, step: 114
	action: tensor([[-0.7232, -1.3363, -0.9201,  0.6808,  0.0325, -1.6781,  0.3158]],
       dtype=torch.float64)
	q_value: tensor([[-48.1784]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6797484389396202, distance: 1.4831286226099676 entropy 1.448572039604187
epoch: 12, step: 115
	action: tensor([[ 1.0912,  0.6588,  2.3731, -0.7347, -1.3933, -0.3205,  0.4883]],
       dtype=torch.float64)
	q_value: tensor([[-40.1607]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9160260265343185, distance: 0.33161108916061965 entropy 1.448572039604187
epoch: 12, step: 116
	action: tensor([[-0.5398, -2.6499, -0.9704, -0.9159,  1.0384, -0.6318, -0.9052]],
       dtype=torch.float64)
	q_value: tensor([[-46.9043]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 12, step: 117
	action: tensor([[-2.0148, -0.5607, -1.5776, -0.7935,  1.5714, -2.3949,  2.6608]],
       dtype=torch.float64)
	q_value: tensor([[-55.3523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 12, step: 118
	action: tensor([[ 1.4087, -0.3507,  0.6848, -0.1573,  2.8055,  1.1822, -0.5615]],
       dtype=torch.float64)
	q_value: tensor([[-55.3523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 12, step: 119
	action: tensor([[-0.4280,  0.8779, -0.4332, -0.0579,  0.0055, -0.7286,  0.2523]],
       dtype=torch.float64)
	q_value: tensor([[-55.3523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 12, step: 120
	action: tensor([[ 1.2750,  1.2991, -0.5903, -0.4519,  0.9787, -0.3319, -0.5464]],
       dtype=torch.float64)
	q_value: tensor([[-55.3523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 12, step: 121
	action: tensor([[ 1.3994, -0.5209,  0.2569,  0.1927,  0.0959,  0.9211, -0.6976]],
       dtype=torch.float64)
	q_value: tensor([[-55.3523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4138868684704641, distance: 0.8760873692855424 entropy 1.448572039604187
epoch: 12, step: 122
	action: tensor([[-0.4579, -1.2432,  1.4321,  1.4841,  1.6177,  0.5171,  0.3268]],
       dtype=torch.float64)
	q_value: tensor([[-27.7568]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3691102424841656, distance: 0.9089362655134711 entropy 1.448572039604187
epoch: 12, step: 123
	action: tensor([[-0.1688,  0.3588,  0.6885, -0.4780, -1.7602,  0.9915,  2.5503]],
       dtype=torch.float64)
	q_value: tensor([[-42.7625]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.02568800917461167, distance: 1.1589490197259167 entropy 1.448572039604187
epoch: 12, step: 124
	action: tensor([[ 0.8334, -0.0569, -0.0199,  2.0398,  0.2486, -0.8646, -1.4877]],
       dtype=torch.float64)
	q_value: tensor([[-65.2892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 12, step: 125
	action: tensor([[ 0.7397,  0.5693, -0.7660, -0.0080, -2.1901, -2.1282, -0.4367]],
       dtype=torch.float64)
	q_value: tensor([[-55.3523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 12, step: 126
	action: tensor([[ 1.5873, -2.2778, -1.6586,  0.9385,  2.9709, -0.6040, -0.6823]],
       dtype=torch.float64)
	q_value: tensor([[-55.3523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 12, step: 127
	action: tensor([[-0.9745, -1.5601, -0.7055, -0.8516, -1.0022, -0.7267,  0.4091]],
       dtype=torch.float64)
	q_value: tensor([[-55.3523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.22640739744309157, distance: 1.2672843566835565 entropy 1.448572039604187
LOSS epoch 12 actor 536.7788843838623 critic 85.92187569537482 
epoch: 13, step: 0
	action: tensor([[-1.2184,  1.2266, -0.4616,  1.3761,  0.2837,  1.0919,  0.0811]],
       dtype=torch.float64)
	q_value: tensor([[-37.5820]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4186143424710572, distance: 0.872547041160514 entropy 1.448572039604187
epoch: 13, step: 1
	action: tensor([[-0.5458, -0.8176, -0.3128,  0.3276,  0.6132,  0.3687,  0.1565]],
       dtype=torch.float64)
	q_value: tensor([[-35.3122]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6854590530783666, distance: 1.485647567954938 entropy 1.448572039604187
epoch: 13, step: 2
	action: tensor([[-0.5663, -0.3458,  0.8638,  0.4987,  0.4967, -0.6415, -1.0335]],
       dtype=torch.float64)
	q_value: tensor([[-23.4359]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.38716036102037465, distance: 1.3477831679704844 entropy 1.448572039604187
epoch: 13, step: 3
	action: tensor([[-0.3208,  0.1226, -0.0143,  0.7298, -0.2384,  1.2306,  2.2508]],
       dtype=torch.float64)
	q_value: tensor([[-30.0801]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 13, step: 4
	action: tensor([[-0.4250, -0.3263,  0.6258, -0.1097, -1.1870,  0.6172,  0.3222]],
       dtype=torch.float64)
	q_value: tensor([[-45.7260]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4973387851566846, distance: 1.4002859470106102 entropy 1.448572039604187
epoch: 13, step: 5
	action: tensor([[ 0.4065,  1.6545, -0.4994, -0.5843,  0.6792,  0.5355, -0.3486]],
       dtype=torch.float64)
	q_value: tensor([[-31.1262]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 13, step: 6
	action: tensor([[-1.6318, -1.7897,  0.6132,  0.1681,  1.2222, -2.3127, -0.5981]],
       dtype=torch.float64)
	q_value: tensor([[-45.7260]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 13, step: 7
	action: tensor([[-0.5780, -0.3076, -0.1144,  0.3254, -0.3072, -1.3425, -0.4628]],
       dtype=torch.float64)
	q_value: tensor([[-45.7260]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.33332268032903056, distance: 1.3213696474446686 entropy 1.448572039604187
epoch: 13, step: 8
	action: tensor([[ 0.6450, -0.2188,  0.2654,  0.6865,  0.0856, -0.8747,  0.7211]],
       dtype=torch.float64)
	q_value: tensor([[-27.8570]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.701723846976297, distance: 0.6249797699963375 entropy 1.448572039604187
epoch: 13, step: 9
	action: tensor([[-0.4045, -1.5033, -1.3266, -0.5682, -0.0319,  0.8363, -1.7816]],
       dtype=torch.float64)
	q_value: tensor([[-25.5544]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 13, step: 10
	action: tensor([[ 0.8661,  0.0281,  1.1518, -0.7827,  0.4718, -0.0172,  1.2767]],
       dtype=torch.float64)
	q_value: tensor([[-45.7260]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5365117750293117, distance: 0.7790692193155366 entropy 1.448572039604187
epoch: 13, step: 11
	action: tensor([[ 0.7739, -0.3405,  1.9634,  0.8698, -0.7433, -0.3116,  0.2750]],
       dtype=torch.float64)
	q_value: tensor([[-31.1833]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21971661361795747, distance: 1.0108410572888533 entropy 1.448572039604187
epoch: 13, step: 12
	action: tensor([[ 0.1244,  1.5037, -1.5371, -1.0225, -0.9884,  0.7250, -0.2647]],
       dtype=torch.float64)
	q_value: tensor([[-36.8340]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 13, step: 13
	action: tensor([[ 0.6931,  0.4300,  1.3344,  0.8200, -0.9856,  0.2300, -1.2729]],
       dtype=torch.float64)
	q_value: tensor([[-45.7260]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 13, step: 14
	action: tensor([[-1.0937, -0.6404,  0.2954, -1.1492, -0.8671, -0.0100,  0.3685]],
       dtype=torch.float64)
	q_value: tensor([[-45.7260]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.179974663057707, distance: 1.6895942243321933 entropy 1.448572039604187
epoch: 13, step: 15
	action: tensor([[ 2.0016,  0.7229, -0.3802, -0.4443, -0.2561,  0.7450, -0.9334]],
       dtype=torch.float64)
	q_value: tensor([[-34.9775]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6583537855337346, distance: 0.6688750551053517 entropy 1.448572039604187
epoch: 13, step: 16
	action: tensor([[-0.4470,  0.7673, -0.6316,  1.2820,  1.0011,  0.8270,  0.3249]],
       dtype=torch.float64)
	q_value: tensor([[-24.3440]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 13, step: 17
	action: tensor([[-0.3393,  1.4858, -1.7112,  1.6627, -0.3259, -0.9356,  1.7251]],
       dtype=torch.float64)
	q_value: tensor([[-45.7260]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 13, step: 18
	action: tensor([[ 0.1607,  0.0329,  0.2891,  0.3931, -0.9699, -0.1206,  0.2377]],
       dtype=torch.float64)
	q_value: tensor([[-45.7260]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6481300108025307, distance: 0.6788093365510116 entropy 1.448572039604187
epoch: 13, step: 19
	action: tensor([[-0.9750, -0.2555,  1.6344,  0.1205,  0.6303,  2.0710,  0.3854]],
       dtype=torch.float64)
	q_value: tensor([[-23.6374]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1220906177013783, distance: 1.212189893473695 entropy 1.448572039604187
epoch: 13, step: 20
	action: tensor([[-0.9603,  0.9482, -0.1587, -0.6485,  0.3220,  1.0865,  0.7616]],
       dtype=torch.float64)
	q_value: tensor([[-43.3097]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.031636699150702996, distance: 1.1260971370481438 entropy 1.448572039604187
epoch: 13, step: 21
	action: tensor([[-0.4243, -0.4024, -0.3487, -2.0194, -0.2894, -0.3155, -1.3605]],
       dtype=torch.float64)
	q_value: tensor([[-38.9493]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.01140047326114968, distance: 1.150848800845054 entropy 1.448572039604187
epoch: 13, step: 22
	action: tensor([[ 1.4454, -0.1423, -0.2851, -0.7733, -0.2778,  0.6662,  1.3948]],
       dtype=torch.float64)
	q_value: tensor([[-39.0160]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1933538169038791, distance: 1.0277754385494173 entropy 1.448572039604187
epoch: 13, step: 23
	action: tensor([[ 0.2682, -1.7972,  1.9227,  0.5283,  1.4975, -0.1204,  0.5089]],
       dtype=torch.float64)
	q_value: tensor([[-35.6118]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 13, step: 24
	action: tensor([[-1.3045, -1.4906, -1.0868,  0.6489, -0.0946, -1.4030, -0.2210]],
       dtype=torch.float64)
	q_value: tensor([[-45.7260]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1442652069298034, distance: 1.6756987382056674 entropy 1.448572039604187
epoch: 13, step: 25
	action: tensor([[-0.8050, -1.4878,  0.7214, -1.4169, -0.3737, -1.3283, -0.6422]],
       dtype=torch.float64)
	q_value: tensor([[-37.0641]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 13, step: 26
	action: tensor([[-0.6274, -0.4795, -0.0856, -1.3116, -1.1209,  0.6326, -0.3144]],
       dtype=torch.float64)
	q_value: tensor([[-45.7260]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9070451530897554, distance: 1.5802914166714284 entropy 1.448572039604187
epoch: 13, step: 27
	action: tensor([[ 1.7774, -0.3253, -1.0612, -1.1696,  0.2852, -0.6630, -1.3108]],
       dtype=torch.float64)
	q_value: tensor([[-35.7755]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5810857760448156, distance: 1.4389125785705807 entropy 1.448572039604187
epoch: 13, step: 28
	action: tensor([[ 1.1541, -0.9814,  0.0280, -0.4401, -0.1583, -0.1544, -0.7431]],
       dtype=torch.float64)
	q_value: tensor([[-31.2260]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7133115061762978, distance: 1.497872539838182 entropy 1.448572039604187
epoch: 13, step: 29
	action: tensor([[-0.2053, -0.0860, -0.5318, -0.4672,  0.7278, -1.1764, -0.0318]],
       dtype=torch.float64)
	q_value: tensor([[-23.5625]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12461266389772407, distance: 1.2135514069404374 entropy 1.448572039604187
epoch: 13, step: 30
	action: tensor([[ 1.2558,  0.8530, -0.2705,  2.0967,  0.1802, -0.8775, -1.4345]],
       dtype=torch.float64)
	q_value: tensor([[-27.2324]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 13, step: 31
	action: tensor([[-1.1868e+00,  3.2312e-01,  1.5352e-03,  1.4731e+00, -1.2364e+00,
         -1.7269e+00,  1.4268e+00]], dtype=torch.float64)
	q_value: tensor([[-45.7260]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3090631168769218, distance: 0.951208848344192 entropy 1.448572039604187
epoch: 13, step: 32
	action: tensor([[-0.2041, -0.4496, -1.9510, -0.2685,  0.9649, -0.2602,  0.6257]],
       dtype=torch.float64)
	q_value: tensor([[-47.5669]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05751100245438279, distance: 1.1767904641477127 entropy 1.448572039604187
epoch: 13, step: 33
	action: tensor([[-1.1474, -1.0659,  1.3368,  1.1625,  0.7702,  1.6671,  0.2277]],
       dtype=torch.float64)
	q_value: tensor([[-36.5677]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24005627880756908, distance: 0.9975792540081153 entropy 1.448572039604187
epoch: 13, step: 34
	action: tensor([[ 0.5550, -0.9384, -0.9515,  0.8013, -1.1584,  0.5828, -0.0722]],
       dtype=torch.float64)
	q_value: tensor([[-41.0553]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0894666443639549, distance: 1.1944381395569712 entropy 1.448572039604187
epoch: 13, step: 35
	action: tensor([[-0.5097, -1.1719,  1.6164,  1.1065, -0.1801,  0.6003,  0.8345]],
       dtype=torch.float64)
	q_value: tensor([[-28.2888]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.337952637406826, distance: 0.9311104978907893 entropy 1.448572039604187
epoch: 13, step: 36
	action: tensor([[ 0.7364,  0.0856,  0.1588, -0.9192, -1.1738, -0.0568,  0.7305]],
       dtype=torch.float64)
	q_value: tensor([[-37.5280]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30064748691802134, distance: 0.9569841903882121 entropy 1.448572039604187
epoch: 13, step: 37
	action: tensor([[-1.2637, -0.6084, -0.2260,  1.0189,  0.6747, -0.1101, -1.5950]],
       dtype=torch.float64)
	q_value: tensor([[-29.7299]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1502695982134727, distance: 1.6780432516310166 entropy 1.448572039604187
epoch: 13, step: 38
	action: tensor([[ 0.7949, -1.2740,  1.3782,  0.3927, -1.3123,  0.7456, -2.0707]],
       dtype=torch.float64)
	q_value: tensor([[-35.3946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11672808543904689, distance: 1.2092898636662497 entropy 1.448572039604187
epoch: 13, step: 39
	action: tensor([[ 1.0535,  0.8872, -0.9038, -0.6751, -0.0148, -1.1554, -0.4367]],
       dtype=torch.float64)
	q_value: tensor([[-42.2332]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6846024783454396, distance: 0.6426667504470608 entropy 1.448572039604187
epoch: 13, step: 40
	action: tensor([[ 1.1347, -0.3161, -0.4108, -0.0392, -2.4032,  0.1267,  1.0647]],
       dtype=torch.float64)
	q_value: tensor([[-28.1642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3672513170095032, distance: 0.9102743772425955 entropy 1.448572039604187
epoch: 13, step: 41
	action: tensor([[-0.0973, -0.5502, -0.4656,  1.0345,  0.2902,  0.8609, -1.8534]],
       dtype=torch.float64)
	q_value: tensor([[-43.9503]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3359844034617615, distance: 0.9324935433672927 entropy 1.448572039604187
epoch: 13, step: 42
	action: tensor([[ 1.3691, -0.3008, -4.3749,  0.7116, -1.6982, -0.8238,  3.5903]],
       dtype=torch.float64)
	q_value: tensor([[-31.8349]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2356090168193423 entropy 1.448572039604187
epoch: 13, step: 43
	action: tensor([[-0.7534,  0.3467,  0.0194,  1.0429, -0.9220,  0.4901, -1.2148]],
       dtype=torch.float64)
	q_value: tensor([[-45.7260]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14339563657537946, distance: 1.0591240611064574 entropy 1.448572039604187
epoch: 13, step: 44
	action: tensor([[ 0.0770,  0.5906, -0.9598,  0.9956,  0.5212,  0.7322, -1.0572]],
       dtype=torch.float64)
	q_value: tensor([[-31.0901]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 13, step: 45
	action: tensor([[-0.5503, -0.0872,  2.7696,  0.1781,  0.0866, -0.0272,  0.6858]],
       dtype=torch.float64)
	q_value: tensor([[-45.7260]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.47584006456465966, distance: 1.3901969816251967 entropy 1.448572039604187
epoch: 13, step: 46
	action: tensor([[ 2.1056,  1.1736,  0.8892,  0.6395, -1.1082, -1.2525, -1.0685]],
       dtype=torch.float64)
	q_value: tensor([[-40.3853]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4416174559638124, distance: 0.8551112296636895 entropy 1.448572039604187
epoch: 13, step: 47
	action: tensor([[-0.5849,  0.3689, -1.9278, -0.6283, -0.6923,  0.9053,  0.0473]],
       dtype=torch.float64)
	q_value: tensor([[-37.1440]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 13, step: 48
	action: tensor([[-0.0470,  1.1789, -0.7278, -1.4964,  0.3977, -2.7550,  0.6230]],
       dtype=torch.float64)
	q_value: tensor([[-45.7260]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 13, step: 49
	action: tensor([[ 0.8667,  0.7736,  0.2561, -0.7768, -0.5925,  0.5945,  1.7983]],
       dtype=torch.float64)
	q_value: tensor([[-45.7260]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 13, step: 50
	action: tensor([[-1.4478, -2.1737,  0.9927,  0.0367,  1.1224,  0.1141, -0.2267]],
       dtype=torch.float64)
	q_value: tensor([[-45.7260]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 13, step: 51
	action: tensor([[ 0.5200, -0.8756,  0.2726, -0.3719, -0.9785, -0.6892,  1.3671]],
       dtype=torch.float64)
	q_value: tensor([[-45.7260]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5263441936354545, distance: 1.4137835779585008 entropy 1.448572039604187
epoch: 13, step: 52
	action: tensor([[-0.3538, -0.2344, -0.7180, -0.4307, -1.4825,  0.3517, -0.4065]],
       dtype=torch.float64)
	q_value: tensor([[-33.8256]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3668884631857352, distance: 1.33789870160389 entropy 1.448572039604187
epoch: 13, step: 53
	action: tensor([[ 0.1406, -0.0192, -1.2013, -0.5559, -0.7243, -0.1014, -0.1503]],
       dtype=torch.float64)
	q_value: tensor([[-31.8129]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5816561066760939, distance: 0.7401562490701734 entropy 1.448572039604187
epoch: 13, step: 54
	action: tensor([[ 1.8620, -0.2325,  0.2960, -1.0454, -0.2503, -0.5307,  0.0748]],
       dtype=torch.float64)
	q_value: tensor([[-25.2536]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2172445695933316, distance: 1.2625413655836948 entropy 1.448572039604187
epoch: 13, step: 55
	action: tensor([[-1.8613, -1.9297, -0.3186,  0.8963, -1.0513, -0.9023, -0.2815]],
       dtype=torch.float64)
	q_value: tensor([[-25.3955]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 13, step: 56
	action: tensor([[ 1.0614, -3.0969, -0.2910, -0.2456, -0.2279, -1.2843,  0.4610]],
       dtype=torch.float64)
	q_value: tensor([[-45.7260]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 13, step: 57
	action: tensor([[ 1.4389, -1.0841,  0.7731, -1.4444,  0.1205,  1.6515,  0.4496]],
       dtype=torch.float64)
	q_value: tensor([[-45.7260]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.27045307881121894, distance: 1.2898404969163841 entropy 1.448572039604187
epoch: 13, step: 58
	action: tensor([[-0.3252, -1.4068, -0.7915, -0.4787,  0.3826,  0.0385,  1.6652]],
       dtype=torch.float64)
	q_value: tensor([[-40.0843]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 13, step: 59
	action: tensor([[ 0.5912, -0.6352, -0.0287, -1.4156, -0.4352, -0.9217,  1.2831]],
       dtype=torch.float64)
	q_value: tensor([[-45.7260]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.37010031990120384, distance: 1.339469648209117 entropy 1.448572039604187
epoch: 13, step: 60
	action: tensor([[ 1.1047, -0.3904, -0.8379,  0.3404, -0.7100, -0.6487, -0.5554]],
       dtype=torch.float64)
	q_value: tensor([[-33.9424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4675876621521007, distance: 0.8349889895911196 entropy 1.448572039604187
epoch: 13, step: 61
	action: tensor([[-0.0941, -1.1753, -0.6032, -1.7056, -0.4976,  0.3538,  0.2072]],
       dtype=torch.float64)
	q_value: tensor([[-22.8222]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.29334901537330826, distance: 1.3014112650073553 entropy 1.448572039604187
epoch: 13, step: 62
	action: tensor([[-1.8190,  0.7929,  0.7184, -0.6078, -0.6128, -0.0705,  2.2088]],
       dtype=torch.float64)
	q_value: tensor([[-36.7731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2793245406888882, distance: 1.7276659600068902 entropy 1.448572039604187
epoch: 13, step: 63
	action: tensor([[ 0.8058,  1.1058, -0.5853,  0.8527,  0.7796, -0.1964,  0.8191]],
       dtype=torch.float64)
	q_value: tensor([[-49.0942]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 13, step: 64
	action: tensor([[ 0.2508, -1.5854, -2.1247,  1.4799,  0.1073, -0.7300,  0.6947]],
       dtype=torch.float64)
	q_value: tensor([[-45.7260]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9358482752577133, distance: 1.5921806853439842 entropy 1.448572039604187
epoch: 13, step: 65
	action: tensor([[ 0.6540, -0.7712, -0.5569, -1.0988, -0.3420,  0.3542,  1.2447]],
       dtype=torch.float64)
	q_value: tensor([[-37.2863]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4956962312872135, distance: 1.3995176919414387 entropy 1.448572039604187
epoch: 13, step: 66
	action: tensor([[-0.2436, -1.2043, -1.0840, -0.9520,  0.6018,  0.1395, -0.3963]],
       dtype=torch.float64)
	q_value: tensor([[-34.5241]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3455090132332972, distance: 1.3273944544917995 entropy 1.448572039604187
epoch: 13, step: 67
	action: tensor([[ 0.7486,  0.4593, -0.2194, -0.0845,  1.3725,  0.9014, -0.8151]],
       dtype=torch.float64)
	q_value: tensor([[-33.2083]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9808693306112514, distance: 0.15827844557105217 entropy 1.448572039604187
epoch: 13, step: 68
	action: tensor([[-1.0699,  0.9617,  0.7623, -0.2732,  0.0851,  1.9836,  1.9384]],
       dtype=torch.float64)
	q_value: tensor([[-28.5652]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 13, step: 69
	action: tensor([[ 1.3396,  0.1964,  0.5732, -0.7029, -1.4060,  1.4831,  0.0452]],
       dtype=torch.float64)
	q_value: tensor([[-45.7260]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 13, step: 70
	action: tensor([[ 0.3453, -0.7364,  2.7083, -0.2586, -0.9128,  0.5661, -0.5726]],
       dtype=torch.float64)
	q_value: tensor([[-45.7260]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08418291424948887, distance: 1.0951183699880385 entropy 1.448572039604187
epoch: 13, step: 71
	action: tensor([[ 0.6779,  1.5358, -0.6744,  1.7137, -2.5260,  2.4509, -0.2717]],
       dtype=torch.float64)
	q_value: tensor([[-41.9550]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 13, step: 72
	action: tensor([[ 0.2681, -2.0182, -0.9646, -0.7355, -0.4713, -0.5036,  0.7751]],
       dtype=torch.float64)
	q_value: tensor([[-45.7260]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 13, step: 73
	action: tensor([[ 0.1628, -1.2265, -1.6765, -0.6882,  1.2278,  0.3209,  1.3693]],
       dtype=torch.float64)
	q_value: tensor([[-45.7260]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.215967689778646, distance: 1.261878993936841 entropy 1.448572039604187
epoch: 13, step: 74
	action: tensor([[-0.7340,  0.7353, -0.6837,  0.9308, -0.0357,  1.7690, -0.2349]],
       dtype=torch.float64)
	q_value: tensor([[-43.3777]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15758202609338934, distance: 1.050317267546684 entropy 1.448572039604187
epoch: 13, step: 75
	action: tensor([[ 0.3478,  0.2851,  0.6048, -0.3040, -0.8665, -0.4929,  0.4595]],
       dtype=torch.float64)
	q_value: tensor([[-33.8180]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 13, step: 76
	action: tensor([[ 0.9649, -0.8883,  1.8094,  0.4295, -0.6399, -1.5203, -0.6517]],
       dtype=torch.float64)
	q_value: tensor([[-45.7260]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 13, step: 77
	action: tensor([[ 0.0017, -1.0107, -0.1866,  0.3891, -1.2567,  1.2063,  0.1895]],
       dtype=torch.float64)
	q_value: tensor([[-45.7260]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4035588039141702, distance: 1.3557262329291395 entropy 1.448572039604187
epoch: 13, step: 78
	action: tensor([[-1.1202,  0.4008, -0.9980, -0.7240,  0.4405, -1.6677, -0.1103]],
       dtype=torch.float64)
	q_value: tensor([[-33.4289]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09294581214576558, distance: 1.19634381435447 entropy 1.448572039604187
epoch: 13, step: 79
	action: tensor([[-0.3294, -0.1544,  0.0752,  0.6383, -0.3752, -0.7212,  0.0047]],
       dtype=torch.float64)
	q_value: tensor([[-38.8696]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10467424696465666, distance: 1.0827974681267472 entropy 1.448572039604187
epoch: 13, step: 80
	action: tensor([[-9.2748e-01,  4.4299e-01, -7.5471e-01,  7.5976e-01, -2.9096e-01,
          6.1682e-04, -1.0302e+00]], dtype=torch.float64)
	q_value: tensor([[-22.7034]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5125860008168732, distance: 1.407397358514676 entropy 1.448572039604187
epoch: 13, step: 81
	action: tensor([[-1.1639,  0.4062,  0.1715,  0.1217, -0.3647, -0.6990,  0.3850]],
       dtype=torch.float64)
	q_value: tensor([[-27.5608]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7816895373315518, distance: 1.5274700661088558 entropy 1.448572039604187
epoch: 13, step: 82
	action: tensor([[-0.6777,  1.0442,  0.4129,  0.7666, -0.9622,  1.8288,  0.0963]],
       dtype=torch.float64)
	q_value: tensor([[-28.9628]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 13, step: 83
	action: tensor([[-0.6885, -1.0954, -0.8283, -0.9573, -2.1982, -1.0961, -1.8010]],
       dtype=torch.float64)
	q_value: tensor([[-45.7260]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3370579987188407, distance: 1.3232192688342448 entropy 1.448572039604187
epoch: 13, step: 84
	action: tensor([[ 0.5868, -1.8535,  0.0366, -0.7748,  0.8646, -0.5882,  0.5369]],
       dtype=torch.float64)
	q_value: tensor([[-47.9108]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 13, step: 85
	action: tensor([[ 0.2796,  0.4386,  0.5703, -1.0190, -0.6128,  0.7082, -1.9234]],
       dtype=torch.float64)
	q_value: tensor([[-45.7260]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4298534333797719, distance: 0.8640720353773326 entropy 1.448572039604187
epoch: 13, step: 86
	action: tensor([[-0.6593, -1.6271, -0.3347,  0.7690,  0.7596, -1.7219,  0.1175]],
       dtype=torch.float64)
	q_value: tensor([[-33.5823]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8816346149855485, distance: 1.5697277655399955 entropy 1.448572039604187
epoch: 13, step: 87
	action: tensor([[-2.1082, -0.6428,  1.3549,  0.6631, -0.1788, -0.1515,  0.1614]],
       dtype=torch.float64)
	q_value: tensor([[-35.5876]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 13, step: 88
	action: tensor([[-1.7577,  1.5294, -2.2467,  0.8333, -0.7282,  0.7958, -1.4829]],
       dtype=torch.float64)
	q_value: tensor([[-45.7260]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 13, step: 89
	action: tensor([[ 0.4137, -0.2599,  0.0609,  2.6834, -1.2727,  0.7621, -0.5152]],
       dtype=torch.float64)
	q_value: tensor([[-45.7260]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 13, step: 90
	action: tensor([[ 0.6634, -0.0826,  1.4912, -0.4632,  0.2339, -0.6301, -0.8432]],
       dtype=torch.float64)
	q_value: tensor([[-45.7260]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5047918654819163, distance: 0.8052867961616087 entropy 1.448572039604187
epoch: 13, step: 91
	action: tensor([[ 0.3451,  1.3970,  0.3359,  0.0743, -1.0611, -0.3218,  1.1723]],
       dtype=torch.float64)
	q_value: tensor([[-28.1416]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 13, step: 92
	action: tensor([[-0.7926, -1.2351,  0.2787, -2.1604,  0.4950, -0.6224,  1.3008]],
       dtype=torch.float64)
	q_value: tensor([[-45.7260]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.02601552597392276, distance: 1.1591340394268386 entropy 1.448572039604187
epoch: 13, step: 93
	action: tensor([[ 0.3849, -1.8559, -0.1245, -1.9974,  1.5398, -0.7476, -0.3987]],
       dtype=torch.float64)
	q_value: tensor([[-43.9667]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 13, step: 94
	action: tensor([[ 0.1409,  0.6112,  0.6274, -1.2181, -1.2024, -0.3481,  0.3137]],
       dtype=torch.float64)
	q_value: tensor([[-45.7260]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2072004745010576, distance: 1.0189160047834493 entropy 1.448572039604187
epoch: 13, step: 95
	action: tensor([[ 0.5133,  0.2673,  0.3922, -0.0973,  0.4382,  0.1906,  0.1776]],
       dtype=torch.float64)
	q_value: tensor([[-31.3572]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8414975606087993, distance: 0.4555905093933829 entropy 1.448572039604187
epoch: 13, step: 96
	action: tensor([[ 1.4012,  1.4209, -0.7951,  0.8410, -0.8486,  1.4281, -0.7162]],
       dtype=torch.float64)
	q_value: tensor([[-19.1341]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 13, step: 97
	action: tensor([[ 0.2616,  0.3413,  1.1947, -0.8170,  1.6388, -0.7495, -0.9266]],
       dtype=torch.float64)
	q_value: tensor([[-45.7260]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 13, step: 98
	action: tensor([[-1.3715,  0.8209,  1.3671,  0.0942, -2.2937, -0.1789,  1.3810]],
       dtype=torch.float64)
	q_value: tensor([[-45.7260]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4990686258226542, distance: 1.40109457242139 entropy 1.448572039604187
epoch: 13, step: 99
	action: tensor([[ 1.3122, -1.2721,  0.8361,  2.1201,  1.1651, -0.5548, -1.4878]],
       dtype=torch.float64)
	q_value: tensor([[-53.1418]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 13, step: 100
	action: tensor([[ 1.1063, -0.9659,  0.4492,  2.0580, -1.0284,  0.8460, -0.0556]],
       dtype=torch.float64)
	q_value: tensor([[-45.7260]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 13, step: 101
	action: tensor([[ 0.3277, -0.7478,  0.3319, -1.6932, -1.1275, -1.0069, -1.2178]],
       dtype=torch.float64)
	q_value: tensor([[-45.7260]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3768126615862857, distance: 1.342746777867515 entropy 1.448572039604187
epoch: 13, step: 102
	action: tensor([[ 0.7432, -0.8305, -1.2506, -0.0028,  0.3006, -0.2725, -0.1198]],
       dtype=torch.float64)
	q_value: tensor([[-35.8305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21808265382940606, distance: 1.2629759265479832 entropy 1.448572039604187
epoch: 13, step: 103
	action: tensor([[-0.6476, -0.3515, -0.5173,  1.2459,  0.6653,  0.6087, -0.5675]],
       dtype=torch.float64)
	q_value: tensor([[-24.2725]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07966330801978705, distance: 1.189052045641411 entropy 1.448572039604187
epoch: 13, step: 104
	action: tensor([[-0.4762,  0.9049,  0.8672,  0.6179, -0.2255, -0.2937,  0.1365]],
       dtype=torch.float64)
	q_value: tensor([[-25.7881]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 13, step: 105
	action: tensor([[-1.6535, -0.0468,  1.8051,  2.0833, -0.9756, -0.0054,  0.3655]],
       dtype=torch.float64)
	q_value: tensor([[-45.7260]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 13, step: 106
	action: tensor([[-1.7220,  1.2529, -0.5060,  0.1037,  1.4588, -1.0999,  0.2905]],
       dtype=torch.float64)
	q_value: tensor([[-45.7260]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 13, step: 107
	action: tensor([[-0.6749,  0.6639,  0.4627, -0.7558,  0.4939,  1.4587, -0.1509]],
       dtype=torch.float64)
	q_value: tensor([[-45.7260]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.159063297927014, distance: 1.0493934447233348 entropy 1.448572039604187
epoch: 13, step: 108
	action: tensor([[-0.6652, -1.3419, -0.0490,  0.3623, -2.3015,  0.0768,  1.9227]],
       dtype=torch.float64)
	q_value: tensor([[-36.1313]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1322531265183433, distance: 1.6709985506810392 entropy 1.448572039604187
epoch: 13, step: 109
	action: tensor([[ 0.1802, -0.8909, -1.2670, -0.7087, -0.2455, -1.4616,  0.1578]],
       dtype=torch.float64)
	q_value: tensor([[-50.5002]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3572792653125819, distance: 0.9174192501870473 entropy 1.448572039604187
epoch: 13, step: 110
	action: tensor([[ 0.9210,  1.6212,  0.1172, -1.2905, -3.1049, -0.5155, -1.6534]],
       dtype=torch.float64)
	q_value: tensor([[-33.9734]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9656967767693762, distance: 0.2119454794638424 entropy 1.448572039604187
epoch: 13, step: 111
	action: tensor([[-1.6774, -0.8177, -0.2654,  1.1593, -1.6811,  1.3922, -0.0252]],
       dtype=torch.float64)
	q_value: tensor([[-51.2499]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 13, step: 112
	action: tensor([[-1.9917,  1.5704, -0.1056,  1.5756,  0.3927, -3.2585,  0.1144]],
       dtype=torch.float64)
	q_value: tensor([[-45.7260]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 13, step: 113
	action: tensor([[ 0.1500,  0.4452, -0.9275,  0.7809, -0.4240,  0.2988,  2.0752]],
       dtype=torch.float64)
	q_value: tensor([[-45.7260]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 13, step: 114
	action: tensor([[ 1.8263,  0.4604, -0.2587, -0.0460,  0.0111, -0.6122, -0.0333]],
       dtype=torch.float64)
	q_value: tensor([[-45.7260]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 13, step: 115
	action: tensor([[-0.5034, -1.1852,  0.5608, -1.7583, -0.7641,  0.7949,  0.0893]],
       dtype=torch.float64)
	q_value: tensor([[-45.7260]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9911856857082746, distance: 1.6147770710626626 entropy 1.448572039604187
epoch: 13, step: 116
	action: tensor([[-1.1988, -2.0808, -0.5253, -0.5605, -0.9283,  1.1995,  1.0183]],
       dtype=torch.float64)
	q_value: tensor([[-39.8110]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 13, step: 117
	action: tensor([[ 0.7110, -0.7650,  1.3744,  1.3050,  0.8893, -0.6107, -0.4225]],
       dtype=torch.float64)
	q_value: tensor([[-45.7260]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.48466717596292075, distance: 0.8214868126724169 entropy 1.448572039604187
epoch: 13, step: 118
	action: tensor([[ 0.2418, -0.0554, -1.4955, -0.1493, -1.0439,  0.3224,  1.1936]],
       dtype=torch.float64)
	q_value: tensor([[-34.8750]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44850048792129926, distance: 0.8498245237268122 entropy 1.448572039604187
epoch: 13, step: 119
	action: tensor([[ 0.6180, -2.5640, -0.2812,  0.8399,  0.2539,  0.9731, -0.5371]],
       dtype=torch.float64)
	q_value: tensor([[-35.5297]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 13, step: 120
	action: tensor([[-0.6294, -0.8664,  0.3065, -1.2903,  0.4482,  0.7476,  1.8677]],
       dtype=torch.float64)
	q_value: tensor([[-45.7260]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9014715115098948, distance: 1.5779804008484497 entropy 1.448572039604187
epoch: 13, step: 121
	action: tensor([[ 0.2436, -2.1164, -0.7877, -1.5049, -0.0601, -0.4344, -2.8893]],
       dtype=torch.float64)
	q_value: tensor([[-43.5871]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 13, step: 122
	action: tensor([[-0.3043,  0.8833,  1.6579, -0.6184, -1.0518,  1.2037,  1.5677]],
       dtype=torch.float64)
	q_value: tensor([[-45.7260]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 13, step: 123
	action: tensor([[ 1.0931, -0.0744, -0.1541, -0.3820, -1.8531, -0.5276,  2.2474]],
       dtype=torch.float64)
	q_value: tensor([[-45.7260]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 13, step: 124
	action: tensor([[ 0.1622, -1.5832,  0.6906,  0.7123,  0.9637, -0.1706, -0.0200]],
       dtype=torch.float64)
	q_value: tensor([[-45.7260]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11467582798795428, distance: 1.208178171648602 entropy 1.448572039604187
epoch: 13, step: 125
	action: tensor([[-0.8681,  2.5774, -0.6451, -2.0258,  1.9451,  0.1017, -0.7285]],
       dtype=torch.float64)
	q_value: tensor([[-27.0193]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 13, step: 126
	action: tensor([[ 0.1626, -1.3719, -1.5407, -1.1874, -0.1230,  0.2395,  1.2288]],
       dtype=torch.float64)
	q_value: tensor([[-45.7260]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0004281024987877746, distance: 1.1445891761071687 entropy 1.448572039604187
epoch: 13, step: 127
	action: tensor([[ 1.1589, -0.7576, -0.4862, -0.2313,  0.0033, -0.2820, -1.4301]],
       dtype=torch.float64)
	q_value: tensor([[-40.7304]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.34446594051880597, distance: 1.3268798398123254 entropy 1.448572039604187
LOSS epoch 13 actor 358.6426918253629 critic 153.78416758724745 
epoch: 14, step: 0
	action: tensor([[-1.1052,  0.2429, -0.4420,  1.5436, -1.1125, -0.4221, -0.3981]],
       dtype=torch.float64)
	q_value: tensor([[-23.8745]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.038191311627553404, distance: 1.1659915103078926 entropy 1.448572039604187
epoch: 14, step: 1
	action: tensor([[ 1.0561,  0.8560, -1.3775,  0.1926, -0.0522, -0.5022,  0.0759]],
       dtype=torch.float64)
	q_value: tensor([[-30.4084]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 14, step: 2
	action: tensor([[ 0.5091, -1.1849, -0.3206, -0.6918, -1.2709,  0.9916,  0.9745]],
       dtype=torch.float64)
	q_value: tensor([[-37.9812]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8440196420085999, distance: 1.5539586727527275 entropy 1.448572039604187
epoch: 14, step: 3
	action: tensor([[ 1.8956, -1.5378,  1.8521,  0.5187,  0.5406, -0.6856,  2.9946]],
       dtype=torch.float64)
	q_value: tensor([[-32.1868]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 14, step: 4
	action: tensor([[-1.2909,  0.0473, -0.6864, -0.4768, -0.5956,  0.7151, -0.4717]],
       dtype=torch.float64)
	q_value: tensor([[-37.9812]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.3033860864910976, distance: 1.7367610185565188 entropy 1.448572039604187
epoch: 14, step: 5
	action: tensor([[ 2.3607, -2.7508, -0.8468,  0.8275,  0.7811,  0.8197, -0.6711]],
       dtype=torch.float64)
	q_value: tensor([[-27.8359]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 14, step: 6
	action: tensor([[-1.4997,  0.8249, -0.8914, -0.4793, -0.7738,  0.8968, -1.7958]],
       dtype=torch.float64)
	q_value: tensor([[-37.9812]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8339756684552013, distance: 1.5497208569588556 entropy 1.448572039604187
epoch: 14, step: 7
	action: tensor([[-0.2613, -1.0551,  0.0546, -0.3333, -2.2599,  1.0286,  0.5411]],
       dtype=torch.float64)
	q_value: tensor([[-35.9700]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0148503379935416, distance: 1.6243443027712452 entropy 1.448572039604187
epoch: 14, step: 8
	action: tensor([[-0.0889,  1.3262, -0.5420, -0.2673, -1.0085, -1.7234,  0.1287]],
       dtype=torch.float64)
	q_value: tensor([[-38.2370]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 14, step: 9
	action: tensor([[ 1.6202, -1.0747,  1.4487, -1.9171, -1.2361, -0.7018,  1.2804]],
       dtype=torch.float64)
	q_value: tensor([[-37.9812]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24002637515684533, distance: 0.9975988810984453 entropy 1.448572039604187
epoch: 14, step: 10
	action: tensor([[-0.0698, -0.4521, -0.0206, -1.6803, -0.8750, -1.9987,  1.9470]],
       dtype=torch.float64)
	q_value: tensor([[-39.3052]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3733721253963149, distance: 0.905860970447492 entropy 1.448572039604187
epoch: 14, step: 11
	action: tensor([[-1.9663, -2.9112, -0.7538, -0.6413,  0.4446,  0.2872, -0.5721]],
       dtype=torch.float64)
	q_value: tensor([[-40.6444]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 14, step: 12
	action: tensor([[-0.1162, -0.8605, -0.9133, -0.7085,  0.9257,  1.4579,  0.8521]],
       dtype=torch.float64)
	q_value: tensor([[-37.9812]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08010415995746778, distance: 1.0975543130020635 entropy 1.448572039604187
epoch: 14, step: 13
	action: tensor([[-0.7283, -0.4589,  0.5764,  0.4664,  0.4728,  0.2729,  1.3495]],
       dtype=torch.float64)
	q_value: tensor([[-33.9744]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3233806118867031, distance: 1.3164339596893273 entropy 1.448572039604187
epoch: 14, step: 14
	action: tensor([[ 0.5550, -1.6858, -0.8651,  0.3583,  2.0839, -0.3588,  0.5862]],
       dtype=torch.float64)
	q_value: tensor([[-25.8212]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 14, step: 15
	action: tensor([[ 0.1714,  0.4952, -1.0775, -0.8145,  1.0860, -1.5227, -0.7581]],
       dtype=torch.float64)
	q_value: tensor([[-37.9812]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6160752149091919, distance: 0.709054745846152 entropy 1.448572039604187
epoch: 14, step: 16
	action: tensor([[-2.0511,  0.4819, -1.8289, -0.8652, -1.1498,  0.8975, -0.3204]],
       dtype=torch.float64)
	q_value: tensor([[-31.6080]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 14, step: 17
	action: tensor([[ 1.0314, -1.2888,  2.2946, -0.2547,  0.8218, -1.8952, -0.5834]],
       dtype=torch.float64)
	q_value: tensor([[-37.9812]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 14, step: 18
	action: tensor([[-1.3357, -0.5129, -1.6665, -0.8678,  1.1269,  1.8001,  0.3613]],
       dtype=torch.float64)
	q_value: tensor([[-37.9812]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6558696226267702, distance: 1.4725490258745522 entropy 1.448572039604187
epoch: 14, step: 19
	action: tensor([[-1.7804, -0.4356,  0.8049, -0.3342, -0.8207,  0.5321,  0.3434]],
       dtype=torch.float64)
	q_value: tensor([[-42.3647]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 14, step: 20
	action: tensor([[ 0.4241, -1.1273, -1.5468, -0.7884,  0.4283,  0.2658, -0.3516]],
       dtype=torch.float64)
	q_value: tensor([[-37.9812]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20359488436412687, distance: 1.2554425962689744 entropy 1.448572039604187
epoch: 14, step: 21
	action: tensor([[-2.4408,  0.3973,  0.3442, -0.3137, -1.2157, -1.0822, -0.9580]],
       dtype=torch.float64)
	q_value: tensor([[-28.4463]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 14, step: 22
	action: tensor([[-0.4804, -1.0706, -0.8919,  1.3113,  0.4736, -0.0914, -1.4176]],
       dtype=torch.float64)
	q_value: tensor([[-37.9812]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.854178054517764, distance: 1.5582330503767803 entropy 1.448572039604187
epoch: 14, step: 23
	action: tensor([[ 1.3756,  0.3372, -0.6778, -0.1185,  1.4256,  0.3305,  0.7776]],
       dtype=torch.float64)
	q_value: tensor([[-29.6575]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7011203488638447, distance: 0.6256117070748316 entropy 1.448572039604187
epoch: 14, step: 24
	action: tensor([[ 1.9151, -0.8857, -0.9438, -0.2589, -0.3568,  2.6239,  1.0197]],
       dtype=torch.float64)
	q_value: tensor([[-27.6371]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 14, step: 25
	action: tensor([[-0.2813,  1.6903, -1.7579,  0.7041,  0.4058,  2.3736,  0.3007]],
       dtype=torch.float64)
	q_value: tensor([[-37.9812]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 14, step: 26
	action: tensor([[ 0.3904, -0.3211,  0.1615, -1.2813,  0.2727,  1.2365,  0.1212]],
       dtype=torch.float64)
	q_value: tensor([[-37.9812]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07385992535730246, distance: 1.1012731012942192 entropy 1.448572039604187
epoch: 14, step: 27
	action: tensor([[ 0.6056,  1.5645, -1.0393,  0.4537, -0.4317,  0.9888, -0.4938]],
       dtype=torch.float64)
	q_value: tensor([[-27.8564]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 14, step: 28
	action: tensor([[-0.5762,  0.5124, -0.3264, -1.0748, -1.7635, -1.0401, -0.3425]],
       dtype=torch.float64)
	q_value: tensor([[-37.9812]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33643628933747316, distance: 0.9321761921039209 entropy 1.448572039604187
epoch: 14, step: 29
	action: tensor([[ 6.9595e-01, -5.6707e-01,  6.8809e-01,  2.8629e-01, -5.9322e-04,
          8.4702e-01, -5.9714e-01]], dtype=torch.float64)
	q_value: tensor([[-32.7934]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6172199534528975, distance: 0.7079968716538443 entropy 1.448572039604187
epoch: 14, step: 30
	action: tensor([[ 0.5998,  2.0431, -0.6408, -0.8781, -0.4356,  1.2667, -0.2766]],
       dtype=torch.float64)
	q_value: tensor([[-22.8604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 14, step: 31
	action: tensor([[ 0.6447, -1.2305, -1.8875, -0.6164, -0.2089, -0.1716,  0.4543]],
       dtype=torch.float64)
	q_value: tensor([[-37.9812]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.22218263039671204, distance: 1.2650996829569932 entropy 1.448572039604187
epoch: 14, step: 32
	action: tensor([[ 1.8319, -0.3101,  0.2149,  0.0407,  1.2066, -0.4484, -0.7222]],
       dtype=torch.float64)
	q_value: tensor([[-29.1919]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21123612404863645, distance: 1.016319362103867 entropy 1.448572039604187
epoch: 14, step: 33
	action: tensor([[ 0.0033,  1.4304, -1.9725, -1.4306,  1.9795,  2.0843,  0.5937]],
       dtype=torch.float64)
	q_value: tensor([[-26.3733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.40254165007044196, distance: 0.8845258118399923 entropy 1.448572039604187
epoch: 14, step: 34
	action: tensor([[ 0.0392, -1.6052, -1.5937, -0.3519,  1.9248, -0.1841,  0.2278]],
       dtype=torch.float64)
	q_value: tensor([[-50.5439]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 14, step: 35
	action: tensor([[-0.3984,  0.8854,  0.1108, -0.1731, -0.3421, -2.4608,  0.8441]],
       dtype=torch.float64)
	q_value: tensor([[-37.9812]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 14, step: 36
	action: tensor([[-1.7291, -0.4133, -1.9493,  1.1968,  1.7517, -1.4758, -0.2597]],
       dtype=torch.float64)
	q_value: tensor([[-37.9812]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 14, step: 37
	action: tensor([[-0.7555,  1.8429, -0.3224,  0.5146, -0.7606, -0.0175, -0.8266]],
       dtype=torch.float64)
	q_value: tensor([[-37.9812]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 14, step: 38
	action: tensor([[-1.1391,  0.9487, -1.1243, -0.7855, -0.6000, -0.1078,  0.0605]],
       dtype=torch.float64)
	q_value: tensor([[-37.9812]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11913107556489921, distance: 1.074019923589326 entropy 1.448572039604187
epoch: 14, step: 39
	action: tensor([[ 0.8883,  0.7949,  1.3520, -0.7741,  0.6807,  0.1269,  0.1939]],
       dtype=torch.float64)
	q_value: tensor([[-31.4632]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 14, step: 40
	action: tensor([[ 0.6544, -0.1462,  0.9928, -0.3754, -1.3253, -1.8608,  0.2991]],
       dtype=torch.float64)
	q_value: tensor([[-37.9812]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3647246436014445, distance: 0.9120900069376247 entropy 1.448572039604187
epoch: 14, step: 41
	action: tensor([[-0.3835, -1.9378, -0.7710,  0.2638,  0.3849,  0.0869,  2.3813]],
       dtype=torch.float64)
	q_value: tensor([[-33.8893]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 14, step: 42
	action: tensor([[-2.0994, -0.3010,  2.2780,  2.2925,  1.7838,  1.2570,  0.0922]],
       dtype=torch.float64)
	q_value: tensor([[-37.9812]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 14, step: 43
	action: tensor([[-0.4132, -0.5090, -1.8586,  0.4276,  0.6003,  0.4755, -0.3141]],
       dtype=torch.float64)
	q_value: tensor([[-37.9812]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8783734593964383, distance: 1.5683668885627993 entropy 1.448572039604187
epoch: 14, step: 44
	action: tensor([[ 1.4896, -1.1444,  0.9430, -0.1752,  0.6912, -1.3608,  0.6716]],
       dtype=torch.float64)
	q_value: tensor([[-25.1807]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04949488655223, distance: 1.1723218424045148 entropy 1.448572039604187
epoch: 14, step: 45
	action: tensor([[ 1.1071, -0.0025,  1.2358, -0.9627,  1.0997,  0.7227, -1.1699]],
       dtype=torch.float64)
	q_value: tensor([[-31.9123]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.49934239308198847, distance: 0.8097055253133414 entropy 1.448572039604187
epoch: 14, step: 46
	action: tensor([[ 0.7139, -0.7394, -1.2439,  0.8313,  0.0157, -1.5104,  0.9207]],
       dtype=torch.float64)
	q_value: tensor([[-29.1859]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1318539333171591, distance: 1.0662353922079086 entropy 1.448572039604187
epoch: 14, step: 47
	action: tensor([[-1.0043, -0.7695, -0.1778, -2.0817,  0.7327, -0.4274,  0.9983]],
       dtype=torch.float64)
	q_value: tensor([[-32.0517]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.01706302700950313, distance: 1.1540659474866612 entropy 1.448572039604187
epoch: 14, step: 48
	action: tensor([[-0.1940,  1.4944,  1.1272,  0.6869, -1.0137, -1.0306,  0.0626]],
       dtype=torch.float64)
	q_value: tensor([[-37.4767]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 14, step: 49
	action: tensor([[ 1.2770, -0.6996, -0.2378, -0.2263, -0.2438, -0.6707, -0.6805]],
       dtype=torch.float64)
	q_value: tensor([[-37.9812]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.40435284847266795, distance: 1.3561096706672908 entropy 1.448572039604187
epoch: 14, step: 50
	action: tensor([[-1.4386, -0.3345,  0.0042, -2.3449, -1.1767,  0.5361,  0.0133]],
       dtype=torch.float64)
	q_value: tensor([[-21.0593]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.37754491739020346, distance: 1.3431037993601842 entropy 1.448572039604187
epoch: 14, step: 51
	action: tensor([[ 0.5303,  1.3352, -1.0624, -0.3045, -0.1395,  0.6548,  1.2546]],
       dtype=torch.float64)
	q_value: tensor([[-41.3389]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 14, step: 52
	action: tensor([[-0.5487, -0.9210, -0.8751, -1.5879, -1.5508,  1.5348, -1.3046]],
       dtype=torch.float64)
	q_value: tensor([[-37.9812]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2875645025302984, distance: 1.298497717985147 entropy 1.448572039604187
epoch: 14, step: 53
	action: tensor([[-0.5374,  0.0790, -0.4451,  0.2615, -0.7602, -0.4745, -0.5450]],
       dtype=torch.float64)
	q_value: tensor([[-40.5420]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12544693472838198, distance: 1.2140014475987524 entropy 1.448572039604187
epoch: 14, step: 54
	action: tensor([[ 0.6910,  1.0384,  0.6610,  1.2742, -0.7361, -0.9433,  0.9104]],
       dtype=torch.float64)
	q_value: tensor([[-20.7281]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 14, step: 55
	action: tensor([[ 0.6958, -0.4685, -1.0438, -1.7243, -0.0690, -0.4806, -1.2282]],
       dtype=torch.float64)
	q_value: tensor([[-37.9812]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 14, step: 56
	action: tensor([[-2.1590, -1.0012, -0.3114, -1.0419, -0.4247,  1.5149,  0.6904]],
       dtype=torch.float64)
	q_value: tensor([[-37.9812]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 14, step: 57
	action: tensor([[-0.2051, -0.5663,  1.5716, -0.6926, -0.3708,  1.0285,  0.8778]],
       dtype=torch.float64)
	q_value: tensor([[-37.9812]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.29844156742881345, distance: 1.3039708964018941 entropy 1.448572039604187
epoch: 14, step: 58
	action: tensor([[-0.3648,  0.2937, -0.4699,  0.2006,  1.1251, -0.0283,  0.8500]],
       dtype=torch.float64)
	q_value: tensor([[-31.4415]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07693224991618774, distance: 1.099444933405063 entropy 1.448572039604187
epoch: 14, step: 59
	action: tensor([[ 0.1279, -0.4285, -1.4122,  2.1369,  0.1752, -0.1783, -0.1745]],
       dtype=torch.float64)
	q_value: tensor([[-24.5573]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 14, step: 60
	action: tensor([[-0.5714, -1.6250, -0.0177, -1.0376, -0.0778,  1.5402, -0.0450]],
       dtype=torch.float64)
	q_value: tensor([[-37.9812]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7708095042584375, distance: 1.522799113774654 entropy 1.448572039604187
epoch: 14, step: 61
	action: tensor([[ 0.0357, -0.8276, -1.3515,  1.2191,  0.1260, -1.3616,  0.6019]],
       dtype=torch.float64)
	q_value: tensor([[-32.6678]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.43241238719770636, distance: 1.3695904759670603 entropy 1.448572039604187
epoch: 14, step: 62
	action: tensor([[-1.3800,  0.3337, -0.7941,  1.2215,  1.6303, -0.4371, -0.1536]],
       dtype=torch.float64)
	q_value: tensor([[-30.9457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8280645764387589, distance: 1.5472213865446898 entropy 1.448572039604187
epoch: 14, step: 63
	action: tensor([[ 0.3970, -0.0318, -0.3920,  0.2341, -0.8741,  0.4132,  0.3977]],
       dtype=torch.float64)
	q_value: tensor([[-32.5418]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 14, step: 64
	action: tensor([[-2.2730, -0.9181,  0.2394,  0.3765,  1.5421, -0.3148,  0.0109]],
       dtype=torch.float64)
	q_value: tensor([[-37.9812]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 14, step: 65
	action: tensor([[-0.5996,  1.2549, -1.1430,  0.4602,  0.6231,  0.0066, -0.1610]],
       dtype=torch.float64)
	q_value: tensor([[-37.9812]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 14, step: 66
	action: tensor([[ 1.5954, -0.3931,  0.0491,  2.0608, -0.8352,  1.0446, -0.7646]],
       dtype=torch.float64)
	q_value: tensor([[-37.9812]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 14, step: 67
	action: tensor([[ 0.5143, -1.5358, -1.1853,  0.8369, -0.5500, -0.5972, -0.2956]],
       dtype=torch.float64)
	q_value: tensor([[-37.9812]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3501353986139708, distance: 1.329674546439258 entropy 1.448572039604187
epoch: 14, step: 68
	action: tensor([[ 2.2643, -0.2664, -0.2812,  0.2316, -0.8447, -0.6908,  0.8306]],
       dtype=torch.float64)
	q_value: tensor([[-26.0105]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 14, step: 69
	action: tensor([[ 0.0567, -0.7830, -0.1842,  0.3603, -1.8077, -0.5659, -0.0686]],
       dtype=torch.float64)
	q_value: tensor([[-37.9812]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09308534932537738, distance: 1.0897826819727396 entropy 1.448572039604187
epoch: 14, step: 70
	action: tensor([[0.5218, 0.9268, 1.5777, 1.0233, 1.5455, 0.1883, 0.0926]],
       dtype=torch.float64)
	q_value: tensor([[-29.5934]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.919116827347477, distance: 0.32545112892105127 entropy 1.448572039604187
epoch: 14, step: 71
	action: tensor([[-0.8630, -0.6454, -0.4568, -0.5011, -0.3867, -0.1732, -0.2154]],
       dtype=torch.float64)
	q_value: tensor([[-34.5171]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 14, step: 72
	action: tensor([[ 1.6010, -0.3655,  0.4611,  0.5757,  0.9410,  1.1742,  1.4077]],
       dtype=torch.float64)
	q_value: tensor([[-37.9812]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 14, step: 73
	action: tensor([[-0.5417,  0.4905, -1.1469,  0.4456, -0.0558, -0.0727, -0.1179]],
       dtype=torch.float64)
	q_value: tensor([[-37.9812]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18496182961782992, distance: 1.245686832393123 entropy 1.448572039604187
epoch: 14, step: 74
	action: tensor([[ 0.4688, -0.0714, -0.9403, -0.3927, -0.3891,  0.0203, -1.2418]],
       dtype=torch.float64)
	q_value: tensor([[-20.0057]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 14, step: 75
	action: tensor([[ 1.0910, -2.4325, -0.8012,  1.4848,  0.1847,  1.7854,  0.4617]],
       dtype=torch.float64)
	q_value: tensor([[-37.9812]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 14, step: 76
	action: tensor([[-1.1631,  1.2035, -0.7559, -0.7472, -0.7706,  0.4104, -0.3437]],
       dtype=torch.float64)
	q_value: tensor([[-37.9812]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0938094795645823, distance: 1.1968164082932495 entropy 1.448572039604187
epoch: 14, step: 77
	action: tensor([[-1.0826, -2.0597, -0.9223, -0.3822,  0.7363,  1.6645,  1.6495]],
       dtype=torch.float64)
	q_value: tensor([[-31.7776]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 14, step: 78
	action: tensor([[-0.2433, -0.3794, -1.2411, -0.2706, -1.3479,  0.4236,  0.1970]],
       dtype=torch.float64)
	q_value: tensor([[-37.9812]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2985714615814834, distance: 1.3040361184199922 entropy 1.448572039604187
epoch: 14, step: 79
	action: tensor([[ 0.7925, -0.5972,  0.7433,  0.1890,  0.6314,  0.2432, -1.8782]],
       dtype=torch.float64)
	q_value: tensor([[-27.5101]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2644806040797668, distance: 0.9814174104205713 entropy 1.448572039604187
epoch: 14, step: 80
	action: tensor([[-1.3763,  1.0826,  0.7340, -0.8100, -0.5970,  1.1882,  0.5780]],
       dtype=torch.float64)
	q_value: tensor([[-30.8311]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.39106170932380935, distance: 1.349677137766451 entropy 1.448572039604187
epoch: 14, step: 81
	action: tensor([[ 0.5894,  1.0714, -1.7229, -0.2091, -1.0386,  0.2368,  1.6947]],
       dtype=torch.float64)
	q_value: tensor([[-35.6778]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 14, step: 82
	action: tensor([[-1.1023, -0.3855, -0.6591, -0.0667, -0.8056,  0.1741, -0.9605]],
       dtype=torch.float64)
	q_value: tensor([[-37.9812]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.3847077199781097, distance: 1.7671534873290349 entropy 1.448572039604187
epoch: 14, step: 83
	action: tensor([[ 0.9698, -0.3047, -0.8585, -1.0499, -0.0901,  0.9932,  0.5061]],
       dtype=torch.float64)
	q_value: tensor([[-26.0021]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23623296405750338, distance: 1.0000855411309315 entropy 1.448572039604187
epoch: 14, step: 84
	action: tensor([[ 0.8596, -1.5146,  0.7579,  0.0190,  0.3815,  0.9942, -0.3516]],
       dtype=torch.float64)
	q_value: tensor([[-26.9963]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 14, step: 85
	action: tensor([[ 1.2720, -0.2214, -0.9832, -0.3555,  1.4080,  0.2626, -0.4547]],
       dtype=torch.float64)
	q_value: tensor([[-37.9812]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28404120218916995, distance: 0.9682794355269629 entropy 1.448572039604187
epoch: 14, step: 86
	action: tensor([[ 1.0637, -1.7675, -0.9082, -1.2604,  0.1093,  1.7668, -0.6920]],
       dtype=torch.float64)
	q_value: tensor([[-27.3737]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 14, step: 87
	action: tensor([[-1.8533,  0.2066,  0.0224,  0.3488,  1.5134, -0.8516,  0.4975]],
       dtype=torch.float64)
	q_value: tensor([[-37.9812]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 14, step: 88
	action: tensor([[-1.1898,  0.5931, -0.9897, -1.0495,  0.6315, -0.4926,  0.9895]],
       dtype=torch.float64)
	q_value: tensor([[-37.9812]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.27599525002772674, distance: 1.292650808393414 entropy 1.448572039604187
epoch: 14, step: 89
	action: tensor([[ 0.5576,  0.0201, -1.5651,  0.0349,  0.9343,  0.5550,  0.0670]],
       dtype=torch.float64)
	q_value: tensor([[-35.2301]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7488197452169792, distance: 0.5735211544578704 entropy 1.448572039604187
epoch: 14, step: 90
	action: tensor([[ 1.5457,  0.9527,  1.1219,  0.9157, -0.3915,  0.5212, -0.6569]],
       dtype=torch.float64)
	q_value: tensor([[-25.0315]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 14, step: 91
	action: tensor([[ 0.7153,  1.0708,  1.7309, -0.6927, -1.0756,  0.4601, -0.7193]],
       dtype=torch.float64)
	q_value: tensor([[-37.9812]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 14, step: 92
	action: tensor([[-0.1059, -0.1672,  0.6294, -1.8528,  0.0858,  0.7866, -0.3445]],
       dtype=torch.float64)
	q_value: tensor([[-37.9812]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5454288260295956, distance: 1.4225947367745722 entropy 1.448572039604187
epoch: 14, step: 93
	action: tensor([[ 0.0826, -1.2923, -0.9173,  1.0898, -0.0764, -1.9001,  1.1959]],
       dtype=torch.float64)
	q_value: tensor([[-29.6064]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.32199245283879785, distance: 1.315743342354899 entropy 1.448572039604187
epoch: 14, step: 94
	action: tensor([[ 0.2788,  0.7289, -0.3638, -1.3500,  0.1544, -0.2458, -1.9738]],
       dtype=torch.float64)
	q_value: tensor([[-36.8293]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6137320298007423, distance: 0.7112152200339025 entropy 1.448572039604187
epoch: 14, step: 95
	action: tensor([[-0.3271,  0.1038,  0.3562,  0.1059,  0.1432, -0.6820,  0.0221]],
       dtype=torch.float64)
	q_value: tensor([[-29.5200]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12882878047514823, distance: 1.2158240506624935 entropy 1.448572039604187
epoch: 14, step: 96
	action: tensor([[ 2.0529, -0.5159, -1.3835, -0.2891, -0.8834,  1.4665,  1.6046]],
       dtype=torch.float64)
	q_value: tensor([[-18.2810]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.009627138112932254, distance: 1.149839440041427 entropy 1.448572039604187
epoch: 14, step: 97
	action: tensor([[ 0.3242, -1.0748, -0.2624, -0.4958, -0.2768, -1.3299, -0.4700]],
       dtype=torch.float64)
	q_value: tensor([[-38.2431]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4616359110324433, distance: 1.3834908640872994 entropy 1.448572039604187
epoch: 14, step: 98
	action: tensor([[ 0.0604,  0.0373, -0.3155, -0.8303, -1.2209, -0.2191,  0.9378]],
       dtype=torch.float64)
	q_value: tensor([[-25.4204]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19463789452268054, distance: 1.0269570691703556 entropy 1.448572039604187
epoch: 14, step: 99
	action: tensor([[-0.5777, -0.3273,  0.9071, -0.6383, -0.6005, -0.8996, -0.5811]],
       dtype=torch.float64)
	q_value: tensor([[-27.2848]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9554552527931897, distance: 1.600223465005936 entropy 1.448572039604187
epoch: 14, step: 100
	action: tensor([[-0.7347, -0.5623, -1.0116,  1.9299, -0.5589, -0.9280, -0.5280]],
       dtype=torch.float64)
	q_value: tensor([[-25.4759]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5421515233909247, distance: 1.4210855285261481 entropy 1.448572039604187
epoch: 14, step: 101
	action: tensor([[ 0.2489,  2.6882,  0.8721,  0.3548, -0.4859,  0.6722, -1.5147]],
       dtype=torch.float64)
	q_value: tensor([[-32.3837]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 14, step: 102
	action: tensor([[-1.1888, -1.8448,  0.4253,  0.5384, -1.4720,  0.9244,  0.2148]],
       dtype=torch.float64)
	q_value: tensor([[-37.9812]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 14, step: 103
	action: tensor([[ 1.3811,  0.2532,  0.1433,  0.8209,  1.0395, -0.0526, -0.1787]],
       dtype=torch.float64)
	q_value: tensor([[-37.9812]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 14, step: 104
	action: tensor([[-0.0443,  0.8863,  0.8237, -0.9202,  0.1959,  0.3213,  0.2984]],
       dtype=torch.float64)
	q_value: tensor([[-37.9812]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 14, step: 105
	action: tensor([[ 1.4825, -1.6765, -0.2586,  0.0476, -0.1286, -0.7353, -0.5500]],
       dtype=torch.float64)
	q_value: tensor([[-37.9812]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8700591996870619, distance: 1.5648920015154237 entropy 1.448572039604187
epoch: 14, step: 106
	action: tensor([[-0.2462,  0.9132, -1.5205,  1.0524, -1.2155, -1.8474, -0.3292]],
       dtype=torch.float64)
	q_value: tensor([[-25.0972]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 14, step: 107
	action: tensor([[ 0.1321, -0.3106,  1.1156, -0.5352,  1.1285,  1.3096,  0.9479]],
       dtype=torch.float64)
	q_value: tensor([[-37.9812]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07050058058187214, distance: 1.1032685917195435 entropy 1.448572039604187
epoch: 14, step: 108
	action: tensor([[-0.0309, -0.3642, -1.1799, -1.0067, -1.1229, -1.8624, -0.2149]],
       dtype=torch.float64)
	q_value: tensor([[-30.7903]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7483865700502073, distance: 0.5740154769593502 entropy 1.448572039604187
epoch: 14, step: 109
	action: tensor([[-0.5874,  0.0891,  1.8340,  0.3144,  1.1963, -0.8473, -0.6773]],
       dtype=torch.float64)
	q_value: tensor([[-33.3147]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.019539412464525285, distance: 1.1331091944540013 entropy 1.448572039604187
epoch: 14, step: 110
	action: tensor([[-1.1332,  0.9793,  1.7854,  1.7776,  0.9373,  0.2505, -0.0846]],
       dtype=torch.float64)
	q_value: tensor([[-33.9197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 14, step: 111
	action: tensor([[ 0.8071,  2.3255, -0.8711,  0.1545, -1.2918, -0.8887, -0.3883]],
       dtype=torch.float64)
	q_value: tensor([[-37.9812]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 14, step: 112
	action: tensor([[ 1.6431, -0.3556, -0.6249,  1.7954,  2.3401, -1.1183, -0.6621]],
       dtype=torch.float64)
	q_value: tensor([[-37.9812]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 14, step: 113
	action: tensor([[ 0.1620,  0.1153, -2.4399,  0.4366, -0.6189,  0.0761,  2.1720]],
       dtype=torch.float64)
	q_value: tensor([[-37.9812]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 14, step: 114
	action: tensor([[-0.4381,  1.4184, -1.1280,  0.0583, -0.5725,  1.2042,  1.3921]],
       dtype=torch.float64)
	q_value: tensor([[-37.9812]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 14, step: 115
	action: tensor([[-0.6034, -3.6282, -0.3549, -0.6093, -0.5686, -0.2484, -1.2181]],
       dtype=torch.float64)
	q_value: tensor([[-37.9812]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 14, step: 116
	action: tensor([[-0.7525, -0.9833, -2.1061, -0.7743, -0.1072, -0.7050, -1.1522]],
       dtype=torch.float64)
	q_value: tensor([[-37.9812]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0867281928304231, distance: 1.1929360426475564 entropy 1.448572039604187
epoch: 14, step: 117
	action: tensor([[-1.3331,  0.0405,  1.4560, -0.0407,  0.0961, -0.2765,  0.2390]],
       dtype=torch.float64)
	q_value: tensor([[-36.1001]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.4144197752682328, distance: 1.7781282547911288 entropy 1.448572039604187
epoch: 14, step: 118
	action: tensor([[-1.6105, -0.9102, -0.1939, -0.4012, -0.1871,  0.1778, -0.3389]],
       dtype=torch.float64)
	q_value: tensor([[-27.7155]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.6557930177652178, distance: 1.864892514162594 entropy 1.448572039604187
epoch: 14, step: 119
	action: tensor([[-0.4561, -1.4721,  1.5225,  0.3423,  1.0366,  0.2176, -2.4829]],
       dtype=torch.float64)
	q_value: tensor([[-28.5725]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 14, step: 120
	action: tensor([[-0.8618, -0.1495,  0.3042, -0.3402, -0.3895,  0.2371, -0.3567]],
       dtype=torch.float64)
	q_value: tensor([[-37.9812]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0779629714880925, distance: 1.6495884055723598 entropy 1.448572039604187
epoch: 14, step: 121
	action: tensor([[-1.0467, -0.5824, -0.4204,  0.0037, -1.9794, -1.3453,  1.2822]],
       dtype=torch.float64)
	q_value: tensor([[-21.4925]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.690731349746446, distance: 1.4879693861059542 entropy 1.448572039604187
epoch: 14, step: 122
	action: tensor([[ 1.9359,  0.7591,  0.9368, -0.1794,  1.0841,  1.5886,  1.0400]],
       dtype=torch.float64)
	q_value: tensor([[-39.9943]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4979309050410976, distance: 0.8108461104709811 entropy 1.448572039604187
epoch: 14, step: 123
	action: tensor([[-2.3429, -1.0645, -0.9118,  0.9253,  0.4250, -0.4024,  2.7816]],
       dtype=torch.float64)
	q_value: tensor([[-35.6702]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.836326434823805, distance: 1.9272355565932822 entropy 1.448572039604187
epoch: 14, step: 124
	action: tensor([[ 1.1738,  0.6221, -1.6300, -1.4702,  0.1515,  0.4203, -0.0695]],
       dtype=torch.float64)
	q_value: tensor([[-44.3819]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4575585475459941, distance: 0.8428166922432339 entropy 1.448572039604187
epoch: 14, step: 125
	action: tensor([[-0.6407, -0.3346, -0.7326,  0.7532, -0.4322, -0.3108,  1.0661]],
       dtype=torch.float64)
	q_value: tensor([[-30.4439]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7449900741284881, distance: 1.5116567015651785 entropy 1.448572039604187
epoch: 14, step: 126
	action: tensor([[ 2.8646,  0.1419, -1.0678, -0.3369, -1.4822,  1.8471, -0.4803]],
       dtype=torch.float64)
	q_value: tensor([[-25.4115]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 14, step: 127
	action: tensor([[ 0.4569, -0.4676,  1.4343, -0.4643,  0.7425,  0.2301,  1.3223]],
       dtype=torch.float64)
	q_value: tensor([[-37.9812]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.02483011851715633, distance: 1.1300478490804693 entropy 1.448572039604187
LOSS epoch 14 actor 325.29028226223363 critic 248.3849265734846 
epoch: 15, step: 0
	action: tensor([[ 0.6228,  0.3482, -2.5036, -1.6236,  0.0469, -0.3524,  0.7419]],
       dtype=torch.float64)
	q_value: tensor([[-27.9021]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 15, step: 1
	action: tensor([[ 0.0382, -0.7182, -1.1526, -0.1540,  0.0804, -0.9951, -1.1731]],
       dtype=torch.float64)
	q_value: tensor([[-34.4121]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05183306318027592, distance: 1.1736270276606144 entropy 1.3432114124298096
epoch: 15, step: 2
	action: tensor([[-0.5619, -0.7344, -1.0318, -0.7542, -1.1060, -0.8358, -1.2536]],
       dtype=torch.float64)
	q_value: tensor([[-26.0287]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06954336863575772, distance: 1.1038365265168486 entropy 1.3432114124298096
epoch: 15, step: 3
	action: tensor([[-0.6959,  0.6612,  0.8058, -0.4831,  1.0978, -0.0837,  0.3381]],
       dtype=torch.float64)
	q_value: tensor([[-29.8230]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5131553031912872, distance: 1.407662189499543 entropy 1.3432114124298096
epoch: 15, step: 4
	action: tensor([[-1.4492,  0.6716,  0.0260, -1.8505, -1.3241,  0.1507, -0.9262]],
       dtype=torch.float64)
	q_value: tensor([[-25.3240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9827513843623852, distance: 1.6113534904296765 entropy 1.3432114124298096
epoch: 15, step: 5
	action: tensor([[-0.4325, -0.5567, -0.8736, -0.5648,  0.0463, -1.1948, -0.9831]],
       dtype=torch.float64)
	q_value: tensor([[-35.1586]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03985445604018367, distance: 1.1669250743960076 entropy 1.3432114124298096
epoch: 15, step: 6
	action: tensor([[-0.1322, -0.9657,  0.5213,  1.2277,  1.5103,  0.1101, -1.5427]],
       dtype=torch.float64)
	q_value: tensor([[-26.7428]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4230203782146743, distance: 0.869234450820321 entropy 1.3432114124298096
epoch: 15, step: 7
	action: tensor([[-0.5459, -0.7512, -1.4098,  0.2445,  0.8910,  0.3655, -0.8458]],
       dtype=torch.float64)
	q_value: tensor([[-35.2312]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9643947020866115, distance: 1.6038770399600195 entropy 1.3432114124298096
epoch: 15, step: 8
	action: tensor([[ 0.8029,  0.4930,  1.2672, -0.7261, -0.4807, -0.2486, -0.2802]],
       dtype=torch.float64)
	q_value: tensor([[-25.5964]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.770874085726535, distance: 0.5477643941012221 entropy 1.3432114124298096
epoch: 15, step: 9
	action: tensor([[-0.8699, -1.2535, -1.1877, -0.4386,  1.0686, -0.5951, -0.2050]],
       dtype=torch.float64)
	q_value: tensor([[-23.1745]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7725088299406513, distance: 1.5235296020391358 entropy 1.3432114124298096
epoch: 15, step: 10
	action: tensor([[ 0.6625, -1.2943, -0.4883, -2.8539, -1.7628,  0.2982, -0.0582]],
       dtype=torch.float64)
	q_value: tensor([[-30.2975]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 15, step: 11
	action: tensor([[-0.9042, -0.8240,  0.1214, -0.3575, -1.1017, -0.2921,  1.3230]],
       dtype=torch.float64)
	q_value: tensor([[-34.4121]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1356993222604794, distance: 1.67234835833455 entropy 1.3432114124298096
epoch: 15, step: 12
	action: tensor([[-0.9609,  0.1266,  1.2427,  1.0540, -0.5436,  0.5515,  0.9851]],
       dtype=torch.float64)
	q_value: tensor([[-28.6535]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 15, step: 13
	action: tensor([[-0.1561,  0.3586, -2.3362,  0.3141,  0.6341, -0.0813, -0.7501]],
       dtype=torch.float64)
	q_value: tensor([[-34.4121]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 15, step: 14
	action: tensor([[ 0.3277, -0.7177,  0.4247, -0.4990, -0.8291, -0.1656, -0.3509]],
       dtype=torch.float64)
	q_value: tensor([[-34.4121]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5080397163815444, distance: 1.4052807040635278 entropy 1.3432114124298096
epoch: 15, step: 15
	action: tensor([[ 2.0026,  1.6451,  1.3050, -0.3551, -1.7355,  0.4718,  0.6597]],
       dtype=torch.float64)
	q_value: tensor([[-20.6434]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 15, step: 16
	action: tensor([[ 0.3636, -1.1216, -1.1619,  0.3187, -0.9909, -0.5757, -0.5169]],
       dtype=torch.float64)
	q_value: tensor([[-34.4121]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.34998482970599887, distance: 1.3296004008288296 entropy 1.3432114124298096
epoch: 15, step: 17
	action: tensor([[ 0.8739, -0.7071, -0.2395,  0.4689,  1.0843, -0.5099, -0.4580]],
       dtype=torch.float64)
	q_value: tensor([[-25.0727]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25046195988930164, distance: 0.9907259365185294 entropy 1.3432114124298096
epoch: 15, step: 18
	action: tensor([[ 0.4982, -0.4251, -0.8299,  1.0413,  2.0047,  0.2896, -0.0248]],
       dtype=torch.float64)
	q_value: tensor([[-24.4393]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5645696227455896, distance: 0.7551201427049595 entropy 1.3432114124298096
epoch: 15, step: 19
	action: tensor([[ 1.1632,  0.8159, -1.3301, -0.1665,  0.1152,  1.9951,  0.1303]],
       dtype=torch.float64)
	q_value: tensor([[-30.4420]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.917100828957793, distance: 0.3294820710310531 entropy 1.3432114124298096
epoch: 15, step: 20
	action: tensor([[-1.1250, -0.3622, -0.0605, -0.4153,  1.5336,  0.3893, -0.0508]],
       dtype=torch.float64)
	q_value: tensor([[-28.7290]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 15, step: 21
	action: tensor([[ 1.5314, -0.7203, -0.3892, -0.8563, -1.7519,  1.0580,  0.2667]],
       dtype=torch.float64)
	q_value: tensor([[-34.4121]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4209874071000934, distance: 1.3641175861453734 entropy 1.3432114124298096
epoch: 15, step: 22
	action: tensor([[-0.6868, -0.8154,  1.0366, -0.1445, -0.7984,  1.3197,  1.2263]],
       dtype=torch.float64)
	q_value: tensor([[-31.5512]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6007806057199452, distance: 1.4478467655132121 entropy 1.3432114124298096
epoch: 15, step: 23
	action: tensor([[-0.1987, -0.5567,  0.5285, -1.3507, -1.0209, -0.8419,  0.1321]],
       dtype=torch.float64)
	q_value: tensor([[-32.3140]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5890195356176284, distance: 1.4425182340365386 entropy 1.3432114124298096
epoch: 15, step: 24
	action: tensor([[ 0.3151,  1.0129, -0.6009,  0.9081,  0.5037, -1.0893, -0.2859]],
       dtype=torch.float64)
	q_value: tensor([[-26.2137]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 15, step: 25
	action: tensor([[ 0.7631, -1.3316,  0.0963, -1.0101,  1.0456, -0.0608,  0.5204]],
       dtype=torch.float64)
	q_value: tensor([[-34.4121]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0444808291271026, distance: 1.6362445558200782 entropy 1.3432114124298096
epoch: 15, step: 26
	action: tensor([[-0.0650, -0.2426, -0.5373,  0.2337,  0.6179, -1.0016, -0.5309]],
       dtype=torch.float64)
	q_value: tensor([[-26.8314]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08685449600134265, distance: 1.1930053641339453 entropy 1.3432114124298096
epoch: 15, step: 27
	action: tensor([[ 0.3930, -1.8290, -0.0240,  0.0259,  0.7203,  2.9369, -0.3818]],
       dtype=torch.float64)
	q_value: tensor([[-22.0584]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 15, step: 28
	action: tensor([[-0.5277,  0.7405,  2.0179,  0.3333,  0.0929,  0.2610, -0.9134]],
       dtype=torch.float64)
	q_value: tensor([[-34.4121]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 15, step: 29
	action: tensor([[-0.7549, -0.1707,  0.5184, -0.7891, -1.9091, -0.5309, -0.0168]],
       dtype=torch.float64)
	q_value: tensor([[-34.4121]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9548893255219013, distance: 1.5999918883359439 entropy 1.3432114124298096
epoch: 15, step: 30
	action: tensor([[ 0.6256, -1.1472, -0.6095, -0.3776, -0.8297, -0.1566, -0.7177]],
       dtype=torch.float64)
	q_value: tensor([[-31.5885]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6972992903695561, distance: 1.4908567229500502 entropy 1.3432114124298096
epoch: 15, step: 31
	action: tensor([[ 0.1230, -0.1651,  0.3424, -1.0305,  1.4297, -0.2190,  0.1849]],
       dtype=torch.float64)
	q_value: tensor([[-23.0494]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.37822207515646666, distance: 1.3434338725995596 entropy 1.3432114124298096
epoch: 15, step: 32
	action: tensor([[-0.5027,  0.6147,  1.3044, -0.4572,  0.5196, -0.2155, -0.0895]],
       dtype=torch.float64)
	q_value: tensor([[-25.9721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.31884214505374575, distance: 1.3141746992004755 entropy 1.3432114124298096
epoch: 15, step: 33
	action: tensor([[ 0.6443, -0.7310,  1.5394,  1.2884,  0.2560,  0.2383,  0.4608]],
       dtype=torch.float64)
	q_value: tensor([[-24.0529]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 15, step: 34
	action: tensor([[ 0.3289,  0.4300, -0.5186, -1.2532,  0.5782, -0.2509, -0.0542]],
       dtype=torch.float64)
	q_value: tensor([[-34.4121]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 15, step: 35
	action: tensor([[ 0.6578, -1.1570,  0.4762,  1.2962,  0.3738,  2.6025,  0.2228]],
       dtype=torch.float64)
	q_value: tensor([[-34.4121]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5554353023507321, distance: 0.7629993706509913 entropy 1.3432114124298096
epoch: 15, step: 36
	action: tensor([[-0.5642, -0.1481,  0.1878, -1.3469,  0.5098, -0.9168,  1.6394]],
       dtype=torch.float64)
	q_value: tensor([[-33.2874]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6570000477920608, distance: 1.47305157826538 entropy 1.3432114124298096
epoch: 15, step: 37
	action: tensor([[-0.0283,  1.3258, -0.9708, -1.0068,  0.3741, -1.4360,  0.6983]],
       dtype=torch.float64)
	q_value: tensor([[-29.8554]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 15, step: 38
	action: tensor([[ 0.5955, -0.9435,  1.4706, -0.0661,  0.7189,  1.6682, -0.9884]],
       dtype=torch.float64)
	q_value: tensor([[-34.4121]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15618119151122944, distance: 1.2304660721179574 entropy 1.3432114124298096
epoch: 15, step: 39
	action: tensor([[ 0.0331, -1.8194,  0.7415, -0.8553,  0.8765, -1.3892,  0.1388]],
       dtype=torch.float64)
	q_value: tensor([[-33.3401]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 15, step: 40
	action: tensor([[ 0.4003, -0.3391,  0.9480, -1.4180, -1.6079,  0.0330, -0.3772]],
       dtype=torch.float64)
	q_value: tensor([[-34.4121]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.36794597731554757, distance: 1.338416144420424 entropy 1.3432114124298096
epoch: 15, step: 41
	action: tensor([[ 0.1609, -1.1328,  1.3442,  0.4532,  0.9030,  0.6128,  1.0004]],
       dtype=torch.float64)
	q_value: tensor([[-29.9667]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.25688359208299216, distance: 1.2829337247284556 entropy 1.3432114124298096
epoch: 15, step: 42
	action: tensor([[ 0.6785, -0.4754, -1.2613,  0.8086,  0.1769,  0.6057,  0.0037]],
       dtype=torch.float64)
	q_value: tensor([[-29.7075]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4359729485685372, distance: 0.8594223825979788 entropy 1.3432114124298096
epoch: 15, step: 43
	action: tensor([[ 1.7401, -0.2425, -1.1241,  0.0563,  1.0552,  1.4544,  0.7692]],
       dtype=torch.float64)
	q_value: tensor([[-19.4483]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6441144227545735, distance: 0.682671680527931 entropy 1.3432114124298096
epoch: 15, step: 44
	action: tensor([[ 0.8662,  0.0255, -1.4794,  0.3249, -0.2902,  1.8877,  0.0484]],
       dtype=torch.float64)
	q_value: tensor([[-30.4879]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6931495036141935, distance: 0.6338990608635958 entropy 1.3432114124298096
epoch: 15, step: 45
	action: tensor([[ 0.6298, -0.8285,  0.0762,  0.4720, -0.0326,  0.1503,  0.2191]],
       dtype=torch.float64)
	q_value: tensor([[-26.7144]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3037403415014318, distance: 0.9548657361397516 entropy 1.3432114124298096
epoch: 15, step: 46
	action: tensor([[ 1.1157,  1.0773, -0.6709, -1.7130,  1.0284, -0.1016, -1.3635]],
       dtype=torch.float64)
	q_value: tensor([[-19.0151]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5155551262593571, distance: 0.796487337139495 entropy 1.3432114124298096
epoch: 15, step: 47
	action: tensor([[ 0.5234, -0.8510,  1.9144,  1.0506,  1.3802, -0.7391, -0.6252]],
       dtype=torch.float64)
	q_value: tensor([[-30.7940]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18971728613327932, distance: 1.0300895473578648 entropy 1.3432114124298096
epoch: 15, step: 48
	action: tensor([[-0.3936,  0.7489, -0.4855, -0.2195, -0.4115, -0.1992,  0.3188]],
       dtype=torch.float64)
	q_value: tensor([[-38.7377]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 15, step: 49
	action: tensor([[-0.2614,  0.9122, -0.4216,  0.2923, -1.1377, -0.1677, -0.5354]],
       dtype=torch.float64)
	q_value: tensor([[-34.4121]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 15, step: 50
	action: tensor([[ 0.6848,  0.2749,  0.6203, -1.1901,  0.7211, -0.8567, -0.9298]],
       dtype=torch.float64)
	q_value: tensor([[-34.4121]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28608953530026393, distance: 0.9668933366008644 entropy 1.3432114124298096
epoch: 15, step: 51
	action: tensor([[-1.3355, -0.0038, -0.6085, -2.3568, -0.4834, -1.4807,  0.9990]],
       dtype=torch.float64)
	q_value: tensor([[-25.3007]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4270700075680054, distance: 0.8661786447311516 entropy 1.3432114124298096
epoch: 15, step: 52
	action: tensor([[ 0.0153, -0.7453, -0.8495,  1.3749,  0.9070,  0.8028, -0.3505]],
       dtype=torch.float64)
	q_value: tensor([[-38.8675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2823870336590396, distance: 0.9693973582989363 entropy 1.3432114124298096
epoch: 15, step: 53
	action: tensor([[ 0.0556,  0.9849,  1.2249,  0.1567, -0.4007,  1.4932, -0.1462]],
       dtype=torch.float64)
	q_value: tensor([[-23.3121]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 15, step: 54
	action: tensor([[ 1.0998, -0.5633, -2.5373,  0.0867, -0.1574,  0.7509,  0.4527]],
       dtype=torch.float64)
	q_value: tensor([[-34.4121]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 15, step: 55
	action: tensor([[ 1.0300, -1.1381, -0.0132, -0.8096, -0.5659,  0.1869, -0.7570]],
       dtype=torch.float64)
	q_value: tensor([[-34.4121]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.92380075164274, distance: 1.587218578589276 entropy 1.3432114124298096
epoch: 15, step: 56
	action: tensor([[ 0.8816, -1.5527,  0.3665, -1.1131, -0.0066, -0.0990,  0.5697]],
       dtype=torch.float64)
	q_value: tensor([[-23.8653]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 15, step: 57
	action: tensor([[ 0.0521, -0.7223,  0.9219,  1.2190, -0.1479, -0.2426,  1.2772]],
       dtype=torch.float64)
	q_value: tensor([[-34.4121]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6217530210494164, distance: 0.7037921642753876 entropy 1.3432114124298096
epoch: 15, step: 58
	action: tensor([[ 0.6011, -0.4404,  0.1435, -1.2580,  0.0663,  0.9728,  1.1537]],
       dtype=torch.float64)
	q_value: tensor([[-28.9708]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07612172006851403, distance: 1.1871002373988466 entropy 1.3432114124298096
epoch: 15, step: 59
	action: tensor([[-0.0884, -1.8656, -0.2991, -0.4531, -0.4201,  1.4749,  0.0457]],
       dtype=torch.float64)
	q_value: tensor([[-28.3424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 15, step: 60
	action: tensor([[-1.9577,  0.9828, -0.8994,  2.0064, -0.8612, -0.1291,  0.8382]],
       dtype=torch.float64)
	q_value: tensor([[-34.4121]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 15, step: 61
	action: tensor([[-0.3002, -2.0105, -0.5996, -0.2770,  0.6505,  0.2311,  0.6044]],
       dtype=torch.float64)
	q_value: tensor([[-34.4121]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 15, step: 62
	action: tensor([[-1.2808,  0.5261, -0.7595,  0.7952, -0.4331, -0.5950,  0.4821]],
       dtype=torch.float64)
	q_value: tensor([[-34.4121]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5421141741046145, distance: 1.4210683198242664 entropy 1.3432114124298096
epoch: 15, step: 63
	action: tensor([[-0.1306,  0.0983,  0.3201,  0.2426, -1.2170, -0.7001, -0.4706]],
       dtype=torch.float64)
	q_value: tensor([[-25.3592]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 15, step: 64
	action: tensor([[-0.4181, -0.8113,  0.2985,  0.8544, -0.9154,  1.2089,  0.7318]],
       dtype=torch.float64)
	q_value: tensor([[-34.4121]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0335690073051389, distance: 1.1633929622960673 entropy 1.3432114124298096
epoch: 15, step: 65
	action: tensor([[-0.3874, -0.2171, -1.4683, -0.8853, -0.8938,  1.2038, -0.5328]],
       dtype=torch.float64)
	q_value: tensor([[-27.3872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09941018835437299, distance: 1.1998765671761182 entropy 1.3432114124298096
epoch: 15, step: 66
	action: tensor([[-0.1327, -0.5020,  2.4942, -1.0589, -1.9124, -0.0788, -1.2790]],
       dtype=torch.float64)
	q_value: tensor([[-28.6574]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.060897680087655726, distance: 1.1786732928919357 entropy 1.3432114124298096
epoch: 15, step: 67
	action: tensor([[-1.1028,  0.2942,  0.8944,  0.8119, -0.8333, -1.2469, -0.0076]],
       dtype=torch.float64)
	q_value: tensor([[-42.8035]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.30626181221706594, distance: 1.307891775000362 entropy 1.3432114124298096
epoch: 15, step: 68
	action: tensor([[ 0.1060, -0.7914,  0.6285, -0.2038,  2.0412,  1.3056, -0.0835]],
       dtype=torch.float64)
	q_value: tensor([[-30.9215]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3428569871461984, distance: 1.3260856482940517 entropy 1.3432114124298096
epoch: 15, step: 69
	action: tensor([[ 1.6015, -1.2526,  0.3194, -0.2723, -1.5724, -0.1270,  0.6988]],
       dtype=torch.float64)
	q_value: tensor([[-32.3348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9064052203285777, distance: 1.5800262511877332 entropy 1.3432114124298096
epoch: 15, step: 70
	action: tensor([[-0.0256,  0.7262, -0.4860,  0.6003, -1.3993,  0.4002,  0.7289]],
       dtype=torch.float64)
	q_value: tensor([[-32.1422]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 15, step: 71
	action: tensor([[-0.2394,  0.0378,  0.3840,  0.5551, -0.2293,  0.8082,  1.6106]],
       dtype=torch.float64)
	q_value: tensor([[-34.4121]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 15, step: 72
	action: tensor([[ 0.2904, -0.0574,  0.1787, -0.9247, -1.7484, -1.0144,  0.1034]],
       dtype=torch.float64)
	q_value: tensor([[-34.4121]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30231618693448437, distance: 0.9558417956511681 entropy 1.3432114124298096
epoch: 15, step: 73
	action: tensor([[-0.8028,  1.0776,  1.1324, -0.7311, -2.0539,  0.3747, -0.8727]],
       dtype=torch.float64)
	q_value: tensor([[-28.2664]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5230874566228025, distance: 1.4122744884649792 entropy 1.3432114124298096
epoch: 15, step: 74
	action: tensor([[ 0.2959, -0.4424,  0.1479,  0.0512,  1.8230, -0.9708, -0.1949]],
       dtype=torch.float64)
	q_value: tensor([[-34.8362]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24077143076683516, distance: 0.9971097530043409 entropy 1.3432114124298096
epoch: 15, step: 75
	action: tensor([[ 1.0150, -0.8144, -1.0688,  0.0370,  0.2476,  1.3903, -0.5940]],
       dtype=torch.float64)
	q_value: tensor([[-30.3098]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4041586396555349, distance: 0.8833280395815517 entropy 1.3432114124298096
epoch: 15, step: 76
	action: tensor([[ 1.4476,  0.6363, -0.0835, -0.4505, -0.1884,  0.6574, -0.9619]],
       dtype=torch.float64)
	q_value: tensor([[-24.6661]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.804274618983114, distance: 0.5062677544139803 entropy 1.3432114124298096
epoch: 15, step: 77
	action: tensor([[-0.4511,  1.6757, -0.2675, -0.6481, -0.0333, -0.4557,  0.9872]],
       dtype=torch.float64)
	q_value: tensor([[-20.5742]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 15, step: 78
	action: tensor([[-0.8146, -0.4114,  1.1860,  0.7305,  0.2695,  1.0531,  0.2511]],
       dtype=torch.float64)
	q_value: tensor([[-34.4121]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27544270413392824, distance: 0.9740764871755963 entropy 1.3432114124298096
epoch: 15, step: 79
	action: tensor([[ 0.3959, -0.8589,  0.5337,  0.5396, -0.1590, -0.4349,  0.7931]],
       dtype=torch.float64)
	q_value: tensor([[-26.2171]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04639202501436712, distance: 1.1174848151500434 entropy 1.3432114124298096
epoch: 15, step: 80
	action: tensor([[-0.2234,  0.7589, -0.6735, -1.2707, -0.7368, -0.3947, -1.7430]],
       dtype=torch.float64)
	q_value: tensor([[-23.1666]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6795433025469981, distance: 0.6478006353063148 entropy 1.3432114124298096
epoch: 15, step: 81
	action: tensor([[ 0.1129, -1.3587, -0.2986,  0.0542, -1.0142,  0.7661, -0.9938]],
       dtype=torch.float64)
	q_value: tensor([[-28.3121]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7517827847688747, distance: 1.5145960515743229 entropy 1.3432114124298096
epoch: 15, step: 82
	action: tensor([[-0.6471,  0.2713, -0.6388, -1.2633,  0.2070,  0.7706, -0.4398]],
       dtype=torch.float64)
	q_value: tensor([[-26.0617]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04746859909709511, distance: 1.171189579268871 entropy 1.3432114124298096
epoch: 15, step: 83
	action: tensor([[ 0.1663, -0.2117,  1.3719, -2.1418, -0.3603, -0.9705,  0.1296]],
       dtype=torch.float64)
	q_value: tensor([[-27.2912]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10926474437168343, distance: 1.205242113362372 entropy 1.3432114124298096
epoch: 15, step: 84
	action: tensor([[ 0.0025,  0.6270, -0.0069,  0.6762,  0.6898,  0.7033,  1.2884]],
       dtype=torch.float64)
	q_value: tensor([[-29.6839]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 15, step: 85
	action: tensor([[ 0.8111, -0.7311,  0.2980,  0.6632,  0.6815,  0.4062,  0.1974]],
       dtype=torch.float64)
	q_value: tensor([[-34.4121]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5949013021078708, distance: 0.7283449558824423 entropy 1.3432114124298096
epoch: 15, step: 86
	action: tensor([[ 1.8413,  0.6265, -1.4480, -1.2922, -1.8846, -0.1684, -0.5280]],
       dtype=torch.float64)
	q_value: tensor([[-22.1874]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 15, step: 87
	action: tensor([[ 0.8664,  0.6656, -0.1755, -1.6200,  0.5215, -0.3315,  0.6435]],
       dtype=torch.float64)
	q_value: tensor([[-34.4121]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 15, step: 88
	action: tensor([[ 0.5197, -1.2923,  0.8975, -1.4938, -0.1730,  1.6955, -0.5204]],
       dtype=torch.float64)
	q_value: tensor([[-34.4121]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.45677580287326847, distance: 1.3811888154216028 entropy 1.3432114124298096
epoch: 15, step: 89
	action: tensor([[-0.4215, -0.4212, -1.1443, -0.8738, -1.4577,  1.2678, -0.2347]],
       dtype=torch.float64)
	q_value: tensor([[-33.5329]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3447479387505228, distance: 1.3270189873126845 entropy 1.3432114124298096
epoch: 15, step: 90
	action: tensor([[-1.3189,  1.2576, -0.6983,  1.7936,  0.1994, -1.2779, -1.2827]],
       dtype=torch.float64)
	q_value: tensor([[-30.5645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 15, step: 91
	action: tensor([[ 0.2220,  0.4619, -0.7372, -0.1060, -0.6051, -1.8300, -0.2349]],
       dtype=torch.float64)
	q_value: tensor([[-34.4121]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 15, step: 92
	action: tensor([[ 0.4676, -0.0018, -1.0275,  0.8285, -0.9941, -0.5828, -0.2511]],
       dtype=torch.float64)
	q_value: tensor([[-34.4121]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 15, step: 93
	action: tensor([[-3.4278e-01, -6.4147e-02,  1.0042e-01,  1.5996e+00,  1.7488e+00,
         -5.8005e-01,  1.4922e-03]], dtype=torch.float64)
	q_value: tensor([[-34.4121]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 15, step: 94
	action: tensor([[-0.0900, -0.0148,  2.4075,  0.7987,  1.3937,  0.8782, -0.4053]],
       dtype=torch.float64)
	q_value: tensor([[-34.4121]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.026053501676144286, distance: 1.1591554906235766 entropy 1.3432114124298096
epoch: 15, step: 95
	action: tensor([[-1.7925, -1.5565, -0.1202, -1.0824,  0.9552,  0.3790, -0.1787]],
       dtype=torch.float64)
	q_value: tensor([[-37.9279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 15, step: 96
	action: tensor([[ 0.7014, -0.0255, -1.6602, -2.7485, -1.4495, -0.6103,  0.1421]],
       dtype=torch.float64)
	q_value: tensor([[-34.4121]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 15, step: 97
	action: tensor([[ 0.8000,  1.0248,  1.7681,  2.0566, -1.4347, -0.6839, -0.4575]],
       dtype=torch.float64)
	q_value: tensor([[-34.4121]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 15, step: 98
	action: tensor([[ 0.5544, -0.1887, -0.0056,  1.1945, -0.7809,  0.0279,  1.9880]],
       dtype=torch.float64)
	q_value: tensor([[-34.4121]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 15, step: 99
	action: tensor([[ 0.7713, -0.1968,  0.4006,  0.3034, -0.4980, -0.0461, -2.5213]],
       dtype=torch.float64)
	q_value: tensor([[-34.4121]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7357686018289344, distance: 0.5882323496074601 entropy 1.3432114124298096
epoch: 15, step: 100
	action: tensor([[ 2.2412,  0.5706, -0.4387, -1.5158, -1.5814,  1.8274, -0.8460]],
       dtype=torch.float64)
	q_value: tensor([[-33.3038]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 15, step: 101
	action: tensor([[ 1.2538, -0.6048,  1.1047,  0.5795,  1.4050,  0.8611,  1.2626]],
       dtype=torch.float64)
	q_value: tensor([[-34.4121]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1268146302812032, distance: 1.2147388791879032 entropy 1.3432114124298096
epoch: 15, step: 102
	action: tensor([[ 0.6531, -2.7250,  1.1317,  0.5618, -0.3735, -1.9396, -0.0735]],
       dtype=torch.float64)
	q_value: tensor([[-33.9960]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 15, step: 103
	action: tensor([[-0.1693, -1.0527, -0.1450, -0.4694, -0.7094, -1.5142,  0.4649]],
       dtype=torch.float64)
	q_value: tensor([[-34.4121]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.181732085968906, distance: 1.2439880458882913 entropy 1.3432114124298096
epoch: 15, step: 104
	action: tensor([[-0.2075,  0.1425,  0.6678, -0.3342, -0.3877,  2.0476, -2.5552]],
       dtype=torch.float64)
	q_value: tensor([[-26.9165]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 15, step: 105
	action: tensor([[ 0.5003,  0.1936, -1.5148, -0.1228, -0.4317,  0.6887, -0.6265]],
       dtype=torch.float64)
	q_value: tensor([[-34.4121]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 15, step: 106
	action: tensor([[-1.2545, -0.3697, -1.0925, -2.9435, -0.3923,  1.2075, -0.2156]],
       dtype=torch.float64)
	q_value: tensor([[-34.4121]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 15, step: 107
	action: tensor([[ 1.3707,  0.0715, -0.1702, -0.4735,  0.6132,  0.7161, -1.3509]],
       dtype=torch.float64)
	q_value: tensor([[-34.4121]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 15, step: 108
	action: tensor([[ 0.3823, -0.2762,  0.4250, -0.1518, -0.3277, -0.5558,  0.1459]],
       dtype=torch.float64)
	q_value: tensor([[-34.4121]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12230649253731563, distance: 1.0720823253167178 entropy 1.3432114124298096
epoch: 15, step: 109
	action: tensor([[ 1.8856,  0.6083, -0.4364,  0.4570,  0.9830,  0.6191, -1.4056]],
       dtype=torch.float64)
	q_value: tensor([[-17.8070]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 15, step: 110
	action: tensor([[-0.3939, -2.0237, -0.7675, -0.8850, -0.1481,  1.0556, -1.7073]],
       dtype=torch.float64)
	q_value: tensor([[-34.4121]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 15, step: 111
	action: tensor([[ 1.4174,  0.6760,  0.4745, -0.9133, -0.1886, -1.8899,  0.9870]],
       dtype=torch.float64)
	q_value: tensor([[-34.4121]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6363723747810599, distance: 0.6900572579558159 entropy 1.3432114124298096
epoch: 15, step: 112
	action: tensor([[ 0.9395,  0.4294, -1.1326, -1.0280,  1.7718, -1.3243, -0.8975]],
       dtype=torch.float64)
	q_value: tensor([[-32.7823]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6678024415614399, distance: 0.659560914293473 entropy 1.3432114124298096
epoch: 15, step: 113
	action: tensor([[ 2.1412, -0.4574,  0.5100, -1.4029,  0.2052, -0.6884,  1.3747]],
       dtype=torch.float64)
	q_value: tensor([[-34.3119]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 15, step: 114
	action: tensor([[0.3223, 1.0591, 0.8294, 1.2661, 0.4434, 1.2830, 0.8197]],
       dtype=torch.float64)
	q_value: tensor([[-34.4121]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 15, step: 115
	action: tensor([[-0.0948, -0.6773,  0.4841,  0.1370, -0.1774,  3.1613,  0.8571]],
       dtype=torch.float64)
	q_value: tensor([[-34.4121]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 15, step: 116
	action: tensor([[-0.1539, -0.9766,  0.9922, -0.4807,  0.3274, -0.6436,  0.7500]],
       dtype=torch.float64)
	q_value: tensor([[-34.4121]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0310876877537276, distance: 1.6308763317447372 entropy 1.3432114124298096
epoch: 15, step: 117
	action: tensor([[-0.0148, -0.2313,  2.0577, -0.0439,  0.0457, -0.3797, -0.0956]],
       dtype=torch.float64)
	q_value: tensor([[-23.7135]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.026939559464151164, distance: 1.1288249546082805 entropy 1.3432114124298096
epoch: 15, step: 118
	action: tensor([[ 0.7098, -0.2723,  0.5409,  1.7011,  0.6112, -0.0295,  0.4112]],
       dtype=torch.float64)
	q_value: tensor([[-28.8560]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9082846042526536, distance: 0.34655950565323246 entropy 1.3432114124298096
epoch: 15, step: 119
	action: tensor([[ 0.3293,  0.1729, -0.5314,  0.4383,  1.0143,  0.0931, -0.3160]],
       dtype=torch.float64)
	q_value: tensor([[-28.3990]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 15, step: 120
	action: tensor([[ 2.2559,  0.2669, -0.5282,  0.4440,  1.5425, -0.9777, -0.4432]],
       dtype=torch.float64)
	q_value: tensor([[-34.4121]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 15, step: 121
	action: tensor([[-1.2337,  0.1232,  2.1793, -0.4208, -2.5954, -0.5260,  1.8800]],
       dtype=torch.float64)
	q_value: tensor([[-34.4121]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 15, step: 122
	action: tensor([[ 1.0377, -0.5221,  0.1917,  0.0162, -0.2357,  0.4779, -1.6167]],
       dtype=torch.float64)
	q_value: tensor([[-34.4121]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3948191142986559, distance: 0.8902239920277749 entropy 1.3432114124298096
epoch: 15, step: 123
	action: tensor([[-0.5655, -0.3570, -1.7276, -1.3352, -1.0942,  1.2951,  0.6248]],
       dtype=torch.float64)
	q_value: tensor([[-24.6079]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1286122579223954, distance: 1.068224209885832 entropy 1.3432114124298096
epoch: 15, step: 124
	action: tensor([[ 0.6988, -0.0248, -0.5018,  0.5192, -2.1196,  1.2789,  0.9691]],
       dtype=torch.float64)
	q_value: tensor([[-35.5351]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6335207594629584, distance: 0.6927577334856618 entropy 1.3432114124298096
epoch: 15, step: 125
	action: tensor([[-0.6400,  0.6055, -0.5033,  1.1297,  1.8870,  0.4419,  1.1093]],
       dtype=torch.float64)
	q_value: tensor([[-33.7851]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 15, step: 126
	action: tensor([[-0.7505,  0.5674, -0.1895, -0.1109, -0.2315,  0.3710,  1.5124]],
       dtype=torch.float64)
	q_value: tensor([[-34.4121]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20593881684780047, distance: 1.2566644531588875 entropy 1.3432114124298096
epoch: 15, step: 127
	action: tensor([[-0.8818, -1.0105, -0.6144,  0.7353,  0.0103,  0.6258,  1.0806]],
       dtype=torch.float64)
	q_value: tensor([[-27.3473]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.117220899130432, distance: 1.6650979242292698 entropy 1.3432114124298096
LOSS epoch 15 actor 299.53484142763716 critic 304.3763437034409 
epoch: 16, step: 0
	action: tensor([[ 0.3953,  1.0464, -0.7118, -0.5027, -0.4977,  0.1648, -0.0192]],
       dtype=torch.float64)
	q_value: tensor([[-25.9072]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 16, step: 1
	action: tensor([[ 0.3491, -1.4125, -0.6700, -0.2350,  2.1067, -1.1717,  0.3955]],
       dtype=torch.float64)
	q_value: tensor([[-34.1629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7263900690162137, distance: 1.5035786758727798 entropy 1.3432114124298096
epoch: 16, step: 2
	action: tensor([[-1.3542, -0.8417,  0.4391, -1.0270, -0.4225,  1.0910,  0.4667]],
       dtype=torch.float64)
	q_value: tensor([[-36.9276]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.4648703268717078, distance: 1.7966096646474987 entropy 1.3432114124298096
epoch: 16, step: 3
	action: tensor([[ 0.5800, -0.6906, -1.0068, -0.0329,  1.0134,  0.6149,  0.2922]],
       dtype=torch.float64)
	q_value: tensor([[-31.0585]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25076863558517215, distance: 0.9905232365656969 entropy 1.3432114124298096
epoch: 16, step: 4
	action: tensor([[-0.7673, -1.2386, -0.7983,  1.0983, -1.0739,  1.5345,  0.2460]],
       dtype=torch.float64)
	q_value: tensor([[-25.3609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7985961572251385, distance: 1.5347001086556655 entropy 1.3432114124298096
epoch: 16, step: 5
	action: tensor([[-1.2068, -0.2477, -0.4397, -0.7545,  0.5965, -0.6397,  0.3997]],
       dtype=torch.float64)
	q_value: tensor([[-31.4576]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0900632984052798, distance: 1.6543843487961645 entropy 1.3432114124298096
epoch: 16, step: 6
	action: tensor([[ 0.7716, -1.2355, -0.9170,  0.1577, -0.3356,  0.3061, -0.4983]],
       dtype=torch.float64)
	q_value: tensor([[-25.2380]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5392464957221108, distance: 1.419746412778279 entropy 1.3432114124298096
epoch: 16, step: 7
	action: tensor([[ 0.3223, -1.4613, -0.5021, -1.3952, -0.8714, -1.0047, -0.4502]],
       dtype=torch.float64)
	q_value: tensor([[-23.6952]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 16, step: 8
	action: tensor([[-1.0544, -1.2996, -1.8484, -0.2887, -0.0998,  1.2929,  0.4279]],
       dtype=torch.float64)
	q_value: tensor([[-34.1629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2265233335016852, distance: 1.7075377692403744 entropy 1.3432114124298096
epoch: 16, step: 9
	action: tensor([[-0.3676, -0.9268, -0.4516, -1.8196, -0.5756,  0.4188,  1.7428]],
       dtype=torch.float64)
	q_value: tensor([[-33.3370]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.29376145754052807, distance: 1.3016187550548184 entropy 1.3432114124298096
epoch: 16, step: 10
	action: tensor([[ 1.1057, -1.4600,  0.0395,  0.5564,  0.0113, -0.2874,  0.9742]],
       dtype=torch.float64)
	q_value: tensor([[-36.1485]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 16, step: 11
	action: tensor([[-1.4482, -0.7505,  0.6652,  0.1237,  1.0818, -0.2957,  1.3413]],
       dtype=torch.float64)
	q_value: tensor([[-34.1629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.7775384818949123, distance: 1.9071582705034311 entropy 1.3432114124298096
epoch: 16, step: 12
	action: tensor([[-0.0725, -0.8387,  0.1512,  1.4771, -0.5749, -0.0185, -0.1449]],
       dtype=torch.float64)
	q_value: tensor([[-29.3062]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5412633506203494, distance: 0.7750655116153751 entropy 1.3432114124298096
epoch: 16, step: 13
	action: tensor([[-0.0296, -1.3150,  1.1819,  0.0902,  0.9850, -0.9471, -0.6143]],
       dtype=torch.float64)
	q_value: tensor([[-28.9746]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6924177849192321, distance: 1.488711295361415 entropy 1.3432114124298096
epoch: 16, step: 14
	action: tensor([[ 1.9400,  0.4782,  0.5805,  0.0336, -1.5021,  0.8510, -2.8388]],
       dtype=torch.float64)
	q_value: tensor([[-32.8741]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8249493997456065, distance: 0.47878275060508735 entropy 1.3432114124298096
epoch: 16, step: 15
	action: tensor([[ 0.8833,  0.6690,  0.2289,  0.9431, -0.2300, -2.9327, -0.7536]],
       dtype=torch.float64)
	q_value: tensor([[-39.8821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 16, step: 16
	action: tensor([[ 1.0384, -1.2199,  0.1200, -0.7487, -1.8701,  0.1765, -0.0682]],
       dtype=torch.float64)
	q_value: tensor([[-34.1629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9681626739007201, distance: 1.6054145283921049 entropy 1.3432114124298096
epoch: 16, step: 17
	action: tensor([[-1.1698, -0.5434, -1.9540, -0.5289, -1.0621,  0.8226, -0.8825]],
       dtype=torch.float64)
	q_value: tensor([[-32.7032]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.035192970942762, distance: 1.6325236829432577 entropy 1.3432114124298096
epoch: 16, step: 18
	action: tensor([[ 0.3380, -0.8654, -1.2430, -1.8921,  0.7587,  0.8407,  1.8124]],
       dtype=torch.float64)
	q_value: tensor([[-32.6187]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4159995575131912, distance: 0.874506982303789 entropy 1.3432114124298096
epoch: 16, step: 19
	action: tensor([[ 0.0399, -1.5903,  1.5748, -1.3038, -0.1810, -0.9923,  0.7487]],
       dtype=torch.float64)
	q_value: tensor([[-40.8522]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 16, step: 20
	action: tensor([[ 0.6945, -0.5340, -0.7281, -1.5956,  0.2858,  1.9073,  0.2522]],
       dtype=torch.float64)
	q_value: tensor([[-34.1629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3214171747444484, distance: 0.9426666116471187 entropy 1.3432114124298096
epoch: 16, step: 21
	action: tensor([[ 1.2378, -1.0140,  0.9087,  0.1195, -0.0643,  1.8802, -0.6158]],
       dtype=torch.float64)
	q_value: tensor([[-35.6192]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2125285136671602, distance: 1.0154864011199285 entropy 1.3432114124298096
epoch: 16, step: 22
	action: tensor([[-0.8483,  0.7670, -0.6926,  0.9105,  0.5587, -0.4439,  0.4495]],
       dtype=torch.float64)
	q_value: tensor([[-34.7968]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 16, step: 23
	action: tensor([[ 2.4457, -0.6454,  1.8266, -1.6066, -0.1736,  0.2555,  1.7897]],
       dtype=torch.float64)
	q_value: tensor([[-34.1629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 16, step: 24
	action: tensor([[-0.0057, -1.4688,  0.1509, -1.6403, -1.3326, -0.0163, -0.9091]],
       dtype=torch.float64)
	q_value: tensor([[-34.1629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7429490969837582, distance: 1.5107724101592772 entropy 1.3432114124298096
epoch: 16, step: 25
	action: tensor([[ 0.7365, -0.5909, -0.7324, -0.4547,  0.2500,  1.5566, -0.2033]],
       dtype=torch.float64)
	q_value: tensor([[-32.9250]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4602649597118944, distance: 0.8407115234470515 entropy 1.3432114124298096
epoch: 16, step: 26
	action: tensor([[ 2.3604, -1.7463, -0.5658, -1.4320,  1.1074, -0.5718, -0.4996]],
       dtype=torch.float64)
	q_value: tensor([[-26.8030]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 16, step: 27
	action: tensor([[-0.6216, -2.8113, -0.7752,  0.4143, -0.9856, -0.2747,  1.4744]],
       dtype=torch.float64)
	q_value: tensor([[-34.1629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 16, step: 28
	action: tensor([[-0.0562, -0.7999,  0.4711, -0.4823, -0.2959,  2.7421, -0.9745]],
       dtype=torch.float64)
	q_value: tensor([[-34.1629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 16, step: 29
	action: tensor([[-0.1612, -1.0866,  2.1843, -1.6232, -0.9513,  0.2151,  1.6530]],
       dtype=torch.float64)
	q_value: tensor([[-34.1629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3493088038246541, distance: 1.3292674501101365 entropy 1.3432114124298096
epoch: 16, step: 30
	action: tensor([[ 1.2563, -2.7479, -0.5965, -1.4502,  0.2331,  0.7316, -0.9076]],
       dtype=torch.float64)
	q_value: tensor([[-39.6475]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 16, step: 31
	action: tensor([[-1.1927, -0.2122,  0.5057, -1.1542,  0.2568, -0.9530,  0.3967]],
       dtype=torch.float64)
	q_value: tensor([[-34.1629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9703501548739832, distance: 1.6063064359912753 entropy 1.3432114124298096
epoch: 16, step: 32
	action: tensor([[-0.2685, -0.9074,  0.3325,  1.6201, -0.0583,  0.4125,  0.0390]],
       dtype=torch.float64)
	q_value: tensor([[-26.0869]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6348011965834288, distance: 0.6915464658696592 entropy 1.3432114124298096
epoch: 16, step: 33
	action: tensor([[ 1.4311, -1.1138, -0.7616,  0.4514,  1.6979,  1.7127, -0.9975]],
       dtype=torch.float64)
	q_value: tensor([[-28.9423]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.049676704465675625, distance: 1.1155585803834533 entropy 1.3432114124298096
epoch: 16, step: 34
	action: tensor([[ 0.0598,  0.3387, -0.1207,  0.7531, -0.8506, -0.8819,  0.4007]],
       dtype=torch.float64)
	q_value: tensor([[-38.4065]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 16, step: 35
	action: tensor([[-0.2465, -0.2906, -0.6329,  0.1619, -0.2255, -0.6339,  0.3576]],
       dtype=torch.float64)
	q_value: tensor([[-34.1629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08280534154096109, distance: 1.1907809768514794 entropy 1.3432114124298096
epoch: 16, step: 36
	action: tensor([[ 0.0842, -1.0180, -0.8369,  0.9357, -0.4671, -0.1839, -0.7064]],
       dtype=torch.float64)
	q_value: tensor([[-19.6382]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.34258171900001244, distance: 1.325949726207764 entropy 1.3432114124298096
epoch: 16, step: 37
	action: tensor([[ 0.7863, -0.5394, -1.0618,  0.1337, -1.0124,  0.4343, -0.3345]],
       dtype=torch.float64)
	q_value: tensor([[-24.5067]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20618928829583905, distance: 1.0195655923858835 entropy 1.3432114124298096
epoch: 16, step: 38
	action: tensor([[-0.8080,  0.3075,  0.3940, -0.2060,  0.2705, -0.3280,  1.7030]],
       dtype=torch.float64)
	q_value: tensor([[-21.9694]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7417071823644978, distance: 1.510234074103142 entropy 1.3432114124298096
epoch: 16, step: 39
	action: tensor([[ 1.0155, -0.5686, -1.2021, -0.0786, -0.2007,  0.2730, -1.5942]],
       dtype=torch.float64)
	q_value: tensor([[-27.7358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18574121180611447, distance: 1.0326137900615366 entropy 1.3432114124298096
epoch: 16, step: 40
	action: tensor([[ 0.9380,  1.1451, -2.5126,  0.0184, -0.0408, -0.7455,  0.0830]],
       dtype=torch.float64)
	q_value: tensor([[-25.2375]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 16, step: 41
	action: tensor([[-0.0439, -1.5174,  0.4104, -0.4625, -1.5823, -1.0086,  0.7582]],
       dtype=torch.float64)
	q_value: tensor([[-34.1629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6320597742700944, distance: 1.461923756965916 entropy 1.3432114124298096
epoch: 16, step: 42
	action: tensor([[ 1.2538,  0.8503, -0.9674, -1.1523,  1.1202,  0.4167,  0.5841]],
       dtype=torch.float64)
	q_value: tensor([[-31.9684]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4853755754048633, distance: 0.8209219923266948 entropy 1.3432114124298096
epoch: 16, step: 43
	action: tensor([[ 1.7033, -1.5808, -1.3494, -1.6061,  0.2165, -0.3617, -1.0782]],
       dtype=torch.float64)
	q_value: tensor([[-29.7385]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 16, step: 44
	action: tensor([[ 0.8126, -0.8059,  0.3083, -1.7657,  0.8443, -0.1637,  0.8502]],
       dtype=torch.float64)
	q_value: tensor([[-34.1629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5917312505459535, distance: 1.4437485621424904 entropy 1.3432114124298096
epoch: 16, step: 45
	action: tensor([[ 0.2106, -0.3725, -1.7140, -0.0576, -0.5481,  0.7710, -0.2431]],
       dtype=torch.float64)
	q_value: tensor([[-30.4772]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.022401594059662022, distance: 1.1570908311827055 entropy 1.3432114124298096
epoch: 16, step: 46
	action: tensor([[-0.4610, -1.7178,  0.3686,  0.9220, -0.3273,  0.6938, -0.5031]],
       dtype=torch.float64)
	q_value: tensor([[-22.6891]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 16, step: 47
	action: tensor([[ 1.4468, -0.1986,  0.1474, -0.3824, -0.2261,  1.3767,  0.7104]],
       dtype=torch.float64)
	q_value: tensor([[-34.1629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 16, step: 48
	action: tensor([[-1.1518,  0.7796, -0.2740,  0.2555, -0.7001,  0.7515,  0.4403]],
       dtype=torch.float64)
	q_value: tensor([[-34.1629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 16, step: 49
	action: tensor([[ 0.3827, -0.4891, -0.4313, -1.7233, -0.4338, -0.7356, -1.2969]],
       dtype=torch.float64)
	q_value: tensor([[-34.1629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09115438936366638, distance: 1.1953629622110373 entropy 1.3432114124298096
epoch: 16, step: 50
	action: tensor([[ 0.8622, -0.6299, -0.4861, -0.8218,  0.6032,  0.1253,  0.7009]],
       dtype=torch.float64)
	q_value: tensor([[-30.0329]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3504222735528595, distance: 1.3298158026585547 entropy 1.3432114124298096
epoch: 16, step: 51
	action: tensor([[ 0.0289, -0.9625, -0.5635, -2.0796, -0.3616, -0.3049,  1.6111]],
       dtype=torch.float64)
	q_value: tensor([[-24.6368]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.048366986987059324, distance: 1.1163270365083684 entropy 1.3432114124298096
epoch: 16, step: 52
	action: tensor([[ 1.2994,  1.1406, -0.0808, -0.6553,  0.7285,  0.8653,  1.8433]],
       dtype=torch.float64)
	q_value: tensor([[-35.1934]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7579814756253664, distance: 0.5629644845381807 entropy 1.3432114124298096
epoch: 16, step: 53
	action: tensor([[ 0.5615, -0.5233, -0.8764, -0.3918,  1.1024,  1.5559,  0.0113]],
       dtype=torch.float64)
	q_value: tensor([[-36.6473]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6539054649969589, distance: 0.6732154341880991 entropy 1.3432114124298096
epoch: 16, step: 54
	action: tensor([[-0.7131,  0.3271,  0.3290, -1.8991, -1.2439, -0.6117, -1.3836]],
       dtype=torch.float64)
	q_value: tensor([[-28.8925]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4143015770697811, distance: 1.360904675380715 entropy 1.3432114124298096
epoch: 16, step: 55
	action: tensor([[ 0.5133, -1.7311, -0.5702, -0.1331,  1.2862,  0.8073,  0.5283]],
       dtype=torch.float64)
	q_value: tensor([[-33.5783]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 16, step: 56
	action: tensor([[-0.3805,  1.1243, -0.0583,  0.7169, -1.1706,  2.3578,  0.0668]],
       dtype=torch.float64)
	q_value: tensor([[-34.1629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21753874178106491, distance: 1.0122507683587565 entropy 1.3432114124298096
epoch: 16, step: 57
	action: tensor([[ 0.5800, -0.4793, -1.5754,  2.1011, -0.4593, -0.5934, -0.1156]],
       dtype=torch.float64)
	q_value: tensor([[-32.3575]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 16, step: 58
	action: tensor([[-0.3598, -1.6218,  0.1014, -0.6874,  0.7570,  1.9023,  1.1123]],
       dtype=torch.float64)
	q_value: tensor([[-34.1629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1690198592946568, distance: 1.2372789884631008 entropy 1.3432114124298096
epoch: 16, step: 59
	action: tensor([[ 0.7330, -0.4373,  1.4654,  2.4748,  0.5281,  0.5248,  0.6486]],
       dtype=torch.float64)
	q_value: tensor([[-35.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2544910521001653, distance: 0.9880595575481114 entropy 1.3432114124298096
epoch: 16, step: 60
	action: tensor([[ 0.8851, -0.7548, -0.9473,  0.9813, -1.5473,  0.1104,  0.2095]],
       dtype=torch.float64)
	q_value: tensor([[-41.9241]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42106488102011996, distance: 0.870706207945363 entropy 1.3432114124298096
epoch: 16, step: 61
	action: tensor([[-0.2862, -1.3330,  0.8618,  0.0019,  0.8841,  1.2422, -0.0663]],
       dtype=torch.float64)
	q_value: tensor([[-30.0227]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.31558839768847635, distance: 1.3125525819931385 entropy 1.3432114124298096
epoch: 16, step: 62
	action: tensor([[ 1.5079, -2.0054, -0.5102,  0.4900, -0.8635, -0.7365,  0.4710]],
       dtype=torch.float64)
	q_value: tensor([[-29.2582]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 16, step: 63
	action: tensor([[ 0.7842, -0.6953, -0.7895, -1.0828,  0.4534, -1.6343, -0.2559]],
       dtype=torch.float64)
	q_value: tensor([[-34.1629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10683912119343209, distance: 1.2039236440111891 entropy 1.3432114124298096
epoch: 16, step: 64
	action: tensor([[ 1.4341,  0.5727, -0.9364, -1.2108,  0.3471,  0.6433,  0.1567]],
       dtype=torch.float64)
	q_value: tensor([[-31.3126]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3348481340295688, distance: 0.9332910479877963 entropy 1.3432114124298096
epoch: 16, step: 65
	action: tensor([[ 0.7571, -1.5356, -1.5688, -0.1528,  2.3290,  0.7821,  1.0390]],
       dtype=torch.float64)
	q_value: tensor([[-26.2686]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24825131906150655, distance: 1.2785205416106968 entropy 1.3432114124298096
epoch: 16, step: 66
	action: tensor([[ 2.5254, -1.3743,  1.5922,  0.0232, -0.2419,  0.7937,  0.1101]],
       dtype=torch.float64)
	q_value: tensor([[-41.4912]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 16, step: 67
	action: tensor([[-0.0925, -0.0203,  0.4977,  0.5443, -0.4297,  0.4270,  0.7069]],
       dtype=torch.float64)
	q_value: tensor([[-34.1629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 16, step: 68
	action: tensor([[-0.0201,  0.4639,  0.2350,  0.1540,  0.7083,  0.2479, -0.3040]],
       dtype=torch.float64)
	q_value: tensor([[-34.1629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 16, step: 69
	action: tensor([[ 0.6647, -0.6069, -1.2488, -0.6653,  0.0613, -0.5324,  0.9731]],
       dtype=torch.float64)
	q_value: tensor([[-34.1629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 16, step: 70
	action: tensor([[ 1.3001, -1.2475,  0.6960, -1.4190, -0.5356,  1.1712,  0.1859]],
       dtype=torch.float64)
	q_value: tensor([[-34.1629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3127369290233728, distance: 1.3111293660412575 entropy 1.3432114124298096
epoch: 16, step: 71
	action: tensor([[ 2.0094, -1.7897, -1.4458,  1.1352,  0.9829, -0.2259,  0.7144]],
       dtype=torch.float64)
	q_value: tensor([[-32.3387]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 16, step: 72
	action: tensor([[ 0.6684,  2.2321, -0.3318, -2.6741,  0.7020,  0.9735, -0.0636]],
       dtype=torch.float64)
	q_value: tensor([[-34.1629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 16, step: 73
	action: tensor([[ 0.4195, -0.4468, -1.8324,  0.3091, -1.3210,  0.6031, -0.7791]],
       dtype=torch.float64)
	q_value: tensor([[-34.1629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09080098803140646, distance: 1.1951693704159314 entropy 1.3432114124298096
epoch: 16, step: 74
	action: tensor([[-0.2415, -1.2645,  0.5819, -1.0052, -1.3459,  0.7893, -0.1108]],
       dtype=torch.float64)
	q_value: tensor([[-26.5451]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1611044693220642, distance: 1.6822656376517946 entropy 1.3432114124298096
epoch: 16, step: 75
	action: tensor([[ 1.0282e+00,  1.1292e+00,  7.3035e-04,  1.0984e-01, -2.8196e-01,
         -1.1680e+00, -1.9128e+00]], dtype=torch.float64)
	q_value: tensor([[-31.2662]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9309274279750462, distance: 0.30075268084685297 entropy 1.3432114124298096
epoch: 16, step: 76
	action: tensor([[-0.3577, -0.4541,  1.0431,  0.1925, -1.7201,  0.4537,  0.2350]],
       dtype=torch.float64)
	q_value: tensor([[-31.7890]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 16, step: 77
	action: tensor([[ 1.1411, -1.8463, -0.2388, -0.9624,  0.5594,  0.6539, -0.1337]],
       dtype=torch.float64)
	q_value: tensor([[-34.1629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 16, step: 78
	action: tensor([[ 0.5233, -1.8050, -0.9089, -0.5880,  0.1720,  0.0820,  1.0537]],
       dtype=torch.float64)
	q_value: tensor([[-34.1629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 16, step: 79
	action: tensor([[-0.5934,  1.4476, -1.1557, -1.2794, -0.0027, -1.1379,  0.8071]],
       dtype=torch.float64)
	q_value: tensor([[-34.1629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 16, step: 80
	action: tensor([[ 1.0215,  1.4361, -0.7818,  0.5232,  0.1641,  0.3574,  1.5961]],
       dtype=torch.float64)
	q_value: tensor([[-34.1629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 16, step: 81
	action: tensor([[-0.5733,  0.4864,  0.0474, -0.2075, -0.9651,  0.3828, -0.4973]],
       dtype=torch.float64)
	q_value: tensor([[-34.1629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1626342804520562, distance: 1.2338951446545452 entropy 1.3432114124298096
epoch: 16, step: 82
	action: tensor([[ 1.4368, -0.8987,  1.2231,  1.2305, -1.6082,  0.4988,  0.2414]],
       dtype=torch.float64)
	q_value: tensor([[-21.6375]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 16, step: 83
	action: tensor([[-0.4819, -0.6789, -0.7612, -0.4533, -1.2213,  0.6628, -0.6329]],
       dtype=torch.float64)
	q_value: tensor([[-34.1629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8385568112472204, distance: 1.5516551968400778 entropy 1.3432114124298096
epoch: 16, step: 84
	action: tensor([[-1.1721, -0.3944,  0.6591, -1.4089,  1.3201, -0.4375, -0.8968]],
       dtype=torch.float64)
	q_value: tensor([[-26.2632]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2541617529840954, distance: 1.7181031402991493 entropy 1.3432114124298096
epoch: 16, step: 85
	action: tensor([[ 0.4317,  1.9849,  0.1695, -0.8793, -1.5302, -0.0676, -1.4763]],
       dtype=torch.float64)
	q_value: tensor([[-35.0727]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 16, step: 86
	action: tensor([[ 0.6190, -0.7178, -0.4752, -0.5499, -0.6052,  1.3617,  0.1354]],
       dtype=torch.float64)
	q_value: tensor([[-34.1629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.017328416089211318, distance: 1.1343860889788493 entropy 1.3432114124298096
epoch: 16, step: 87
	action: tensor([[-0.6874, -0.9630, -0.8256,  0.9229, -0.5244,  1.0163, -0.8497]],
       dtype=torch.float64)
	q_value: tensor([[-25.8477]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8753485824425598, distance: 1.567103553969136 entropy 1.3432114124298096
epoch: 16, step: 88
	action: tensor([[-1.1448,  0.9763, -1.2816,  1.3506,  0.2669,  0.3963, -1.2260]],
       dtype=torch.float64)
	q_value: tensor([[-26.8351]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5466796531740059, distance: 1.42317032465498 entropy 1.3432114124298096
epoch: 16, step: 89
	action: tensor([[ 0.0773,  0.1440, -0.4447, -0.0751, -0.1167, -0.1685,  2.4095]],
       dtype=torch.float64)
	q_value: tensor([[-28.4616]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 16, step: 90
	action: tensor([[-0.0425,  0.9848, -1.6920,  0.0467,  1.0867,  0.7353, -0.7543]],
       dtype=torch.float64)
	q_value: tensor([[-34.1629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 16, step: 91
	action: tensor([[-0.5204, -0.6746,  0.0831, -1.3221, -1.8049,  0.6394, -0.8232]],
       dtype=torch.float64)
	q_value: tensor([[-34.1629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9110254056192741, distance: 1.5819396944605744 entropy 1.3432114124298096
epoch: 16, step: 92
	action: tensor([[ 0.4465,  0.8791,  0.5363, -1.8222,  0.5808, -0.2767,  2.0372]],
       dtype=torch.float64)
	q_value: tensor([[-33.6214]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29190057782271106, distance: 0.9629501679951379 entropy 1.3432114124298096
epoch: 16, step: 93
	action: tensor([[-0.0325, -0.2464, -0.9357, -1.1065, -0.5074, -0.7608, -1.5485]],
       dtype=torch.float64)
	q_value: tensor([[-36.9918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41621786663730564, distance: 0.8743435143640517 entropy 1.3432114124298096
epoch: 16, step: 94
	action: tensor([[ 2.2817, -0.4626,  0.6127,  0.9385, -0.6934,  0.6715, -0.7240]],
       dtype=torch.float64)
	q_value: tensor([[-28.4618]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 16, step: 95
	action: tensor([[ 0.6616, -0.0723, -0.3733,  0.5288, -0.4334, -0.6330,  0.3451]],
       dtype=torch.float64)
	q_value: tensor([[-34.1629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 16, step: 96
	action: tensor([[ 0.6501,  0.0286, -0.2045,  0.5616,  0.4462,  0.0055, -0.4836]],
       dtype=torch.float64)
	q_value: tensor([[-34.1629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 16, step: 97
	action: tensor([[ 0.6211, -0.2885,  0.1682, -0.7766,  0.7872,  0.4865, -0.9398]],
       dtype=torch.float64)
	q_value: tensor([[-34.1629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09010408250000645, distance: 1.0915724134042553 entropy 1.3432114124298096
epoch: 16, step: 98
	action: tensor([[ 0.9396,  0.5105, -0.1407, -0.7196, -0.6535,  0.5997, -0.1749]],
       dtype=torch.float64)
	q_value: tensor([[-23.6001]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8847812802394587, distance: 0.38843487545145217 entropy 1.3432114124298096
epoch: 16, step: 99
	action: tensor([[ 0.2944,  0.0366, -0.4171,  0.5261, -0.0515,  0.0124, -1.1252]],
       dtype=torch.float64)
	q_value: tensor([[-20.8610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 16, step: 100
	action: tensor([[-0.5156, -1.8606, -2.4053,  0.3652, -0.2083,  0.4666,  0.1423]],
       dtype=torch.float64)
	q_value: tensor([[-34.1629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 16, step: 101
	action: tensor([[ 0.3381, -1.1154, -0.3539,  0.8662, -0.8111, -0.2052, -0.7437]],
       dtype=torch.float64)
	q_value: tensor([[-34.1629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.030518136324959255, distance: 1.1267473304509914 entropy 1.3432114124298096
epoch: 16, step: 102
	action: tensor([[-0.5915, -0.0055, -1.3791,  0.3557, -0.0182,  1.0927, -1.9369]],
       dtype=torch.float64)
	q_value: tensor([[-27.1060]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6419366361690333, distance: 1.4663407027793474 entropy 1.3432114124298096
epoch: 16, step: 103
	action: tensor([[-0.9747, -1.7517, -0.1707, -1.1362, -1.3368,  2.0300,  0.0237]],
       dtype=torch.float64)
	q_value: tensor([[-28.4985]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 16, step: 104
	action: tensor([[ 0.7589, -0.8153, -0.8701,  0.4023, -0.5691, -0.3458, -0.3795]],
       dtype=torch.float64)
	q_value: tensor([[-34.1629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09821264220787684, distance: 1.0866977417910588 entropy 1.3432114124298096
epoch: 16, step: 105
	action: tensor([[ 0.1504,  0.3335,  0.2973, -0.0286, -0.5344,  1.6040,  1.8654]],
       dtype=torch.float64)
	q_value: tensor([[-22.1412]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 16, step: 106
	action: tensor([[-1.5190, -0.2570,  1.2398,  0.5734, -0.3623, -0.5454,  1.1602]],
       dtype=torch.float64)
	q_value: tensor([[-34.1629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.4131628794150366, distance: 1.7776653665660138 entropy 1.3432114124298096
epoch: 16, step: 107
	action: tensor([[-1.1085, -0.1509,  0.7599,  1.6732,  1.5328,  0.4457,  1.4135]],
       dtype=torch.float64)
	q_value: tensor([[-30.7486]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 16, step: 108
	action: tensor([[ 0.4696,  0.3317,  0.2693, -0.1335,  0.7087,  0.8217, -0.9216]],
       dtype=torch.float64)
	q_value: tensor([[-34.1629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 16, step: 109
	action: tensor([[-0.7299, -0.9033, -0.7781, -1.4276, -0.2860,  0.2272,  0.2556]],
       dtype=torch.float64)
	q_value: tensor([[-34.1629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1517572294809182, distance: 1.2281097146386262 entropy 1.3432114124298096
epoch: 16, step: 110
	action: tensor([[-0.9983,  0.2007, -1.1369, -0.1719,  0.3640,  0.2246,  0.2620]],
       dtype=torch.float64)
	q_value: tensor([[-28.5159]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7209924255465836, distance: 1.5012263294070287 entropy 1.3432114124298096
epoch: 16, step: 111
	action: tensor([[-0.0778, -0.8908, -0.9238,  0.6318,  0.2977,  0.3268, -0.6283]],
       dtype=torch.float64)
	q_value: tensor([[-23.0789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.44262838294478124, distance: 1.3744657729975236 entropy 1.3432114124298096
epoch: 16, step: 112
	action: tensor([[-7.7968e-01,  2.1174e-01, -1.3327e-01, -7.1390e-01, -1.0499e-03,
          9.8542e-01, -1.1975e+00]], dtype=torch.float64)
	q_value: tensor([[-21.7006]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.46603879798771164, distance: 1.3855730425152224 entropy 1.3432114124298096
epoch: 16, step: 113
	action: tensor([[ 0.4728, -1.2373,  0.3461, -1.4563,  0.7870, -0.2558,  0.4445]],
       dtype=torch.float64)
	q_value: tensor([[-26.0845]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8770621053808201, distance: 1.5678193288397864 entropy 1.3432114124298096
epoch: 16, step: 114
	action: tensor([[-0.1581, -1.8692,  0.2914, -0.6711, -0.5100,  0.1573,  1.7899]],
       dtype=torch.float64)
	q_value: tensor([[-28.6497]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 16, step: 115
	action: tensor([[-1.0948, -2.5798,  0.6475, -1.5165, -0.8915,  0.6923, -0.0265]],
       dtype=torch.float64)
	q_value: tensor([[-34.1629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 16, step: 116
	action: tensor([[ 1.6988, -0.1972,  1.3420, -0.1065, -0.4662, -1.2582, -2.2653]],
       dtype=torch.float64)
	q_value: tensor([[-34.1629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33929015902567494, distance: 0.9301694700039546 entropy 1.3432114124298096
epoch: 16, step: 117
	action: tensor([[ 0.5008, -1.0663, -1.2600, -1.5581, -0.0082,  0.6375, -0.2593]],
       dtype=torch.float64)
	q_value: tensor([[-43.8218]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1965943135328987, distance: 1.2517862032343543 entropy 1.3432114124298096
epoch: 16, step: 118
	action: tensor([[ 0.9506, -1.4395, -0.2425,  0.6440, -0.0667,  1.2153,  0.4117]],
       dtype=torch.float64)
	q_value: tensor([[-30.8682]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 16, step: 119
	action: tensor([[-0.0304, -2.3574,  0.6170,  1.0421,  0.7082, -0.0116, -0.0496]],
       dtype=torch.float64)
	q_value: tensor([[-34.1629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 16, step: 120
	action: tensor([[ 1.5985, -1.3414, -2.6843,  0.8064, -1.2710, -1.4811,  1.1170]],
       dtype=torch.float64)
	q_value: tensor([[-34.1629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 16, step: 121
	action: tensor([[-0.2598, -0.6808, -0.0192,  0.7550,  1.7367,  0.1262, -0.4307]],
       dtype=torch.float64)
	q_value: tensor([[-34.1629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.023868301090566924, distance: 1.1579204978452189 entropy 1.3432114124298096
epoch: 16, step: 122
	action: tensor([[ 0.4846, -1.2136,  0.1116, -0.3874,  0.5354, -1.2635,  1.0357]],
       dtype=torch.float64)
	q_value: tensor([[-29.7224]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8018764913661716, distance: 1.5360989873847295 entropy 1.3432114124298096
epoch: 16, step: 123
	action: tensor([[ 0.4997, -0.5193, -0.4717,  0.0377, -0.9573, -0.0864,  1.0361]],
       dtype=torch.float64)
	q_value: tensor([[-29.2166]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21788564620967454, distance: 1.0120263526446005 entropy 1.3432114124298096
epoch: 16, step: 124
	action: tensor([[-0.1002, -0.5748,  0.0329, -1.4380, -0.2033, -0.2259, -0.3005]],
       dtype=torch.float64)
	q_value: tensor([[-24.2997]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5797966792226583, distance: 1.4383258691395522 entropy 1.3432114124298096
epoch: 16, step: 125
	action: tensor([[ 0.2428, -0.9037,  0.8687,  1.3570, -0.7623,  0.9805, -0.0258]],
       dtype=torch.float64)
	q_value: tensor([[-23.7060]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.828948882388639, distance: 0.47328163357727027 entropy 1.3432114124298096
epoch: 16, step: 126
	action: tensor([[ 0.7817, -1.4292, -1.7011,  0.3386, -0.1211, -0.3189,  1.3550]],
       dtype=torch.float64)
	q_value: tensor([[-32.4861]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 16, step: 127
	action: tensor([[-0.4356, -1.3561, -1.5295, -1.6136,  0.0616,  1.3730,  0.6897]],
       dtype=torch.float64)
	q_value: tensor([[-34.1629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.011338627161145753, distance: 1.1508136136961356 entropy 1.3432114124298096
LOSS epoch 16 actor 299.324631291965 critic 300.0278858378402 
epoch: 17, step: 0
	action: tensor([[ 2.1269, -3.1387,  0.4992, -0.6605, -0.6727,  0.6999,  1.4365]],
       dtype=torch.float64)
	q_value: tensor([[-40.8570]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 17, step: 1
	action: tensor([[-0.6111, -1.2520, -0.3181,  0.7324,  1.4091,  0.6765,  1.5897]],
       dtype=torch.float64)
	q_value: tensor([[-36.1521]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6607586407170969, distance: 1.4747213021494825 entropy 1.3432114124298096
epoch: 17, step: 2
	action: tensor([[-0.0447, -1.8104,  0.0871, -0.6077, -0.9269,  2.2983,  1.9227]],
       dtype=torch.float64)
	q_value: tensor([[-36.3958]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 17, step: 3
	action: tensor([[ 0.6953, -0.3072, -1.3369, -2.1507, -0.3250,  1.6175, -0.0194]],
       dtype=torch.float64)
	q_value: tensor([[-36.1521]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 17, step: 4
	action: tensor([[ 0.1692, -2.4445, -0.0990,  1.3945,  0.2310,  1.1542,  0.0582]],
       dtype=torch.float64)
	q_value: tensor([[-36.1521]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 17, step: 5
	action: tensor([[-0.9152,  0.1944, -0.0197,  0.8503,  1.1265, -0.7975,  1.1107]],
       dtype=torch.float64)
	q_value: tensor([[-36.1521]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.48457672639930993, distance: 1.3943057462684652 entropy 1.3432114124298096
epoch: 17, step: 6
	action: tensor([[ 1.3110, -1.3221,  0.0825, -0.9182,  0.3707,  0.7258, -0.4806]],
       dtype=torch.float64)
	q_value: tensor([[-30.8428]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9250038496157191, distance: 1.5877148048734122 entropy 1.3432114124298096
epoch: 17, step: 7
	action: tensor([[-0.1756, -1.7656, -0.2121, -0.2701,  0.3641,  1.7015,  0.8889]],
       dtype=torch.float64)
	q_value: tensor([[-32.5767]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 17, step: 8
	action: tensor([[ 0.2100, -1.7163,  1.0510,  0.1559, -0.8709,  1.9080, -0.4210]],
       dtype=torch.float64)
	q_value: tensor([[-36.1521]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2316879700569504, distance: 1.0030567617363122 entropy 1.3432114124298096
epoch: 17, step: 9
	action: tensor([[ 0.0810, -0.6522,  0.1517, -1.0800, -0.2893,  0.5290,  0.7466]],
       dtype=torch.float64)
	q_value: tensor([[-40.8395]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6553993223973966, distance: 1.472339894275065 entropy 1.3432114124298096
epoch: 17, step: 10
	action: tensor([[ 0.4668,  1.0601, -0.0532,  0.1964,  2.1331,  0.0894,  0.2556]],
       dtype=torch.float64)
	q_value: tensor([[-27.0420]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 17, step: 11
	action: tensor([[-0.0609, -0.0832, -0.8589, -0.9608, -0.1847,  0.1998,  1.2833]],
       dtype=torch.float64)
	q_value: tensor([[-36.1521]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2416436359446298, distance: 0.9965368463445234 entropy 1.3432114124298096
epoch: 17, step: 12
	action: tensor([[ 1.0684, -1.1901,  0.4695, -1.1925, -1.8438, -1.7208,  1.5012]],
       dtype=torch.float64)
	q_value: tensor([[-28.2892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5451733263349332, distance: 1.422477135905847 entropy 1.3432114124298096
epoch: 17, step: 13
	action: tensor([[ 0.8600, -1.1902, -2.0637, -2.2954,  1.2406, -0.3098, -2.0420]],
       dtype=torch.float64)
	q_value: tensor([[-47.6043]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4407205246077778, distance: 0.8557977378032874 entropy 1.3432114124298096
epoch: 17, step: 14
	action: tensor([[ 1.1911,  0.2012, -1.9847, -2.4444, -1.1857,  0.4431,  0.0296]],
       dtype=torch.float64)
	q_value: tensor([[-52.6557]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 17, step: 15
	action: tensor([[ 0.3484,  0.1860, -0.3544, -0.4352,  0.6561,  1.3111,  0.5375]],
       dtype=torch.float64)
	q_value: tensor([[-36.1521]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 17, step: 16
	action: tensor([[ 0.4716, -1.6486, -1.4286,  0.6902, -0.8288,  0.2280,  0.6972]],
       dtype=torch.float64)
	q_value: tensor([[-36.1521]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6738599974524875, distance: 1.4805267498763721 entropy 1.3432114124298096
epoch: 17, step: 17
	action: tensor([[-0.0390, -2.7857,  0.2878, -0.1616,  0.0628, -0.6067,  0.1007]],
       dtype=torch.float64)
	q_value: tensor([[-30.8174]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 17, step: 18
	action: tensor([[-1.0188,  0.3675, -0.1215,  0.5632,  1.0418, -0.1211, -0.1812]],
       dtype=torch.float64)
	q_value: tensor([[-36.1521]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 17, step: 19
	action: tensor([[ 0.7692, -0.8714,  1.1490, -2.4251, -1.4009, -1.3410, -0.0093]],
       dtype=torch.float64)
	q_value: tensor([[-36.1521]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15947227480817783, distance: 1.2322160956769193 entropy 1.3432114124298096
epoch: 17, step: 20
	action: tensor([[ 0.8223, -1.5682,  1.6514, -1.0010,  0.6903,  0.6707,  1.7662]],
       dtype=torch.float64)
	q_value: tensor([[-41.4838]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 17, step: 21
	action: tensor([[ 1.1439, -0.7385,  0.5214,  0.4185, -0.9191,  0.6040,  1.0555]],
       dtype=torch.float64)
	q_value: tensor([[-36.1521]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4443343808461452, distance: 0.8530283332795635 entropy 1.3432114124298096
epoch: 17, step: 22
	action: tensor([[-0.7895, -1.7245, -0.4112, -0.7495, -2.4276, -0.3674,  0.8232]],
       dtype=torch.float64)
	q_value: tensor([[-33.1976]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 17, step: 23
	action: tensor([[ 0.9736, -1.8949, -0.5997,  0.6524, -0.9093,  0.6187,  3.3769]],
       dtype=torch.float64)
	q_value: tensor([[-36.1521]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 17, step: 24
	action: tensor([[ 0.0671, -0.3646, -1.0919,  0.3621, -0.6310,  1.3832,  0.4456]],
       dtype=torch.float64)
	q_value: tensor([[-36.1521]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09837650645287255, distance: 1.1993123635800615 entropy 1.3432114124298096
epoch: 17, step: 25
	action: tensor([[-1.1122,  0.2954, -0.4799, -0.0755, -0.7069,  0.3406,  0.6800]],
       dtype=torch.float64)
	q_value: tensor([[-26.1895]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9287936670873054, distance: 1.5892769291987858 entropy 1.3432114124298096
epoch: 17, step: 26
	action: tensor([[ 0.7187, -0.6575, -1.5033, -0.6323, -1.0359,  1.4621,  1.9987]],
       dtype=torch.float64)
	q_value: tensor([[-25.8831]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0778199537252271, distance: 1.1880365528786148 entropy 1.3432114124298096
epoch: 17, step: 27
	action: tensor([[ 0.6884, -2.6256, -1.3455,  1.0528, -1.1999,  0.2712,  1.8635]],
       dtype=torch.float64)
	q_value: tensor([[-42.2792]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 17, step: 28
	action: tensor([[ 0.9031, -2.0285, -1.5764,  0.0657, -0.3630, -0.4190,  0.0226]],
       dtype=torch.float64)
	q_value: tensor([[-36.1521]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 17, step: 29
	action: tensor([[ 0.2910, -1.0506,  0.3966, -1.4949,  0.1486, -0.1437, -0.8457]],
       dtype=torch.float64)
	q_value: tensor([[-36.1521]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8846886395127533, distance: 1.571001138008755 entropy 1.3432114124298096
epoch: 17, step: 30
	action: tensor([[ 1.2560, -2.9478,  0.6202,  1.3819,  0.3548,  0.1142,  1.2048]],
       dtype=torch.float64)
	q_value: tensor([[-31.9465]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 17, step: 31
	action: tensor([[ 0.5304, -3.1934, -0.9932,  0.2839, -0.6275,  0.8519,  0.6098]],
       dtype=torch.float64)
	q_value: tensor([[-36.1521]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 17, step: 32
	action: tensor([[ 0.4847,  0.2743, -3.0455,  0.9037,  1.9035, -0.9944, -1.4665]],
       dtype=torch.float64)
	q_value: tensor([[-36.1521]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 17, step: 33
	action: tensor([[ 1.7297, -0.7970,  0.1638, -1.3984,  1.0041, -0.3755,  0.6073]],
       dtype=torch.float64)
	q_value: tensor([[-36.1521]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4784986496221013, distance: 1.391448571831354 entropy 1.3432114124298096
epoch: 17, step: 34
	action: tensor([[ 1.4241, -0.7372, -0.8519,  0.4302, -1.5300,  0.6943, -0.8110]],
       dtype=torch.float64)
	q_value: tensor([[-35.1905]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12162747165165144, distance: 1.0724969492953038 entropy 1.3432114124298096
epoch: 17, step: 35
	action: tensor([[ 1.6452, -0.9990,  1.3233, -0.8163,  0.8214, -0.4876,  1.1369]],
       dtype=torch.float64)
	q_value: tensor([[-33.8363]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11806369175845544, distance: 1.074670442849736 entropy 1.3432114124298096
epoch: 17, step: 36
	action: tensor([[ 0.4313, -2.2060, -1.4706, -1.1184,  1.6590, -0.0753,  1.6417]],
       dtype=torch.float64)
	q_value: tensor([[-38.6387]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 17, step: 37
	action: tensor([[-0.5520, -2.4522, -2.4008,  2.0251, -0.7822,  1.3021,  0.8376]],
       dtype=torch.float64)
	q_value: tensor([[-36.1521]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 17, step: 38
	action: tensor([[-0.0368, -0.2620,  0.9625, -0.1182, -0.4361,  0.0453,  1.3568]],
       dtype=torch.float64)
	q_value: tensor([[-36.1521]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03419639911554384, distance: 1.1637460071724801 entropy 1.3432114124298096
epoch: 17, step: 39
	action: tensor([[ 0.1312, -1.2827,  0.6427, -0.5104,  0.7729,  0.1234,  1.0549]],
       dtype=torch.float64)
	q_value: tensor([[-27.8486]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.040542683297521, distance: 1.6346679023186128 entropy 1.3432114124298096
epoch: 17, step: 40
	action: tensor([[-0.8651, -0.2611,  1.9470, -0.5747,  2.2436,  0.6701,  0.7859]],
       dtype=torch.float64)
	q_value: tensor([[-29.6574]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2350356762748271, distance: 1.7107987457899205 entropy 1.3432114124298096
epoch: 17, step: 41
	action: tensor([[-0.4866, -2.5877, -0.1679, -1.8136,  0.6805,  1.5055,  0.3206]],
       dtype=torch.float64)
	q_value: tensor([[-44.0700]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 17, step: 42
	action: tensor([[-0.2067, -1.7873, -0.6139, -0.2314, -0.7518,  1.3776,  0.7709]],
       dtype=torch.float64)
	q_value: tensor([[-36.1521]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 17, step: 43
	action: tensor([[-0.7530, -1.7494,  0.5427, -0.8205, -0.8721,  1.3267,  0.7320]],
       dtype=torch.float64)
	q_value: tensor([[-36.1521]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1603787481456198, distance: 1.6819831528561293 entropy 1.3432114124298096
epoch: 17, step: 44
	action: tensor([[ 0.4417, -0.7402, -0.4029, -0.3039, -0.3954,  0.0589, -0.4618]],
       dtype=torch.float64)
	q_value: tensor([[-36.3065]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.29807503388634027, distance: 1.3037868362106508 entropy 1.3432114124298096
epoch: 17, step: 45
	action: tensor([[ 1.0239, -1.5856, -1.4517, -1.8020, -0.6582,  0.7359,  1.4731]],
       dtype=torch.float64)
	q_value: tensor([[-21.4900]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 17, step: 46
	action: tensor([[-0.8757, -1.9070, -1.1934,  1.7177,  0.4226,  0.1083, -0.1560]],
       dtype=torch.float64)
	q_value: tensor([[-36.1521]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 17, step: 47
	action: tensor([[-0.0965, -1.4314,  0.4329, -0.7709,  0.8124,  0.2796, -0.2320]],
       dtype=torch.float64)
	q_value: tensor([[-36.1521]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0923379730227718, distance: 1.6552843604728802 entropy 1.3432114124298096
epoch: 17, step: 48
	action: tensor([[-0.5068, -0.3973,  1.3904, -1.0773,  0.0172,  1.0895,  1.8260]],
       dtype=torch.float64)
	q_value: tensor([[-29.1827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8052655778559643, distance: 1.537542906187335 entropy 1.3432114124298096
epoch: 17, step: 49
	action: tensor([[-1.3265, -1.2136, -0.5098,  0.4345, -0.7988, -0.3061, -0.7892]],
       dtype=torch.float64)
	q_value: tensor([[-39.4066]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.5773629896548158, distance: 1.8371494504141266 entropy 1.3432114124298096
epoch: 17, step: 50
	action: tensor([[ 0.7097, -1.4079,  1.0077,  0.2184,  0.6437,  1.7345,  0.0798]],
       dtype=torch.float64)
	q_value: tensor([[-34.0809]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 17, step: 51
	action: tensor([[ 1.2034, -1.8972, -0.5246,  0.4417,  1.0683,  1.9284, -0.4985]],
       dtype=torch.float64)
	q_value: tensor([[-36.1521]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 17, step: 52
	action: tensor([[ 0.1824, -0.3964, -0.4907, -0.4797,  1.0621, -0.4222,  0.4325]],
       dtype=torch.float64)
	q_value: tensor([[-36.1521]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16409905516005208, distance: 1.2346721772609572 entropy 1.3432114124298096
epoch: 17, step: 53
	action: tensor([[-0.3531, -0.7290, -0.8892, -0.5097, -0.7611, -1.9898,  0.2496]],
       dtype=torch.float64)
	q_value: tensor([[-26.1731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3510999690235692, distance: 0.9218188628989831 entropy 1.3432114124298096
epoch: 17, step: 54
	action: tensor([[ 0.8101, -0.7728, -0.4330, -0.4573,  1.7525, -0.2677,  0.2748]],
       dtype=torch.float64)
	q_value: tensor([[-36.1152]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.41307565698866, distance: 1.360314729753634 entropy 1.3432114124298096
epoch: 17, step: 55
	action: tensor([[ 0.5533, -1.0936, -0.9935, -1.4024,  0.0734,  0.2638,  0.4401]],
       dtype=torch.float64)
	q_value: tensor([[-35.2035]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.42084710004841686, distance: 1.3640502385951838 entropy 1.3432114124298096
epoch: 17, step: 56
	action: tensor([[ 0.5617, -2.2012, -1.6940, -0.1712, -0.9868, -1.0053,  0.4146]],
       dtype=torch.float64)
	q_value: tensor([[-31.1693]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 17, step: 57
	action: tensor([[ 0.6953,  0.6698,  0.5992, -0.8830,  1.0703, -0.4496,  1.0774]],
       dtype=torch.float64)
	q_value: tensor([[-36.1521]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 17, step: 58
	action: tensor([[-0.3822, -2.5748,  0.0637, -1.0988,  1.1302,  0.1663,  1.2505]],
       dtype=torch.float64)
	q_value: tensor([[-36.1521]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 17, step: 59
	action: tensor([[ 1.2720, -0.9858,  1.1919,  0.7902,  1.2739,  1.5115,  0.7589]],
       dtype=torch.float64)
	q_value: tensor([[-36.1521]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3573657577792624, distance: 1.333230185171734 entropy 1.3432114124298096
epoch: 17, step: 60
	action: tensor([[-0.1930, -1.9623, -0.6114,  0.1764, -0.3030,  0.7182,  0.2581]],
       dtype=torch.float64)
	q_value: tensor([[-44.2895]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 17, step: 61
	action: tensor([[-1.4376, -2.2556,  1.2177, -0.0221,  0.7202,  1.0118,  1.4668]],
       dtype=torch.float64)
	q_value: tensor([[-36.1521]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 17, step: 62
	action: tensor([[ 1.7692, -0.5340, -1.0568, -0.1446,  0.2718,  0.0536, -0.0229]],
       dtype=torch.float64)
	q_value: tensor([[-36.1521]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 17, step: 63
	action: tensor([[ 1.8384, -0.9811,  0.2463, -1.4369, -1.5893,  1.1444,  0.5476]],
       dtype=torch.float64)
	q_value: tensor([[-36.1521]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.090897987422492, distance: 1.1952225094052058 entropy 1.3432114124298096
epoch: 17, step: 64
	action: tensor([[ 1.0500, -1.1799, -0.6222, -0.5808, -0.2375,  1.6030,  0.0022]],
       dtype=torch.float64)
	q_value: tensor([[-40.5933]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23526396231032476, distance: 1.2718520051184303 entropy 1.3432114124298096
epoch: 17, step: 65
	action: tensor([[ 3.1457, -1.1902,  0.5209, -1.1632,  1.5739, -0.1082,  0.0491]],
       dtype=torch.float64)
	q_value: tensor([[-34.3580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 17, step: 66
	action: tensor([[ 9.7637e-02, -5.4957e-01, -2.3863e-01, -3.2238e-04, -3.7733e-01,
          1.0116e+00,  6.3871e-01]], dtype=torch.float64)
	q_value: tensor([[-36.1521]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10324879263038411, distance: 1.0836590899899488 entropy 1.3432114124298096
epoch: 17, step: 67
	action: tensor([[ 0.1923, -0.5677,  0.6972,  0.7550,  0.5492,  0.2646,  0.9146]],
       dtype=torch.float64)
	q_value: tensor([[-23.8652]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6491173851545851, distance: 0.6778562717887024 entropy 1.3432114124298096
epoch: 17, step: 68
	action: tensor([[ 0.3463, -2.5295,  1.7379, -2.0706, -0.5229,  0.3899,  0.2073]],
       dtype=torch.float64)
	q_value: tensor([[-28.3746]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 17, step: 69
	action: tensor([[ 1.4908, -2.7883,  0.0307, -1.0582, -0.2091,  0.0055, -0.2992]],
       dtype=torch.float64)
	q_value: tensor([[-36.1521]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 17, step: 70
	action: tensor([[-1.3022, -1.1822,  0.2587, -0.7478, -0.8826, -0.4035,  0.1635]],
       dtype=torch.float64)
	q_value: tensor([[-36.1521]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9380034640431567, distance: 1.5930667298419732 entropy 1.3432114124298096
epoch: 17, step: 71
	action: tensor([[ 0.4062, -2.0118, -0.2656, -1.1544, -0.0952,  0.0262,  0.6357]],
       dtype=torch.float64)
	q_value: tensor([[-31.5795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 17, step: 72
	action: tensor([[ 0.0981,  0.8859, -0.3903,  1.1288, -0.3066, -0.4242,  0.9495]],
       dtype=torch.float64)
	q_value: tensor([[-36.1521]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 17, step: 73
	action: tensor([[ 1.0198,  0.2843,  0.8367, -1.9590, -1.1382,  0.3095,  1.3421]],
       dtype=torch.float64)
	q_value: tensor([[-36.1521]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2894400411434561, distance: 0.9646217690444255 entropy 1.3432114124298096
epoch: 17, step: 74
	action: tensor([[ 0.7845, -1.4099, -2.4266, -0.1666, -0.8797,  1.3209,  0.7318]],
       dtype=torch.float64)
	q_value: tensor([[-38.6362]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6442805417848276, distance: 1.46738694854147 entropy 1.3432114124298096
epoch: 17, step: 75
	action: tensor([[ 0.0778, -0.8639, -0.9079, -1.4069, -0.2574,  1.1774,  0.0251]],
       dtype=torch.float64)
	q_value: tensor([[-38.9756]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.27580617895145854, distance: 1.2925550353355326 entropy 1.3432114124298096
epoch: 17, step: 76
	action: tensor([[-0.0333, -0.9528, -1.2146,  0.6188, -0.3783,  0.2412,  1.6469]],
       dtype=torch.float64)
	q_value: tensor([[-32.6596]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7350718948264987, distance: 1.507354600556666 entropy 1.3432114124298096
epoch: 17, step: 77
	action: tensor([[ 2.0499, -2.6538,  1.7796,  0.0940, -0.0896,  0.6981,  1.0658]],
       dtype=torch.float64)
	q_value: tensor([[-32.8600]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 17, step: 78
	action: tensor([[-0.2340, -1.5572, -0.3687, -1.1789,  0.6717, -0.5237, -1.3533]],
       dtype=torch.float64)
	q_value: tensor([[-36.1521]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5865303286528734, distance: 1.4413879352328984 entropy 1.3432114124298096
epoch: 17, step: 79
	action: tensor([[-0.5587, -0.7817, -0.6721,  0.1843,  0.6567,  0.6021,  0.6003]],
       dtype=torch.float64)
	q_value: tensor([[-37.6864]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7295251957350772, distance: 1.5049433070733491 entropy 1.3432114124298096
epoch: 17, step: 80
	action: tensor([[-1.1238, -0.0059,  0.8578,  0.6552, -0.9557,  1.2019,  1.4215]],
       dtype=torch.float64)
	q_value: tensor([[-24.9818]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09825462755335379, distance: 1.1992458222232045 entropy 1.3432114124298096
epoch: 17, step: 81
	action: tensor([[ 1.6972, -1.4931,  0.9929,  0.7558, -0.6342, -0.4738,  0.9421]],
       dtype=torch.float64)
	q_value: tensor([[-35.8488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8137362732092774, distance: 1.541145926022151 entropy 1.3432114124298096
epoch: 17, step: 82
	action: tensor([[ 0.2873,  0.2651, -0.4189,  1.5707,  0.4505,  0.2646,  0.3765]],
       dtype=torch.float64)
	q_value: tensor([[-42.5336]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 17, step: 83
	action: tensor([[-1.6135, -0.5104, -0.8174, -0.5374, -0.4715,  0.6434,  0.0756]],
       dtype=torch.float64)
	q_value: tensor([[-36.1521]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.706507430807903, distance: 1.882614091848089 entropy 1.3432114124298096
epoch: 17, step: 84
	action: tensor([[ 0.6296, -1.3153,  0.1113, -1.4952, -0.2455,  2.7318,  0.1537]],
       dtype=torch.float64)
	q_value: tensor([[-30.6379]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 17, step: 85
	action: tensor([[ 0.0970, -0.8708,  0.8431, -0.0570,  0.2231,  0.3763, -0.1962]],
       dtype=torch.float64)
	q_value: tensor([[-36.1521]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.32095350681309576, distance: 1.3152262232913048 entropy 1.3432114124298096
epoch: 17, step: 86
	action: tensor([[-0.0098,  0.6103, -0.4430,  1.6802, -2.2773,  0.3559,  1.0605]],
       dtype=torch.float64)
	q_value: tensor([[-25.3785]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 17, step: 87
	action: tensor([[ 0.3536, -0.2676,  0.3935, -1.4850,  0.3035, -0.7137,  1.6536]],
       dtype=torch.float64)
	q_value: tensor([[-36.1521]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3570751126872187, distance: 1.333087438991122 entropy 1.3432114124298096
epoch: 17, step: 88
	action: tensor([[ 0.4826, -0.9953,  0.1568, -0.4000, -0.9662,  0.8228,  1.1349]],
       dtype=torch.float64)
	q_value: tensor([[-33.5931]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4645678489366314, distance: 1.3848777611744638 entropy 1.3432114124298096
epoch: 17, step: 89
	action: tensor([[-0.5695, -1.3504, -0.0575, -0.3377, -0.6848,  0.2151,  0.0113]],
       dtype=torch.float64)
	q_value: tensor([[-31.0648]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 17, step: 90
	action: tensor([[ 1.6250, -0.3027, -0.6616, -0.3565, -1.0084,  2.1761, -0.7343]],
       dtype=torch.float64)
	q_value: tensor([[-36.1521]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 17, step: 91
	action: tensor([[-2.1952,  0.3237,  0.3379,  1.3320, -0.1919,  0.1826,  0.6560]],
       dtype=torch.float64)
	q_value: tensor([[-36.1521]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 17, step: 92
	action: tensor([[ 1.6344, -2.3222, -0.9475, -0.2386, -0.2850,  0.4215, -0.5555]],
       dtype=torch.float64)
	q_value: tensor([[-36.1521]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 17, step: 93
	action: tensor([[ 0.4300,  0.1241,  0.1050, -0.0987, -0.0239, -0.1232,  1.9784]],
       dtype=torch.float64)
	q_value: tensor([[-36.1521]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6651860931016882, distance: 0.6621531319616636 entropy 1.3432114124298096
epoch: 17, step: 94
	action: tensor([[-0.8592, -1.2407, -0.4828, -0.4168,  0.1239, -0.5873,  0.1487]],
       dtype=torch.float64)
	q_value: tensor([[-33.0847]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8330611979914191, distance: 1.549334442166211 entropy 1.3432114124298096
epoch: 17, step: 95
	action: tensor([[ 1.1433, -1.3854, -1.1707, -0.2591,  0.7807,  1.1472,  0.8398]],
       dtype=torch.float64)
	q_value: tensor([[-28.4330]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 17, step: 96
	action: tensor([[ 1.3111,  2.2847, -2.5881,  0.6985,  1.5301, -0.2612,  0.2049]],
       dtype=torch.float64)
	q_value: tensor([[-36.1521]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 17, step: 97
	action: tensor([[ 1.2722, -0.5306, -1.1669, -1.5068, -0.8799,  0.3940,  0.7644]],
       dtype=torch.float64)
	q_value: tensor([[-36.1521]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 17, step: 98
	action: tensor([[ 1.1301, -0.9783,  1.3055, -1.1174, -0.0041, -0.2380,  0.8781]],
       dtype=torch.float64)
	q_value: tensor([[-36.1521]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10835678136999105, distance: 1.2047487508554182 entropy 1.3432114124298096
epoch: 17, step: 99
	action: tensor([[-0.0557, -1.2220, -0.4234, -0.4172,  0.4552, -0.3693,  1.7861]],
       dtype=torch.float64)
	q_value: tensor([[-33.3711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8994099997515035, distance: 1.5771247721117876 entropy 1.3432114124298096
epoch: 17, step: 100
	action: tensor([[ 0.4163,  1.5002, -1.9948,  0.6537,  1.0049,  1.1540,  0.0577]],
       dtype=torch.float64)
	q_value: tensor([[-32.6879]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 17, step: 101
	action: tensor([[ 2.1874, -0.0449, -0.0164,  0.3726, -2.0068,  1.1575,  1.0222]],
       dtype=torch.float64)
	q_value: tensor([[-36.1521]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 17, step: 102
	action: tensor([[-0.5523, -0.9310,  0.7803, -0.8294,  0.0957,  0.5212,  0.9775]],
       dtype=torch.float64)
	q_value: tensor([[-36.1521]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2247403623833661, distance: 1.7068539452216662 entropy 1.3432114124298096
epoch: 17, step: 103
	action: tensor([[-0.7336, -0.3774,  0.7593, -0.0151,  1.1359,  0.3612,  1.3653]],
       dtype=torch.float64)
	q_value: tensor([[-28.8737]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7955588853697975, distance: 1.533403744553971 entropy 1.3432114124298096
epoch: 17, step: 104
	action: tensor([[ 0.1512, -2.4951, -2.8119, -2.3090,  1.0305,  1.0318,  0.3146]],
       dtype=torch.float64)
	q_value: tensor([[-30.4089]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 17, step: 105
	action: tensor([[ 0.7308, -1.8522, -1.4820,  0.2203,  1.8863, -1.0856, -1.5552]],
       dtype=torch.float64)
	q_value: tensor([[-36.1521]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 17, step: 106
	action: tensor([[-0.0156, -0.1385, -1.5427,  0.2421, -2.4264, -1.2417,  2.3049]],
       dtype=torch.float64)
	q_value: tensor([[-36.1521]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15547568846625615, distance: 1.051629526829963 entropy 1.3432114124298096
epoch: 17, step: 107
	action: tensor([[ 1.0591, -2.0742, -1.4024,  0.1382,  0.6071,  0.0370,  0.0901]],
       dtype=torch.float64)
	q_value: tensor([[-51.5194]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 17, step: 108
	action: tensor([[ 0.1754, -0.1013, -0.2998, -0.7353,  1.8863,  1.1265,  0.8545]],
       dtype=torch.float64)
	q_value: tensor([[-36.1521]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.47758471066894936, distance: 0.8271125921294628 entropy 1.3432114124298096
epoch: 17, step: 109
	action: tensor([[ 0.1913, -1.2731, -1.5419, -1.1469,  0.4429, -0.3533,  1.3742]],
       dtype=torch.float64)
	q_value: tensor([[-35.7115]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0793688343540383, distance: 1.0979928944884092 entropy 1.3432114124298096
epoch: 17, step: 110
	action: tensor([[ 1.1610, -3.3619,  0.8914,  1.0613,  1.6500,  0.0813,  0.3705]],
       dtype=torch.float64)
	q_value: tensor([[-36.3823]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 17, step: 111
	action: tensor([[ 0.9140, -0.9880, -1.2204, -1.5656,  0.1956, -1.6100, -0.3481]],
       dtype=torch.float64)
	q_value: tensor([[-36.1521]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 17, step: 112
	action: tensor([[ 0.0148, -1.6243, -0.6260,  0.9151, -1.9167, -0.3613,  0.2500]],
       dtype=torch.float64)
	q_value: tensor([[-36.1521]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2767538823600457, distance: 1.293035018672453 entropy 1.3432114124298096
epoch: 17, step: 113
	action: tensor([[ 0.0549, -0.7789,  0.1671, -0.3214, -0.2413, -1.0181,  0.1811]],
       dtype=torch.float64)
	q_value: tensor([[-37.8551]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5544832970674589, distance: 1.4267560513846187 entropy 1.3432114124298096
epoch: 17, step: 114
	action: tensor([[-1.3742, -1.5590,  0.3638, -0.6924,  1.6611,  0.4901,  0.5909]],
       dtype=torch.float64)
	q_value: tensor([[-25.1225]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 17, step: 115
	action: tensor([[ 1.7551,  0.2169, -1.3180, -0.4068, -0.8699,  0.2748,  0.0268]],
       dtype=torch.float64)
	q_value: tensor([[-36.1521]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 17, step: 116
	action: tensor([[ 1.5941, -1.0857, -1.2513, -0.3946, -0.5581, -1.5164, -0.2984]],
       dtype=torch.float64)
	q_value: tensor([[-36.1521]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 17, step: 117
	action: tensor([[ 0.6871,  0.2416, -0.4820, -0.1594,  0.0228, -0.8830, -0.0172]],
       dtype=torch.float64)
	q_value: tensor([[-36.1521]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 17, step: 118
	action: tensor([[ 1.4728, -0.9925, -0.8580, -1.1145, -0.9348,  0.2893,  1.1423]],
       dtype=torch.float64)
	q_value: tensor([[-36.1521]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9657973036457337, distance: 1.6044495315823293 entropy 1.3432114124298096
epoch: 17, step: 119
	action: tensor([[ 0.4260, -1.3902,  0.0630, -0.9042, -0.5708,  1.3620,  0.9037]],
       dtype=torch.float64)
	q_value: tensor([[-35.4356]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 17, step: 120
	action: tensor([[ 0.4636, -0.0465,  0.4390, -0.7198, -0.1599,  0.9277,  0.7050]],
       dtype=torch.float64)
	q_value: tensor([[-36.1521]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5128736210989999, distance: 0.7986886583487259 entropy 1.3432114124298096
epoch: 17, step: 121
	action: tensor([[-0.4117, -1.3577, -0.4287, -0.0086,  0.6833, -1.9141,  0.9645]],
       dtype=torch.float64)
	q_value: tensor([[-26.8421]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5775153450349808, distance: 1.437286973602063 entropy 1.3432114124298096
epoch: 17, step: 122
	action: tensor([[ 0.5379,  1.6890, -0.3703, -1.5452, -0.7516, -0.2338,  0.2566]],
       dtype=torch.float64)
	q_value: tensor([[-37.7551]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 17, step: 123
	action: tensor([[ 0.1804, -1.4804,  2.2137, -0.1904,  0.6152,  1.2802,  1.2471]],
       dtype=torch.float64)
	q_value: tensor([[-36.1521]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7052185627722474, distance: 1.4943307009597844 entropy 1.3432114124298096
epoch: 17, step: 124
	action: tensor([[ 0.2796, -1.8257, -1.4107,  0.8278, -1.1576,  0.5872,  1.6557]],
       dtype=torch.float64)
	q_value: tensor([[-43.3831]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 17, step: 125
	action: tensor([[ 0.4648, -1.9840, -0.0793, -0.1338,  0.4959,  0.4895,  2.1710]],
       dtype=torch.float64)
	q_value: tensor([[-36.1521]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 17, step: 126
	action: tensor([[ 0.6565, -2.7187,  0.2607, -1.1791, -1.4799,  1.2045,  1.5752]],
       dtype=torch.float64)
	q_value: tensor([[-36.1521]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 17, step: 127
	action: tensor([[-0.9492, -1.1207,  1.8407,  0.9482, -1.6134,  1.4238,  0.9076]],
       dtype=torch.float64)
	q_value: tensor([[-36.1521]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1926140471259803, distance: 1.24970254265984 entropy 1.3432114124298096
LOSS epoch 17 actor 290.475918743212 critic 220.833813321612 
epoch: 18, step: 0
	action: tensor([[ 0.8559, -4.1212, -3.9940, -0.2101,  0.1670,  0.5678,  1.5832]],
       dtype=torch.float64)
	q_value: tensor([[-55.6825]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 18, step: 1
	action: tensor([[ 0.0125, -1.4488, -0.3622, -0.7994, -0.7863,  1.5913,  1.1141]],
       dtype=torch.float64)
	q_value: tensor([[-39.7918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.707453259668986, distance: 1.4953095440542405 entropy 1.2378510236740112
epoch: 18, step: 2
	action: tensor([[ 2.5121, -1.1972, -1.8072, -0.9272, -2.0356, -0.0716,  1.4622]],
       dtype=torch.float64)
	q_value: tensor([[-42.4622]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 18, step: 3
	action: tensor([[ 1.7379, -1.6399, -0.9798, -0.5111,  1.0306, -0.5343,  1.9134]],
       dtype=torch.float64)
	q_value: tensor([[-39.7918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.106262849060312, distance: 1.6607833303037243 entropy 1.2378510236740112
epoch: 18, step: 4
	action: tensor([[ 0.7262, -4.5701, -2.2774,  0.4361, -0.6184,  1.2838,  1.5256]],
       dtype=torch.float64)
	q_value: tensor([[-50.2438]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 18, step: 5
	action: tensor([[ 1.2773, -0.3312, -2.5774, -0.5272,  0.7791,  1.9532,  1.2386]],
       dtype=torch.float64)
	q_value: tensor([[-39.7918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 18, step: 6
	action: tensor([[ 0.8038, -1.2959, -2.0543, -1.0363, -1.0422,  0.6453,  2.0809]],
       dtype=torch.float64)
	q_value: tensor([[-39.7918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.251991485559385, distance: 1.2804345404039437 entropy 1.2378510236740112
epoch: 18, step: 7
	action: tensor([[ 3.1351, -2.9815, -1.7660, -1.1304, -0.7117,  1.6520,  2.0675]],
       dtype=torch.float64)
	q_value: tensor([[-51.8590]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 18, step: 8
	action: tensor([[ 0.7063, -1.4450,  0.4044,  0.3899, -1.6206,  0.5640,  1.2858]],
       dtype=torch.float64)
	q_value: tensor([[-39.7918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23101002320124664, distance: 1.26966014686499 entropy 1.2378510236740112
epoch: 18, step: 9
	action: tensor([[ 0.9093, -1.7570,  0.5946,  0.5124, -0.6133,  0.8194,  0.0640]],
       dtype=torch.float64)
	q_value: tensor([[-43.8469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 18, step: 10
	action: tensor([[ 0.3621,  0.0506,  0.3660, -0.3951, -0.0060,  0.4735,  2.2196]],
       dtype=torch.float64)
	q_value: tensor([[-39.7918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5286210627117618, distance: 0.7856729114842281 entropy 1.2378510236740112
epoch: 18, step: 11
	action: tensor([[ 1.5551, -3.7987, -2.0076, -1.0036,  1.1501,  0.6280,  2.0594]],
       dtype=torch.float64)
	q_value: tensor([[-41.2946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 18, step: 12
	action: tensor([[ 0.8917, -0.1710, -2.3558, -1.4951, -0.3783,  1.1585,  0.0261]],
       dtype=torch.float64)
	q_value: tensor([[-39.7918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7558535107420705, distance: 0.5654340204226123 entropy 1.2378510236740112
epoch: 18, step: 13
	action: tensor([[ 1.2328, -3.2080, -1.9660, -0.3555, -0.7291,  0.5545,  0.4032]],
       dtype=torch.float64)
	q_value: tensor([[-42.9251]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 18, step: 14
	action: tensor([[ 2.7187, -1.4308,  0.3605,  0.2874,  0.4947,  0.8050,  0.5922]],
       dtype=torch.float64)
	q_value: tensor([[-39.7918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 18, step: 15
	action: tensor([[ 1.3898, -1.5453, -1.0467,  0.1771,  0.3354,  0.7120,  2.5955]],
       dtype=torch.float64)
	q_value: tensor([[-39.7918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23512015092470917, distance: 1.2717779674517373 entropy 1.2378510236740112
epoch: 18, step: 16
	action: tensor([[ 1.4724, -4.2515, -0.7695, -1.2166, -0.8973,  1.7700,  3.3548]],
       dtype=torch.float64)
	q_value: tensor([[-54.2110]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 18, step: 17
	action: tensor([[ 0.4902, -1.0022, -0.7453, -1.4338, -0.4165,  0.3848, -0.7647]],
       dtype=torch.float64)
	q_value: tensor([[-39.7918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5259090610042187, distance: 1.4135820417579983 entropy 1.2378510236740112
epoch: 18, step: 18
	action: tensor([[ 1.7087, -1.1252, -1.4222, -0.0899,  0.4131,  1.5151,  2.6782]],
       dtype=torch.float64)
	q_value: tensor([[-36.1400]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04927147269134102, distance: 1.1157964002938305 entropy 1.2378510236740112
epoch: 18, step: 19
	action: tensor([[ 3.1064, -6.0552, -1.6419, -0.2884, -1.1239,  2.8423,  1.4445]],
       dtype=torch.float64)
	q_value: tensor([[-58.2031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 18, step: 20
	action: tensor([[ 1.5128, -2.5757, -1.8038,  0.4233, -1.6432,  0.8935,  1.9657]],
       dtype=torch.float64)
	q_value: tensor([[-39.7918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 18, step: 21
	action: tensor([[ 0.2873, -1.2616,  0.1688, -0.7004, -0.3130, -0.7361,  0.1241]],
       dtype=torch.float64)
	q_value: tensor([[-39.7918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9084730232750551, distance: 1.5808829151951411 entropy 1.2378510236740112
epoch: 18, step: 22
	action: tensor([[-0.1907, -2.2862, -0.5003, -1.7663,  0.8189,  0.3581,  1.3854]],
       dtype=torch.float64)
	q_value: tensor([[-31.8412]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 18, step: 23
	action: tensor([[ 0.9451, -2.2239, -0.5808, -0.3158, -0.5417,  2.1763,  1.1952]],
       dtype=torch.float64)
	q_value: tensor([[-39.7918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 18, step: 24
	action: tensor([[ 0.8180, -1.4319, -2.0795, -0.0210, -0.4801,  1.2583,  1.4056]],
       dtype=torch.float64)
	q_value: tensor([[-39.7918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6636887908610889, distance: 1.476021686927927 entropy 1.2378510236740112
epoch: 18, step: 25
	action: tensor([[ 0.6968, -4.6552, -0.2028, -1.6464, -0.9778,  1.6888,  1.5200]],
       dtype=torch.float64)
	q_value: tensor([[-44.8085]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 18, step: 26
	action: tensor([[-1.2186, -1.4577, -0.0726,  0.3197, -1.0141,  0.1346,  0.7798]],
       dtype=torch.float64)
	q_value: tensor([[-39.7918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.5424579912526526, distance: 1.8246668676773417 entropy 1.2378510236740112
epoch: 18, step: 27
	action: tensor([[-0.3217, -3.7802, -1.8541,  0.1349, -1.6140,  1.0484,  0.8481]],
       dtype=torch.float64)
	q_value: tensor([[-34.5949]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 18, step: 28
	action: tensor([[-0.7853, -2.0805, -0.6801, -0.1593,  1.0001,  0.7034,  1.4135]],
       dtype=torch.float64)
	q_value: tensor([[-39.7918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 18, step: 29
	action: tensor([[-0.1518, -2.0065, -0.7618, -0.1472, -0.0957, -0.9491,  0.9704]],
       dtype=torch.float64)
	q_value: tensor([[-39.7918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 18, step: 30
	action: tensor([[ 1.9920, -1.8334, -0.7400, -1.4621, -1.6188, -1.3508, -0.2347]],
       dtype=torch.float64)
	q_value: tensor([[-39.7918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 18, step: 31
	action: tensor([[ 0.8568, -1.9642, -0.8461, -1.4122,  0.8294,  1.0668,  1.1273]],
       dtype=torch.float64)
	q_value: tensor([[-39.7918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 18, step: 32
	action: tensor([[ 1.2478, -1.1169, -0.0557, -1.2098, -0.2306,  0.7721,  1.5963]],
       dtype=torch.float64)
	q_value: tensor([[-39.7918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8065517176717991, distance: 1.5380905106753353 entropy 1.2378510236740112
epoch: 18, step: 33
	action: tensor([[ 3.4103, -4.5695, -2.7008, -0.8855, -0.4247,  1.3254,  3.2007]],
       dtype=torch.float64)
	q_value: tensor([[-43.9342]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 18, step: 34
	action: tensor([[ 0.0258, -1.4248, -0.2354,  0.0490, -0.6949,  0.5568,  0.2364]],
       dtype=torch.float64)
	q_value: tensor([[-39.7918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7363958451167789, distance: 1.5079295859545192 entropy 1.2378510236740112
epoch: 18, step: 35
	action: tensor([[ 0.9008, -1.1008, -1.1894, -0.4819,  0.6170, -0.2903,  0.2190]],
       dtype=torch.float64)
	q_value: tensor([[-30.3356]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.655402498356803, distance: 1.4723413066501427 entropy 1.2378510236740112
epoch: 18, step: 36
	action: tensor([[ 1.2734, -1.6632, -0.9659, -0.4731,  0.5224, -0.1130,  0.8396]],
       dtype=torch.float64)
	q_value: tensor([[-34.3942]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 18, step: 37
	action: tensor([[ 0.3131, -0.7818, -1.5592, -0.6171, -1.2019,  0.5995, -0.1322]],
       dtype=torch.float64)
	q_value: tensor([[-39.7918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16574396856603557, distance: 1.235544187194361 entropy 1.2378510236740112
epoch: 18, step: 38
	action: tensor([[ 0.1484, -2.1065, -2.6158, -2.0222, -0.9889,  0.2724,  0.5223]],
       dtype=torch.float64)
	q_value: tensor([[-34.5409]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 18, step: 39
	action: tensor([[ 1.1814, -1.2693, -1.1576,  0.1653,  0.2476, -0.1432,  2.2013]],
       dtype=torch.float64)
	q_value: tensor([[-39.7918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5939517978281665, distance: 1.44475526306038 entropy 1.2378510236740112
epoch: 18, step: 40
	action: tensor([[ 1.1391, -2.9529, -2.1560, -1.5941,  0.9992,  0.9971,  1.5715]],
       dtype=torch.float64)
	q_value: tensor([[-48.5662]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 18, step: 41
	action: tensor([[ 2.4229, -2.2731, -1.1289, -0.1067,  0.4003,  0.2144, -0.5166]],
       dtype=torch.float64)
	q_value: tensor([[-39.7918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 18, step: 42
	action: tensor([[ 1.1019, -0.1130,  0.0808, -2.1006,  0.0674,  1.8556,  2.5489]],
       dtype=torch.float64)
	q_value: tensor([[-39.7918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3126335104289645, distance: 0.9487479949584571 entropy 1.2378510236740112
epoch: 18, step: 43
	action: tensor([[ 2.4711, -4.1869, -1.6577, -1.3645,  0.9193,  1.9084,  2.8320]],
       dtype=torch.float64)
	q_value: tensor([[-59.5940]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 18, step: 44
	action: tensor([[ 1.9086, -3.5197, -0.6654, -0.2250,  0.6574,  1.3814,  1.4697]],
       dtype=torch.float64)
	q_value: tensor([[-39.7918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 18, step: 45
	action: tensor([[-0.6032, -0.6854, -1.0412, -0.4085,  0.3217,  0.1744,  0.6396]],
       dtype=torch.float64)
	q_value: tensor([[-39.7918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6583819562481354, distance: 1.4736656996088422 entropy 1.2378510236740112
epoch: 18, step: 46
	action: tensor([[ 2.3285, -0.8787, -1.6281,  0.9082, -0.3401,  2.3347,  1.3196]],
       dtype=torch.float64)
	q_value: tensor([[-29.0283]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 18, step: 47
	action: tensor([[ 1.4480, -3.8561, -0.7036, -0.3406, -1.0045,  1.3388,  0.6860]],
       dtype=torch.float64)
	q_value: tensor([[-39.7918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 18, step: 48
	action: tensor([[ 1.1064, -2.3237, -0.6066, -1.0568, -0.7740, -0.0209,  1.0217]],
       dtype=torch.float64)
	q_value: tensor([[-39.7918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 18, step: 49
	action: tensor([[-0.5818, -2.2395, -1.7204, -0.7136,  0.2156, -0.3760,  0.5514]],
       dtype=torch.float64)
	q_value: tensor([[-39.7918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 18, step: 50
	action: tensor([[-0.1887, -2.6767, -1.8170,  0.0091,  1.0621,  0.2507,  2.6172]],
       dtype=torch.float64)
	q_value: tensor([[-39.7918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 18, step: 51
	action: tensor([[ 0.5162, -2.0825, -0.7362, -0.2833,  0.0673,  1.7198,  1.5119]],
       dtype=torch.float64)
	q_value: tensor([[-39.7918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 18, step: 52
	action: tensor([[-0.2915, -1.4704,  0.5083, -0.1506, -1.4395,  1.2133,  1.5265]],
       dtype=torch.float64)
	q_value: tensor([[-39.7918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8720276612825124, distance: 1.5657154031495624 entropy 1.2378510236740112
epoch: 18, step: 53
	action: tensor([[ 0.9183, -2.7462, -1.0690,  0.4760,  0.1603, -0.2643,  0.6751]],
       dtype=torch.float64)
	q_value: tensor([[-44.2984]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 18, step: 54
	action: tensor([[ 0.1468, -1.6045, -0.7305,  0.0764, -0.0749,  0.7550,  1.6614]],
       dtype=torch.float64)
	q_value: tensor([[-39.7918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6058195393683963, distance: 1.450123739480031 entropy 1.2378510236740112
epoch: 18, step: 55
	action: tensor([[ 1.1909, -3.1731, -1.7032,  0.2217, -0.5414,  1.0006,  1.0730]],
       dtype=torch.float64)
	q_value: tensor([[-37.4702]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 18, step: 56
	action: tensor([[ 1.8695, -1.8186,  0.5178, -0.8083, -1.1681,  0.3503,  1.2986]],
       dtype=torch.float64)
	q_value: tensor([[-39.7918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 18, step: 57
	action: tensor([[ 0.1549, -0.1761,  0.0114,  0.3652,  0.3517,  0.7598,  1.6711]],
       dtype=torch.float64)
	q_value: tensor([[-39.7918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6957143677776602, distance: 0.6312442227621596 entropy 1.2378510236740112
epoch: 18, step: 58
	action: tensor([[ 1.6483, -2.7682, -1.7370, -1.5938, -1.9762,  0.5809, -0.5822]],
       dtype=torch.float64)
	q_value: tensor([[-34.9048]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 18, step: 59
	action: tensor([[-0.8311, -2.4524, -1.4495, -2.0083,  1.2641,  0.4071,  2.9849]],
       dtype=torch.float64)
	q_value: tensor([[-39.7918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 18, step: 60
	action: tensor([[-0.0410, -2.5621, -0.8487, -1.0893, -2.2527,  0.6035,  0.7712]],
       dtype=torch.float64)
	q_value: tensor([[-39.7918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 18, step: 61
	action: tensor([[ 0.9535, -1.1870, -1.1506,  0.5520, -1.1450,  1.2266,  1.5031]],
       dtype=torch.float64)
	q_value: tensor([[-39.7918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3351097001627874, distance: 1.3222548506777299 entropy 1.2378510236740112
epoch: 18, step: 62
	action: tensor([[ 2.3603, -3.4681, -2.6310, -1.5987,  0.3197,  1.5088,  2.5770]],
       dtype=torch.float64)
	q_value: tensor([[-43.3138]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 18, step: 63
	action: tensor([[ 0.1180, -3.2648, -1.2830, -0.5509, -0.1428,  1.0746,  0.3409]],
       dtype=torch.float64)
	q_value: tensor([[-39.7918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 18, step: 64
	action: tensor([[ 0.1849, -0.8911, -0.3446, -0.5350, -0.3693, -0.5324,  2.2185]],
       dtype=torch.float64)
	q_value: tensor([[-39.7918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5343822896441717, distance: 1.41750135206548 entropy 1.2378510236740112
epoch: 18, step: 65
	action: tensor([[ 2.5529, -5.5895, -3.2272,  0.0789, -0.9191,  2.6053,  1.3017]],
       dtype=torch.float64)
	q_value: tensor([[-41.7084]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 18, step: 66
	action: tensor([[ 1.2796, -2.6943, -0.0685, -1.2043, -0.4100,  1.2596,  1.4893]],
       dtype=torch.float64)
	q_value: tensor([[-39.7918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 18, step: 67
	action: tensor([[ 1.3164, -3.5744, -0.5776, -1.0516, -0.6774,  1.3992,  2.1619]],
       dtype=torch.float64)
	q_value: tensor([[-39.7918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 18, step: 68
	action: tensor([[ 1.7499e-01, -2.7803e+00,  4.8121e-01, -2.7297e-01, -2.4883e-03,
          1.2885e+00,  1.0790e+00]], dtype=torch.float64)
	q_value: tensor([[-39.7918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 18, step: 69
	action: tensor([[ 1.0474, -1.8106, -0.0821, -1.0324, -0.4520,  1.0298,  1.3751]],
       dtype=torch.float64)
	q_value: tensor([[-39.7918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 18, step: 70
	action: tensor([[-0.9998, -0.4303, -0.0898, -0.0992, -0.6568,  0.2348,  1.1801]],
       dtype=torch.float64)
	q_value: tensor([[-39.7918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.3367286342048756, distance: 1.7492860527702354 entropy 1.2378510236740112
epoch: 18, step: 71
	action: tensor([[ 1.3620, -0.8896, -2.5024, -0.7597, -2.0643,  1.7118,  1.1516]],
       dtype=torch.float64)
	q_value: tensor([[-30.5826]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.015284925682681472, distance: 1.153056696434476 entropy 1.2378510236740112
epoch: 18, step: 72
	action: tensor([[ 2.0591, -4.4632, -3.2328, -1.1419, -0.0829,  0.7516,  1.2061]],
       dtype=torch.float64)
	q_value: tensor([[-55.7758]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 18, step: 73
	action: tensor([[ 0.5246, -3.8081, -0.8377,  1.0791,  0.1920,  1.4966,  0.6457]],
       dtype=torch.float64)
	q_value: tensor([[-39.7918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 18, step: 74
	action: tensor([[ 0.2588, -2.5447,  0.2385, -0.8538, -1.1446, -0.5024,  0.5230]],
       dtype=torch.float64)
	q_value: tensor([[-39.7918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 18, step: 75
	action: tensor([[ 1.1671, -2.9361, -2.1635, -0.7928, -0.4940, -0.5072,  1.6664]],
       dtype=torch.float64)
	q_value: tensor([[-39.7918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 18, step: 76
	action: tensor([[ 0.1205, -1.7493, -2.2629, -1.8731, -0.5632,  0.1180,  1.0125]],
       dtype=torch.float64)
	q_value: tensor([[-39.7918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7692182526363055, distance: 0.5497401060852669 entropy 1.2378510236740112
epoch: 18, step: 77
	action: tensor([[ 1.6151, -3.7069, -2.1870, -1.1704,  0.0839,  0.6342,  1.7981]],
       dtype=torch.float64)
	q_value: tensor([[-48.6935]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 18, step: 78
	action: tensor([[ 0.1260, -2.4731, -0.1487, -0.8954, -2.5208,  0.6237,  0.1276]],
       dtype=torch.float64)
	q_value: tensor([[-39.7918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 18, step: 79
	action: tensor([[ 2.0019, -1.9061, -1.1118, -0.4784, -0.0393,  2.0127,  1.2177]],
       dtype=torch.float64)
	q_value: tensor([[-39.7918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 18, step: 80
	action: tensor([[ 2.2418, -1.8539, -0.7193, -0.9353, -1.3975, -0.0925,  0.7273]],
       dtype=torch.float64)
	q_value: tensor([[-39.7918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 18, step: 81
	action: tensor([[-0.2996, -2.5830, -1.2278, -1.3579, -1.1296,  0.9328,  0.3422]],
       dtype=torch.float64)
	q_value: tensor([[-39.7918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 18, step: 82
	action: tensor([[-0.1550, -2.5979, -0.3031, -0.4820,  1.2137,  0.6468,  1.2696]],
       dtype=torch.float64)
	q_value: tensor([[-39.7918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 18, step: 83
	action: tensor([[-0.0732, -0.5665,  0.7730, -0.2723,  0.8902,  0.0657,  2.3016]],
       dtype=torch.float64)
	q_value: tensor([[-39.7918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.47243019022742394, distance: 1.3885900533335296 entropy 1.2378510236740112
epoch: 18, step: 84
	action: tensor([[ 3.3388, -4.5278, -0.2186, -0.5643, -0.3983,  1.6924,  1.5938]],
       dtype=torch.float64)
	q_value: tensor([[-43.1926]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 18, step: 85
	action: tensor([[ 0.6954, -1.9489, -1.5938, -0.3418, -0.2169,  2.0200,  0.3186]],
       dtype=torch.float64)
	q_value: tensor([[-39.7918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 18, step: 86
	action: tensor([[-0.4889, -1.6694, -0.1768, -1.4792, -0.3847,  0.7103,  0.6343]],
       dtype=torch.float64)
	q_value: tensor([[-39.7918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7042026463080162, distance: 1.4938854966398951 entropy 1.2378510236740112
epoch: 18, step: 87
	action: tensor([[ 0.4348, -2.6440, -0.4580,  0.7610,  0.2550,  1.5589,  2.1032]],
       dtype=torch.float64)
	q_value: tensor([[-38.0508]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 18, step: 88
	action: tensor([[ 1.9009, -2.8066,  0.1550, -1.7235, -1.1536,  2.3152, -0.4531]],
       dtype=torch.float64)
	q_value: tensor([[-39.7918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 18, step: 89
	action: tensor([[ 1.1243, -1.7585, -2.2591, -1.1045,  0.5295, -0.7417,  0.0670]],
       dtype=torch.float64)
	q_value: tensor([[-39.7918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 18, step: 90
	action: tensor([[-0.1354, -2.1911, -1.1268, -1.4253, -0.5177,  1.8923, -0.0921]],
       dtype=torch.float64)
	q_value: tensor([[-39.7918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 18, step: 91
	action: tensor([[ 0.0786, -1.5732, -2.7837, -2.7956, -0.2350,  0.5134,  0.6849]],
       dtype=torch.float64)
	q_value: tensor([[-39.7918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 18, step: 92
	action: tensor([[ 1.0982, -1.2946, -2.0494, -1.2982, -0.2123,  1.1390,  0.0578]],
       dtype=torch.float64)
	q_value: tensor([[-39.7918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 18, step: 93
	action: tensor([[ 0.4825, -2.2967,  0.8090, -0.8924,  1.1380,  0.8607, -0.7310]],
       dtype=torch.float64)
	q_value: tensor([[-39.7918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 18, step: 94
	action: tensor([[ 1.7630, -2.1191, -0.8280, -1.3836, -0.8570,  1.4403,  1.5322]],
       dtype=torch.float64)
	q_value: tensor([[-39.7918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 18, step: 95
	action: tensor([[ 1.5362, -1.2110, -1.1522, -0.6640, -1.3806, -0.6576,  0.7915]],
       dtype=torch.float64)
	q_value: tensor([[-39.7918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 18, step: 96
	action: tensor([[ 0.3951, -2.4390,  0.0645, -0.1756, -1.4527,  1.0097,  0.7017]],
       dtype=torch.float64)
	q_value: tensor([[-39.7918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 18, step: 97
	action: tensor([[ 1.5396, -2.2135, -1.0736,  0.1092, -0.0393,  0.2744,  2.2057]],
       dtype=torch.float64)
	q_value: tensor([[-39.7918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 18, step: 98
	action: tensor([[-0.6689, -2.3096,  0.5699, -0.7632, -1.4478,  0.6776,  1.2859]],
       dtype=torch.float64)
	q_value: tensor([[-39.7918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 18, step: 99
	action: tensor([[ 1.6192, -1.2397, -0.4537,  1.0387, -0.1616,  0.9591,  2.3672]],
       dtype=torch.float64)
	q_value: tensor([[-39.7918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5641903110847342, distance: 0.7554489708481426 entropy 1.2378510236740112
epoch: 18, step: 100
	action: tensor([[ 2.4542, -3.5905, -1.5513, -0.4665,  0.3028,  2.8588,  2.9810]],
       dtype=torch.float64)
	q_value: tensor([[-54.0328]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 18, step: 101
	action: tensor([[ 0.8832, -2.2814,  0.2387,  1.2307, -0.9010,  0.7175,  0.2393]],
       dtype=torch.float64)
	q_value: tensor([[-39.7918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 18, step: 102
	action: tensor([[-0.0533, -0.7487, -0.4309, -0.0595, -0.8248, -0.1334,  0.3701]],
       dtype=torch.float64)
	q_value: tensor([[-39.7918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.37929937723168083, distance: 1.3439588247876513 entropy 1.2378510236740112
epoch: 18, step: 103
	action: tensor([[-1.2202e-03, -1.5542e+00, -4.5628e-01, -3.0126e-01,  4.7504e-01,
          7.8844e-01, -8.5344e-01]], dtype=torch.float64)
	q_value: tensor([[-25.9889]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 18, step: 104
	action: tensor([[ 1.1089, -1.4272, -1.2749, -0.6335,  0.8127,  1.3438,  1.5879]],
       dtype=torch.float64)
	q_value: tensor([[-39.7918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1405388285566307, distance: 1.2221140325269853 entropy 1.2378510236740112
epoch: 18, step: 105
	action: tensor([[ 2.6676, -3.2721, -1.5374,  0.0928, -1.4408,  1.9417,  2.3688]],
       dtype=torch.float64)
	q_value: tensor([[-49.0901]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 18, step: 106
	action: tensor([[ 0.9091, -1.1309,  0.0729, -0.3597, -0.4011,  0.1060,  1.2266]],
       dtype=torch.float64)
	q_value: tensor([[-39.7918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7435113319674416, distance: 1.5110160605946137 entropy 1.2378510236740112
epoch: 18, step: 107
	action: tensor([[ 0.8493, -3.8883, -3.1719,  0.6159,  0.1486,  0.8333,  0.6857]],
       dtype=torch.float64)
	q_value: tensor([[-35.2480]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 18, step: 108
	action: tensor([[ 0.7211, -1.8232, -0.7518, -0.9058,  1.0790,  2.0696,  1.6962]],
       dtype=torch.float64)
	q_value: tensor([[-39.7918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 18, step: 109
	action: tensor([[ 0.2566, -0.4079,  0.2885, -0.6147, -0.7901,  0.6910,  1.3838]],
       dtype=torch.float64)
	q_value: tensor([[-39.7918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1101612828021461, distance: 1.2057290699859138 entropy 1.2378510236740112
epoch: 18, step: 110
	action: tensor([[ 1.8929, -2.7345, -1.1778,  0.4018, -1.2889,  0.3688,  1.0065]],
       dtype=torch.float64)
	q_value: tensor([[-34.6231]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 18, step: 111
	action: tensor([[ 0.7707, -0.3073, -0.9322,  2.2332, -0.3640,  2.0757,  1.1878]],
       dtype=torch.float64)
	q_value: tensor([[-39.7918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 18, step: 112
	action: tensor([[ 1.1440, -2.2285, -1.8701, -1.0410,  0.0937,  0.7611,  0.4331]],
       dtype=torch.float64)
	q_value: tensor([[-39.7918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 18, step: 113
	action: tensor([[ 1.6590, -1.9996, -0.9904, -0.5645, -1.1813,  1.1338,  1.2766]],
       dtype=torch.float64)
	q_value: tensor([[-39.7918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 18, step: 114
	action: tensor([[ 0.8346, -2.7496,  0.5302, -0.9678,  0.1063, -0.8429,  1.2849]],
       dtype=torch.float64)
	q_value: tensor([[-39.7918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 18, step: 115
	action: tensor([[ 1.7050, -1.7549, -1.9273, -1.2066,  0.2371,  1.2863, -0.0725]],
       dtype=torch.float64)
	q_value: tensor([[-39.7918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 18, step: 116
	action: tensor([[ 1.6094, -2.2260, -0.5941,  1.8578, -1.5670,  0.3959,  0.3476]],
       dtype=torch.float64)
	q_value: tensor([[-39.7918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 18, step: 117
	action: tensor([[ 0.8924, -1.6285, -0.6879,  0.6579, -0.1840,  0.2651,  1.7740]],
       dtype=torch.float64)
	q_value: tensor([[-39.7918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04826109107188392, distance: 1.1716325437900026 entropy 1.2378510236740112
epoch: 18, step: 118
	action: tensor([[ 2.1379, -2.0122, -2.2117, -1.4496, -0.8382,  1.3956,  1.3922]],
       dtype=torch.float64)
	q_value: tensor([[-41.8843]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 18, step: 119
	action: tensor([[ 0.6261, -1.1313, -0.5871, -1.6766,  0.4071,  0.9321,  1.5123]],
       dtype=torch.float64)
	q_value: tensor([[-39.7918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4887633572836696, distance: 1.3962703917019468 entropy 1.2378510236740112
epoch: 18, step: 120
	action: tensor([[ 1.6849, -4.0313, -3.0817,  0.0569, -0.9278,  0.6770,  1.0092]],
       dtype=torch.float64)
	q_value: tensor([[-46.1781]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 18, step: 121
	action: tensor([[ 0.4791, -2.0607, -1.2648, -2.0176, -0.6308,  1.1461,  0.8375]],
       dtype=torch.float64)
	q_value: tensor([[-39.7918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 18, step: 122
	action: tensor([[ 0.6970, -2.3426,  0.6410, -0.0550,  1.0774,  1.9194,  1.4518]],
       dtype=torch.float64)
	q_value: tensor([[-39.7918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 18, step: 123
	action: tensor([[ 2.0252, -2.6615, -0.2866,  0.2972,  0.9292,  1.8135,  0.5264]],
       dtype=torch.float64)
	q_value: tensor([[-39.7918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 18, step: 124
	action: tensor([[ 0.4878, -2.0716, -1.0191, -1.1478, -0.2643,  0.6180,  1.2386]],
       dtype=torch.float64)
	q_value: tensor([[-39.7918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 18, step: 125
	action: tensor([[ 1.7694, -0.8692, -0.2949,  1.7049,  1.0480,  1.2530, -0.3879]],
       dtype=torch.float64)
	q_value: tensor([[-39.7918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 18, step: 126
	action: tensor([[ 0.2860, -1.3868, -1.2329, -1.1916, -0.0555,  0.2129, -0.1790]],
       dtype=torch.float64)
	q_value: tensor([[-39.7918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23597929409336027, distance: 1.2722202116372316 entropy 1.2378510236740112
epoch: 18, step: 127
	action: tensor([[ 1.2317, -3.1653, -0.1825, -0.8344,  0.6182,  0.9850,  1.0345]],
       dtype=torch.float64)
	q_value: tensor([[-35.2443]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
LOSS epoch 18 actor 195.5907312285272 critic 109.39704216198984 
epoch: 19, step: 0
	action: tensor([[-0.3503, -1.0031, -1.2843, -0.3220, -0.3706,  0.2919, -1.2182]],
       dtype=torch.float64)
	q_value: tensor([[-44.8126]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.799802844063956, distance: 1.5352148411893676 entropy 1.2378510236740112
epoch: 19, step: 1
	action: tensor([[ 0.9064, -0.9548, -2.0602,  0.7657,  1.0312,  0.5727,  0.6762]],
       dtype=torch.float64)
	q_value: tensor([[-39.5404]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21596625529265245, distance: 1.2618782496126737 entropy 1.2378510236740112
epoch: 19, step: 2
	action: tensor([[ 1.2220, -2.4378,  0.5953,  0.1964, -0.1377,  1.1712,  0.3687]],
       dtype=torch.float64)
	q_value: tensor([[-46.5716]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 19, step: 3
	action: tensor([[-0.5595, -1.5082, -0.3596, -1.3429, -0.3458,  0.0445, -0.6312]],
       dtype=torch.float64)
	q_value: tensor([[-44.8126]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4317339949338106, distance: 1.3692661176935605 entropy 1.2378510236740112
epoch: 19, step: 4
	action: tensor([[ 0.5453,  0.0117, -1.5943,  0.0656,  0.2273,  0.9574,  0.4727]],
       dtype=torch.float64)
	q_value: tensor([[-41.5706]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.636998144206582, distance: 0.6894632400057299 entropy 1.2378510236740112
epoch: 19, step: 5
	action: tensor([[ 0.6708, -1.4598, -1.3789, -0.7008, -0.4203,  0.2747,  0.9051]],
       dtype=torch.float64)
	q_value: tensor([[-34.7259]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.577852028740059, distance: 1.437440343040889 entropy 1.2378510236740112
epoch: 19, step: 6
	action: tensor([[ 0.3696, -0.8733, -0.4806,  1.1587, -1.2178,  1.8563,  2.4841]],
       dtype=torch.float64)
	q_value: tensor([[-41.9530]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1555595154088636, distance: 1.0515773334979037 entropy 1.2378510236740112
epoch: 19, step: 7
	action: tensor([[ 0.5785,  0.1052, -0.9285,  1.7779,  0.6637,  1.0805,  0.9849]],
       dtype=torch.float64)
	q_value: tensor([[-60.9563]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 19, step: 8
	action: tensor([[ 0.4086, -1.6191, -1.6621,  0.5742,  0.0455,  0.0343,  1.4634]],
       dtype=torch.float64)
	q_value: tensor([[-44.8126]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8090397984838982, distance: 1.5391493169925963 entropy 1.2378510236740112
epoch: 19, step: 9
	action: tensor([[ 0.0568, -2.7393, -0.4033,  0.1723,  0.2379, -0.1413,  1.1706]],
       dtype=torch.float64)
	q_value: tensor([[-46.4648]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 19, step: 10
	action: tensor([[ 0.1668, -1.1895, -0.2807,  0.2984,  1.0270,  0.2080,  2.1226]],
       dtype=torch.float64)
	q_value: tensor([[-44.8126]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4578858739291469, distance: 1.3817149518919007 entropy 1.2378510236740112
epoch: 19, step: 11
	action: tensor([[ 0.5562, -1.4367, -0.4320,  0.0761, -0.9678,  0.7846,  1.4957]],
       dtype=torch.float64)
	q_value: tensor([[-49.7718]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 19, step: 12
	action: tensor([[ 0.5205, -0.8411, -0.1851,  1.6675,  0.4869,  0.2165, -0.4198]],
       dtype=torch.float64)
	q_value: tensor([[-44.8126]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8424298638770837, distance: 0.4542486532254164 entropy 1.2378510236740112
epoch: 19, step: 13
	action: tensor([[-0.7652, -0.8062,  0.2772,  0.1998, -1.0631,  0.7553,  1.0263]],
       dtype=torch.float64)
	q_value: tensor([[-43.4557]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9990543868390549, distance: 1.6179645362036146 entropy 1.2378510236740112
epoch: 19, step: 14
	action: tensor([[ 0.2226, -0.1789, -0.7768,  1.4559, -0.6097,  0.1746, -0.3636]],
       dtype=torch.float64)
	q_value: tensor([[-39.6138]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 19, step: 15
	action: tensor([[ 0.4348, -1.7065, -0.1471,  0.5984, -0.9244,  0.8482,  2.1922]],
       dtype=torch.float64)
	q_value: tensor([[-44.8126]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07213474339478188, distance: 1.1848991235126645 entropy 1.2378510236740112
epoch: 19, step: 16
	action: tensor([[ 0.9895, -0.8348, -1.7315, -0.6419, -0.6860,  2.2071,  0.0470]],
       dtype=torch.float64)
	q_value: tensor([[-52.1077]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0354927500532386, distance: 1.1238528247712927 entropy 1.2378510236740112
epoch: 19, step: 17
	action: tensor([[-0.0858, -1.4875, -0.1215,  0.3374, -0.2737,  1.3870,  1.7492]],
       dtype=torch.float64)
	q_value: tensor([[-53.3455]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 19, step: 18
	action: tensor([[-1.0014,  0.8000, -0.6068,  0.4770, -0.0340, -0.9839,  0.0283]],
       dtype=torch.float64)
	q_value: tensor([[-44.8126]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 19, step: 19
	action: tensor([[-0.0264, -1.3557,  1.3782,  0.1185,  0.8397,  0.7559,  0.3537]],
       dtype=torch.float64)
	q_value: tensor([[-44.8126]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5255549098831158, distance: 1.4134179917734406 entropy 1.2378510236740112
epoch: 19, step: 20
	action: tensor([[-0.2579, -1.6288,  0.5756,  0.4098,  0.2355, -1.9466,  0.0696]],
       dtype=torch.float64)
	q_value: tensor([[-44.8725]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 19, step: 21
	action: tensor([[ 1.0910, -0.5763, -0.4723, -0.4034, -1.4155,  1.8270,  1.5626]],
       dtype=torch.float64)
	q_value: tensor([[-44.8126]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19084376987614038, distance: 1.0293732648248946 entropy 1.2378510236740112
epoch: 19, step: 22
	action: tensor([[ 1.2142, -0.6465, -2.1453, -1.7413, -0.8696, -0.3954, -0.5104]],
       dtype=torch.float64)
	q_value: tensor([[-55.7295]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4680222668475724, distance: 0.834648122000165 entropy 1.2378510236740112
epoch: 19, step: 23
	action: tensor([[ 0.0777, -2.3304, -0.4329, -0.5060, -0.3867,  0.7418,  1.1500]],
       dtype=torch.float64)
	q_value: tensor([[-49.9039]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 19, step: 24
	action: tensor([[ 0.4545, -0.4763, -0.8883, -1.3973,  0.9113, -0.6867,  0.3149]],
       dtype=torch.float64)
	q_value: tensor([[-44.8126]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14150782233926518, distance: 1.222633072058957 entropy 1.2378510236740112
epoch: 19, step: 25
	action: tensor([[ 1.1034, -1.0383,  0.1007, -0.4153,  1.2722,  0.6142,  0.8764]],
       dtype=torch.float64)
	q_value: tensor([[-41.9065]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6802857069388928, distance: 1.4833657931952389 entropy 1.2378510236740112
epoch: 19, step: 26
	action: tensor([[ 0.1659, -0.4848, -0.9742, -0.3777,  0.2954, -0.1275,  0.2566]],
       dtype=torch.float64)
	q_value: tensor([[-46.6480]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.053926465375293886, distance: 1.1130614463416282 entropy 1.2378510236740112
epoch: 19, step: 27
	action: tensor([[-0.0364, -1.3151,  0.3025,  0.8137,  0.1111,  0.0754, -1.3357]],
       dtype=torch.float64)
	q_value: tensor([[-29.2421]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0824161621549131, distance: 1.1905669637504193 entropy 1.2378510236740112
epoch: 19, step: 28
	action: tensor([[-0.2979, -0.1853, -0.1997, -1.4523, -0.5053,  0.1975,  0.9626]],
       dtype=torch.float64)
	q_value: tensor([[-46.6158]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.37311552170705076, distance: 1.3409427343904634 entropy 1.2378510236740112
epoch: 19, step: 29
	action: tensor([[ 0.4989, -0.9644, -0.8862, -0.1666, -0.9678, -0.1031,  0.5550]],
       dtype=torch.float64)
	q_value: tensor([[-37.3406]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.39362858853043825, distance: 1.3509218206652474 entropy 1.2378510236740112
epoch: 19, step: 30
	action: tensor([[ 0.1744, -0.4539, -0.2954,  1.0657, -0.8484,  0.2109,  0.1657]],
       dtype=torch.float64)
	q_value: tensor([[-35.9758]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4764326848966155, distance: 0.8280240607576318 entropy 1.2378510236740112
epoch: 19, step: 31
	action: tensor([[ 1.5612, -0.8560, -1.2174, -0.0961,  0.3388, -0.3487, -0.7514]],
       dtype=torch.float64)
	q_value: tensor([[-34.2643]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5702839296358349, distance: 1.4339888891195172 entropy 1.2378510236740112
epoch: 19, step: 32
	action: tensor([[-0.7082, -1.4274,  0.6870, -0.2350, -1.0790,  1.1597,  0.5236]],
       dtype=torch.float64)
	q_value: tensor([[-40.8020]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 19, step: 33
	action: tensor([[ 0.2925, -1.1045,  0.1004, -0.0801, -0.8764,  1.2552,  0.9383]],
       dtype=torch.float64)
	q_value: tensor([[-44.8126]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.26016000006836926, distance: 1.2846047937743912 entropy 1.2378510236740112
epoch: 19, step: 34
	action: tensor([[ 1.7485, -1.5717, -0.1800,  0.1958, -0.8044,  0.4399,  1.5948]],
       dtype=torch.float64)
	q_value: tensor([[-42.7677]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 19, step: 35
	action: tensor([[ 1.1259, -1.0497, -0.9537,  0.6793, -0.2805,  2.8550,  0.1768]],
       dtype=torch.float64)
	q_value: tensor([[-44.8126]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 19, step: 36
	action: tensor([[ 0.3537, -0.4119, -1.1042,  0.1034,  0.2697,  0.6355,  2.1067]],
       dtype=torch.float64)
	q_value: tensor([[-44.8126]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29180137969270525, distance: 0.9630176158052414 entropy 1.2378510236740112
epoch: 19, step: 37
	action: tensor([[-0.0473, -1.7182,  0.2189, -0.1116, -1.0787,  0.0479,  2.0958]],
       dtype=torch.float64)
	q_value: tensor([[-46.6426]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 19, step: 38
	action: tensor([[ 0.2895, -0.9715, -0.9695, -0.3415, -0.3424,  0.5557,  0.7334]],
       dtype=torch.float64)
	q_value: tensor([[-44.8126]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4620583933004294, distance: 1.3836907969579244 entropy 1.2378510236740112
epoch: 19, step: 39
	action: tensor([[ 0.4946, -0.9940, -0.1394,  0.3895, -1.6337,  0.5841,  2.3534]],
       dtype=torch.float64)
	q_value: tensor([[-34.9633]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18410941642044532, distance: 1.2452387036437464 entropy 1.2378510236740112
epoch: 19, step: 40
	action: tensor([[ 1.4637, -0.1246,  0.4300, -1.4723,  0.6159,  1.1792,  1.6341]],
       dtype=torch.float64)
	q_value: tensor([[-56.6697]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03938622947709802, distance: 1.121582171704949 entropy 1.2378510236740112
epoch: 19, step: 41
	action: tensor([[ 1.3263, -2.6177, -0.6836, -0.0846,  0.3575,  1.2664,  0.8051]],
       dtype=torch.float64)
	q_value: tensor([[-54.4041]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 19, step: 42
	action: tensor([[-1.0196,  0.4455,  1.4826,  1.3374, -0.3389,  0.8240,  0.7763]],
       dtype=torch.float64)
	q_value: tensor([[-44.8126]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 19, step: 43
	action: tensor([[ 0.9431, -0.2220,  0.0184, -0.5575, -0.8855, -0.3636,  0.4673]],
       dtype=torch.float64)
	q_value: tensor([[-44.8126]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13956853047582152, distance: 1.0614873825103537 entropy 1.2378510236740112
epoch: 19, step: 44
	action: tensor([[-0.9821, -1.0766,  0.2700,  0.0803,  0.0692,  2.1511, -1.3349]],
       dtype=torch.float64)
	q_value: tensor([[-33.3204]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3675618425354763, distance: 1.3382282099923046 entropy 1.2378510236740112
epoch: 19, step: 45
	action: tensor([[ 0.7179, -0.6057, -0.4768, -0.8736,  0.3520,  0.0397, -0.9014]],
       dtype=torch.float64)
	q_value: tensor([[-55.1989]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3053272685691095, distance: 1.3074238364015482 entropy 1.2378510236740112
epoch: 19, step: 46
	action: tensor([[ 0.1532,  0.6403, -0.7264, -0.9544,  0.8508, -1.1081, -0.2548]],
       dtype=torch.float64)
	q_value: tensor([[-35.2160]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6046354477928347, distance: 0.7195410200871856 entropy 1.2378510236740112
epoch: 19, step: 47
	action: tensor([[ 0.1629, -1.0477,  0.1089, -0.7081,  0.6090,  0.3231,  0.3877]],
       dtype=torch.float64)
	q_value: tensor([[-36.5309]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8371129073692378, distance: 1.5510457837951164 entropy 1.2378510236740112
epoch: 19, step: 48
	action: tensor([[ 0.6486, -0.7898, -0.0360, -0.0376, -1.3760,  1.2745,  0.9073]],
       dtype=torch.float64)
	q_value: tensor([[-34.8644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04527511615991975, distance: 1.1181390480555655 entropy 1.2378510236740112
epoch: 19, step: 49
	action: tensor([[-0.1919, -2.1706, -0.9451, -0.0777, -0.2366,  0.0208,  1.7465]],
       dtype=torch.float64)
	q_value: tensor([[-44.9300]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 19, step: 50
	action: tensor([[-0.0597, -1.3058, -0.6660,  0.8304, -0.2370,  0.5983,  0.7299]],
       dtype=torch.float64)
	q_value: tensor([[-44.8126]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4709967975028797, distance: 1.3879140010713544 entropy 1.2378510236740112
epoch: 19, step: 51
	action: tensor([[-1.1085, -1.9275, -0.3279,  0.2792, -0.3246,  1.0826, -0.2294]],
       dtype=torch.float64)
	q_value: tensor([[-36.0018]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 19, step: 52
	action: tensor([[ 0.5819, -0.7188,  0.4695,  0.7628,  0.2436,  1.2137, -0.4939]],
       dtype=torch.float64)
	q_value: tensor([[-44.8126]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8077369730279811, distance: 0.5017698717855706 entropy 1.2378510236740112
epoch: 19, step: 53
	action: tensor([[-0.3776, -0.2436,  1.0377,  1.1844, -0.6165,  1.1225,  0.1412]],
       dtype=torch.float64)
	q_value: tensor([[-40.7801]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 19, step: 54
	action: tensor([[ 0.2868, -1.0850, -0.3766, -0.6869, -0.1326,  0.2817, -0.3193]],
       dtype=torch.float64)
	q_value: tensor([[-44.8126]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7373202870029132, distance: 1.5083309367716295 entropy 1.2378510236740112
epoch: 19, step: 55
	action: tensor([[ 0.9055, -0.5280, -0.4972,  0.4693,  0.3634,  1.9150, -0.2016]],
       dtype=torch.float64)
	q_value: tensor([[-33.6716]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8483655200286182, distance: 0.4456107675160617 entropy 1.2378510236740112
epoch: 19, step: 56
	action: tensor([[ 0.7403, -1.6413, -1.7358, -0.0601, -0.5383,  0.0657,  0.9906]],
       dtype=torch.float64)
	q_value: tensor([[-42.2898]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 19, step: 57
	action: tensor([[ 1.3886, -1.6479, -0.7440,  0.7077,  0.6860, -0.2100,  0.8817]],
       dtype=torch.float64)
	q_value: tensor([[-44.8126]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17692134434188556, distance: 1.241453373185259 entropy 1.2378510236740112
epoch: 19, step: 58
	action: tensor([[ 0.7165, -1.6559, -2.0255, -0.1809, -0.6318,  0.3995,  1.5598]],
       dtype=torch.float64)
	q_value: tensor([[-46.7582]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 19, step: 59
	action: tensor([[-0.9085, -2.0443, -0.0320, -0.0324, -0.1518,  0.3700, -0.8243]],
       dtype=torch.float64)
	q_value: tensor([[-44.8126]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 19, step: 60
	action: tensor([[-1.0275, -1.5812, -0.2193, -0.4329, -0.7617,  0.2882,  1.2655]],
       dtype=torch.float64)
	q_value: tensor([[-44.8126]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2379491050568352, distance: 1.7119134186177323 entropy 1.2378510236740112
epoch: 19, step: 61
	action: tensor([[ 1.0835, -0.0675, -1.0661, -0.1268, -1.6276,  1.7316,  0.8743]],
       dtype=torch.float64)
	q_value: tensor([[-41.8905]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4563173162911368, distance: 0.8437804208194172 entropy 1.2378510236740112
epoch: 19, step: 62
	action: tensor([[ 0.5001, -3.3380, -0.2223, -0.1812, -0.2344,  0.9727,  0.7668]],
       dtype=torch.float64)
	q_value: tensor([[-49.5529]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 19, step: 63
	action: tensor([[ 1.3092, -0.2671,  0.6770, -0.8224, -0.2100,  0.8221,  2.0897]],
       dtype=torch.float64)
	q_value: tensor([[-44.8126]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34793415176579046, distance: 0.9240647861227999 entropy 1.2378510236740112
epoch: 19, step: 64
	action: tensor([[ 0.7511, -0.8055, -0.4350,  0.7219, -1.6116,  0.7784,  2.2438]],
       dtype=torch.float64)
	q_value: tensor([[-54.3527]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26841460335769274, distance: 0.978789286774468 entropy 1.2378510236740112
epoch: 19, step: 65
	action: tensor([[ 0.2065, -2.5586,  0.1927, -0.6192,  0.8658,  0.7044,  0.8333]],
       dtype=torch.float64)
	q_value: tensor([[-57.1695]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 19, step: 66
	action: tensor([[ 0.7442, -1.1008,  0.0730,  0.1737,  0.6719,  0.9305,  0.0706]],
       dtype=torch.float64)
	q_value: tensor([[-44.8126]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07255744092089211, distance: 1.1020472212958425 entropy 1.2378510236740112
epoch: 19, step: 67
	action: tensor([[-0.9156, -2.1728,  0.1439,  0.5845, -0.2190,  2.2955,  0.3387]],
       dtype=torch.float64)
	q_value: tensor([[-39.8379]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 19, step: 68
	action: tensor([[-0.2461, -1.5067, -0.5194, -0.5711, -1.0312,  0.8583,  0.1051]],
       dtype=torch.float64)
	q_value: tensor([[-44.8126]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9972390550033645, distance: 1.6172297363764634 entropy 1.2378510236740112
epoch: 19, step: 69
	action: tensor([[ 0.4791, -1.5111,  1.3538, -0.1857,  0.6026, -0.1903,  0.5147]],
       dtype=torch.float64)
	q_value: tensor([[-40.1389]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 19, step: 70
	action: tensor([[ 0.4972, -0.3563, -0.6891,  0.7879,  0.3063, -1.2451,  0.3880]],
       dtype=torch.float64)
	q_value: tensor([[-44.8126]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41931410545186554, distance: 0.872021778845041 entropy 1.2378510236740112
epoch: 19, step: 71
	action: tensor([[ 1.6651, -0.3689,  0.7854, -0.1859, -1.6558, -0.1500,  1.5254]],
       dtype=torch.float64)
	q_value: tensor([[-38.9928]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.013574101841884567, distance: 1.136550994290136 entropy 1.2378510236740112
epoch: 19, step: 72
	action: tensor([[ 0.9191, -2.5103,  0.2138, -0.3588, -0.2442,  1.0856,  1.0416]],
       dtype=torch.float64)
	q_value: tensor([[-55.1619]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 19, step: 73
	action: tensor([[ 0.7605,  0.0166, -0.1105, -0.2727, -1.0960,  0.0017, -0.6181]],
       dtype=torch.float64)
	q_value: tensor([[-44.8126]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 19, step: 74
	action: tensor([[ 0.6261, -1.9394, -0.8414, -0.1713, -0.2992,  0.2743, -0.1191]],
       dtype=torch.float64)
	q_value: tensor([[-44.8126]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 19, step: 75
	action: tensor([[ 0.4138, -1.6074, -0.2067,  0.8456, -0.9234,  0.6190,  0.1853]],
       dtype=torch.float64)
	q_value: tensor([[-44.8126]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.030100041262297705, distance: 1.1269902626472892 entropy 1.2378510236740112
epoch: 19, step: 76
	action: tensor([[ 1.6298, -1.0159, -1.1065,  1.4434,  0.7812, -0.5382,  1.0837]],
       dtype=torch.float64)
	q_value: tensor([[-40.9633]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4424862029454295, distance: 0.8544457678945804 entropy 1.2378510236740112
epoch: 19, step: 77
	action: tensor([[ 0.6981, -1.4838,  0.4841,  0.1803,  0.4653,  0.2976,  2.5864]],
       dtype=torch.float64)
	q_value: tensor([[-54.8362]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 19, step: 78
	action: tensor([[ 0.7149, -1.4157, -0.5222,  0.2317, -1.6332,  0.4802, -0.1441]],
       dtype=torch.float64)
	q_value: tensor([[-44.8126]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5135853754547886, distance: 1.4078622196789126 entropy 1.2378510236740112
epoch: 19, step: 79
	action: tensor([[ 0.8201, -0.6120, -0.3765, -1.2625, -0.9234,  1.1680,  0.8699]],
       dtype=torch.float64)
	q_value: tensor([[-44.8736]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.26500261754710785, distance: 1.287070704698706 entropy 1.2378510236740112
epoch: 19, step: 80
	action: tensor([[ 1.0327, -1.6732, -0.0792, -0.0524, -1.0115,  0.5042,  1.3439]],
       dtype=torch.float64)
	q_value: tensor([[-44.9506]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 19, step: 81
	action: tensor([[-1.5858, -0.3667,  0.6409,  0.3654, -0.7884, -0.2049,  1.5609]],
       dtype=torch.float64)
	q_value: tensor([[-44.8126]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.6130466773464143, distance: 1.8498234349390394 entropy 1.2378510236740112
epoch: 19, step: 82
	action: tensor([[ 0.7557, -0.8243,  0.1801,  0.3123,  1.1602,  1.2439, -0.4009]],
       dtype=torch.float64)
	q_value: tensor([[-42.3926]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35934852871447087, distance: 0.9159412268258038 entropy 1.2378510236740112
epoch: 19, step: 83
	action: tensor([[ 0.0439, -2.8747, -0.0867,  1.2507, -2.0203,  1.6196,  1.0569]],
       dtype=torch.float64)
	q_value: tensor([[-43.8479]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 19, step: 84
	action: tensor([[ 1.2931, -0.9385,  1.2832,  0.0706, -0.2149,  0.0809,  0.1200]],
       dtype=torch.float64)
	q_value: tensor([[-44.8126]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.34109304426882403, distance: 1.325214405853513 entropy 1.2378510236740112
epoch: 19, step: 85
	action: tensor([[ 1.1516, -1.9201,  0.5991,  2.4450, -1.0413,  0.1193,  1.4925]],
       dtype=torch.float64)
	q_value: tensor([[-46.2213]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 19, step: 86
	action: tensor([[ 0.0838, -0.7989, -0.6446,  0.2483, -0.6810, -1.2401, -0.2630]],
       dtype=torch.float64)
	q_value: tensor([[-44.8126]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04852646315829601, distance: 1.1162334946606414 entropy 1.2378510236740112
epoch: 19, step: 87
	action: tensor([[-0.3457, -1.8672,  0.4079, -0.2123, -0.8682, -0.1747,  1.2264]],
       dtype=torch.float64)
	q_value: tensor([[-38.1578]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 19, step: 88
	action: tensor([[-2.4952e-01,  2.6250e-01,  2.7465e-02, -2.2030e-04, -4.0455e-01,
          9.2931e-01,  1.0279e+00]], dtype=torch.float64)
	q_value: tensor([[-44.8126]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 19, step: 89
	action: tensor([[ 1.4135, -0.8981,  0.3806,  0.6956, -1.6064, -0.0729,  1.9615]],
       dtype=torch.float64)
	q_value: tensor([[-44.8126]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06279362025199742, distance: 1.1078330349082655 entropy 1.2378510236740112
epoch: 19, step: 90
	action: tensor([[-0.1239, -0.1912, -0.0585,  0.7726,  0.2567, -0.1191,  0.0603]],
       dtype=torch.float64)
	q_value: tensor([[-60.4087]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3574827619472406, distance: 0.9172740031927348 entropy 1.2378510236740112
epoch: 19, step: 91
	action: tensor([[ 0.1643, -2.2797,  1.5136,  1.1433,  0.7336, -0.1771,  0.4096]],
       dtype=torch.float64)
	q_value: tensor([[-26.0282]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 19, step: 92
	action: tensor([[ 0.7208, -1.3321, -0.1800,  0.3181,  0.9388, -1.2787,  1.6758]],
       dtype=torch.float64)
	q_value: tensor([[-44.8126]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.506140887435224, distance: 1.4043957047676257 entropy 1.2378510236740112
epoch: 19, step: 93
	action: tensor([[ 1.5255, -1.7143, -0.3656,  1.1305, -0.5093,  0.0930,  1.5247]],
       dtype=torch.float64)
	q_value: tensor([[-51.0780]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 19, step: 94
	action: tensor([[ 0.5976, -0.2910, -0.2666, -0.0749, -0.9429, -0.0915,  0.0573]],
       dtype=torch.float64)
	q_value: tensor([[-44.8126]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4310010241208183, distance: 0.8632019954596872 entropy 1.2378510236740112
epoch: 19, step: 95
	action: tensor([[-0.3124, -1.4467,  0.1415, -0.3143,  0.0304, -0.2279, -0.5483]],
       dtype=torch.float64)
	q_value: tensor([[-28.8096]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0678037769170206, distance: 1.645551032635267 entropy 1.2378510236740112
epoch: 19, step: 96
	action: tensor([[ 0.6994, -1.5660, -0.1542,  0.4618, -1.7962,  0.6434,  1.9873]],
       dtype=torch.float64)
	q_value: tensor([[-35.3942]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 19, step: 97
	action: tensor([[ 0.9701, -1.6532,  1.2549, -0.3039,  1.0262, -0.6797,  1.4389]],
       dtype=torch.float64)
	q_value: tensor([[-44.8126]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3586255704279111, distance: 1.333848747521361 entropy 1.2378510236740112
epoch: 19, step: 98
	action: tensor([[ 0.7220, -0.5088, -0.7168, -1.8058, -0.4142, -0.2063,  2.3638]],
       dtype=torch.float64)
	q_value: tensor([[-51.9227]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2259249514890651, distance: 1.2670350690685384 entropy 1.2378510236740112
epoch: 19, step: 99
	action: tensor([[ 0.2416, -2.2234, -0.3573,  0.8347, -0.3937,  1.2248,  0.9334]],
       dtype=torch.float64)
	q_value: tensor([[-55.7243]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 19, step: 100
	action: tensor([[-0.5768,  0.0250, -0.0950,  0.0498,  0.6614, -0.9648,  2.5698]],
       dtype=torch.float64)
	q_value: tensor([[-44.8126]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5549410452261978, distance: 1.4269661041026631 entropy 1.2378510236740112
epoch: 19, step: 101
	action: tensor([[ 0.8601, -1.6982, -0.0326, -0.0936, -0.2401,  0.4968,  0.4657]],
       dtype=torch.float64)
	q_value: tensor([[-50.8404]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 19, step: 102
	action: tensor([[ 1.3649, -1.3127, -0.1924,  1.4417, -0.7077, -0.0838,  0.1441]],
       dtype=torch.float64)
	q_value: tensor([[-44.8126]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5155366963874308, distance: 0.7965024874907546 entropy 1.2378510236740112
epoch: 19, step: 103
	action: tensor([[-0.6573, -1.4844, -2.0933, -1.0124,  0.4042,  1.9454,  0.3481]],
       dtype=torch.float64)
	q_value: tensor([[-51.9460]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 19, step: 104
	action: tensor([[ 0.2476, -0.1367, -2.7573,  1.2498, -0.4213,  0.4438,  0.9009]],
       dtype=torch.float64)
	q_value: tensor([[-44.8126]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3327088747603877, distance: 1.3210654609839478 entropy 1.2378510236740112
epoch: 19, step: 105
	action: tensor([[ 0.8553, -0.5711, -0.4035,  0.8320, -0.7203, -0.4391,  1.2621]],
       dtype=torch.float64)
	q_value: tensor([[-45.3458]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6657507246874594, distance: 0.6615945674526365 entropy 1.2378510236740112
epoch: 19, step: 106
	action: tensor([[-0.5659, -2.4647, -0.1807, -0.5341,  0.7878,  0.9429,  0.2030]],
       dtype=torch.float64)
	q_value: tensor([[-43.9600]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 19, step: 107
	action: tensor([[-0.9073, -0.0306, -0.5598,  0.5315, -0.3076,  1.4156,  1.5474]],
       dtype=torch.float64)
	q_value: tensor([[-44.8126]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 19, step: 108
	action: tensor([[-0.5396,  0.9502, -1.8401,  0.5329, -0.4922,  0.7992,  0.1625]],
       dtype=torch.float64)
	q_value: tensor([[-44.8126]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 19, step: 109
	action: tensor([[-0.7969, -1.0370,  0.1403, -0.7699, -1.2869,  0.7309,  1.1107]],
       dtype=torch.float64)
	q_value: tensor([[-44.8126]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.280227606621985, distance: 1.728008175812053 entropy 1.2378510236740112
epoch: 19, step: 110
	action: tensor([[ 2.2643, -1.7455, -0.0580,  0.0948, -0.2814,  0.1196,  0.7554]],
       dtype=torch.float64)
	q_value: tensor([[-46.2775]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 19, step: 111
	action: tensor([[-0.6012, -0.7005, -0.9107, -0.5812,  0.0911,  0.0590,  0.7408]],
       dtype=torch.float64)
	q_value: tensor([[-44.8126]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5701854154485806, distance: 1.433943906660289 entropy 1.2378510236740112
epoch: 19, step: 112
	action: tensor([[ 0.1998, -0.0247, -1.0813, -0.6356, -0.6065,  2.0089,  0.7933]],
       dtype=torch.float64)
	q_value: tensor([[-33.5783]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.40989864258548314, distance: 0.8790629984391368 entropy 1.2378510236740112
epoch: 19, step: 113
	action: tensor([[ 0.6144, -0.9027, -1.0039, -0.7955, -0.2140,  1.3989,  1.7475]],
       dtype=torch.float64)
	q_value: tensor([[-46.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13127464161409397, distance: 1.2171405159113673 entropy 1.2378510236740112
epoch: 19, step: 114
	action: tensor([[ 0.6445, -2.8773, -1.0352,  0.9368, -0.2153,  0.3348,  2.8827]],
       dtype=torch.float64)
	q_value: tensor([[-52.0373]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 19, step: 115
	action: tensor([[-0.5845, -0.4023, -0.0013, -0.3050,  0.7896,  1.2587,  0.8520]],
       dtype=torch.float64)
	q_value: tensor([[-44.8126]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12873852656284424, distance: 1.2157754449427352 entropy 1.2378510236740112
epoch: 19, step: 116
	action: tensor([[ 1.2886, -0.5084, -0.4674, -0.0984, -2.2991,  1.4589, -1.0207]],
       dtype=torch.float64)
	q_value: tensor([[-38.7825]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.018340648017875938, distance: 1.1338016833184308 entropy 1.2378510236740112
epoch: 19, step: 117
	action: tensor([[ 1.7067, -2.2449, -1.9682, -0.2386,  0.7538,  0.2512,  1.8290]],
       dtype=torch.float64)
	q_value: tensor([[-54.8292]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 19, step: 118
	action: tensor([[ 0.2665, -0.2743, -0.4741, -0.8813, -1.5298, -0.0808,  0.8252]],
       dtype=torch.float64)
	q_value: tensor([[-44.8126]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.060435728675173994, distance: 1.1092257427164733 entropy 1.2378510236740112
epoch: 19, step: 119
	action: tensor([[ 0.3694, -1.5071, -1.8927, -2.2943,  0.3872,  0.9474, -0.1365]],
       dtype=torch.float64)
	q_value: tensor([[-39.1726]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 19, step: 120
	action: tensor([[-0.1038, -0.8299,  0.0839, -0.7990,  0.6303,  1.2462, -0.9510]],
       dtype=torch.float64)
	q_value: tensor([[-44.8126]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.288696708548708, distance: 1.299068502546406 entropy 1.2378510236740112
epoch: 19, step: 121
	action: tensor([[ 1.9320, -0.1844, -0.3187, -1.1787,  0.2301,  0.1112, -1.0058]],
       dtype=torch.float64)
	q_value: tensor([[-42.8342]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4252612569080818, distance: 1.366167448284125 entropy 1.2378510236740112
epoch: 19, step: 122
	action: tensor([[ 0.2393,  1.3479, -0.0599, -0.4973, -0.3782,  0.1878,  1.1791]],
       dtype=torch.float64)
	q_value: tensor([[-38.8371]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 19, step: 123
	action: tensor([[ 0.6415, -0.5217, -0.6208,  0.3387,  0.1775, -0.1820,  0.4692]],
       dtype=torch.float64)
	q_value: tensor([[-44.8126]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33320390534388433, distance: 0.934443864982864 entropy 1.2378510236740112
epoch: 19, step: 124
	action: tensor([[-0.0209, -2.2108,  0.3813, -0.1456, -0.0735, -0.4334,  1.7681]],
       dtype=torch.float64)
	q_value: tensor([[-29.1631]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 19, step: 125
	action: tensor([[ 0.6123, -0.3385, -0.1393, -0.2062, -1.5691,  0.0296,  1.4501]],
       dtype=torch.float64)
	q_value: tensor([[-44.8126]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27391306233934287, distance: 0.9751041509297804 entropy 1.2378510236740112
epoch: 19, step: 126
	action: tensor([[-1.1057, -0.6994, -0.5146, -0.6251,  1.7847, -0.0582,  0.7735]],
       dtype=torch.float64)
	q_value: tensor([[-44.1366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1377598332399477, distance: 1.6731549001014967 entropy 1.2378510236740112
epoch: 19, step: 127
	action: tensor([[ 0.8987, -2.3365, -0.0914, -0.0312,  1.1360,  0.1426,  1.4076]],
       dtype=torch.float64)
	q_value: tensor([[-45.5096]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
LOSS epoch 19 actor 471.6040441111403 critic 61.068170395636216 
epoch: 20, step: 0
	action: tensor([[ 0.3939,  1.5231,  0.3628,  0.4602, -0.7554,  0.6019,  0.2002]],
       dtype=torch.float64)
	q_value: tensor([[-46.3676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 20, step: 1
	action: tensor([[-0.3842, -0.6820, -1.0877,  0.2247,  0.5959,  0.6911, -2.4225]],
       dtype=torch.float64)
	q_value: tensor([[-46.3676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6039998566698965, distance: 1.4493018809250817 entropy 1.2378510236740112
epoch: 20, step: 2
	action: tensor([[ 0.5564,  0.2660,  0.3126, -0.6962, -1.1437, -1.0107,  0.0033]],
       dtype=torch.float64)
	q_value: tensor([[-54.6531]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5633614973055798, distance: 0.7561669781877558 entropy 1.2378510236740112
epoch: 20, step: 3
	action: tensor([[ 9.2019e-01, -1.2862e+00, -4.7874e-01, -4.2280e-01, -6.7483e-01,
         -7.3776e-04,  2.0358e+00]], dtype=torch.float64)
	q_value: tensor([[-37.3013]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9041196254151087, distance: 1.579078818036257 entropy 1.2378510236740112
epoch: 20, step: 4
	action: tensor([[-0.4123, -0.0322,  0.6639,  0.0102,  0.6804,  0.2947,  0.6983]],
       dtype=torch.float64)
	q_value: tensor([[-53.9862]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09198375886355792, distance: 1.1958171643280362 entropy 1.2378510236740112
epoch: 20, step: 5
	action: tensor([[-0.0809, -0.3196, -0.5398,  0.4995,  0.3751,  0.4195,  0.4033]],
       dtype=torch.float64)
	q_value: tensor([[-31.4483]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16531615679428402, distance: 1.0454847366177873 entropy 1.2378510236740112
epoch: 20, step: 6
	action: tensor([[ 0.0465, -0.1494, -0.1127,  2.0707, -0.5678, -0.5506,  0.9919]],
       dtype=torch.float64)
	q_value: tensor([[-27.4764]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 20, step: 7
	action: tensor([[ 0.4845,  0.0132,  1.5818, -1.5075,  0.5356,  1.5011, -0.4729]],
       dtype=torch.float64)
	q_value: tensor([[-46.3676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 20, step: 8
	action: tensor([[ 0.2581, -1.3744,  1.0650,  0.3109, -0.0964, -0.7380, -0.3607]],
       dtype=torch.float64)
	q_value: tensor([[-46.3676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6991193343024253, distance: 1.491655844744667 entropy 1.2378510236740112
epoch: 20, step: 9
	action: tensor([[-0.6529, -1.3345, -0.6563, -1.2791,  0.3822, -1.5411, -1.4827]],
       dtype=torch.float64)
	q_value: tensor([[-47.8121]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 20, step: 10
	action: tensor([[ 0.5646,  0.5160, -0.2086,  0.3331,  0.3933,  0.5664,  0.3465]],
       dtype=torch.float64)
	q_value: tensor([[-46.3676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 20, step: 11
	action: tensor([[ 0.4414,  2.1887, -0.6408,  0.1558, -0.7717, -1.1102, -0.0329]],
       dtype=torch.float64)
	q_value: tensor([[-46.3676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 20, step: 12
	action: tensor([[-0.6759,  0.2823, -0.8621, -0.8258, -0.3914,  0.8808,  1.5678]],
       dtype=torch.float64)
	q_value: tensor([[-46.3676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2311184708773828, distance: 1.2697160719417193 entropy 1.2378510236740112
epoch: 20, step: 13
	action: tensor([[ 1.0364, -0.8454, -0.0917,  1.1982,  0.9141, -0.6552, -0.7422]],
       dtype=torch.float64)
	q_value: tensor([[-44.7098]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6316358474859332, distance: 0.6945369783860308 entropy 1.2378510236740112
epoch: 20, step: 14
	action: tensor([[-0.0427,  0.4622, -0.4902,  0.4315,  0.6951,  1.0286,  1.2099]],
       dtype=torch.float64)
	q_value: tensor([[-50.3402]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 20, step: 15
	action: tensor([[-1.4121, -0.5949, -1.7966, -0.2207, -0.7292,  0.0166, -0.8803]],
       dtype=torch.float64)
	q_value: tensor([[-46.3676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.5745147822298549, distance: 1.836134065864286 entropy 1.2378510236740112
epoch: 20, step: 16
	action: tensor([[ 0.1271,  0.4625,  0.1798, -0.5261,  0.1549,  0.1577, -1.3274]],
       dtype=torch.float64)
	q_value: tensor([[-45.5301]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5340729691629702, distance: 0.7811162030974422 entropy 1.2378510236740112
epoch: 20, step: 17
	action: tensor([[-0.3919,  0.1234, -0.4649,  0.2752,  2.0124, -0.9201, -0.7472]],
       dtype=torch.float64)
	q_value: tensor([[-33.5865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2520771629771008, distance: 1.2804783515837557 entropy 1.2378510236740112
epoch: 20, step: 18
	action: tensor([[-0.4160, -1.1919,  0.3208,  0.2854,  0.7982, -0.0845, -0.1791]],
       dtype=torch.float64)
	q_value: tensor([[-50.3622]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8765557162671818, distance: 1.5676078333887011 entropy 1.2378510236740112
epoch: 20, step: 19
	action: tensor([[ 0.3755, -0.5337,  0.4663, -0.1759,  0.2094, -0.1505,  0.3669]],
       dtype=torch.float64)
	q_value: tensor([[-38.6822]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07689928619172681, distance: 1.1875290375206768 entropy 1.2378510236740112
epoch: 20, step: 20
	action: tensor([[ 1.4327,  0.3517,  0.4923, -0.0366,  0.2671,  0.0426, -0.6800]],
       dtype=torch.float64)
	q_value: tensor([[-29.8643]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7353235357480574, distance: 0.5887275445718355 entropy 1.2378510236740112
epoch: 20, step: 21
	action: tensor([[ 1.3925,  0.6358,  0.7490, -0.0556, -1.0462,  0.9961, -1.4233]],
       dtype=torch.float64)
	q_value: tensor([[-35.6447]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8894361158139807, distance: 0.3805076012761989 entropy 1.2378510236740112
epoch: 20, step: 22
	action: tensor([[-0.2202,  0.8563, -0.8381,  1.2106, -1.8530,  0.8761, -0.2994]],
       dtype=torch.float64)
	q_value: tensor([[-46.5029]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 20, step: 23
	action: tensor([[-1.0230,  0.2856, -0.8037, -0.7844,  0.0512,  1.0295,  0.1129]],
       dtype=torch.float64)
	q_value: tensor([[-46.3676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5162621487840842, distance: 1.40910657081191 entropy 1.2378510236740112
epoch: 20, step: 24
	action: tensor([[ 0.8820,  0.9697,  0.0088,  1.1062,  0.7598, -1.0529,  0.0129]],
       dtype=torch.float64)
	q_value: tensor([[-39.3104]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 20, step: 25
	action: tensor([[-0.0328, -0.4168,  0.1765, -0.6312,  0.4591,  0.3395,  1.5886]],
       dtype=torch.float64)
	q_value: tensor([[-46.3676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.30688626637716476, distance: 1.3082043543271673 entropy 1.2378510236740112
epoch: 20, step: 26
	action: tensor([[ 0.0845,  0.1028,  0.4384, -0.9705,  0.0781,  0.4685, -0.0235]],
       dtype=torch.float64)
	q_value: tensor([[-41.0801]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.01585897708152817, distance: 1.135233924872006 entropy 1.2378510236740112
epoch: 20, step: 27
	action: tensor([[ 1.0681, -0.6875,  0.5202, -0.5513, -0.7772,  0.1403,  1.4700]],
       dtype=torch.float64)
	q_value: tensor([[-30.4358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.19720443588034775, distance: 1.2521052944243498 entropy 1.2378510236740112
epoch: 20, step: 28
	action: tensor([[ 1.0495, -1.0797,  0.2849,  0.4057,  1.1813,  0.3288,  0.1026]],
       dtype=torch.float64)
	q_value: tensor([[-46.4732]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11437517286921439, distance: 1.2080152231752284 entropy 1.2378510236740112
epoch: 20, step: 29
	action: tensor([[ 0.7224,  0.3281,  0.6915, -0.7005, -0.7012,  0.1219,  0.5468]],
       dtype=torch.float64)
	q_value: tensor([[-46.5893]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6865159895335311, distance: 0.6407142606045508 entropy 1.2378510236740112
epoch: 20, step: 30
	action: tensor([[-1.6022, -0.6033,  0.5334, -0.4568, -0.3320, -0.8376,  0.2594]],
       dtype=torch.float64)
	q_value: tensor([[-34.5919]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.4704313806545755, distance: 1.798635210160459 entropy 1.2378510236740112
epoch: 20, step: 31
	action: tensor([[ 0.8478,  0.8099,  2.2623, -0.6083, -0.6534, -0.0989,  1.6520]],
       dtype=torch.float64)
	q_value: tensor([[-39.2104]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 20, step: 32
	action: tensor([[-0.0749, -1.4313, -1.4234,  0.1417, -0.5850,  1.2797,  1.3118]],
       dtype=torch.float64)
	q_value: tensor([[-46.3676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8927949217538456, distance: 1.5743760493984025 entropy 1.2378510236740112
epoch: 20, step: 33
	action: tensor([[ 0.3840, -0.6272, -0.2659, -0.0103, -1.1450, -0.9988,  0.3964]],
       dtype=torch.float64)
	q_value: tensor([[-48.2189]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1507559804729819, distance: 1.0545640006718944 entropy 1.2378510236740112
epoch: 20, step: 34
	action: tensor([[ 0.2675, -0.4071,  0.5512,  1.0611,  0.7413, -0.6651,  0.9253]],
       dtype=torch.float64)
	q_value: tensor([[-39.3068]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6945942147853076, distance: 0.6324050408891346 entropy 1.2378510236740112
epoch: 20, step: 35
	action: tensor([[-0.7690, -1.2067,  0.2382, -1.4928,  0.7210,  0.0937, -0.2610]],
       dtype=torch.float64)
	q_value: tensor([[-43.6359]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7057975298886401, distance: 1.494584361965857 entropy 1.2378510236740112
epoch: 20, step: 36
	action: tensor([[ 0.1660, -1.1421,  0.3999,  0.9311, -0.3518,  0.1799,  0.2392]],
       dtype=torch.float64)
	q_value: tensor([[-46.5725]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2360628717529638, distance: 1.0001968953637677 entropy 1.2378510236740112
epoch: 20, step: 37
	action: tensor([[ 0.1577, -1.1101, -0.8714,  0.3926,  0.0367,  0.7048,  0.6603]],
       dtype=torch.float64)
	q_value: tensor([[-42.0144]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4263056371604108, distance: 1.366667895839453 entropy 1.2378510236740112
epoch: 20, step: 38
	action: tensor([[ 0.6662,  0.0855, -0.8332,  1.7929, -2.2792, -1.4086, -0.1362]],
       dtype=torch.float64)
	q_value: tensor([[-36.2983]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 20, step: 39
	action: tensor([[ 0.5178, -0.3660, -1.0446, -0.0974, -0.8124,  0.2299,  0.6454]],
       dtype=torch.float64)
	q_value: tensor([[-46.3676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3152877243684227, distance: 0.9469144637658109 entropy 1.2378510236740112
epoch: 20, step: 40
	action: tensor([[-1.1163, -0.7860, -0.6169, -1.2961, -0.1562,  0.5880,  0.0087]],
       dtype=torch.float64)
	q_value: tensor([[-33.1263]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6705360086677992, distance: 1.4790559877060785 entropy 1.2378510236740112
epoch: 20, step: 41
	action: tensor([[ 0.3481,  1.1917, -0.6508,  0.1107,  0.0325,  0.0355,  0.9092]],
       dtype=torch.float64)
	q_value: tensor([[-42.6165]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 20, step: 42
	action: tensor([[-0.3597, -1.7340, -0.2006,  0.0228, -0.3856,  0.0514, -2.1794]],
       dtype=torch.float64)
	q_value: tensor([[-46.3676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0069258206381067, distance: 1.621146837917376 entropy 1.2378510236740112
epoch: 20, step: 43
	action: tensor([[-0.5143, -1.8643, -0.6587,  0.2672,  0.1620,  1.0305,  0.0079]],
       dtype=torch.float64)
	q_value: tensor([[-54.2767]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 20, step: 44
	action: tensor([[ 0.3222, -1.3207, -0.5224, -1.0838, -0.6068,  0.3636,  0.7453]],
       dtype=torch.float64)
	q_value: tensor([[-46.3676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8239906177644472, distance: 1.5454963840955724 entropy 1.2378510236740112
epoch: 20, step: 45
	action: tensor([[-0.3374, -1.3907, -0.4209,  1.2148,  0.4095, -0.5236,  1.0387]],
       dtype=torch.float64)
	q_value: tensor([[-42.6627]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 20, step: 46
	action: tensor([[ 0.7375, -1.2587,  0.4962, -0.5312,  0.1367,  1.5164, -0.0344]],
       dtype=torch.float64)
	q_value: tensor([[-46.3676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24233970655550552, distance: 1.2754894661325282 entropy 1.2378510236740112
epoch: 20, step: 47
	action: tensor([[ 0.3473,  0.5875, -1.8279,  0.3005,  0.7479,  0.8010, -0.1295]],
       dtype=torch.float64)
	q_value: tensor([[-48.4116]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 20, step: 48
	action: tensor([[-0.5743, -0.1495, -0.2173,  0.1023,  0.0921, -0.4712,  0.2296]],
       dtype=torch.float64)
	q_value: tensor([[-46.3676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5054336917398035, distance: 1.4040659550018957 entropy 1.2378510236740112
epoch: 20, step: 49
	action: tensor([[ 0.3667, -0.6609, -0.7134,  0.2966,  0.5112, -0.6416,  0.6341]],
       dtype=torch.float64)
	q_value: tensor([[-26.2130]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1203777442882743, distance: 1.2112643350596348 entropy 1.2378510236740112
epoch: 20, step: 50
	action: tensor([[ 0.1194,  0.6864, -1.1674,  0.0061, -0.4021,  0.6090,  0.4022]],
       dtype=torch.float64)
	q_value: tensor([[-34.8191]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 20, step: 51
	action: tensor([[ 0.3409, -0.9026,  1.2489,  0.2794, -1.4464,  0.0404,  1.2221]],
       dtype=torch.float64)
	q_value: tensor([[-46.3676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13623635260750144, distance: 1.2198067527141245 entropy 1.2378510236740112
epoch: 20, step: 52
	action: tensor([[ 0.3222, -1.4165,  1.1155,  0.9124, -0.9648, -0.0110,  0.4203]],
       dtype=torch.float64)
	q_value: tensor([[-51.7465]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 20, step: 53
	action: tensor([[-0.5076, -0.4380,  0.0563, -0.0672, -1.0443, -0.6102,  1.5292]],
       dtype=torch.float64)
	q_value: tensor([[-46.3676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4841529736853103, distance: 1.3941067390357784 entropy 1.2378510236740112
epoch: 20, step: 54
	action: tensor([[-1.2228,  0.6403,  0.1835,  0.5866,  0.8154,  0.9028,  0.5992]],
       dtype=torch.float64)
	q_value: tensor([[-41.8339]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25541535779030866, distance: 0.9874468537539849 entropy 1.2378510236740112
epoch: 20, step: 55
	action: tensor([[-0.4127, -0.4777, -0.4752, -0.1628,  0.1441, -0.3927,  0.5430]],
       dtype=torch.float64)
	q_value: tensor([[-38.2653]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5922390273611711, distance: 1.4439788282723314 entropy 1.2378510236740112
epoch: 20, step: 56
	action: tensor([[ 0.2151,  0.6141, -0.8338, -0.2521, -0.0200,  0.3160,  0.7814]],
       dtype=torch.float64)
	q_value: tensor([[-29.3505]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 20, step: 57
	action: tensor([[-0.6290,  0.5713, -0.0937, -0.2758, -0.8735,  1.1554,  1.2258]],
       dtype=torch.float64)
	q_value: tensor([[-46.3676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.19180158854593699, distance: 1.2492767953125565 entropy 1.2378510236740112
epoch: 20, step: 58
	action: tensor([[-1.1511,  0.1927,  0.2143,  0.4990,  0.6109,  1.4736, -0.0896]],
       dtype=torch.float64)
	q_value: tensor([[-39.9338]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1775985110601601, distance: 1.0377640866871503 entropy 1.2378510236740112
epoch: 20, step: 59
	action: tensor([[-0.7906,  0.9899, -0.3006,  0.3376,  0.4415,  0.7444, -1.0373]],
       dtype=torch.float64)
	q_value: tensor([[-40.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 20, step: 60
	action: tensor([[ 1.0480,  0.0426,  0.1587,  0.2537, -0.7228, -0.1519,  1.0750]],
       dtype=torch.float64)
	q_value: tensor([[-46.3676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8194350176649439, distance: 0.48626550021323994 entropy 1.2378510236740112
epoch: 20, step: 61
	action: tensor([[ 0.1360, -1.4024,  0.2252,  0.2613,  1.6140,  1.1871, -0.0484]],
       dtype=torch.float64)
	q_value: tensor([[-39.4645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1659428143288062, distance: 1.2356495586382965 entropy 1.2378510236740112
epoch: 20, step: 62
	action: tensor([[ 0.4090, -0.2482, -1.5787, -0.8264,  0.4168,  0.4307,  0.6573]],
       dtype=torch.float64)
	q_value: tensor([[-50.2154]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6392233596379242, distance: 0.687346773335545 entropy 1.2378510236740112
epoch: 20, step: 63
	action: tensor([[ 1.0679,  0.2867,  0.5189, -1.9322,  0.5173,  0.1404, -0.4205]],
       dtype=torch.float64)
	q_value: tensor([[-40.9812]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1809264707259164, distance: 1.0356622312917336 entropy 1.2378510236740112
epoch: 20, step: 64
	action: tensor([[-0.5360, -0.3861,  1.8713,  1.0810,  0.6291, -0.1067,  0.9654]],
       dtype=torch.float64)
	q_value: tensor([[-43.2910]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09646244224019851, distance: 1.0877517687766014 entropy 1.2378510236740112
epoch: 20, step: 65
	action: tensor([[-0.6320, -0.0469, -0.6705,  1.2077, -2.1414,  0.5284, -0.9114]],
       dtype=torch.float64)
	q_value: tensor([[-51.0954]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 20, step: 66
	action: tensor([[ 1.6928, -0.2814, -0.1143, -0.6769,  0.8114,  0.5485,  0.1319]],
       dtype=torch.float64)
	q_value: tensor([[-46.3676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 20, step: 67
	action: tensor([[ 0.9434, -0.5429,  0.9311, -0.5849, -0.4492, -0.0580,  1.0377]],
       dtype=torch.float64)
	q_value: tensor([[-46.3676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03594524761024531, distance: 1.164729551392399 entropy 1.2378510236740112
epoch: 20, step: 68
	action: tensor([[-0.5643,  0.2999,  0.4937,  0.8459,  2.2000, -0.1763, -0.0138]],
       dtype=torch.float64)
	q_value: tensor([[-41.5303]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 20, step: 69
	action: tensor([[ 0.1899, -1.5570, -1.0782,  0.7377,  1.2641,  0.1624, -0.4998]],
       dtype=torch.float64)
	q_value: tensor([[-46.3676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.588412517131131, distance: 1.442242680823857 entropy 1.2378510236740112
epoch: 20, step: 70
	action: tensor([[ 0.1207,  0.5939,  1.5806, -1.2609,  0.9359,  2.1892, -0.9889]],
       dtype=torch.float64)
	q_value: tensor([[-45.8414]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 20, step: 71
	action: tensor([[-0.4692, -0.7731,  0.6731, -0.8035, -1.1206, -1.1394,  0.6435]],
       dtype=torch.float64)
	q_value: tensor([[-46.3676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6832107905960074, distance: 1.4846563720285024 entropy 1.2378510236740112
epoch: 20, step: 72
	action: tensor([[ 0.5818,  0.0035,  0.1538, -1.0940,  1.1250, -0.0303, -0.6685]],
       dtype=torch.float64)
	q_value: tensor([[-43.0550]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08812921734451862, distance: 1.0927563619453928 entropy 1.2378510236740112
epoch: 20, step: 73
	action: tensor([[ 0.6573,  1.4727, -0.0076, -1.4962, -1.1910,  0.0986,  0.1664]],
       dtype=torch.float64)
	q_value: tensor([[-39.3391]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9512357847835026, distance: 0.25270120504755217 entropy 1.2378510236740112
epoch: 20, step: 74
	action: tensor([[ 0.4618,  1.0900,  0.1581, -0.2151, -0.7452, -0.3548,  0.7821]],
       dtype=torch.float64)
	q_value: tensor([[-39.6307]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 20, step: 75
	action: tensor([[ 1.1811,  0.2315, -1.2400,  0.5274, -0.7522,  0.6253, -0.0830]],
       dtype=torch.float64)
	q_value: tensor([[-46.3676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 20, step: 76
	action: tensor([[-0.2523, -1.1463,  0.8760, -0.0953,  0.2049, -0.6510, -0.1382]],
       dtype=torch.float64)
	q_value: tensor([[-46.3676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0902528218623053, distance: 1.6544593555037765 entropy 1.2378510236740112
epoch: 20, step: 77
	action: tensor([[ 1.4366, -0.1539,  0.0197,  0.5312,  1.1069, -0.0918,  1.1415]],
       dtype=torch.float64)
	q_value: tensor([[-40.0798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.672529619380331, distance: 0.6548513174278014 entropy 1.2378510236740112
epoch: 20, step: 78
	action: tensor([[-0.4855, -0.7896, -0.0958,  0.2935,  0.8179,  1.2774, -1.2989]],
       dtype=torch.float64)
	q_value: tensor([[-47.2521]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.006314952946778751, distance: 1.147951807639831 entropy 1.2378510236740112
epoch: 20, step: 79
	action: tensor([[ 0.0951, -1.3313,  2.0452, -1.0999, -0.5297,  0.6676, -0.1453]],
       dtype=torch.float64)
	q_value: tensor([[-46.2339]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 20, step: 80
	action: tensor([[ 0.4192, -0.2888,  0.2623, -0.7372, -1.0239,  0.2617, -0.7221]],
       dtype=torch.float64)
	q_value: tensor([[-46.3676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05255628150890823, distance: 1.174030438978877 entropy 1.2378510236740112
epoch: 20, step: 81
	action: tensor([[ 0.7699,  0.7251, -0.3352, -0.6752,  0.5096,  0.0700, -0.6373]],
       dtype=torch.float64)
	q_value: tensor([[-34.9789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9380740584330359, distance: 0.28476919174050763 entropy 1.2378510236740112
epoch: 20, step: 82
	action: tensor([[-0.0745,  0.5633,  0.8364, -0.4407, -1.0064,  0.4335, -0.3659]],
       dtype=torch.float64)
	q_value: tensor([[-30.0252]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 20, step: 83
	action: tensor([[ 1.0508,  0.2272,  0.5607, -1.0722,  0.3863,  0.4712,  1.6426]],
       dtype=torch.float64)
	q_value: tensor([[-46.3676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5774452957090869, distance: 0.743871917803665 entropy 1.2378510236740112
epoch: 20, step: 84
	action: tensor([[ 1.7058,  0.6302,  1.5400,  0.9679, -0.3622,  0.9124,  1.0044]],
       dtype=torch.float64)
	q_value: tensor([[-48.0381]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.01691716669828891, distance: 1.134623435207569 entropy 1.2378510236740112
epoch: 20, step: 85
	action: tensor([[-0.0936, -0.0657, -1.2289, -1.1998,  0.4195,  0.2812,  0.1431]],
       dtype=torch.float64)
	q_value: tensor([[-57.7007]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5603847230464918, distance: 0.7587401765800673 entropy 1.2378510236740112
epoch: 20, step: 86
	action: tensor([[ 0.1115,  0.2630,  1.6857,  1.1308,  0.0072, -0.3559,  0.2703]],
       dtype=torch.float64)
	q_value: tensor([[-38.5233]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 20, step: 87
	action: tensor([[ 0.4244, -1.3163, -0.5434, -0.6216,  2.1939, -1.0716, -1.6192]],
       dtype=torch.float64)
	q_value: tensor([[-46.3676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8075971611450703, distance: 1.5385354893890788 entropy 1.2378510236740112
epoch: 20, step: 88
	action: tensor([[-0.1065, -0.6925,  0.4853, -0.1141,  0.6622,  0.6985,  0.6325]],
       dtype=torch.float64)
	q_value: tensor([[-69.0625]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18300482036377463, distance: 1.24465775788289 entropy 1.2378510236740112
epoch: 20, step: 89
	action: tensor([[0.2748, 0.4684, 0.2141, 1.7530, 0.2700, 0.7395, 0.5468]],
       dtype=torch.float64)
	q_value: tensor([[-34.9989]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 20, step: 90
	action: tensor([[ 0.8566, -1.1981, -1.1993,  0.2724,  0.9632,  0.1461, -0.0393]],
       dtype=torch.float64)
	q_value: tensor([[-46.3676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4424520337097111, distance: 1.3743817619665362 entropy 1.2378510236740112
epoch: 20, step: 91
	action: tensor([[ 0.8385, -0.5325, -0.4544, -0.9078,  0.0707,  0.2677,  0.9800]],
       dtype=torch.float64)
	q_value: tensor([[-43.5814]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2211986247170845, distance: 1.2645903008440957 entropy 1.2378510236740112
epoch: 20, step: 92
	action: tensor([[ 0.5598, -0.7241, -0.1155,  0.2977,  0.7917,  0.2050,  0.5332]],
       dtype=torch.float64)
	q_value: tensor([[-38.4997]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2292618859136457, distance: 1.0046391800654844 entropy 1.2378510236740112
epoch: 20, step: 93
	action: tensor([[ 0.4502, -1.8430, -0.5298,  1.0106,  1.0627, -0.7283,  0.8112]],
       dtype=torch.float64)
	q_value: tensor([[-35.6416]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 20, step: 94
	action: tensor([[-1.2193, -1.3999,  0.2713,  0.7678, -1.1974, -0.8042, -0.2471]],
       dtype=torch.float64)
	q_value: tensor([[-46.3676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.932753787158296, distance: 1.5909076117283254 entropy 1.2378510236740112
epoch: 20, step: 95
	action: tensor([[-1.4376, -0.0203,  0.8153,  1.6481,  0.2839,  0.2538, -1.0878]],
       dtype=torch.float64)
	q_value: tensor([[-50.8833]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35054493799073816, distance: 0.9222130135320564 entropy 1.2378510236740112
epoch: 20, step: 96
	action: tensor([[ 0.8103, -0.6736, -0.4906,  0.5773, -0.0077, -0.5029,  1.1501]],
       dtype=torch.float64)
	q_value: tensor([[-51.9158]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 20, step: 97
	action: tensor([[-0.9290, -0.3764, -0.3353, -0.3080,  0.7382,  0.7308, -0.1472]],
       dtype=torch.float64)
	q_value: tensor([[-46.3676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8352504114543129, distance: 1.5502593462880292 entropy 1.2378510236740112
epoch: 20, step: 98
	action: tensor([[-0.0211,  0.9934,  0.6954,  0.3976, -0.2851,  0.5413,  0.1939]],
       dtype=torch.float64)
	q_value: tensor([[-35.1530]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 20, step: 99
	action: tensor([[ 0.6642, -0.7338,  0.4990, -2.0484, -0.9252, -0.7120,  0.2004]],
       dtype=torch.float64)
	q_value: tensor([[-46.3676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.33725981403065397, distance: 1.3233191283143075 entropy 1.2378510236740112
epoch: 20, step: 100
	action: tensor([[-0.3319, -0.9755, -0.0517, -0.5507, -0.4769, -1.1769, -0.7800]],
       dtype=torch.float64)
	q_value: tensor([[-47.4405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4663426925681706, distance: 1.3857166425074359 entropy 1.2378510236740112
epoch: 20, step: 101
	action: tensor([[ 0.9323, -0.8270,  0.6593,  0.6405,  0.0084, -0.3860, -0.6253]],
       dtype=torch.float64)
	q_value: tensor([[-42.9142]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15989653495547207, distance: 1.048873423312895 entropy 1.2378510236740112
epoch: 20, step: 102
	action: tensor([[ 1.6712, -0.5171,  1.4507, -1.0622, -2.1024,  0.1352, -0.5020]],
       dtype=torch.float64)
	q_value: tensor([[-45.7152]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4630660094586504, distance: 0.8385271757240043 entropy 1.2378510236740112
epoch: 20, step: 103
	action: tensor([[ 0.6662, -0.3029,  0.2967, -0.2960, -1.0419,  1.0639, -0.5061]],
       dtype=torch.float64)
	q_value: tensor([[-60.1652]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5429949998091242, distance: 0.773601261321827 entropy 1.2378510236740112
epoch: 20, step: 104
	action: tensor([[ 0.4156,  0.5610, -0.0687,  0.0809,  0.0824,  1.0438, -0.6844]],
       dtype=torch.float64)
	q_value: tensor([[-38.7492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 20, step: 105
	action: tensor([[-0.1834, -0.3259,  0.3068,  0.3565,  0.7244, -0.8471, -1.0113]],
       dtype=torch.float64)
	q_value: tensor([[-46.3676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.176824025846684, distance: 1.2414020448308314 entropy 1.2378510236740112
epoch: 20, step: 106
	action: tensor([[-1.5430, -0.5307,  0.2196, -0.8237,  0.2237, -0.3247,  0.3887]],
       dtype=torch.float64)
	q_value: tensor([[-41.4983]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.4933743201161729, distance: 1.8069678867776653 entropy 1.2378510236740112
epoch: 20, step: 107
	action: tensor([[-0.9975,  0.5683,  0.1710, -0.4819,  0.3412, -0.7381, -0.5592]],
       dtype=torch.float64)
	q_value: tensor([[-37.2297]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8708545533273206, distance: 1.5652247477297778 entropy 1.2378510236740112
epoch: 20, step: 108
	action: tensor([[ 0.0851, -1.0040,  0.1342,  0.6744, -0.1511,  0.1947, -1.0785]],
       dtype=torch.float64)
	q_value: tensor([[-33.9446]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0044375239858276405, distance: 1.1418024034464331 entropy 1.2378510236740112
epoch: 20, step: 109
	action: tensor([[ 0.3466,  1.2518, -0.0192, -0.7107,  0.3992,  0.7908, -1.1884]],
       dtype=torch.float64)
	q_value: tensor([[-42.1486]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 20, step: 110
	action: tensor([[-0.6301, -1.8481,  1.2157,  0.0232, -0.1377, -1.2416,  0.8332]],
       dtype=torch.float64)
	q_value: tensor([[-46.3676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 20, step: 111
	action: tensor([[-0.0816,  0.0762,  0.2409, -0.9008, -1.6767, -0.3912,  1.0093]],
       dtype=torch.float64)
	q_value: tensor([[-46.3676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07448078467506503, distance: 1.1861948110352336 entropy 1.2378510236740112
epoch: 20, step: 112
	action: tensor([[-0.3143, -1.0463,  0.4111, -1.3427, -0.9778, -0.7112,  0.4594]],
       dtype=torch.float64)
	q_value: tensor([[-42.5486]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6436993878389607, distance: 1.4671276092565153 entropy 1.2378510236740112
epoch: 20, step: 113
	action: tensor([[-0.8837, -0.8408, -0.1564,  1.9588, -2.4022, -0.9356,  0.6371]],
       dtype=torch.float64)
	q_value: tensor([[-44.1325]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3386101240339269, distance: 0.9306480349276489 entropy 1.2378510236740112
epoch: 20, step: 114
	action: tensor([[-0.0050, -0.7472, -1.4248, -0.0518, -1.8881,  1.6336,  1.3572]],
       dtype=torch.float64)
	q_value: tensor([[-67.3153]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3041532064308077, distance: 1.306835730193901 entropy 1.2378510236740112
epoch: 20, step: 115
	action: tensor([[-1.1080, -0.9747,  0.4660,  0.2366,  0.0351,  2.0415, -2.1344]],
       dtype=torch.float64)
	q_value: tensor([[-56.7880]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.30445276844037217, distance: 1.30698581066668 entropy 1.2378510236740112
epoch: 20, step: 116
	action: tensor([[ 1.4501,  0.7544, -0.4439, -0.2728, -0.1678,  0.4334, -0.0464]],
       dtype=torch.float64)
	q_value: tensor([[-63.7252]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 20, step: 117
	action: tensor([[-0.2688, -0.7650,  0.0314,  0.4606, -0.5579,  0.3338,  0.9345]],
       dtype=torch.float64)
	q_value: tensor([[-46.3676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2923812632900753, distance: 1.3009242815513802 entropy 1.2378510236740112
epoch: 20, step: 118
	action: tensor([[ 0.2855,  1.3099,  0.9092,  1.3076, -0.8070,  0.4288,  0.6026]],
       dtype=torch.float64)
	q_value: tensor([[-33.7525]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 20, step: 119
	action: tensor([[-0.7808,  0.3709,  1.6663, -0.4709, -0.0700,  0.4907,  1.6238]],
       dtype=torch.float64)
	q_value: tensor([[-46.3676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 20, step: 120
	action: tensor([[-0.9602, -1.8871,  1.8220, -0.3148, -0.1224, -0.5646,  0.7904]],
       dtype=torch.float64)
	q_value: tensor([[-46.3676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 20, step: 121
	action: tensor([[ 0.5038,  2.3396, -0.8051, -0.0351,  0.2116, -1.1445,  0.5007]],
       dtype=torch.float64)
	q_value: tensor([[-46.3676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 20, step: 122
	action: tensor([[-0.3388,  0.3612,  1.0426,  1.0545,  0.4622, -0.4811,  0.3261]],
       dtype=torch.float64)
	q_value: tensor([[-46.3676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 20, step: 123
	action: tensor([[ 0.7613, -2.2988,  0.2154, -0.0425,  1.1727,  0.6779,  0.7974]],
       dtype=torch.float64)
	q_value: tensor([[-46.3676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 20, step: 124
	action: tensor([[ 2.3604, -0.8710,  0.5277, -0.1002, -0.8644,  1.2943, -0.3357]],
       dtype=torch.float64)
	q_value: tensor([[-46.3676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10957906240512305, distance: 1.0798274790756088 entropy 1.2378510236740112
epoch: 20, step: 125
	action: tensor([[ 1.5664, -0.3970, -1.0933, -0.6797, -0.5000, -0.8903, -0.5692]],
       dtype=torch.float64)
	q_value: tensor([[-50.3623]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3989269159962141, distance: 1.353487366515105 entropy 1.2378510236740112
epoch: 20, step: 126
	action: tensor([[ 1.1659, -0.9646, -0.1496, -0.0757,  0.7003,  0.9219,  0.3866]],
       dtype=torch.float64)
	q_value: tensor([[-42.1775]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12460274012544614, distance: 1.2135460526372341 entropy 1.2378510236740112
epoch: 20, step: 127
	action: tensor([[-0.0642, -0.6922, -0.0096,  0.2737,  0.1717,  1.5386,  0.5276]],
       dtype=torch.float64)
	q_value: tensor([[-43.6098]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4452303934464428, distance: 0.852340300336615 entropy 1.2378510236740112
LOSS epoch 20 actor 571.5868885271573 critic 72.70302451573787 
epoch: 21, step: 0
	action: tensor([[-0.7767, -0.7200, -0.1418, -0.9993, -0.9013,  0.6191, -0.3597]],
       dtype=torch.float64)
	q_value: tensor([[-36.7425]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0572473834209513, distance: 1.6413452874946406 entropy 1.1324905157089233
epoch: 21, step: 1
	action: tensor([[ 0.3190, -0.4037,  1.4598, -0.5458,  0.4352,  0.0492, -0.0189]],
       dtype=torch.float64)
	q_value: tensor([[-37.7362]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.025206836135287736, distance: 1.1298295537536835 entropy 1.1324905157089233
epoch: 21, step: 2
	action: tensor([[ 1.1521, -0.4294, -0.8193,  0.2634,  0.5015, -0.9107, -1.3058]],
       dtype=torch.float64)
	q_value: tensor([[-35.8557]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16422273789250164, distance: 1.0461692943998278 entropy 1.1324905157089233
epoch: 21, step: 3
	action: tensor([[-1.1875, -0.7266,  1.1421,  0.0307, -0.1801,  0.8047, -0.2461]],
       dtype=torch.float64)
	q_value: tensor([[-41.9038]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0117261805955056, distance: 1.6230844881170723 entropy 1.1324905157089233
epoch: 21, step: 4
	action: tensor([[-0.0729,  0.4371,  0.5298, -0.5869,  0.2172, -0.9578, -0.4347]],
       dtype=torch.float64)
	q_value: tensor([[-39.3367]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04722069180353705, distance: 1.11699917333529 entropy 1.1324905157089233
epoch: 21, step: 5
	action: tensor([[-0.2853, -0.8248, -0.4180,  0.5073,  1.5806,  0.2525,  0.2362]],
       dtype=torch.float64)
	q_value: tensor([[-29.8792]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.34816838651594684, distance: 1.328705592050751 entropy 1.1324905157089233
epoch: 21, step: 6
	action: tensor([[ 0.8951,  0.4028,  0.0654, -0.4919,  1.0279, -0.6140,  1.9099]],
       dtype=torch.float64)
	q_value: tensor([[-40.3951]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7782077775887118, distance: 0.5389268852242467 entropy 1.1324905157089233
epoch: 21, step: 7
	action: tensor([[-0.9546,  0.3960,  0.8646,  0.1989,  0.7595,  2.0197,  1.1993]],
       dtype=torch.float64)
	q_value: tensor([[-45.1914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 21, step: 8
	action: tensor([[-0.6956,  0.8865, -0.0476, -0.5642,  0.3379,  0.5950,  0.3839]],
       dtype=torch.float64)
	q_value: tensor([[-43.2704]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08423406583244208, distance: 1.0950877864680177 entropy 1.1324905157089233
epoch: 21, step: 9
	action: tensor([[-0.5215,  0.4114, -0.6238,  0.0558, -0.4621, -0.2125,  0.8959]],
       dtype=torch.float64)
	q_value: tensor([[-31.9984]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 21, step: 10
	action: tensor([[-0.0207, -1.2747, -0.7795,  0.2290, -0.7282, -0.6037,  0.3859]],
       dtype=torch.float64)
	q_value: tensor([[-43.2704]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5819338801067011, distance: 1.439298447577333 entropy 1.1324905157089233
epoch: 21, step: 11
	action: tensor([[ 0.3868,  0.9089, -0.1184,  0.0745,  0.8342, -0.6865, -0.1179]],
       dtype=torch.float64)
	q_value: tensor([[-36.6027]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 21, step: 12
	action: tensor([[-0.9065,  0.6043, -0.6236, -0.4537,  0.0474, -0.0896,  0.5680]],
       dtype=torch.float64)
	q_value: tensor([[-43.2704]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.28230769763458996, distance: 1.2958442856449655 entropy 1.1324905157089233
epoch: 21, step: 13
	action: tensor([[-0.9792,  0.4303, -0.0200,  0.5959, -1.1525,  0.9872, -0.1670]],
       dtype=torch.float64)
	q_value: tensor([[-30.1757]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.26880200399911613, distance: 1.2890020891256393 entropy 1.1324905157089233
epoch: 21, step: 14
	action: tensor([[-0.0869, -0.2563, -1.1329,  0.2754, -2.0668,  1.1002, -0.5893]],
       dtype=torch.float64)
	q_value: tensor([[-33.9949]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20146462318338698, distance: 1.254331090609873 entropy 1.1324905157089233
epoch: 21, step: 15
	action: tensor([[-0.6488,  0.1254,  0.5556,  0.9904, -0.3106, -0.1354,  0.9146]],
       dtype=torch.float64)
	q_value: tensor([[-44.2722]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 21, step: 16
	action: tensor([[-0.7284, -2.0272, -0.1095,  0.5827, -0.1057,  0.7206,  0.0819]],
       dtype=torch.float64)
	q_value: tensor([[-43.2704]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 21, step: 17
	action: tensor([[ 0.5653, -1.4874,  0.8418, -0.2322, -0.9437, -0.6315, -0.4026]],
       dtype=torch.float64)
	q_value: tensor([[-43.2704]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8850348956433414, distance: 1.571145444018119 entropy 1.1324905157089233
epoch: 21, step: 18
	action: tensor([[-0.8437,  0.1404, -0.9865, -1.2115, -0.0660,  0.1694,  1.0274]],
       dtype=torch.float64)
	q_value: tensor([[-43.2651]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08087441466233902, distance: 1.189718765163627 entropy 1.1324905157089233
epoch: 21, step: 19
	action: tensor([[-0.9748, -0.6711,  0.8683, -0.6698,  0.8103, -1.1265, -0.2188]],
       dtype=torch.float64)
	q_value: tensor([[-38.3605]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.263307295125378, distance: 1.7215849399737884 entropy 1.1324905157089233
epoch: 21, step: 20
	action: tensor([[ 0.6564, -0.6558, -0.0815, -1.0597,  0.4502, -0.2143, -0.9802]],
       dtype=torch.float64)
	q_value: tensor([[-41.6317]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5802659379384099, distance: 1.4385394715750364 entropy 1.1324905157089233
epoch: 21, step: 21
	action: tensor([[ 0.6472,  0.2109, -0.8115, -0.5252,  0.4121,  0.4658, -0.0541]],
       dtype=torch.float64)
	q_value: tensor([[-37.6435]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8252438665481168, distance: 0.4783798814740442 entropy 1.1324905157089233
epoch: 21, step: 22
	action: tensor([[-0.1465, -1.6245, -0.1509,  0.4714,  0.7508,  1.5183,  1.0570]],
       dtype=torch.float64)
	q_value: tensor([[-28.1524]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 21, step: 23
	action: tensor([[-0.2500,  0.9732,  0.1808, -1.7701,  1.1428,  0.4242, -0.2414]],
       dtype=torch.float64)
	q_value: tensor([[-43.2704]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18906698535280064, distance: 1.0305028189707675 entropy 1.1324905157089233
epoch: 21, step: 24
	action: tensor([[ 0.2510, -1.1066,  1.2474,  0.0995, -0.5446, -0.4178,  0.9025]],
       dtype=torch.float64)
	q_value: tensor([[-44.4463]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6655768039606218, distance: 1.4768589703545199 entropy 1.1324905157089233
epoch: 21, step: 25
	action: tensor([[-0.4106,  0.4381, -0.7063, -0.4654,  0.7152, -0.1733,  0.4255]],
       dtype=torch.float64)
	q_value: tensor([[-41.7859]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18937246248429251, distance: 1.030308706330928 entropy 1.1324905157089233
epoch: 21, step: 26
	action: tensor([[-0.2600, -0.4795,  1.3533,  1.3149,  1.3521, -0.1877, -0.4631]],
       dtype=torch.float64)
	q_value: tensor([[-29.9939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6472403130075115, distance: 0.6796669741873574 entropy 1.1324905157089233
epoch: 21, step: 27
	action: tensor([[-1.3760, -0.0497, -0.4502,  0.2752,  0.8357, -1.5613, -0.1142]],
       dtype=torch.float64)
	q_value: tensor([[-51.1817]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.225509459107689, distance: 1.7071489509594573 entropy 1.1324905157089233
epoch: 21, step: 28
	action: tensor([[ 0.5211,  1.1060, -0.1553,  0.9856,  1.5331,  0.0040, -0.6619]],
       dtype=torch.float64)
	q_value: tensor([[-43.2862]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 21, step: 29
	action: tensor([[-0.3655, -0.4034, -0.4824, -0.0991, -0.1112, -0.7447,  0.6844]],
       dtype=torch.float64)
	q_value: tensor([[-43.2704]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3113621971484404, distance: 1.3104426620138188 entropy 1.1324905157089233
epoch: 21, step: 30
	action: tensor([[-1.0334, -0.7653,  0.4572, -0.3616, -0.0916,  0.3333, -1.3087]],
       dtype=torch.float64)
	q_value: tensor([[-29.8914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.4127150613567236, distance: 1.7775004154979936 entropy 1.1324905157089233
epoch: 21, step: 31
	action: tensor([[-0.3589,  0.7191,  1.2374, -0.4708,  0.0809, -0.0298,  0.1671]],
       dtype=torch.float64)
	q_value: tensor([[-41.2787]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 21, step: 32
	action: tensor([[-0.1881,  0.5001, -0.2856, -0.6043,  0.3540, -1.7237,  0.8972]],
       dtype=torch.float64)
	q_value: tensor([[-43.2704]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24635624657282562, distance: 0.9934356599589502 entropy 1.1324905157089233
epoch: 21, step: 33
	action: tensor([[ 1.7936, -0.4441, -1.2913,  1.7588, -0.1548,  0.7479, -0.2363]],
       dtype=torch.float64)
	q_value: tensor([[-36.8499]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7644985269751701, distance: 0.5553330326818533 entropy 1.1324905157089233
epoch: 21, step: 34
	action: tensor([[ 1.0562, -0.2423,  1.2057,  0.2024,  0.8366, -1.8741,  0.3434]],
       dtype=torch.float64)
	q_value: tensor([[-43.9326]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7873398642844927, distance: 0.5277153617287424 entropy 1.1324905157089233
epoch: 21, step: 35
	action: tensor([[ 0.1824,  0.1047,  1.4848,  0.9310, -0.0350,  0.6833,  0.0223]],
       dtype=torch.float64)
	q_value: tensor([[-51.1033]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8744395680945078, distance: 0.4054927500714313 entropy 1.1324905157089233
epoch: 21, step: 36
	action: tensor([[-0.1450,  0.7998,  0.3205,  0.1505,  0.6747,  0.0385, -0.3309]],
       dtype=torch.float64)
	q_value: tensor([[-40.5929]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 21, step: 37
	action: tensor([[-0.5933,  0.4888,  0.5909, -0.5057,  0.2611,  0.6714,  0.2976]],
       dtype=torch.float64)
	q_value: tensor([[-43.2704]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13222445554767748, distance: 1.2176513620070704 entropy 1.1324905157089233
epoch: 21, step: 38
	action: tensor([[ 1.2651, -0.1273, -0.8394,  0.3058, -0.6169,  0.6948, -0.9572]],
       dtype=torch.float64)
	q_value: tensor([[-30.9368]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 21, step: 39
	action: tensor([[ 0.0033, -0.4423, -0.6384, -0.3475,  0.1652,  0.1660,  0.1545]],
       dtype=torch.float64)
	q_value: tensor([[-43.2704]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07840402568871396, distance: 1.1883584085183432 entropy 1.1324905157089233
epoch: 21, step: 40
	action: tensor([[ 1.4897, -0.6279, -0.8059, -0.0227,  1.2967, -0.2350,  1.4956]],
       dtype=torch.float64)
	q_value: tensor([[-25.7254]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16057625210948312, distance: 1.2328025758266026 entropy 1.1324905157089233
epoch: 21, step: 41
	action: tensor([[ 0.2206,  0.3664, -0.3612, -0.3336, -0.4374, -0.9298,  0.3034]],
       dtype=torch.float64)
	q_value: tensor([[-49.2969]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6667860809718253, distance: 0.6605691091901918 entropy 1.1324905157089233
epoch: 21, step: 42
	action: tensor([[-0.8583,  1.2118,  0.3580,  0.8103,  0.5131, -0.9373, -0.1181]],
       dtype=torch.float64)
	q_value: tensor([[-26.8249]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 21, step: 43
	action: tensor([[ 0.7587, -0.6953,  1.3257,  0.2466,  0.3360,  0.3192, -0.9138]],
       dtype=torch.float64)
	q_value: tensor([[-43.2704]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11172671607605511, distance: 1.0785244461909058 entropy 1.1324905157089233
epoch: 21, step: 44
	action: tensor([[-0.6692, -0.5198,  1.3397,  0.5414,  0.4129,  0.7640,  0.4951]],
       dtype=torch.float64)
	q_value: tensor([[-44.8297]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.01741465460097169, distance: 1.1542654269439443 entropy 1.1324905157089233
epoch: 21, step: 45
	action: tensor([[ 0.2768,  0.8775,  1.9948, -0.0930, -0.2395, -0.2647,  0.7511]],
       dtype=torch.float64)
	q_value: tensor([[-38.5642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 21, step: 46
	action: tensor([[ 0.3426, -0.1144,  0.5946, -1.1629,  0.1713,  0.4660, -0.7403]],
       dtype=torch.float64)
	q_value: tensor([[-43.2704]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06537638138226165, distance: 1.1811586249053014 entropy 1.1324905157089233
epoch: 21, step: 47
	action: tensor([[ 1.3905,  0.4169,  0.6208, -0.3656, -0.6389, -1.5864,  0.2393]],
       dtype=torch.float64)
	q_value: tensor([[-32.5673]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6000990340112973, distance: 0.7236572540043593 entropy 1.1324905157089233
epoch: 21, step: 48
	action: tensor([[-0.7965,  0.1878,  0.0287, -0.3531,  1.0157, -0.3005,  0.9148]],
       dtype=torch.float64)
	q_value: tensor([[-43.0364]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8221191790882705, distance: 1.5447033306815996 entropy 1.1324905157089233
epoch: 21, step: 49
	action: tensor([[-0.3237, -0.1266, -0.1213,  1.4293,  0.5546, -0.1242,  0.6621]],
       dtype=torch.float64)
	q_value: tensor([[-33.6560]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 21, step: 50
	action: tensor([[-0.9376, -0.6679, -0.3474, -1.3982, -0.2860, -0.1947,  0.6112]],
       dtype=torch.float64)
	q_value: tensor([[-43.2704]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.35633316732401443, distance: 1.3327229738042026 entropy 1.1324905157089233
epoch: 21, step: 51
	action: tensor([[-0.7499,  0.0841, -1.3409, -0.3535,  0.8829,  0.0421, -0.7690]],
       dtype=torch.float64)
	q_value: tensor([[-38.3537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.33927458759448315, distance: 1.3243156380708077 entropy 1.1324905157089233
epoch: 21, step: 52
	action: tensor([[ 0.1893,  0.3069, -1.0592, -0.4466, -0.9311, -0.8659,  1.2805]],
       dtype=torch.float64)
	q_value: tensor([[-35.2729]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8910874512931114, distance: 0.37765536083055273 entropy 1.1324905157089233
epoch: 21, step: 53
	action: tensor([[-1.0752,  0.5536,  0.5436, -1.0429, -0.1842, -0.2331,  0.3812]],
       dtype=torch.float64)
	q_value: tensor([[-37.5281]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1326238028537476, distance: 1.6711437896994197 entropy 1.1324905157089233
epoch: 21, step: 54
	action: tensor([[ 0.0233,  0.4467, -0.5971, -0.3484, -0.9074,  0.3124,  1.1727]],
       dtype=torch.float64)
	q_value: tensor([[-32.3572]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 21, step: 55
	action: tensor([[-0.1906, -0.0409, -0.9123, -0.1495, -0.2836,  0.3074,  0.3984]],
       dtype=torch.float64)
	q_value: tensor([[-43.2704]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.057608490627874986, distance: 1.110893371902092 entropy 1.1324905157089233
epoch: 21, step: 56
	action: tensor([[-1.2940, -0.7404,  0.7327,  1.3347,  1.0435, -0.6173, -0.1099]],
       dtype=torch.float64)
	q_value: tensor([[-24.8366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6913952164687234, distance: 1.4882614834909114 entropy 1.1324905157089233
epoch: 21, step: 57
	action: tensor([[-0.8148, -1.0802,  0.1699, -0.4205, -0.2950, -1.0336,  0.1931]],
       dtype=torch.float64)
	q_value: tensor([[-47.0660]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8506300513839344, distance: 1.556741482983235 entropy 1.1324905157089233
epoch: 21, step: 58
	action: tensor([[-0.1210, -0.2189, -0.4938,  0.0734,  0.5246,  0.6392, -0.9487]],
       dtype=torch.float64)
	q_value: tensor([[-37.0299]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16503626597546783, distance: 1.0456600107977134 entropy 1.1324905157089233
epoch: 21, step: 59
	action: tensor([[-0.7208, -0.6421,  1.0861, -0.0137, -0.4331, -0.7442,  0.0478]],
       dtype=torch.float64)
	q_value: tensor([[-30.0571]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.11843860557626, distance: 1.665576690762194 entropy 1.1324905157089233
epoch: 21, step: 60
	action: tensor([[ 0.6943, -0.4798, -0.6663, -1.0449, -0.5917, -0.2526, -0.4369]],
       dtype=torch.float64)
	q_value: tensor([[-35.9726]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16689109009741343, distance: 1.2361519410752468 entropy 1.1324905157089233
epoch: 21, step: 61
	action: tensor([[-0.0967, -0.1588,  0.7287,  0.2866,  0.3222,  0.3180, -0.3096]],
       dtype=torch.float64)
	q_value: tensor([[-32.6396]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4147992563668653, distance: 0.8754052118775324 entropy 1.1324905157089233
epoch: 21, step: 62
	action: tensor([[-1.7355, -1.0255,  0.7821, -1.6391,  1.0718, -0.1645,  1.7893]],
       dtype=torch.float64)
	q_value: tensor([[-28.7167]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.657566824240085, distance: 1.4733034851862779 entropy 1.1324905157089233
epoch: 21, step: 63
	action: tensor([[ 0.0429,  0.9430,  0.7161,  0.5733, -0.9881,  0.2602, -1.5411]],
       dtype=torch.float64)
	q_value: tensor([[-52.0759]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 21, step: 64
	action: tensor([[-6.4111e-01,  2.0558e-01,  1.2361e-01,  2.7241e-01, -7.0773e-01,
          8.6628e-02, -3.9872e-04]], dtype=torch.float64)
	q_value: tensor([[-43.2704]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15664655563980068, distance: 1.2307136791399684 entropy 1.1324905157089233
epoch: 21, step: 65
	action: tensor([[-0.1637, -0.0760,  0.6193, -0.4225, -0.5664,  0.3708, -0.7921]],
       dtype=torch.float64)
	q_value: tensor([[-25.6297]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11498271293095663, distance: 1.2083444738882145 entropy 1.1324905157089233
epoch: 21, step: 66
	action: tensor([[-0.2424,  0.1407,  0.6498, -0.3229,  0.1127, -0.9397, -0.3346]],
       dtype=torch.float64)
	q_value: tensor([[-30.5000]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.28355761973903704, distance: 1.2964756902260879 entropy 1.1324905157089233
epoch: 21, step: 67
	action: tensor([[ 0.4207, -1.5105, -0.1830, -0.3795, -0.1910, -0.2712,  0.4572]],
       dtype=torch.float64)
	q_value: tensor([[-30.7550]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9248969848192679, distance: 1.587670734007184 entropy 1.1324905157089233
epoch: 21, step: 68
	action: tensor([[-1.2249,  0.3639, -0.4194, -0.5183,  1.3900,  2.1556,  0.4053]],
       dtype=torch.float64)
	q_value: tensor([[-34.3122]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0796388854708392, distance: 1.097831844131887 entropy 1.1324905157089233
epoch: 21, step: 69
	action: tensor([[ 0.2661, -1.6178,  1.4319,  1.3479,  0.6255, -0.2092, -0.7347]],
       dtype=torch.float64)
	q_value: tensor([[-51.9771]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 21, step: 70
	action: tensor([[-0.6710, -0.4987, -0.9295,  0.0769, -1.4313, -0.6987,  0.6091]],
       dtype=torch.float64)
	q_value: tensor([[-43.2704]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.517272589806782, distance: 1.409576008750286 entropy 1.1324905157089233
epoch: 21, step: 71
	action: tensor([[ 0.1692,  0.1641,  0.2250, -0.6718, -1.3311,  0.4824,  0.5146]],
       dtype=torch.float64)
	q_value: tensor([[-38.6606]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19945158189582757, distance: 1.0238833839234218 entropy 1.1324905157089233
epoch: 21, step: 72
	action: tensor([[ 1.5488, -0.0417, -0.2007,  0.0575, -0.5273,  0.2590,  0.7862]],
       dtype=torch.float64)
	q_value: tensor([[-33.4769]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.531979481657614, distance: 0.7828690787068472 entropy 1.1324905157089233
epoch: 21, step: 73
	action: tensor([[ 0.4134,  0.7539, -1.1955, -0.9867, -0.4414, -0.7516, -0.3921]],
       dtype=torch.float64)
	q_value: tensor([[-36.3357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8863945594467764, distance: 0.3857058788697331 entropy 1.1324905157089233
epoch: 21, step: 74
	action: tensor([[-0.5823,  0.5641, -0.1953,  1.0969,  0.9116,  0.4049, -1.0798]],
       dtype=torch.float64)
	q_value: tensor([[-31.8655]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 21, step: 75
	action: tensor([[-1.0771, -0.4062, -1.7953, -1.7048, -0.4708,  0.3612,  0.0872]],
       dtype=torch.float64)
	q_value: tensor([[-43.2704]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30467343401592395, distance: 0.9542256898788783 entropy 1.1324905157089233
epoch: 21, step: 76
	action: tensor([[ 0.1552, -1.7476,  0.5145,  0.3564, -1.0987, -1.1088,  0.0972]],
       dtype=torch.float64)
	q_value: tensor([[-46.9204]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 21, step: 77
	action: tensor([[-0.5071,  0.3563,  0.5966, -1.0722, -0.6609,  0.5421, -0.3597]],
       dtype=torch.float64)
	q_value: tensor([[-43.2704]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.637630283845404, distance: 1.4644165342860895 entropy 1.1324905157089233
epoch: 21, step: 78
	action: tensor([[ 1.7801, -0.7031,  0.6747, -1.3437,  0.0460,  0.2140, -0.3160]],
       dtype=torch.float64)
	q_value: tensor([[-32.9283]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21256938565316896, distance: 1.2601144532554978 entropy 1.1324905157089233
epoch: 21, step: 79
	action: tensor([[ 0.0607,  1.6786, -0.9683, -0.2103, -0.3795,  0.1486,  0.2303]],
       dtype=torch.float64)
	q_value: tensor([[-40.7124]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 21, step: 80
	action: tensor([[-0.0201,  0.3626,  0.2704, -0.5458, -0.6845, -0.3169,  0.1720]],
       dtype=torch.float64)
	q_value: tensor([[-43.2704]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2558734598217135, distance: 0.987143046112706 entropy 1.1324905157089233
epoch: 21, step: 81
	action: tensor([[-0.4069, -0.6939,  0.9574, -0.6120,  0.0052,  0.9494,  0.1575]],
       dtype=torch.float64)
	q_value: tensor([[-25.7192]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6936376301743599, distance: 1.4892477085078901 entropy 1.1324905157089233
epoch: 21, step: 82
	action: tensor([[-0.0052,  0.1997,  0.3091,  0.3721, -0.3747, -0.0052, -0.6244]],
       dtype=torch.float64)
	q_value: tensor([[-35.8204]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 21, step: 83
	action: tensor([[-0.9170, -1.0403, -0.3111,  0.4281, -0.4497,  1.2604, -1.1382]],
       dtype=torch.float64)
	q_value: tensor([[-43.2704]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9642960406426748, distance: 1.6038367622064076 entropy 1.1324905157089233
epoch: 21, step: 84
	action: tensor([[-0.0185, -1.0590, -2.3216,  0.9405, -0.5607,  0.3529,  0.2366]],
       dtype=torch.float64)
	q_value: tensor([[-43.6129]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1113156440227945, distance: 1.6627741955333075 entropy 1.1324905157089233
epoch: 21, step: 85
	action: tensor([[-0.7731,  0.6477, -0.0389, -0.6400, -1.6274, -0.4878, -0.7783]],
       dtype=torch.float64)
	q_value: tensor([[-42.6549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23606353281729242, distance: 1.2722635652685201 entropy 1.1324905157089233
epoch: 21, step: 86
	action: tensor([[ 0.0379,  1.0047, -0.2225,  0.4957, -0.6727, -0.1531, -0.7527]],
       dtype=torch.float64)
	q_value: tensor([[-39.4911]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 21, step: 87
	action: tensor([[-0.0508, -0.5476,  1.4372, -0.1047, -0.7958, -0.1958,  0.4921]],
       dtype=torch.float64)
	q_value: tensor([[-43.2704]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3766189855324904, distance: 1.3426523325407613 entropy 1.1324905157089233
epoch: 21, step: 88
	action: tensor([[-0.8488, -0.6882,  0.3175, -0.2405, -0.0113,  0.0427,  0.5489]],
       dtype=torch.float64)
	q_value: tensor([[-37.6581]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2697071621391416, distance: 1.7240172515188346 entropy 1.1324905157089233
epoch: 21, step: 89
	action: tensor([[-0.2304, -0.2927,  0.3018,  0.0228,  0.6413,  0.3303,  0.3250]],
       dtype=torch.float64)
	q_value: tensor([[-28.3951]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.01385095584856888, distance: 1.1522421306334747 entropy 1.1324905157089233
epoch: 21, step: 90
	action: tensor([[-1.2783,  0.3063, -0.5641, -0.2144, -0.4879, -1.0753,  0.1507]],
       dtype=torch.float64)
	q_value: tensor([[-26.9737]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6754474881329566, distance: 1.4812286501019554 entropy 1.1324905157089233
epoch: 21, step: 91
	action: tensor([[-2.4273e-01,  5.8460e-01, -1.9988e+00, -1.9045e-03,  9.4885e-01,
          1.3433e+00,  1.2368e+00]], dtype=torch.float64)
	q_value: tensor([[-34.7923]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 21, step: 92
	action: tensor([[-0.9045,  0.1611,  1.0029,  0.6002, -0.8183,  0.1310,  0.2785]],
       dtype=torch.float64)
	q_value: tensor([[-43.2704]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08489821559045363, distance: 1.1919312073900366 entropy 1.1324905157089233
epoch: 21, step: 93
	action: tensor([[-0.9399,  0.9973, -1.7488,  0.2559, -1.3190,  0.6028, -1.1504]],
       dtype=torch.float64)
	q_value: tensor([[-34.2238]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.38351589797783725, distance: 1.346011499533123 entropy 1.1324905157089233
epoch: 21, step: 94
	action: tensor([[-0.9050, -0.5277,  0.0306, -0.5262, -1.5430,  1.5785, -0.8599]],
       dtype=torch.float64)
	q_value: tensor([[-39.6164]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0556439993107518, distance: 1.6407055443233718 entropy 1.1324905157089233
epoch: 21, step: 95
	action: tensor([[ 0.5029, -0.6376,  1.0127,  0.0718, -0.5350, -0.0519,  0.6590]],
       dtype=torch.float64)
	q_value: tensor([[-46.9999]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08129345671768728, distance: 1.0968445913666907 entropy 1.1324905157089233
epoch: 21, step: 96
	action: tensor([[ 0.3035,  0.0471,  1.1160,  0.4113, -0.7268,  0.4162,  0.4046]],
       dtype=torch.float64)
	q_value: tensor([[-35.7062]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8756052390171833, distance: 0.40360611563418197 entropy 1.1324905157089233
epoch: 21, step: 97
	action: tensor([[-0.1025,  0.3481,  0.0121, -1.1831,  0.1765,  0.8049, -0.2812]],
       dtype=torch.float64)
	q_value: tensor([[-34.7060]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 21, step: 98
	action: tensor([[ 0.5067, -0.4028,  0.6720, -0.4842, -0.6168,  1.9010, -0.2959]],
       dtype=torch.float64)
	q_value: tensor([[-43.2704]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6404463780675953, distance: 0.6861807452180139 entropy 1.1324905157089233
epoch: 21, step: 99
	action: tensor([[-0.5952,  0.2744, -0.2569, -0.8760, -0.4243,  0.4499, -1.0189]],
       dtype=torch.float64)
	q_value: tensor([[-44.6401]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.36872913706498944, distance: 1.338799216445146 entropy 1.1324905157089233
epoch: 21, step: 100
	action: tensor([[ 0.2743, -1.0621, -0.9625, -0.3343,  0.1508,  0.1815,  0.0939]],
       dtype=torch.float64)
	q_value: tensor([[-32.4849]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5074448240024825, distance: 1.4050034987505622 entropy 1.1324905157089233
epoch: 21, step: 101
	action: tensor([[-0.6737, -0.8875,  0.8718, -0.0493, -0.6020,  1.1158, -0.6795]],
       dtype=torch.float64)
	q_value: tensor([[-33.3316]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6101355918143452, distance: 1.452071221766089 entropy 1.1324905157089233
epoch: 21, step: 102
	action: tensor([[ 1.2958, -1.0078,  0.1234, -0.2443, -0.1271,  0.9500, -0.9974]],
       dtype=torch.float64)
	q_value: tensor([[-42.0812]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20503808130103263, distance: 1.2561950538123656 entropy 1.1324905157089233
epoch: 21, step: 103
	action: tensor([[-0.7312, -0.4155,  0.4801, -0.1625,  0.2185,  1.4807, -0.4692]],
       dtype=torch.float64)
	q_value: tensor([[-42.9034]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17299178062690657, distance: 1.2393791271019654 entropy 1.1324905157089233
epoch: 21, step: 104
	action: tensor([[-0.0182, -0.1504, -0.0239, -0.0816, -0.6046,  0.4847,  0.8399]],
       dtype=torch.float64)
	q_value: tensor([[-38.8607]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1607370715661196, distance: 1.048348584898895 entropy 1.1324905157089233
epoch: 21, step: 105
	action: tensor([[ 0.9632,  0.9375,  0.9444,  0.3373,  0.6362, -1.2825, -1.0056]],
       dtype=torch.float64)
	q_value: tensor([[-28.0653]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 21, step: 106
	action: tensor([[-1.2946, -0.1922,  0.0514,  0.5633,  0.0139, -0.6200, -0.7229]],
       dtype=torch.float64)
	q_value: tensor([[-43.2704]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2384197419774505, distance: 1.7120934154066534 entropy 1.1324905157089233
epoch: 21, step: 107
	action: tensor([[ 0.0127, -0.1294, -1.5394,  0.3122,  0.5992,  0.4711,  0.8533]],
       dtype=torch.float64)
	q_value: tensor([[-36.6002]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.01552973020028503, distance: 1.135423806689834 entropy 1.1324905157089233
epoch: 21, step: 108
	action: tensor([[-0.2018, -0.0912, -0.6508, -0.8287,  0.2576,  0.5906, -1.3747]],
       dtype=torch.float64)
	q_value: tensor([[-35.1375]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12126997958369301, distance: 1.0727151768334902 entropy 1.1324905157089233
epoch: 21, step: 109
	action: tensor([[ 1.5173, -1.2872, -0.6796, -0.4864, -0.0752, -0.3509,  1.1960]],
       dtype=torch.float64)
	q_value: tensor([[-35.9087]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0926761259974973, distance: 1.6554181143701956 entropy 1.1324905157089233
epoch: 21, step: 110
	action: tensor([[ 2.3841, -0.2130,  0.3111, -0.2656,  1.0199,  0.6450, -0.1976]],
       dtype=torch.float64)
	q_value: tensor([[-46.0512]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 21, step: 111
	action: tensor([[ 1.1226, -1.7859,  0.1973,  1.0909,  0.5078,  0.1583, -0.1589]],
       dtype=torch.float64)
	q_value: tensor([[-43.2704]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 21, step: 112
	action: tensor([[ 0.4969, -1.3137, -0.0213, -1.0618,  0.0622,  0.2157,  2.1808]],
       dtype=torch.float64)
	q_value: tensor([[-43.2704]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0129542859866603, distance: 1.623579837510334 entropy 1.1324905157089233
epoch: 21, step: 113
	action: tensor([[ 1.4112, -0.8625, -0.4677,  0.3943, -0.8450,  0.3785,  0.3905]],
       dtype=torch.float64)
	q_value: tensor([[-50.9829]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1501172645405825, distance: 1.0549604946204991 entropy 1.1324905157089233
epoch: 21, step: 114
	action: tensor([[-0.0046, -2.0735,  1.1759,  0.0139,  0.6208, -0.1058, -1.5582]],
       dtype=torch.float64)
	q_value: tensor([[-39.2395]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 21, step: 115
	action: tensor([[ 1.2750,  0.5791, -0.1569, -0.1241,  0.6406,  0.6203, -0.6459]],
       dtype=torch.float64)
	q_value: tensor([[-43.2704]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 21, step: 116
	action: tensor([[ 0.3096,  0.3220,  0.2619, -0.8450, -0.9584,  1.4995, -0.7059]],
       dtype=torch.float64)
	q_value: tensor([[-43.2704]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5403642063993281, distance: 0.7758247212013701 entropy 1.1324905157089233
epoch: 21, step: 117
	action: tensor([[-0.4074, -1.4236, -0.5452,  0.3805,  0.0855, -0.7854,  0.2641]],
       dtype=torch.float64)
	q_value: tensor([[-40.0050]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9083606941149942, distance: 1.580836390601677 entropy 1.1324905157089233
epoch: 21, step: 118
	action: tensor([[ 0.4826,  1.0466, -1.5344,  0.0370,  0.0943, -0.2447, -0.5061]],
       dtype=torch.float64)
	q_value: tensor([[-36.6182]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 21, step: 119
	action: tensor([[ 0.0581,  0.0628, -1.4449, -0.1708,  0.7156, -0.0130,  0.0054]],
       dtype=torch.float64)
	q_value: tensor([[-43.2704]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 21, step: 120
	action: tensor([[ 0.3508,  1.1118, -0.2495,  0.1392,  0.3062,  0.4564,  0.5052]],
       dtype=torch.float64)
	q_value: tensor([[-43.2704]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 21, step: 121
	action: tensor([[ 0.6400,  0.0103,  1.2772,  0.7485, -0.5316,  0.1671, -0.9136]],
       dtype=torch.float64)
	q_value: tensor([[-43.2704]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 21, step: 122
	action: tensor([[ 1.4830,  0.3943,  0.7261, -0.5420,  0.3145,  0.0739,  0.6825]],
       dtype=torch.float64)
	q_value: tensor([[-43.2704]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 21, step: 123
	action: tensor([[ 0.2334, -0.5087,  1.0084,  1.0881, -0.3771,  0.8767, -0.6460]],
       dtype=torch.float64)
	q_value: tensor([[-43.2704]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 21, step: 124
	action: tensor([[-0.4911,  0.9824,  0.1260, -0.6596, -0.6690,  0.1423, -2.0809]],
       dtype=torch.float64)
	q_value: tensor([[-43.2704]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 21, step: 125
	action: tensor([[ 0.7780, -1.4161, -1.0406, -0.0663, -0.7790,  0.5880, -0.2386]],
       dtype=torch.float64)
	q_value: tensor([[-43.2704]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6322396912524448, distance: 1.4620043354062437 entropy 1.1324905157089233
epoch: 21, step: 126
	action: tensor([[ 0.8810,  0.8396, -0.0984,  1.1831,  0.0085, -0.2647,  0.2167]],
       dtype=torch.float64)
	q_value: tensor([[-38.2946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 21, step: 127
	action: tensor([[-0.3984,  2.5469,  0.2042,  0.5492, -0.4265,  0.4132,  0.6444]],
       dtype=torch.float64)
	q_value: tensor([[-43.2704]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
LOSS epoch 21 actor 454.0381965722394 critic 126.9228144372667 
epoch: 22, step: 0
	action: tensor([[ 1.2252,  0.4075, -0.3969, -0.1013,  0.8200,  0.5834,  0.0483]],
       dtype=torch.float64)
	q_value: tensor([[-39.1554]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 22, step: 1
	action: tensor([[-0.4319,  0.1929,  0.7838,  0.6063,  0.8147, -0.2128, -0.8648]],
       dtype=torch.float64)
	q_value: tensor([[-39.1554]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 22, step: 2
	action: tensor([[0.3608, 0.6934, 1.2508, 0.6911, 1.0642, 0.3613, 0.2704]],
       dtype=torch.float64)
	q_value: tensor([[-39.1554]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 22, step: 3
	action: tensor([[-0.4010, -0.1329,  0.0183,  0.0096, -0.6124,  0.2501,  0.7905]],
       dtype=torch.float64)
	q_value: tensor([[-39.1554]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.25056081705661315, distance: 1.2797027458928536 entropy 1.1324905157089233
epoch: 22, step: 4
	action: tensor([[ 1.5189, -0.6792, -0.2086, -1.5787, -0.0018, -0.6496,  0.0686]],
       dtype=torch.float64)
	q_value: tensor([[-24.6471]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.525920635424278, distance: 1.4135874029429245 entropy 1.1324905157089233
epoch: 22, step: 5
	action: tensor([[ 0.5506, -0.8242, -0.4241, -0.4213,  1.4249, -0.6130,  0.3882]],
       dtype=torch.float64)
	q_value: tensor([[-37.4600]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5190598803662747, distance: 1.4104059784044134 entropy 1.1324905157089233
epoch: 22, step: 6
	action: tensor([[-0.0615,  1.1648,  0.0018, -0.3656,  0.2566, -0.6616,  0.3834]],
       dtype=torch.float64)
	q_value: tensor([[-37.8937]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 22, step: 7
	action: tensor([[-0.0679, -1.0432, -0.1005,  0.3824,  0.1552, -0.2841,  0.4270]],
       dtype=torch.float64)
	q_value: tensor([[-39.1554]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.57251085886155, distance: 1.4350053487277015 entropy 1.1324905157089233
epoch: 22, step: 8
	action: tensor([[-0.7088, -0.2566,  0.3496,  0.0872,  0.3361,  0.4027,  0.6571]],
       dtype=torch.float64)
	q_value: tensor([[-28.7330]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5291364487568555, distance: 1.4150761568774886 entropy 1.1324905157089233
epoch: 22, step: 9
	action: tensor([[-0.7067,  0.6890, -0.9983,  0.2641,  0.0506, -0.8102, -0.2492]],
       dtype=torch.float64)
	q_value: tensor([[-25.0612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.012552448534176697, distance: 1.137139411820177 entropy 1.1324905157089233
epoch: 22, step: 10
	action: tensor([[-1.6591,  0.6111, -0.4184,  0.0902,  0.7166,  0.5459,  0.2064]],
       dtype=torch.float64)
	q_value: tensor([[-27.2924]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6722490612109846, distance: 1.4798141429911436 entropy 1.1324905157089233
epoch: 22, step: 11
	action: tensor([[-0.9776, -1.2493, -0.4490,  0.0814,  0.2693, -0.0306,  0.6162]],
       dtype=torch.float64)
	q_value: tensor([[-32.4329]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.402750894352684, distance: 1.773826207253386 entropy 1.1324905157089233
epoch: 22, step: 12
	action: tensor([[0.8130, 1.5096, 0.2274, 0.3459, 1.6656, 1.0044, 0.8031]],
       dtype=torch.float64)
	q_value: tensor([[-32.5343]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 22, step: 13
	action: tensor([[ 0.3359, -0.1598,  0.0211, -0.3639, -0.5596, -1.0029,  1.1863]],
       dtype=torch.float64)
	q_value: tensor([[-39.1554]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17520108729869677, distance: 1.0392756049402876 entropy 1.1324905157089233
epoch: 22, step: 14
	action: tensor([[ 0.2552,  0.2179, -0.9143, -1.3268,  0.4385,  0.0336,  0.5995]],
       dtype=torch.float64)
	q_value: tensor([[-31.8768]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5703828623755078, distance: 0.7500625670122514 entropy 1.1324905157089233
epoch: 22, step: 15
	action: tensor([[-0.4217,  0.5119,  1.1108, -0.0064,  0.9528,  0.7305,  0.3131]],
       dtype=torch.float64)
	q_value: tensor([[-33.1709]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 22, step: 16
	action: tensor([[ 0.0092,  0.5025, -1.0568,  0.2403,  0.2917,  0.0397, -0.1813]],
       dtype=torch.float64)
	q_value: tensor([[-39.1554]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 22, step: 17
	action: tensor([[ 0.2732, -0.4933,  0.3308, -0.3921,  0.6097, -0.1458, -1.1099]],
       dtype=torch.float64)
	q_value: tensor([[-39.1554]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2401906414745223, distance: 1.2743857839777735 entropy 1.1324905157089233
epoch: 22, step: 18
	action: tensor([[-0.2314,  0.5299, -0.3713,  0.5254, -1.4363, -0.1187,  0.9023]],
       dtype=torch.float64)
	q_value: tensor([[-32.3834]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 22, step: 19
	action: tensor([[ 0.9652, -0.2564, -0.2097,  0.0539, -0.3300,  0.6802,  0.0285]],
       dtype=torch.float64)
	q_value: tensor([[-39.1554]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7270485678748279, distance: 0.597859840495515 entropy 1.1324905157089233
epoch: 22, step: 20
	action: tensor([[ 0.1969,  0.3302,  1.9796, -1.1639,  0.4478,  1.1836,  1.5982]],
       dtype=torch.float64)
	q_value: tensor([[-25.9427]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 22, step: 21
	action: tensor([[-0.6574, -0.8176,  0.6684, -0.1998, -0.6929,  0.9579, -0.7888]],
       dtype=torch.float64)
	q_value: tensor([[-39.1554]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8666932896468882, distance: 1.5634830466609042 entropy 1.1324905157089233
epoch: 22, step: 22
	action: tensor([[ 1.3984, -0.8544, -0.3007, -1.2107, -0.8550,  0.1346, -0.4669]],
       dtype=torch.float64)
	q_value: tensor([[-36.1819]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8489007216453048, distance: 1.5560139608753716 entropy 1.1324905157089233
epoch: 22, step: 23
	action: tensor([[-1.0068,  1.3921,  0.1371,  0.4675, -0.1306, -0.9735,  1.6192]],
       dtype=torch.float64)
	q_value: tensor([[-36.6406]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 22, step: 24
	action: tensor([[ 0.4712, -0.4316,  0.0463, -0.3967, -0.3444,  0.0083, -0.3095]],
       dtype=torch.float64)
	q_value: tensor([[-39.1554]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.006725942177389421, distance: 1.1404893644811842 entropy 1.1324905157089233
epoch: 22, step: 25
	action: tensor([[-1.0002,  0.2474, -0.0587, -0.1381,  0.5914,  0.0342,  0.2972]],
       dtype=torch.float64)
	q_value: tensor([[-23.0026]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8165328845779962, distance: 1.5423336196692703 entropy 1.1324905157089233
epoch: 22, step: 26
	action: tensor([[-0.2469,  0.3973, -0.0723,  0.0424, -0.5059, -1.0983,  0.8703]],
       dtype=torch.float64)
	q_value: tensor([[-25.8619]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 22, step: 27
	action: tensor([[ 1.1414,  0.6081, -2.2442, -0.8506, -0.6063,  0.4525,  0.2210]],
       dtype=torch.float64)
	q_value: tensor([[-39.1554]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 22, step: 28
	action: tensor([[-0.5207,  0.4346, -0.8458,  0.5444, -0.2966,  0.4479,  0.2064]],
       dtype=torch.float64)
	q_value: tensor([[-39.1554]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18780365297506618, distance: 1.2471796661422845 entropy 1.1324905157089233
epoch: 22, step: 29
	action: tensor([[-0.7859, -0.5128,  0.1066, -0.7034,  1.0854,  0.3947, -0.1827]],
       dtype=torch.float64)
	q_value: tensor([[-21.8204]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9951324648077373, distance: 1.6163766238974009 entropy 1.1324905157089233
epoch: 22, step: 30
	action: tensor([[ 0.8119, -0.5500,  0.8918,  1.6287,  1.1544, -1.2854,  0.4623]],
       dtype=torch.float64)
	q_value: tensor([[-33.5131]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8882952500003233, distance: 0.3824657181137005 entropy 1.1324905157089233
epoch: 22, step: 31
	action: tensor([[ 0.9568,  1.1831,  0.2712,  0.2900,  0.5621,  0.3589, -0.3677]],
       dtype=torch.float64)
	q_value: tensor([[-50.4027]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 22, step: 32
	action: tensor([[ 0.3526, -0.3648,  0.0913,  0.2775, -0.5238, -0.4169,  0.6555]],
       dtype=torch.float64)
	q_value: tensor([[-39.1554]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3632524844430043, distance: 0.9131462140145602 entropy 1.1324905157089233
epoch: 22, step: 33
	action: tensor([[ 0.8728,  0.4403,  0.3485, -0.8660, -0.8272,  1.3189,  0.9879]],
       dtype=torch.float64)
	q_value: tensor([[-26.2620]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9055760250134866, distance: 0.351639644891947 entropy 1.1324905157089233
epoch: 22, step: 34
	action: tensor([[ 0.7557,  0.5223, -0.1772,  0.8358, -0.8518, -0.5476,  0.5873]],
       dtype=torch.float64)
	q_value: tensor([[-38.2591]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9375346455427034, distance: 0.2860067596529387 entropy 1.1324905157089233
epoch: 22, step: 35
	action: tensor([[-2.0987,  0.4658, -0.5248, -1.5710,  0.6309,  0.0705,  0.1273]],
       dtype=torch.float64)
	q_value: tensor([[-31.2404]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 22, step: 36
	action: tensor([[-1.1242e+00, -2.2180e-01,  4.3208e-01,  6.6932e-01, -4.3874e-05,
          9.6117e-02, -2.8047e-01]], dtype=torch.float64)
	q_value: tensor([[-39.1554]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6719838405526937, distance: 1.479696788210289 entropy 1.1324905157089233
epoch: 22, step: 37
	action: tensor([[ 1.1666,  0.8805, -1.7786, -0.8900, -0.6080,  0.2344,  1.7970]],
       dtype=torch.float64)
	q_value: tensor([[-28.5373]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 22, step: 38
	action: tensor([[ 0.6962, -0.0104,  0.6657, -1.5441, -0.1128, -0.4990, -0.7843]],
       dtype=torch.float64)
	q_value: tensor([[-39.1554]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07735110213472751, distance: 1.1877781268707386 entropy 1.1324905157089233
epoch: 22, step: 39
	action: tensor([[-0.0588,  0.1192, -0.4000, -0.8378, -0.8890,  0.5883, -0.4621]],
       dtype=torch.float64)
	q_value: tensor([[-32.6154]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.056511314882565, distance: 1.1115398604891211 entropy 1.1324905157089233
epoch: 22, step: 40
	action: tensor([[ 0.6220, -0.7094, -0.0512,  0.3056,  0.2168, -0.0104, -0.3144]],
       dtype=torch.float64)
	q_value: tensor([[-28.0248]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2066249618551843, distance: 1.0192857657666616 entropy 1.1324905157089233
epoch: 22, step: 41
	action: tensor([[-1.2265, -1.5209,  0.1164, -0.6120,  0.4937, -1.0356,  0.1505]],
       dtype=torch.float64)
	q_value: tensor([[-26.7442]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 22, step: 42
	action: tensor([[-0.0150, -1.0357, -0.5120,  0.1955,  0.8661, -0.6267,  0.2606]],
       dtype=torch.float64)
	q_value: tensor([[-39.1554]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7669950042869993, distance: 1.5211580991216087 entropy 1.1324905157089233
epoch: 22, step: 43
	action: tensor([[-0.0594,  1.2112,  0.1543, -0.2287, -0.6947,  0.0662, -0.3461]],
       dtype=torch.float64)
	q_value: tensor([[-33.5074]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 22, step: 44
	action: tensor([[-0.1546, -0.8054, -0.1502,  0.5317, -1.8211, -0.1575, -0.0580]],
       dtype=torch.float64)
	q_value: tensor([[-39.1554]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18254836574555755, distance: 1.244417613217051 entropy 1.1324905157089233
epoch: 22, step: 45
	action: tensor([[ 1.1087, -0.2739,  1.0430, -0.5293, -0.8632,  0.3980, -0.2904]],
       dtype=torch.float64)
	q_value: tensor([[-37.9963]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4944450433452796, distance: 0.8136560905083432 entropy 1.1324905157089233
epoch: 22, step: 46
	action: tensor([[ 0.8067,  0.5199,  0.7511,  1.6626, -0.6602,  0.7733,  0.2414]],
       dtype=torch.float64)
	q_value: tensor([[-33.3978]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4679748939161158, distance: 0.8346852841270643 entropy 1.1324905157089233
epoch: 22, step: 47
	action: tensor([[-0.4052, -0.5289, -1.0462, -0.0071, -0.2602,  0.4586,  0.2767]],
       dtype=torch.float64)
	q_value: tensor([[-37.5513]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 22, step: 48
	action: tensor([[-0.5359,  0.3931, -1.1407,  0.3228, -0.2179, -1.7264,  1.2794]],
       dtype=torch.float64)
	q_value: tensor([[-39.1554]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 22, step: 49
	action: tensor([[ 0.2767,  1.0000, -0.6791,  1.3592, -0.3813, -1.0506, -0.7478]],
       dtype=torch.float64)
	q_value: tensor([[-39.1554]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 22, step: 50
	action: tensor([[-0.0096, -0.8371,  0.3135,  0.0407,  0.7007,  0.0186, -0.8772]],
       dtype=torch.float64)
	q_value: tensor([[-39.1554]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.43565894246739467, distance: 1.3711416823805456 entropy 1.1324905157089233
epoch: 22, step: 51
	action: tensor([[-0.5086,  0.0381,  0.0945, -0.9152,  0.5530, -1.3209,  0.1791]],
       dtype=torch.float64)
	q_value: tensor([[-33.1526]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5098524371116078, distance: 1.406125050662525 entropy 1.1324905157089233
epoch: 22, step: 52
	action: tensor([[-0.3447, -0.8145,  0.2539, -0.5409, -0.2534,  0.4710, -0.6075]],
       dtype=torch.float64)
	q_value: tensor([[-31.4114]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9279831772074487, distance: 1.5889429825891042 entropy 1.1324905157089233
epoch: 22, step: 53
	action: tensor([[-0.7701, -0.3481, -0.0505,  0.8776, -1.2349, -0.1252,  0.0541]],
       dtype=torch.float64)
	q_value: tensor([[-29.5270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4010549670265233, distance: 1.3545164394452716 entropy 1.1324905157089233
epoch: 22, step: 54
	action: tensor([[ 1.0230, -0.1235,  0.2941,  1.6197, -1.0879,  0.9685, -0.4972]],
       dtype=torch.float64)
	q_value: tensor([[-31.7868]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 22, step: 55
	action: tensor([[ 0.2350,  0.9490, -0.1185, -0.3404,  0.5170, -0.4126,  1.0942]],
       dtype=torch.float64)
	q_value: tensor([[-39.1554]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 22, step: 56
	action: tensor([[ 0.0545, -0.2391,  0.3421, -0.7427, -0.2287,  1.0802,  1.0696]],
       dtype=torch.float64)
	q_value: tensor([[-39.1554]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07173432480523245, distance: 1.102536152734314 entropy 1.1324905157089233
epoch: 22, step: 57
	action: tensor([[-0.0881, -0.6778, -0.3034,  0.8071,  0.1933,  0.6801, -0.6051]],
       dtype=torch.float64)
	q_value: tensor([[-34.1944]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24439439560061227, distance: 0.9947278527128152 entropy 1.1324905157089233
epoch: 22, step: 58
	action: tensor([[ 0.2322,  1.2282,  0.6949,  0.7815,  0.1961, -0.2370,  0.2988]],
       dtype=torch.float64)
	q_value: tensor([[-28.4506]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 22, step: 59
	action: tensor([[-1.2608, -1.0312, -0.1410,  1.2574,  0.4383,  0.5372, -0.2836]],
       dtype=torch.float64)
	q_value: tensor([[-39.1554]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8209657027128068, distance: 1.5442143229115268 entropy 1.1324905157089233
epoch: 22, step: 60
	action: tensor([[ 0.0174,  0.6911, -1.2712,  0.3467, -0.3919, -0.8397,  1.1268]],
       dtype=torch.float64)
	q_value: tensor([[-35.3854]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 22, step: 61
	action: tensor([[ 0.7601,  0.5434, -0.4650, -0.4027, -0.4248,  0.6023,  0.4017]],
       dtype=torch.float64)
	q_value: tensor([[-39.1554]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 22, step: 62
	action: tensor([[-0.3594, -1.8518,  0.3589, -1.4251,  0.1734,  0.4748,  0.7158]],
       dtype=torch.float64)
	q_value: tensor([[-39.1554]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 22, step: 63
	action: tensor([[ 0.1588, -0.0118, -0.3971, -1.2433, -0.2011,  0.6956,  0.3103]],
       dtype=torch.float64)
	q_value: tensor([[-39.1554]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19732046049404917, distance: 1.0252453062610944 entropy 1.1324905157089233
epoch: 22, step: 64
	action: tensor([[ 1.3243, -0.8954,  0.7289, -0.7018,  0.1106,  0.0669,  0.1961]],
       dtype=torch.float64)
	q_value: tensor([[-30.4526]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4538490999135041, distance: 1.3798006945741557 entropy 1.1324905157089233
epoch: 22, step: 65
	action: tensor([[ 0.1685, -1.4895, -0.3060, -0.6211,  0.5056, -0.1255,  1.6177]],
       dtype=torch.float64)
	q_value: tensor([[-34.5320]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 22, step: 66
	action: tensor([[-0.7435,  0.7157,  0.6767, -0.5221, -0.9007,  0.1345, -0.4426]],
       dtype=torch.float64)
	q_value: tensor([[-39.1554]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 22, step: 67
	action: tensor([[ 2.1713e-01, -1.3468e+00,  5.4685e-04, -4.1910e-02,  9.6476e-01,
          1.6582e+00,  5.5262e-01]], dtype=torch.float64)
	q_value: tensor([[-39.1554]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10922458540673896, distance: 1.080042397604222 entropy 1.1324905157089233
epoch: 22, step: 68
	action: tensor([[ 0.3480, -0.1955, -1.6529,  0.3927,  1.0241,  0.0732,  0.9123]],
       dtype=torch.float64)
	q_value: tensor([[-41.3938]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25840946609160653, distance: 0.9854595034770132 entropy 1.1324905157089233
epoch: 22, step: 69
	action: tensor([[-0.1357, -0.4767,  0.3997, -0.8140, -0.5083,  0.8472,  1.4097]],
       dtype=torch.float64)
	q_value: tensor([[-35.5882]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.48583878214222453, distance: 1.3948982780381414 entropy 1.1324905157089233
epoch: 22, step: 70
	action: tensor([[-0.1167,  0.5148, -0.0739,  0.9450, -0.1708, -0.3576, -0.7084]],
       dtype=torch.float64)
	q_value: tensor([[-36.7134]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 22, step: 71
	action: tensor([[-0.2394,  0.1732, -0.3040,  0.3272, -0.6208, -0.3083, -0.4267]],
       dtype=torch.float64)
	q_value: tensor([[-39.1554]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 22, step: 72
	action: tensor([[-0.0401,  0.1727,  0.1422,  1.6514,  0.0592, -0.5274, -0.2719]],
       dtype=torch.float64)
	q_value: tensor([[-39.1554]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 22, step: 73
	action: tensor([[-0.0864, -1.1182,  1.0510, -0.8402,  0.5294,  0.1499, -0.8801]],
       dtype=torch.float64)
	q_value: tensor([[-39.1554]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1650827367127814, distance: 1.6838133243748175 entropy 1.1324905157089233
epoch: 22, step: 74
	action: tensor([[ 0.4348, -0.4392, -0.0716, -0.5487, -0.7845,  0.1956,  2.3551]],
       dtype=torch.float64)
	q_value: tensor([[-37.9330]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0796022820947635, distance: 1.189018440708351 entropy 1.1324905157089233
epoch: 22, step: 75
	action: tensor([[-0.6365, -0.0502, -1.8399, -0.1803, -0.4187,  0.2674, -1.0298]],
       dtype=torch.float64)
	q_value: tensor([[-43.0799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4398076723452091, distance: 1.3731213979683512 entropy 1.1324905157089233
epoch: 22, step: 76
	action: tensor([[ 0.1989, -0.3571, -0.0829, -0.6219,  0.3104,  0.8277,  0.5663]],
       dtype=torch.float64)
	q_value: tensor([[-31.8513]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15516024412373275, distance: 1.0518259093432631 entropy 1.1324905157089233
epoch: 22, step: 77
	action: tensor([[-0.7904, -0.1825, -0.2256, -0.1890, -0.5658, -0.0764,  0.3149]],
       dtype=torch.float64)
	q_value: tensor([[-28.9323]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8110446584222268, distance: 1.5400019583581286 entropy 1.1324905157089233
epoch: 22, step: 78
	action: tensor([[-0.9500,  1.0731,  0.0456,  0.4610,  0.2542, -0.7928,  0.8040]],
       dtype=torch.float64)
	q_value: tensor([[-24.5764]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 22, step: 79
	action: tensor([[-1.3159,  0.6295, -0.2339, -0.1035, -1.6783,  1.4510,  0.0678]],
       dtype=torch.float64)
	q_value: tensor([[-39.1554]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 22, step: 80
	action: tensor([[-0.7279,  0.2459,  0.1189,  0.4485, -0.1849,  1.5754,  0.8876]],
       dtype=torch.float64)
	q_value: tensor([[-39.1554]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3014076573490385, distance: 0.9564639456890415 entropy 1.1324905157089233
epoch: 22, step: 81
	action: tensor([[-0.6192, -0.5576,  0.9008, -0.5753, -1.1269,  0.3080, -1.1469]],
       dtype=torch.float64)
	q_value: tensor([[-33.5431]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.159420330303099, distance: 1.6816100188640286 entropy 1.1324905157089233
epoch: 22, step: 82
	action: tensor([[-1.0247, -0.0934, -0.3247,  0.0330,  1.5129,  1.5921,  0.9556]],
       dtype=torch.float64)
	q_value: tensor([[-38.7677]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10359380250326233, distance: 1.202157360181179 entropy 1.1324905157089233
epoch: 22, step: 83
	action: tensor([[ 0.1697,  1.6121, -0.5843,  0.0743, -1.9421, -0.8583, -0.2291]],
       dtype=torch.float64)
	q_value: tensor([[-42.3215]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 22, step: 84
	action: tensor([[ 0.8145,  0.1557, -1.1748,  0.7189, -1.1611,  1.7124,  0.2500]],
       dtype=torch.float64)
	q_value: tensor([[-39.1554]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 22, step: 85
	action: tensor([[-0.5403, -0.1154, -0.3429,  1.3318,  0.3271,  0.2372, -0.4282]],
       dtype=torch.float64)
	q_value: tensor([[-39.1554]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0779425050075554, distance: 1.0988431228034667 entropy 1.1324905157089233
epoch: 22, step: 86
	action: tensor([[ 0.4591,  0.3191, -0.1364, -0.5320,  0.3643,  0.1335, -0.4560]],
       dtype=torch.float64)
	q_value: tensor([[-28.7086]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 22, step: 87
	action: tensor([[ 0.1857, -0.5660, -0.3626,  0.9616,  0.1245, -0.2320, -0.6019]],
       dtype=torch.float64)
	q_value: tensor([[-39.1554]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34922695154226013, distance: 0.9231482962736205 entropy 1.1324905157089233
epoch: 22, step: 88
	action: tensor([[-1.4085, -1.3111, -0.6021,  0.1265, -0.0136, -0.0564,  0.4554]],
       dtype=torch.float64)
	q_value: tensor([[-29.1908]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.5902398793369517, distance: 1.8417330686885012 entropy 1.1324905157089233
epoch: 22, step: 89
	action: tensor([[ 0.5062, -0.9361, -0.3672, -0.5565, -0.8865,  1.0403,  0.0704]],
       dtype=torch.float64)
	q_value: tensor([[-34.7728]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.483191406178918, distance: 1.3936550521263478 entropy 1.1324905157089233
epoch: 22, step: 90
	action: tensor([[ 0.3305, -0.1642, -0.2307, -0.0514, -0.6230,  0.2911,  0.2426]],
       dtype=torch.float64)
	q_value: tensor([[-33.9788]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4019029139536754, distance: 0.8849985039195959 entropy 1.1324905157089233
epoch: 22, step: 91
	action: tensor([[-0.1615, -1.8548, -1.0051, -0.8272, -0.6636,  1.6728,  0.5207]],
       dtype=torch.float64)
	q_value: tensor([[-21.8669]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 22, step: 92
	action: tensor([[ 0.0772, -0.0877, -0.6485,  0.2052, -1.3329, -0.3469, -0.5336]],
       dtype=torch.float64)
	q_value: tensor([[-39.1554]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.43532560053307146, distance: 0.8599154315113597 entropy 1.1324905157089233
epoch: 22, step: 93
	action: tensor([[-0.2415, -0.9641,  0.4324,  0.3851, -0.8148,  0.7504,  1.6636]],
       dtype=torch.float64)
	q_value: tensor([[-29.3942]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.28973032095240736, distance: 1.2995893637305176 entropy 1.1324905157089233
epoch: 22, step: 94
	action: tensor([[-0.5682, -0.8396,  0.0626,  0.3651, -0.5298,  0.1447,  0.1320]],
       dtype=torch.float64)
	q_value: tensor([[-37.3454]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7637144173512991, distance: 1.5197453589157495 entropy 1.1324905157089233
epoch: 22, step: 95
	action: tensor([[ 0.4720, -0.9782, -0.4603, -0.3080,  0.0822, -0.0321, -0.4583]],
       dtype=torch.float64)
	q_value: tensor([[-27.4773]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5561655381360768, distance: 1.4275278508077003 entropy 1.1324905157089233
epoch: 22, step: 96
	action: tensor([[ 1.0355,  0.1743,  0.4723,  0.1897,  1.2106, -0.2075,  1.2963]],
       dtype=torch.float64)
	q_value: tensor([[-29.3903]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8988753173181546, distance: 0.3639027011080512 entropy 1.1324905157089233
epoch: 22, step: 97
	action: tensor([[-0.1412, -0.2771, -1.0251,  0.8105,  0.1232,  0.8323, -0.1457]],
       dtype=torch.float64)
	q_value: tensor([[-38.6846]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 22, step: 98
	action: tensor([[-0.8438,  0.2862, -0.7860,  0.1533,  0.1950,  1.5730, -0.3608]],
       dtype=torch.float64)
	q_value: tensor([[-39.1554]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18925625092752663, distance: 1.2479420383928876 entropy 1.1324905157089233
epoch: 22, step: 99
	action: tensor([[ 0.1608,  0.9937, -0.0389, -0.3250,  1.0336, -0.0261,  0.3756]],
       dtype=torch.float64)
	q_value: tensor([[-32.2023]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 22, step: 100
	action: tensor([[ 0.0033, -0.7337,  0.2091, -1.0969,  0.3365,  0.3538, -0.0297]],
       dtype=torch.float64)
	q_value: tensor([[-39.1554]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7502540353193063, distance: 1.513935026908157 entropy 1.1324905157089233
epoch: 22, step: 101
	action: tensor([[-0.7398,  0.0280,  0.4323, -0.4229, -0.7041, -0.3255,  0.4200]],
       dtype=torch.float64)
	q_value: tensor([[-29.8814]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8284655906568132, distance: 1.5473910807008646 entropy 1.1324905157089233
epoch: 22, step: 102
	action: tensor([[ 0.5564,  0.2106, -0.2345, -0.8078,  1.3564,  0.3122,  0.2736]],
       dtype=torch.float64)
	q_value: tensor([[-26.1916]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5913079196835496, distance: 0.7315681748577646 entropy 1.1324905157089233
epoch: 22, step: 103
	action: tensor([[ 1.3628,  1.0188, -0.8849, -0.0270,  0.4587, -0.7242,  0.1423]],
       dtype=torch.float64)
	q_value: tensor([[-33.0321]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8030425959578169, distance: 0.5078586443078791 entropy 1.1324905157089233
epoch: 22, step: 104
	action: tensor([[ 1.3191,  1.1230, -0.8868, -2.0120,  0.5145, -0.7477, -0.0805]],
       dtype=torch.float64)
	q_value: tensor([[-31.3483]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2794445853736637, distance: 0.9713827485875711 entropy 1.1324905157089233
epoch: 22, step: 105
	action: tensor([[ 0.1984,  1.5693, -0.5977,  0.6333,  1.1846, -0.2906, -0.8539]],
       dtype=torch.float64)
	q_value: tensor([[-40.4207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 22, step: 106
	action: tensor([[-0.6282,  0.4955,  0.9599,  0.1091, -0.8817,  1.0621, -0.4052]],
       dtype=torch.float64)
	q_value: tensor([[-39.1554]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.254074966851213, distance: 0.9883352481757953 entropy 1.1324905157089233
epoch: 22, step: 107
	action: tensor([[-0.7604, -1.3736,  0.1172,  0.8563,  0.5032, -0.7754,  0.1225]],
       dtype=torch.float64)
	q_value: tensor([[-32.9147]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0686144946629863, distance: 1.6458735842000274 entropy 1.1324905157089233
epoch: 22, step: 108
	action: tensor([[ 0.4658, -0.1714,  0.6459, -1.0765, -0.6558,  0.0125,  0.8900]],
       dtype=torch.float64)
	q_value: tensor([[-36.8047]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14716076545543388, distance: 1.2256566779823799 entropy 1.1324905157089233
epoch: 22, step: 109
	action: tensor([[ 0.1797, -0.9585,  0.9531, -0.8833,  0.3645, -0.6763,  0.4206]],
       dtype=torch.float64)
	q_value: tensor([[-31.0378]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8255322047913751, distance: 1.5461493518125402 entropy 1.1324905157089233
epoch: 22, step: 110
	action: tensor([[-0.2443, -0.7506,  0.8185,  0.8814,  1.1753,  0.0113,  0.2866]],
       dtype=torch.float64)
	q_value: tensor([[-33.4415]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19699836393104753, distance: 1.0254509891342574 entropy 1.1324905157089233
epoch: 22, step: 111
	action: tensor([[ 0.1087, -1.0041,  0.3600, -1.0504,  0.7361,  0.6257,  0.0955]],
       dtype=torch.float64)
	q_value: tensor([[-36.1624]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8589393485501959, distance: 1.5602324400568883 entropy 1.1324905157089233
epoch: 22, step: 112
	action: tensor([[-0.3967, -0.5590, -0.2488, -1.2533,  0.7648,  0.4075, -0.1714]],
       dtype=torch.float64)
	q_value: tensor([[-34.8583]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4852511198638967, distance: 1.3946224035023214 entropy 1.1324905157089233
epoch: 22, step: 113
	action: tensor([[ 0.9147,  0.0259, -0.2188,  0.1026, -0.0750,  0.3772,  1.3275]],
       dtype=torch.float64)
	q_value: tensor([[-34.3423]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8674406812679383, distance: 0.41664082737420605 entropy 1.1324905157089233
epoch: 22, step: 114
	action: tensor([[-0.9320, -0.0130, -0.0702,  0.2198, -0.0358,  0.2604, -0.8072]],
       dtype=torch.float64)
	q_value: tensor([[-32.3175]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6853825402616192, distance: 1.4856138464682045 entropy 1.1324905157089233
epoch: 22, step: 115
	action: tensor([[ 0.3384,  0.3550, -0.4286,  0.0763, -0.6332,  0.0351,  0.2884]],
       dtype=torch.float64)
	q_value: tensor([[-26.7211]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 22, step: 116
	action: tensor([[-0.0134,  1.3812,  0.4835, -0.4148, -0.4493, -0.0099, -0.7683]],
       dtype=torch.float64)
	q_value: tensor([[-39.1554]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 22, step: 117
	action: tensor([[-0.8059, -0.2637,  0.8248,  0.1605, -2.3028,  0.3375,  0.0270]],
       dtype=torch.float64)
	q_value: tensor([[-39.1554]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8352078223406527, distance: 1.5502413584008896 entropy 1.1324905157089233
epoch: 22, step: 118
	action: tensor([[-0.8628,  1.4541,  0.5976, -0.3505,  0.9148,  0.9565,  0.3246]],
       dtype=torch.float64)
	q_value: tensor([[-45.1469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 22, step: 119
	action: tensor([[-0.3902,  0.6486,  0.3375, -1.1175, -0.6884,  0.9542,  1.5220]],
       dtype=torch.float64)
	q_value: tensor([[-39.1554]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18422689704271522, distance: 1.2453004748734058 entropy 1.1324905157089233
epoch: 22, step: 120
	action: tensor([[-0.2882,  0.0031,  1.0337,  0.2610, -0.5578,  0.0350,  0.3110]],
       dtype=torch.float64)
	q_value: tensor([[-38.7449]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20227536603610397, distance: 1.0220760105866593 entropy 1.1324905157089233
epoch: 22, step: 121
	action: tensor([[-0.8608,  0.9773,  1.7476, -0.0295,  1.0227, -0.5754,  0.4989]],
       dtype=torch.float64)
	q_value: tensor([[-27.6849]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 22, step: 122
	action: tensor([[ 0.1450, -0.2674, -0.5097, -0.1484, -0.1954,  0.6556,  0.7966]],
       dtype=torch.float64)
	q_value: tensor([[-39.1554]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23143956874250549, distance: 1.0032188966975826 entropy 1.1324905157089233
epoch: 22, step: 123
	action: tensor([[ 0.4757,  0.2815, -0.2080, -1.0836, -0.1111,  0.8861,  0.1285]],
       dtype=torch.float64)
	q_value: tensor([[-25.7927]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6166799195859116, distance: 0.708496123812933 entropy 1.1324905157089233
epoch: 22, step: 124
	action: tensor([[ 0.3149,  1.2263, -0.6811,  0.4323,  0.4009, -0.2976, -0.4661]],
       dtype=torch.float64)
	q_value: tensor([[-29.3792]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 22, step: 125
	action: tensor([[ 1.2463, -0.6788, -0.2911, -0.0189, -0.0290, -0.7340,  0.5694]],
       dtype=torch.float64)
	q_value: tensor([[-39.1554]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2599428945195068, distance: 1.2844941305085635 entropy 1.1324905157089233
epoch: 22, step: 126
	action: tensor([[-0.1482, -0.8364,  0.8408,  0.7869,  0.8270, -0.6251,  0.7334]],
       dtype=torch.float64)
	q_value: tensor([[-32.9277]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.051442349955884925, distance: 1.1734090300488766 entropy 1.1324905157089233
epoch: 22, step: 127
	action: tensor([[ 0.7858,  0.2542,  0.6674, -0.2504, -0.3094,  0.4842, -0.9447]],
       dtype=torch.float64)
	q_value: tensor([[-36.6988]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9034724300154672, distance: 0.3555350158783125 entropy 1.1324905157089233
LOSS epoch 22 actor 385.63555272979596 critic 206.57872784576313 
epoch: 23, step: 0
	action: tensor([[ 1.1067,  1.0445, -0.3729, -0.4288,  0.4206, -0.0446,  0.6873]],
       dtype=torch.float64)
	q_value: tensor([[-27.0443]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.911958814826396, distance: 0.3395467933440137 entropy 1.1324905157089233
epoch: 23, step: 1
	action: tensor([[-0.3210, -0.7046,  1.0143, -0.0930,  0.5075, -0.3311, -0.2334]],
       dtype=torch.float64)
	q_value: tensor([[-28.3120]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7797804389312013, distance: 1.5266514969438472 entropy 1.1324905157089233
epoch: 23, step: 2
	action: tensor([[ 0.4865,  0.4649,  0.6832,  0.0353, -0.4301, -1.3850,  0.3257]],
       dtype=torch.float64)
	q_value: tensor([[-29.4939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 23, step: 3
	action: tensor([[ 0.0673,  0.3189,  0.4650, -0.2041, -0.8951,  0.9389, -0.0419]],
       dtype=torch.float64)
	q_value: tensor([[-36.7619]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 23, step: 4
	action: tensor([[-0.1444,  2.3698,  0.7211, -0.4442, -0.2739,  0.0784,  0.9834]],
       dtype=torch.float64)
	q_value: tensor([[-36.7619]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 23, step: 5
	action: tensor([[ 1.7581, -0.3465, -0.0815,  0.4959, -0.8424, -0.3615,  0.9427]],
       dtype=torch.float64)
	q_value: tensor([[-36.7619]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 23, step: 6
	action: tensor([[-0.9724, -0.3690, -0.5586, -0.4424,  0.7424,  1.0213, -0.7121]],
       dtype=torch.float64)
	q_value: tensor([[-36.7619]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7315942800391944, distance: 1.505843243032516 entropy 1.1324905157089233
epoch: 23, step: 7
	action: tensor([[ 0.6601, -0.0846, -0.8154, -0.1547, -0.4211,  1.4331, -1.1238]],
       dtype=torch.float64)
	q_value: tensor([[-32.2835]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6765394767061577, distance: 0.6508296585495937 entropy 1.1324905157089233
epoch: 23, step: 8
	action: tensor([[-0.3948, -0.2769,  1.1796,  0.2975,  1.4112,  0.4209,  0.9682]],
       dtype=torch.float64)
	q_value: tensor([[-31.2576]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1195157174347583, distance: 1.210798267561929 entropy 1.1324905157089233
epoch: 23, step: 9
	action: tensor([[-0.1774, -0.0503, -0.5826,  0.4196,  0.3334,  0.0014, -0.6144]],
       dtype=torch.float64)
	q_value: tensor([[-36.4293]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06875473356422901, distance: 1.1043042215105923 entropy 1.1324905157089233
epoch: 23, step: 10
	action: tensor([[ 0.0819, -1.4243, -0.4125, -0.7243, -0.5251,  0.7511,  0.3531]],
       dtype=torch.float64)
	q_value: tensor([[-21.8650]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8853444077425849, distance: 1.5712744253345654 entropy 1.1324905157089233
epoch: 23, step: 11
	action: tensor([[ 0.6532, -0.7825, -0.4074, -0.6294,  0.1749,  0.4109, -0.5575]],
       dtype=torch.float64)
	q_value: tensor([[-32.7119]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3551493333984581, distance: 1.3321412335928575 entropy 1.1324905157089233
epoch: 23, step: 12
	action: tensor([[-1.6257, -0.3841,  0.5073,  1.1787, -0.4925,  0.1812, -0.2565]],
       dtype=torch.float64)
	q_value: tensor([[-28.6472]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6828878259606399, distance: 1.4845139316077214 entropy 1.1324905157089233
epoch: 23, step: 13
	action: tensor([[ 0.4496, -0.5963,  0.4200,  0.4976, -1.1574,  1.3202,  0.2583]],
       dtype=torch.float64)
	q_value: tensor([[-34.8159]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6117338622528377, distance: 0.7130524083905885 entropy 1.1324905157089233
epoch: 23, step: 14
	action: tensor([[-0.8345, -0.1401,  0.2914, -1.8881, -0.1611,  0.7386, -0.2115]],
       dtype=torch.float64)
	q_value: tensor([[-34.0646]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8821911322905764, distance: 1.569959881827387 entropy 1.1324905157089233
epoch: 23, step: 15
	action: tensor([[-0.3050,  0.1834,  1.2084,  0.9839,  0.2701, -0.7483, -0.1980]],
       dtype=torch.float64)
	q_value: tensor([[-36.5830]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 23, step: 16
	action: tensor([[-0.3868, -0.1859,  2.0178, -0.0142,  0.6727, -0.7610,  0.4343]],
       dtype=torch.float64)
	q_value: tensor([[-36.7619]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15654344331062942, distance: 1.2306588202975006 entropy 1.1324905157089233
epoch: 23, step: 17
	action: tensor([[-1.2184,  0.5570, -0.1804, -0.7419,  0.0791,  1.1101, -0.2089]],
       dtype=torch.float64)
	q_value: tensor([[-38.1410]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5968038073107123, distance: 1.446047213736491 entropy 1.1324905157089233
epoch: 23, step: 18
	action: tensor([[-0.0970, -0.0323, -0.2907,  0.4506,  0.1069, -1.0109, -1.4221]],
       dtype=torch.float64)
	q_value: tensor([[-32.3185]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20336737198688692, distance: 1.0213762106419912 entropy 1.1324905157089233
epoch: 23, step: 19
	action: tensor([[-1.5449,  1.0994, -1.1277, -0.4526, -0.0770, -0.1728,  0.5392]],
       dtype=torch.float64)
	q_value: tensor([[-33.2378]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2601034603428245, distance: 1.284575975204854 entropy 1.1324905157089233
epoch: 23, step: 20
	action: tensor([[ 0.1547, -0.5876, -0.5899,  0.4944,  0.3687, -0.2002,  0.2484]],
       dtype=torch.float64)
	q_value: tensor([[-33.5132]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.017378557856741805, distance: 1.1542449507333516 entropy 1.1324905157089233
epoch: 23, step: 21
	action: tensor([[-0.4014,  0.3845,  0.2071,  1.0094, -0.9441, -0.7884,  1.0463]],
       dtype=torch.float64)
	q_value: tensor([[-24.3167]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 23, step: 22
	action: tensor([[ 1.1316,  0.4888,  0.2726,  0.3880,  0.0775,  0.0953, -0.1936]],
       dtype=torch.float64)
	q_value: tensor([[-36.7619]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 23, step: 23
	action: tensor([[-0.5025,  0.1029, -1.1567,  0.7423, -2.1111,  0.1499,  1.3442]],
       dtype=torch.float64)
	q_value: tensor([[-36.7619]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 23, step: 24
	action: tensor([[ 0.8320, -0.4822, -0.1925, -0.7763, -0.1053,  0.2490,  0.2995]],
       dtype=torch.float64)
	q_value: tensor([[-36.7619]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15775352760283745, distance: 1.231302467358203 entropy 1.1324905157089233
epoch: 23, step: 25
	action: tensor([[-0.2787,  0.7235, -0.7935,  2.3910,  0.8214, -0.5762,  0.5083]],
       dtype=torch.float64)
	q_value: tensor([[-25.7512]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 23, step: 26
	action: tensor([[-0.3787,  1.6457,  0.0416, -1.1409,  0.2809,  0.0662, -0.7284]],
       dtype=torch.float64)
	q_value: tensor([[-36.7619]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 23, step: 27
	action: tensor([[-1.3391, -0.4882, -0.3731, -0.6808, -0.0997,  0.1551, -0.0965]],
       dtype=torch.float64)
	q_value: tensor([[-36.7619]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.3821241836229756, distance: 1.7661959816246797 entropy 1.1324905157089233
epoch: 23, step: 28
	action: tensor([[ 0.2737, -0.1833, -0.2749,  0.3031, -0.1933,  0.5199, -0.4885]],
       dtype=torch.float64)
	q_value: tensor([[-29.4666]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5539825477277472, distance: 0.7642450237502372 entropy 1.1324905157089233
epoch: 23, step: 29
	action: tensor([[-0.4510,  0.0895, -0.2522,  0.1143, -0.2523,  1.2313,  0.4203]],
       dtype=torch.float64)
	q_value: tensor([[-20.9446]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 23, step: 30
	action: tensor([[-0.0880,  0.6389, -0.7440,  0.7602,  1.9301,  0.3261,  0.3547]],
       dtype=torch.float64)
	q_value: tensor([[-36.7619]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 23, step: 31
	action: tensor([[ 0.2964,  0.4294, -0.6512, -0.7494,  0.1517,  1.1703, -0.3507]],
       dtype=torch.float64)
	q_value: tensor([[-36.7619]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 23, step: 32
	action: tensor([[ 0.6151, -0.6598,  0.0675, -0.0952,  0.9789,  0.3801, -0.1869]],
       dtype=torch.float64)
	q_value: tensor([[-36.7619]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09748105627930148, distance: 1.0871384507864075 entropy 1.1324905157089233
epoch: 23, step: 33
	action: tensor([[ 0.0776,  1.1974,  1.0099,  0.6140, -0.2109, -0.6314, -0.3593]],
       dtype=torch.float64)
	q_value: tensor([[-29.5619]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 23, step: 34
	action: tensor([[-0.1204, -1.0277,  1.3639,  1.3100,  0.6346, -1.1053,  0.6366]],
       dtype=torch.float64)
	q_value: tensor([[-36.7619]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15163085753397287, distance: 1.0540206635566787 entropy 1.1324905157089233
epoch: 23, step: 35
	action: tensor([[-1.5838, -0.0745,  0.7531,  0.3026,  0.5573,  0.4724,  0.8591]],
       dtype=torch.float64)
	q_value: tensor([[-44.7674]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.169658871221027, distance: 1.6855918450986431 entropy 1.1324905157089233
epoch: 23, step: 36
	action: tensor([[-0.8565,  0.4303,  0.2411,  1.0856,  0.3493, -1.2207,  0.1985]],
       dtype=torch.float64)
	q_value: tensor([[-30.9736]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07144446440235397, distance: 1.1845176217141797 entropy 1.1324905157089233
epoch: 23, step: 37
	action: tensor([[-0.3170,  0.5097, -0.2975,  0.0237,  0.3822, -0.7301,  0.1079]],
       dtype=torch.float64)
	q_value: tensor([[-34.0977]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 23, step: 38
	action: tensor([[-0.9070, -0.0319, -0.3370,  0.7055, -0.9144,  0.8385,  1.1111]],
       dtype=torch.float64)
	q_value: tensor([[-36.7619]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6183050663853531, distance: 1.4557503062798895 entropy 1.1324905157089233
epoch: 23, step: 39
	action: tensor([[ 0.7158,  1.1224, -0.1303,  0.6054,  0.5161, -0.1237, -0.9932]],
       dtype=torch.float64)
	q_value: tensor([[-30.5345]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 23, step: 40
	action: tensor([[ 0.6017, -2.1279, -0.8291, -0.7886,  0.5177,  0.4998, -0.3725]],
       dtype=torch.float64)
	q_value: tensor([[-36.7619]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 23, step: 41
	action: tensor([[ 0.8783,  0.1991, -1.5579, -0.3567, -0.2311,  0.5394,  0.1575]],
       dtype=torch.float64)
	q_value: tensor([[-36.7619]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 23, step: 42
	action: tensor([[ 0.8320, -0.9554, -0.9857, -0.9804, -0.6832,  0.7673,  0.5388]],
       dtype=torch.float64)
	q_value: tensor([[-36.7619]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5484459106463453, distance: 1.423982699728289 entropy 1.1324905157089233
epoch: 23, step: 43
	action: tensor([[-0.6803, -0.3010, -0.6381,  0.6599, -1.4186, -0.1985, -0.6779]],
       dtype=torch.float64)
	q_value: tensor([[-34.8085]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6100576370050155, distance: 1.4520360702838375 entropy 1.1324905157089233
epoch: 23, step: 44
	action: tensor([[ 0.9604, -1.4863,  0.3670,  0.2067, -0.3320,  0.7559,  0.1516]],
       dtype=torch.float64)
	q_value: tensor([[-32.0804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 23, step: 45
	action: tensor([[ 1.1071, -0.4523, -1.3839, -0.7657, -0.7526,  0.8373, -0.3783]],
       dtype=torch.float64)
	q_value: tensor([[-36.7619]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 23, step: 46
	action: tensor([[ 0.9029, -0.0842,  0.1610, -0.3469,  0.1822,  0.9637,  1.4553]],
       dtype=torch.float64)
	q_value: tensor([[-36.7619]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7693655267785421, distance: 0.5495646688502004 entropy 1.1324905157089233
epoch: 23, step: 47
	action: tensor([[-2.2202,  1.2400, -0.1330, -0.7591, -0.3238,  0.5992, -0.2999]],
       dtype=torch.float64)
	q_value: tensor([[-34.8009]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 23, step: 48
	action: tensor([[-0.7373,  0.6452, -1.3091, -0.4393, -0.6130,  0.2704,  0.1048]],
       dtype=torch.float64)
	q_value: tensor([[-36.7619]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04759388975042689, distance: 1.1167803909671468 entropy 1.1324905157089233
epoch: 23, step: 49
	action: tensor([[ 0.2687, -0.1263, -0.3988,  0.6283, -0.2367,  0.0016, -1.1846]],
       dtype=torch.float64)
	q_value: tensor([[-26.7403]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 23, step: 50
	action: tensor([[-0.0627, -0.0753,  0.9044, -0.2304, -0.6850, -0.0975, -0.4328]],
       dtype=torch.float64)
	q_value: tensor([[-36.7619]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.02511186461730075, distance: 1.15862347435651 entropy 1.1324905157089233
epoch: 23, step: 51
	action: tensor([[ 1.5120,  1.1804,  0.7696,  0.6615, -0.9785,  0.0657,  0.6639]],
       dtype=torch.float64)
	q_value: tensor([[-26.4567]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 23, step: 52
	action: tensor([[ 0.2375, -0.3657, -0.3833,  0.2757, -1.2108, -0.3195, -0.2751]],
       dtype=torch.float64)
	q_value: tensor([[-36.7619]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39539939806648905, distance: 0.8897970895623228 entropy 1.1324905157089233
epoch: 23, step: 53
	action: tensor([[ 1.2851, -0.7757, -0.1201,  0.0698,  1.1880,  0.0303,  0.5016]],
       dtype=torch.float64)
	q_value: tensor([[-26.9642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1009127394906455, distance: 1.200696215905353 entropy 1.1324905157089233
epoch: 23, step: 54
	action: tensor([[-0.7301,  0.0264, -0.6220, -0.0783, -1.5548,  0.7533, -2.1207]],
       dtype=torch.float64)
	q_value: tensor([[-35.7834]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7858892094830394, distance: 1.5292692285448481 entropy 1.1324905157089233
epoch: 23, step: 55
	action: tensor([[ 0.5499,  0.5574,  0.8743, -0.1860, -0.6919,  0.5171, -1.0116]],
       dtype=torch.float64)
	q_value: tensor([[-39.9876]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 23, step: 56
	action: tensor([[ 0.8148, -0.9340,  0.3554,  0.0989, -0.4060, -1.0013, -0.3793]],
       dtype=torch.float64)
	q_value: tensor([[-36.7619]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4168464736832622, distance: 1.3621285321455046 entropy 1.1324905157089233
epoch: 23, step: 57
	action: tensor([[ 0.1177,  0.6918,  0.2437, -0.6985,  0.2960, -0.6164,  0.8068]],
       dtype=torch.float64)
	q_value: tensor([[-33.1559]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 23, step: 58
	action: tensor([[-0.2629, -0.2195,  0.1630,  0.7319, -1.5401,  0.3196, -0.6134]],
       dtype=torch.float64)
	q_value: tensor([[-36.7619]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22204467481123047, distance: 1.0093319531384957 entropy 1.1324905157089233
epoch: 23, step: 59
	action: tensor([[-0.8507, -0.4727, -0.1022,  0.3618,  0.2174, -0.1712, -1.1058]],
       dtype=torch.float64)
	q_value: tensor([[-32.9636]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9582711845568554, distance: 1.6013752425772996 entropy 1.1324905157089233
epoch: 23, step: 60
	action: tensor([[ 1.2300,  0.2803, -0.0508, -0.3163,  0.3203,  0.4327, -0.0026]],
       dtype=torch.float64)
	q_value: tensor([[-30.3279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7782529846253115, distance: 0.5388719587512739 entropy 1.1324905157089233
epoch: 23, step: 61
	action: tensor([[ 1.5169, -0.0463,  0.1545, -0.3097,  0.8268,  0.7315, -0.0969]],
       dtype=torch.float64)
	q_value: tensor([[-24.8019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3054002095052145, distance: 0.9537268673183814 entropy 1.1324905157089233
epoch: 23, step: 62
	action: tensor([[ 0.2538, -0.1434, -0.5652,  0.2144, -0.1776,  0.2097, -0.2702]],
       dtype=torch.float64)
	q_value: tensor([[-32.0683]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4905112462975244, distance: 0.8168155448865145 entropy 1.1324905157089233
epoch: 23, step: 63
	action: tensor([[-0.5562, -1.5529,  0.1284,  0.7380,  0.8283, -0.9647, -0.0461]],
       dtype=torch.float64)
	q_value: tensor([[-18.4541]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 23, step: 64
	action: tensor([[ 0.2106,  0.1056, -0.4765, -1.2458,  0.8534,  1.2027,  0.5993]],
       dtype=torch.float64)
	q_value: tensor([[-36.7619]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 23, step: 65
	action: tensor([[ 0.9232,  0.2964, -0.8906, -1.6871, -0.5264,  0.4455, -0.3742]],
       dtype=torch.float64)
	q_value: tensor([[-36.7619]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 23, step: 66
	action: tensor([[ 0.2747, -0.9168,  0.2168,  0.3297, -0.0565, -1.2639, -0.7771]],
       dtype=torch.float64)
	q_value: tensor([[-36.7619]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3678196103803899, distance: 1.3383543234658346 entropy 1.1324905157089233
epoch: 23, step: 67
	action: tensor([[-0.0392, -0.0279,  0.4411,  0.0147,  0.5661, -0.2808,  0.1673]],
       dtype=torch.float64)
	q_value: tensor([[-35.2994]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13816317043626314, distance: 1.0623539031830347 entropy 1.1324905157089233
epoch: 23, step: 68
	action: tensor([[ 0.3884,  0.0483,  0.5371,  0.5601,  0.4015,  1.3042, -1.7355]],
       dtype=torch.float64)
	q_value: tensor([[-22.7557]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9805535412239104, distance: 0.15957944742964356 entropy 1.1324905157089233
epoch: 23, step: 69
	action: tensor([[ 0.4208,  1.5783,  0.4122,  1.5998,  0.1306, -0.3747, -0.3723]],
       dtype=torch.float64)
	q_value: tensor([[-38.4079]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 23, step: 70
	action: tensor([[ 0.3614,  0.1261,  0.8806,  1.4951, -0.2147,  0.7045, -0.1584]],
       dtype=torch.float64)
	q_value: tensor([[-36.7619]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 23, step: 71
	action: tensor([[ 0.5900, -0.0195, -0.3853,  1.0860, -0.3460,  0.0917,  1.7808]],
       dtype=torch.float64)
	q_value: tensor([[-36.7619]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 23, step: 72
	action: tensor([[-0.1866,  0.2044,  0.4816, -0.8296, -0.5866,  0.3903,  0.9061]],
       dtype=torch.float64)
	q_value: tensor([[-36.7619]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21257639872332046, distance: 1.2601180972771868 entropy 1.1324905157089233
epoch: 23, step: 73
	action: tensor([[ 0.5121, -0.9185,  1.4223,  0.8484,  1.9936,  0.2330, -0.0406]],
       dtype=torch.float64)
	q_value: tensor([[-27.7737]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.01185937221980038, distance: 1.1511098564001798 entropy 1.1324905157089233
epoch: 23, step: 74
	action: tensor([[ 0.6467, -1.3024, -1.1029, -0.1271,  1.1809,  0.4862, -1.2663]],
       dtype=torch.float64)
	q_value: tensor([[-47.7829]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 23, step: 75
	action: tensor([[ 0.5201, -1.1950, -1.3740,  0.2573,  0.1114, -0.2346, -0.7870]],
       dtype=torch.float64)
	q_value: tensor([[-36.7619]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6260205177016003, distance: 1.4592164065489823 entropy 1.1324905157089233
epoch: 23, step: 76
	action: tensor([[ 1.0373, -0.2096, -0.2000, -0.2108,  0.9918, -0.1972,  0.0190]],
       dtype=torch.float64)
	q_value: tensor([[-34.3883]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37311125056063843, distance: 0.9060495127634135 entropy 1.1324905157089233
epoch: 23, step: 77
	action: tensor([[-0.2402, -1.1463, -0.0948, -0.3076,  0.1768,  0.1747, -1.1218]],
       dtype=torch.float64)
	q_value: tensor([[-28.9500]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9065985039183455, distance: 1.5801063457558275 entropy 1.1324905157089233
epoch: 23, step: 78
	action: tensor([[-0.6557,  0.7632, -0.7939, -1.0778, -0.0807,  1.2571, -1.0994]],
       dtype=torch.float64)
	q_value: tensor([[-32.9312]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 23, step: 79
	action: tensor([[ 0.7517,  0.5790, -0.3964, -0.2176, -0.3941,  1.2803, -0.1286]],
       dtype=torch.float64)
	q_value: tensor([[-36.7619]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 23, step: 80
	action: tensor([[-1.1592, -0.1683,  0.5740,  0.8010, -0.0085,  0.6351, -0.4846]],
       dtype=torch.float64)
	q_value: tensor([[-36.7619]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.19535059849033143, distance: 1.2511354939391912 entropy 1.1324905157089233
epoch: 23, step: 81
	action: tensor([[-1.2470,  0.7683, -0.8365, -1.6983, -1.3073, -1.0308, -0.2420]],
       dtype=torch.float64)
	q_value: tensor([[-30.1543]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37182212376262225, distance: 0.906980629211842 entropy 1.1324905157089233
epoch: 23, step: 82
	action: tensor([[-0.0649, -0.5000,  0.2641, -0.4490, -0.7806,  1.3108,  0.4909]],
       dtype=torch.float64)
	q_value: tensor([[-39.0459]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14675284258907562, distance: 1.2254387408566378 entropy 1.1324905157089233
epoch: 23, step: 83
	action: tensor([[ 0.1358,  0.1289,  0.0985,  0.4681,  0.4571, -0.3942,  0.0006]],
       dtype=torch.float64)
	q_value: tensor([[-31.9483]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 23, step: 84
	action: tensor([[ 0.5713, -0.0287, -0.7406, -0.2767, -0.4715, -0.6702,  0.3304]],
       dtype=torch.float64)
	q_value: tensor([[-36.7619]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 23, step: 85
	action: tensor([[ 0.8275, -2.1666,  0.0502,  0.5576,  1.2583,  0.8932, -0.2021]],
       dtype=torch.float64)
	q_value: tensor([[-36.7619]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 23, step: 86
	action: tensor([[-0.6797, -0.3986, -1.7211, -0.7851,  0.5246,  0.3407, -0.2015]],
       dtype=torch.float64)
	q_value: tensor([[-36.7619]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15318536107349767, distance: 1.2288708815247082 entropy 1.1324905157089233
epoch: 23, step: 87
	action: tensor([[ 0.0206,  0.6777, -0.4264, -0.6207,  0.8761,  0.3072,  0.3936]],
       dtype=torch.float64)
	q_value: tensor([[-33.3790]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 23, step: 88
	action: tensor([[-1.0879, -0.3115,  0.1122, -2.5739, -0.1464, -0.3349, -0.6938]],
       dtype=torch.float64)
	q_value: tensor([[-36.7619]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 23, step: 89
	action: tensor([[-0.5232, -0.2567, -0.6569, -0.4701, -0.2817, -0.8025, -0.2243]],
       dtype=torch.float64)
	q_value: tensor([[-36.7619]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.19862750728856127, distance: 1.252849238401539 entropy 1.1324905157089233
epoch: 23, step: 90
	action: tensor([[ 0.0839,  0.9863,  1.5669,  0.0346, -0.1866, -0.4627,  0.5872]],
       dtype=torch.float64)
	q_value: tensor([[-26.1519]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 23, step: 91
	action: tensor([[-0.2628, -0.9194,  0.5786, -1.0749,  0.1032, -0.0387, -0.7398]],
       dtype=torch.float64)
	q_value: tensor([[-36.7619]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1176316714542898, distance: 1.6652594432686625 entropy 1.1324905157089233
epoch: 23, step: 92
	action: tensor([[ 0.0047, -1.6287, -0.7092, -0.2297, -0.2520, -0.4855, -1.8355]],
       dtype=torch.float64)
	q_value: tensor([[-31.7936]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 23, step: 93
	action: tensor([[ 0.6186, -0.4551,  0.0356,  0.0996,  1.1858, -0.0418, -0.1264]],
       dtype=torch.float64)
	q_value: tensor([[-36.7619]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36704060630297364, distance: 0.9104259291911339 entropy 1.1324905157089233
epoch: 23, step: 94
	action: tensor([[-1.5884, -0.8351, -0.7505, -1.2963, -1.1876,  0.5927, -0.1533]],
       dtype=torch.float64)
	q_value: tensor([[-30.4404]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7337690105005792, distance: 1.506788549804792 entropy 1.1324905157089233
epoch: 23, step: 95
	action: tensor([[ 0.3591, -0.3488, -1.3691, -0.8436,  0.0264, -0.2338,  0.2900]],
       dtype=torch.float64)
	q_value: tensor([[-41.0300]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.43747949112936035, distance: 0.8582738364962356 entropy 1.1324905157089233
epoch: 23, step: 96
	action: tensor([[ 0.3525,  0.0522, -0.5514, -1.0581,  0.9938,  1.4011, -0.1411]],
       dtype=torch.float64)
	q_value: tensor([[-28.7496]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7663677989218316, distance: 0.5531246885990252 entropy 1.1324905157089233
epoch: 23, step: 97
	action: tensor([[ 1.3806,  0.3386,  0.7227, -0.5950, -0.3567, -0.5165, -0.5082]],
       dtype=torch.float64)
	q_value: tensor([[-35.5668]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.649691918508581, distance: 0.6773010852778905 entropy 1.1324905157089233
epoch: 23, step: 98
	action: tensor([[ 0.0513,  0.5116,  0.7058,  0.4633, -0.4117,  0.2422,  0.5095]],
       dtype=torch.float64)
	q_value: tensor([[-28.4339]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 23, step: 99
	action: tensor([[ 0.4203, -0.2256, -1.2072,  0.3817,  0.1389, -0.0154, -0.2881]],
       dtype=torch.float64)
	q_value: tensor([[-36.7619]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42325285568533577, distance: 0.8690593165709104 entropy 1.1324905157089233
epoch: 23, step: 100
	action: tensor([[ 0.1684,  0.3386, -0.8431, -0.9311, -0.5126,  0.0703, -0.9514]],
       dtype=torch.float64)
	q_value: tensor([[-23.0753]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6779397571091059, distance: 0.6494193898961798 entropy 1.1324905157089233
epoch: 23, step: 101
	action: tensor([[ 1.3465e+00,  1.3794e-02, -7.1127e-01, -1.8613e+00, -5.2404e-01,
         -2.6555e-04,  1.5761e+00]], dtype=torch.float64)
	q_value: tensor([[-25.6454]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2950446809035503, distance: 1.30226410340172 entropy 1.1324905157089233
epoch: 23, step: 102
	action: tensor([[ 0.4558,  0.5498, -0.3405,  0.0570,  0.9321,  0.4713, -0.3716]],
       dtype=torch.float64)
	q_value: tensor([[-40.8707]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 23, step: 103
	action: tensor([[-0.5097,  1.0704, -0.1382, -0.6600, -0.3167, -0.8442, -0.3519]],
       dtype=torch.float64)
	q_value: tensor([[-36.7619]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 23, step: 104
	action: tensor([[-0.0618, -2.0982, -0.6568, -0.8121, -0.1208, -0.8664, -1.2538]],
       dtype=torch.float64)
	q_value: tensor([[-36.7619]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 23, step: 105
	action: tensor([[-1.3952,  0.3909,  0.4893,  0.1718, -1.6873,  0.4264,  1.2025]],
       dtype=torch.float64)
	q_value: tensor([[-36.7619]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9503704100571047, distance: 1.5981415505259644 entropy 1.1324905157089233
epoch: 23, step: 106
	action: tensor([[ 1.1136, -0.1385,  0.9777, -0.1222, -0.0389, -0.6304, -0.2000]],
       dtype=torch.float64)
	q_value: tensor([[-37.4706]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5414780518657537, distance: 0.7748841145041886 entropy 1.1324905157089233
epoch: 23, step: 107
	action: tensor([[-0.4274, -0.6124,  0.2471,  0.5224, -0.0832,  0.8255, -0.1594]],
       dtype=torch.float64)
	q_value: tensor([[-30.6639]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08130799221867002, distance: 1.0968359143583386 entropy 1.1324905157089233
epoch: 23, step: 108
	action: tensor([[-0.0725,  0.8695,  1.6692, -0.5763,  0.6525, -0.5786,  1.1152]],
       dtype=torch.float64)
	q_value: tensor([[-26.6891]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 23, step: 109
	action: tensor([[ 0.6792, -0.6767,  0.2156,  0.6003,  0.7087,  0.5521,  0.9099]],
       dtype=torch.float64)
	q_value: tensor([[-36.7619]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6602463295275474, distance: 0.6670198707635117 entropy 1.1324905157089233
epoch: 23, step: 110
	action: tensor([[ 0.4636,  0.2276, -0.9764, -0.3923, -0.0122, -0.3761,  0.2645]],
       dtype=torch.float64)
	q_value: tensor([[-32.6255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7556515027058532, distance: 0.5656678935353965 entropy 1.1324905157089233
epoch: 23, step: 111
	action: tensor([[ 0.5452,  0.8721,  0.2122, -1.3611,  0.2529,  0.1259, -0.1895]],
       dtype=torch.float64)
	q_value: tensor([[-22.7761]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7223892123557212, distance: 0.602941063197906 entropy 1.1324905157089233
epoch: 23, step: 112
	action: tensor([[-0.6713, -0.2188,  0.5337, -0.3409, -0.9866,  0.5720,  0.5365]],
       dtype=torch.float64)
	q_value: tensor([[-27.5499]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8387019459034024, distance: 1.5517164390323328 entropy 1.1324905157089233
epoch: 23, step: 113
	action: tensor([[-1.2564,  0.4579, -1.0812, -1.6830,  0.7147, -0.0821, -0.3189]],
       dtype=torch.float64)
	q_value: tensor([[-28.9493]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0016925387207749054, distance: 1.1453122680527485 entropy 1.1324905157089233
epoch: 23, step: 114
	action: tensor([[-0.5527, -1.5371, -0.3222,  0.9972, -0.6229, -0.2307,  1.5128]],
       dtype=torch.float64)
	q_value: tensor([[-38.9288]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7848130746662207, distance: 1.5288084082873228 entropy 1.1324905157089233
epoch: 23, step: 115
	action: tensor([[ 0.9217, -0.6822,  1.1034,  0.0058, -0.2143, -0.4600, -0.0463]],
       dtype=torch.float64)
	q_value: tensor([[-36.6738]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.025733839041197415, distance: 1.1589749115590937 entropy 1.1324905157089233
epoch: 23, step: 116
	action: tensor([[-0.3438,  1.3107, -0.0644,  1.2424, -0.6126,  0.7040,  0.2402]],
       dtype=torch.float64)
	q_value: tensor([[-32.5215]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 23, step: 117
	action: tensor([[-0.4861, -0.5192,  0.0048,  0.1562, -0.3049, -0.2056,  0.1522]],
       dtype=torch.float64)
	q_value: tensor([[-36.7619]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6155610565882144, distance: 1.45451559227633 entropy 1.1324905157089233
epoch: 23, step: 118
	action: tensor([[-0.9419, -0.9514, -0.8663, -0.7763, -0.3568,  0.8945, -0.2026]],
       dtype=torch.float64)
	q_value: tensor([[-22.2827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0356609208675533, distance: 1.632711354441158 entropy 1.1324905157089233
epoch: 23, step: 119
	action: tensor([[ 0.0864,  0.9621, -1.0073, -1.5191,  0.1364,  0.2001, -0.3393]],
       dtype=torch.float64)
	q_value: tensor([[-33.6234]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9068098326042068, distance: 0.3493347098271632 entropy 1.1324905157089233
epoch: 23, step: 120
	action: tensor([[ 0.7389, -0.0870, -0.2237, -0.4483, -0.9788,  0.5444,  1.1882]],
       dtype=torch.float64)
	q_value: tensor([[-31.5463]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.47561245034235455, distance: 0.8286724092430628 entropy 1.1324905157089233
epoch: 23, step: 121
	action: tensor([[-0.5864,  0.2469,  0.5027, -0.7499,  0.4732,  0.5877, -0.2841]],
       dtype=torch.float64)
	q_value: tensor([[-31.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4876401706680704, distance: 1.3957435893352215 entropy 1.1324905157089233
epoch: 23, step: 122
	action: tensor([[ 1.3343, -1.2740,  1.1884, -0.0554,  0.1031, -0.4341, -0.0323]],
       dtype=torch.float64)
	q_value: tensor([[-27.1068]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5880817490942025, distance: 1.4420925080530453 entropy 1.1324905157089233
epoch: 23, step: 123
	action: tensor([[-0.2992, -0.2364,  0.3278,  0.3917,  0.0521,  0.6557, -0.3587]],
       dtype=torch.float64)
	q_value: tensor([[-39.6294]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31127945969859816, distance: 0.9496820097049036 entropy 1.1324905157089233
epoch: 23, step: 124
	action: tensor([[ 0.1324, -0.2815, -0.5546, -0.2048, -0.3909, -0.7986, -0.6697]],
       dtype=torch.float64)
	q_value: tensor([[-24.0068]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21072246662976035, distance: 1.0166502310960839 entropy 1.1324905157089233
epoch: 23, step: 125
	action: tensor([[-1.6629,  0.5997,  1.4751, -1.0401,  1.1428, -0.0821, -0.1958]],
       dtype=torch.float64)
	q_value: tensor([[-25.2709]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1635098580344514, distance: 1.6832015890329237 entropy 1.1324905157089233
epoch: 23, step: 126
	action: tensor([[-1.1872,  0.0447, -1.5795, -0.2545, -0.7365,  0.7077,  1.0478]],
       dtype=torch.float64)
	q_value: tensor([[-39.8382]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.3178353257567585, distance: 1.74219989033357 entropy 1.1324905157089233
epoch: 23, step: 127
	action: tensor([[-0.6392,  1.4344, -0.2740, -0.9720, -0.1654,  0.2897,  1.4249]],
       dtype=torch.float64)
	q_value: tensor([[-35.7314]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
LOSS epoch 23 actor 347.72739412555933 critic 241.35493663751393 
epoch: 24, step: 0
	action: tensor([[-0.8652, -0.2156,  1.5022, -0.5527,  0.0253, -0.5179,  0.4113]],
       dtype=torch.float64)
	q_value: tensor([[-36.5948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2646778645603254, distance: 1.7221061229785841 entropy 1.0271300077438354
epoch: 24, step: 1
	action: tensor([[ 0.2843, -0.0451,  0.3246,  1.3931,  0.3769, -0.5671,  0.7002]],
       dtype=torch.float64)
	q_value: tensor([[-30.4834]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 24, step: 2
	action: tensor([[-1.0775, -0.2220, -0.0161, -0.0807, -0.4851,  0.6182,  0.0186]],
       dtype=torch.float64)
	q_value: tensor([[-36.5948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1364422695786347, distance: 1.6726392135778956 entropy 1.0271300077438354
epoch: 24, step: 3
	action: tensor([[-0.1466,  0.1816, -1.0649,  0.3174, -0.6812, -1.5820,  0.0427]],
       dtype=torch.float64)
	q_value: tensor([[-25.8855]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 24, step: 4
	action: tensor([[ 0.3393,  0.3856,  0.1747,  0.5602, -0.2152, -0.3265,  0.3593]],
       dtype=torch.float64)
	q_value: tensor([[-36.5948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 24, step: 5
	action: tensor([[-0.5040, -0.4838,  1.0518,  0.2375, -0.6906, -0.7940, -0.0879]],
       dtype=torch.float64)
	q_value: tensor([[-36.5948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5907963728986663, distance: 1.4433245186671602 entropy 1.0271300077438354
epoch: 24, step: 6
	action: tensor([[-0.2643,  0.5845,  0.3506, -0.1689, -0.0579, -0.1227, -0.3250]],
       dtype=torch.float64)
	q_value: tensor([[-32.0162]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 24, step: 7
	action: tensor([[-0.9306, -0.8416, -0.3332,  0.1965,  0.4988,  0.8338,  0.7069]],
       dtype=torch.float64)
	q_value: tensor([[-36.5948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9717941140651924, distance: 1.6068949141724211 entropy 1.0271300077438354
epoch: 24, step: 8
	action: tensor([[ 0.8397, -0.4816, -0.0131, -0.2861, -0.7975, -0.9620,  0.0737]],
       dtype=torch.float64)
	q_value: tensor([[-30.5251]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04036913456820024, distance: 1.1672138248929926 entropy 1.0271300077438354
epoch: 24, step: 9
	action: tensor([[ 1.8043,  0.7844, -0.4542,  0.3665,  0.5845,  0.9139,  0.1480]],
       dtype=torch.float64)
	q_value: tensor([[-29.0577]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 24, step: 10
	action: tensor([[ 0.5212,  0.0422, -1.3252,  0.8517, -0.3742, -0.3898, -0.5619]],
       dtype=torch.float64)
	q_value: tensor([[-36.5948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 24, step: 11
	action: tensor([[-0.0985,  0.7774,  0.0548,  0.1473, -0.3860,  0.8092, -0.7223]],
       dtype=torch.float64)
	q_value: tensor([[-36.5948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 24, step: 12
	action: tensor([[-0.9395, -1.2839,  1.2887, -0.6635, -0.1971,  0.2313,  1.2365]],
       dtype=torch.float64)
	q_value: tensor([[-36.5948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.55928903745211, distance: 1.8306965431495157 entropy 1.0271300077438354
epoch: 24, step: 13
	action: tensor([[-0.0414, -0.1142, -0.0783,  0.5938, -0.5381, -0.7529, -0.9031]],
       dtype=torch.float64)
	q_value: tensor([[-36.6434]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41683992332497166, distance: 0.8738775561202269 entropy 1.0271300077438354
epoch: 24, step: 14
	action: tensor([[ 0.0036, -0.0268,  0.1382, -0.6292,  0.4258,  0.7181, -0.1966]],
       dtype=torch.float64)
	q_value: tensor([[-29.5837]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20754837360459188, distance: 1.0186924180819488 entropy 1.0271300077438354
epoch: 24, step: 15
	action: tensor([[ 0.3599, -0.1020,  0.7167, -0.7548, -0.9951,  1.1503,  0.6118]],
       dtype=torch.float64)
	q_value: tensor([[-24.7411]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37113873415060694, distance: 0.9074738435391679 entropy 1.0271300077438354
epoch: 24, step: 16
	action: tensor([[-0.0336, -0.3490, -0.1187, -0.4389, -0.3773, -0.3562,  1.7022]],
       dtype=torch.float64)
	q_value: tensor([[-34.0334]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23838099554781333, distance: 1.2734556733727822 entropy 1.0271300077438354
epoch: 24, step: 17
	action: tensor([[-0.1363,  0.6327,  0.6923, -0.9143,  0.6357,  0.9060, -0.6706]],
       dtype=torch.float64)
	q_value: tensor([[-31.8644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3350505147299735, distance: 0.9331490545009696 entropy 1.0271300077438354
epoch: 24, step: 18
	action: tensor([[ 0.1840,  0.1851,  0.7055, -0.1139,  0.1561,  1.0217, -0.1313]],
       dtype=torch.float64)
	q_value: tensor([[-31.2958]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 24, step: 19
	action: tensor([[-0.8621, -0.0982, -1.1189,  0.3566,  0.3707, -0.0789, -0.3896]],
       dtype=torch.float64)
	q_value: tensor([[-36.5948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9638569848032734, distance: 1.6036575088671363 entropy 1.0271300077438354
epoch: 24, step: 20
	action: tensor([[-0.1200,  0.9761, -0.3309,  0.0189,  0.4080,  0.2142, -0.3931]],
       dtype=torch.float64)
	q_value: tensor([[-25.7378]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 24, step: 21
	action: tensor([[-1.0695,  1.2541, -0.4746,  0.0628, -0.3733, -0.2687, -0.9572]],
       dtype=torch.float64)
	q_value: tensor([[-36.5948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 24, step: 22
	action: tensor([[-0.3283,  0.2300, -0.1938,  0.8103,  0.5896, -0.4773,  0.2817]],
       dtype=torch.float64)
	q_value: tensor([[-36.5948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 24, step: 23
	action: tensor([[-0.7754, -0.0170,  1.6465, -0.2032,  0.9248,  0.5449,  0.6546]],
       dtype=torch.float64)
	q_value: tensor([[-36.5948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8041702785211793, distance: 1.53707640273703 entropy 1.0271300077438354
epoch: 24, step: 24
	action: tensor([[-0.0298, -0.7179, -1.8539,  0.9876, -0.3026,  0.4161,  0.3824]],
       dtype=torch.float64)
	q_value: tensor([[-35.3363]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8357023821726928, distance: 1.5504502272414944 entropy 1.0271300077438354
epoch: 24, step: 25
	action: tensor([[ 0.9559, -0.3876,  0.3640,  0.2431,  0.1760,  0.1648,  0.5960]],
       dtype=torch.float64)
	q_value: tensor([[-31.5705]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5892271781269518, distance: 0.7334280980408184 entropy 1.0271300077438354
epoch: 24, step: 26
	action: tensor([[ 0.5455,  0.6105, -0.5128,  0.1256,  0.3304,  0.7614, -0.5073]],
       dtype=torch.float64)
	q_value: tensor([[-27.4813]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 24, step: 27
	action: tensor([[-0.0622,  0.1633,  0.7318, -0.8092, -0.8956,  0.9954,  0.4682]],
       dtype=torch.float64)
	q_value: tensor([[-36.5948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.059398573287392065, distance: 1.109837793526658 entropy 1.0271300077438354
epoch: 24, step: 28
	action: tensor([[-0.0197, -0.7251, -0.1395,  0.0148,  0.1849, -0.6883,  0.1290]],
       dtype=torch.float64)
	q_value: tensor([[-31.8080]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5161498944822287, distance: 1.4090544092505972 entropy 1.0271300077438354
epoch: 24, step: 29
	action: tensor([[ 0.5470, -0.1421, -1.9139, -0.3400,  0.2373,  0.7857,  0.1816]],
       dtype=torch.float64)
	q_value: tensor([[-25.7490]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.612264354546734, distance: 0.7125651162118992 entropy 1.0271300077438354
epoch: 24, step: 30
	action: tensor([[ 0.3190, -0.8586, -0.3640,  0.2382, -0.0634,  0.5929, -0.4848]],
       dtype=torch.float64)
	q_value: tensor([[-32.6514]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06352984040491172, distance: 1.107397821671435 entropy 1.0271300077438354
epoch: 24, step: 31
	action: tensor([[ 0.0393, -0.1168,  0.0760,  1.0739, -0.9728, -0.1614,  0.6589]],
       dtype=torch.float64)
	q_value: tensor([[-27.1068]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 24, step: 32
	action: tensor([[-0.7933,  0.7523, -1.0279,  0.2387,  0.3664,  0.4694,  0.4485]],
       dtype=torch.float64)
	q_value: tensor([[-36.5948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 24, step: 33
	action: tensor([[ 0.8238, -0.0620,  0.7519, -0.4443, -0.7680,  0.3476, -1.1343]],
       dtype=torch.float64)
	q_value: tensor([[-36.5948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.61120315490515, distance: 0.7135395651253067 entropy 1.0271300077438354
epoch: 24, step: 34
	action: tensor([[-1.0982, -0.4854, -0.8993, -0.7304, -0.9015, -0.2326, -0.0479]],
       dtype=torch.float64)
	q_value: tensor([[-31.0621]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7429593268668921, distance: 1.5107768437375575 entropy 1.0271300077438354
epoch: 24, step: 35
	action: tensor([[ 0.4638, -1.1989,  0.7118, -0.1407,  0.0961,  1.2694,  0.1043]],
       dtype=torch.float64)
	q_value: tensor([[-32.1613]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10893052755670007, distance: 1.2050605324870973 entropy 1.0271300077438354
epoch: 24, step: 36
	action: tensor([[-0.0139, -0.3194,  0.3181,  0.3791,  0.9076,  0.7011, -0.2142]],
       dtype=torch.float64)
	q_value: tensor([[-35.9015]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.541648425456844, distance: 0.7747401387841888 entropy 1.0271300077438354
epoch: 24, step: 37
	action: tensor([[-1.0128,  0.2353, -0.5417,  1.4479, -0.1672,  1.0188, -0.9197]],
       dtype=torch.float64)
	q_value: tensor([[-28.6941]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 24, step: 38
	action: tensor([[-0.0178, -1.7330, -0.9341, -1.4192, -0.7199,  0.2130, -1.3296]],
       dtype=torch.float64)
	q_value: tensor([[-36.5948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23413272995254086, distance: 1.2712695022142155 entropy 1.0271300077438354
epoch: 24, step: 39
	action: tensor([[-0.4968,  0.3896, -0.3675,  1.0632, -0.0648,  0.7206,  0.3554]],
       dtype=torch.float64)
	q_value: tensor([[-41.6782]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 24, step: 40
	action: tensor([[-0.1707, -0.3230, -0.0216,  0.3891,  0.0997,  0.6810,  1.5920]],
       dtype=torch.float64)
	q_value: tensor([[-36.5948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27193659669740644, distance: 0.9764304040240034 entropy 1.0271300077438354
epoch: 24, step: 41
	action: tensor([[-0.3214, -1.1350,  0.0190,  0.0963, -0.8237, -0.2164,  0.9991]],
       dtype=torch.float64)
	q_value: tensor([[-32.0187]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8839752906876204, distance: 1.5707038003330875 entropy 1.0271300077438354
epoch: 24, step: 42
	action: tensor([[ 0.6368,  1.3864,  0.1831,  1.3814,  0.0018, -0.1991, -0.8567]],
       dtype=torch.float64)
	q_value: tensor([[-31.2050]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 24, step: 43
	action: tensor([[ 0.4644, -1.1364, -0.1219,  0.3229,  0.3610, -0.5763, -0.5867]],
       dtype=torch.float64)
	q_value: tensor([[-36.5948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5154788206398349, distance: 1.4087425389613935 entropy 1.0271300077438354
epoch: 24, step: 44
	action: tensor([[-0.2781, -1.2434, -0.4286,  0.1993, -0.0202,  0.2613,  1.3016]],
       dtype=torch.float64)
	q_value: tensor([[-32.6753]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.900069799292295, distance: 1.5773986718619541 entropy 1.0271300077438354
epoch: 24, step: 45
	action: tensor([[ 1.1064, -1.1145, -0.0522, -0.2638,  0.2908,  0.4793, -0.0255]],
       dtype=torch.float64)
	q_value: tensor([[-32.8738]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.520916084657987, distance: 1.4112674330416508 entropy 1.0271300077438354
epoch: 24, step: 46
	action: tensor([[-0.3551,  1.0147, -0.8379, -1.9759, -0.4930, -0.5453,  0.9857]],
       dtype=torch.float64)
	q_value: tensor([[-32.6668]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5703648042676994, distance: 0.7500783305458535 entropy 1.0271300077438354
epoch: 24, step: 47
	action: tensor([[ 0.0681, -0.2935,  0.2639, -0.5918,  0.1521, -0.0569,  0.3640]],
       dtype=torch.float64)
	q_value: tensor([[-36.2682]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3068527241487451, distance: 1.3081875661903728 entropy 1.0271300077438354
epoch: 24, step: 48
	action: tensor([[-0.8868, -0.1692,  0.2442, -0.2896, -0.7102,  1.1057, -0.2189]],
       dtype=torch.float64)
	q_value: tensor([[-22.5034]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.834853645852462, distance: 1.5500917607567577 entropy 1.0271300077438354
epoch: 24, step: 49
	action: tensor([[-0.3714,  0.1969, -0.5570, -0.0322, -0.2856,  0.3529, -0.4692]],
       dtype=torch.float64)
	q_value: tensor([[-29.9963]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.026895567546162402, distance: 1.1596310433568424 entropy 1.0271300077438354
epoch: 24, step: 50
	action: tensor([[ 0.1457,  0.2668, -0.3145, -0.4163,  0.1758, -0.2978,  0.0811]],
       dtype=torch.float64)
	q_value: tensor([[-20.2624]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4600107060346219, distance: 0.8409095176599461 entropy 1.0271300077438354
epoch: 24, step: 51
	action: tensor([[-0.1461, -0.0959, -0.4346, -0.4956,  0.9617,  1.1056, -0.1620]],
       dtype=torch.float64)
	q_value: tensor([[-20.1046]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.40493027409223237, distance: 0.8827558846437096 entropy 1.0271300077438354
epoch: 24, step: 52
	action: tensor([[ 0.3584, -2.0981,  0.4872,  0.0969, -0.2401,  0.5037, -0.5285]],
       dtype=torch.float64)
	q_value: tensor([[-30.6989]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 24, step: 53
	action: tensor([[ 0.1477, -0.1881, -0.3931, -0.1973, -0.6316,  0.3005,  0.8273]],
       dtype=torch.float64)
	q_value: tensor([[-36.5948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21680516025081797, distance: 1.0127251653312481 entropy 1.0271300077438354
epoch: 24, step: 54
	action: tensor([[ 0.1391,  0.2251,  1.3180, -0.5721, -0.3248, -0.9140,  0.3287]],
       dtype=torch.float64)
	q_value: tensor([[-24.2650]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05249436604013058, distance: 1.113903564701584 entropy 1.0271300077438354
epoch: 24, step: 55
	action: tensor([[-0.6594,  0.4341,  0.4358,  1.6172,  0.9391, -0.1190, -0.9483]],
       dtype=torch.float64)
	q_value: tensor([[-29.8467]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 24, step: 56
	action: tensor([[-0.8817, -0.3397, -0.8798,  0.6139,  0.1468,  0.4283,  0.7785]],
       dtype=torch.float64)
	q_value: tensor([[-36.5948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.012002535199556, distance: 1.6231959673710599 entropy 1.0271300077438354
epoch: 24, step: 57
	action: tensor([[-0.4841, -1.3958, -0.2725,  0.3723,  1.0182, -0.2162, -0.6333]],
       dtype=torch.float64)
	q_value: tensor([[-27.7995]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0916779093678626, distance: 1.6550232460540237 entropy 1.0271300077438354
epoch: 24, step: 58
	action: tensor([[ 0.1080, -0.6678,  0.8552,  0.2231,  0.7258, -0.2738,  0.6080]],
       dtype=torch.float64)
	q_value: tensor([[-35.5477]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07981003212307836, distance: 1.1891328378045745 entropy 1.0271300077438354
epoch: 24, step: 59
	action: tensor([[ 0.7611,  0.3392,  0.7146,  0.8156, -0.4197, -0.1488,  0.3801]],
       dtype=torch.float64)
	q_value: tensor([[-29.8059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9537062639577314, distance: 0.2462168715308407 entropy 1.0271300077438354
epoch: 24, step: 60
	action: tensor([[-1.4805,  1.3288, -0.0084, -0.0140, -0.4575, -0.1680, -0.0475]],
       dtype=torch.float64)
	q_value: tensor([[-28.5758]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 24, step: 61
	action: tensor([[-0.1763,  0.2049, -1.0347,  0.2334,  1.1832, -0.0491, -0.5008]],
       dtype=torch.float64)
	q_value: tensor([[-36.5948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 24, step: 62
	action: tensor([[-0.4825, -0.3916,  0.8641, -1.0119, -0.0782, -0.6234, -1.2927]],
       dtype=torch.float64)
	q_value: tensor([[-36.5948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.03466493798165, distance: 1.6323118892098472 entropy 1.0271300077438354
epoch: 24, step: 63
	action: tensor([[ 0.0047,  0.5400,  0.2678, -0.7322,  0.1717,  0.1180,  1.1155]],
       dtype=torch.float64)
	q_value: tensor([[-35.4530]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3382448228697442, distance: 0.930905008834418 entropy 1.0271300077438354
epoch: 24, step: 64
	action: tensor([[-0.0672,  0.5840, -0.4498,  0.2875, -0.2598, -0.5903, -0.1049]],
       dtype=torch.float64)
	q_value: tensor([[-27.5570]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 24, step: 65
	action: tensor([[ 1.0352, -0.0435,  0.6179, -0.4672, -0.2423,  1.0465, -0.1842]],
       dtype=torch.float64)
	q_value: tensor([[-36.5948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7800649947888634, distance: 0.5366657409829407 entropy 1.0271300077438354
epoch: 24, step: 66
	action: tensor([[-0.5565, -0.3494,  0.0622, -0.3717, -0.1916,  0.6311, -1.1598]],
       dtype=torch.float64)
	q_value: tensor([[-30.3393]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6136936455887883, distance: 1.4536747191444688 entropy 1.0271300077438354
epoch: 24, step: 67
	action: tensor([[-0.9138,  0.2111,  1.3137, -0.0898, -0.2930, -1.0157,  0.1244]],
       dtype=torch.float64)
	q_value: tensor([[-29.5250]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9308210655668288, distance: 1.5901119722167656 entropy 1.0271300077438354
epoch: 24, step: 68
	action: tensor([[-0.2776, -0.4090,  0.9462,  0.3390, -1.0245,  0.3366,  0.3304]],
       dtype=torch.float64)
	q_value: tensor([[-31.5315]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03484294497288798, distance: 1.1242313404927298 entropy 1.0271300077438354
epoch: 24, step: 69
	action: tensor([[-0.4979,  0.4818, -0.4766, -0.5075, -0.8717, -0.5984,  0.7574]],
       dtype=torch.float64)
	q_value: tensor([[-30.8705]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22276046868440025, distance: 1.0088675049449476 entropy 1.0271300077438354
epoch: 24, step: 70
	action: tensor([[ 0.3000, -0.9500,  0.2897, -0.0817, -0.3821,  1.0776,  2.1070]],
       dtype=torch.float64)
	q_value: tensor([[-26.8471]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04108016146351723, distance: 1.1676126154007755 entropy 1.0271300077438354
epoch: 24, step: 71
	action: tensor([[ 0.9068,  0.5194, -0.6646,  0.2217, -1.4940,  1.1792,  0.6495]],
       dtype=torch.float64)
	q_value: tensor([[-41.2328]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9330890460957443, distance: 0.296009264588556 entropy 1.0271300077438354
epoch: 24, step: 72
	action: tensor([[-0.3046, -0.5229, -0.1426,  0.1030,  0.0365, -0.3277, -0.7246]],
       dtype=torch.float64)
	q_value: tensor([[-33.4873]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 24, step: 73
	action: tensor([[-0.2662, -1.0352,  0.5241,  0.0327, -0.0086,  1.0653,  0.1789]],
       dtype=torch.float64)
	q_value: tensor([[-36.5948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2582572768382103, distance: 1.2836346111286334 entropy 1.0271300077438354
epoch: 24, step: 74
	action: tensor([[-0.4638, -0.6322,  0.9763, -0.5845,  0.5832,  0.1713,  0.3458]],
       dtype=torch.float64)
	q_value: tensor([[-31.5416]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0542336837858883, distance: 1.6401426283918414 entropy 1.0271300077438354
epoch: 24, step: 75
	action: tensor([[-1.0278, -0.5963,  1.1616, -0.6590,  0.3984,  0.5972,  0.0340]],
       dtype=torch.float64)
	q_value: tensor([[-28.5275]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.468036432990564, distance: 1.7977631596879071 entropy 1.0271300077438354
epoch: 24, step: 76
	action: tensor([[ 0.5428,  0.1913,  1.0438,  0.8893,  0.7399,  0.5730, -0.7854]],
       dtype=torch.float64)
	q_value: tensor([[-32.4039]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9112375101750095, distance: 0.34093487715076004 entropy 1.0271300077438354
epoch: 24, step: 77
	action: tensor([[ 1.0804,  0.0515,  0.2215,  0.3972, -0.7611, -1.3794,  0.3793]],
       dtype=torch.float64)
	q_value: tensor([[-36.2630]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6430954616775229, distance: 0.683648284681258 entropy 1.0271300077438354
epoch: 24, step: 78
	action: tensor([[-1.1081,  0.1687,  0.1359, -0.5970, -0.2121,  0.4044, -0.0726]],
       dtype=torch.float64)
	q_value: tensor([[-34.9209]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1387134475900456, distance: 1.6735280399186927 entropy 1.0271300077438354
epoch: 24, step: 79
	action: tensor([[ 0.2317,  0.3236, -1.1299,  0.3438, -1.1516, -0.2959, -0.6040]],
       dtype=torch.float64)
	q_value: tensor([[-26.7267]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 24, step: 80
	action: tensor([[-0.0854, -0.8110, -0.0374, -1.1002,  0.5999,  0.5223,  0.7599]],
       dtype=torch.float64)
	q_value: tensor([[-36.5948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6435639732695246, distance: 1.4670671740738501 entropy 1.0271300077438354
epoch: 24, step: 81
	action: tensor([[-0.6266,  0.4813, -1.4956, -0.4208,  1.0918, -0.0379, -0.4678]],
       dtype=torch.float64)
	q_value: tensor([[-32.8173]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 24, step: 82
	action: tensor([[ 0.5684, -0.7220, -0.3454,  0.8073,  0.1997,  0.3446,  0.1203]],
       dtype=torch.float64)
	q_value: tensor([[-36.5948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5991189483244842, distance: 0.724543488534869 entropy 1.0271300077438354
epoch: 24, step: 83
	action: tensor([[0.0979, 1.2112, 0.4544, 0.1132, 0.2203, 1.0322, 0.4747]],
       dtype=torch.float64)
	q_value: tensor([[-26.9024]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 24, step: 84
	action: tensor([[-0.0771, -0.7531,  0.0177, -0.5343, -0.4024,  0.5382,  0.7125]],
       dtype=torch.float64)
	q_value: tensor([[-36.5948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6169841211821319, distance: 1.455156055235764 entropy 1.0271300077438354
epoch: 24, step: 85
	action: tensor([[ 0.5250,  0.0974,  0.5170,  0.0342,  0.5739, -1.0185, -0.0646]],
       dtype=torch.float64)
	q_value: tensor([[-28.1475]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5846695524945261, distance: 0.7374856568800329 entropy 1.0271300077438354
epoch: 24, step: 86
	action: tensor([[-0.6207, -1.5209,  0.5611,  0.0162,  0.4892,  0.7368,  0.3946]],
       dtype=torch.float64)
	q_value: tensor([[-28.0402]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7355187007209705, distance: 1.5075486707763288 entropy 1.0271300077438354
epoch: 24, step: 87
	action: tensor([[-0.9925, -0.8635,  0.4179, -0.1514,  0.9642,  0.4238, -0.4391]],
       dtype=torch.float64)
	q_value: tensor([[-32.2678]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2171736544374716, distance: 1.7039488273978147 entropy 1.0271300077438354
epoch: 24, step: 88
	action: tensor([[-1.3118, -0.1890, -1.0783,  0.4250, -1.1043,  0.5484,  0.3023]],
       dtype=torch.float64)
	q_value: tensor([[-33.2757]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.5889827083439907, distance: 1.841286072559187 entropy 1.0271300077438354
epoch: 24, step: 89
	action: tensor([[-0.9301, -0.4168,  0.2192,  0.1970,  0.6683, -0.5318, -1.0589]],
       dtype=torch.float64)
	q_value: tensor([[-31.7177]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1746139181464588, distance: 1.6875155169661717 entropy 1.0271300077438354
epoch: 24, step: 90
	action: tensor([[ 0.3804,  1.3091, -0.1490, -0.9988,  0.3107,  1.1258, -0.4554]],
       dtype=torch.float64)
	q_value: tensor([[-34.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 24, step: 91
	action: tensor([[-0.0823, -0.4836,  0.0506, -0.7944, -0.8130, -0.6125, -0.4444]],
       dtype=torch.float64)
	q_value: tensor([[-36.5948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.41315880950876704, distance: 1.3603547530649702 entropy 1.0271300077438354
epoch: 24, step: 92
	action: tensor([[-0.3774, -0.2066, -0.4556, -0.5568,  0.3928, -0.5083, -1.4059]],
       dtype=torch.float64)
	q_value: tensor([[-27.0778]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3588143781077464, distance: 1.3339414265286615 entropy 1.0271300077438354
epoch: 24, step: 93
	action: tensor([[ 0.3886, -1.7362,  0.0668,  0.3750,  0.7942, -0.3870,  0.3051]],
       dtype=torch.float64)
	q_value: tensor([[-32.2457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 24, step: 94
	action: tensor([[-0.3762,  0.2826,  0.2457,  1.2573, -0.0197,  1.2438,  0.3666]],
       dtype=torch.float64)
	q_value: tensor([[-36.5948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 24, step: 95
	action: tensor([[ 0.1157, -1.3216, -0.3614, -0.5806,  0.8368,  0.1011, -0.3116]],
       dtype=torch.float64)
	q_value: tensor([[-36.5948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8353534405816603, distance: 1.5503028606839446 entropy 1.0271300077438354
epoch: 24, step: 96
	action: tensor([[ 0.9812, -0.7530,  1.4409,  0.3129,  0.4199, -0.2794, -0.1733]],
       dtype=torch.float64)
	q_value: tensor([[-33.9717]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.025833246622667838, distance: 1.1590310704266855 entropy 1.0271300077438354
epoch: 24, step: 97
	action: tensor([[ 0.7271, -1.0068,  0.5556, -0.3112, -0.2522,  0.4819,  1.6554]],
       dtype=torch.float64)
	q_value: tensor([[-38.5308]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3474110129591572, distance: 1.328332319741672 entropy 1.0271300077438354
epoch: 24, step: 98
	action: tensor([[ 1.2257, -0.5391,  1.7032, -0.4286, -0.3604,  0.4542, -0.6280]],
       dtype=torch.float64)
	q_value: tensor([[-37.4464]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30309681995113325, distance: 0.9553069039954661 entropy 1.0271300077438354
epoch: 24, step: 99
	action: tensor([[ 0.4104, -0.6785,  0.0702, -0.9471,  0.0029, -0.2882, -0.1038]],
       dtype=torch.float64)
	q_value: tensor([[-38.6212]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6377365061029434, distance: 1.4644640270289364 entropy 1.0271300077438354
epoch: 24, step: 100
	action: tensor([[-0.7035, -0.7656, -1.1409, -0.5252,  0.4348,  0.1590, -0.8291]],
       dtype=torch.float64)
	q_value: tensor([[-26.7478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7191950133724065, distance: 1.5004421808704727 entropy 1.0271300077438354
epoch: 24, step: 101
	action: tensor([[ 0.0971,  0.3486, -0.5304,  0.2743, -0.8020, -0.0091,  0.0087]],
       dtype=torch.float64)
	q_value: tensor([[-32.5269]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 24, step: 102
	action: tensor([[-1.1549, -1.6105, -0.9081,  0.4908, -1.4076,  0.7220, -0.2009]],
       dtype=torch.float64)
	q_value: tensor([[-36.5948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.5624340722510817, distance: 1.831821042277751 entropy 1.0271300077438354
epoch: 24, step: 103
	action: tensor([[-0.9668,  0.3870, -0.2243,  1.0819, -0.5573,  0.4339, -0.1331]],
       dtype=torch.float64)
	q_value: tensor([[-39.3060]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09333082783576829, distance: 1.1965545158256372 entropy 1.0271300077438354
epoch: 24, step: 104
	action: tensor([[-0.8681,  0.0860,  0.1599, -0.1514,  0.5214,  1.2113, -0.1006]],
       dtype=torch.float64)
	q_value: tensor([[-26.4001]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20203039277731216, distance: 1.25462638805466 entropy 1.0271300077438354
epoch: 24, step: 105
	action: tensor([[ 0.0960, -0.5506, -1.2929, -0.2923, -1.1152, -0.0022, -0.3266]],
       dtype=torch.float64)
	q_value: tensor([[-29.8818]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.02641252824166962, distance: 1.1593582730394425 entropy 1.0271300077438354
epoch: 24, step: 106
	action: tensor([[-0.3416,  0.1013,  0.1934,  0.2243, -0.3589,  0.2994,  0.2813]],
       dtype=torch.float64)
	q_value: tensor([[-29.6127]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1261495311536186, distance: 1.069732659047593 entropy 1.0271300077438354
epoch: 24, step: 107
	action: tensor([[ 0.6343, -0.6134,  0.9522, -0.0247, -0.4821, -0.5035,  0.3424]],
       dtype=torch.float64)
	q_value: tensor([[-20.8813]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05396800207277286, distance: 1.1748174978635024 entropy 1.0271300077438354
epoch: 24, step: 108
	action: tensor([[-0.7805,  0.0671, -0.1694,  0.1698,  0.0778,  0.7864,  0.2044]],
       dtype=torch.float64)
	q_value: tensor([[-30.3487]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2516772447980753, distance: 1.280273840439028 entropy 1.0271300077438354
epoch: 24, step: 109
	action: tensor([[ 0.8319,  0.6068, -0.2078,  0.2106, -0.1529,  1.5344, -0.4205]],
       dtype=torch.float64)
	q_value: tensor([[-23.7184]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 24, step: 110
	action: tensor([[ 0.6837, -0.3139, -0.0819,  0.2405,  0.1417,  0.9425, -0.2235]],
       dtype=torch.float64)
	q_value: tensor([[-36.5948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8371964927292945, distance: 0.4617305089346696 entropy 1.0271300077438354
epoch: 24, step: 111
	action: tensor([[-0.7933,  0.5755,  0.1617,  0.5368, -1.1454, -0.5899, -0.0560]],
       dtype=torch.float64)
	q_value: tensor([[-26.2713]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 24, step: 112
	action: tensor([[ 0.3406,  0.3470,  0.1005, -1.1903,  0.6201,  0.4258,  0.6811]],
       dtype=torch.float64)
	q_value: tensor([[-36.5948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4322697342385068, distance: 0.8622391077346954 entropy 1.0271300077438354
epoch: 24, step: 113
	action: tensor([[-0.9272, -0.9137,  1.0826,  1.1141,  0.7357,  0.7770,  0.2549]],
       dtype=torch.float64)
	q_value: tensor([[-29.9121]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16443424929886696, distance: 1.0460369082065988 entropy 1.0271300077438354
epoch: 24, step: 114
	action: tensor([[-0.4725,  0.4965,  0.1243,  0.7508, -0.1063, -0.5214, -0.4068]],
       dtype=torch.float64)
	q_value: tensor([[-37.1992]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 24, step: 115
	action: tensor([[ 0.3642, -1.4096,  0.3054,  0.0087,  1.6895,  1.1037, -0.2869]],
       dtype=torch.float64)
	q_value: tensor([[-36.5948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.41928562425836136, distance: 1.363300503830914 entropy 1.0271300077438354
epoch: 24, step: 116
	action: tensor([[-1.0025,  0.4431, -0.8905,  0.1338, -1.5836, -0.1547,  1.6180]],
       dtype=torch.float64)
	q_value: tensor([[-43.0307]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6640755476502329, distance: 1.4761932419201762 entropy 1.0271300077438354
epoch: 24, step: 117
	action: tensor([[-0.6177, -0.7420,  0.2194,  0.3048, -1.5906,  0.3114,  0.2535]],
       dtype=torch.float64)
	q_value: tensor([[-37.8022]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9080897583084881, distance: 1.5807241685288813 entropy 1.0271300077438354
epoch: 24, step: 118
	action: tensor([[-0.9757,  0.4770,  0.1142,  0.8963, -0.0130, -0.6877,  0.0038]],
       dtype=torch.float64)
	q_value: tensor([[-34.8623]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15497862044948696, distance: 1.2298259873823323 entropy 1.0271300077438354
epoch: 24, step: 119
	action: tensor([[-0.0884, -0.9472,  0.7085, -0.6251,  0.0199, -0.6594, -2.1825]],
       dtype=torch.float64)
	q_value: tensor([[-28.8155]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0510001473300106, distance: 1.6388512588433972 entropy 1.0271300077438354
epoch: 24, step: 120
	action: tensor([[-0.4060,  0.6221,  0.3739,  0.2198, -0.1451,  0.1131,  0.3741]],
       dtype=torch.float64)
	q_value: tensor([[-46.9039]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 24, step: 121
	action: tensor([[ 0.1555,  0.1849,  0.6569,  1.1404, -0.0316,  0.7050,  1.1646]],
       dtype=torch.float64)
	q_value: tensor([[-36.5948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 24, step: 122
	action: tensor([[ 0.8479, -0.9088, -1.0993, -0.2743, -0.5567,  0.9195, -0.2833]],
       dtype=torch.float64)
	q_value: tensor([[-36.5948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.22142112274503867, distance: 1.2647054975154157 entropy 1.0271300077438354
epoch: 24, step: 123
	action: tensor([[-0.4032,  0.4492,  0.0739, -0.9099, -0.3953, -0.4044, -1.3515]],
       dtype=torch.float64)
	q_value: tensor([[-32.7927]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1758208666515375, distance: 1.2408728283338961 entropy 1.0271300077438354
epoch: 24, step: 124
	action: tensor([[-0.1487, -0.1426,  0.2816,  0.2710,  0.2743, -1.5741,  1.8480]],
       dtype=torch.float64)
	q_value: tensor([[-29.3253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06778857960994622, distance: 1.1824950435876247 entropy 1.0271300077438354
epoch: 24, step: 125
	action: tensor([[ 0.6126, -0.0541,  0.9564, -0.6468, -0.0981, -0.1016, -0.5002]],
       dtype=torch.float64)
	q_value: tensor([[-38.1165]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3230389208577795, distance: 0.9415394976458773 entropy 1.0271300077438354
epoch: 24, step: 126
	action: tensor([[-0.8114,  0.2156, -0.2083,  0.5675,  2.0014,  0.7854, -0.7160]],
       dtype=torch.float64)
	q_value: tensor([[-26.8051]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 24, step: 127
	action: tensor([[-1.5178, -0.2566,  1.0212, -0.0145,  0.0475,  0.9067,  0.3080]],
       dtype=torch.float64)
	q_value: tensor([[-36.5948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2745900465701792, distance: 1.7258707184407767 entropy 1.0271300077438354
LOSS epoch 24 actor 391.0439898625427 critic 246.58011067339226 
epoch: 25, step: 0
	action: tensor([[-1.6362, -0.2985,  0.0863,  0.5862,  0.5854,  0.4221,  1.1984]],
       dtype=torch.float64)
	q_value: tensor([[-35.2193]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2134064087004206, distance: 1.7025006045649511 entropy 1.0271300077438354
epoch: 25, step: 1
	action: tensor([[-0.8434, -0.0307, -0.8333, -0.5290, -0.9941, -1.0501, -1.3342]],
       dtype=torch.float64)
	q_value: tensor([[-34.3959]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06344629192204554, distance: 1.1800882166849196 entropy 1.0271300077438354
epoch: 25, step: 2
	action: tensor([[-0.2111, -1.5764,  0.2741,  0.9215, -0.7970,  1.2187, -0.1210]],
       dtype=torch.float64)
	q_value: tensor([[-37.7360]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 25, step: 3
	action: tensor([[ 0.1964, -0.3858,  0.6635, -0.2549, -0.2588,  0.9591, -0.7133]],
       dtype=torch.float64)
	q_value: tensor([[-37.6152]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31751374445564795, distance: 0.9453739870910732 entropy 1.0271300077438354
epoch: 25, step: 4
	action: tensor([[ 0.1352, -0.3075, -0.5633,  0.5869, -1.1905,  1.0571, -0.7058]],
       dtype=torch.float64)
	q_value: tensor([[-31.9456]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10300830479682199, distance: 1.0838043863413218 entropy 1.0271300077438354
epoch: 25, step: 5
	action: tensor([[ 0.1535,  0.0674,  0.5018, -0.0635, -0.8650,  0.0355,  0.2270]],
       dtype=torch.float64)
	q_value: tensor([[-32.5306]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4738065729156138, distance: 0.8300980672545435 entropy 1.0271300077438354
epoch: 25, step: 6
	action: tensor([[ 0.5986, -0.0026, -0.6432, -1.1078,  0.6623,  0.8728, -0.3627]],
       dtype=torch.float64)
	q_value: tensor([[-25.6730]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5660115258061269, distance: 0.7538688371225803 entropy 1.0271300077438354
epoch: 25, step: 7
	action: tensor([[-0.2141, -0.1267,  0.8128,  1.3018, -0.3909, -0.2061,  0.9899]],
       dtype=torch.float64)
	q_value: tensor([[-33.7352]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 25, step: 8
	action: tensor([[ 0.0592, -1.3871,  1.2371,  0.6751,  1.1992,  1.0956, -0.1783]],
       dtype=torch.float64)
	q_value: tensor([[-37.6152]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08735281909220993, distance: 1.1932788294383785 entropy 1.0271300077438354
epoch: 25, step: 9
	action: tensor([[ 0.7142, -0.1751,  0.4485, -0.3835,  0.0893,  0.5415,  0.5724]],
       dtype=torch.float64)
	q_value: tensor([[-45.2186]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5082770154081631, distance: 0.8024480901566152 entropy 1.0271300077438354
epoch: 25, step: 10
	action: tensor([[ 0.2773, -0.5344,  0.4511,  1.2199,  0.8432,  0.5183,  0.1404]],
       dtype=torch.float64)
	q_value: tensor([[-27.8197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8989635227773619, distance: 0.3637439604008749 entropy 1.0271300077438354
epoch: 25, step: 11
	action: tensor([[ 0.1980, -1.5586, -0.0229,  0.9990,  0.4831, -0.8510, -1.3828]],
       dtype=torch.float64)
	q_value: tensor([[-35.1242]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 25, step: 12
	action: tensor([[ 0.9593, -0.3675,  0.7613, -0.5652, -1.0457,  0.2040, -0.2411]],
       dtype=torch.float64)
	q_value: tensor([[-37.6152]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22700985061094003, distance: 1.0061058472577156 entropy 1.0271300077438354
epoch: 25, step: 13
	action: tensor([[ 1.1367, -0.1343, -0.5862,  0.2530,  0.8338, -0.5409, -0.2775]],
       dtype=torch.float64)
	q_value: tensor([[-32.5784]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6080452973338584, distance: 0.7164314346511769 entropy 1.0271300077438354
epoch: 25, step: 14
	action: tensor([[ 0.2746,  0.0359, -0.4077,  1.6922,  0.3630, -0.1682, -0.7968]],
       dtype=torch.float64)
	q_value: tensor([[-31.5707]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 25, step: 15
	action: tensor([[ 0.8516, -0.4020,  0.7441,  0.3516, -0.7109,  1.3975,  0.9774]],
       dtype=torch.float64)
	q_value: tensor([[-37.6152]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8814786666474161, distance: 0.39396256608259317 entropy 1.0271300077438354
epoch: 25, step: 16
	action: tensor([[ 0.9838, -0.5213,  0.6951,  0.9596,  0.4367, -0.6473,  0.7738]],
       dtype=torch.float64)
	q_value: tensor([[-39.0382]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6373632245181822, distance: 0.689116447324102 entropy 1.0271300077438354
epoch: 25, step: 17
	action: tensor([[-0.6494, -1.4551,  0.0842,  0.0820,  0.2902,  1.2471,  1.2796]],
       dtype=torch.float64)
	q_value: tensor([[-38.7451]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 25, step: 18
	action: tensor([[-0.4534, -0.7214,  0.8529, -0.4575,  0.5898,  0.4182,  1.4600]],
       dtype=torch.float64)
	q_value: tensor([[-37.6152]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9150319581340121, distance: 1.5835971307976282 entropy 1.0271300077438354
epoch: 25, step: 19
	action: tensor([[ 0.9471,  0.4561, -0.0780, -0.6032,  0.3486,  0.2753,  0.1846]],
       dtype=torch.float64)
	q_value: tensor([[-35.7196]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8390959254807997, distance: 0.4590290955269591 entropy 1.0271300077438354
epoch: 25, step: 20
	action: tensor([[-0.0693, -0.0858, -0.0372, -0.7942, -0.1946,  1.5426,  0.3651]],
       dtype=torch.float64)
	q_value: tensor([[-25.5465]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2434696770706134, distance: 0.9953363460960214 entropy 1.0271300077438354
epoch: 25, step: 21
	action: tensor([[-0.1325, -0.6118,  0.0536, -0.3640, -0.1929,  0.3866,  0.5719]],
       dtype=torch.float64)
	q_value: tensor([[-35.5346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.482774702532095, distance: 1.3934592642004429 entropy 1.0271300077438354
epoch: 25, step: 22
	action: tensor([[-0.3941, -0.8595, -0.0255,  0.0931,  0.1199, -0.4957,  0.1838]],
       dtype=torch.float64)
	q_value: tensor([[-26.1368]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8688457964803107, distance: 1.5643842226457978 entropy 1.0271300077438354
epoch: 25, step: 23
	action: tensor([[ 0.4069,  0.3183,  0.4055, -0.5046,  1.0574,  0.5627, -0.0282]],
       dtype=torch.float64)
	q_value: tensor([[-27.5155]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7144002888351657, distance: 0.6115550759746703 entropy 1.0271300077438354
epoch: 25, step: 24
	action: tensor([[-0.8301,  0.1899,  0.3618,  1.1159, -0.6181,  0.4481,  0.1438]],
       dtype=torch.float64)
	q_value: tensor([[-29.3948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 25, step: 25
	action: tensor([[ 0.6326, -0.4187,  0.2979, -0.5831, -0.7049,  0.6551, -0.4108]],
       dtype=torch.float64)
	q_value: tensor([[-37.6152]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14791820230609654, distance: 1.056324462218571 entropy 1.0271300077438354
epoch: 25, step: 26
	action: tensor([[-0.8154, -0.1139,  0.3217, -0.0324, -1.0379,  0.7843, -1.4633]],
       dtype=torch.float64)
	q_value: tensor([[-29.4763]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7723471922298069, distance: 1.5234601339926093 entropy 1.0271300077438354
epoch: 25, step: 27
	action: tensor([[-0.2327, -0.9793,  0.7128,  0.6033,  0.1489,  0.1010, -1.5223]],
       dtype=torch.float64)
	q_value: tensor([[-38.2086]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10909287176242644, distance: 1.205148737938466 entropy 1.0271300077438354
epoch: 25, step: 28
	action: tensor([[ 0.6332, -0.2244, -0.9854,  0.4632,  0.2153,  0.4508, -0.5259]],
       dtype=torch.float64)
	q_value: tensor([[-43.2610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.654066350164149, distance: 0.6730589408648604 entropy 1.0271300077438354
epoch: 25, step: 29
	action: tensor([[-0.2795, -0.3806,  1.0732,  0.0454,  0.4949,  0.3926,  0.4135]],
       dtype=torch.float64)
	q_value: tensor([[-26.1237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09948295245143379, distance: 1.1999162732345832 entropy 1.0271300077438354
epoch: 25, step: 30
	action: tensor([[ 0.9952, -1.1626, -0.1921, -0.8336,  0.1114, -1.0428,  0.9361]],
       dtype=torch.float64)
	q_value: tensor([[-29.7195]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7930426316434496, distance: 1.5323289300088125 entropy 1.0271300077438354
epoch: 25, step: 31
	action: tensor([[ 0.5718, -0.0884, -0.4752,  0.5604, -0.3165, -0.2453, -0.2381]],
       dtype=torch.float64)
	q_value: tensor([[-38.0820]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7856385976696446, distance: 0.5298220003723679 entropy 1.0271300077438354
epoch: 25, step: 32
	action: tensor([[-0.0476,  0.0677,  0.1597, -1.1289, -0.3710, -0.6960, -1.2680]],
       dtype=torch.float64)
	q_value: tensor([[-23.2027]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1892695368100481, distance: 1.2479490091214402 entropy 1.0271300077438354
epoch: 25, step: 33
	action: tensor([[-0.4953, -0.6007, -0.0716, -0.5830, -0.1235,  0.9174, -0.3520]],
       dtype=torch.float64)
	q_value: tensor([[-32.0158]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7406817392460046, distance: 1.5097894278481416 entropy 1.0271300077438354
epoch: 25, step: 34
	action: tensor([[-0.0504, -0.2718, -0.1155, -0.1531, -0.1144,  1.9654, -0.1987]],
       dtype=torch.float64)
	q_value: tensor([[-30.0684]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41606895460446436, distance: 0.8744550216891507 entropy 1.0271300077438354
epoch: 25, step: 35
	action: tensor([[ 0.6800, -1.3210, -0.0480, -0.4735,  0.3666, -0.1558, -1.5540]],
       dtype=torch.float64)
	q_value: tensor([[-37.6665]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9549327845661777, distance: 1.6000096729058837 entropy 1.0271300077438354
epoch: 25, step: 36
	action: tensor([[ 0.6749, -0.1068, -0.4229, -0.4240, -0.4737,  2.4011,  0.0620]],
       dtype=torch.float64)
	q_value: tensor([[-42.6979]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7661124203675833, distance: 0.5534269106443321 entropy 1.0271300077438354
epoch: 25, step: 37
	action: tensor([[ 0.8455, -0.5018,  0.6043, -0.0423,  0.6433, -1.0905,  0.3608]],
       dtype=torch.float64)
	q_value: tensor([[-41.0501]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23695853904827457, distance: 0.999610390042345 entropy 1.0271300077438354
epoch: 25, step: 38
	action: tensor([[-0.2216,  0.4775,  0.4398,  0.7088, -1.7194,  0.4606, -0.8674]],
       dtype=torch.float64)
	q_value: tensor([[-34.1285]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 25, step: 39
	action: tensor([[-1.0943, -0.2971,  0.1783, -1.5759, -1.7018,  0.7595,  0.5572]],
       dtype=torch.float64)
	q_value: tensor([[-37.6152]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.012858880714821, distance: 1.6235413617455723 entropy 1.0271300077438354
epoch: 25, step: 40
	action: tensor([[ 0.3562, -1.1678, -0.7023,  1.1517,  0.0354,  1.1048, -0.0717]],
       dtype=torch.float64)
	q_value: tensor([[-43.0314]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2919844859981072, distance: 0.9628931125994963 entropy 1.0271300077438354
epoch: 25, step: 41
	action: tensor([[-0.2284, -0.8817, -0.9399, -0.3263,  0.0367, -0.0837, -1.0886]],
       dtype=torch.float64)
	q_value: tensor([[-35.4701]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5599372970962113, distance: 1.4292567903812563 entropy 1.0271300077438354
epoch: 25, step: 42
	action: tensor([[ 0.4982, -0.1988, -0.4922,  0.5540, -0.4039, -0.2166, -0.4557]],
       dtype=torch.float64)
	q_value: tensor([[-33.4468]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6622251326472071, distance: 0.6650745960181736 entropy 1.0271300077438354
epoch: 25, step: 43
	action: tensor([[-0.4370,  0.6434, -0.6173,  0.2834, -0.1875,  1.1173,  0.0619]],
       dtype=torch.float64)
	q_value: tensor([[-24.4076]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 25, step: 44
	action: tensor([[-0.3759, -0.4417,  0.8142, -0.4733, -0.4669, -1.3057, -0.1413]],
       dtype=torch.float64)
	q_value: tensor([[-37.6152]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6661651822975398, distance: 1.4771198034470794 entropy 1.0271300077438354
epoch: 25, step: 45
	action: tensor([[-1.3083, -1.9472, -0.2730,  0.2290,  0.4359, -0.6442,  0.4342]],
       dtype=torch.float64)
	q_value: tensor([[-32.9504]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 25, step: 46
	action: tensor([[-0.4784,  0.2665,  0.1066, -0.5974, -0.2551,  0.5116,  0.1771]],
       dtype=torch.float64)
	q_value: tensor([[-37.6152]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24489978436038684, distance: 1.2768029843941626 entropy 1.0271300077438354
epoch: 25, step: 47
	action: tensor([[ 0.5336, -1.0297, -0.2902,  0.7192,  0.4217,  0.7069, -0.2530]],
       dtype=torch.float64)
	q_value: tensor([[-24.9616]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4189699242109991, distance: 0.8722801707575064 entropy 1.0271300077438354
epoch: 25, step: 48
	action: tensor([[ 0.4426, -0.3478, -0.5652, -0.4345,  0.5759, -0.5487, -0.0713]],
       dtype=torch.float64)
	q_value: tensor([[-33.2600]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.007803655272570875, distance: 1.1398704749126205 entropy 1.0271300077438354
epoch: 25, step: 49
	action: tensor([[ 0.8152, -0.2304,  0.8161,  0.6960, -0.8331,  0.7461,  1.2834]],
       dtype=torch.float64)
	q_value: tensor([[-28.0089]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9067786463861005, distance: 0.34939315760899886 entropy 1.0271300077438354
epoch: 25, step: 50
	action: tensor([[ 0.3189, -0.6623, -0.2011, -0.0944,  1.6354, -0.4779, -0.1415]],
       dtype=torch.float64)
	q_value: tensor([[-38.0356]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.19049608756616343, distance: 1.2485923781360628 entropy 1.0271300077438354
epoch: 25, step: 51
	action: tensor([[ 0.7813, -0.8777, -0.2502,  1.1673,  0.7821, -0.4415, -0.2990]],
       dtype=torch.float64)
	q_value: tensor([[-39.0277]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5499137749085821, distance: 0.7677230041201012 entropy 1.0271300077438354
epoch: 25, step: 52
	action: tensor([[ 0.2661, -0.6222, -0.1061,  0.5167,  0.4958, -0.4957, -0.0657]],
       dtype=torch.float64)
	q_value: tensor([[-38.4165]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09521419489358418, distance: 1.088502880129369 entropy 1.0271300077438354
epoch: 25, step: 53
	action: tensor([[-0.8142, -0.0807,  0.5528, -0.1461,  0.0965, -0.3656, -0.5511]],
       dtype=torch.float64)
	q_value: tensor([[-28.4624]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9732359692635733, distance: 1.6074823199185637 entropy 1.0271300077438354
epoch: 25, step: 54
	action: tensor([[-0.5207, -0.4803,  0.4513, -0.2335, -0.2711, -0.7905,  0.3667]],
       dtype=torch.float64)
	q_value: tensor([[-28.2059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8739122203456493, distance: 1.5665033030772955 entropy 1.0271300077438354
epoch: 25, step: 55
	action: tensor([[-0.0367,  0.1370, -0.5132, -0.1253, -0.1298, -0.0039,  0.2055]],
       dtype=torch.float64)
	q_value: tensor([[-26.8692]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31607858113300935, distance: 0.9463674529249654 entropy 1.0271300077438354
epoch: 25, step: 56
	action: tensor([[-0.3673, -1.0273, -0.8342, -1.4322, -0.2854,  0.5650,  0.9577]],
       dtype=torch.float64)
	q_value: tensor([[-19.6624]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2694022685393982, distance: 1.2893069636262142 entropy 1.0271300077438354
epoch: 25, step: 57
	action: tensor([[-0.6013, -0.3136,  0.9018, -0.0909,  0.1394,  1.2827,  1.0808]],
       dtype=torch.float64)
	q_value: tensor([[-39.1896]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07783601403879992, distance: 1.1880454041568047 entropy 1.0271300077438354
epoch: 25, step: 58
	action: tensor([[-0.3819,  1.1684,  0.5991,  0.6963,  0.7199,  0.4353, -0.3356]],
       dtype=torch.float64)
	q_value: tensor([[-37.0200]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 25, step: 59
	action: tensor([[-2.8239e-01,  2.9785e-01,  7.1539e-04,  2.9820e-01,  8.5141e-02,
          3.2033e-01, -1.1108e+00]], dtype=torch.float64)
	q_value: tensor([[-37.6152]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 25, step: 60
	action: tensor([[-0.1394,  0.6513, -0.8728,  0.7800, -0.4309,  0.4230,  0.5187]],
       dtype=torch.float64)
	q_value: tensor([[-37.6152]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 25, step: 61
	action: tensor([[ 0.6738, -0.7991,  0.0990,  0.5240, -0.3148,  1.8664, -0.4399]],
       dtype=torch.float64)
	q_value: tensor([[-37.6152]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7244641455406212, distance: 0.6006835700766108 entropy 1.0271300077438354
epoch: 25, step: 62
	action: tensor([[-0.5778,  0.3942, -0.6330,  0.2532, -0.8403, -0.3064,  0.6341]],
       dtype=torch.float64)
	q_value: tensor([[-41.0660]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0033369393738220676, distance: 1.1462519675435388 entropy 1.0271300077438354
epoch: 25, step: 63
	action: tensor([[ 0.0958, -0.8760,  0.3728,  0.1414, -0.2102, -0.8898,  0.3309]],
       dtype=torch.float64)
	q_value: tensor([[-27.1777]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5007926727849539, distance: 1.4019000254473468 entropy 1.0271300077438354
epoch: 25, step: 64
	action: tensor([[ 0.0431,  0.3980,  1.4186, -0.0761,  0.8111, -0.1840, -0.2960]],
       dtype=torch.float64)
	q_value: tensor([[-30.4821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 25, step: 65
	action: tensor([[ 0.6936, -1.3976,  0.1684, -1.3540,  0.9853,  0.9723,  0.6945]],
       dtype=torch.float64)
	q_value: tensor([[-37.6152]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7982620680765022, distance: 1.5345575668008409 entropy 1.0271300077438354
epoch: 25, step: 66
	action: tensor([[ 0.7428, -0.7472,  1.0826,  0.0385,  0.3403, -0.0531,  0.1474]],
       dtype=torch.float64)
	q_value: tensor([[-44.3227]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06110101987747818, distance: 1.1787862442636563 entropy 1.0271300077438354
epoch: 25, step: 67
	action: tensor([[ 0.0953, -0.5700, -0.8351, -0.0164,  1.3013,  0.7061,  0.4746]],
       dtype=torch.float64)
	q_value: tensor([[-33.7169]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16978890645336864, distance: 1.042679798642659 entropy 1.0271300077438354
epoch: 25, step: 68
	action: tensor([[ 0.2433,  0.6208, -0.6524, -0.3671,  0.8215,  0.5396, -0.4795]],
       dtype=torch.float64)
	q_value: tensor([[-35.9897]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 25, step: 69
	action: tensor([[ 0.7056, -0.2856,  0.2104,  1.4099, -1.5864,  0.1243, -0.4110]],
       dtype=torch.float64)
	q_value: tensor([[-37.6152]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 25, step: 70
	action: tensor([[ 0.7371, -0.2783,  0.5897, -0.9894,  0.1568,  0.6610,  0.1538]],
       dtype=torch.float64)
	q_value: tensor([[-37.6152]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14718283956638534, distance: 1.0567801779319104 entropy 1.0271300077438354
epoch: 25, step: 71
	action: tensor([[ 1.0500, -0.0496,  0.7658,  0.3540,  0.1078,  0.6823, -0.5734]],
       dtype=torch.float64)
	q_value: tensor([[-30.5490]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7864077149510634, distance: 0.5288706597225477 entropy 1.0271300077438354
epoch: 25, step: 72
	action: tensor([[ 0.8577, -0.0425,  0.7234,  0.0461,  0.1682, -0.1606, -0.3053]],
       dtype=torch.float64)
	q_value: tensor([[-33.0977]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7002428619966249, distance: 0.6265294070840021 entropy 1.0271300077438354
epoch: 25, step: 73
	action: tensor([[-0.1994, -0.2451, -0.1015,  0.4736,  0.6681, -0.0059, -0.3369]],
       dtype=torch.float64)
	q_value: tensor([[-28.1842]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13193136077697154, distance: 1.0661878439081685 entropy 1.0271300077438354
epoch: 25, step: 74
	action: tensor([[-1.2045,  1.2501,  0.7493,  0.3482,  1.7567,  0.7678,  0.8109]],
       dtype=torch.float64)
	q_value: tensor([[-26.0863]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 25, step: 75
	action: tensor([[ 0.1123, -1.2286,  1.3231, -0.7993, -1.2092,  0.2374,  0.3700]],
       dtype=torch.float64)
	q_value: tensor([[-37.6152]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.00240980308357, distance: 1.619321845016794 entropy 1.0271300077438354
epoch: 25, step: 76
	action: tensor([[ 0.5389, -0.7786,  1.5341,  0.4920, -1.1228, -1.0241, -0.6934]],
       dtype=torch.float64)
	q_value: tensor([[-40.8458]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23736635368546333, distance: 1.27293387668286 entropy 1.0271300077438354
epoch: 25, step: 77
	action: tensor([[ 0.1459, -0.3147, -0.1413,  0.2851,  0.8053,  0.5641, -0.4147]],
       dtype=torch.float64)
	q_value: tensor([[-49.1827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5192617352713519, distance: 0.7934344242658601 entropy 1.0271300077438354
epoch: 25, step: 78
	action: tensor([[-0.5772, -0.9252, -0.4538, -0.1759, -0.5770,  0.2939,  0.7225]],
       dtype=torch.float64)
	q_value: tensor([[-28.1348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.020564106134997, distance: 1.6266458524068503 entropy 1.0271300077438354
epoch: 25, step: 79
	action: tensor([[ 0.0832, -0.2447, -0.0785, -0.7708,  0.0324,  0.3030, -0.7168]],
       dtype=torch.float64)
	q_value: tensor([[-30.2697]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15065447560680245, distance: 1.2275216449323083 entropy 1.0271300077438354
epoch: 25, step: 80
	action: tensor([[ 0.0403, -0.0284, -0.4992,  0.8204,  0.6919,  0.4730, -0.1375]],
       dtype=torch.float64)
	q_value: tensor([[-26.1258]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 25, step: 81
	action: tensor([[-0.3620, -1.1575, -1.0096,  0.1759, -0.0313,  0.1561,  1.0384]],
       dtype=torch.float64)
	q_value: tensor([[-37.6152]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0270494717316123, distance: 1.6292542679303008 entropy 1.0271300077438354
epoch: 25, step: 82
	action: tensor([[ 0.3169, -1.2538,  0.5660,  0.7597, -0.3165, -0.9456, -0.4953]],
       dtype=torch.float64)
	q_value: tensor([[-34.1139]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3533801231361027, distance: 1.331271363525362 entropy 1.0271300077438354
epoch: 25, step: 83
	action: tensor([[-0.6229, -0.2405,  0.8090,  0.6958, -0.8876, -0.8073, -0.6470]],
       dtype=torch.float64)
	q_value: tensor([[-41.7598]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12696574541588435, distance: 1.2148203297079803 entropy 1.0271300077438354
epoch: 25, step: 84
	action: tensor([[ 0.2403,  0.3711,  0.5935, -0.1847,  0.5715,  0.5722, -1.2148]],
       dtype=torch.float64)
	q_value: tensor([[-37.9439]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 25, step: 85
	action: tensor([[-0.4933,  0.6990, -0.0283,  1.2206, -0.4162,  0.1688,  1.5433]],
       dtype=torch.float64)
	q_value: tensor([[-37.6152]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 25, step: 86
	action: tensor([[ 0.7419, -1.3868, -0.5318,  0.4810, -0.7445,  0.3790, -1.0733]],
       dtype=torch.float64)
	q_value: tensor([[-37.6152]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1782282630667742, distance: 1.2421424712692708 entropy 1.0271300077438354
epoch: 25, step: 87
	action: tensor([[-0.2481,  1.6007,  0.2022, -0.2828, -0.4024,  0.9376, -0.4439]],
       dtype=torch.float64)
	q_value: tensor([[-37.9693]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 25, step: 88
	action: tensor([[-0.7740,  0.0660,  1.4283,  0.4698,  1.8325,  0.5781, -0.1217]],
       dtype=torch.float64)
	q_value: tensor([[-37.6152]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.25713824266780816, distance: 1.283063682378982 entropy 1.0271300077438354
epoch: 25, step: 89
	action: tensor([[-0.8477, -2.0173,  1.3873,  0.0841, -0.1255,  0.0961,  1.0642]],
       dtype=torch.float64)
	q_value: tensor([[-45.8778]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 25, step: 90
	action: tensor([[-0.4813,  1.2786,  0.3056, -0.7104,  0.3202,  0.8128, -0.4533]],
       dtype=torch.float64)
	q_value: tensor([[-37.6152]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 25, step: 91
	action: tensor([[-0.3473, -1.0938,  1.1736, -1.6462,  0.1386,  0.5847, -0.8843]],
       dtype=torch.float64)
	q_value: tensor([[-37.6152]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1077319820119471, distance: 1.6613624332873993 entropy 1.0271300077438354
epoch: 25, step: 92
	action: tensor([[ 0.5303, -0.0225,  0.0075, -0.8294, -1.8090,  0.4718, -0.1180]],
       dtype=torch.float64)
	q_value: tensor([[-43.6163]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14038127643318632, distance: 1.0609859344389019 entropy 1.0271300077438354
epoch: 25, step: 93
	action: tensor([[ 0.5944, -0.2830, -1.1940, -0.1069, -0.1495,  0.7658, -0.1054]],
       dtype=torch.float64)
	q_value: tensor([[-35.6465]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44251629894590605, distance: 0.8544227050148557 entropy 1.0271300077438354
epoch: 25, step: 94
	action: tensor([[ 0.2173,  1.0250, -0.4917,  0.8632, -0.5864, -0.2174,  2.0794]],
       dtype=torch.float64)
	q_value: tensor([[-27.4455]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 25, step: 95
	action: tensor([[ 0.8195, -0.6385,  0.0861,  0.9485, -0.1864, -0.1555, -0.0660]],
       dtype=torch.float64)
	q_value: tensor([[-37.6152]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7206750129033286, distance: 0.6047997276971119 entropy 1.0271300077438354
epoch: 25, step: 96
	action: tensor([[-1.0243,  0.6389,  0.8849, -1.0225,  0.6674,  1.2401,  0.4048]],
       dtype=torch.float64)
	q_value: tensor([[-32.7103]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5974150268977849, distance: 1.4463239439814466 entropy 1.0271300077438354
epoch: 25, step: 97
	action: tensor([[ 0.5809,  0.0849, -0.0703, -0.2580,  0.2692,  2.0550, -0.0744]],
       dtype=torch.float64)
	q_value: tensor([[-39.7424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 25, step: 98
	action: tensor([[-0.8339, -0.5880, -0.6465,  1.0003,  0.1433, -0.1061,  0.7292]],
       dtype=torch.float64)
	q_value: tensor([[-37.6152]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.041130304938401, distance: 1.6349032556632241 entropy 1.0271300077438354
epoch: 25, step: 99
	action: tensor([[ 0.2622, -0.5950,  0.8229, -0.2439, -0.0249,  0.3723, -0.1101]],
       dtype=torch.float64)
	q_value: tensor([[-30.9709]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10521201974515781, distance: 1.2030384086070394 entropy 1.0271300077438354
epoch: 25, step: 100
	action: tensor([[-0.0756,  1.1328,  1.1746,  1.0219, -0.7957,  0.4752,  0.6906]],
       dtype=torch.float64)
	q_value: tensor([[-28.1269]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 25, step: 101
	action: tensor([[ 1.7434, -0.5054,  1.2904, -0.2605, -0.8209,  0.8688,  0.8414]],
       dtype=torch.float64)
	q_value: tensor([[-37.6152]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1536784686054764, distance: 1.052747910865278 entropy 1.0271300077438354
epoch: 25, step: 102
	action: tensor([[ 0.2128,  1.7561,  1.1104,  0.5590, -0.2204,  1.1234,  1.6931]],
       dtype=torch.float64)
	q_value: tensor([[-41.9372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 25, step: 103
	action: tensor([[-0.5054, -0.4577, -0.2725,  0.7653, -0.8773, -0.0058, -0.3896]],
       dtype=torch.float64)
	q_value: tensor([[-37.6152]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3168171506507216, distance: 1.3131653976976456 entropy 1.0271300077438354
epoch: 25, step: 104
	action: tensor([[-0.1568, -0.7936, -0.7364,  0.4955,  0.3476, -0.5609,  0.2182]],
       dtype=torch.float64)
	q_value: tensor([[-30.0426]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5485781168323978, distance: 1.424043488194396 entropy 1.0271300077438354
epoch: 25, step: 105
	action: tensor([[-0.1850,  0.4374,  1.6231,  0.3491,  0.2299, -0.1350, -0.2240]],
       dtype=torch.float64)
	q_value: tensor([[-30.0033]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 25, step: 106
	action: tensor([[-0.5442, -0.4379,  0.1065,  1.7111, -1.1407, -0.2648,  0.6723]],
       dtype=torch.float64)
	q_value: tensor([[-37.6152]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 25, step: 107
	action: tensor([[ 0.8161, -1.0059,  1.0482,  0.0866, -0.0128,  1.2236,  0.0836]],
       dtype=torch.float64)
	q_value: tensor([[-37.6152]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12101430087357612, distance: 1.0728712261766145 entropy 1.0271300077438354
epoch: 25, step: 108
	action: tensor([[-0.0123, -0.2560, -0.2464, -0.6423, -1.0738,  0.9509, -0.1066]],
       dtype=torch.float64)
	q_value: tensor([[-39.2659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1990837401421912, distance: 1.2530876513359146 entropy 1.0271300077438354
epoch: 25, step: 109
	action: tensor([[-1.4845,  0.0818, -0.5592, -0.2608, -0.0708, -0.0155,  0.9467]],
       dtype=torch.float64)
	q_value: tensor([[-31.0590]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.4564567955267687, distance: 1.793540790672175 entropy 1.0271300077438354
epoch: 25, step: 110
	action: tensor([[-0.4218,  0.5307,  0.1309,  0.4980,  0.5341,  1.1306,  0.9842]],
       dtype=torch.float64)
	q_value: tensor([[-31.3025]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 25, step: 111
	action: tensor([[-1.0861, -0.6750, -0.2193, -0.7369, -0.6999,  0.2051, -0.3946]],
       dtype=torch.float64)
	q_value: tensor([[-37.6152]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.25141815100327, distance: 1.7170572467719085 entropy 1.0271300077438354
epoch: 25, step: 112
	action: tensor([[-0.0808, -0.0083,  0.7016, -0.7255,  0.0782,  1.1547,  1.0960]],
       dtype=torch.float64)
	q_value: tensor([[-32.4577]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2093960531354101, distance: 1.017504133843999 entropy 1.0271300077438354
epoch: 25, step: 113
	action: tensor([[ 0.8054, -0.9182, -0.3079, -0.1943,  0.1094, -0.7715, -1.2377]],
       dtype=torch.float64)
	q_value: tensor([[-36.1376]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5380217464332757, distance: 1.4191814677090224 entropy 1.0271300077438354
epoch: 25, step: 114
	action: tensor([[-0.1957,  0.2520,  0.0194,  0.9927,  1.2302,  0.4887,  0.5206]],
       dtype=torch.float64)
	q_value: tensor([[-37.6090]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 25, step: 115
	action: tensor([[-0.1145, -1.5329,  0.2171,  0.6294, -1.2556,  1.8093,  0.4828]],
       dtype=torch.float64)
	q_value: tensor([[-37.6152]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.19125504204859367, distance: 1.2489903104811588 entropy 1.0271300077438354
epoch: 25, step: 116
	action: tensor([[-0.2097, -1.0341, -0.6157,  0.1522, -0.0782, -0.0284, -0.1303]],
       dtype=torch.float64)
	q_value: tensor([[-44.6824]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8028814318354278, distance: 1.5365272833496932 entropy 1.0271300077438354
epoch: 25, step: 117
	action: tensor([[ 0.0277, -1.2240, -1.1049, -0.8304,  0.0854, -0.1570, -0.8464]],
       dtype=torch.float64)
	q_value: tensor([[-29.2564]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.36847653108366596, distance: 1.3386756696140891 entropy 1.0271300077438354
epoch: 25, step: 118
	action: tensor([[-1.0625, -1.4933,  0.2496, -0.3616,  0.3643,  1.8880, -1.0150]],
       dtype=torch.float64)
	q_value: tensor([[-37.6528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 25, step: 119
	action: tensor([[ 0.2980, -0.7153,  0.4592,  0.1110,  0.0534,  0.3170,  0.1804]],
       dtype=torch.float64)
	q_value: tensor([[-37.6152]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08966700295783503, distance: 1.09183455695255 entropy 1.0271300077438354
epoch: 25, step: 120
	action: tensor([[ 0.7458, -0.6943,  0.2587,  0.4088, -1.1574, -1.0482,  1.1919]],
       dtype=torch.float64)
	q_value: tensor([[-26.9875]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18584246940373517, distance: 1.0325495824385895 entropy 1.0271300077438354
epoch: 25, step: 121
	action: tensor([[ 0.6546, -1.6115, -0.6084,  0.4987, -0.1114,  0.5916, -0.5796]],
       dtype=torch.float64)
	q_value: tensor([[-39.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 25, step: 122
	action: tensor([[ 0.3039, -0.1281,  0.4621,  0.0586,  0.6134,  0.3256,  0.4072]],
       dtype=torch.float64)
	q_value: tensor([[-37.6152]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6085909963543485, distance: 0.7159325350316457 entropy 1.0271300077438354
epoch: 25, step: 123
	action: tensor([[-0.5583,  0.1424, -1.6106, -0.1551, -0.4479, -0.7403,  1.4620]],
       dtype=torch.float64)
	q_value: tensor([[-26.2205]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17293078094456427, distance: 1.0407049549049405 entropy 1.0271300077438354
epoch: 25, step: 124
	action: tensor([[-0.6267, -0.7625,  0.9101, -0.8780, -0.3471,  0.4699, -0.6419]],
       dtype=torch.float64)
	q_value: tensor([[-36.3313]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2746139925623203, distance: 1.7258798030619709 entropy 1.0271300077438354
epoch: 25, step: 125
	action: tensor([[ 0.3447, -0.7496,  1.2034, -0.2568,  0.2316,  0.5172,  0.4476]],
       dtype=torch.float64)
	q_value: tensor([[-34.8146]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13974788131054505, distance: 1.2216901997963678 entropy 1.0271300077438354
epoch: 25, step: 126
	action: tensor([[ 0.8356, -0.7605,  0.1180, -1.0516, -0.4510,  0.8593,  1.4652]],
       dtype=torch.float64)
	q_value: tensor([[-33.1038]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.33950646790242, distance: 1.324430278276027 entropy 1.0271300077438354
epoch: 25, step: 127
	action: tensor([[ 0.4973, -0.6321,  0.1518,  1.1686, -0.0440,  0.8452, -0.0663]],
       dtype=torch.float64)
	q_value: tensor([[-40.4433]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9276388799221293, distance: 0.30782885973559904 entropy 1.0271300077438354
LOSS epoch 25 actor 445.8623716109648 critic 183.08439897872435 
epoch: 26, step: 0
	action: tensor([[ 0.4150, -0.7400,  0.4720, -0.2836,  0.0649,  0.7328, -0.9374]],
       dtype=torch.float64)
	q_value: tensor([[-35.1535]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.016776494230798633, distance: 1.1347046107522527 entropy 1.0271300077438354
epoch: 26, step: 1
	action: tensor([[ 0.1720, -0.0592,  0.3105, -0.6180, -0.2400,  0.0568,  0.9590]],
       dtype=torch.float64)
	q_value: tensor([[-35.5323]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.029216288248164357, distance: 1.127503590984123 entropy 1.0271300077438354
epoch: 26, step: 2
	action: tensor([[ 0.7304, -0.4733,  0.2650, -0.6116,  0.8289,  0.1935,  0.5078]],
       dtype=torch.float64)
	q_value: tensor([[-27.9127]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07334910779502568, distance: 1.1855699776692479 entropy 1.0271300077438354
epoch: 26, step: 3
	action: tensor([[0.6707, 0.7163, 0.7550, 0.7550, 0.8855, 0.3594, 1.3556]],
       dtype=torch.float64)
	q_value: tensor([[-32.3204]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8587443950125176, distance: 0.4300901912526265 entropy 1.0271300077438354
epoch: 26, step: 4
	action: tensor([[ 0.6141, -1.0240,  0.0775,  0.1763,  0.0733,  0.6693, -0.1739]],
       dtype=torch.float64)
	q_value: tensor([[-41.4187]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0565065485821018, distance: 1.1115426681148746 entropy 1.0271300077438354
epoch: 26, step: 5
	action: tensor([[ 1.3633, -1.0245, -0.0471,  0.0763, -0.8261, -0.2074,  0.9733]],
       dtype=torch.float64)
	q_value: tensor([[-33.7173]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4966571328606273, distance: 1.3999671758686723 entropy 1.0271300077438354
epoch: 26, step: 6
	action: tensor([[-0.0219,  0.3390,  0.2211,  1.1519, -1.4779,  1.0104, -0.0340]],
       dtype=torch.float64)
	q_value: tensor([[-40.3286]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 26, step: 7
	action: tensor([[ 0.6878, -0.6349,  0.6362,  0.1474, -0.8744,  0.9584,  0.4655]],
       dtype=torch.float64)
	q_value: tensor([[-38.3659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5436039841270748, distance: 0.7730856563653188 entropy 1.0271300077438354
epoch: 26, step: 8
	action: tensor([[ 1.3695, -0.9344, -0.7942,  0.3889, -0.1595,  0.8201,  0.4155]],
       dtype=torch.float64)
	q_value: tensor([[-35.4399]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2875366190435068, distance: 0.9659129017679539 entropy 1.0271300077438354
epoch: 26, step: 9
	action: tensor([[ 0.5365,  0.3945,  0.7304,  0.5607, -0.4066,  1.0997, -0.4927]],
       dtype=torch.float64)
	q_value: tensor([[-37.9608]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9426610671214114, distance: 0.27401951690762916 entropy 1.0271300077438354
epoch: 26, step: 10
	action: tensor([[-1.0465, -1.2406,  0.4125, -0.3037, -1.5839,  0.7665, -1.0802]],
       dtype=torch.float64)
	q_value: tensor([[-33.7329]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 26, step: 11
	action: tensor([[ 1.0115, -1.7585,  0.3147,  0.3104, -0.7038,  0.0025, -0.1066]],
       dtype=torch.float64)
	q_value: tensor([[-38.3659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 26, step: 12
	action: tensor([[ 1.4897, -0.8397,  0.7539, -1.0075, -0.5640,  1.2660,  0.3743]],
       dtype=torch.float64)
	q_value: tensor([[-38.3659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.044658606820299984, distance: 1.118500006447157 entropy 1.0271300077438354
epoch: 26, step: 13
	action: tensor([[-0.4876,  0.6865,  0.3238,  0.2782,  1.1823,  0.4319,  0.8886]],
       dtype=torch.float64)
	q_value: tensor([[-44.0974]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 26, step: 14
	action: tensor([[ 0.2781, -1.2178,  0.2101, -1.0971, -0.7129,  2.1399, -0.2127]],
       dtype=torch.float64)
	q_value: tensor([[-38.3659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.27406997311593084, distance: 1.2916752364883044 entropy 1.0271300077438354
epoch: 26, step: 15
	action: tensor([[ 1.3830,  0.7301, -0.0433,  0.1880, -0.9542,  0.4426,  0.2229]],
       dtype=torch.float64)
	q_value: tensor([[-50.3021]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8967340788428203, distance: 0.36773520168456125 entropy 1.0271300077438354
epoch: 26, step: 16
	action: tensor([[-0.2769, -1.0730,  0.8288,  0.1923,  0.6860,  0.3122, -0.1920]],
       dtype=torch.float64)
	q_value: tensor([[-31.9107]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5500240421073859, distance: 1.4247081560054706 entropy 1.0271300077438354
epoch: 26, step: 17
	action: tensor([[ 1.0055e+00, -7.7680e-01, -3.3416e-01, -3.7352e-01, -6.7733e-01,
          8.9721e-01,  1.5291e-04]], dtype=torch.float64)
	q_value: tensor([[-35.8020]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05784670113385415, distance: 1.1769772308374844 entropy 1.0271300077438354
epoch: 26, step: 18
	action: tensor([[ 1.0786, -0.5160,  0.6113,  0.6997, -0.5026,  0.7019, -0.1424]],
       dtype=torch.float64)
	q_value: tensor([[-34.6882]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.737867047295546, distance: 0.5858919123034959 entropy 1.0271300077438354
epoch: 26, step: 19
	action: tensor([[ 0.5412, -0.4340, -0.8415,  1.2058,  1.4804, -1.4469, -1.0777]],
       dtype=torch.float64)
	q_value: tensor([[-36.5566]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21175848420192867, distance: 1.015982776797474 entropy 1.0271300077438354
epoch: 26, step: 20
	action: tensor([[-0.5403, -0.2884, -0.1000, -0.5060, -0.4786,  1.3988, -0.7554]],
       dtype=torch.float64)
	q_value: tensor([[-51.0344]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.49565506563130657, distance: 1.3994984325293494 entropy 1.0271300077438354
epoch: 26, step: 21
	action: tensor([[-0.7368, -0.8343,  0.7284,  0.3198, -0.8918, -0.2254, -0.3383]],
       dtype=torch.float64)
	q_value: tensor([[-35.9642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8442121695900651, distance: 1.5540397923052187 entropy 1.0271300077438354
epoch: 26, step: 22
	action: tensor([[-0.5024, -0.9876, -0.0690,  0.0751,  0.5457,  1.0571,  1.3862]],
       dtype=torch.float64)
	q_value: tensor([[-37.3351]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4709503242880422, distance: 1.3878920767085314 entropy 1.0271300077438354
epoch: 26, step: 23
	action: tensor([[-0.3855, -1.3092, -0.3881, -0.0740,  0.4571, -0.3455, -0.5123]],
       dtype=torch.float64)
	q_value: tensor([[-38.6948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 26, step: 24
	action: tensor([[-0.7267, -1.0564,  1.0619, -0.0946, -0.1998,  0.7195,  1.0885]],
       dtype=torch.float64)
	q_value: tensor([[-38.3659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8632775707221418, distance: 1.5620519428196968 entropy 1.0271300077438354
epoch: 26, step: 25
	action: tensor([[-0.3035,  0.2096,  0.4260,  0.2705,  0.7440, -0.9766, -0.4875]],
       dtype=torch.float64)
	q_value: tensor([[-38.1881]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 26, step: 26
	action: tensor([[-1.1155, -0.6819,  0.4252,  0.1289, -1.1621,  0.5694, -0.0061]],
       dtype=torch.float64)
	q_value: tensor([[-38.3659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.4079400213376707, distance: 1.7757406056853995 entropy 1.0271300077438354
epoch: 26, step: 27
	action: tensor([[ 1.2780, -1.3475, -0.6728,  0.0964, -1.5727, -0.0394, -0.6521]],
       dtype=torch.float64)
	q_value: tensor([[-37.2848]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5837919465528256, distance: 1.4401434662487798 entropy 1.0271300077438354
epoch: 26, step: 28
	action: tensor([[-1.2334, -0.6823,  0.7698,  0.9427,  0.1240,  0.2615,  0.3038]],
       dtype=torch.float64)
	q_value: tensor([[-43.8945]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6065250191798726, distance: 1.4504422437267557 entropy 1.0271300077438354
epoch: 26, step: 29
	action: tensor([[-0.1811,  0.0161, -0.7396, -0.2432, -0.6722, -0.1047,  0.6024]],
       dtype=torch.float64)
	q_value: tensor([[-36.1966]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10427940740169539, distance: 1.0830361991989856 entropy 1.0271300077438354
epoch: 26, step: 30
	action: tensor([[ 0.4174, -0.3904,  0.2636, -0.4698,  0.5311,  0.8909,  0.1392]],
       dtype=torch.float64)
	q_value: tensor([[-25.5887]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2934887072099086, distance: 0.9618697067986146 entropy 1.0271300077438354
epoch: 26, step: 31
	action: tensor([[ 0.4558, -0.0497,  0.2430, -0.9317,  0.3256,  0.9870,  0.2312]],
       dtype=torch.float64)
	q_value: tensor([[-31.0110]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4127673750601343, distance: 0.8769236465364069 entropy 1.0271300077438354
epoch: 26, step: 32
	action: tensor([[-1.4335,  0.0681, -0.6346, -0.3152,  0.1547,  1.0594,  0.5953]],
       dtype=torch.float64)
	q_value: tensor([[-32.3510]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1715155375271427, distance: 1.6863129059954893 entropy 1.0271300077438354
epoch: 26, step: 33
	action: tensor([[-0.8233, -0.4607,  0.9751,  0.3429,  0.2179,  1.5408,  1.1471]],
       dtype=torch.float64)
	q_value: tensor([[-35.2016]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11380026783069708, distance: 1.0772648772316507 entropy 1.0271300077438354
epoch: 26, step: 34
	action: tensor([[ 5.9627e-01, -4.3538e-01, -3.8138e-04,  7.4977e-01, -7.9330e-01,
         -3.8888e-01,  1.2051e+00]], dtype=torch.float64)
	q_value: tensor([[-42.2649]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7162723041875612, distance: 0.609547506225319 entropy 1.0271300077438354
epoch: 26, step: 35
	action: tensor([[-0.9498, -0.6110, -0.2547, -1.3113,  0.1460,  1.7896,  0.1021]],
       dtype=torch.float64)
	q_value: tensor([[-36.3206]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6457813985800231, distance: 1.4680564922376171 entropy 1.0271300077438354
epoch: 26, step: 36
	action: tensor([[ 0.7031, -1.2935,  0.8291,  0.3825,  0.3642,  2.5072, -0.3392]],
       dtype=torch.float64)
	q_value: tensor([[-45.8771]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 26, step: 37
	action: tensor([[ 0.1872, -1.1493,  0.6473, -0.0596, -1.5872,  0.6304, -0.4379]],
       dtype=torch.float64)
	q_value: tensor([[-38.3659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6139205907838023, distance: 1.4537769358493364 entropy 1.0271300077438354
epoch: 26, step: 38
	action: tensor([[-0.1186,  0.4249, -0.7312, -0.5876, -0.2361,  1.4730,  0.2218]],
       dtype=torch.float64)
	q_value: tensor([[-43.8444]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 26, step: 39
	action: tensor([[-0.0447, -1.0300,  1.1225,  0.1792,  0.0730, -0.0384,  0.7396]],
       dtype=torch.float64)
	q_value: tensor([[-38.3659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5152096057797055, distance: 1.408617406473585 entropy 1.0271300077438354
epoch: 26, step: 40
	action: tensor([[ 0.9642,  0.6020, -0.6193, -0.6267,  0.3926, -0.3650, -1.3917]],
       dtype=torch.float64)
	q_value: tensor([[-35.5748]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7838345350878141, distance: 0.5320468164692388 entropy 1.0271300077438354
epoch: 26, step: 41
	action: tensor([[ 1.1741, -0.4379,  0.1086,  0.6739,  1.4366,  1.2306,  1.6046]],
       dtype=torch.float64)
	q_value: tensor([[-32.9118]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5164868406258147, distance: 0.7957210416422877 entropy 1.0271300077438354
epoch: 26, step: 42
	action: tensor([[0.7421, 0.1936, 0.6571, 0.2299, 0.5627, 0.1558, 0.9379]],
       dtype=torch.float64)
	q_value: tensor([[-50.0284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9415613449891753, distance: 0.2766347915947618 entropy 1.0271300077438354
epoch: 26, step: 43
	action: tensor([[ 0.6018,  0.2083, -0.7848,  1.1964, -0.0804,  0.0933, -0.5574]],
       dtype=torch.float64)
	q_value: tensor([[-32.6575]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 26, step: 44
	action: tensor([[-0.4062, -3.1759,  0.9723, -0.1372, -0.6425,  0.2913,  0.7241]],
       dtype=torch.float64)
	q_value: tensor([[-38.3659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 26, step: 45
	action: tensor([[ 0.0058, -0.6598,  1.1256,  0.0035, -0.4600,  0.3395,  1.3530]],
       dtype=torch.float64)
	q_value: tensor([[-38.3659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11071034521181389, distance: 1.2060271972417054 entropy 1.0271300077438354
epoch: 26, step: 46
	action: tensor([[ 1.1245, -0.4317, -0.1085, -0.2919, -0.9234, -1.0406,  1.3969]],
       dtype=torch.float64)
	q_value: tensor([[-35.9218]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04092192585150167, distance: 1.1675238782772628 entropy 1.0271300077438354
epoch: 26, step: 47
	action: tensor([[-1.1429, -0.1772, -0.4242,  0.4226, -0.9672,  0.9531, -0.2771]],
       dtype=torch.float64)
	q_value: tensor([[-40.5963]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0981997743406011, distance: 1.6576014244071384 entropy 1.0271300077438354
epoch: 26, step: 48
	action: tensor([[-0.2970, -1.0954,  0.6320,  0.0654, -0.1576,  0.5078, -0.5902]],
       dtype=torch.float64)
	q_value: tensor([[-33.0955]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6198108771772493, distance: 1.45642742670605 entropy 1.0271300077438354
epoch: 26, step: 49
	action: tensor([[-0.3985, -0.1572, -0.3474,  0.2014, -1.8418, -0.6358,  0.7518]],
       dtype=torch.float64)
	q_value: tensor([[-36.1432]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04792582101285714, distance: 1.116585764628057 entropy 1.0271300077438354
epoch: 26, step: 50
	action: tensor([[ 0.9236, -0.6037,  1.0238, -0.1448, -0.2363,  0.4348, -0.0435]],
       dtype=torch.float64)
	q_value: tensor([[-39.2456]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22465870578660208, distance: 1.0076347830063521 entropy 1.0271300077438354
epoch: 26, step: 51
	action: tensor([[-0.8697,  1.2275,  0.1387,  0.2446,  0.5119,  0.1458, -0.7555]],
       dtype=torch.float64)
	q_value: tensor([[-34.3244]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 26, step: 52
	action: tensor([[-0.9030, -1.1205, -0.0079, -0.3243, -0.1814,  0.7034,  0.6454]],
       dtype=torch.float64)
	q_value: tensor([[-38.3659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2457373603675879, distance: 1.7148896344408466 entropy 1.0271300077438354
epoch: 26, step: 53
	action: tensor([[-1.0119,  0.8075,  0.3058,  0.5060, -0.3450, -0.2079,  0.5897]],
       dtype=torch.float64)
	q_value: tensor([[-33.9620]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 26, step: 54
	action: tensor([[-0.9880, -0.5291,  0.7565, -0.7706, -0.4285, -0.2775,  0.2534]],
       dtype=torch.float64)
	q_value: tensor([[-38.3659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.4453049759859962, distance: 1.7894650024571686 entropy 1.0271300077438354
epoch: 26, step: 55
	action: tensor([[ 0.5144, -0.7458,  0.7066,  0.6396,  0.0485, -1.2717, -0.6757]],
       dtype=torch.float64)
	q_value: tensor([[-31.5361]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0781206601418325, distance: 1.098736961310165 entropy 1.0271300077438354
epoch: 26, step: 56
	action: tensor([[ 0.5061, -0.2971, -0.1437, -0.0556,  1.3276,  0.4877, -0.9906]],
       dtype=torch.float64)
	q_value: tensor([[-42.9774]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.490600835267063, distance: 0.8167437269321167 entropy 1.0271300077438354
epoch: 26, step: 57
	action: tensor([[-0.2224,  0.1166,  1.2715, -0.1427,  0.0190,  0.7821, -0.2738]],
       dtype=torch.float64)
	q_value: tensor([[-38.8623]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34492530526427556, distance: 0.9261943019535339 entropy 1.0271300077438354
epoch: 26, step: 58
	action: tensor([[ 0.3885, -0.4240, -0.2220, -0.8362, -1.2013, -0.0903, -0.2050]],
       dtype=torch.float64)
	q_value: tensor([[-32.3542]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18212086506375735, distance: 1.244192659430043 entropy 1.0271300077438354
epoch: 26, step: 59
	action: tensor([[ 0.2225, -0.5578,  0.2072,  0.3499, -1.2013,  0.2972, -0.1802]],
       dtype=torch.float64)
	q_value: tensor([[-30.9751]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21618591382250407, distance: 1.0131254504816263 entropy 1.0271300077438354
epoch: 26, step: 60
	action: tensor([[-0.7758, -0.8984,  0.1111,  0.3735,  1.1986, -0.5360, -0.4378]],
       dtype=torch.float64)
	q_value: tensor([[-32.9353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.180979464876673, distance: 1.6899835664184584 entropy 1.0271300077438354
epoch: 26, step: 61
	action: tensor([[ 0.1146, -0.2269, -0.1632, -1.5427, -0.0309,  1.4294,  0.1352]],
       dtype=torch.float64)
	q_value: tensor([[-38.9608]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04000776351018254, distance: 1.121219271282459 entropy 1.0271300077438354
epoch: 26, step: 62
	action: tensor([[-1.2718, -0.6062, -0.3184, -0.3165, -0.7345,  0.1164, -0.1339]],
       dtype=torch.float64)
	q_value: tensor([[-41.1937]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.5441536398496392, distance: 1.8252752313406277 entropy 1.0271300077438354
epoch: 26, step: 63
	action: tensor([[-1.1101, -0.4385,  0.8102, -0.1968,  0.6629,  1.1023, -0.6747]],
       dtype=torch.float64)
	q_value: tensor([[-32.7265]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9165002187756872, distance: 1.5842040887725768 entropy 1.0271300077438354
epoch: 26, step: 64
	action: tensor([[ 1.0386, -0.5629, -0.4429,  0.4982, -0.5396,  0.9985,  1.2053]],
       dtype=torch.float64)
	q_value: tensor([[-39.8227]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7209180793245292, distance: 0.6045365244266432 entropy 1.0271300077438354
epoch: 26, step: 65
	action: tensor([[-1.0918, -1.2812, -0.1147,  0.4734,  0.6460,  0.6114,  0.0943]],
       dtype=torch.float64)
	q_value: tensor([[-37.1499]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1553895888593275, distance: 1.6800398518504953 entropy 1.0271300077438354
epoch: 26, step: 66
	action: tensor([[-0.3058, -0.0679,  0.9655,  0.2266,  0.1566, -0.5521,  0.1248]],
       dtype=torch.float64)
	q_value: tensor([[-36.2974]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10575012589808686, distance: 1.203331240923586 entropy 1.0271300077438354
epoch: 26, step: 67
	action: tensor([[ 1.2273, -0.6655, -0.0825,  0.1198,  0.6392, -0.8682,  0.2364]],
       dtype=torch.float64)
	q_value: tensor([[-29.9991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.01762114312940044, distance: 1.1543825524819056 entropy 1.0271300077438354
epoch: 26, step: 68
	action: tensor([[-0.6709,  1.0987, -0.0979,  0.5551,  0.3367,  0.8006, -1.1317]],
       dtype=torch.float64)
	q_value: tensor([[-36.3034]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 26, step: 69
	action: tensor([[-0.1974, -1.4745,  0.5514,  0.7682, -0.8326,  1.2669,  0.4099]],
       dtype=torch.float64)
	q_value: tensor([[-38.3659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08800871876431038, distance: 1.0928285603549006 entropy 1.0271300077438354
epoch: 26, step: 70
	action: tensor([[ 0.2325, -0.5584,  0.1574, -0.6614,  0.0152,  0.1501, -0.3315]],
       dtype=torch.float64)
	q_value: tensor([[-42.0677]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.43216030329910815, distance: 1.369469956589779 entropy 1.0271300077438354
epoch: 26, step: 71
	action: tensor([[ 0.0233, -0.0534, -0.9879, -0.1800, -0.1130, -0.7959,  1.0389]],
       dtype=torch.float64)
	q_value: tensor([[-27.0007]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3776788249436107, distance: 0.9027426944293034 entropy 1.0271300077438354
epoch: 26, step: 72
	action: tensor([[ 0.6408, -0.6018, -0.2365,  0.3001,  0.6082, -0.0124,  0.2441]],
       dtype=torch.float64)
	q_value: tensor([[-30.5311]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2997244593930478, distance: 0.9576155111206344 entropy 1.0271300077438354
epoch: 26, step: 73
	action: tensor([[-0.6960, -0.4909, -0.5207, -0.5732, -0.0540,  0.1633,  0.4464]],
       dtype=torch.float64)
	q_value: tensor([[-29.5443]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7993467859757157, distance: 1.5350203222434178 entropy 1.0271300077438354
epoch: 26, step: 74
	action: tensor([[-0.2894, -0.1545,  0.3815,  0.8867, -1.1844,  1.0005,  0.2224]],
       dtype=torch.float64)
	q_value: tensor([[-28.2200]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 26, step: 75
	action: tensor([[ 0.6953, -1.0711,  0.5877, -0.6719, -1.2623,  0.5238,  1.0543]],
       dtype=torch.float64)
	q_value: tensor([[-38.3659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6698828067088738, distance: 1.478766794080897 entropy 1.0271300077438354
epoch: 26, step: 76
	action: tensor([[ 0.0326,  0.4614,  0.7892, -1.3514,  0.0864,  0.9771, -0.1683]],
       dtype=torch.float64)
	q_value: tensor([[-40.7090]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17479778210474617, distance: 1.0395296632432929 entropy 1.0271300077438354
epoch: 26, step: 77
	action: tensor([[ 0.2478, -1.2410, -0.1798,  0.5159,  1.3436, -0.5178, -0.0559]],
       dtype=torch.float64)
	q_value: tensor([[-34.7384]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.49098548935322883, distance: 1.3973120415685145 entropy 1.0271300077438354
epoch: 26, step: 78
	action: tensor([[ 0.4704, -0.5241,  0.7891, -1.3809, -0.4761,  0.3362, -1.0019]],
       dtype=torch.float64)
	q_value: tensor([[-41.7764]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.43996440742762255, distance: 1.3731961337969287 entropy 1.0271300077438354
epoch: 26, step: 79
	action: tensor([[ 0.9259, -0.7798,  0.3114,  0.1618,  0.3250, -0.2629,  0.0759]],
       dtype=torch.float64)
	q_value: tensor([[-37.6169]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.01059268692006654, distance: 1.1503891285084475 entropy 1.0271300077438354
epoch: 26, step: 80
	action: tensor([[ 0.3814, -1.6329, -0.4291,  0.1451, -0.5974,  1.1550, -0.8877]],
       dtype=torch.float64)
	q_value: tensor([[-32.4263]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 26, step: 81
	action: tensor([[-1.0610, -0.5777,  0.6935,  0.3501,  0.1579,  0.8939,  0.8610]],
       dtype=torch.float64)
	q_value: tensor([[-38.3659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5341047471246883, distance: 1.4173731458513503 entropy 1.0271300077438354
epoch: 26, step: 82
	action: tensor([[ 1.6084,  0.0522,  0.4464, -0.1478,  0.0273,  0.3613,  0.7646]],
       dtype=torch.float64)
	q_value: tensor([[-34.5021]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4402723533790579, distance: 0.8561405602510654 entropy 1.0271300077438354
epoch: 26, step: 83
	action: tensor([[ 0.1701, -0.0388, -2.3644,  0.2361, -0.4914,  0.1049,  1.3782]],
       dtype=torch.float64)
	q_value: tensor([[-35.0820]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24925381991393736, distance: 0.9915240639075548 entropy 1.0271300077438354
epoch: 26, step: 84
	action: tensor([[ 0.6458, -1.1713, -0.5939,  0.2269,  1.1408,  1.1346, -0.2291]],
       dtype=torch.float64)
	q_value: tensor([[-39.8942]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10193641593105696, distance: 1.0844517563280485 entropy 1.0271300077438354
epoch: 26, step: 85
	action: tensor([[ 0.1314,  0.4993,  1.0135, -0.1605, -1.1185,  1.2506,  0.7899]],
       dtype=torch.float64)
	q_value: tensor([[-43.1301]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 26, step: 86
	action: tensor([[ 0.0461, -0.8774,  0.1346, -1.1599,  0.1166,  0.8267,  1.2442]],
       dtype=torch.float64)
	q_value: tensor([[-38.3659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6910447088236134, distance: 1.488107269369096 entropy 1.0271300077438354
epoch: 26, step: 87
	action: tensor([[-0.9837, -0.0945,  1.0516,  0.2850, -0.4374,  0.5051,  0.8151]],
       dtype=torch.float64)
	q_value: tensor([[-39.7805]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4989162735269128, distance: 1.4010233730797415 entropy 1.0271300077438354
epoch: 26, step: 88
	action: tensor([[-0.5811, -0.9133,  0.0514, -0.5076,  1.4305,  1.0452, -0.9359]],
       dtype=torch.float64)
	q_value: tensor([[-33.4351]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5455559516335236, distance: 1.4226532462617323 entropy 1.0271300077438354
epoch: 26, step: 89
	action: tensor([[ 0.7575, -0.3419, -0.7916, -0.3247, -0.3414, -0.1623, -0.1857]],
       dtype=torch.float64)
	q_value: tensor([[-45.4530]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2740305915740401, distance: 0.975025229331932 entropy 1.0271300077438354
epoch: 26, step: 90
	action: tensor([[-0.0224, -0.2540,  0.7906,  0.2498, -0.1192,  0.1879,  1.3285]],
       dtype=torch.float64)
	q_value: tensor([[-26.3450]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3160231873606887, distance: 0.946405777358392 entropy 1.0271300077438354
epoch: 26, step: 91
	action: tensor([[ 0.2681, -0.1664,  0.9660,  1.2584, -0.6283, -0.6699, -0.5707]],
       dtype=torch.float64)
	q_value: tensor([[-32.3306]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8418927120302973, distance: 0.4550222544771456 entropy 1.0271300077438354
epoch: 26, step: 92
	action: tensor([[-0.4652,  1.4125, -0.9884, -0.4780,  0.7574,  0.5560,  0.3735]],
       dtype=torch.float64)
	q_value: tensor([[-43.1389]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 26, step: 93
	action: tensor([[ 0.4251, -1.6678, -0.6142, -0.1193, -0.2988,  0.1984, -0.4171]],
       dtype=torch.float64)
	q_value: tensor([[-38.3659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6811552310402045, distance: 1.4837495539687424 entropy 1.0271300077438354
epoch: 26, step: 94
	action: tensor([[ 0.2837, -0.7326, -1.2989, -0.1709,  0.4407, -0.8302, -0.0951]],
       dtype=torch.float64)
	q_value: tensor([[-34.5472]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08594980027921095, distance: 1.1925087330070647 entropy 1.0271300077438354
epoch: 26, step: 95
	action: tensor([[ 0.2577,  0.9029, -0.3808, -1.3773, -0.8924,  0.3968, -1.2122]],
       dtype=torch.float64)
	q_value: tensor([[-35.7631]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7358193438677414, distance: 0.5881758658927387 entropy 1.0271300077438354
epoch: 26, step: 96
	action: tensor([[ 0.4376, -1.1186,  1.0938, -0.1036,  1.0045,  1.0211,  0.0304]],
       dtype=torch.float64)
	q_value: tensor([[-35.3069]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4517799816657635, distance: 1.3788184787039681 entropy 1.0271300077438354
epoch: 26, step: 97
	action: tensor([[-0.3316, -1.7991,  0.0945, -0.3755,  1.0088,  0.6930, -0.0606]],
       dtype=torch.float64)
	q_value: tensor([[-43.1574]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 26, step: 98
	action: tensor([[ 0.2330, -1.5795, -0.0789, -0.4409, -0.9751,  0.0669,  0.1917]],
       dtype=torch.float64)
	q_value: tensor([[-38.3659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9254687184701582, distance: 1.5879065017908993 entropy 1.0271300077438354
epoch: 26, step: 99
	action: tensor([[-0.5738, -1.5049, -0.4446, -0.1397, -0.1815,  0.6762,  0.7593]],
       dtype=torch.float64)
	q_value: tensor([[-35.0755]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 26, step: 100
	action: tensor([[ 0.0539, -0.4796, -0.1767,  0.5373, -1.2135,  0.9883, -0.3300]],
       dtype=torch.float64)
	q_value: tensor([[-38.3659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11293080592074389, distance: 1.0777932068613916 entropy 1.0271300077438354
epoch: 26, step: 101
	action: tensor([[-0.8740, -1.1279, -0.6330,  0.0833,  0.4026, -0.1092,  1.1634]],
       dtype=torch.float64)
	q_value: tensor([[-34.4885]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.3139054327135202, distance: 1.7407223125812141 entropy 1.0271300077438354
epoch: 26, step: 102
	action: tensor([[ 0.7579, -0.5077, -0.2256, -0.6895, -0.0733,  0.1691,  0.8638]],
       dtype=torch.float64)
	q_value: tensor([[-36.4213]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14327965596339398, distance: 1.22358158172741 entropy 1.0271300077438354
epoch: 26, step: 103
	action: tensor([[-0.1347, -0.1165,  1.0162,  0.1488,  0.2117, -0.0580,  0.1957]],
       dtype=torch.float64)
	q_value: tensor([[-30.7276]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14674507832019446, distance: 1.0570513720855765 entropy 1.0271300077438354
epoch: 26, step: 104
	action: tensor([[ 0.1997,  0.5592, -0.2437,  1.2095,  1.0102, -0.1547,  0.0797]],
       dtype=torch.float64)
	q_value: tensor([[-28.3550]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 26, step: 105
	action: tensor([[-0.3260, -0.4447, -0.1292,  0.1173, -1.0526,  0.5946,  0.5345]],
       dtype=torch.float64)
	q_value: tensor([[-38.3659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4199868337610597, distance: 1.3636372370459542 entropy 1.0271300077438354
epoch: 26, step: 106
	action: tensor([[-0.4727, -1.3690,  0.0353,  1.0213, -0.2655,  1.7416, -0.0294]],
       dtype=torch.float64)
	q_value: tensor([[-30.0823]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0781215615174875, distance: 1.0987364241601565 entropy 1.0271300077438354
epoch: 26, step: 107
	action: tensor([[ 0.4860, -0.1337, -0.1787, -1.0943,  0.1822, -0.9819,  0.5612]],
       dtype=torch.float64)
	q_value: tensor([[-43.3933]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0673476447803738, distance: 1.1822508674000705 entropy 1.0271300077438354
epoch: 26, step: 108
	action: tensor([[ 0.7073, -0.4895, -0.9425, -1.0856, -0.2409,  0.5709,  0.9659]],
       dtype=torch.float64)
	q_value: tensor([[-30.7790]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0010773491800157853, distance: 1.1449605172338602 entropy 1.0271300077438354
epoch: 26, step: 109
	action: tensor([[-0.4764, -0.9403,  0.8286, -0.2275,  0.3248,  0.4653,  0.8833]],
       dtype=torch.float64)
	q_value: tensor([[-36.6625]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8981463408604022, distance: 1.5766000619851213 entropy 1.0271300077438354
epoch: 26, step: 110
	action: tensor([[ 0.6851,  0.1174, -0.8000, -0.2318,  0.4493, -1.0662,  0.3092]],
       dtype=torch.float64)
	q_value: tensor([[-33.7892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5753526103446864, distance: 0.7457116408694421 entropy 1.0271300077438354
epoch: 26, step: 111
	action: tensor([[-0.2636, -0.8231,  0.2979, -0.0927, -0.1760,  0.2041,  0.7545]],
       dtype=torch.float64)
	q_value: tensor([[-30.4755]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6628485404914126, distance: 1.4756489055491229 entropy 1.0271300077438354
epoch: 26, step: 112
	action: tensor([[ 0.1272, -0.2209,  0.1101, -0.9218,  1.1236,  1.4399,  0.8713]],
       dtype=torch.float64)
	q_value: tensor([[-28.9199]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3359863954332749, distance: 0.9324921446787848 entropy 1.0271300077438354
epoch: 26, step: 113
	action: tensor([[ 0.1728, -0.5672,  0.4704,  0.4771,  0.4106,  0.2736,  0.0645]],
       dtype=torch.float64)
	q_value: tensor([[-42.1797]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.437200214062487, distance: 0.8584868655791524 entropy 1.0271300077438354
epoch: 26, step: 114
	action: tensor([[-0.0755, -0.4611,  0.2255,  0.7380,  0.3177,  1.0047,  0.1502]],
       dtype=torch.float64)
	q_value: tensor([[-29.1493]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6930199142268756, distance: 0.6340329011549769 entropy 1.0271300077438354
epoch: 26, step: 115
	action: tensor([[ 0.4945, -1.2775,  0.0209,  0.3972, -0.1922,  0.3457,  0.3412]],
       dtype=torch.float64)
	q_value: tensor([[-31.5935]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18180869155515134, distance: 1.2440283658941222 entropy 1.0271300077438354
epoch: 26, step: 116
	action: tensor([[ 0.0468, -1.0337, -0.7674,  0.4026,  0.7386, -0.0817, -0.8009]],
       dtype=torch.float64)
	q_value: tensor([[-34.0264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.553985258396271, distance: 1.4265274749136454 entropy 1.0271300077438354
epoch: 26, step: 117
	action: tensor([[-0.3110,  0.0457,  0.1452,  1.0547, -1.6236,  0.0655, -0.6905]],
       dtype=torch.float64)
	q_value: tensor([[-37.1637]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 26, step: 118
	action: tensor([[ 0.5293, -1.2660,  0.1994, -0.6043,  0.4740,  1.9090,  2.1635]],
       dtype=torch.float64)
	q_value: tensor([[-38.3659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04981492158047196, distance: 1.1725005738403558 entropy 1.0271300077438354
epoch: 26, step: 119
	action: tensor([[-0.4251, -1.5599,  0.6619, -0.5399,  0.1905,  0.5250,  0.4360]],
       dtype=torch.float64)
	q_value: tensor([[-54.0874]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 26, step: 120
	action: tensor([[ 0.8669, -2.4335,  0.8272,  0.0930, -0.3510,  0.8162,  0.7557]],
       dtype=torch.float64)
	q_value: tensor([[-38.3659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 26, step: 121
	action: tensor([[-0.7979, -2.1343,  1.0358, -0.2808,  0.3899,  1.0824,  0.4765]],
       dtype=torch.float64)
	q_value: tensor([[-38.3659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 26, step: 122
	action: tensor([[ 0.0479, -1.2419,  1.2692,  0.1639,  0.4175,  1.9335,  0.1212]],
       dtype=torch.float64)
	q_value: tensor([[-38.3659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1491411033140171, distance: 1.0555661758227624 entropy 1.0271300077438354
epoch: 26, step: 123
	action: tensor([[-0.2050, -0.0691,  0.1978, -0.3894,  0.3182,  0.7982,  0.8850]],
       dtype=torch.float64)
	q_value: tensor([[-49.8009]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13331701864008172, distance: 1.0653365508544306 entropy 1.0271300077438354
epoch: 26, step: 124
	action: tensor([[-0.2250, -0.5867, -0.4910, -0.2138,  0.9596, -1.1119, -0.4545]],
       dtype=torch.float64)
	q_value: tensor([[-30.8455]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5859728398909207, distance: 1.4411346692966438 entropy 1.0271300077438354
epoch: 26, step: 125
	action: tensor([[ 0.7651,  0.0295, -0.5671, -0.6564,  0.7798, -0.4399, -0.0451]],
       dtype=torch.float64)
	q_value: tensor([[-37.4244]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3747415883076506, distance: 0.9048705728131883 entropy 1.0271300077438354
epoch: 26, step: 126
	action: tensor([[-1.3955, -1.4928,  0.7788,  0.0095, -1.0504,  0.7150,  0.0804]],
       dtype=torch.float64)
	q_value: tensor([[-30.4415]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.5225902034487788, distance: 1.8175235452614957 entropy 1.0271300077438354
epoch: 26, step: 127
	action: tensor([[ 0.3839, -0.2803,  1.0445, -0.9609, -0.3058,  0.5014,  1.3003]],
       dtype=torch.float64)
	q_value: tensor([[-43.3934]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.022739344987695698, distance: 1.13125861713818 entropy 1.0271300077438354
LOSS epoch 26 actor 458.5491295902836 critic 129.21385244747893 
epoch: 27, step: 0
	action: tensor([[-0.3767,  0.2281, -0.0529, -0.1233,  0.2287,  0.3686,  0.4183]],
       dtype=torch.float64)
	q_value: tensor([[-37.0346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10200988882444151, distance: 1.084407394525936 entropy 0.9217694997787476
epoch: 27, step: 1
	action: tensor([[ 0.7121, -0.4429,  0.0422, -0.0262, -0.5113, -1.2591,  0.7098]],
       dtype=torch.float64)
	q_value: tensor([[-23.6027]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10069226906342377, distance: 1.085202677706609 entropy 0.9217694997787476
epoch: 27, step: 2
	action: tensor([[-0.2472, -0.0645,  0.6740,  0.5485,  0.6332, -0.3434,  0.5428]],
       dtype=torch.float64)
	q_value: tensor([[-35.2539]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2731398335714591, distance: 0.9756232195950265 entropy 0.9217694997787476
epoch: 27, step: 3
	action: tensor([[ 0.8230, -0.6009,  0.4553,  0.3339, -0.4475,  0.0862, -0.4011]],
       dtype=torch.float64)
	q_value: tensor([[-29.9247]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39130928549897515, distance: 0.8928017474386554 entropy 0.9217694997787476
epoch: 27, step: 4
	action: tensor([[-0.0484,  0.8209,  0.3255,  1.6580, -0.6816,  1.1223,  0.5673]],
       dtype=torch.float64)
	q_value: tensor([[-32.3668]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 27, step: 5
	action: tensor([[-0.3690, -1.9299,  0.8856,  0.5532, -0.5649,  2.2874,  0.3829]],
       dtype=torch.float64)
	q_value: tensor([[-37.8362]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 27, step: 6
	action: tensor([[ 0.2446, -2.6743,  1.5240, -0.4159,  0.0383,  2.4187,  0.2029]],
       dtype=torch.float64)
	q_value: tensor([[-37.8362]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 27, step: 7
	action: tensor([[ 1.1127, -0.7986,  1.5901,  0.6831, -0.7597,  1.0489, -0.1309]],
       dtype=torch.float64)
	q_value: tensor([[-37.8362]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 27, step: 8
	action: tensor([[-0.5595, -0.9454,  0.5441, -0.1039, -0.5510,  1.3406, -0.1214]],
       dtype=torch.float64)
	q_value: tensor([[-37.8362]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5649650389931498, distance: 1.4315582141089325 entropy 0.9217694997787476
epoch: 27, step: 9
	action: tensor([[ 0.4831, -0.6849,  1.0784,  0.3751,  0.7927,  0.9159,  0.7212]],
       dtype=torch.float64)
	q_value: tensor([[-38.0478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3032239210691222, distance: 0.9552197856441161 entropy 0.9217694997787476
epoch: 27, step: 10
	action: tensor([[ 0.0046, -0.4911,  0.0548,  0.6505, -0.7128,  0.1546,  1.0421]],
       dtype=torch.float64)
	q_value: tensor([[-40.2249]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28154693650301366, distance: 0.9699646207327762 entropy 0.9217694997787476
epoch: 27, step: 11
	action: tensor([[ 0.1876, -0.6939,  0.1072, -1.0871, -0.6006, -0.1449, -0.6559]],
       dtype=torch.float64)
	q_value: tensor([[-30.4037]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6988089267134927, distance: 1.491519585203062 entropy 0.9217694997787476
epoch: 27, step: 12
	action: tensor([[-0.2562,  1.0004, -0.2286,  0.0318, -0.1355,  0.2434,  0.6418]],
       dtype=torch.float64)
	q_value: tensor([[-32.8383]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 27, step: 13
	action: tensor([[ 0.0892, -1.1781, -0.7252,  0.1035,  0.0459,  1.4063,  0.3174]],
       dtype=torch.float64)
	q_value: tensor([[-37.8362]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.27841827040620615, distance: 1.2938775503211226 entropy 0.9217694997787476
epoch: 27, step: 14
	action: tensor([[-0.0831, -0.1048,  1.2079, -0.6547,  0.3295, -0.4863,  0.1921]],
       dtype=torch.float64)
	q_value: tensor([[-36.7271]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.39065793154785466, distance: 1.349481240926808 entropy 0.9217694997787476
epoch: 27, step: 15
	action: tensor([[-0.2569, -0.8764,  0.4345,  0.2146, -0.1285,  0.9280,  1.1592]],
       dtype=torch.float64)
	q_value: tensor([[-30.3764]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12306246038612301, distance: 1.2127147186942846 entropy 0.9217694997787476
epoch: 27, step: 16
	action: tensor([[ 0.1112, -0.1278,  0.9329,  0.2709, -0.3281,  0.2546, -0.2202]],
       dtype=torch.float64)
	q_value: tensor([[-34.3050]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5700115465963074, distance: 0.750386634524273 entropy 0.9217694997787476
epoch: 27, step: 17
	action: tensor([[-0.1060, -0.2957,  0.0872,  0.0795,  0.8266,  1.4006,  0.9091]],
       dtype=torch.float64)
	q_value: tensor([[-29.2334]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.580846871164246, distance: 0.7408717744503882 entropy 0.9217694997787476
epoch: 27, step: 18
	action: tensor([[ 0.2170, -1.1978,  0.9601, -0.6936, -1.3533, -0.3850,  0.4973]],
       dtype=torch.float64)
	q_value: tensor([[-36.8030]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.082076465825505, distance: 1.6512203445991702 entropy 0.9217694997787476
epoch: 27, step: 19
	action: tensor([[ 0.2305, -0.2377,  0.1855, -0.0528, -0.3812,  0.2708,  0.3557]],
       dtype=torch.float64)
	q_value: tensor([[-40.3863]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3257238095731043, distance: 0.9396705273819456 entropy 0.9217694997787476
epoch: 27, step: 20
	action: tensor([[-0.1166,  0.5598,  0.4697,  0.5153, -0.2093,  0.3305,  0.0564]],
       dtype=torch.float64)
	q_value: tensor([[-23.9232]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 27, step: 21
	action: tensor([[-0.1853, -1.1463,  1.0464, -0.1638, -1.0403,  0.7568, -0.7682]],
       dtype=torch.float64)
	q_value: tensor([[-37.8362]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7185557391272837, distance: 1.500163188894708 entropy 0.9217694997787476
epoch: 27, step: 22
	action: tensor([[ 1.6651, -0.2579, -0.3922, -0.7045, -1.3419,  0.4308,  0.5139]],
       dtype=torch.float64)
	q_value: tensor([[-43.4184]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20336076032893202, distance: 1.255320485588034 entropy 0.9217694997787476
epoch: 27, step: 23
	action: tensor([[-0.6710, -0.8165, -0.3314,  0.5695, -0.2287,  0.0196, -0.5127]],
       dtype=torch.float64)
	q_value: tensor([[-38.2612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8950322882019601, distance: 1.57530626529226 entropy 0.9217694997787476
epoch: 27, step: 24
	action: tensor([[-0.3898,  0.2747, -0.4340, -0.8487,  0.9424,  0.4407, -0.3868]],
       dtype=torch.float64)
	q_value: tensor([[-31.5259]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11924573617541812, distance: 1.0739500200063659 entropy 0.9217694997787476
epoch: 27, step: 25
	action: tensor([[-1.6453, -0.8116,  0.4846,  0.1171,  1.2263,  0.9126, -0.8166]],
       dtype=torch.float64)
	q_value: tensor([[-32.2578]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.3231825578884773, distance: 1.7442083549471343 entropy 0.9217694997787476
epoch: 27, step: 26
	action: tensor([[-1.2870, -0.3228,  0.4668,  0.9377, -0.1125,  0.6325,  1.8023]],
       dtype=torch.float64)
	q_value: tensor([[-44.4527]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3986844554963942, distance: 1.3533700689492845 entropy 0.9217694997787476
epoch: 27, step: 27
	action: tensor([[ 0.4798, -0.8876,  1.1309,  0.5700,  0.9834, -0.1060,  0.2447]],
       dtype=torch.float64)
	q_value: tensor([[-39.9505]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12674337802289593, distance: 1.0693691155260598 entropy 0.9217694997787476
epoch: 27, step: 28
	action: tensor([[-0.8840, -0.8949,  0.7891, -1.2539,  0.7746,  0.8520,  0.0350]],
       dtype=torch.float64)
	q_value: tensor([[-40.8865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1608867125050017, distance: 1.6821808814439707 entropy 0.9217694997787476
epoch: 27, step: 29
	action: tensor([[-0.4069,  0.1010,  0.4865,  0.1378, -0.4446,  0.5150,  0.6637]],
       dtype=torch.float64)
	q_value: tensor([[-40.9848]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12375022056711982, distance: 1.0712002222188233 entropy 0.9217694997787476
epoch: 27, step: 30
	action: tensor([[ 0.7263, -0.3998,  0.0760,  0.5720, -0.3230,  0.4532,  0.2469]],
       dtype=torch.float64)
	q_value: tensor([[-27.1439]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 27, step: 31
	action: tensor([[ 0.5352, -1.4734, -0.6226, -0.3930, -0.9173,  0.8148,  0.5907]],
       dtype=torch.float64)
	q_value: tensor([[-37.8362]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.773890264548114, distance: 1.5241231806343598 entropy 0.9217694997787476
epoch: 27, step: 32
	action: tensor([[ 0.1582, -0.6157,  0.5869, -0.0009, -0.5609,  0.4468, -0.6976]],
       dtype=torch.float64)
	q_value: tensor([[-37.4407]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.026699734483674464, distance: 1.1289640537498788 entropy 0.9217694997787476
epoch: 27, step: 33
	action: tensor([[ 0.5423, -0.3697, -1.2659,  0.4207,  0.2607,  0.4519, -0.4111]],
       dtype=torch.float64)
	q_value: tensor([[-31.9558]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.40987081289904814, distance: 0.8790837268774518 entropy 0.9217694997787476
epoch: 27, step: 34
	action: tensor([[-0.5141, -0.0712,  0.2052, -0.6072,  0.1065,  1.0239, -0.7011]],
       dtype=torch.float64)
	q_value: tensor([[-29.3807]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21774243093386092, distance: 1.2627995332080053 entropy 0.9217694997787476
epoch: 27, step: 35
	action: tensor([[-0.6388,  0.0172,  0.7241,  0.7319,  0.1974, -0.1114, -0.5322]],
       dtype=torch.float64)
	q_value: tensor([[-31.0403]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1945201147159401, distance: 1.0270321598546537 entropy 0.9217694997787476
epoch: 27, step: 36
	action: tensor([[ 0.5882, -0.2426, -0.7317, -1.9692, -0.3334,  0.9002,  0.0420]],
       dtype=torch.float64)
	q_value: tensor([[-33.0436]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0051410829529090485, distance: 1.1472820673233506 entropy 0.9217694997787476
epoch: 27, step: 37
	action: tensor([[ 0.5724, -0.9085, -0.1020,  0.1035, -0.8876,  0.6814,  0.2873]],
       dtype=torch.float64)
	q_value: tensor([[-41.0376]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.02068933430834141, distance: 1.1561215103783289 entropy 0.9217694997787476
epoch: 27, step: 38
	action: tensor([[-0.1778, -1.4934,  0.0889, -0.9792, -0.5607,  0.7366,  0.2015]],
       dtype=torch.float64)
	q_value: tensor([[-32.4809]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 27, step: 39
	action: tensor([[ 0.7325, -1.0839, -0.0148, -0.9531, -0.4217,  2.0692, -0.3534]],
       dtype=torch.float64)
	q_value: tensor([[-37.8362]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.053315813678138646, distance: 1.1744539569859196 entropy 0.9217694997787476
epoch: 27, step: 40
	action: tensor([[ 0.2528,  1.0850,  1.1794,  0.1560, -0.2933, -0.4038, -0.9162]],
       dtype=torch.float64)
	q_value: tensor([[-48.2646]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 27, step: 41
	action: tensor([[-0.5433, -1.3760,  0.2026,  0.3787, -0.7306,  1.1366,  0.0023]],
       dtype=torch.float64)
	q_value: tensor([[-37.8362]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6966225293338999, distance: 1.490559470031785 entropy 0.9217694997787476
epoch: 27, step: 42
	action: tensor([[ 0.9695,  0.7962,  0.8942, -0.4497, -0.9438,  0.2566,  0.3931]],
       dtype=torch.float64)
	q_value: tensor([[-38.9086]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9702795841441894, distance: 0.1972804883076088 entropy 0.9217694997787476
epoch: 27, step: 43
	action: tensor([[-0.2466,  0.3031,  0.2257,  0.3265, -0.0553,  0.7056, -0.2846]],
       dtype=torch.float64)
	q_value: tensor([[-33.7830]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 27, step: 44
	action: tensor([[-0.2727, -0.3881,  0.5549, -1.6174,  0.4132,  0.3278,  0.6470]],
       dtype=torch.float64)
	q_value: tensor([[-37.8362]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8122680131384761, distance: 1.5405220036821856 entropy 0.9217694997787476
epoch: 27, step: 45
	action: tensor([[ 0.3002, -0.4307,  0.4886, -0.1898, -0.0034, -0.2193, -0.8555]],
       dtype=torch.float64)
	q_value: tensor([[-36.4922]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.061903944854661885, distance: 1.1792321480563952 entropy 0.9217694997787476
epoch: 27, step: 46
	action: tensor([[-0.8304, -0.9797, -0.1185, -0.2550, -0.3677,  1.1311,  0.5693]],
       dtype=torch.float64)
	q_value: tensor([[-30.6578]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0921835055685092, distance: 1.6552232584225026 entropy 0.9217694997787476
epoch: 27, step: 47
	action: tensor([[ 0.5372, -0.3818,  0.0420,  0.8421, -0.1793, -0.3364,  0.2583]],
       dtype=torch.float64)
	q_value: tensor([[-34.9394]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7540651349885764, distance: 0.567501147146986 entropy 0.9217694997787476
epoch: 27, step: 48
	action: tensor([[ 0.6018, -0.0410,  0.4019,  0.1038,  0.8988,  0.0530, -0.6045]],
       dtype=torch.float64)
	q_value: tensor([[-30.5653]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7449262042607672, distance: 0.5779491318868738 entropy 0.9217694997787476
epoch: 27, step: 49
	action: tensor([[1.0020, 1.0324, 0.3113, 0.1318, 0.3315, 0.3927, 0.0398]],
       dtype=torch.float64)
	q_value: tensor([[-32.3640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 27, step: 50
	action: tensor([[-0.7575, -0.4664,  1.8846, -0.5587, -0.6516,  2.1574,  0.1962]],
       dtype=torch.float64)
	q_value: tensor([[-37.8362]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09788080668713905, distance: 1.1990417068680963 entropy 0.9217694997787476
epoch: 27, step: 51
	action: tensor([[-0.0091, -0.5786,  0.4885, -0.4482, -0.0988,  0.9655,  1.9925]],
       dtype=torch.float64)
	q_value: tensor([[-51.9444]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08381917851554865, distance: 1.191338314019228 entropy 0.9217694997787476
epoch: 27, step: 52
	action: tensor([[ 1.2482, -1.1005,  0.3390, -0.6788, -1.1770,  0.5614, -0.1559]],
       dtype=torch.float64)
	q_value: tensor([[-42.1557]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5970960343661798, distance: 1.446179526418116 entropy 0.9217694997787476
epoch: 27, step: 53
	action: tensor([[-0.1035, -0.6530, -0.0157,  0.8193,  0.1322,  0.5986,  0.4741]],
       dtype=torch.float64)
	q_value: tensor([[-40.4329]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3723210009730151, distance: 0.9066204113074878 entropy 0.9217694997787476
epoch: 27, step: 54
	action: tensor([[-0.1884, -0.4405, -0.4282,  0.1960, -0.3004,  0.5379, -0.5030]],
       dtype=torch.float64)
	q_value: tensor([[-29.3969]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14568453141322624, distance: 1.224867800504199 entropy 0.9217694997787476
epoch: 27, step: 55
	action: tensor([[0.4936, 0.5225, 0.8451, 1.3272, 0.1528, 0.7993, 0.2279]],
       dtype=torch.float64)
	q_value: tensor([[-25.6970]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 27, step: 56
	action: tensor([[-0.0339, -0.0778,  0.6555, -0.4303, -0.2488,  0.0670,  0.2490]],
       dtype=torch.float64)
	q_value: tensor([[-37.8362]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09537737272199975, distance: 1.1976738740245692 entropy 0.9217694997787476
epoch: 27, step: 57
	action: tensor([[-0.2412, -0.8836,  0.2275, -0.1257, -0.1297,  1.2434,  0.8024]],
       dtype=torch.float64)
	q_value: tensor([[-24.7935]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24096929595019811, distance: 1.2747857831614113 entropy 0.9217694997787476
epoch: 27, step: 58
	action: tensor([[ 0.3709, -0.5395, -1.2272, -0.6939,  0.3084, -0.4473, -0.5545]],
       dtype=torch.float64)
	q_value: tensor([[-34.7503]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13265227317621142, distance: 1.0657450288757897 entropy 0.9217694997787476
epoch: 27, step: 59
	action: tensor([[ 0.4967,  0.0091,  1.2493,  0.5216, -0.5390,  1.4907,  0.8391]],
       dtype=torch.float64)
	q_value: tensor([[-34.1850]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9478560439922303, distance: 0.2613115954960945 entropy 0.9217694997787476
epoch: 27, step: 60
	action: tensor([[ 0.1636, -0.2899,  0.5175,  1.3511,  0.1975,  0.9084,  0.1198]],
       dtype=torch.float64)
	q_value: tensor([[-41.8726]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9319928740609285, distance: 0.2984241077052609 entropy 0.9217694997787476
epoch: 27, step: 61
	action: tensor([[ 0.4181, -0.5097,  0.7261,  0.1529,  0.1146,  0.4924,  0.6526]],
       dtype=torch.float64)
	q_value: tensor([[-35.7754]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4626567636155636, distance: 0.8388466734675959 entropy 0.9217694997787476
epoch: 27, step: 62
	action: tensor([[-0.4243, -0.1443, -0.4159, -0.3483, -0.8178,  0.9600,  0.3055]],
       dtype=torch.float64)
	q_value: tensor([[-30.6174]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4341024975720582, distance: 1.3703982310159433 entropy 0.9217694997787476
epoch: 27, step: 63
	action: tensor([[0.0702, 0.1664, 0.5725, 0.7151, 0.7391, 0.6112, 0.9276]],
       dtype=torch.float64)
	q_value: tensor([[-29.1135]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 27, step: 64
	action: tensor([[ 0.1241, -2.2081,  0.5856, -0.6334, -0.8175,  1.4814, -0.3452]],
       dtype=torch.float64)
	q_value: tensor([[-37.8362]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 27, step: 65
	action: tensor([[ 0.3894, -1.3479,  0.4981, -0.1439,  0.2902,  0.6777,  0.0766]],
       dtype=torch.float64)
	q_value: tensor([[-37.8362]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4812282758017723, distance: 1.392732436048485 entropy 0.9217694997787476
epoch: 27, step: 66
	action: tensor([[ 0.1172, -0.2270, -0.2669,  0.2939,  0.7681, -0.1116,  0.8733]],
       dtype=torch.float64)
	q_value: tensor([[-35.5983]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3110634628420531, distance: 0.949830917887702 entropy 0.9217694997787476
epoch: 27, step: 67
	action: tensor([[ 0.0656,  0.9901,  0.4228,  0.3544, -0.8795,  0.9680,  0.2033]],
       dtype=torch.float64)
	q_value: tensor([[-28.8450]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 27, step: 68
	action: tensor([[-1.2177, -1.4192,  0.2298,  0.7354,  1.0089,  1.6414,  0.2290]],
       dtype=torch.float64)
	q_value: tensor([[-37.8362]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.27198600101385817, distance: 1.2906184198088801 entropy 0.9217694997787476
epoch: 27, step: 69
	action: tensor([[ 0.2998,  0.5741,  0.3450,  0.7126,  0.3265,  0.7038, -0.8923]],
       dtype=torch.float64)
	q_value: tensor([[-45.2411]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 27, step: 70
	action: tensor([[ 0.1738, -1.5363,  0.6398, -1.4348, -0.1307,  0.8094, -0.5006]],
       dtype=torch.float64)
	q_value: tensor([[-37.8362]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0051266424546323, distance: 1.6204200083585705 entropy 0.9217694997787476
epoch: 27, step: 71
	action: tensor([[-0.1732, -1.0905,  0.0180, -1.2365, -0.1288,  1.7111, -0.1162]],
       dtype=torch.float64)
	q_value: tensor([[-41.8863]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5629451432017438, distance: 1.4306340618158189 entropy 0.9217694997787476
epoch: 27, step: 72
	action: tensor([[-0.1184, -0.6857,  0.5153, -0.7645,  0.3405, -0.1259, -0.9295]],
       dtype=torch.float64)
	q_value: tensor([[-44.5356]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8928526126235636, distance: 1.574400042074639 entropy 0.9217694997787476
epoch: 27, step: 73
	action: tensor([[-0.1193, -0.9107,  0.1211, -0.9517,  0.5607,  0.8831, -1.1125]],
       dtype=torch.float64)
	q_value: tensor([[-34.0177]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6718713365534341, distance: 1.4796470046529364 entropy 0.9217694997787476
epoch: 27, step: 74
	action: tensor([[ 0.2506,  0.8880,  0.2479,  0.0017, -0.0539,  0.7895, -0.4838]],
       dtype=torch.float64)
	q_value: tensor([[-40.7241]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 27, step: 75
	action: tensor([[ 1.0903, -1.6145,  1.1414, -0.7870, -0.7795,  0.7967, -0.3462]],
       dtype=torch.float64)
	q_value: tensor([[-37.8362]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.34249838045594383, distance: 1.325908572498832 entropy 0.9217694997787476
epoch: 27, step: 76
	action: tensor([[ 0.1485, -0.6520, -0.0746, -0.8671,  0.9393,  0.7736,  0.7235]],
       dtype=torch.float64)
	q_value: tensor([[-43.7128]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2308577422044651, distance: 1.2695816133482596 entropy 0.9217694997787476
epoch: 27, step: 77
	action: tensor([[-0.9502, -0.8561,  0.6256,  0.0620,  0.3608,  0.7445,  0.3729]],
       dtype=torch.float64)
	q_value: tensor([[-36.5251]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9013604367120374, distance: 1.5779343111719217 entropy 0.9217694997787476
epoch: 27, step: 78
	action: tensor([[ 0.0886, -0.9277,  0.2371, -0.1954,  0.1053,  0.0757,  0.2334]],
       dtype=torch.float64)
	q_value: tensor([[-32.9023]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6193505106858117, distance: 1.456220446360163 entropy 0.9217694997787476
epoch: 27, step: 79
	action: tensor([[-0.3323, -0.1218,  0.5625,  0.2966, -0.1326,  0.3943, -1.3701]],
       dtype=torch.float64)
	q_value: tensor([[-28.5524]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18684543624364225, distance: 1.0319133836107843 entropy 0.9217694997787476
epoch: 27, step: 80
	action: tensor([[ 1.1918, -0.2775,  0.5798, -0.1732, -1.2874,  0.6593, -0.1322]],
       dtype=torch.float64)
	q_value: tensor([[-36.3701]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5732915415439201, distance: 0.747519143512614 entropy 0.9217694997787476
epoch: 27, step: 81
	action: tensor([[-0.2199,  0.5323, -0.2396, -0.8207, -0.2020,  0.2921,  0.3440]],
       dtype=torch.float64)
	q_value: tensor([[-36.6690]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28778779164596036, distance: 0.9657426247754034 entropy 0.9217694997787476
epoch: 27, step: 82
	action: tensor([[ 0.9934, -1.0041,  0.4472, -0.1726, -0.1032,  0.4844, -0.0154]],
       dtype=torch.float64)
	q_value: tensor([[-26.1756]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2799234556865593, distance: 1.2946390196229869 entropy 0.9217694997787476
epoch: 27, step: 83
	action: tensor([[ 0.8368,  0.4708,  0.3091, -0.3309, -0.9704, -0.3993,  0.8175]],
       dtype=torch.float64)
	q_value: tensor([[-34.7576]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8703895640280169, distance: 0.41198051769579264 entropy 0.9217694997787476
epoch: 27, step: 84
	action: tensor([[ 0.4542, -0.0103,  1.4029,  0.9592,  0.2080, -0.2005, -0.1942]],
       dtype=torch.float64)
	q_value: tensor([[-31.2444]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8468509709302477, distance: 0.44783065345498657 entropy 0.9217694997787476
epoch: 27, step: 85
	action: tensor([[ 0.4331, -0.2994,  1.1544,  1.2373,  0.1787,  1.4062,  0.2101]],
       dtype=torch.float64)
	q_value: tensor([[-39.8748]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7331749718279466, distance: 0.5911122712868551 entropy 0.9217694997787476
epoch: 27, step: 86
	action: tensor([[ 1.0175, -1.1057, -0.1084, -0.4963,  0.4761,  0.0263,  0.9946]],
       dtype=torch.float64)
	q_value: tensor([[-43.3583]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.807780765959061, distance: 1.5386136249813351 entropy 0.9217694997787476
epoch: 27, step: 87
	action: tensor([[ 0.4952, -0.1020, -0.6256, -0.5603,  0.0865, -0.4587, -0.9908]],
       dtype=torch.float64)
	q_value: tensor([[-38.5136]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30642853799268177, distance: 0.9530206277124945 entropy 0.9217694997787476
epoch: 27, step: 88
	action: tensor([[ 0.2807, -0.3660,  0.7533,  0.0439, -0.5531,  0.4236, -0.3317]],
       dtype=torch.float64)
	q_value: tensor([[-30.1220]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3818847263996703, distance: 0.899686970233257 entropy 0.9217694997787476
epoch: 27, step: 89
	action: tensor([[ 0.0418, -0.8366, -1.4329,  0.3292, -0.5915,  1.4221,  1.1701]],
       dtype=torch.float64)
	q_value: tensor([[-30.0127]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5442871425957341, distance: 1.422069169732035 entropy 0.9217694997787476
epoch: 27, step: 90
	action: tensor([[-0.4348,  0.4900,  0.8833,  0.1543, -0.8293,  0.4807,  0.8693]],
       dtype=torch.float64)
	q_value: tensor([[-39.3756]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 27, step: 91
	action: tensor([[ 0.2519, -1.8480,  2.1024, -0.4614, -1.5830,  1.3438,  0.5918]],
       dtype=torch.float64)
	q_value: tensor([[-37.8362]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 27, step: 92
	action: tensor([[-0.6217, -1.0066, -0.1457,  0.3450, -0.5960,  0.9698, -0.2703]],
       dtype=torch.float64)
	q_value: tensor([[-37.8362]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8266255281017616, distance: 1.546612282045957 entropy 0.9217694997787476
epoch: 27, step: 93
	action: tensor([[-0.3271, -0.1408,  0.2282, -0.8528, -0.6277,  1.5418, -1.7007]],
       dtype=torch.float64)
	q_value: tensor([[-35.2962]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21951506092904616, distance: 1.2637183080842875 entropy 0.9217694997787476
epoch: 27, step: 94
	action: tensor([[ 0.0773, -0.9682,  1.5866, -0.3406, -1.1542,  2.4777,  0.4070]],
       dtype=torch.float64)
	q_value: tensor([[-42.9778]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 27, step: 95
	action: tensor([[ 0.4027, -0.6517,  1.5026,  0.2190, -1.3286,  1.6546, -0.5190]],
       dtype=torch.float64)
	q_value: tensor([[-37.8362]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6327606908828876, distance: 0.6934757423139529 entropy 0.9217694997787476
epoch: 27, step: 96
	action: tensor([[ 0.5816, -1.0062, -1.2021,  0.1395,  0.4893, -0.2717, -0.0760]],
       dtype=torch.float64)
	q_value: tensor([[-49.4182]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.40700226831643316, distance: 1.357388270593154 entropy 0.9217694997787476
epoch: 27, step: 97
	action: tensor([[ 1.3267, -0.5124,  0.4250, -1.1772,  1.3497,  0.4936,  0.6161]],
       dtype=torch.float64)
	q_value: tensor([[-35.6712]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3823406185331557, distance: 1.3454396681026797 entropy 0.9217694997787476
epoch: 27, step: 98
	action: tensor([[ 0.0555, -1.0692, -0.0622, -0.8215, -0.5990,  0.8252,  2.1440]],
       dtype=torch.float64)
	q_value: tensor([[-42.9809]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8462962015689808, distance: 1.5549176072980018 entropy 0.9217694997787476
epoch: 27, step: 99
	action: tensor([[-0.6419, -0.9081,  0.3888,  0.3980, -0.8880,  1.2566, -0.5484]],
       dtype=torch.float64)
	q_value: tensor([[-45.2359]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4711321392527139, distance: 1.3879778483854133 entropy 0.9217694997787476
epoch: 27, step: 100
	action: tensor([[-0.0277, -0.8423,  0.7551,  0.0203,  0.2480,  0.0677,  0.2679]],
       dtype=torch.float64)
	q_value: tensor([[-40.5522]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4321880051819964, distance: 1.3694832011661089 entropy 0.9217694997787476
epoch: 27, step: 101
	action: tensor([[-0.4838,  0.7560, -0.8670,  0.1392,  0.3586,  0.7742,  0.3186]],
       dtype=torch.float64)
	q_value: tensor([[-30.2874]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 27, step: 102
	action: tensor([[ 0.6648, -1.5019,  1.7739,  0.0483, -1.1200,  1.5217,  1.2382]],
       dtype=torch.float64)
	q_value: tensor([[-37.8362]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08981662063719154, distance: 1.0917448290914837 entropy 0.9217694997787476
epoch: 27, step: 103
	action: tensor([[-2.1439e-04,  8.4743e-02,  7.2481e-01,  7.8873e-01,  2.0585e-01,
         -4.6837e-01, -1.6048e-01]], dtype=torch.float64)
	q_value: tensor([[-51.4014]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 27, step: 104
	action: tensor([[-0.2238, -1.4933, -0.1276, -0.7476,  0.2404,  0.5931,  0.0681]],
       dtype=torch.float64)
	q_value: tensor([[-37.8362]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8618168723071598, distance: 1.5614395450254037 entropy 0.9217694997787476
epoch: 27, step: 105
	action: tensor([[ 0.2475, -0.8658,  0.8891,  0.6317, -0.8206,  1.0638, -0.1243]],
       dtype=torch.float64)
	q_value: tensor([[-34.5999]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5355835626438967, distance: 0.7798489370825432 entropy 0.9217694997787476
epoch: 27, step: 106
	action: tensor([[ 0.1612, -0.6586,  0.8045,  0.0054,  2.1330, -0.2811, -0.9869]],
       dtype=torch.float64)
	q_value: tensor([[-40.1036]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06633935990348228, distance: 1.1816923205527408 entropy 0.9217694997787476
epoch: 27, step: 107
	action: tensor([[0.2534, 0.4940, 0.4777, 1.0938, 0.4213, 0.5868, 0.3748]],
       dtype=torch.float64)
	q_value: tensor([[-51.4376]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 27, step: 108
	action: tensor([[-0.2860, -1.6210, -0.8144, -0.6778, -0.4418,  0.6509, -0.3304]],
       dtype=torch.float64)
	q_value: tensor([[-37.8362]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.826924063898961, distance: 1.5467386627024384 entropy 0.9217694997787476
epoch: 27, step: 109
	action: tensor([[-0.2783, -0.7798, -0.4187,  0.5570,  0.4606,  0.7166,  1.0314]],
       dtype=torch.float64)
	q_value: tensor([[-36.5473]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14259152909243955, distance: 1.2232132964643232 entropy 0.9217694997787476
epoch: 27, step: 110
	action: tensor([[ 0.6153,  0.2155,  0.6779, -0.3908,  0.1282,  0.6960, -0.7436]],
       dtype=torch.float64)
	q_value: tensor([[-33.0388]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8312982662211638, distance: 0.4700201386154831 entropy 0.9217694997787476
epoch: 27, step: 111
	action: tensor([[ 0.2337,  0.3906, -0.7212, -0.5934,  0.6515,  1.0022,  0.2038]],
       dtype=torch.float64)
	q_value: tensor([[-30.0818]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8809517887946551, distance: 0.39483726083962095 entropy 0.9217694997787476
epoch: 27, step: 112
	action: tensor([[ 0.6797,  0.2030,  0.6218,  0.0080,  0.6908,  0.2529, -0.0233]],
       dtype=torch.float64)
	q_value: tensor([[-31.4859]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8967534782604953, distance: 0.36770065890528814 entropy 0.9217694997787476
epoch: 27, step: 113
	action: tensor([[-0.4172, -0.9454,  0.3056, -0.6646, -0.4671, -0.4435,  0.7724]],
       dtype=torch.float64)
	q_value: tensor([[-29.4540]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.039815083085938, distance: 1.6343764380023957 entropy 0.9217694997787476
epoch: 27, step: 114
	action: tensor([[-0.4685, -0.8903,  0.7883,  0.0117,  0.5359,  0.0662,  0.3083]],
       dtype=torch.float64)
	q_value: tensor([[-32.0768]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8676591625933383, distance: 1.5638874866114396 entropy 0.9217694997787476
epoch: 27, step: 115
	action: tensor([[ 0.6119, -0.8085, -0.5503,  0.5704,  0.5179,  0.8473,  0.2986]],
       dtype=torch.float64)
	q_value: tensor([[-31.1500]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5453746453007629, distance: 0.7715845444144735 entropy 0.9217694997787476
epoch: 27, step: 116
	action: tensor([[-0.5961,  0.2560, -0.8334,  1.0588, -0.0016,  0.6141, -0.0185]],
       dtype=torch.float64)
	q_value: tensor([[-33.3803]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 27, step: 117
	action: tensor([[-0.8995, -1.4325,  0.4516, -0.0074, -0.5623,  1.4951,  0.2898]],
       dtype=torch.float64)
	q_value: tensor([[-37.8362]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8507712113466743, distance: 1.5568008534066444 entropy 0.9217694997787476
epoch: 27, step: 118
	action: tensor([[-0.9375,  0.4605,  0.1964, -0.8276,  0.2902,  0.9907, -0.5178]],
       dtype=torch.float64)
	q_value: tensor([[-40.6558]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5541881497241568, distance: 1.426620596970488 entropy 0.9217694997787476
epoch: 27, step: 119
	action: tensor([[ 0.2963, -0.8254,  1.0376, -0.5293, -0.0014,  0.6560,  0.3704]],
       dtype=torch.float64)
	q_value: tensor([[-33.8812]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3341983332712011, distance: 1.32180347765148 entropy 0.9217694997787476
epoch: 27, step: 120
	action: tensor([[ 0.4448, -0.9759, -0.3958, -0.3813, -1.0404, -0.3547, -0.8038]],
       dtype=torch.float64)
	q_value: tensor([[-33.6214]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5103632661928255, distance: 1.4063628980194973 entropy 0.9217694997787476
epoch: 27, step: 121
	action: tensor([[-0.3814, -0.2836, -0.1287, -0.0723, -0.4949,  0.6063,  0.7142]],
       dtype=torch.float64)
	q_value: tensor([[-35.6115]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3447627259074786, distance: 1.3270262833952957 entropy 0.9217694997787476
epoch: 27, step: 122
	action: tensor([[ 0.2886,  0.1092,  0.7821, -0.7978,  0.5005, -0.2420,  0.5197]],
       dtype=torch.float64)
	q_value: tensor([[-26.5439]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09211153175748032, distance: 1.0903676128637174 entropy 0.9217694997787476
epoch: 27, step: 123
	action: tensor([[-0.2158,  0.4013,  0.7424,  0.3247,  1.7820,  0.6429, -0.0660]],
       dtype=torch.float64)
	q_value: tensor([[-29.0716]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 27, step: 124
	action: tensor([[ 0.2907, -1.6675,  1.7473, -0.2349, -0.4317,  1.5526,  0.6296]],
       dtype=torch.float64)
	q_value: tensor([[-37.8362]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12036913219117285, distance: 1.2112596796895292 entropy 0.9217694997787476
epoch: 27, step: 125
	action: tensor([[ 0.3135, -0.5789, -0.2156,  1.0837,  0.3416,  1.6187,  0.0935]],
       dtype=torch.float64)
	q_value: tensor([[-47.5453]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8164328040849559, distance: 0.49029134884090675 entropy 0.9217694997787476
epoch: 27, step: 126
	action: tensor([[-0.6923, -0.1887,  0.3110,  0.7046, -0.1746, -0.4905,  0.1138]],
       dtype=torch.float64)
	q_value: tensor([[-37.5102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 27, step: 127
	action: tensor([[ 0.3533, -1.3931,  0.5984,  0.0470,  0.2826,  1.5274,  0.4237]],
       dtype=torch.float64)
	q_value: tensor([[-37.8362]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1274661920785849, distance: 1.0689264535282916 entropy 0.9217694997787476
LOSS epoch 27 actor 464.19165412941584 critic 147.01764672472845 
epoch: 28, step: 0
	action: tensor([[ 0.0443, -1.2058, -0.0014, -0.3765,  0.4935,  0.4979,  0.0627]],
       dtype=torch.float64)
	q_value: tensor([[-38.1050]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7776106144669876, distance: 1.5257206023399783 entropy 0.9217694997787476
epoch: 28, step: 1
	action: tensor([[ 0.2138, -0.6876,  0.6410,  0.5857, -0.8257,  0.2137, -0.3060]],
       dtype=torch.float64)
	q_value: tensor([[-32.0763]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37049009744852857, distance: 0.9079417282448774 entropy 0.9217694997787476
epoch: 28, step: 2
	action: tensor([[-0.5503, -0.2951, -0.8570,  0.3260, -0.9723, -0.0023,  0.5956]],
       dtype=torch.float64)
	q_value: tensor([[-33.4109]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6009203853963356, distance: 1.4479099769032946 entropy 0.9217694997787476
epoch: 28, step: 3
	action: tensor([[ 0.5548,  0.4979,  0.4014,  0.6491, -0.1847,  1.1637, -0.1998]],
       dtype=torch.float64)
	q_value: tensor([[-28.2950]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 28, step: 4
	action: tensor([[ 1.1377, -1.6225,  1.0671,  0.0439,  0.6667, -0.0208,  0.5439]],
       dtype=torch.float64)
	q_value: tensor([[-36.3851]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6261523318790796, distance: 1.4592755514091833 entropy 0.9217694997787476
epoch: 28, step: 5
	action: tensor([[ 1.0230, -0.3624, -0.2178,  0.8699, -0.1588,  0.3193,  1.1145]],
       dtype=torch.float64)
	q_value: tensor([[-40.5516]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9221513547419761, distance: 0.3192877253467915 entropy 0.9217694997787476
epoch: 28, step: 6
	action: tensor([[ 0.1852, -0.3512,  0.3073, -0.4301,  0.0284,  0.5026,  0.8461]],
       dtype=torch.float64)
	q_value: tensor([[-33.9196]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07630863784038422, distance: 1.0998162557571263 entropy 0.9217694997787476
epoch: 28, step: 7
	action: tensor([[ 0.6841, -0.1380,  0.1526,  0.4105,  0.0224,  0.1984,  0.5623]],
       dtype=torch.float64)
	q_value: tensor([[-27.0613]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8679787814996969, distance: 0.4157943289536761 entropy 0.9217694997787476
epoch: 28, step: 8
	action: tensor([[ 0.5234, -0.7619,  0.6593,  0.4776,  0.1422, -0.4078, -0.5642]],
       dtype=torch.float64)
	q_value: tensor([[-25.9746]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13194302192003515, distance: 1.066180682601923 entropy 0.9217694997787476
epoch: 28, step: 9
	action: tensor([[ 0.6257, -1.1554,  0.5682, -0.5319,  0.4199, -0.2033,  1.5175]],
       dtype=torch.float64)
	q_value: tensor([[-35.3574]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8742919993367422, distance: 1.5666620338059167 entropy 0.9217694997787476
epoch: 28, step: 10
	action: tensor([[ 0.0575,  0.0404, -0.1002,  0.2401,  0.2918,  0.2910,  0.0890]],
       dtype=torch.float64)
	q_value: tensor([[-38.0598]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5714300938507224, distance: 0.7491478358253337 entropy 0.9217694997787476
epoch: 28, step: 11
	action: tensor([[-0.0882, -0.8358,  0.0587,  0.1524, -0.0261,  1.2034, -0.7840]],
       dtype=torch.float64)
	q_value: tensor([[-21.6877]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12919081804595522, distance: 1.0678695259024262 entropy 0.9217694997787476
epoch: 28, step: 12
	action: tensor([[ 0.9726, -0.7185,  1.2304,  0.0814, -0.0906,  0.2119, -0.9909]],
       dtype=torch.float64)
	q_value: tensor([[-33.8819]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03191307123763332, distance: 1.1259364308211928 entropy 0.9217694997787476
epoch: 28, step: 13
	action: tensor([[ 1.4859,  0.1840, -0.1536, -0.6143, -0.0717,  1.2729,  0.3642]],
       dtype=torch.float64)
	q_value: tensor([[-39.7768]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6660194088170923, distance: 0.6613286047253804 entropy 0.9217694997787476
epoch: 28, step: 14
	action: tensor([[ 1.2108,  0.4336,  0.0337, -0.3287, -0.2931,  0.1649,  0.6360]],
       dtype=torch.float64)
	q_value: tensor([[-35.1728]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8319768431614534, distance: 0.4690738940400611 entropy 0.9217694997787476
epoch: 28, step: 15
	action: tensor([[ 0.1657, -1.7631,  0.3931,  0.0619, -0.3569,  0.5267,  0.9600]],
       dtype=torch.float64)
	q_value: tensor([[-28.2789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 28, step: 16
	action: tensor([[ 0.6475, -0.4576, -0.5387, -0.2220, -0.7858,  0.7540, -0.4020]],
       dtype=torch.float64)
	q_value: tensor([[-36.3851]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25221236330555064, distance: 0.9895684344232789 entropy 0.9217694997787476
epoch: 28, step: 17
	action: tensor([[ 0.3970, -0.2688,  1.6891, -0.4500, -0.5721,  0.9487, -0.1164]],
       dtype=torch.float64)
	q_value: tensor([[-28.2361]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5629282013962977, distance: 0.7565420743058321 entropy 0.9217694997787476
epoch: 28, step: 18
	action: tensor([[ 0.4758, -0.6089,  0.7615, -0.3650,  0.2311,  0.3544,  0.8893]],
       dtype=torch.float64)
	q_value: tensor([[-35.8636]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05537566465923183, distance: 1.1756017696931682 entropy 0.9217694997787476
epoch: 28, step: 19
	action: tensor([[ 0.2498, -0.3450, -0.8552,  0.2573, -0.0269,  0.6025, -0.5601]],
       dtype=torch.float64)
	q_value: tensor([[-30.1706]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29507771360423574, distance: 0.960787432895621 entropy 0.9217694997787476
epoch: 28, step: 20
	action: tensor([[ 0.0361, -0.4863, -0.7173,  0.1334,  0.1142, -0.3358, -0.0463]],
       dtype=torch.float64)
	q_value: tensor([[-24.8346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08420779057688055, distance: 1.1915518768322448 entropy 0.9217694997787476
epoch: 28, step: 21
	action: tensor([[ 0.5730, -0.6532, -1.0195, -0.0338,  0.1903,  1.0568, -1.1108]],
       dtype=torch.float64)
	q_value: tensor([[-25.6387]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27754068760712636, distance: 0.972665226908848 entropy 0.9217694997787476
epoch: 28, step: 22
	action: tensor([[-0.7039, -0.4893, -0.7207, -0.0129, -0.0111,  1.3315,  0.7463]],
       dtype=torch.float64)
	q_value: tensor([[-34.1424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.684401259377551, distance: 1.485181298755707 entropy 0.9217694997787476
epoch: 28, step: 23
	action: tensor([[-0.9255, -0.1451,  0.6339, -0.8208,  0.5239,  0.4706, -1.1448]],
       dtype=torch.float64)
	q_value: tensor([[-32.0707]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2317757719343811, distance: 1.709550650425194 entropy 0.9217694997787476
epoch: 28, step: 24
	action: tensor([[-0.2459, -1.0818,  0.1900, -0.2455, -0.2950,  0.4160,  0.3020]],
       dtype=torch.float64)
	q_value: tensor([[-35.4389]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9086490177531272, distance: 1.5809558059947801 entropy 0.9217694997787476
epoch: 28, step: 25
	action: tensor([[ 0.1350, -1.2676,  0.9878, -0.0321,  0.7224,  0.2381,  0.1553]],
       dtype=torch.float64)
	q_value: tensor([[-29.1074]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7127443083346334, distance: 1.4976245813101463 entropy 0.9217694997787476
epoch: 28, step: 26
	action: tensor([[ 1.3456, -0.6235, -0.1072,  0.5657,  0.8044,  0.8878,  0.3412]],
       dtype=torch.float64)
	q_value: tensor([[-35.2088]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5301902085891306, distance: 0.7843641310083492 entropy 0.9217694997787476
epoch: 28, step: 27
	action: tensor([[-0.7656, -0.5517, -0.3367,  0.0066, -0.2102,  0.5595,  1.3656]],
       dtype=torch.float64)
	q_value: tensor([[-37.7170]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0305683505341325, distance: 1.6306678156542493 entropy 0.9217694997787476
epoch: 28, step: 28
	action: tensor([[-0.2366, -1.0312,  0.8536,  0.6900,  0.1049,  0.5272, -0.6317]],
       dtype=torch.float64)
	q_value: tensor([[-31.3547]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07204072539067008, distance: 1.1023541759506792 entropy 0.9217694997787476
epoch: 28, step: 29
	action: tensor([[ 0.3778, -0.9659,  0.2611, -0.0021,  0.8276,  0.0904,  0.2921]],
       dtype=torch.float64)
	q_value: tensor([[-36.9873]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.367325130132268, distance: 1.3381123874698113 entropy 0.9217694997787476
epoch: 28, step: 30
	action: tensor([[-0.2886,  0.1355,  0.8594,  0.6339,  0.7364,  0.1763, -0.9656]],
       dtype=torch.float64)
	q_value: tensor([[-31.6500]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 28, step: 31
	action: tensor([[ 0.4509, -1.2186,  0.6255, -1.0352, -0.2016,  0.1933, -0.2931]],
       dtype=torch.float64)
	q_value: tensor([[-36.3851]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0554579031980018, distance: 1.6406312766401814 entropy 0.9217694997787476
epoch: 28, step: 32
	action: tensor([[ 0.1590, -1.2841,  0.3909,  0.1503, -0.6169, -0.7971,  1.0462]],
       dtype=torch.float64)
	q_value: tensor([[-34.6135]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 28, step: 33
	action: tensor([[ 0.8940, -1.7363,  0.6084,  0.1908, -0.2962,  0.6884, -0.7030]],
       dtype=torch.float64)
	q_value: tensor([[-36.3851]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18457674208146857, distance: 1.2454844050065725 entropy 0.9217694997787476
epoch: 28, step: 34
	action: tensor([[ 0.6882, -0.8845,  1.3433,  0.4580, -1.3306,  1.7306,  0.7044]],
       dtype=torch.float64)
	q_value: tensor([[-39.4150]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5870554579938863, distance: 0.7353643274836089 entropy 0.9217694997787476
epoch: 28, step: 35
	action: tensor([[-0.5482,  0.0749, -0.1612, -0.1365, -0.2361,  1.3081, -0.7796]],
       dtype=torch.float64)
	q_value: tensor([[-45.3428]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.046762722109874755, distance: 1.1707948871998715 entropy 0.9217694997787476
epoch: 28, step: 36
	action: tensor([[ 0.3627,  0.7002, -0.3148, -0.0522, -0.3228,  1.2716, -0.4405]],
       dtype=torch.float64)
	q_value: tensor([[-30.2695]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 28, step: 37
	action: tensor([[ 0.6153, -1.0815,  0.4992, -0.4729,  1.3586, -0.2063,  0.0118]],
       dtype=torch.float64)
	q_value: tensor([[-36.3851]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7463792857750122, distance: 1.5122583076518077 entropy 0.9217694997787476
epoch: 28, step: 38
	action: tensor([[ 0.2832, -1.1523, -0.2208, -1.1777,  0.3308,  0.6640, -0.2811]],
       dtype=torch.float64)
	q_value: tensor([[-39.3791]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7800214489808395, distance: 1.5267548596648057 entropy 0.9217694997787476
epoch: 28, step: 39
	action: tensor([[-0.1459, -0.6097, -0.1288, -0.3339, -0.0865,  1.1904,  0.9180]],
       dtype=torch.float64)
	q_value: tensor([[-36.6902]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1757074262577749, distance: 1.240812968664119 entropy 0.9217694997787476
epoch: 28, step: 40
	action: tensor([[-0.0120, -0.1041,  0.1144, -0.5525, -0.3829,  1.5370,  0.2091]],
       dtype=torch.float64)
	q_value: tensor([[-31.7344]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3107871746004648, distance: 0.9500213569155093 entropy 0.9217694997787476
epoch: 28, step: 41
	action: tensor([[-0.5075,  0.3980,  0.5663, -0.8291, -0.0171, -0.9469, -0.2033]],
       dtype=torch.float64)
	q_value: tensor([[-32.7446]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5373538643332385, distance: 1.4188732962467325 entropy 0.9217694997787476
epoch: 28, step: 42
	action: tensor([[ 0.1850,  0.1281,  0.7687, -0.0268, -0.3776,  0.7735, -0.0853]],
       dtype=torch.float64)
	q_value: tensor([[-27.9551]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 28, step: 43
	action: tensor([[-0.3128, -0.4493, -0.0364, -0.2042, -0.3748,  0.7992, -0.6046]],
       dtype=torch.float64)
	q_value: tensor([[-36.3851]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.33020349972897645, distance: 1.3198231336221409 entropy 0.9217694997787476
epoch: 28, step: 44
	action: tensor([[ 0.0927,  0.1773,  0.9206, -0.3020, -0.7301,  0.0073, -0.1098]],
       dtype=torch.float64)
	q_value: tensor([[-27.5627]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30993083105870334, distance: 0.9506113721286066 entropy 0.9217694997787476
epoch: 28, step: 45
	action: tensor([[ 0.9363,  0.3444,  0.0860, -1.4292,  0.8243,  1.0196,  0.4358]],
       dtype=torch.float64)
	q_value: tensor([[-27.4257]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.565233106571609, distance: 0.7545446190096861 entropy 0.9217694997787476
epoch: 28, step: 46
	action: tensor([[-0.4471,  0.3229,  0.9691, -1.2747,  0.0281,  1.0137,  0.2119]],
       dtype=torch.float64)
	q_value: tensor([[-36.3424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4281546417715163, distance: 1.3675534552661532 entropy 0.9217694997787476
epoch: 28, step: 47
	action: tensor([[ 0.0274, -0.5318,  0.1350, -0.2706, -0.3419,  0.5993, -0.0696]],
       dtype=torch.float64)
	q_value: tensor([[-34.0513]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17838874304502017, distance: 1.242227061079141 entropy 0.9217694997787476
epoch: 28, step: 48
	action: tensor([[-0.4311, -1.2446,  0.0951, -0.2674, -0.6314,  0.2862, -0.0320]],
       dtype=torch.float64)
	q_value: tensor([[-24.6773]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0927542234015943, distance: 1.655449003682126 entropy 0.9217694997787476
epoch: 28, step: 49
	action: tensor([[ 0.8060,  0.6710, -0.4529, -0.8351, -0.1454,  1.0434, -0.2299]],
       dtype=torch.float64)
	q_value: tensor([[-31.8018]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9821497447538369, distance: 0.15288993941754161 entropy 0.9217694997787476
epoch: 28, step: 50
	action: tensor([[-0.1206, -0.6289,  0.5036, -0.2524, -0.4200, -0.0461,  0.3764]],
       dtype=torch.float64)
	q_value: tensor([[-28.9525]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5763192118643656, distance: 1.4367419669743005 entropy 0.9217694997787476
epoch: 28, step: 51
	action: tensor([[-0.5008,  0.0172, -0.2259, -0.1568,  0.4182, -0.1989, -0.3145]],
       dtype=torch.float64)
	q_value: tensor([[-25.7451]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3652902703786387, distance: 1.3371163241396693 entropy 0.9217694997787476
epoch: 28, step: 52
	action: tensor([[-1.5852, -0.7980, -0.3078,  0.4208,  0.7729, -0.0944,  0.5549]],
       dtype=torch.float64)
	q_value: tensor([[-24.1564]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.896915652342484, distance: 1.9477114548219348 entropy 0.9217694997787476
epoch: 28, step: 53
	action: tensor([[-0.1555, -0.5010,  0.6381, -0.4363,  0.0199, -0.0415,  0.2073]],
       dtype=torch.float64)
	q_value: tensor([[-34.6364]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6467994516530413, distance: 1.4685104797382103 entropy 0.9217694997787476
epoch: 28, step: 54
	action: tensor([[ 0.7045, -0.6373, -0.1699,  0.2265, -0.1598,  0.2597, -0.0155]],
       dtype=torch.float64)
	q_value: tensor([[-24.9314]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33266612275925433, distance: 0.9348206116056023 entropy 0.9217694997787476
epoch: 28, step: 55
	action: tensor([[ 0.8746,  1.4012, -0.4089, -0.9130, -0.8859,  0.5842, -0.1856]],
       dtype=torch.float64)
	q_value: tensor([[-26.4660]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9547215559410169, distance: 0.24350194832720193 entropy 0.9217694997787476
epoch: 28, step: 56
	action: tensor([[-0.5650, -0.7742,  0.7298, -0.0700,  0.2617,  0.3497, -0.2785]],
       dtype=torch.float64)
	q_value: tensor([[-30.9341]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 28, step: 57
	action: tensor([[-0.9420, -0.6979, -0.4965, -0.4174, -1.0863, -0.0722,  0.1656]],
       dtype=torch.float64)
	q_value: tensor([[-36.3851]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0858487925463254, distance: 1.652715516334737 entropy 0.9217694997787476
epoch: 28, step: 58
	action: tensor([[ 0.9919, -0.6544, -0.4068,  0.0468,  0.4992,  0.9952,  0.8291]],
       dtype=torch.float64)
	q_value: tensor([[-32.3672]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5230330158350647, distance: 0.7903161418459734 entropy 0.9217694997787476
epoch: 28, step: 59
	action: tensor([[-1.0234,  0.3648,  0.8662,  0.3663, -0.6727,  0.1959, -0.1024]],
       dtype=torch.float64)
	q_value: tensor([[-35.2024]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 28, step: 60
	action: tensor([[-0.3650, -1.3598,  0.5275,  0.3336, -0.2356,  1.2086,  0.4811]],
       dtype=torch.float64)
	q_value: tensor([[-36.3851]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18909676120360497, distance: 1.2478583555850336 entropy 0.9217694997787476
epoch: 28, step: 61
	action: tensor([[-0.5326, -0.1904,  0.2008, -0.7288, -1.0915,  0.3035,  0.5363]],
       dtype=torch.float64)
	q_value: tensor([[-34.7576]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8713654532295882, distance: 1.5654384518315327 entropy 0.9217694997787476
epoch: 28, step: 62
	action: tensor([[ 1.3901, -1.1357, -0.9894, -0.6149,  0.6812,  0.3527,  0.3709]],
       dtype=torch.float64)
	q_value: tensor([[-29.3866]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8229596153085859, distance: 1.5450595299301242 entropy 0.9217694997787476
epoch: 28, step: 63
	action: tensor([[ 0.0710, -0.5073,  0.5077, -1.7441,  0.0763,  0.4700, -0.4833]],
       dtype=torch.float64)
	q_value: tensor([[-41.1149]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6967664449090343, distance: 1.4906226868554586 entropy 0.9217694997787476
epoch: 28, step: 64
	action: tensor([[-1.0231, -0.2493, -0.4392, -0.6379,  0.4649,  1.2449,  0.2137]],
       dtype=torch.float64)
	q_value: tensor([[-35.1675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7357699916605367, distance: 1.5076578080646952 entropy 0.9217694997787476
epoch: 28, step: 65
	action: tensor([[-0.2105, -0.6163,  0.6632, -0.6155,  0.6696, -0.2147, -0.3067]],
       dtype=torch.float64)
	q_value: tensor([[-34.4639]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9269062060504973, distance: 1.588499128922561 entropy 0.9217694997787476
epoch: 28, step: 66
	action: tensor([[-0.2862, -1.4662, -0.2908, -0.6857, -0.4860,  0.2291, -0.4827]],
       dtype=torch.float64)
	q_value: tensor([[-30.5916]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 28, step: 67
	action: tensor([[-0.3629, -0.9485,  0.2763,  0.5417, -0.5841,  0.7906,  0.9288]],
       dtype=torch.float64)
	q_value: tensor([[-36.3851]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24535857004118133, distance: 1.277038234240831 entropy 0.9217694997787476
epoch: 28, step: 68
	action: tensor([[ 0.5164, -1.4903,  0.0460,  0.6509,  0.3584,  0.1277, -0.3897]],
       dtype=torch.float64)
	q_value: tensor([[-31.3354]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 28, step: 69
	action: tensor([[ 0.8127, -1.8896,  1.1879, -0.2370, -0.6225,  0.1074, -0.2143]],
       dtype=torch.float64)
	q_value: tensor([[-36.3851]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 28, step: 70
	action: tensor([[ 0.2890, -0.1244,  0.8657, -0.1315,  0.0758,  0.9325,  0.9026]],
       dtype=torch.float64)
	q_value: tensor([[-36.3851]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6897761545130144, distance: 0.6373739092572429 entropy 0.9217694997787476
epoch: 28, step: 71
	action: tensor([[ 8.0297e-02, -9.0117e-01,  1.3288e-01, -5.7765e-01, -3.2708e-01,
          9.5403e-02,  1.5389e-04]], dtype=torch.float64)
	q_value: tensor([[-31.9940]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8108423119329431, distance: 1.5399159243962126 entropy 0.9217694997787476
epoch: 28, step: 72
	action: tensor([[-0.4095, -0.1494, -0.0658, -1.1365,  0.4307,  0.9677, -0.9477]],
       dtype=torch.float64)
	q_value: tensor([[-27.7164]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.29897370217003294, distance: 1.304238069432308 entropy 0.9217694997787476
epoch: 28, step: 73
	action: tensor([[-0.5331, -1.0370,  0.9585, -0.1614,  0.0877, -0.1232, -0.6522]],
       dtype=torch.float64)
	q_value: tensor([[-33.3850]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.154650918101093, distance: 1.6797519450467957 entropy 0.9217694997787476
epoch: 28, step: 74
	action: tensor([[ 0.2158, -0.2642,  0.4521, -0.5274,  1.0219,  0.3754, -0.7893]],
       dtype=torch.float64)
	q_value: tensor([[-35.1079]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.00794027672350861, distance: 1.1488784760738762 entropy 0.9217694997787476
epoch: 28, step: 75
	action: tensor([[-0.3614, -0.2961,  0.0344, -0.4503, -0.0417,  0.6764,  0.0498]],
       dtype=torch.float64)
	q_value: tensor([[-32.9688]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.33840329947718795, distance: 1.3238847898277726 entropy 0.9217694997787476
epoch: 28, step: 76
	action: tensor([[-0.6099, -0.0217, -1.1858, -1.0794,  0.3732, -0.3092, -1.2790]],
       dtype=torch.float64)
	q_value: tensor([[-24.4825]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24278473604873885, distance: 0.995786818756962 entropy 0.9217694997787476
epoch: 28, step: 77
	action: tensor([[-0.0355, -0.1113,  1.5624,  0.7281, -0.0641,  0.6671, -0.4282]],
       dtype=torch.float64)
	q_value: tensor([[-36.8604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7154948036079187, distance: 0.6103821080577547 entropy 0.9217694997787476
epoch: 28, step: 78
	action: tensor([[ 0.6219, -0.6953,  0.0284, -0.8999,  0.5038,  0.8991,  0.3689]],
       dtype=torch.float64)
	q_value: tensor([[-36.8148]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2048401493881764, distance: 1.256091882259254 entropy 0.9217694997787476
epoch: 28, step: 79
	action: tensor([[ 0.5750,  0.3991, -1.1874,  0.9117, -0.0157,  0.8565,  0.3975]],
       dtype=torch.float64)
	q_value: tensor([[-33.7735]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 28, step: 80
	action: tensor([[ 0.1953, -0.8587,  1.1949, -1.2300, -0.1173,  1.3758,  0.6687]],
       dtype=torch.float64)
	q_value: tensor([[-36.3851]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.33922995578774584, distance: 1.3242935712397745 entropy 0.9217694997787476
epoch: 28, step: 81
	action: tensor([[-0.9389, -0.7972,  0.5757, -0.0913,  0.7888,  0.3226, -1.0949]],
       dtype=torch.float64)
	q_value: tensor([[-40.1009]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1869116536973565, distance: 1.692280354150319 entropy 0.9217694997787476
epoch: 28, step: 82
	action: tensor([[-0.2950, -0.1507, -0.0486,  1.1175,  0.6236,  0.6587,  0.1306]],
       dtype=torch.float64)
	q_value: tensor([[-37.9757]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 28, step: 83
	action: tensor([[-0.3644, -1.7039,  0.4327, -0.6478, -0.7430,  2.1896,  0.1088]],
       dtype=torch.float64)
	q_value: tensor([[-36.3851]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.406285087802841, distance: 1.3570422809230194 entropy 0.9217694997787476
epoch: 28, step: 84
	action: tensor([[ 0.1540,  0.4100, -0.2427, -0.3642, -0.0452,  0.4242,  0.6377]],
       dtype=torch.float64)
	q_value: tensor([[-44.1966]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 28, step: 85
	action: tensor([[-0.5100, -1.5085, -0.8114,  0.9896,  0.2530,  1.4991, -0.5615]],
       dtype=torch.float64)
	q_value: tensor([[-36.3851]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3371621730039194, distance: 1.323270815863835 entropy 0.9217694997787476
epoch: 28, step: 86
	action: tensor([[ 0.0980,  0.0663,  0.7220, -0.3108, -0.1484,  0.2737, -0.7920]],
       dtype=torch.float64)
	q_value: tensor([[-38.8953]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.291786593536307, distance: 0.9630276689558336 entropy 0.9217694997787476
epoch: 28, step: 87
	action: tensor([[ 2.1065,  0.1199,  0.9099, -0.7722, -0.0753,  0.5921, -1.2124]],
       dtype=torch.float64)
	q_value: tensor([[-27.1890]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 28, step: 88
	action: tensor([[-0.5697, -2.1660,  0.9970,  0.2613,  0.0952,  0.5295,  0.6065]],
       dtype=torch.float64)
	q_value: tensor([[-36.3851]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 28, step: 89
	action: tensor([[ 0.5510, -1.0690,  0.4915, -0.4209, -0.1356,  1.9663,  0.7303]],
       dtype=torch.float64)
	q_value: tensor([[-36.3851]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21636747911941667, distance: 1.013008101814187 entropy 0.9217694997787476
epoch: 28, step: 90
	action: tensor([[ 0.3729, -0.3347, -0.0315,  0.1436,  0.3321,  0.3585, -0.0223]],
       dtype=torch.float64)
	q_value: tensor([[-42.0547]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5165671371203586, distance: 0.7956549666431431 entropy 0.9217694997787476
epoch: 28, step: 91
	action: tensor([[ 4.9585e-02, -7.2373e-01,  6.1433e-04, -7.4275e-02,  6.6773e-01,
         -2.8171e-01, -1.1565e-01]], dtype=torch.float64)
	q_value: tensor([[-23.7808]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.47152190408130346, distance: 1.388161703086855 entropy 0.9217694997787476
epoch: 28, step: 92
	action: tensor([[ 0.0917, -1.5271,  0.4433,  0.9760,  0.1121,  0.8852,  0.7179]],
       dtype=torch.float64)
	q_value: tensor([[-29.2698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 28, step: 93
	action: tensor([[ 0.6246, -0.3761,  0.6569, -0.6255, -0.2339, -0.3940,  0.0100]],
       dtype=torch.float64)
	q_value: tensor([[-36.3851]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12248933468761747, distance: 1.212405240508063 entropy 0.9217694997787476
epoch: 28, step: 94
	action: tensor([[-0.1571, -0.4166, -0.3460,  0.5857,  0.3885,  0.8223,  1.0919]],
       dtype=torch.float64)
	q_value: tensor([[-27.2841]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27312245325562345, distance: 0.9756348838319133 entropy 0.9217694997787476
epoch: 28, step: 95
	action: tensor([[-0.8721, -0.4398,  0.4458,  0.7073, -0.3577,  0.4521,  1.0704]],
       dtype=torch.float64)
	q_value: tensor([[-30.4842]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3122794533129183, distance: 1.3109008883885735 entropy 0.9217694997787476
epoch: 28, step: 96
	action: tensor([[ 1.4554, -0.0231,  0.1695,  0.2418, -0.1244,  0.3747,  0.3951]],
       dtype=torch.float64)
	q_value: tensor([[-30.8361]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6713960316268859, distance: 0.6559837711767946 entropy 0.9217694997787476
epoch: 28, step: 97
	action: tensor([[ 0.1073,  0.1317, -0.0831,  0.5486, -0.4064,  1.6487,  0.4201]],
       dtype=torch.float64)
	q_value: tensor([[-30.4829]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 28, step: 98
	action: tensor([[ 0.1198, -0.2804,  0.5478, -1.0516, -0.8637,  0.3253,  0.1778]],
       dtype=torch.float64)
	q_value: tensor([[-36.3851]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.43029309662385407, distance: 1.3685769289572869 entropy 0.9217694997787476
epoch: 28, step: 99
	action: tensor([[ 0.6275, -0.1177,  0.9050,  0.6165, -0.6223,  1.1759,  0.6660]],
       dtype=torch.float64)
	q_value: tensor([[-29.2511]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9744711285296179, distance: 0.18284054269824826 entropy 0.9217694997787476
epoch: 28, step: 100
	action: tensor([[-0.3151, -0.6528, -1.1732, -0.1025, -0.2247, -0.0422, -0.3815]],
       dtype=torch.float64)
	q_value: tensor([[-34.7276]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5068759772720464, distance: 1.4047383789109567 entropy 0.9217694997787476
epoch: 28, step: 101
	action: tensor([[ 0.3843, -0.3443,  1.1840, -0.3323,  0.4839,  0.2040,  0.4530]],
       dtype=torch.float64)
	q_value: tensor([[-28.4895]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13599324959636938, distance: 1.0636904524017765 entropy 0.9217694997787476
epoch: 28, step: 102
	action: tensor([[ 0.2213, -0.6225,  0.2410,  0.0243,  0.1767,  1.2278, -0.2835]],
       dtype=torch.float64)
	q_value: tensor([[-30.2225]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4690303762560827, distance: 0.833856908782029 entropy 0.9217694997787476
epoch: 28, step: 103
	action: tensor([[ 0.4444, -0.0836,  1.3525,  0.1942, -0.3912,  0.1401, -0.4554]],
       dtype=torch.float64)
	q_value: tensor([[-30.6606]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6838517858360336, distance: 0.6434311166057026 entropy 0.9217694997787476
epoch: 28, step: 104
	action: tensor([[ 0.4939, -0.0433, -0.0388, -0.2598,  0.0663,  0.8313, -0.0670]],
       dtype=torch.float64)
	q_value: tensor([[-33.1094]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7340475320262279, distance: 0.5901449643469378 entropy 0.9217694997787476
epoch: 28, step: 105
	action: tensor([[-0.0180, -1.0144, -0.6245,  0.4792,  0.8453, -0.4677,  0.3594]],
       dtype=torch.float64)
	q_value: tensor([[-25.0080]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6754185683705312, distance: 1.4812158663633381 entropy 0.9217694997787476
epoch: 28, step: 106
	action: tensor([[-0.1242, -0.8517,  0.2702,  0.2203,  0.5421,  0.5824,  0.8721]],
       dtype=torch.float64)
	q_value: tensor([[-34.8531]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14238429536849906, distance: 1.2231023633201457 entropy 0.9217694997787476
epoch: 28, step: 107
	action: tensor([[ 0.2600, -0.1340, -0.7581,  0.4006,  0.1661, -0.4726, -0.5119]],
       dtype=torch.float64)
	q_value: tensor([[-30.6381]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42607736658900075, distance: 0.8669286773126309 entropy 0.9217694997787476
epoch: 28, step: 108
	action: tensor([[-0.7126, -0.8442,  0.6623,  0.0140, -0.2871, -0.0479, -0.4912]],
       dtype=torch.float64)
	q_value: tensor([[-25.8467]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.065001134947496, distance: 1.6444354881613452 entropy 0.9217694997787476
epoch: 28, step: 109
	action: tensor([[ 0.9191,  0.1327,  0.1678,  0.3618, -0.2826, -0.4161, -0.0585]],
       dtype=torch.float64)
	q_value: tensor([[-31.6609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8856636107402318, distance: 0.3869447243493048 entropy 0.9217694997787476
epoch: 28, step: 110
	action: tensor([[ 0.6430,  0.0892, -0.0525, -0.3092,  0.7031,  0.7450, -1.2314]],
       dtype=torch.float64)
	q_value: tensor([[-26.6217]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8023853343650665, distance: 0.5087053196872866 entropy 0.9217694997787476
epoch: 28, step: 111
	action: tensor([[ 0.2584, -0.1112,  0.4882, -0.7360,  0.2102,  0.4473,  0.0638]],
       dtype=torch.float64)
	q_value: tensor([[-32.8015]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10295972992083058, distance: 1.0838337316246922 entropy 0.9217694997787476
epoch: 28, step: 112
	action: tensor([[ 0.9379,  0.7378,  0.3009, -0.2646,  0.1226,  0.4927,  0.2736]],
       dtype=torch.float64)
	q_value: tensor([[-25.3446]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.979018299300334, distance: 0.16575896909708066 entropy 0.9217694997787476
epoch: 28, step: 113
	action: tensor([[ 0.4795,  0.3639,  0.3491, -0.3320, -1.0125,  0.4058,  0.4055]],
       dtype=torch.float64)
	q_value: tensor([[-27.5870]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7699996215592851, distance: 0.5488086761883052 entropy 0.9217694997787476
epoch: 28, step: 114
	action: tensor([[ 0.4029,  0.2782,  0.1766, -0.9789, -1.1734,  0.3927, -0.4312]],
       dtype=torch.float64)
	q_value: tensor([[-27.3040]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3620474983443486, distance: 0.914009828234816 entropy 0.9217694997787476
epoch: 28, step: 115
	action: tensor([[ 0.1580, -0.7710,  0.0124,  0.0721, -0.1747,  0.0216,  0.4435]],
       dtype=torch.float64)
	q_value: tensor([[-29.4517]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.22902234103041308, distance: 1.2686346879555244 entropy 0.9217694997787476
epoch: 28, step: 116
	action: tensor([[-0.2140,  0.5624,  0.0044,  0.7916,  0.5391,  0.0234, -0.0841]],
       dtype=torch.float64)
	q_value: tensor([[-25.9496]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 28, step: 117
	action: tensor([[ 1.0092, -1.0405,  0.8313, -1.1067,  0.0449,  1.4773, -0.3256]],
       dtype=torch.float64)
	q_value: tensor([[-36.3851]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2795227944422938, distance: 1.294436369894874 entropy 0.9217694997787476
epoch: 28, step: 118
	action: tensor([[-0.1511, -1.2501,  1.1595, -0.8190, -0.4619,  0.2476,  1.0149]],
       dtype=torch.float64)
	q_value: tensor([[-42.3093]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1956884567452892, distance: 1.695672795463087 entropy 0.9217694997787476
epoch: 28, step: 119
	action: tensor([[-0.7342,  0.0298,  0.0463, -1.5244,  0.4942, -0.2826,  0.1597]],
       dtype=torch.float64)
	q_value: tensor([[-35.9046]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7068395282541007, distance: 1.4950407809362567 entropy 0.9217694997787476
epoch: 28, step: 120
	action: tensor([[ 0.0894, -1.0095,  0.4723,  0.4214,  0.7271,  0.9553, -0.2055]],
       dtype=torch.float64)
	q_value: tensor([[-32.3528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22274134753028707, distance: 1.0088799146279486 entropy 0.9217694997787476
epoch: 28, step: 121
	action: tensor([[ 0.2940,  0.3884,  0.2309, -0.2245, -0.0538,  1.1737,  0.8672]],
       dtype=torch.float64)
	q_value: tensor([[-34.9756]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 28, step: 122
	action: tensor([[-0.2548, -0.0808, -0.1266, -1.2404,  0.4029,  0.1143, -0.2324]],
       dtype=torch.float64)
	q_value: tensor([[-36.3851]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.29605033764715416, distance: 1.3027696367434145 entropy 0.9217694997787476
epoch: 28, step: 123
	action: tensor([[ 0.5206, -0.4889,  1.2441, -0.4358,  0.4718,  0.1032,  0.4722]],
       dtype=torch.float64)
	q_value: tensor([[-29.2136]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.004007830044137295, distance: 1.146635129576274 entropy 0.9217694997787476
epoch: 28, step: 124
	action: tensor([[ 0.1691,  0.2468,  0.0455,  0.2619, -0.5587, -0.0474,  0.0111]],
       dtype=torch.float64)
	q_value: tensor([[-31.4589]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 28, step: 125
	action: tensor([[-0.1050, -0.7124,  0.9508, -0.3334,  0.0496,  0.8397,  0.3223]],
       dtype=torch.float64)
	q_value: tensor([[-36.3851]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2887396829880813, distance: 1.2990901625207139 entropy 0.9217694997787476
epoch: 28, step: 126
	action: tensor([[-0.6503,  0.0971,  0.1507, -0.2851,  1.0421,  0.6362,  0.8571]],
       dtype=torch.float64)
	q_value: tensor([[-30.4542]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.270719505057329, distance: 1.2899757358137325 entropy 0.9217694997787476
epoch: 28, step: 127
	action: tensor([[ 0.1320, -0.7793,  0.3353, -0.8631, -0.4862,  1.1429,  0.8928]],
       dtype=torch.float64)
	q_value: tensor([[-31.5407]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4262325457960474, distance: 1.3666328777836059 entropy 0.9217694997787476
LOSS epoch 28 actor 439.8680170199971 critic 182.36644076555922 
epoch: 29, step: 0
	action: tensor([[-0.1079, -0.6100,  0.3362, -0.3003, -0.6843, -0.6756,  0.0500]],
       dtype=torch.float64)
	q_value: tensor([[-32.6996]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5742458298212794, distance: 1.4357967601969375 entropy 0.9217694997787476
epoch: 29, step: 1
	action: tensor([[ 0.9818,  0.2540,  0.8841, -0.1834, -1.1742, -0.8628, -0.4552]],
       dtype=torch.float64)
	q_value: tensor([[-27.1985]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7141096468082326, distance: 0.6118661729444584 entropy 0.9217694997787476
epoch: 29, step: 2
	action: tensor([[ 1.5753, -0.4200,  0.7046,  0.3194, -0.6948,  1.0126,  0.1516]],
       dtype=torch.float64)
	q_value: tensor([[-35.7998]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.46490640959636087, distance: 0.837088870250926 entropy 0.9217694997787476
epoch: 29, step: 3
	action: tensor([[ 0.7099, -0.8235,  0.2851, -0.0988, -1.0566,  0.2038, -0.9090]],
       dtype=torch.float64)
	q_value: tensor([[-34.8920]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1366083683274557, distance: 1.2200064251452487 entropy 0.9217694997787476
epoch: 29, step: 4
	action: tensor([[ 0.8629,  0.6780,  0.1809,  0.0058,  0.8217,  0.0684, -0.9020]],
       dtype=torch.float64)
	q_value: tensor([[-32.5283]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9851465973868561, distance: 0.13946642539332008 entropy 0.9217694997787476
epoch: 29, step: 5
	action: tensor([[-0.1096, -0.3774,  0.1356,  0.0754,  0.1165,  0.1682, -0.2835]],
       dtype=torch.float64)
	q_value: tensor([[-30.2212]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.013735750744743758, distance: 1.1521766634438892 entropy 0.9217694997787476
epoch: 29, step: 6
	action: tensor([[ 0.3679,  0.0169, -0.3588,  0.9222,  0.8381,  1.1891, -0.9445]],
       dtype=torch.float64)
	q_value: tensor([[-22.5755]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 29, step: 7
	action: tensor([[ 0.6377, -0.3701,  0.2294, -1.2712,  0.1744,  0.0521, -0.0969]],
       dtype=torch.float64)
	q_value: tensor([[-34.8824]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.34841405016943683, distance: 1.3288266451050021 entropy 0.9217694997787476
epoch: 29, step: 8
	action: tensor([[ 0.9764, -0.0137,  0.8502,  0.0304, -0.8845, -0.5484, -0.2892]],
       dtype=torch.float64)
	q_value: tensor([[-28.5788]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6052991413719477, distance: 0.7189368241154148 entropy 0.9217694997787476
epoch: 29, step: 9
	action: tensor([[ 0.0864,  0.9741, -0.6741,  0.1698, -0.1148,  0.4441, -0.0947]],
       dtype=torch.float64)
	q_value: tensor([[-32.4645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 29, step: 10
	action: tensor([[-0.3136, -0.3004, -0.7143,  0.2299, -0.5687, -0.0662, -0.1974]],
       dtype=torch.float64)
	q_value: tensor([[-34.8824]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.265080931305695, distance: 1.2871105440567807 entropy 0.9217694997787476
epoch: 29, step: 11
	action: tensor([[ 0.4202, -0.4810, -0.4297, -0.4663, -0.7204, -0.3308, -0.0844]],
       dtype=torch.float64)
	q_value: tensor([[-23.3209]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0032037276341876364, distance: 1.1425096998134916 entropy 0.9217694997787476
epoch: 29, step: 12
	action: tensor([[-1.3185,  0.7871,  0.1416, -0.2877,  0.7108, -0.4069,  0.5327]],
       dtype=torch.float64)
	q_value: tensor([[-25.4754]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9346587624954277, distance: 1.591691439803434 entropy 0.9217694997787476
epoch: 29, step: 13
	action: tensor([[ 0.0084, -0.6205,  0.5111,  0.0007, -0.1615,  0.5219,  0.4380]],
       dtype=torch.float64)
	q_value: tensor([[-30.2178]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.016559712434855545, distance: 1.1537803555045303 entropy 0.9217694997787476
epoch: 29, step: 14
	action: tensor([[-0.1499, -0.7388, -1.1375, -0.6218,  0.3198,  0.8047, -0.1445]],
       dtype=torch.float64)
	q_value: tensor([[-24.4543]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2331803502316352, distance: 1.2707788884679365 entropy 0.9217694997787476
epoch: 29, step: 15
	action: tensor([[ 1.0020, -0.9579, -0.1808,  0.7702, -0.5760, -0.1288, -1.7481]],
       dtype=torch.float64)
	q_value: tensor([[-31.6074]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3327451145502821, distance: 0.9347652829745845 entropy 0.9217694997787476
epoch: 29, step: 16
	action: tensor([[ 1.2811,  0.2380, -0.6610, -0.4004, -0.4425,  0.6620,  0.4690]],
       dtype=torch.float64)
	q_value: tensor([[-43.7188]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7356073926321951, distance: 0.5884117643283945 entropy 0.9217694997787476
epoch: 29, step: 17
	action: tensor([[-0.4574, -0.6468,  0.1432,  0.1166,  0.4418,  0.5310,  0.3551]],
       dtype=torch.float64)
	q_value: tensor([[-28.4004]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.40282248208899163, distance: 1.3553705721014344 entropy 0.9217694997787476
epoch: 29, step: 18
	action: tensor([[ 0.4522, -1.0229, -1.5983, -0.5577,  0.2250,  0.2623,  0.3197]],
       dtype=torch.float64)
	q_value: tensor([[-24.7701]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.28161523564414837, distance: 1.2954943524389655 entropy 0.9217694997787476
epoch: 29, step: 19
	action: tensor([[ 0.5683, -0.7457, -0.0584,  0.3327, -0.2861, -0.8394, -0.1527]],
       dtype=torch.float64)
	q_value: tensor([[-34.7723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.00797963112257527, distance: 1.1489009044765042 entropy 0.9217694997787476
epoch: 29, step: 20
	action: tensor([[ 0.2398, -0.6011,  1.4809,  1.3717, -0.1563, -0.1106,  0.2812]],
       dtype=torch.float64)
	q_value: tensor([[-30.7282]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5637175875461233, distance: 0.7558585782186725 entropy 0.9217694997787476
epoch: 29, step: 21
	action: tensor([[ 1.5181, -0.2825, -0.2476, -0.4654,  0.7305,  0.9277, -0.6665]],
       dtype=torch.float64)
	q_value: tensor([[-40.8318]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11515012523259383, distance: 1.076444120872414 entropy 0.9217694997787476
epoch: 29, step: 22
	action: tensor([[ 0.0229, -0.5207,  0.2833, -1.1444, -0.6828,  0.7642,  0.2779]],
       dtype=torch.float64)
	q_value: tensor([[-35.0569]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6005897398338935, distance: 1.447760447377979 entropy 0.9217694997787476
epoch: 29, step: 23
	action: tensor([[ 0.8739,  0.0788,  0.5340,  1.0276, -0.2811, -0.3663, -0.4893]],
       dtype=torch.float64)
	q_value: tensor([[-29.4225]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9417713574822066, distance: 0.27613726943081507 entropy 0.9217694997787476
epoch: 29, step: 24
	action: tensor([[ 0.2059,  0.0280, -0.4682, -0.1518,  1.1680,  2.0690, -0.6638]],
       dtype=torch.float64)
	q_value: tensor([[-33.3570]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8649731505237679, distance: 0.42050073626792006 entropy 0.9217694997787476
epoch: 29, step: 25
	action: tensor([[-0.2356,  0.3320, -0.7501, -0.5683,  0.9158,  1.2400, -1.8966]],
       dtype=torch.float64)
	q_value: tensor([[-38.8630]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 29, step: 26
	action: tensor([[ 1.0938, -0.4174,  0.3320, -0.6741, -0.4209, -0.1270,  0.8391]],
       dtype=torch.float64)
	q_value: tensor([[-34.8824]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11069711670473237, distance: 1.2060200153572065 entropy 0.9217694997787476
epoch: 29, step: 27
	action: tensor([[-0.0762, -0.9411, -0.1424,  0.0347,  0.1591,  0.7045, -0.4116]],
       dtype=torch.float64)
	q_value: tensor([[-29.4344]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2847384687440724, distance: 1.297071919821738 entropy 0.9217694997787476
epoch: 29, step: 28
	action: tensor([[ 0.8657, -0.2747,  1.0261, -0.6468,  0.1680,  1.0267, -0.0558]],
       dtype=torch.float64)
	q_value: tensor([[-27.7962]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.49326992016030224, distance: 0.8146011817857135 entropy 0.9217694997787476
epoch: 29, step: 29
	action: tensor([[-7.6467e-01,  1.6609e-01,  4.8379e-01,  8.8658e-01, -1.8847e-01,
          7.0177e-04,  2.9666e-01]], dtype=torch.float64)
	q_value: tensor([[-30.3103]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 29, step: 30
	action: tensor([[ 0.8202,  0.4449,  0.2290, -1.1087,  0.2444,  0.8477,  0.6182]],
       dtype=torch.float64)
	q_value: tensor([[-34.8824]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 29, step: 31
	action: tensor([[ 0.2422, -0.2747,  1.0518, -0.3987, -0.2852, -0.0613, -0.4045]],
       dtype=torch.float64)
	q_value: tensor([[-34.8824]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.004989863540867612, distance: 1.147195762102936 entropy 0.9217694997787476
epoch: 29, step: 32
	action: tensor([[ 0.5903, -0.9874, -0.0912, -0.0815, -0.1673,  0.2083, -0.7372]],
       dtype=torch.float64)
	q_value: tensor([[-27.2016]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3819371941655969, distance: 1.3452433262071446 entropy 0.9217694997787476
epoch: 29, step: 33
	action: tensor([[-0.0553,  0.0888,  0.3880, -0.3554,  0.0280, -0.0653, -0.0901]],
       dtype=torch.float64)
	q_value: tensor([[-30.2394]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07205882580909251, distance: 1.102343424848944 entropy 0.9217694997787476
epoch: 29, step: 34
	action: tensor([[ 0.6842,  0.7309, -0.4498, -1.0061, -0.4648, -0.1615,  1.4017]],
       dtype=torch.float64)
	q_value: tensor([[-20.9824]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8425253191255571, distance: 0.454111041530702 entropy 0.9217694997787476
epoch: 29, step: 35
	action: tensor([[-0.8296, -0.0233, -0.1153, -0.6529, -0.1622,  1.0532,  0.3993]],
       dtype=torch.float64)
	q_value: tensor([[-32.0344]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6921072930684069, distance: 1.4885747292215796 entropy 0.9217694997787476
epoch: 29, step: 36
	action: tensor([[-0.7681, -0.1913,  0.4121, -0.8683,  1.0224,  0.7237,  0.1458]],
       dtype=torch.float64)
	q_value: tensor([[-28.8476]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8480155107522593, distance: 1.5556414245357715 entropy 0.9217694997787476
epoch: 29, step: 37
	action: tensor([[-0.0167,  0.3567,  0.2879, -0.9470, -0.3850, -0.2680, -0.0257]],
       dtype=torch.float64)
	q_value: tensor([[-31.9325]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.00292470033380976, distance: 1.1460164642250696 entropy 0.9217694997787476
epoch: 29, step: 38
	action: tensor([[-0.9955,  0.0656, -0.6936, -1.1131, -0.3348,  0.1392, -0.4660]],
       dtype=torch.float64)
	q_value: tensor([[-23.1027]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5465548119278008, distance: 1.4231128874404066 entropy 0.9217694997787476
epoch: 29, step: 39
	action: tensor([[ 0.1948, -0.3674, -0.6509,  0.4748, -0.3225,  0.9977,  0.0441]],
       dtype=torch.float64)
	q_value: tensor([[-29.3082]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2658958620801771, distance: 0.9804727530543859 entropy 0.9217694997787476
epoch: 29, step: 40
	action: tensor([[ 0.3109, -1.8297,  0.4825,  0.4870,  0.5974,  0.1999, -0.4059]],
       dtype=torch.float64)
	q_value: tensor([[-24.0307]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 29, step: 41
	action: tensor([[ 1.4860, -1.4663,  1.5877, -0.6228, -0.3667,  0.2934, -1.1062]],
       dtype=torch.float64)
	q_value: tensor([[-34.8824]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 29, step: 42
	action: tensor([[ 0.4305, -0.9813,  0.9170,  0.4748, -0.8566,  0.9241, -0.5701]],
       dtype=torch.float64)
	q_value: tensor([[-34.8824]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3330984339648412, distance: 0.9345177655251933 entropy 0.9217694997787476
epoch: 29, step: 43
	action: tensor([[ 0.5480, -0.0806,  0.3440,  0.2501,  0.1591,  0.8208,  0.4941]],
       dtype=torch.float64)
	q_value: tensor([[-36.6264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9284004616522007, distance: 0.3062046660430351 entropy 0.9217694997787476
epoch: 29, step: 44
	action: tensor([[-0.7776, -0.8420, -1.1389,  0.8688, -0.1512, -0.6290,  0.0865]],
       dtype=torch.float64)
	q_value: tensor([[-25.9696]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2369463184426057, distance: 1.711529836121536 entropy 0.9217694997787476
epoch: 29, step: 45
	action: tensor([[ 0.4160,  0.0797, -0.4756, -0.1786, -0.0591, -0.1119,  0.2055]],
       dtype=torch.float64)
	q_value: tensor([[-34.1409]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6047946984778334, distance: 0.7193960918926708 entropy 0.9217694997787476
epoch: 29, step: 46
	action: tensor([[ 0.9972, -0.0278,  0.1809, -0.0374,  0.9018,  0.6826,  0.0199]],
       dtype=torch.float64)
	q_value: tensor([[-20.8006]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7542319591602443, distance: 0.5673086389212295 entropy 0.9217694997787476
epoch: 29, step: 47
	action: tensor([[ 0.7020,  0.9612, -0.7608, -0.1623,  0.7129,  0.8924,  0.3872]],
       dtype=torch.float64)
	q_value: tensor([[-29.5370]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 29, step: 48
	action: tensor([[-1.1771, -0.0450, -0.2237, -0.9733, -0.4059, -0.4133,  0.0395]],
       dtype=torch.float64)
	q_value: tensor([[-34.8824]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9037467730684268, distance: 1.5789242079845005 entropy 0.9217694997787476
epoch: 29, step: 49
	action: tensor([[-0.5644, -0.3368,  0.8375, -0.0395,  0.1778,  0.0223, -0.5313]],
       dtype=torch.float64)
	q_value: tensor([[-28.7677]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6627481020021835, distance: 1.4756043391977025 entropy 0.9217694997787476
epoch: 29, step: 50
	action: tensor([[ 0.2851,  0.5418,  0.0602, -0.4875,  0.0455,  1.0575,  0.6021]],
       dtype=torch.float64)
	q_value: tensor([[-27.8212]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 29, step: 51
	action: tensor([[-0.0409, -1.4930,  0.3772, -0.6916, -0.4018,  0.4781, -0.3770]],
       dtype=torch.float64)
	q_value: tensor([[-34.8824]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0629691520290345, distance: 1.6436262180888643 entropy 0.9217694997787476
epoch: 29, step: 52
	action: tensor([[-0.0199,  0.2791, -1.1156,  0.3486,  0.6251, -0.0254,  1.1843]],
       dtype=torch.float64)
	q_value: tensor([[-31.3235]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 29, step: 53
	action: tensor([[-0.4661, -0.3836, -0.0621, -0.7578,  0.3189,  0.6046,  0.4302]],
       dtype=torch.float64)
	q_value: tensor([[-34.8824]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5729819530398901, distance: 1.4352202827100584 entropy 0.9217694997787476
epoch: 29, step: 54
	action: tensor([[ 1.1207,  0.6538, -0.3037, -0.2417,  0.3415,  0.5698,  0.4361]],
       dtype=torch.float64)
	q_value: tensor([[-26.8985]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9394651248091628, distance: 0.28155258492339047 entropy 0.9217694997787476
epoch: 29, step: 55
	action: tensor([[ 1.0543,  0.1183,  0.3190,  0.7577, -0.2785,  0.6033,  0.0465]],
       dtype=torch.float64)
	q_value: tensor([[-27.0664]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9365168231498776, distance: 0.28832746866351433 entropy 0.9217694997787476
epoch: 29, step: 56
	action: tensor([[-0.4124, -0.5329, -0.3775, -0.0740, -0.2144,  0.3958,  0.1242]],
       dtype=torch.float64)
	q_value: tensor([[-28.1325]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5500861815935425, distance: 1.4247367135449234 entropy 0.9217694997787476
epoch: 29, step: 57
	action: tensor([[ 0.2870,  0.3387, -0.4796, -0.8433, -0.5091,  0.9607,  0.6596]],
       dtype=torch.float64)
	q_value: tensor([[-22.5571]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.612455699758103, distance: 0.7123892712128642 entropy 0.9217694997787476
epoch: 29, step: 58
	action: tensor([[-0.4775, -0.7835,  1.4071,  0.1895,  0.6190, -0.4397, -0.1200]],
       dtype=torch.float64)
	q_value: tensor([[-27.8720]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7288756204935394, distance: 1.5046606671554195 entropy 0.9217694997787476
epoch: 29, step: 59
	action: tensor([[ 0.1411,  0.1371, -0.7819, -0.5090, -0.3084,  0.2816,  0.6066]],
       dtype=torch.float64)
	q_value: tensor([[-34.9387]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4881536294405413, distance: 0.8187032366413033 entropy 0.9217694997787476
epoch: 29, step: 60
	action: tensor([[ 1.6227, -0.0110,  0.1423,  0.2027,  0.2899,  0.4863,  0.1721]],
       dtype=torch.float64)
	q_value: tensor([[-23.4703]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5145459836506829, distance: 0.797316483247788 entropy 0.9217694997787476
epoch: 29, step: 61
	action: tensor([[ 0.5337, -0.2476,  0.9682, -0.4355,  0.9539,  0.0440,  1.1616]],
       dtype=torch.float64)
	q_value: tensor([[-30.9101]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23898718909306826, distance: 0.9982807052240201 entropy 0.9217694997787476
epoch: 29, step: 62
	action: tensor([[-0.0406,  0.2726,  0.3786, -0.2382,  0.5937,  0.7662, -0.7147]],
       dtype=torch.float64)
	q_value: tensor([[-33.2427]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5974060102102592, distance: 0.726089801579834 entropy 0.9217694997787476
epoch: 29, step: 63
	action: tensor([[ 0.3157,  0.8211, -0.4889, -0.4137,  0.4579,  0.0646, -0.0312]],
       dtype=torch.float64)
	q_value: tensor([[-27.0430]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 29, step: 64
	action: tensor([[ 0.2684, -1.0402,  0.2671, -0.5301,  0.5277, -0.5611, -0.2557]],
       dtype=torch.float64)
	q_value: tensor([[-34.8824]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9159204512428638, distance: 1.5839644489436877 entropy 0.9217694997787476
epoch: 29, step: 65
	action: tensor([[-0.6579, -0.5964, -1.2649, -0.3400, -0.0648,  1.0476, -0.0282]],
       dtype=torch.float64)
	q_value: tensor([[-32.4734]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8361295832787949, distance: 1.5506306256349134 entropy 0.9217694997787476
epoch: 29, step: 66
	action: tensor([[-0.8777,  0.3046,  0.7189, -0.5121, -0.2660,  0.4583,  1.4822]],
       dtype=torch.float64)
	q_value: tensor([[-30.4737]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7727155125834311, distance: 1.5236184247125122 entropy 0.9217694997787476
epoch: 29, step: 67
	action: tensor([[ 0.0464, -0.3151, -0.3220, -1.3797, -0.3927,  0.2537, -0.3446]],
       dtype=torch.float64)
	q_value: tensor([[-30.9294]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2837532951376638, distance: 1.296574508824403 entropy 0.9217694997787476
epoch: 29, step: 68
	action: tensor([[ 0.0721, -0.6979,  0.0265,  0.3199, -0.5966, -0.3145,  0.9564]],
       dtype=torch.float64)
	q_value: tensor([[-28.3393]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1038886986775498, distance: 1.2023179663417591 entropy 0.9217694997787476
epoch: 29, step: 69
	action: tensor([[-0.8248, -0.3100,  0.0656,  0.5383, -0.3074,  0.2276,  0.2829]],
       dtype=torch.float64)
	q_value: tensor([[-28.4459]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5389739487402723, distance: 1.4196207133756464 entropy 0.9217694997787476
epoch: 29, step: 70
	action: tensor([[ 0.9493, -0.2527, -0.3353, -0.2320, -0.2523,  1.3960, -0.2070]],
       dtype=torch.float64)
	q_value: tensor([[-24.3808]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7523655740649274, distance: 0.5694586617517137 entropy 0.9217694997787476
epoch: 29, step: 71
	action: tensor([[ 0.0858,  0.4494,  1.8422, -0.3842,  0.1110,  1.3922,  0.3584]],
       dtype=torch.float64)
	q_value: tensor([[-30.5430]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 29, step: 72
	action: tensor([[-0.9283, -0.3107, -0.8936,  0.0898, -0.7599,  0.3221, -0.3698]],
       dtype=torch.float64)
	q_value: tensor([[-34.8824]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.214738147127148, distance: 1.7030126986230785 entropy 0.9217694997787476
epoch: 29, step: 73
	action: tensor([[-0.6183,  0.1326,  0.8398, -0.2921,  0.5148,  1.3848, -0.4043]],
       dtype=torch.float64)
	q_value: tensor([[-27.3784]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06882125996509869, distance: 1.1042647761034186 entropy 0.9217694997787476
epoch: 29, step: 74
	action: tensor([[-0.3167,  0.1049, -0.1766, -0.0239,  0.2827,  0.1432, -0.7575]],
       dtype=torch.float64)
	q_value: tensor([[-32.4418]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06037012985352308, distance: 1.109264464192943 entropy 0.9217694997787476
epoch: 29, step: 75
	action: tensor([[ 0.6333, -0.1536, -0.1365, -0.4355,  0.3694,  0.0697,  0.0823]],
       dtype=torch.float64)
	q_value: tensor([[-23.6853]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.360682511428814, distance: 0.9149871304689148 entropy 0.9217694997787476
epoch: 29, step: 76
	action: tensor([[ 1.5220, -1.1047, -0.2802,  1.1234, -0.0476, -0.2787,  0.1753]],
       dtype=torch.float64)
	q_value: tensor([[-23.5657]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25646519013836744, distance: 0.9867504793880773 entropy 0.9217694997787476
epoch: 29, step: 77
	action: tensor([[ 1.9519, -0.2064, -0.5994,  0.4686,  1.5582,  0.5338, -0.3606]],
       dtype=torch.float64)
	q_value: tensor([[-41.2909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 29, step: 78
	action: tensor([[ 0.7363, -0.8000,  0.8900, -0.1956, -0.8719,  0.7438, -0.8347]],
       dtype=torch.float64)
	q_value: tensor([[-34.8824]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08897636668546616, distance: 1.0922486458549678 entropy 0.9217694997787476
epoch: 29, step: 79
	action: tensor([[ 0.1314,  0.4984, -0.1831, -0.7049, -0.0069,  1.4809, -0.6276]],
       dtype=torch.float64)
	q_value: tensor([[-34.1345]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 29, step: 80
	action: tensor([[-0.3500, -0.4676, -0.7268, -0.2201,  0.6661,  1.8661, -1.2517]],
       dtype=torch.float64)
	q_value: tensor([[-34.8824]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14209022808457394, distance: 1.0599307714492845 entropy 0.9217694997787476
epoch: 29, step: 81
	action: tensor([[-0.0811,  1.2818, -0.5074, -0.9519,  0.6826,  0.0396,  0.4593]],
       dtype=torch.float64)
	q_value: tensor([[-38.4399]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 29, step: 82
	action: tensor([[-1.0741, -0.0860,  1.8052, -0.4870,  0.5580,  0.5821,  0.4272]],
       dtype=torch.float64)
	q_value: tensor([[-34.8824]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2814096719264434, distance: 1.7284560157230338 entropy 0.9217694997787476
epoch: 29, step: 83
	action: tensor([[-0.4201, -0.4348,  0.4044,  0.0632, -0.2584,  0.0396, -0.2212]],
       dtype=torch.float64)
	q_value: tensor([[-34.4657]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.46869400262624716, distance: 1.3868272097625145 entropy 0.9217694997787476
epoch: 29, step: 84
	action: tensor([[ 0.5781, -0.4060,  0.8014, -0.1753, -0.0913,  0.1956,  0.5092]],
       dtype=torch.float64)
	q_value: tensor([[-23.6573]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3030007363090842, distance: 0.9553727569042012 entropy 0.9217694997787476
epoch: 29, step: 85
	action: tensor([[-0.7349,  0.2494, -0.4854, -0.6545,  0.4758, -0.6266, -0.0628]],
       dtype=torch.float64)
	q_value: tensor([[-25.8675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4624179044480685, distance: 1.3838609070206682 entropy 0.9217694997787476
epoch: 29, step: 86
	action: tensor([[ 0.0401,  0.8695,  0.1116, -0.9825, -0.3118,  0.2654, -1.0280]],
       dtype=torch.float64)
	q_value: tensor([[-26.3937]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 29, step: 87
	action: tensor([[ 0.9179, -0.4517,  0.1801, -0.0158, -0.8557,  0.4112,  0.5794]],
       dtype=torch.float64)
	q_value: tensor([[-34.8824]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42600722528063195, distance: 0.8669816510501893 entropy 0.9217694997787476
epoch: 29, step: 88
	action: tensor([[ 0.1053, -0.1635,  0.6451, -0.6425, -0.3421,  0.8804, -0.1469]],
       dtype=torch.float64)
	q_value: tensor([[-27.8380]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17362457005085286, distance: 1.040268364335131 entropy 0.9217694997787476
epoch: 29, step: 89
	action: tensor([[ 0.7283,  0.4059, -0.3333,  0.1058,  0.7748,  0.3402,  1.0030]],
       dtype=torch.float64)
	q_value: tensor([[-25.9932]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9837226821620205, distance: 0.14599840631919694 entropy 0.9217694997787476
epoch: 29, step: 90
	action: tensor([[ 0.2027,  0.3138, -0.1237, -0.4309,  0.6270,  0.2855, -0.7785]],
       dtype=torch.float64)
	q_value: tensor([[-29.7581]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6224284666288742, distance: 0.7031634934015486 entropy 0.9217694997787476
epoch: 29, step: 91
	action: tensor([[-0.6852, -0.4426, -0.1922, -0.8604,  1.2019,  0.1415, -1.1766]],
       dtype=torch.float64)
	q_value: tensor([[-25.4057]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7222234565680483, distance: 1.5017631492652772 entropy 0.9217694997787476
epoch: 29, step: 92
	action: tensor([[ 0.0980, -0.5532,  0.0783, -0.7854, -0.0342,  0.2756,  0.7512]],
       dtype=torch.float64)
	q_value: tensor([[-37.9394]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.46935193906007777, distance: 1.3871378061094095 entropy 0.9217694997787476
epoch: 29, step: 93
	action: tensor([[ 0.6269,  0.8277,  0.1047, -0.3269,  1.2447, -0.0116, -0.4282]],
       dtype=torch.float64)
	q_value: tensor([[-26.4454]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9627388181942481, distance: 0.22089455252423537 entropy 0.9217694997787476
epoch: 29, step: 94
	action: tensor([[-1.7413, -0.9058,  0.3687, -0.8278, -0.4855, -0.3561,  0.6888]],
       dtype=torch.float64)
	q_value: tensor([[-30.4046]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 29, step: 95
	action: tensor([[ 0.3017, -0.4412, -0.6703,  0.0347,  0.4861,  0.7387,  0.0214]],
       dtype=torch.float64)
	q_value: tensor([[-34.8824]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38550062854436284, distance: 0.8970515785577862 entropy 0.9217694997787476
epoch: 29, step: 96
	action: tensor([[-0.4040, -0.0553, -0.4488,  0.5389, -0.0501,  0.1345, -0.2030]],
       dtype=torch.float64)
	q_value: tensor([[-26.0982]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07226364240593908, distance: 1.184970349511048 entropy 0.9217694997787476
epoch: 29, step: 97
	action: tensor([[ 0.3800,  0.9869,  0.2528,  0.0437, -0.8311,  0.2461, -0.6903]],
       dtype=torch.float64)
	q_value: tensor([[-21.4310]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 29, step: 98
	action: tensor([[ 0.6272, -0.1832,  0.3804, -0.2811, -0.6987, -0.8769, -0.8213]],
       dtype=torch.float64)
	q_value: tensor([[-34.8824]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24672383108489004, distance: 0.9931933597348663 entropy 0.9217694997787476
epoch: 29, step: 99
	action: tensor([[ 0.4822, -1.2411,  0.6119,  0.2798,  0.4440, -0.1424, -0.3936]],
       dtype=torch.float64)
	q_value: tensor([[-31.5174]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.48993083885701405, distance: 1.396817758913732 entropy 0.9217694997787476
epoch: 29, step: 100
	action: tensor([[-0.4702, -1.0489,  0.2449,  0.9691,  0.3193,  0.8274,  0.2872]],
       dtype=torch.float64)
	q_value: tensor([[-34.7932]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16111074612084197, distance: 1.0481151748482802 entropy 0.9217694997787476
epoch: 29, step: 101
	action: tensor([[ 1.6410,  0.1737,  0.6165, -0.0916,  1.4039, -0.4714, -0.8257]],
       dtype=torch.float64)
	q_value: tensor([[-31.0375]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6656527252839766, distance: 0.6616915476498931 entropy 0.9217694997787476
epoch: 29, step: 102
	action: tensor([[ 0.3794, -0.5074, -0.1896,  0.6607,  0.6286,  0.9528,  0.3197]],
       dtype=torch.float64)
	q_value: tensor([[-42.0410]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7823444873474463, distance: 0.5338773902798056 entropy 0.9217694997787476
epoch: 29, step: 103
	action: tensor([[ 0.3989, -0.9066,  0.5061,  0.8146,  0.8670, -0.2105, -0.1043]],
       dtype=torch.float64)
	q_value: tensor([[-28.6243]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3499509181728715, distance: 0.922634665184553 entropy 0.9217694997787476
epoch: 29, step: 104
	action: tensor([[-0.2935, -0.3165,  0.3716,  1.2331, -0.2612,  1.3311, -0.0189]],
       dtype=torch.float64)
	q_value: tensor([[-35.1763]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 29, step: 105
	action: tensor([[-0.6795, -0.1570,  0.9039, -0.2846,  0.6479, -0.0525,  0.1568]],
       dtype=torch.float64)
	q_value: tensor([[-34.8824]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9104528604774593, distance: 1.581702701343226 entropy 0.9217694997787476
epoch: 29, step: 106
	action: tensor([[-0.8286, -0.5119,  0.6418, -0.3544, -0.2306,  0.5917, -0.0791]],
       dtype=torch.float64)
	q_value: tensor([[-26.9849]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.098644146805912, distance: 1.6577769447375517 entropy 0.9217694997787476
epoch: 29, step: 107
	action: tensor([[ 0.1565, -0.1148, -0.0773,  0.3483,  1.2973,  0.2106, -0.2367]],
       dtype=torch.float64)
	q_value: tensor([[-26.4762]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5848784349382397, distance: 0.7373001814581681 entropy 0.9217694997787476
epoch: 29, step: 108
	action: tensor([[ 0.4683,  0.7105, -1.2857, -0.2908, -0.6499,  0.5326, -0.1571]],
       dtype=torch.float64)
	q_value: tensor([[-30.9455]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 29, step: 109
	action: tensor([[-0.0998, -1.1913,  0.9447,  0.8659, -0.1367, -0.3861,  0.1686]],
       dtype=torch.float64)
	q_value: tensor([[-34.8824]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.19767944543157645, distance: 1.252353665952469 entropy 0.9217694997787476
epoch: 29, step: 110
	action: tensor([[ 0.3721, -0.8003, -0.2733, -0.4806, -1.1485,  1.4147,  0.8388]],
       dtype=torch.float64)
	q_value: tensor([[-37.0738]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2988734162040019, distance: 1.3041877222624172 entropy 0.9217694997787476
epoch: 29, step: 111
	action: tensor([[ 0.7168, -0.2603, -0.8727, -0.6907,  0.6537, -0.1899, -0.4487]],
       dtype=torch.float64)
	q_value: tensor([[-34.3780]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19423435049012305, distance: 1.0272143264297442 entropy 0.9217694997787476
epoch: 29, step: 112
	action: tensor([[-0.2364, -1.0163, -0.3808,  0.9214, -0.1358,  0.7282,  1.1102]],
       dtype=torch.float64)
	q_value: tensor([[-30.9063]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.22318157439361985, distance: 1.2656165883585735 entropy 0.9217694997787476
epoch: 29, step: 113
	action: tensor([[-0.3667,  0.2710,  0.1727, -0.8629,  0.1736, -0.6592, -0.1002]],
       dtype=torch.float64)
	q_value: tensor([[-31.1536]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4156746283867083, distance: 1.3615651210325264 entropy 0.9217694997787476
epoch: 29, step: 114
	action: tensor([[-0.2068,  0.0804,  1.2240, -0.6460,  0.3441, -0.1260, -0.7510]],
       dtype=torch.float64)
	q_value: tensor([[-24.8620]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.404781350739869, distance: 1.3563165459369944 entropy 0.9217694997787476
epoch: 29, step: 115
	action: tensor([[ 0.7386, -0.2195,  0.2556,  0.0379,  0.2908,  0.2013,  0.2256]],
       dtype=torch.float64)
	q_value: tensor([[-30.1761]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.632864255984587, distance: 0.6933779519247046 entropy 0.9217694997787476
epoch: 29, step: 116
	action: tensor([[ 0.6361, -0.0538, -0.8584, -0.1948,  0.0159,  0.3717, -0.7551]],
       dtype=torch.float64)
	q_value: tensor([[-24.3338]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6749112622742748, distance: 0.6524656542314667 entropy 0.9217694997787476
epoch: 29, step: 117
	action: tensor([[-0.2397, -0.4771,  0.0801, -0.2434,  0.6518, -0.0674,  0.0557]],
       dtype=torch.float64)
	q_value: tensor([[-25.1852]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5377731108692616, distance: 1.4190667511104536 entropy 0.9217694997787476
epoch: 29, step: 118
	action: tensor([[ 0.2284, -1.1088,  0.2137,  0.2438, -0.4291, -0.5436, -0.5942]],
       dtype=torch.float64)
	q_value: tensor([[-25.3276]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5657405936567963, distance: 1.4319128910834713 entropy 0.9217694997787476
epoch: 29, step: 119
	action: tensor([[-0.2026, -0.6711,  0.5565, -1.2078,  0.0210,  0.8775,  0.5148]],
       dtype=torch.float64)
	q_value: tensor([[-33.5944]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.821133150076871, distance: 1.5442853205897056 entropy 0.9217694997787476
epoch: 29, step: 120
	action: tensor([[-0.4058, -0.1677, -0.2398, -0.1423, -0.2968,  0.3203, -0.4024]],
       dtype=torch.float64)
	q_value: tensor([[-31.4899]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3242917170882249, distance: 1.3168870430462893 entropy 0.9217694997787476
epoch: 29, step: 121
	action: tensor([[ 0.5510, -0.3738,  1.1940,  0.1035,  0.4286,  1.0503, -0.4862]],
       dtype=torch.float64)
	q_value: tensor([[-21.5662]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5346679257958963, distance: 0.7806173280727501 entropy 0.9217694997787476
epoch: 29, step: 122
	action: tensor([[ 0.2382,  0.5751,  0.0830,  1.2821,  0.7589, -0.5091, -0.6413]],
       dtype=torch.float64)
	q_value: tensor([[-33.0252]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 29, step: 123
	action: tensor([[-0.2626,  0.1461, -0.9315,  0.1159, -0.6412,  0.4504,  0.6209]],
       dtype=torch.float64)
	q_value: tensor([[-34.8824]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04084009320580084, distance: 1.167477984611589 entropy 0.9217694997787476
epoch: 29, step: 124
	action: tensor([[ 0.2190,  0.7281, -0.6593, -0.1282,  0.3786,  1.1485, -0.2531]],
       dtype=torch.float64)
	q_value: tensor([[-23.7660]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 29, step: 125
	action: tensor([[-1.1873,  0.5261, -0.4187, -1.0145,  0.7566,  0.5718,  1.5293]],
       dtype=torch.float64)
	q_value: tensor([[-34.8824]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.588653754383857, distance: 1.4423521956536975 entropy 0.9217694997787476
epoch: 29, step: 126
	action: tensor([[-0.1145, -0.2205,  0.2614, -0.7668,  0.7178,  0.4751,  0.0358]],
       dtype=torch.float64)
	q_value: tensor([[-37.2281]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.26323012277690494, distance: 1.2861686805613388 entropy 0.9217694997787476
epoch: 29, step: 127
	action: tensor([[ 0.9808, -0.0862, -0.7367, -1.5038,  0.0958, -0.5588, -0.0772]],
       dtype=torch.float64)
	q_value: tensor([[-26.5675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09967275502042994, distance: 1.2000198388981715 entropy 0.9217694997787476
LOSS epoch 29 actor 382.8610840937337 critic 245.40949511040532 
epoch: 30, step: 0
	action: tensor([[-0.1434, -0.5460,  1.3034, -0.4985,  0.2659,  0.8943,  0.2748]],
       dtype=torch.float64)
	q_value: tensor([[-32.2041]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3408180898130426, distance: 1.3250785494021509 entropy 0.8164090514183044
epoch: 30, step: 1
	action: tensor([[ 0.5442, -0.6556, -0.0269,  1.0034,  0.0147,  0.3177, -0.4875]],
       dtype=torch.float64)
	q_value: tensor([[-29.2822]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8143933079831465, distance: 0.49300748004038314 entropy 0.8164090514183044
epoch: 30, step: 2
	action: tensor([[ 0.1099, -1.1980, -0.3880,  1.2624, -0.1186,  0.2011,  0.6905]],
       dtype=torch.float64)
	q_value: tensor([[-30.8756]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08737690634780937, distance: 1.0932070415552602 entropy 0.8164090514183044
epoch: 30, step: 3
	action: tensor([[ 0.0066, -0.4137,  0.5590, -0.9136, -0.5699,  1.1307,  0.0262]],
       dtype=torch.float64)
	q_value: tensor([[-33.8082]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23910308079494702, distance: 1.2738268877055525 entropy 0.8164090514183044
epoch: 30, step: 4
	action: tensor([[-0.5489, -0.1746,  0.0521,  1.1796, -0.2587,  0.0638,  0.1563]],
       dtype=torch.float64)
	q_value: tensor([[-28.8570]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2541931786818832, distance: 0.9882569309569028 entropy 0.8164090514183044
epoch: 30, step: 5
	action: tensor([[ 0.5667, -0.1108, -0.1189, -0.6375, -0.3584,  0.2352,  0.6697]],
       dtype=torch.float64)
	q_value: tensor([[-27.5854]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 30, step: 6
	action: tensor([[-0.3607,  0.2434,  0.8547, -0.4001, -0.1661,  0.3664, -0.1016]],
       dtype=torch.float64)
	q_value: tensor([[-33.9535]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11996505363364096, distance: 1.2110412301671956 entropy 0.8164090514183044
epoch: 30, step: 7
	action: tensor([[1.1306, 0.5536, 1.0640, 0.5836, 0.4845, 2.0661, 0.0023]],
       dtype=torch.float64)
	q_value: tensor([[-23.6448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 30, step: 8
	action: tensor([[-0.1088, -0.2317,  0.6612, -0.4842,  0.2417,  0.0286,  0.3119]],
       dtype=torch.float64)
	q_value: tensor([[-33.9535]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.36623541095483314, distance: 1.3375790624605601 entropy 0.8164090514183044
epoch: 30, step: 9
	action: tensor([[ 0.9840,  0.7638, -0.3788,  0.5137,  0.7909,  0.6355,  0.8589]],
       dtype=torch.float64)
	q_value: tensor([[-22.4682]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 30, step: 10
	action: tensor([[ 0.1601, -0.3886,  0.1829, -0.2686,  0.0795,  0.3621,  0.1628]],
       dtype=torch.float64)
	q_value: tensor([[-33.9535]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04940448414556209, distance: 1.1157183449452932 entropy 0.8164090514183044
epoch: 30, step: 11
	action: tensor([[-0.4718, -0.5549,  0.7083, -0.0635,  0.3061, -0.0825,  0.0240]],
       dtype=torch.float64)
	q_value: tensor([[-21.3115]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7564839771290104, distance: 1.5166270215828563 entropy 0.8164090514183044
epoch: 30, step: 12
	action: tensor([[ 0.2833,  0.3451,  0.1416, -1.1964,  0.8246, -0.2668, -0.4679]],
       dtype=torch.float64)
	q_value: tensor([[-25.1613]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16205343912312542, distance: 1.0475261050930285 entropy 0.8164090514183044
epoch: 30, step: 13
	action: tensor([[ 0.3255, -0.8397,  1.0814,  1.0121, -0.6029,  0.2474,  1.6603]],
       dtype=torch.float64)
	q_value: tensor([[-29.6573]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5172994758303988, distance: 0.7950520806803756 entropy 0.8164090514183044
epoch: 30, step: 14
	action: tensor([[-0.1090, -0.8381,  0.2869, -0.4346,  1.0904,  0.7011,  0.5216]],
       dtype=torch.float64)
	q_value: tensor([[-37.7641]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.47404345608295095, distance: 1.3893505483515092 entropy 0.8164090514183044
epoch: 30, step: 15
	action: tensor([[ 0.2559, -0.0471,  0.6097, -0.0658, -0.3182,  0.8086, -0.4164]],
       dtype=torch.float64)
	q_value: tensor([[-31.4087]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7026740263615956, distance: 0.6239835177408795 entropy 0.8164090514183044
epoch: 30, step: 16
	action: tensor([[-0.4345, -0.1732,  0.1219,  0.8975,  0.6912,  0.1886, -0.1490]],
       dtype=torch.float64)
	q_value: tensor([[-24.5250]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 30, step: 17
	action: tensor([[ 0.0425,  0.6096,  0.4848,  0.0529, -0.2090, -0.3965, -0.0190]],
       dtype=torch.float64)
	q_value: tensor([[-33.9535]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 30, step: 18
	action: tensor([[ 0.5256, -0.2056, -0.4307,  0.0558, -0.2914, -0.1942, -0.4777]],
       dtype=torch.float64)
	q_value: tensor([[-33.9535]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5300232623916634, distance: 0.7845034799327882 entropy 0.8164090514183044
epoch: 30, step: 19
	action: tensor([[-0.8022, -0.6638,  0.7624,  0.4334,  0.6114, -1.0581,  0.5936]],
       dtype=torch.float64)
	q_value: tensor([[-22.8937]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0000753641057627, distance: 1.6183776550649909 entropy 0.8164090514183044
epoch: 30, step: 20
	action: tensor([[-0.3911, -0.1749, -0.2819,  0.2226, -0.1263,  0.4966,  1.3863]],
       dtype=torch.float64)
	q_value: tensor([[-33.9295]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15215455527296395, distance: 1.2283215290629512 entropy 0.8164090514183044
epoch: 30, step: 21
	action: tensor([[-0.5848,  0.3101,  0.4461, -0.1404,  0.3036,  0.5159,  0.9489]],
       dtype=torch.float64)
	q_value: tensor([[-27.6365]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04005243313276963, distance: 1.1670361540942567 entropy 0.8164090514183044
epoch: 30, step: 22
	action: tensor([[ 0.0948,  0.0389, -0.5243, -0.1782,  0.9844,  0.2180, -0.4764]],
       dtype=torch.float64)
	q_value: tensor([[-26.0142]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4395519634015417, distance: 0.8566913252201362 entropy 0.8164090514183044
epoch: 30, step: 23
	action: tensor([[ 1.2237, -0.0628, -0.3908, -0.7929,  0.1446,  0.5703, -0.2245]],
       dtype=torch.float64)
	q_value: tensor([[-27.4753]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2985539427226894, distance: 0.9584155082967908 entropy 0.8164090514183044
epoch: 30, step: 24
	action: tensor([[-0.4103, -0.1466,  0.0153, -0.6007, -0.2588,  0.6161, -0.0961]],
       dtype=torch.float64)
	q_value: tensor([[-28.1512]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.40553234242399183, distance: 1.3566790387726713 entropy 0.8164090514183044
epoch: 30, step: 25
	action: tensor([[0.0748, 0.0358, 0.3491, 0.3339, 0.5419, 0.4176, 0.2843]],
       dtype=torch.float64)
	q_value: tensor([[-22.8292]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 30, step: 26
	action: tensor([[ 0.8144, -0.1903, -1.1796,  0.6545, -0.0208,  0.0730, -0.2519]],
       dtype=torch.float64)
	q_value: tensor([[-33.9535]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6965261795530651, distance: 0.6304016037040154 entropy 0.8164090514183044
epoch: 30, step: 27
	action: tensor([[-0.4270, -0.2376,  0.0999, -0.4274, -1.1151,  0.2409,  0.6039]],
       dtype=torch.float64)
	q_value: tensor([[-26.7244]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5703673986009483, distance: 1.434027000691493 entropy 0.8164090514183044
epoch: 30, step: 28
	action: tensor([[-0.0240, -1.4957, -0.5037,  0.2932,  0.1995,  0.0022, -0.1507]],
       dtype=torch.float64)
	q_value: tensor([[-26.0743]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 30, step: 29
	action: tensor([[ 0.6145,  0.6245,  0.4032, -0.3275,  0.7803,  0.3549, -0.1399]],
       dtype=torch.float64)
	q_value: tensor([[-33.9535]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 30, step: 30
	action: tensor([[ 0.0727, -0.8505,  0.3802, -0.2824, -0.1766,  0.4618, -0.7410]],
       dtype=torch.float64)
	q_value: tensor([[-33.9535]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4447012939479493, distance: 1.3754529026385707 entropy 0.8164090514183044
epoch: 30, step: 31
	action: tensor([[ 0.1774,  0.3153,  0.3563, -0.5777,  0.0687,  0.5949,  0.3077]],
       dtype=torch.float64)
	q_value: tensor([[-28.0344]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5402550494120083, distance: 0.7759168394209022 entropy 0.8164090514183044
epoch: 30, step: 32
	action: tensor([[ 0.3025, -0.8035,  0.3440,  0.8048,  0.0314,  0.2898,  0.6505]],
       dtype=torch.float64)
	q_value: tensor([[-23.5187]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5573283833938637, distance: 0.7613731049795125 entropy 0.8164090514183044
epoch: 30, step: 33
	action: tensor([[-0.7203,  0.3782,  0.6026, -0.0083, -0.0959,  0.2664,  1.2190]],
       dtype=torch.float64)
	q_value: tensor([[-29.0156]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.19771851248446404, distance: 1.2523740910203442 entropy 0.8164090514183044
epoch: 30, step: 34
	action: tensor([[ 0.8021, -0.0402,  0.9039,  0.1087,  0.4867,  0.9318, -0.8497]],
       dtype=torch.float64)
	q_value: tensor([[-26.9866]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 30, step: 35
	action: tensor([[-0.5770, -0.1933, -0.0301, -0.1072, -0.0625,  0.4979, -0.2173]],
       dtype=torch.float64)
	q_value: tensor([[-33.9535]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4514854037005782, distance: 1.378678584865779 entropy 0.8164090514183044
epoch: 30, step: 36
	action: tensor([[-0.4237,  0.1452,  0.7727,  0.1566,  0.5126,  0.1568, -0.1988]],
       dtype=torch.float64)
	q_value: tensor([[-21.6264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10931127510779848, distance: 1.0799898418033294 entropy 0.8164090514183044
epoch: 30, step: 37
	action: tensor([[ 1.1339,  0.3220,  0.1333,  1.2582,  0.5779,  0.4807, -0.1940]],
       dtype=torch.float64)
	q_value: tensor([[-25.6114]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 30, step: 38
	action: tensor([[ 0.1490,  0.2304, -0.4767,  0.0434,  0.8471, -0.9591,  0.4663]],
       dtype=torch.float64)
	q_value: tensor([[-33.9535]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 30, step: 39
	action: tensor([[ 0.7662,  0.3919, -0.3819,  0.0138,  0.4002,  0.2772,  0.0438]],
       dtype=torch.float64)
	q_value: tensor([[-33.9535]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 30, step: 40
	action: tensor([[-0.4777,  0.3901,  0.2222,  0.0650,  0.5676, -0.3781, -0.3654]],
       dtype=torch.float64)
	q_value: tensor([[-33.9535]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1057282374464581, distance: 1.2033193308241017 entropy 0.8164090514183044
epoch: 30, step: 41
	action: tensor([[-0.2757, -0.3520, -0.0338,  0.1054,  0.2671,  0.5151, -0.7740]],
       dtype=torch.float64)
	q_value: tensor([[-25.3544]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1022967395820833, distance: 1.201450699741423 entropy 0.8164090514183044
epoch: 30, step: 42
	action: tensor([[ 0.7048,  0.0559, -0.1169, -0.8009,  0.1950, -0.3209, -0.3735]],
       dtype=torch.float64)
	q_value: tensor([[-25.3465]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27536332840987976, distance: 0.9741298410714477 entropy 0.8164090514183044
epoch: 30, step: 43
	action: tensor([[-0.2295, -0.6044, -0.3334,  0.2150, -0.0688,  0.0156, -0.3621]],
       dtype=torch.float64)
	q_value: tensor([[-25.0826]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.374577277654772, distance: 1.3416562976695945 entropy 0.8164090514183044
epoch: 30, step: 44
	action: tensor([[ 0.1532,  0.2586,  0.0756,  0.3462, -0.1201, -0.7168,  0.6567]],
       dtype=torch.float64)
	q_value: tensor([[-23.8757]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 30, step: 45
	action: tensor([[-0.1689,  0.1443, -0.2539,  0.0864, -0.2816,  1.0702, -0.1966]],
       dtype=torch.float64)
	q_value: tensor([[-33.9535]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 30, step: 46
	action: tensor([[-0.3037, -0.1949, -0.1661,  0.0347,  0.6310,  0.3627,  0.6355]],
       dtype=torch.float64)
	q_value: tensor([[-33.9535]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.028155424085916314, distance: 1.1603421774651526 entropy 0.8164090514183044
epoch: 30, step: 47
	action: tensor([[ 0.5664,  0.3353,  0.6093, -0.3803, -0.8203, -0.9354, -0.0605]],
       dtype=torch.float64)
	q_value: tensor([[-24.4105]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6193650793123369, distance: 0.706010252733971 entropy 0.8164090514183044
epoch: 30, step: 48
	action: tensor([[ 0.6271, -1.2409,  0.7969, -1.0565,  0.0418,  0.3299,  0.3808]],
       dtype=torch.float64)
	q_value: tensor([[-29.8411]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9393899570337252, distance: 1.5936364865488588 entropy 0.8164090514183044
epoch: 30, step: 49
	action: tensor([[1.6564, 0.3641, 0.6255, 0.3827, 0.8844, 0.8446, 0.7231]],
       dtype=torch.float64)
	q_value: tensor([[-31.5505]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3465764882929253, distance: 0.9250262815422116 entropy 0.8164090514183044
epoch: 30, step: 50
	action: tensor([[ 0.9352, -0.5817, -0.1746,  0.2276, -0.0085, -0.1218,  0.2812]],
       dtype=torch.float64)
	q_value: tensor([[-37.1340]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28805326492168526, distance: 0.9655626203114859 entropy 0.8164090514183044
epoch: 30, step: 51
	action: tensor([[ 0.0346, -0.8568,  0.8817,  0.6051, -0.4547,  0.3707, -0.1800]],
       dtype=torch.float64)
	q_value: tensor([[-26.7955]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26939926176988815, distance: 0.9781303769471297 entropy 0.8164090514183044
epoch: 30, step: 52
	action: tensor([[ 0.4837,  0.0925,  0.3689, -0.1167, -1.5092,  0.7408,  0.0456]],
       dtype=torch.float64)
	q_value: tensor([[-31.3076]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6956786095509673, distance: 0.6312813121093399 entropy 0.8164090514183044
epoch: 30, step: 53
	action: tensor([[ 0.1355, -0.5087,  0.4162, -0.8163,  0.8523,  0.3793,  0.3905]],
       dtype=torch.float64)
	q_value: tensor([[-29.9653]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4079068329460691, distance: 1.3578245343303335 entropy 0.8164090514183044
epoch: 30, step: 54
	action: tensor([[-0.7196, -1.1002,  0.8028,  0.1075, -0.7356,  0.6986, -0.8947]],
       dtype=torch.float64)
	q_value: tensor([[-28.2265]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.809225714871384, distance: 1.5392284047300226 entropy 0.8164090514183044
epoch: 30, step: 55
	action: tensor([[ 0.2939, -0.9481,  0.1986,  0.1496,  0.3231,  0.6944,  0.4176]],
       dtype=torch.float64)
	q_value: tensor([[-35.9208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07940446622258213, distance: 1.0979716460674875 entropy 0.8164090514183044
epoch: 30, step: 56
	action: tensor([[-0.0277,  0.5896,  0.0676,  0.2303,  0.7441,  0.8634,  0.6346]],
       dtype=torch.float64)
	q_value: tensor([[-27.3464]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 30, step: 57
	action: tensor([[-0.3675, -0.1048,  0.2826,  0.0151, -0.1551,  0.2471,  0.7175]],
       dtype=torch.float64)
	q_value: tensor([[-33.9535]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11763594762271179, distance: 1.2097813196924638 entropy 0.8164090514183044
epoch: 30, step: 58
	action: tensor([[-0.5270, -0.0256,  0.5175, -0.0978,  0.0618,  0.7830,  0.7107]],
       dtype=torch.float64)
	q_value: tensor([[-22.0231]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0504421621984783, distance: 1.172850792793267 entropy 0.8164090514183044
epoch: 30, step: 59
	action: tensor([[ 0.4848,  0.1266,  0.6396, -0.3877,  0.3184,  0.6372, -0.0179]],
       dtype=torch.float64)
	q_value: tensor([[-25.2993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7231364053069452, distance: 0.6021291047263655 entropy 0.8164090514183044
epoch: 30, step: 60
	action: tensor([[ 0.2636, -0.3667,  0.2809,  0.3415, -0.2298,  0.6654,  0.8417]],
       dtype=torch.float64)
	q_value: tensor([[-24.0709]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6439703687447171, distance: 0.6828098311720033 entropy 0.8164090514183044
epoch: 30, step: 61
	action: tensor([[-0.4432, -0.0081,  0.5082,  0.8020,  0.1949,  0.9306,  0.1410]],
       dtype=torch.float64)
	q_value: tensor([[-25.0312]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 30, step: 62
	action: tensor([[ 0.3029,  0.2552,  0.5302, -0.4276,  0.0320,  0.3473, -0.5844]],
       dtype=torch.float64)
	q_value: tensor([[-33.9535]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5734423034776995, distance: 0.7473870774783881 entropy 0.8164090514183044
epoch: 30, step: 63
	action: tensor([[ 0.4822,  0.4390,  0.3116, -0.0371,  0.0642,  0.1804, -0.2532]],
       dtype=torch.float64)
	q_value: tensor([[-23.3182]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 30, step: 64
	action: tensor([[-0.0241, -0.3355, -0.0629,  0.4464, -0.4388,  0.8686,  0.5786]],
       dtype=torch.float64)
	q_value: tensor([[-33.9535]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3501354376211335, distance: 0.9225037088252701 entropy 0.8164090514183044
epoch: 30, step: 65
	action: tensor([[-0.7678,  1.0713,  0.6096,  0.0594, -0.0854,  0.3642, -0.2387]],
       dtype=torch.float64)
	q_value: tensor([[-23.6458]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 30, step: 66
	action: tensor([[-0.5991,  0.9537,  0.6511, -0.7072,  0.1829,  1.0266, -0.5538]],
       dtype=torch.float64)
	q_value: tensor([[-33.9535]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12680045041837784, distance: 1.069334170211725 entropy 0.8164090514183044
epoch: 30, step: 67
	action: tensor([[ 0.2557,  0.7374, -0.2401, -0.0021, -0.2734,  1.1738,  0.8024]],
       dtype=torch.float64)
	q_value: tensor([[-30.8209]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 30, step: 68
	action: tensor([[-0.1314, -0.0267, -0.0922, -0.5705, -1.1283, -0.0240, -0.8098]],
       dtype=torch.float64)
	q_value: tensor([[-33.9535]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08888524793485719, distance: 1.1941193896690796 entropy 0.8164090514183044
epoch: 30, step: 69
	action: tensor([[-0.3843, -0.9578, -0.1323, -0.1183, -0.4700,  1.3562,  0.5419]],
       dtype=torch.float64)
	q_value: tensor([[-27.2544]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.585754159834444, distance: 1.4410353112663137 entropy 0.8164090514183044
epoch: 30, step: 70
	action: tensor([[ 0.3087, -0.0145,  0.7742, -0.1220,  0.5952,  0.3196, -0.2927]],
       dtype=torch.float64)
	q_value: tensor([[-29.9428]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5574269654438444, distance: 0.7612883221541197 entropy 0.8164090514183044
epoch: 30, step: 71
	action: tensor([[ 0.0963,  0.0785, -0.5757, -0.5173,  0.7345, -0.3095,  0.6993]],
       dtype=torch.float64)
	q_value: tensor([[-26.0616]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29809925204049836, distance: 0.9587260895588888 entropy 0.8164090514183044
epoch: 30, step: 72
	action: tensor([[ 0.2765, -0.7608,  0.3834,  0.4451,  0.1766, -0.3724,  0.6483]],
       dtype=torch.float64)
	q_value: tensor([[-26.3653]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.036088152603576895, distance: 1.1235058869240682 entropy 0.8164090514183044
epoch: 30, step: 73
	action: tensor([[-0.2717, -0.2112, -0.5895, -1.0479, -0.4232, -0.0629, -1.2840]],
       dtype=torch.float64)
	q_value: tensor([[-28.6946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08431675345191225, distance: 1.1916117508054074 entropy 0.8164090514183044
epoch: 30, step: 74
	action: tensor([[ 0.0150,  1.0326, -0.2989,  0.2427,  1.3195,  0.4092, -1.0267]],
       dtype=torch.float64)
	q_value: tensor([[-31.2955]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 30, step: 75
	action: tensor([[ 0.2194, -0.4762,  0.4924,  0.4206, -0.2223, -0.3965,  0.6351]],
       dtype=torch.float64)
	q_value: tensor([[-33.9535]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2609410394329432, distance: 0.983776030162848 entropy 0.8164090514183044
epoch: 30, step: 76
	action: tensor([[ 0.6178, -0.1901,  0.0500, -0.0065,  0.3869, -0.3087,  0.2285]],
       dtype=torch.float64)
	q_value: tensor([[-26.9886]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.46721182250208326, distance: 0.8352836546314253 entropy 0.8164090514183044
epoch: 30, step: 77
	action: tensor([[-0.2646, -1.2478, -0.2042,  0.4857, -0.3828, -0.1009,  0.2821]],
       dtype=torch.float64)
	q_value: tensor([[-24.1184]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.738261953490988, distance: 1.5087396559623842 entropy 0.8164090514183044
epoch: 30, step: 78
	action: tensor([[ 9.9418e-01, -3.5700e-01, -7.9900e-01, -3.0161e-01,  6.7802e-01,
          7.8175e-01, -8.8723e-04]], dtype=torch.float64)
	q_value: tensor([[-29.7873]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.517861161445436, distance: 0.7945893721470872 entropy 0.8164090514183044
epoch: 30, step: 79
	action: tensor([[-0.3023, -0.1815,  0.1959, -0.6830,  0.1640,  0.0745,  0.2501]],
       dtype=torch.float64)
	q_value: tensor([[-30.4776]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.48751334798607915, distance: 1.395684093859894 entropy 0.8164090514183044
epoch: 30, step: 80
	action: tensor([[-0.1194, -1.0942,  0.1248, -0.2780,  0.6765,  1.1713,  1.3673]],
       dtype=torch.float64)
	q_value: tensor([[-22.4627]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3085518049172433, distance: 1.3090376980787888 entropy 0.8164090514183044
epoch: 30, step: 81
	action: tensor([[ 0.1315, -0.3885,  0.1332, -0.1444,  0.9758, -0.0705, -0.2537]],
       dtype=torch.float64)
	q_value: tensor([[-34.7457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.024543954546346836, distance: 1.1583024922669678 entropy 0.8164090514183044
epoch: 30, step: 82
	action: tensor([[ 0.9394,  1.0252, -0.3089, -0.7638,  0.0514, -0.7821,  0.8894]],
       dtype=torch.float64)
	q_value: tensor([[-28.2060]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9117402205906255, distance: 0.339968056173359 entropy 0.8164090514183044
epoch: 30, step: 83
	action: tensor([[ 1.3945, -0.0187, -0.2313,  0.9789, -0.5381,  0.0434,  0.6277]],
       dtype=torch.float64)
	q_value: tensor([[-30.5511]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9035821756313674, distance: 0.355332848218026 entropy 0.8164090514183044
epoch: 30, step: 84
	action: tensor([[-0.0460, -0.0558, -0.0991, -0.3912, -0.0511,  0.3163,  0.2788]],
       dtype=torch.float64)
	q_value: tensor([[-32.4918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14173843525489116, distance: 1.0601480657616866 entropy 0.8164090514183044
epoch: 30, step: 85
	action: tensor([[ 0.4983, -0.7307, -0.0333, -0.3119, -0.2686, -0.8041, -1.2248]],
       dtype=torch.float64)
	q_value: tensor([[-20.5594]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.38634381568909726, distance: 1.3473864265097104 entropy 0.8164090514183044
epoch: 30, step: 86
	action: tensor([[ 0.4796, -0.2834,  0.2755,  0.4505,  0.0967, -0.3002,  1.0667]],
       dtype=torch.float64)
	q_value: tensor([[-35.3742]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5981955714247469, distance: 0.7253774540049395 entropy 0.8164090514183044
epoch: 30, step: 87
	action: tensor([[-0.2510, -0.0865,  0.1715,  0.4636,  0.2809,  0.4485,  0.9701]],
       dtype=torch.float64)
	q_value: tensor([[-28.2891]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 30, step: 88
	action: tensor([[-0.0048,  0.2977,  0.0475,  0.8536, -0.2073,  0.3879,  0.0772]],
       dtype=torch.float64)
	q_value: tensor([[-33.9535]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 30, step: 89
	action: tensor([[-0.0048,  0.2447, -0.0226,  0.6725, -0.2551,  0.6072, -0.6080]],
       dtype=torch.float64)
	q_value: tensor([[-33.9535]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 30, step: 90
	action: tensor([[ 0.5490, -0.9694, -0.1354,  0.5932, -0.3204,  0.1789,  0.2974]],
       dtype=torch.float64)
	q_value: tensor([[-33.9535]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17415207317459624, distance: 1.0399362922584827 entropy 0.8164090514183044
epoch: 30, step: 91
	action: tensor([[ 0.6951, -0.4916, -0.2340, -0.2697,  0.8488,  0.2829, -0.0239]],
       dtype=torch.float64)
	q_value: tensor([[-28.6664]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1821775396982045, distance: 1.0348709836327286 entropy 0.8164090514183044
epoch: 30, step: 92
	action: tensor([[-0.1299, -0.2563, -1.3969, -1.0820, -0.8379,  0.2623,  0.5406]],
       dtype=torch.float64)
	q_value: tensor([[-28.4578]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39660462698064025, distance: 0.8889097730817251 entropy 0.8164090514183044
epoch: 30, step: 93
	action: tensor([[-0.6740,  0.4403, -0.1105, -0.6765,  0.5072,  0.2688, -0.9606]],
       dtype=torch.float64)
	q_value: tensor([[-30.2951]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3029161331397898, distance: 1.3062157742232139 entropy 0.8164090514183044
epoch: 30, step: 94
	action: tensor([[-1.3101, -0.7757, -0.5057, -0.5419, -0.3176,  1.0186,  0.5891]],
       dtype=torch.float64)
	q_value: tensor([[-27.7650]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.476493717073137, distance: 1.8008407461185685 entropy 0.8164090514183044
epoch: 30, step: 95
	action: tensor([[ 1.0376,  0.1602,  0.0915, -0.0769,  0.4960,  0.2825, -0.1243]],
       dtype=torch.float64)
	q_value: tensor([[-32.0240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8452025954023952, distance: 0.45023425137618145 entropy 0.8164090514183044
epoch: 30, step: 96
	action: tensor([[ 1.1029, -0.2705, -0.0943,  0.0857, -1.2820, -0.1547, -0.3881]],
       dtype=torch.float64)
	q_value: tensor([[-25.5836]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5103430974362707, distance: 0.8007604847056247 entropy 0.8164090514183044
epoch: 30, step: 97
	action: tensor([[ 0.6967, -0.1185, -0.0311,  0.6137,  0.1376, -0.9212,  0.9537]],
       dtype=torch.float64)
	q_value: tensor([[-30.7131]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7301259209010634, distance: 0.5944800433451646 entropy 0.8164090514183044
epoch: 30, step: 98
	action: tensor([[ 0.4867,  0.8271, -0.0605, -0.0596,  1.1174,  0.4483, -0.9226]],
       dtype=torch.float64)
	q_value: tensor([[-32.0946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 30, step: 99
	action: tensor([[ 0.1303, -0.6348,  0.1627, -0.1447, -0.7817, -0.1224, -0.0892]],
       dtype=torch.float64)
	q_value: tensor([[-33.9535]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2545369191116953, distance: 1.2817355101601098 entropy 0.8164090514183044
epoch: 30, step: 100
	action: tensor([[ 0.7554,  1.0276, -0.9992, -0.7505, -0.3054,  0.3547,  1.3871]],
       dtype=torch.float64)
	q_value: tensor([[-25.1536]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9584772289533602, distance: 0.2331845958696402 entropy 0.8164090514183044
epoch: 30, step: 101
	action: tensor([[-0.9504,  0.1676, -0.0798,  0.0886,  0.3113, -0.1997,  0.0141]],
       dtype=torch.float64)
	q_value: tensor([[-32.6850]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 30, step: 102
	action: tensor([[-0.2088,  0.1862,  0.1122, -0.2777, -1.0207, -0.0881, -0.5244]],
       dtype=torch.float64)
	q_value: tensor([[-33.9535]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09220163071483511, distance: 1.0903135074168637 entropy 0.8164090514183044
epoch: 30, step: 103
	action: tensor([[ 0.5828, -0.6120,  0.2209, -0.2418,  0.2642,  0.0266,  0.5032]],
       dtype=torch.float64)
	q_value: tensor([[-24.9800]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07552488053979811, distance: 1.1867709964781228 entropy 0.8164090514183044
epoch: 30, step: 104
	action: tensor([[ 0.1808,  0.1782, -0.3279, -0.2521,  1.5862, -0.5032, -0.3919]],
       dtype=torch.float64)
	q_value: tensor([[-25.1549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3925541347917032, distance: 0.8918883340915227 entropy 0.8164090514183044
epoch: 30, step: 105
	action: tensor([[-0.0791,  0.7173, -0.2414, -1.0795, -0.1009,  1.1849, -0.1440]],
       dtype=torch.float64)
	q_value: tensor([[-34.9772]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 30, step: 106
	action: tensor([[-0.3686,  0.2895,  0.1165, -1.1311, -0.4930,  0.6058, -0.1239]],
       dtype=torch.float64)
	q_value: tensor([[-33.9535]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.30136589882654774, distance: 1.3054384627950535 entropy 0.8164090514183044
epoch: 30, step: 107
	action: tensor([[-0.4025,  0.3495, -0.3046,  0.1622, -0.6844,  0.5082,  0.6649]],
       dtype=torch.float64)
	q_value: tensor([[-25.9319]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 30, step: 108
	action: tensor([[-0.4846,  0.2044,  0.2403, -0.4904, -0.4254,  0.3187,  0.0536]],
       dtype=torch.float64)
	q_value: tensor([[-33.9535]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3344801888971096, distance: 1.3219430888558896 entropy 0.8164090514183044
epoch: 30, step: 109
	action: tensor([[-1.0820, -0.7712,  0.7283, -0.1041,  0.2056,  2.0445, -0.5984]],
       dtype=torch.float64)
	q_value: tensor([[-21.7859]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.362768980976405, distance: 1.3358811234063137 entropy 0.8164090514183044
epoch: 30, step: 110
	action: tensor([[-0.6764, -0.8983, -0.1814,  0.5417,  1.2756,  0.3246, -0.1599]],
       dtype=torch.float64)
	q_value: tensor([[-39.9333]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7552902333112501, distance: 1.5161115680215138 entropy 0.8164090514183044
epoch: 30, step: 111
	action: tensor([[ 1.0273, -0.4473, -0.1308, -0.6879,  0.4074,  1.0242,  0.2855]],
       dtype=torch.float64)
	q_value: tensor([[-32.8014]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23629630114481515, distance: 1.0000440731076232 entropy 0.8164090514183044
epoch: 30, step: 112
	action: tensor([[ 0.1875, -0.9517,  0.3331,  0.0053,  1.0417,  0.7079, -0.3439]],
       dtype=torch.float64)
	q_value: tensor([[-31.1032]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1830612974976893, distance: 1.244687467763486 entropy 0.8164090514183044
epoch: 30, step: 113
	action: tensor([[ 0.3088, -0.6109,  0.7058,  0.6162,  0.2741,  1.0974, -0.2378]],
       dtype=torch.float64)
	q_value: tensor([[-32.7726]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7535678289344445, distance: 0.5680746307597467 entropy 0.8164090514183044
epoch: 30, step: 114
	action: tensor([[ 0.5642, -1.0595,  0.2417, -0.6087, -0.0487,  0.6804,  0.7237]],
       dtype=torch.float64)
	q_value: tensor([[-30.9132]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5741214602745153, distance: 1.4357400432230283 entropy 0.8164090514183044
epoch: 30, step: 115
	action: tensor([[-0.0471, -0.1183, -0.0553, -1.1539, -0.5012,  0.7148, -0.8062]],
       dtype=torch.float64)
	q_value: tensor([[-29.8171]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20634825278468272, distance: 1.2568777641144926 entropy 0.8164090514183044
epoch: 30, step: 116
	action: tensor([[-0.8448,  0.3130, -0.4798, -0.1708, -0.0632,  0.0423,  1.2535]],
       dtype=torch.float64)
	q_value: tensor([[-28.0523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5195257025298854, distance: 1.4106222134630966 entropy 0.8164090514183044
epoch: 30, step: 117
	action: tensor([[ 0.3919,  0.2210,  0.4665,  0.1676, -0.4608,  0.8073,  0.3423]],
       dtype=torch.float64)
	q_value: tensor([[-27.9446]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 30, step: 118
	action: tensor([[-0.3493, -1.4926,  1.0583, -0.0976,  1.0046, -0.2837,  0.2164]],
       dtype=torch.float64)
	q_value: tensor([[-33.9535]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1228796424271286, distance: 1.6673216114887326 entropy 0.8164090514183044
epoch: 30, step: 119
	action: tensor([[ 0.4485, -0.4713, -0.0103, -0.3023,  0.4246,  0.4776, -0.2234]],
       dtype=torch.float64)
	q_value: tensor([[-34.6297]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19298722482011066, distance: 1.0280089557652916 entropy 0.8164090514183044
epoch: 30, step: 120
	action: tensor([[-0.6910, -0.3399,  0.6091,  0.4520, -0.9049,  0.6181,  0.5368]],
       dtype=torch.float64)
	q_value: tensor([[-24.7710]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23645530220666222, distance: 1.2724651707760632 entropy 0.8164090514183044
epoch: 30, step: 121
	action: tensor([[ 0.2575, -0.3127,  0.7418, -0.0261,  0.3063,  1.0649,  0.6824]],
       dtype=torch.float64)
	q_value: tensor([[-27.9127]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6249920566879246, distance: 0.7007722999159889 entropy 0.8164090514183044
epoch: 30, step: 122
	action: tensor([[ 0.1048, -0.4865, -0.3380, -0.1896,  0.3784,  1.0094,  0.4177]],
       dtype=torch.float64)
	q_value: tensor([[-28.4314]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2685807354789097, distance: 0.9786781462475298 entropy 0.8164090514183044
epoch: 30, step: 123
	action: tensor([[-0.7721, -1.5272,  0.7189, -0.1883,  0.0569,  0.4129,  0.2672]],
       dtype=torch.float64)
	q_value: tensor([[-26.1792]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 30, step: 124
	action: tensor([[-0.6016,  0.5760, -0.0885,  0.0185, -0.0913,  0.3948, -1.0241]],
       dtype=torch.float64)
	q_value: tensor([[-33.9535]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.043451708306461834, distance: 1.1192062931900204 entropy 0.8164090514183044
epoch: 30, step: 125
	action: tensor([[ 1.0682, -0.1299,  0.8239, -0.5571, -0.0981,  0.4883, -0.9587]],
       dtype=torch.float64)
	q_value: tensor([[-25.6273]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 30, step: 126
	action: tensor([[ 0.4446,  0.0920, -0.0391,  0.5526, -0.5046, -0.0216,  0.3799]],
       dtype=torch.float64)
	q_value: tensor([[-33.9535]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 30, step: 127
	action: tensor([[ 0.2341, -0.9760,  0.3363,  0.1510, -0.3245,  0.5935, -0.1163]],
       dtype=torch.float64)
	q_value: tensor([[-33.9535]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11160324904084296, distance: 1.2065118645403392 entropy 0.8164090514183044
LOSS epoch 30 actor 361.30929759151235 critic 300.48600938900404 
epoch: 31, step: 0
	action: tensor([[ 1.2486, -1.2867, -0.4968, -0.2780, -0.5763,  1.1247,  0.1058]],
       dtype=torch.float64)
	q_value: tensor([[-28.4049]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 31, step: 1
	action: tensor([[ 0.5712, -0.7314, -0.1951,  0.4330, -1.1894, -0.6343,  0.3318]],
       dtype=torch.float64)
	q_value: tensor([[-34.3742]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3289424322292833, distance: 0.9374251101554646 entropy 0.8164090514183044
epoch: 31, step: 2
	action: tensor([[ 0.4988, -0.5627,  0.4106, -0.0332, -0.2510,  0.6875, -0.1791]],
       dtype=torch.float64)
	q_value: tensor([[-34.2700]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3784676767048818, distance: 0.9021703565357134 entropy 0.8164090514183044
epoch: 31, step: 3
	action: tensor([[-0.3704, -0.2492, -0.2662, -0.1877,  0.1596,  0.8256, -0.8071]],
       dtype=torch.float64)
	q_value: tensor([[-26.0934]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10016470998042082, distance: 1.2002882322441928 entropy 0.8164090514183044
epoch: 31, step: 4
	action: tensor([[ 0.3920, -0.3761,  0.1002,  0.3739,  0.1172,  0.4379, -0.6408]],
       dtype=torch.float64)
	q_value: tensor([[-26.6131]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6435034000676998, distance: 0.6832574713300366 entropy 0.8164090514183044
epoch: 31, step: 5
	action: tensor([[ 0.1072, -0.8467,  0.5739, -0.3708,  0.1070,  1.0420,  0.9992]],
       dtype=torch.float64)
	q_value: tensor([[-27.0634]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14166478916855874, distance: 1.2227171302826525 entropy 0.8164090514183044
epoch: 31, step: 6
	action: tensor([[-2.1974e-04, -4.9034e-01,  4.0597e-01,  2.1242e-01, -8.3972e-03,
          8.3164e-01,  8.4544e-01]], dtype=torch.float64)
	q_value: tensor([[-31.0884]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37468066189600013, distance: 0.9049146579250696 entropy 0.8164090514183044
epoch: 31, step: 7
	action: tensor([[ 0.6038, -0.4271, -0.6630,  0.6488, -0.9921,  0.5879,  1.2881]],
       dtype=torch.float64)
	q_value: tensor([[-26.8938]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4977391897617578, distance: 0.8110009066465986 entropy 0.8164090514183044
epoch: 31, step: 8
	action: tensor([[ 0.3715, -0.2524,  0.5494, -0.4567,  0.6378,  0.6775,  0.7396]],
       dtype=torch.float64)
	q_value: tensor([[-32.4073]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31347589662895037, distance: 0.9481664585840875 entropy 0.8164090514183044
epoch: 31, step: 9
	action: tensor([[ 0.0447, -0.6339, -0.4069, -0.0362, -0.1390,  0.5049,  0.1972]],
       dtype=torch.float64)
	q_value: tensor([[-28.8049]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12641040972246742, distance: 1.2145209788966282 entropy 0.8164090514183044
epoch: 31, step: 10
	action: tensor([[ 0.7885, -0.2126, -0.1925, -0.5116,  0.2055, -0.4176, -0.0842]],
       dtype=torch.float64)
	q_value: tensor([[-23.7268]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13651396983247566, distance: 1.063369871212801 entropy 0.8164090514183044
epoch: 31, step: 11
	action: tensor([[ 0.8177, -0.7157,  0.4238,  0.9649,  0.4755,  0.9364, -0.0847]],
       dtype=torch.float64)
	q_value: tensor([[-26.1063]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7805403381321012, distance: 0.5360854821174484 entropy 0.8164090514183044
epoch: 31, step: 12
	action: tensor([[ 0.6890, -0.2511, -0.1464, -0.7190,  1.1076,  1.0687,  0.0843]],
       dtype=torch.float64)
	q_value: tensor([[-35.2731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42209177483313587, distance: 0.8699336518927614 entropy 0.8164090514183044
epoch: 31, step: 13
	action: tensor([[ 1.7545, -0.5742,  0.4272,  0.0533,  0.3598,  0.1562,  0.3662]],
       dtype=torch.float64)
	q_value: tensor([[-33.8251]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09110658154237972, distance: 1.195336775117163 entropy 0.8164090514183044
epoch: 31, step: 14
	action: tensor([[ 0.8331,  0.2379,  0.5393,  0.2680,  0.7529,  1.5295, -0.0776]],
       dtype=torch.float64)
	q_value: tensor([[-34.5746]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8752989443166307, distance: 0.40410270571826246 entropy 0.8164090514183044
epoch: 31, step: 15
	action: tensor([[ 0.4137, -0.8085, -0.9376, -0.5049,  0.4182,  1.0581, -0.3360]],
       dtype=torch.float64)
	q_value: tensor([[-35.0313]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08628344667030918, distance: 1.0938617587003605 entropy 0.8164090514183044
epoch: 31, step: 16
	action: tensor([[ 1.2220,  0.8387, -0.1338, -0.3676,  1.1575,  0.3924,  0.4873]],
       dtype=torch.float64)
	q_value: tensor([[-33.5127]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8421691992700548, distance: 0.4546242244496439 entropy 0.8164090514183044
epoch: 31, step: 17
	action: tensor([[ 1.1317, -0.6240, -0.0060, -0.2541,  0.6659, -0.1139, -0.3059]],
       dtype=torch.float64)
	q_value: tensor([[-34.0969]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12623926738194657, distance: 1.2144287106358997 entropy 0.8164090514183044
epoch: 31, step: 18
	action: tensor([[ 1.3550, -0.3753, -0.0898, -0.7883,  0.0278,  0.4873,  0.4331]],
       dtype=torch.float64)
	q_value: tensor([[-32.2334]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07724356190686055, distance: 1.1877188439137263 entropy 0.8164090514183044
epoch: 31, step: 19
	action: tensor([[ 0.7185, -0.3619, -0.6880, -0.4478,  0.3519,  1.1629,  1.1063]],
       dtype=torch.float64)
	q_value: tensor([[-30.8859]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5698408349466042, distance: 0.750535576928936 entropy 0.8164090514183044
epoch: 31, step: 20
	action: tensor([[ 5.4603e-01, -6.7489e-01, -1.2863e-03,  1.6867e+00, -1.8954e-01,
         -4.3740e-01, -1.6657e-01]], dtype=torch.float64)
	q_value: tensor([[-34.4405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9072625600396255, distance: 0.34848512472021487 entropy 0.8164090514183044
epoch: 31, step: 21
	action: tensor([[ 0.2752, -1.1579, -0.2872, -0.3838, -0.3082,  1.0044,  0.4219]],
       dtype=torch.float64)
	q_value: tensor([[-40.9370]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.536436190905079, distance: 1.4184497578812123 entropy 0.8164090514183044
epoch: 31, step: 22
	action: tensor([[ 0.2216, -1.0195, -0.1915, -0.3183,  0.1715,  0.1132,  0.9292]],
       dtype=torch.float64)
	q_value: tensor([[-31.1450]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6511232142061738, distance: 1.470437043517097 entropy 0.8164090514183044
epoch: 31, step: 23
	action: tensor([[ 0.5256, -1.5992,  0.5417, -0.3009, -0.2192,  1.0812, -0.1890]],
       dtype=torch.float64)
	q_value: tensor([[-29.8924]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 31, step: 24
	action: tensor([[-0.3534,  0.4462,  0.0257, -1.6947, -0.6334,  0.6235,  0.3839]],
       dtype=torch.float64)
	q_value: tensor([[-34.3742]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.25844356773230426, distance: 1.2837296316754623 entropy 0.8164090514183044
epoch: 31, step: 25
	action: tensor([[ 0.9172, -0.3911, -0.2440, -0.4783,  0.9666,  0.3699,  0.4522]],
       dtype=torch.float64)
	q_value: tensor([[-31.9862]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1777557527774285, distance: 1.0376648726110733 entropy 0.8164090514183044
epoch: 31, step: 26
	action: tensor([[ 1.0062, -0.8447, -0.1391,  0.3054,  0.2522,  1.1180,  0.9284]],
       dtype=torch.float64)
	q_value: tensor([[-31.9141]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5047140119797411, distance: 0.8053500947314988 entropy 0.8164090514183044
epoch: 31, step: 27
	action: tensor([[ 1.3667, -0.6145,  0.6426,  0.3451,  0.3685,  0.8699,  1.0445]],
       dtype=torch.float64)
	q_value: tensor([[-34.6942]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20948959335840178, distance: 1.0174439391160637 entropy 0.8164090514183044
epoch: 31, step: 28
	action: tensor([[-0.3827, -0.9547,  0.3069, -0.3640,  0.2070,  0.7212,  0.9331]],
       dtype=torch.float64)
	q_value: tensor([[-37.0520]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.729903607613831, distance: 1.5051079352842363 entropy 0.8164090514183044
epoch: 31, step: 29
	action: tensor([[ 0.1800,  0.4932,  0.6302, -0.4105,  1.0069,  0.9003,  1.0271]],
       dtype=torch.float64)
	q_value: tensor([[-29.1278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 31, step: 30
	action: tensor([[ 1.1276,  0.1579, -0.6636, -0.7180, -0.6088,  0.1491,  0.1441]],
       dtype=torch.float64)
	q_value: tensor([[-34.3742]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 31, step: 31
	action: tensor([[ 0.2800, -0.4158,  0.6233, -0.6727,  0.6308,  0.4501, -0.0292]],
       dtype=torch.float64)
	q_value: tensor([[-34.3742]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16315935802673232, distance: 1.2341737436714761 entropy 0.8164090514183044
epoch: 31, step: 32
	action: tensor([[ 0.4438, -1.1269,  0.1608, -0.8853,  0.8174,  1.9825,  0.0933]],
       dtype=torch.float64)
	q_value: tensor([[-27.5970]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06007951430298597, distance: 1.1782187079793516 entropy 0.8164090514183044
epoch: 31, step: 33
	action: tensor([[ 0.2536, -1.3653,  0.0145, -1.2562, -0.3060, -0.1685,  0.5013]],
       dtype=torch.float64)
	q_value: tensor([[-43.3088]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 31, step: 34
	action: tensor([[ 0.2178,  0.0315,  0.3694, -0.4824,  0.2312,  0.1983, -0.4189]],
       dtype=torch.float64)
	q_value: tensor([[-34.3742]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2828192754878822, distance: 0.9691053644093504 entropy 0.8164090514183044
epoch: 31, step: 35
	action: tensor([[ 0.3677, -0.2010,  0.4629, -0.9290,  0.2715,  0.0973,  1.3552]],
       dtype=torch.float64)
	q_value: tensor([[-23.7801]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11957622854212535, distance: 1.2108309896366691 entropy 0.8164090514183044
epoch: 31, step: 36
	action: tensor([[ 0.2151, -0.8237,  0.3110,  0.0460,  1.0826,  0.1257, -0.1224]],
       dtype=torch.float64)
	q_value: tensor([[-31.0435]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2231524511512737, distance: 1.2656015214712073 entropy 0.8164090514183044
epoch: 31, step: 37
	action: tensor([[ 0.2372, -1.0669,  0.2553, -0.4508,  1.3462,  0.7345,  1.0376]],
       dtype=torch.float64)
	q_value: tensor([[-32.1781]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5693541961009589, distance: 1.433564308272564 entropy 0.8164090514183044
epoch: 31, step: 38
	action: tensor([[ 0.4523, -0.7353,  0.2431, -0.2626,  0.9007,  0.8005,  0.3034]],
       dtype=torch.float64)
	q_value: tensor([[-38.2503]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.021625294591540722, distance: 1.1566514639249157 entropy 0.8164090514183044
epoch: 31, step: 39
	action: tensor([[ 0.1408, -0.4189,  0.9542, -0.1728,  0.2931,  0.8438,  0.3305]],
       dtype=torch.float64)
	q_value: tensor([[-31.7210]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24180099520590015, distance: 0.996433450075299 entropy 0.8164090514183044
epoch: 31, step: 40
	action: tensor([[-1.1124, -0.8381,  0.2668, -0.7178, -0.0893,  0.3857,  0.9786]],
       dtype=torch.float64)
	q_value: tensor([[-28.3990]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.4382346222791282, distance: 1.786876100563726 entropy 0.8164090514183044
epoch: 31, step: 41
	action: tensor([[ 1.2889, -0.6204, -1.4050, -1.2655, -0.8318,  0.7511,  0.0227]],
       dtype=torch.float64)
	q_value: tensor([[-30.7299]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.39715001722419485, distance: 1.352627502357614 entropy 0.8164090514183044
epoch: 31, step: 42
	action: tensor([[ 0.2442, -0.5002,  0.6965,  0.2800,  0.4519,  1.2708,  0.4078]],
       dtype=torch.float64)
	q_value: tensor([[-37.3578]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6739286065669644, distance: 0.6534510241477771 entropy 0.8164090514183044
epoch: 31, step: 43
	action: tensor([[ 1.7053, -0.8309, -0.5095, -0.3035,  0.9763,  0.6350, -0.7918]],
       dtype=torch.float64)
	q_value: tensor([[-31.8575]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4877246281948098, distance: 1.3957832089222626 entropy 0.8164090514183044
epoch: 31, step: 44
	action: tensor([[ 0.0917, -0.8095, -0.0500, -0.2846,  0.9484,  1.3420,  1.4116]],
       dtype=torch.float64)
	q_value: tensor([[-41.7826]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11070780588037321, distance: 1.0791428395508331 entropy 0.8164090514183044
epoch: 31, step: 45
	action: tensor([[ 0.5622, -0.7497,  0.5625, -0.9415,  0.6364,  0.8309, -0.3700]],
       dtype=torch.float64)
	q_value: tensor([[-37.7944]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4204363894006975, distance: 1.3638530778845332 entropy 0.8164090514183044
epoch: 31, step: 46
	action: tensor([[ 1.4231,  1.1000,  0.0321,  0.0490,  1.1775,  1.0523, -0.0888]],
       dtype=torch.float64)
	q_value: tensor([[-33.1016]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 31, step: 47
	action: tensor([[-0.0380,  0.5101,  0.1751, -0.0847,  0.5838,  0.3877,  0.8816]],
       dtype=torch.float64)
	q_value: tensor([[-34.3742]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 31, step: 48
	action: tensor([[ 0.6834, -0.9721,  1.0425, -0.4443,  0.6611, -0.9342, -0.4890]],
       dtype=torch.float64)
	q_value: tensor([[-34.3742]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.32384163577069036, distance: 1.316663241667628 entropy 0.8164090514183044
epoch: 31, step: 49
	action: tensor([[-0.3642, -1.2078,  0.7786, -1.0121,  0.3640,  0.4301, -0.1786]],
       dtype=torch.float64)
	q_value: tensor([[-39.7589]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2582785327997463, distance: 1.7196713121224925 entropy 0.8164090514183044
epoch: 31, step: 50
	action: tensor([[ 0.0540, -0.3332, -0.0045, -1.2752,  0.0835,  0.7511,  0.9667]],
       dtype=torch.float64)
	q_value: tensor([[-33.8669]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.28220451326356044, distance: 1.2957921477814192 entropy 0.8164090514183044
epoch: 31, step: 51
	action: tensor([[-0.5134,  0.6087, -0.0453, -0.7679,  1.1708, -0.3100,  0.6779]],
       dtype=torch.float64)
	q_value: tensor([[-31.7519]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24339389124272914, distance: 1.276030508304047 entropy 0.8164090514183044
epoch: 31, step: 52
	action: tensor([[ 0.6944, -0.3361,  0.6358, -0.8243,  0.5586,  0.7159,  0.2947]],
       dtype=torch.float64)
	q_value: tensor([[-31.7507]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13484665658330275, distance: 1.0643960115190818 entropy 0.8164090514183044
epoch: 31, step: 53
	action: tensor([[ 0.6048, -2.0946,  0.2845, -0.1564, -0.2793,  0.7370, -0.6297]],
       dtype=torch.float64)
	q_value: tensor([[-29.8621]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 31, step: 54
	action: tensor([[ 0.3842, -0.2235,  0.0497, -0.3110, -0.4183,  0.6469,  0.5623]],
       dtype=torch.float64)
	q_value: tensor([[-34.3742]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.40522134825839706, distance: 0.8825399613236721 entropy 0.8164090514183044
epoch: 31, step: 55
	action: tensor([[-0.0306, -1.2223,  0.5300, -0.4541,  0.6053,  0.0226, -0.5077]],
       dtype=torch.float64)
	q_value: tensor([[-24.5729]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0876821871665263, distance: 1.6534416989333454 entropy 0.8164090514183044
epoch: 31, step: 56
	action: tensor([[-0.0246, -0.2362,  0.0216, -0.6130, -0.4116,  1.0685,  0.2271]],
       dtype=torch.float64)
	q_value: tensor([[-34.1922]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05012961021020279, distance: 1.1152927218575241 entropy 0.8164090514183044
epoch: 31, step: 57
	action: tensor([[ 1.2144, -0.5400,  0.4550, -0.7702,  0.7175, -0.1565,  0.3677]],
       dtype=torch.float64)
	q_value: tensor([[-26.9042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1811707728915155, distance: 1.2436925687221827 entropy 0.8164090514183044
epoch: 31, step: 58
	action: tensor([[ 0.1437, -0.9377,  0.2894,  0.6836, -0.1004,  1.1461,  0.7295]],
       dtype=torch.float64)
	q_value: tensor([[-32.6697]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5457018215646183, distance: 0.7713068546851171 entropy 0.8164090514183044
epoch: 31, step: 59
	action: tensor([[ 0.9610, -1.1092,  0.9640, -0.0775,  0.3084, -0.1206, -1.1751]],
       dtype=torch.float64)
	q_value: tensor([[-30.8322]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5294791914752102, distance: 1.4152347365368092 entropy 0.8164090514183044
epoch: 31, step: 60
	action: tensor([[ 0.0996, -1.2528, -1.0939,  0.1734,  0.8768,  0.9042,  1.2623]],
       dtype=torch.float64)
	q_value: tensor([[-41.5806]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4147468599153721, distance: 1.361118894125162 entropy 0.8164090514183044
epoch: 31, step: 61
	action: tensor([[ 0.3127, -0.5175, -0.3042, -0.3518,  0.4167,  1.0055,  0.1141]],
       dtype=torch.float64)
	q_value: tensor([[-39.6536]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.291709870680135, distance: 0.9630798313631606 entropy 0.8164090514183044
epoch: 31, step: 62
	action: tensor([[ 1.1085,  1.1026, -0.3967, -0.0938, -0.2411,  0.7855,  0.2171]],
       dtype=torch.float64)
	q_value: tensor([[-28.2531]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 31, step: 63
	action: tensor([[ 0.4969,  0.6820, -0.1434,  0.7381, -0.2599,  0.0382, -0.0356]],
       dtype=torch.float64)
	q_value: tensor([[-34.3742]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 31, step: 64
	action: tensor([[ 0.4464, -0.5864, -0.3326,  0.0875, -0.5361,  0.0157,  1.1565]],
       dtype=torch.float64)
	q_value: tensor([[-34.3742]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12478717434852848, distance: 1.070566205544891 entropy 0.8164090514183044
epoch: 31, step: 65
	action: tensor([[-0.2544, -0.4246,  0.3487, -0.2471, -0.2226,  0.3657,  0.6458]],
       dtype=torch.float64)
	q_value: tensor([[-29.0783]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.38397708461838587, distance: 1.3462358232287452 entropy 0.8164090514183044
epoch: 31, step: 66
	action: tensor([[-0.3498,  0.0181, -0.2669, -0.2123, -0.2561,  0.1423,  0.5875]],
       dtype=torch.float64)
	q_value: tensor([[-23.8351]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12151498892441026, distance: 1.2118789288838534 entropy 0.8164090514183044
epoch: 31, step: 67
	action: tensor([[ 0.5392,  0.3477, -0.5472,  0.1842,  0.4683, -0.2436,  1.2495]],
       dtype=torch.float64)
	q_value: tensor([[-21.7125]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 31, step: 68
	action: tensor([[ 1.0539, -1.0119, -0.0045,  0.1257,  0.0970, -0.3397, -0.3518]],
       dtype=torch.float64)
	q_value: tensor([[-34.3742]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.40886402557497714, distance: 1.3582860275107753 entropy 0.8164090514183044
epoch: 31, step: 69
	action: tensor([[ 0.0566, -1.6156, -0.7430, -0.6258,  0.4441,  1.2211,  0.0286]],
       dtype=torch.float64)
	q_value: tensor([[-33.5260]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 31, step: 70
	action: tensor([[-0.7493,  0.1938,  0.3679,  0.0752,  0.0459,  0.1391,  0.0699]],
       dtype=torch.float64)
	q_value: tensor([[-34.3742]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3663088043405831, distance: 1.3376149889696187 entropy 0.8164090514183044
epoch: 31, step: 71
	action: tensor([[ 0.1285, -0.3881, -0.9742, -0.5541,  0.0141,  0.5470,  0.5883]],
       dtype=torch.float64)
	q_value: tensor([[-22.8726]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10778641092256747, distance: 1.0809139208547078 entropy 0.8164090514183044
epoch: 31, step: 72
	action: tensor([[-0.2499, -0.8508,  0.1600,  0.5761, -0.0842, -1.5394,  0.9068]],
       dtype=torch.float64)
	q_value: tensor([[-27.2465]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4481625789538797, distance: 1.3770996051022002 entropy 0.8164090514183044
epoch: 31, step: 73
	action: tensor([[-0.8378, -0.3822,  0.1399,  0.3233,  1.4171,  0.7489,  0.2651]],
       dtype=torch.float64)
	q_value: tensor([[-39.2679]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4655206029785848, distance: 1.385328144326131 entropy 0.8164090514183044
epoch: 31, step: 74
	action: tensor([[ 0.0906, -1.1818, -0.6338, -0.3904, -0.8808,  0.7143,  0.4739]],
       dtype=torch.float64)
	q_value: tensor([[-34.1584]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8535486780529422, distance: 1.557968567027113 entropy 0.8164090514183044
epoch: 31, step: 75
	action: tensor([[-0.8957, -0.3570,  0.5825,  0.2533, -1.2056,  0.8053, -0.0320]],
       dtype=torch.float64)
	q_value: tensor([[-32.0969]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.785377520491187, distance: 1.529050131446195 entropy 0.8164090514183044
epoch: 31, step: 76
	action: tensor([[ 0.4935, -0.2714,  0.1123, -0.4166,  0.6018,  0.5380,  0.2006]],
       dtype=torch.float64)
	q_value: tensor([[-31.7611]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36220176226821754, distance: 0.9138993127442548 entropy 0.8164090514183044
epoch: 31, step: 77
	action: tensor([[ 0.3474,  0.6289, -0.9564,  0.5408, -0.2883,  0.0536, -0.4959]],
       dtype=torch.float64)
	q_value: tensor([[-26.2177]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 31, step: 78
	action: tensor([[-0.1441, -0.3967,  0.2265, -0.5163,  0.0902, -0.1082, -1.4354]],
       dtype=torch.float64)
	q_value: tensor([[-34.3742]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5044046519482377, distance: 1.403585998051542 entropy 0.8164090514183044
epoch: 31, step: 79
	action: tensor([[ 0.4697, -0.2516, -0.0263,  0.3290,  0.4692,  0.4179,  0.8418]],
       dtype=torch.float64)
	q_value: tensor([[-33.3516]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7573324005009865, distance: 0.5637188928502486 entropy 0.8164090514183044
epoch: 31, step: 80
	action: tensor([[-0.0232, -0.1740, -0.2266, -0.1577,  0.0227,  0.5468,  1.1848]],
       dtype=torch.float64)
	q_value: tensor([[-27.8730]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21401522080551583, distance: 1.0145273542494193 entropy 0.8164090514183044
epoch: 31, step: 81
	action: tensor([[ 1.1976,  0.5127,  0.5236,  0.1889, -0.5420,  0.4500,  0.2073]],
       dtype=torch.float64)
	q_value: tensor([[-27.0038]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9063764710234272, distance: 0.35014602208476486 entropy 0.8164090514183044
epoch: 31, step: 82
	action: tensor([[ 0.2898, -0.1142, -0.6371, -0.0569,  0.4131,  1.1014, -0.1268]],
       dtype=torch.float64)
	q_value: tensor([[-28.9091]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.704859297991407, distance: 0.6216862273410149 entropy 0.8164090514183044
epoch: 31, step: 83
	action: tensor([[ 0.8108, -0.8540,  0.1341, -0.7919,  0.5727, -0.0617, -0.5002]],
       dtype=torch.float64)
	q_value: tensor([[-27.4423]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6859294315579014, distance: 1.4858548610046174 entropy 0.8164090514183044
epoch: 31, step: 84
	action: tensor([[ 0.7541, -0.1938, -0.1521, -1.3309,  0.0258,  0.6792,  0.4878]],
       dtype=torch.float64)
	q_value: tensor([[-33.5047]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03587988667087283, distance: 1.123627254538951 entropy 0.8164090514183044
epoch: 31, step: 85
	action: tensor([[ 0.2937, -1.5207,  0.2994, -0.4182,  0.4289,  0.6408, -1.0893]],
       dtype=torch.float64)
	q_value: tensor([[-31.1475]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 31, step: 86
	action: tensor([[ 0.3882,  0.1233, -0.5613,  0.6825, -0.5388,  0.5947, -0.1711]],
       dtype=torch.float64)
	q_value: tensor([[-34.3742]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 31, step: 87
	action: tensor([[ 0.0193, -0.9028,  0.2409,  0.1151,  0.7212, -0.0595,  0.3073]],
       dtype=torch.float64)
	q_value: tensor([[-34.3742]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4475886269922489, distance: 1.376826684312549 entropy 0.8164090514183044
epoch: 31, step: 88
	action: tensor([[ 0.4963, -0.2394,  0.2945, -0.8036, -0.2168, -0.5518,  0.5834]],
       dtype=torch.float64)
	q_value: tensor([[-28.9288]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15390383727701495, distance: 1.229253637426871 entropy 0.8164090514183044
epoch: 31, step: 89
	action: tensor([[ 0.3373, -1.0169,  0.1904, -0.3582,  0.0180,  1.6840, -0.3546]],
       dtype=torch.float64)
	q_value: tensor([[-26.7180]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0831401116558208, distance: 1.0957416753372202 entropy 0.8164090514183044
epoch: 31, step: 90
	action: tensor([[-0.5504, -0.7200, -0.7641, -0.2159,  0.4353,  1.4363, -0.4967]],
       dtype=torch.float64)
	q_value: tensor([[-35.9451]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.38777394785628383, distance: 1.3480812195161527 entropy 0.8164090514183044
epoch: 31, step: 91
	action: tensor([[ 1.1869, -0.6564,  0.4831, -0.6533,  0.3501,  0.5167,  0.3123]],
       dtype=torch.float64)
	q_value: tensor([[-33.2918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18662503413248288, distance: 1.2465607447078892 entropy 0.8164090514183044
epoch: 31, step: 92
	action: tensor([[ 0.1850, -1.0326, -0.2184, -0.6740,  2.0210, -0.0248,  1.4844]],
       dtype=torch.float64)
	q_value: tensor([[-31.5701]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7893034141690936, distance: 1.5307303340470737 entropy 0.8164090514183044
epoch: 31, step: 93
	action: tensor([[ 0.7147, -0.6342,  0.6783, -0.0522, -0.1963,  0.7165,  0.1247]],
       dtype=torch.float64)
	q_value: tensor([[-45.5984]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3275053235729313, distance: 0.9384283484091581 entropy 0.8164090514183044
epoch: 31, step: 94
	action: tensor([[ 0.9644, -1.2137, -0.3664,  0.9191,  0.8870,  0.4184,  0.4715]],
       dtype=torch.float64)
	q_value: tensor([[-27.8466]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33535868268802227, distance: 0.9329327974990073 entropy 0.8164090514183044
epoch: 31, step: 95
	action: tensor([[ 0.8256, -0.6485, -0.4437, -0.3350, -0.0100,  0.3299,  0.0204]],
       dtype=torch.float64)
	q_value: tensor([[-39.2175]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03845515457960569, distance: 1.1661396617591644 entropy 0.8164090514183044
epoch: 31, step: 96
	action: tensor([[-0.2429, -0.0572,  0.5984, -0.9012,  0.7555,  1.6419,  0.8893]],
       dtype=torch.float64)
	q_value: tensor([[-27.2416]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1799943217309975, distance: 1.0362513829142763 entropy 0.8164090514183044
epoch: 31, step: 97
	action: tensor([[ 0.1059,  0.5474,  0.8489, -1.2961,  0.2308,  1.0144,  1.7564]],
       dtype=torch.float64)
	q_value: tensor([[-37.4092]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3650447356232973, distance: 0.9118601935843786 entropy 0.8164090514183044
epoch: 31, step: 98
	action: tensor([[ 0.4723,  0.6586, -0.0054, -1.0756,  1.0993, -0.2531,  0.6169]],
       dtype=torch.float64)
	q_value: tensor([[-39.7157]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6459285495085778, distance: 0.680929498028725 entropy 0.8164090514183044
epoch: 31, step: 99
	action: tensor([[ 0.4637, -1.3772, -0.1070, -0.0284,  0.0661,  1.2752,  0.4755]],
       dtype=torch.float64)
	q_value: tensor([[-31.9123]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05699988027723624, distance: 1.176506043277208 entropy 0.8164090514183044
epoch: 31, step: 100
	action: tensor([[ 1.4969, -0.5964, -0.0036, -0.1000,  0.4599,  0.1648, -0.0047]],
       dtype=torch.float64)
	q_value: tensor([[-32.8197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07071560141130528, distance: 1.1841146619894347 entropy 0.8164090514183044
epoch: 31, step: 101
	action: tensor([[-0.5394, -0.3238,  0.3903, -0.5037,  0.6486,  0.6130,  0.1012]],
       dtype=torch.float64)
	q_value: tensor([[-32.5391]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6484580842328826, distance: 1.4692498248741448 entropy 0.8164090514183044
epoch: 31, step: 102
	action: tensor([[ 1.0289, -0.5509,  0.1412,  0.3870, -0.8333,  0.3292, -0.8544]],
       dtype=torch.float64)
	q_value: tensor([[-27.0540]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5405324870096289, distance: 0.7756826867783534 entropy 0.8164090514183044
epoch: 31, step: 103
	action: tensor([[-0.1431, -0.7337,  0.4217, -0.3174, -0.2330,  0.3741, -0.3936]],
       dtype=torch.float64)
	q_value: tensor([[-32.2328]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5879910928339942, distance: 1.4420513462616127 entropy 0.8164090514183044
epoch: 31, step: 104
	action: tensor([[-0.0346, -0.6621, -0.0874,  0.1089, -0.2653, -0.3215,  0.0848]],
       dtype=torch.float64)
	q_value: tensor([[-26.1063]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.32550423502784365, distance: 1.3174897742330671 entropy 0.8164090514183044
epoch: 31, step: 105
	action: tensor([[-0.4592, -0.2818,  0.8212, -1.0433,  0.5276,  0.9581, -0.0310]],
       dtype=torch.float64)
	q_value: tensor([[-24.8830]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7347581330923627, distance: 1.507218303156466 entropy 0.8164090514183044
epoch: 31, step: 106
	action: tensor([[ 0.1851,  0.6128, -0.9224, -0.4747,  0.8412,  0.6415, -1.3820]],
       dtype=torch.float64)
	q_value: tensor([[-31.3782]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 31, step: 107
	action: tensor([[-0.0543,  1.0439, -0.5712,  0.1202,  0.4149, -0.6178,  1.3668]],
       dtype=torch.float64)
	q_value: tensor([[-34.3742]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 31, step: 108
	action: tensor([[ 0.5609, -0.7940, -0.1038,  0.1572, -0.2016,  0.2617, -0.8605]],
       dtype=torch.float64)
	q_value: tensor([[-34.3742]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11626065181342127, distance: 1.075768415786654 entropy 0.8164090514183044
epoch: 31, step: 109
	action: tensor([[-0.1157, -0.2996,  0.7298, -0.1712,  0.6480, -0.3821,  0.3419]],
       dtype=torch.float64)
	q_value: tensor([[-30.2047]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3352231793220626, distance: 1.3223110427571285 entropy 0.8164090514183044
epoch: 31, step: 110
	action: tensor([[ 0.8962,  0.1086,  0.8841, -0.5918,  0.3093,  0.6767,  0.2293]],
       dtype=torch.float64)
	q_value: tensor([[-27.4172]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7097428925716831, distance: 0.6165213557302528 entropy 0.8164090514183044
epoch: 31, step: 111
	action: tensor([[ 0.7332, -1.0671,  0.2802, -0.0547,  0.1117,  1.1380,  0.7637]],
       dtype=torch.float64)
	q_value: tensor([[-28.8755]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08137483483787433, distance: 1.0967960115844169 entropy 0.8164090514183044
epoch: 31, step: 112
	action: tensor([[0.1448, 0.3840, 0.3463, 0.1327, 0.9543, 1.1989, 0.6783]],
       dtype=torch.float64)
	q_value: tensor([[-32.9011]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 31, step: 113
	action: tensor([[-0.8584, -0.0275, -0.3636,  0.2760,  0.7130,  0.1679,  0.3029]],
       dtype=torch.float64)
	q_value: tensor([[-34.3742]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6589400423233158, distance: 1.4739136410202942 entropy 0.8164090514183044
epoch: 31, step: 114
	action: tensor([[ 0.2887,  0.2533,  0.6078, -0.4496,  0.1325, -0.3852, -0.3437]],
       dtype=torch.float64)
	q_value: tensor([[-26.1278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.346945308641753, distance: 0.9247651823730973 entropy 0.8164090514183044
epoch: 31, step: 115
	action: tensor([[ 0.6707, -0.7665, -0.0251,  0.2484,  0.7840,  1.0275,  0.4997]],
       dtype=torch.float64)
	q_value: tensor([[-24.9963]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5099886468854236, distance: 0.8010502576544123 entropy 0.8164090514183044
epoch: 31, step: 116
	action: tensor([[-0.0410, -0.3735,  0.3503, -0.2226,  1.1153,  0.8946, -0.0686]],
       dtype=torch.float64)
	q_value: tensor([[-32.9761]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1473625781644633, distance: 1.0566688092789205 entropy 0.8164090514183044
epoch: 31, step: 117
	action: tensor([[-0.5909, -0.8150, -0.8581, -0.8221,  0.1592,  0.1019,  0.6677]],
       dtype=torch.float64)
	q_value: tensor([[-31.2494]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5398930120457854, distance: 1.4200445433476414 entropy 0.8164090514183044
epoch: 31, step: 118
	action: tensor([[ 1.0116, -0.4175,  0.1707, -0.8020,  0.7669,  0.5913, -0.5245]],
       dtype=torch.float64)
	q_value: tensor([[-30.5400]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04697897017723052, distance: 1.1709158167341231 entropy 0.8164090514183044
epoch: 31, step: 119
	action: tensor([[ 1.1928,  0.1199,  0.4825, -0.9521,  0.5208,  0.0615, -0.0351]],
       dtype=torch.float64)
	q_value: tensor([[-32.7071]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4120782011739341, distance: 0.8774380727102611 entropy 0.8164090514183044
epoch: 31, step: 120
	action: tensor([[-0.2021, -0.0480,  0.6614,  0.2423,  0.2072,  1.3870,  0.4493]],
       dtype=torch.float64)
	q_value: tensor([[-30.1181]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7065971371763222, distance: 0.6198532274188225 entropy 0.8164090514183044
epoch: 31, step: 121
	action: tensor([[ 0.7411, -0.7239, -1.0136, -0.3876,  0.5380,  0.6936,  0.4001]],
       dtype=torch.float64)
	q_value: tensor([[-30.5885]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1099765694874093, distance: 1.0795864206227894 entropy 0.8164090514183044
epoch: 31, step: 122
	action: tensor([[ 1.0895, -0.1623, -0.0684, -0.8744,  0.4488,  1.5014, -0.5808]],
       dtype=torch.float64)
	q_value: tensor([[-33.4354]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5216862394962474, distance: 0.7914311338916982 entropy 0.8164090514183044
epoch: 31, step: 123
	action: tensor([[ 1.1707, -0.5261, -0.0797, -0.2885, -0.1105,  0.5533,  0.4330]],
       dtype=torch.float64)
	q_value: tensor([[-35.7434]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15964995072387267, distance: 1.0490273428491308 entropy 0.8164090514183044
epoch: 31, step: 124
	action: tensor([[ 0.5633, -0.2486,  0.2287,  0.6819,  0.1103,  1.1561,  0.9852]],
       dtype=torch.float64)
	q_value: tensor([[-29.2227]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9748466432848499, distance: 0.18149082223518517 entropy 0.8164090514183044
epoch: 31, step: 125
	action: tensor([[ 0.3549,  0.5159, -0.2603, -0.5803,  0.5112,  0.7805,  1.3376]],
       dtype=torch.float64)
	q_value: tensor([[-31.7457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9097494984001759, distance: 0.34378071099097834 entropy 0.8164090514183044
epoch: 31, step: 126
	action: tensor([[-0.0734, -0.0226,  0.6131, -0.8054, -0.0913,  0.7315,  0.4795]],
       dtype=torch.float64)
	q_value: tensor([[-32.3011]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04668078572670553, distance: 1.1707490637416011 entropy 0.8164090514183044
epoch: 31, step: 127
	action: tensor([[ 1.0232, -0.8220, -0.4316, -0.1248,  0.5532,  0.9193,  0.3766]],
       dtype=torch.float64)
	q_value: tensor([[-26.9452]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18583698714853503, distance: 1.0325530588486058 entropy 0.8164090514183044
LOSS epoch 31 actor 431.37155942306487 critic 171.46712309697313 
epoch: 32, step: 0
	action: tensor([[ 1.1111, -1.2848, -0.0507, -0.4022,  0.7706,  0.9822, -0.0192]],
       dtype=torch.float64)
	q_value: tensor([[-34.0772]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5632366286420534, distance: 1.4307674604703615 entropy 0.8164090514183044
epoch: 32, step: 1
	action: tensor([[ 1.0483, -2.3447,  0.8455,  0.1601,  0.6106,  1.2201,  1.4321]],
       dtype=torch.float64)
	q_value: tensor([[-38.7832]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 32, step: 2
	action: tensor([[ 0.0140, -1.1891, -1.1229,  0.6437,  0.3034,  0.3946,  0.6691]],
       dtype=torch.float64)
	q_value: tensor([[-34.6846]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7233367700607161, distance: 1.5022484704519172 entropy 0.8164090514183044
epoch: 32, step: 3
	action: tensor([[ 0.5348, -1.0409, -0.3833, -0.1209,  1.1210,  1.5400,  0.2826]],
       dtype=torch.float64)
	q_value: tensor([[-33.9165]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23027441362765422, distance: 1.0039790600464171 entropy 0.8164090514183044
epoch: 32, step: 4
	action: tensor([[ 0.9887,  0.6485,  0.6028, -0.7327,  1.2901,  1.1567, -0.0370]],
       dtype=torch.float64)
	q_value: tensor([[-39.5435]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.883941361769526, distance: 0.38984810589461727 entropy 0.8164090514183044
epoch: 32, step: 5
	action: tensor([[ 1.3826, -0.8380, -1.1329, -0.1564,  0.3966,  1.3135,  0.1563]],
       dtype=torch.float64)
	q_value: tensor([[-36.9881]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23519634088002972, distance: 1.0007639942925137 entropy 0.8164090514183044
epoch: 32, step: 6
	action: tensor([[ 0.7630, -1.2954,  0.4395, -1.0965,  0.5538,  1.0188,  0.2637]],
       dtype=torch.float64)
	q_value: tensor([[-39.2528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8148058397016882, distance: 1.5416002685657066 entropy 0.8164090514183044
epoch: 32, step: 7
	action: tensor([[ 0.5476, -2.0172,  0.0434, -0.8050,  0.5867,  2.1336, -0.6292]],
       dtype=torch.float64)
	q_value: tensor([[-37.8166]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 32, step: 8
	action: tensor([[ 0.2923, -1.0115,  1.0049, -0.3298, -0.2262,  0.3114, -0.0530]],
       dtype=torch.float64)
	q_value: tensor([[-34.6846]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5854115013729335, distance: 1.4408796094453875 entropy 0.8164090514183044
epoch: 32, step: 9
	action: tensor([[ 0.6464, -0.7615,  0.7591, -0.6554,  0.4396,  1.8438,  0.2308]],
       dtype=torch.float64)
	q_value: tensor([[-30.7915]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21215818235621842, distance: 1.015725154022767 entropy 0.8164090514183044
epoch: 32, step: 10
	action: tensor([[ 1.9573, -1.0567,  0.9185, -1.2809,  1.1231,  0.8644,  0.6779]],
       dtype=torch.float64)
	q_value: tensor([[-39.0487]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 32, step: 11
	action: tensor([[ 0.3335,  0.6210, -0.0305, -0.5377,  0.1151, -0.0978,  0.0471]],
       dtype=torch.float64)
	q_value: tensor([[-34.6846]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 32, step: 12
	action: tensor([[-1.5012, -2.0631,  0.6839, -0.4517,  0.9146,  0.1420,  0.3795]],
       dtype=torch.float64)
	q_value: tensor([[-34.6846]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 32, step: 13
	action: tensor([[-0.1544, -0.6922, -0.1032,  0.1038,  0.2724,  0.2467,  0.1817]],
       dtype=torch.float64)
	q_value: tensor([[-34.6846]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3166720200622757, distance: 1.3130930316450737 entropy 0.8164090514183044
epoch: 32, step: 14
	action: tensor([[-0.3721, -0.9199, -0.1389, -1.4904,  0.5933,  1.0694,  1.4614]],
       dtype=torch.float64)
	q_value: tensor([[-25.0756]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.40747571456457665, distance: 1.3576166271302152 entropy 0.8164090514183044
epoch: 32, step: 15
	action: tensor([[ 1.0727, -1.2427,  0.2031, -1.3435,  1.1625,  1.6009,  0.3857]],
       dtype=torch.float64)
	q_value: tensor([[-41.1227]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5622816445566523, distance: 1.4303303644890628 entropy 0.8164090514183044
epoch: 32, step: 16
	action: tensor([[ 1.1048, -1.9417,  0.5569, -0.6942,  0.7498,  1.8996, -0.1052]],
       dtype=torch.float64)
	q_value: tensor([[-46.8558]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 32, step: 17
	action: tensor([[ 0.3629,  0.0484, -0.4554,  1.0402,  0.1926,  0.7612, -0.4427]],
       dtype=torch.float64)
	q_value: tensor([[-34.6846]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 32, step: 18
	action: tensor([[ 0.1888, -0.1443,  0.1015,  0.8918, -0.3558,  0.8466, -0.1399]],
       dtype=torch.float64)
	q_value: tensor([[-34.6846]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 32, step: 19
	action: tensor([[ 0.1952, -0.0785, -0.0958, -0.7354,  0.1684,  0.0389, -0.4720]],
       dtype=torch.float64)
	q_value: tensor([[-34.6846]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09840599611710776, distance: 1.0865812351008575 entropy 0.8164090514183044
epoch: 32, step: 20
	action: tensor([[ 1.2657, -0.7730, -0.0300, -0.7817,  0.9637,  1.3117,  0.6286]],
       dtype=torch.float64)
	q_value: tensor([[-25.1600]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23911177566461106, distance: 1.2738313569621889 entropy 0.8164090514183044
epoch: 32, step: 21
	action: tensor([[ 2.3490, -0.8099, -0.2018, -0.4474,  0.8189,  1.3038,  1.2058]],
       dtype=torch.float64)
	q_value: tensor([[-40.6315]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 32, step: 22
	action: tensor([[-0.1799, -0.5482,  0.5822,  1.1518, -0.2123,  0.3158,  0.3723]],
       dtype=torch.float64)
	q_value: tensor([[-34.6846]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.675634709395714, distance: 0.6517392567189965 entropy 0.8164090514183044
epoch: 32, step: 23
	action: tensor([[ 1.5044, -0.8038,  0.1406, -0.4868,  0.2446,  1.3706,  0.3753]],
       dtype=torch.float64)
	q_value: tensor([[-31.6251]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07033451044175365, distance: 1.1839039171548338 entropy 0.8164090514183044
epoch: 32, step: 24
	action: tensor([[ 1.0413, -1.0454,  0.1195, -0.6858,  1.8337,  1.2659,  0.7908]],
       dtype=torch.float64)
	q_value: tensor([[-38.5771]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5622226378691437, distance: 1.4303033527610272 entropy 0.8164090514183044
epoch: 32, step: 25
	action: tensor([[ 1.0832, -1.6313, -0.4352,  0.4945,  0.7114,  1.2215,  0.4404]],
       dtype=torch.float64)
	q_value: tensor([[-47.1855]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 32, step: 26
	action: tensor([[ 1.4774, -0.5741,  0.4459,  0.2922, -0.0314,  0.7360,  0.3314]],
       dtype=torch.float64)
	q_value: tensor([[-34.6846]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28546984481304616, distance: 0.9673128882565544 entropy 0.8164090514183044
epoch: 32, step: 27
	action: tensor([[ 0.8528, -2.0744, -0.4336, -0.4132,  1.4928,  0.7729,  1.1217]],
       dtype=torch.float64)
	q_value: tensor([[-33.9438]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 32, step: 28
	action: tensor([[ 0.8289,  0.2711,  0.4667,  0.1421,  0.0159, -0.6561,  0.8408]],
       dtype=torch.float64)
	q_value: tensor([[-34.6846]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 32, step: 29
	action: tensor([[-0.0526,  0.5074,  0.4042,  0.1891, -1.0447, -1.1055,  0.0109]],
       dtype=torch.float64)
	q_value: tensor([[-34.6846]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 32, step: 30
	action: tensor([[ 0.2620,  1.4020, -0.7960, -0.5354,  0.1848,  0.1865,  0.9660]],
       dtype=torch.float64)
	q_value: tensor([[-34.6846]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 32, step: 31
	action: tensor([[-0.7260, -0.3707,  0.2219, -0.7645,  0.0756,  0.9924, -0.0319]],
       dtype=torch.float64)
	q_value: tensor([[-34.6846]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8171432028757437, distance: 1.5425926943562562 entropy 0.8164090514183044
epoch: 32, step: 32
	action: tensor([[0.7581, 0.6991, 0.5772, 0.8239, 0.1200, 2.1880, 0.3508]],
       dtype=torch.float64)
	q_value: tensor([[-29.4670]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 32, step: 33
	action: tensor([[ 0.0400,  0.4617,  1.8066, -0.2352,  1.0068, -0.5412,  0.1258]],
       dtype=torch.float64)
	q_value: tensor([[-34.6846]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 32, step: 34
	action: tensor([[-0.3260,  0.1549, -0.6078, -0.3961,  0.6689,  0.0536,  0.3675]],
       dtype=torch.float64)
	q_value: tensor([[-34.6846]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.044915570083331136, distance: 1.118349571878537 entropy 0.8164090514183044
epoch: 32, step: 35
	action: tensor([[0.6587, 0.3866, 0.2438, 0.0956, 0.3963, 1.4950, 0.4456]],
       dtype=torch.float64)
	q_value: tensor([[-25.5937]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 32, step: 36
	action: tensor([[-0.6919, -0.5520,  0.0277, -0.3938,  0.4115, -0.3838, -0.4906]],
       dtype=torch.float64)
	q_value: tensor([[-34.6846]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.045708276096382, distance: 1.6367356589979862 entropy 0.8164090514183044
epoch: 32, step: 37
	action: tensor([[ 0.9564, -1.1698,  0.4107, -0.1962,  0.1764,  0.9641, -0.1062]],
       dtype=torch.float64)
	q_value: tensor([[-30.0202]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.29087724758886013, distance: 1.3001670823460652 entropy 0.8164090514183044
epoch: 32, step: 38
	action: tensor([[ 0.8462, -1.3410,  1.0075, -0.8041,  1.1273,  1.5706,  0.7280]],
       dtype=torch.float64)
	q_value: tensor([[-35.1108]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 32, step: 39
	action: tensor([[ 0.0265, -0.0965, -0.2271, -0.0024, -0.3353, -0.4961, -0.0262]],
       dtype=torch.float64)
	q_value: tensor([[-34.6846]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2575099673187967, distance: 0.9860569699661056 entropy 0.8164090514183044
epoch: 32, step: 40
	action: tensor([[ 0.4091, -1.4914, -0.2405, -0.2284,  0.5036,  0.3890,  0.9065]],
       dtype=torch.float64)
	q_value: tensor([[-23.1987]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.661001525991211, distance: 1.4748291369015274 entropy 0.8164090514183044
epoch: 32, step: 41
	action: tensor([[-0.0070, -0.4826, -0.5314, -1.1661,  0.8042,  1.5838,  1.5441]],
       dtype=torch.float64)
	q_value: tensor([[-33.8632]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30341649137634064, distance: 0.9550877778984361 entropy 0.8164090514183044
epoch: 32, step: 42
	action: tensor([[ 1.2122, -1.2024,  0.0241, -0.3786,  1.6224,  2.4108,  0.4479]],
       dtype=torch.float64)
	q_value: tensor([[-43.2602]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 32, step: 43
	action: tensor([[ 0.7528, -0.4110,  0.7427, -0.4064,  0.2419,  0.7150,  0.2631]],
       dtype=torch.float64)
	q_value: tensor([[-34.6846]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3460548430222071, distance: 0.925395444351328 entropy 0.8164090514183044
epoch: 32, step: 44
	action: tensor([[ 0.6359, -1.7508,  0.3797, -1.7772,  1.1568,  1.1888,  0.2643]],
       dtype=torch.float64)
	q_value: tensor([[-28.9151]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 32, step: 45
	action: tensor([[ 1.3316,  0.8404, -0.0164,  0.3725, -0.1773, -0.3352, -0.4496]],
       dtype=torch.float64)
	q_value: tensor([[-34.6846]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 32, step: 46
	action: tensor([[-0.4979,  0.8752, -0.3374, -0.0919,  0.0086,  0.0213,  0.5292]],
       dtype=torch.float64)
	q_value: tensor([[-34.6846]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 32, step: 47
	action: tensor([[-0.2725,  0.2753, -0.7964,  0.0921, -0.1341,  0.4364,  0.4929]],
       dtype=torch.float64)
	q_value: tensor([[-34.6846]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 32, step: 48
	action: tensor([[ 0.6170,  0.2937,  0.6235, -0.8054, -0.0216,  1.0180, -0.3327]],
       dtype=torch.float64)
	q_value: tensor([[-34.6846]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7948144082600546, distance: 0.5183583798745423 entropy 0.8164090514183044
epoch: 32, step: 49
	action: tensor([[ 1.0474, -1.3870,  0.5107, -0.6119,  0.7872,  1.6130,  0.2645]],
       dtype=torch.float64)
	q_value: tensor([[-29.3452]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4134578327949061, distance: 1.3604986704469564 entropy 0.8164090514183044
epoch: 32, step: 50
	action: tensor([[ 1.3728, -1.4270, -0.0536,  0.0032,  0.5501,  1.8261,  0.2340]],
       dtype=torch.float64)
	q_value: tensor([[-42.3452]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 32, step: 51
	action: tensor([[ 0.4015,  0.1505, -0.5984, -1.0729,  0.0784,  0.3480,  0.4467]],
       dtype=torch.float64)
	q_value: tensor([[-34.6846]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 32, step: 52
	action: tensor([[-0.4181,  0.0477,  0.3311, -0.5910,  0.0387, -0.1854,  0.0506]],
       dtype=torch.float64)
	q_value: tensor([[-34.6846]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5275844355789028, distance: 1.4143578513278383 entropy 0.8164090514183044
epoch: 32, step: 53
	action: tensor([[ 0.5516, -0.3275,  0.3372,  0.5100, -0.1969,  0.9703,  0.4506]],
       dtype=torch.float64)
	q_value: tensor([[-23.5215]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9138135344029915, distance: 0.33595122487480555 entropy 0.8164090514183044
epoch: 32, step: 54
	action: tensor([[ 0.3498, -1.1370,  0.2021, -0.4480,  0.6032,  0.3961,  0.7085]],
       dtype=torch.float64)
	q_value: tensor([[-27.9625]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7324243493431615, distance: 1.5062041256324978 entropy 0.8164090514183044
epoch: 32, step: 55
	action: tensor([[ 1.2019, -0.7495,  0.1083, -0.6475, -0.4618,  0.1194,  1.2921]],
       dtype=torch.float64)
	q_value: tensor([[-32.2022]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.44598346077539075, distance: 1.3760631218503903 entropy 0.8164090514183044
epoch: 32, step: 56
	action: tensor([[ 0.5772, -0.9929, -0.1002, -1.0516,  0.6472,  0.8873,  0.3297]],
       dtype=torch.float64)
	q_value: tensor([[-35.1329]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5599625626847247, distance: 1.4292683648425488 entropy 0.8164090514183044
epoch: 32, step: 57
	action: tensor([[ 1.6632, -0.7932,  0.5097, -1.2046,  1.1701,  1.1738,  0.9611]],
       dtype=torch.float64)
	q_value: tensor([[-35.9210]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5657508170375163, distance: 1.431917565857443 entropy 0.8164090514183044
epoch: 32, step: 58
	action: tensor([[ 1.4380, -1.0584,  0.1762, -1.8530,  1.0409,  2.0247,  2.8974]],
       dtype=torch.float64)
	q_value: tensor([[-44.9527]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.33777609974076905, distance: 1.323574555426669 entropy 0.8164090514183044
epoch: 32, step: 59
	action: tensor([[ 1.6251, -2.1312,  0.4756, -0.7984,  1.4971,  2.8856,  1.8183]],
       dtype=torch.float64)
	q_value: tensor([[-62.1428]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 32, step: 60
	action: tensor([[-0.0508,  0.9164, -0.7304,  0.4082,  0.5648,  0.1393,  0.0395]],
       dtype=torch.float64)
	q_value: tensor([[-34.6846]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 32, step: 61
	action: tensor([[-0.0724,  0.7329,  0.9096,  0.0340, -0.5450,  1.3422,  0.3630]],
       dtype=torch.float64)
	q_value: tensor([[-34.6846]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 32, step: 62
	action: tensor([[ 0.9423, -0.2636,  0.9856, -0.3009, -0.0273,  0.6000,  0.0170]],
       dtype=torch.float64)
	q_value: tensor([[-34.6846]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5509169143693919, distance: 0.7668669872181024 entropy 0.8164090514183044
epoch: 32, step: 63
	action: tensor([[ 1.4700, -0.3176,  0.2486, -0.5742, -0.0132,  2.1221,  1.2247]],
       dtype=torch.float64)
	q_value: tensor([[-29.5102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5237535726232851, distance: 0.7897189486262167 entropy 0.8164090514183044
epoch: 32, step: 64
	action: tensor([[ 1.4356, -1.5672,  0.0371, -0.1620,  0.2243,  1.7050,  0.5236]],
       dtype=torch.float64)
	q_value: tensor([[-44.4880]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 32, step: 65
	action: tensor([[-0.6502, -0.7616,  1.1044, -0.0443, -0.3539, -0.4608, -0.0082]],
       dtype=torch.float64)
	q_value: tensor([[-34.6846]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1241902229834078, distance: 1.6678362007494953 entropy 0.8164090514183044
epoch: 32, step: 66
	action: tensor([[ 0.8647, -1.7598, -0.1685, -1.9145, -0.3717,  1.5916,  0.7249]],
       dtype=torch.float64)
	q_value: tensor([[-31.9600]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 32, step: 67
	action: tensor([[-0.7573,  0.8415,  0.2729, -0.5290, -1.4984,  0.2963, -0.7632]],
       dtype=torch.float64)
	q_value: tensor([[-34.6846]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3835956206924811, distance: 1.3460502797686917 entropy 0.8164090514183044
epoch: 32, step: 68
	action: tensor([[ 0.6538, -0.5960, -0.1198, -0.4523,  0.7868, -0.1581,  1.0156]],
       dtype=torch.float64)
	q_value: tensor([[-33.3178]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20931265348730066, distance: 1.25842110086976 entropy 0.8164090514183044
epoch: 32, step: 69
	action: tensor([[-0.3551, -0.9227, -0.3029, -1.1186, -0.1238,  1.4760,  0.3473]],
       dtype=torch.float64)
	q_value: tensor([[-32.2570]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5686167773842621, distance: 1.4332274622731074 entropy 0.8164090514183044
epoch: 32, step: 70
	action: tensor([[-0.1329, -0.5513,  0.1070, -0.6410,  0.3106,  0.9680,  0.7568]],
       dtype=torch.float64)
	q_value: tensor([[-36.6131]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2258571139393506, distance: 1.267000012377882 entropy 0.8164090514183044
epoch: 32, step: 71
	action: tensor([[ 0.5360, -0.0227,  0.9759, -0.1949,  0.6598,  0.9216,  0.6977]],
       dtype=torch.float64)
	q_value: tensor([[-29.9839]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7085034439620228, distance: 0.617836280489883 entropy 0.8164090514183044
epoch: 32, step: 72
	action: tensor([[ 1.4637, -0.9722, -0.9680, -0.6112,  0.1461,  0.9554,  0.5863]],
       dtype=torch.float64)
	q_value: tensor([[-32.3132]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4323135484600962, distance: 1.3695432231891105 entropy 0.8164090514183044
epoch: 32, step: 73
	action: tensor([[ 1.7064, -0.8494, -0.1088, -0.5933,  1.6229,  1.2792,  0.1371]],
       dtype=torch.float64)
	q_value: tensor([[-39.5537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 32, step: 74
	action: tensor([[-0.2962, -0.7038, -0.1385,  0.2231, -0.2462, -0.4181, -0.9203]],
       dtype=torch.float64)
	q_value: tensor([[-34.6846]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5105265616205144, distance: 1.40643892159246 entropy 0.8164090514183044
epoch: 32, step: 75
	action: tensor([[ 1.6799, -1.6366, -0.0611, -1.6042,  0.6948,  0.5772,  0.7169]],
       dtype=torch.float64)
	q_value: tensor([[-32.3675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 32, step: 76
	action: tensor([[ 0.2210, -0.5820, -0.3306, -0.4179, -1.2756,  0.0818,  1.3802]],
       dtype=torch.float64)
	q_value: tensor([[-34.6846]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23775731242685239, distance: 1.2731349591353003 entropy 0.8164090514183044
epoch: 32, step: 77
	action: tensor([[ 0.2773, -1.7166, -0.0535, -1.0780,  1.0604,  1.7003,  0.5099]],
       dtype=torch.float64)
	q_value: tensor([[-33.6796]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 32, step: 78
	action: tensor([[ 0.6627,  0.0651, -0.1719, -0.8045,  0.3399,  0.5253, -0.2635]],
       dtype=torch.float64)
	q_value: tensor([[-34.6846]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 32, step: 79
	action: tensor([[ 0.9398, -1.0270,  0.2374, -0.7292, -0.8245,  1.0893, -0.5414]],
       dtype=torch.float64)
	q_value: tensor([[-34.6846]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3438267035693723, distance: 1.3265643645595844 entropy 0.8164090514183044
epoch: 32, step: 80
	action: tensor([[ 0.4769, -0.8516,  0.1117, -0.7963,  0.9661,  1.9167,  1.2934]],
       dtype=torch.float64)
	q_value: tensor([[-36.0403]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18860926752970364, distance: 1.0307936031411709 entropy 0.8164090514183044
epoch: 32, step: 81
	action: tensor([[ 1.1842, -0.4739,  0.1057, -0.5832,  0.2269,  0.9701,  0.6048]],
       dtype=torch.float64)
	q_value: tensor([[-43.7890]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2141013546539675, distance: 1.0144717631349995 entropy 0.8164090514183044
epoch: 32, step: 82
	action: tensor([[ 2.2246, -0.5137,  0.9155, -1.3190,  1.0053,  0.9227,  0.3684]],
       dtype=torch.float64)
	q_value: tensor([[-33.2512]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 32, step: 83
	action: tensor([[ 0.2636, -0.0948,  0.1160,  0.3645, -0.2151, -0.1222,  0.4135]],
       dtype=torch.float64)
	q_value: tensor([[-34.6846]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6134770297370566, distance: 0.711449940510957 entropy 0.8164090514183044
epoch: 32, step: 84
	action: tensor([[ 1.3061,  0.2255, -0.7137, -0.8061,  0.4029,  0.3040, -0.0419]],
       dtype=torch.float64)
	q_value: tensor([[-23.6814]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3863076242722737, distance: 0.8964623553187755 entropy 0.8164090514183044
epoch: 32, step: 85
	action: tensor([[ 1.7380, -0.4887,  0.3292, -0.3482,  0.1270,  1.3514,  0.1699]],
       dtype=torch.float64)
	q_value: tensor([[-30.5679]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 32, step: 86
	action: tensor([[-1.1830, -1.1853, -0.7374, -0.5586, -0.2781,  0.4596, -0.7890]],
       dtype=torch.float64)
	q_value: tensor([[-34.6846]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1884735942005542, distance: 1.692884578245661 entropy 0.8164090514183044
epoch: 32, step: 87
	action: tensor([[ 1.1681, -0.5252, -0.1154, -0.0046,  0.5676,  0.8221,  0.1273]],
       dtype=torch.float64)
	q_value: tensor([[-37.0021]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4350693622136983, distance: 0.860110515876767 entropy 0.8164090514183044
epoch: 32, step: 88
	action: tensor([[ 0.7910, -1.0540, -0.1405,  0.0454,  1.5216,  1.2174,  1.1554]],
       dtype=torch.float64)
	q_value: tensor([[-32.4009]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.043155197049238314, distance: 1.1687756534380698 entropy 0.8164090514183044
epoch: 32, step: 89
	action: tensor([[ 2.2886, -1.8151, -0.1966, -1.0757,  1.2381,  1.3025,  1.6952]],
       dtype=torch.float64)
	q_value: tensor([[-43.7132]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 32, step: 90
	action: tensor([[ 0.2710, -0.0971,  1.2226, -0.4605, -0.0661, -0.8107,  0.5024]],
       dtype=torch.float64)
	q_value: tensor([[-34.6846]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.025139212855960325, distance: 1.129868742300204 entropy 0.8164090514183044
epoch: 32, step: 91
	action: tensor([[ 1.7598, -0.6410,  0.1636, -0.1753,  0.6974,  0.8458,  0.8094]],
       dtype=torch.float64)
	q_value: tensor([[-31.5979]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.22944616464285317, distance: 1.268853410989423 entropy 0.8164090514183044
epoch: 32, step: 92
	action: tensor([[ 0.6916, -2.7417,  0.1642, -1.6656,  1.5901,  1.3350,  0.5437]],
       dtype=torch.float64)
	q_value: tensor([[-38.7726]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 32, step: 93
	action: tensor([[-0.0713,  0.5718,  0.3229, -0.9980,  0.1716, -0.1004, -0.1905]],
       dtype=torch.float64)
	q_value: tensor([[-34.6846]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10101282092769337, distance: 1.0850092540689609 entropy 0.8164090514183044
epoch: 32, step: 94
	action: tensor([[-0.1626, -0.8719,  0.3365, -0.3807,  0.3214,  0.0795,  1.3610]],
       dtype=torch.float64)
	q_value: tensor([[-25.8933]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8085014276813405, distance: 1.5389202742945642 entropy 0.8164090514183044
epoch: 32, step: 95
	action: tensor([[ 1.3219, -2.0387,  0.8318, -0.0274,  0.8828,  0.9788,  0.2671]],
       dtype=torch.float64)
	q_value: tensor([[-31.4751]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 32, step: 96
	action: tensor([[-0.3617,  0.8059, -0.0869, -0.3072,  0.2756,  0.1165,  0.5316]],
       dtype=torch.float64)
	q_value: tensor([[-34.6846]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 32, step: 97
	action: tensor([[-0.7345,  0.2280, -0.1548, -0.9271,  0.4126,  0.2466,  0.9759]],
       dtype=torch.float64)
	q_value: tensor([[-34.6846]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6163078310268852, distance: 1.4548517199544677 entropy 0.8164090514183044
epoch: 32, step: 98
	action: tensor([[-0.4885,  0.6066, -1.3484, -0.0310,  0.5576,  0.1892,  0.7033]],
       dtype=torch.float64)
	q_value: tensor([[-29.9299]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 32, step: 99
	action: tensor([[-0.0859, -0.5210, -1.0394, -0.4674,  0.3223,  1.2133,  0.5907]],
       dtype=torch.float64)
	q_value: tensor([[-34.6846]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.02217188392002256, distance: 1.1315870106213655 entropy 0.8164090514183044
epoch: 32, step: 100
	action: tensor([[ 1.0019, -0.5099, -1.1657, -1.1100,  0.5485,  0.2620,  1.0398]],
       dtype=torch.float64)
	q_value: tensor([[-33.0909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.043949544693869136, distance: 1.1692205716601105 entropy 0.8164090514183044
epoch: 32, step: 101
	action: tensor([[ 0.0181, -0.9623,  0.6207, -0.3223,  0.5769,  0.7004,  0.4896]],
       dtype=torch.float64)
	q_value: tensor([[-38.3918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5400647514712351, distance: 1.420123727691207 entropy 0.8164090514183044
epoch: 32, step: 102
	action: tensor([[ 0.3455, -1.0892, -0.3115, -0.3881,  0.7501,  2.0626,  0.7124]],
       dtype=torch.float64)
	q_value: tensor([[-30.5525]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23394353053710049, distance: 1.0015833251396298 entropy 0.8164090514183044
epoch: 32, step: 103
	action: tensor([[ 0.9875, -1.0216, -0.8243, -0.2638, -0.1355,  0.7984,  0.2485]],
       dtype=torch.float64)
	q_value: tensor([[-41.9553]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2616398396449837, distance: 1.2853588453392966 entropy 0.8164090514183044
epoch: 32, step: 104
	action: tensor([[-0.0408, -0.6346,  0.0132, -0.7625,  1.0634,  0.1144,  0.1265]],
       dtype=torch.float64)
	q_value: tensor([[-33.4759]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6115829633939212, distance: 1.4527237179075538 entropy 0.8164090514183044
epoch: 32, step: 105
	action: tensor([[ 0.0634, -0.5313, -0.0368, -0.6851,  1.1405,  1.3095,  0.8052]],
       dtype=torch.float64)
	q_value: tensor([[-32.1186]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12711011437693087, distance: 1.0691445436737943 entropy 0.8164090514183044
epoch: 32, step: 106
	action: tensor([[ 0.0845, -1.5729, -0.5133,  0.0345,  0.5711,  1.7619,  0.6455]],
       dtype=torch.float64)
	q_value: tensor([[-36.6421]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 32, step: 107
	action: tensor([[-0.1508,  0.2063,  0.5047,  0.4659,  1.0033, -0.0255,  0.3131]],
       dtype=torch.float64)
	q_value: tensor([[-34.6846]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 32, step: 108
	action: tensor([[ 0.8200,  0.3429, -0.2973,  0.4646,  0.5462,  0.9004,  0.6278]],
       dtype=torch.float64)
	q_value: tensor([[-34.6846]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 32, step: 109
	action: tensor([[-0.0684, -0.2506,  0.6045, -1.4216,  0.4698,  0.4041,  0.2764]],
       dtype=torch.float64)
	q_value: tensor([[-34.6846]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6255493020620482, distance: 1.459004953062241 entropy 0.8164090514183044
epoch: 32, step: 110
	action: tensor([[ 1.7787, -1.0649, -0.2592, -0.5865, -0.0313,  0.6519,  0.4869]],
       dtype=torch.float64)
	q_value: tensor([[-31.1487]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7258838541821471, distance: 1.5033582188489087 entropy 0.8164090514183044
epoch: 32, step: 111
	action: tensor([[ 2.0861, -1.0944,  0.1666,  0.0689,  0.9251,  0.8002,  0.3038]],
       dtype=torch.float64)
	q_value: tensor([[-37.1702]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 32, step: 112
	action: tensor([[-0.4788, -0.8478, -0.5753, -0.5197, -0.2155,  0.3003,  0.1381]],
       dtype=torch.float64)
	q_value: tensor([[-34.6846]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7794868386708023, distance: 1.5265255702413014 entropy 0.8164090514183044
epoch: 32, step: 113
	action: tensor([[-0.7626, -0.2959,  0.1496, -0.9820, -0.5862,  0.9409,  1.4066]],
       dtype=torch.float64)
	q_value: tensor([[-28.0737]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0545290325936154, distance: 1.640260530450193 entropy 0.8164090514183044
epoch: 32, step: 114
	action: tensor([[ 0.5099, -0.9349,  0.4771, -0.4014,  0.7431,  1.0978,  0.2097]],
       dtype=torch.float64)
	q_value: tensor([[-35.3847]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17913841968301703, distance: 1.242622143153677 entropy 0.8164090514183044
epoch: 32, step: 115
	action: tensor([[ 0.6697, -0.7991,  0.3175, -0.2094,  1.0854,  1.5732,  0.1090]],
       dtype=torch.float64)
	q_value: tensor([[-34.5714]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20367208876729004, distance: 1.0211808506811273 entropy 0.8164090514183044
epoch: 32, step: 116
	action: tensor([[ 0.6689, -1.2035, -0.1946, -0.4177,  0.4027,  2.0208,  1.5222]],
       dtype=torch.float64)
	q_value: tensor([[-39.1856]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10432894227512735, distance: 1.0830062519122867 entropy 0.8164090514183044
epoch: 32, step: 117
	action: tensor([[ 1.0912, -1.1136,  0.1152, -0.8204,  0.7726,  1.8682,  0.2598]],
       dtype=torch.float64)
	q_value: tensor([[-44.1399]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1898543567461095, distance: 1.2482558091094949 entropy 0.8164090514183044
epoch: 32, step: 118
	action: tensor([[ 1.5836, -1.6664,  0.6238, -0.2026,  1.4651,  2.0622,  1.0940]],
       dtype=torch.float64)
	q_value: tensor([[-44.0352]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 32, step: 119
	action: tensor([[-1.0469,  0.1891,  0.0843,  0.3946,  0.0926,  0.3385,  0.6877]],
       dtype=torch.float64)
	q_value: tensor([[-34.6846]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4557332541775134, distance: 1.3806944997158215 entropy 0.8164090514183044
epoch: 32, step: 120
	action: tensor([[ 0.2265, -0.8543, -0.7423, -1.2854, -0.0657,  0.3777,  1.2130]],
       dtype=torch.float64)
	q_value: tensor([[-25.7125]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3543225684636757, distance: 1.3317348076485143 entropy 0.8164090514183044
epoch: 32, step: 121
	action: tensor([[ 0.4869, -1.0097, -0.6505,  0.3875,  1.1631,  1.2117,  0.7940]],
       dtype=torch.float64)
	q_value: tensor([[-35.3269]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3091354721961671, distance: 0.95115904146348 entropy 0.8164090514183044
epoch: 32, step: 122
	action: tensor([[ 0.0947, -0.4544,  0.3290, -0.5859,  0.4326,  1.1760, -0.4504]],
       dtype=torch.float64)
	q_value: tensor([[-39.1587]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12641329082411834, distance: 1.0695712048641308 entropy 0.8164090514183044
epoch: 32, step: 123
	action: tensor([[ 1.2707, -1.0235,  0.5080, -0.9095,  0.9379,  1.0308,  1.0058]],
       dtype=torch.float64)
	q_value: tensor([[-30.9948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6616174015073528, distance: 1.4751025342946427 entropy 0.8164090514183044
epoch: 32, step: 124
	action: tensor([[ 1.7449, -2.0641,  0.1320, -0.1666,  1.4823,  2.2359,  1.0821]],
       dtype=torch.float64)
	q_value: tensor([[-41.5540]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 32, step: 125
	action: tensor([[ 0.9472,  0.1642, -0.7462,  0.2238,  0.6830, -0.6275,  0.4798]],
       dtype=torch.float64)
	q_value: tensor([[-34.6846]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8120710770963463, distance: 0.4960820415267249 entropy 0.8164090514183044
epoch: 32, step: 126
	action: tensor([[-0.5468, -0.8629,  0.9161, -0.2733,  0.8779,  2.0164, -0.2245]],
       dtype=torch.float64)
	q_value: tensor([[-31.5550]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06447312177855347, distance: 1.1806578070148623 entropy 0.8164090514183044
epoch: 32, step: 127
	action: tensor([[ 1.1072, -1.5128,  0.5147, -0.5368,  0.8479,  1.8969,  1.1714]],
       dtype=torch.float64)
	q_value: tensor([[-43.2162]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
LOSS epoch 32 actor 358.5362108479599 critic 213.3841911343507 
epoch: 33, step: 0
	action: tensor([[ 0.6575, -0.3852,  0.3329, -0.3007,  0.0746,  1.0428,  0.2869]],
       dtype=torch.float64)
	q_value: tensor([[-35.7305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5621692205323674, distance: 0.7571986619629785 entropy 0.7110485434532166
epoch: 33, step: 1
	action: tensor([[ 2.5762, -3.1858,  0.3592, -0.2528,  1.9329,  4.0275,  1.4488]],
       dtype=torch.float64)
	q_value: tensor([[-29.6274]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 33, step: 2
	action: tensor([[ 1.2514, -0.9032,  0.3694, -0.2196,  0.0881,  0.9925, -0.4061]],
       dtype=torch.float64)
	q_value: tensor([[-35.7305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0680350100867293, distance: 1.1826314872675714 entropy 0.7110485434532166
epoch: 33, step: 3
	action: tensor([[ 3.5016, -4.4953,  1.0133, -1.6724,  2.8796,  4.8947,  2.0586]],
       dtype=torch.float64)
	q_value: tensor([[-36.5524]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 33, step: 4
	action: tensor([[-0.2258, -0.3387,  0.1164,  0.1537,  0.3797,  0.9485,  0.4029]],
       dtype=torch.float64)
	q_value: tensor([[-35.7305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2858071290873532, distance: 0.9670845578398117 entropy 0.7110485434532166
epoch: 33, step: 5
	action: tensor([[ 1.2534, -2.4133,  0.2840, -0.8518,  1.0195,  2.1774,  0.9536]],
       dtype=torch.float64)
	q_value: tensor([[-27.2960]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 33, step: 6
	action: tensor([[ 0.4286, -0.2429,  1.4331, -0.4413, -0.3630,  0.6187,  0.3984]],
       dtype=torch.float64)
	q_value: tensor([[-35.7305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4654601763561709, distance: 0.8366556077395548 entropy 0.7110485434532166
epoch: 33, step: 7
	action: tensor([[ 3.4844, -4.2641,  0.6417, -1.4297,  2.1884,  5.8946,  2.5445]],
       dtype=torch.float64)
	q_value: tensor([[-32.0029]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 33, step: 8
	action: tensor([[ 0.1580, -0.0727,  0.1961, -0.4644, -0.1091,  0.7004,  0.5717]],
       dtype=torch.float64)
	q_value: tensor([[-35.7305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3302278956196074, distance: 0.936526824112771 entropy 0.7110485434532166
epoch: 33, step: 9
	action: tensor([[ 1.6513, -0.8395,  1.0484,  0.2038,  1.8268,  3.4290,  1.4218]],
       dtype=torch.float64)
	q_value: tensor([[-26.3100]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 33, step: 10
	action: tensor([[ 0.2137, -0.3522, -0.2956,  0.4231,  0.9973, -0.0555,  0.2056]],
       dtype=torch.float64)
	q_value: tensor([[-35.7305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3588289330992255, distance: 0.9163125852071302 entropy 0.7110485434532166
epoch: 33, step: 11
	action: tensor([[ 1.9152, -2.0006, -0.0462, -0.2509,  0.3403,  1.5375,  0.1135]],
       dtype=torch.float64)
	q_value: tensor([[-30.2442]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 33, step: 12
	action: tensor([[ 0.3778,  0.2466,  0.0420, -0.1253,  0.0600,  0.5286,  0.3910]],
       dtype=torch.float64)
	q_value: tensor([[-35.7305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 33, step: 13
	action: tensor([[ 0.8065, -0.5905,  0.8711,  0.2778,  0.8675,  1.1550,  0.0259]],
       dtype=torch.float64)
	q_value: tensor([[-35.7305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32602886860284297, distance: 0.9394579383767326 entropy 0.7110485434532166
epoch: 33, step: 14
	action: tensor([[ 4.6133, -3.2824,  1.1825, -1.7547,  2.1886,  6.1800,  2.5017]],
       dtype=torch.float64)
	q_value: tensor([[-37.4963]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 33, step: 15
	action: tensor([[ 0.4474, -0.3253,  0.7046,  0.1361,  0.5834,  0.1666,  0.5449]],
       dtype=torch.float64)
	q_value: tensor([[-35.7305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5169226863343808, distance: 0.7953623235960722 entropy 0.7110485434532166
epoch: 33, step: 16
	action: tensor([[ 3.2783, -2.9185,  0.8170, -0.9160,  2.6487,  3.6346,  1.0058]],
       dtype=torch.float64)
	q_value: tensor([[-29.0542]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 33, step: 17
	action: tensor([[ 0.0373,  0.1085,  0.1201, -0.7643, -0.0290,  0.8666, -0.2032]],
       dtype=torch.float64)
	q_value: tensor([[-35.7305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2777497859024811, distance: 0.9725244595634907 entropy 0.7110485434532166
epoch: 33, step: 18
	action: tensor([[ 1.7240, -2.3524,  0.2393, -0.8357,  2.0432,  2.4275,  1.1317]],
       dtype=torch.float64)
	q_value: tensor([[-26.9284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 33, step: 19
	action: tensor([[ 0.3295, -0.6759,  0.5664,  0.1006,  0.8121,  0.5654,  0.8673]],
       dtype=torch.float64)
	q_value: tensor([[-35.7305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1996270682880953, distance: 1.0237711562024978 entropy 0.7110485434532166
epoch: 33, step: 20
	action: tensor([[ 3.3557, -2.6411,  0.6056, -0.9900,  1.3888,  3.9411,  1.0727]],
       dtype=torch.float64)
	q_value: tensor([[-32.6655]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 33, step: 21
	action: tensor([[ 0.0160, -1.2153,  0.1046, -0.2582, -0.9614,  1.0288, -0.4419]],
       dtype=torch.float64)
	q_value: tensor([[-35.7305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7450755006983087, distance: 1.5116937029407864 entropy 0.7110485434532166
epoch: 33, step: 22
	action: tensor([[ 3.0873, -4.2580,  0.1738, -0.3979,  2.8693,  3.3542,  1.2678]],
       dtype=torch.float64)
	q_value: tensor([[-36.2186]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 33, step: 23
	action: tensor([[ 0.6713, -0.5613,  0.2145, -0.5382, -0.1613,  0.9157, -0.0322]],
       dtype=torch.float64)
	q_value: tensor([[-35.7305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16281916018380116, distance: 1.0470473775908866 entropy 0.7110485434532166
epoch: 33, step: 24
	action: tensor([[ 2.2614, -3.0070,  0.4438, -1.4944,  1.3039,  3.3302,  1.4597]],
       dtype=torch.float64)
	q_value: tensor([[-29.6009]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 33, step: 25
	action: tensor([[ 0.7486, -0.3542,  0.1265,  0.2283,  0.3961,  0.3356,  0.7174]],
       dtype=torch.float64)
	q_value: tensor([[-35.7305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6811451885844944, distance: 0.6461795069929632 entropy 0.7110485434532166
epoch: 33, step: 26
	action: tensor([[ 3.0275, -2.9334,  0.9015, -0.9813,  1.8807,  3.4537,  1.6827]],
       dtype=torch.float64)
	q_value: tensor([[-29.2506]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 33, step: 27
	action: tensor([[ 0.4225, -0.4483, -0.7174,  0.2109, -0.0290,  1.3616,  0.1577]],
       dtype=torch.float64)
	q_value: tensor([[-35.7305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5136617338226952, distance: 0.7980423050368062 entropy 0.7110485434532166
epoch: 33, step: 28
	action: tensor([[ 1.5236, -3.2770,  0.6361, -0.7167,  1.8320,  3.1018,  0.8806]],
       dtype=torch.float64)
	q_value: tensor([[-30.4414]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 33, step: 29
	action: tensor([[-0.7029, -0.8412, -0.4503,  0.4035,  1.2164, -0.0853, -0.3025]],
       dtype=torch.float64)
	q_value: tensor([[-35.7305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0921306647928786, distance: 1.6552023558982165 entropy 0.7110485434532166
epoch: 33, step: 30
	action: tensor([[ 2.0519, -1.4287, -0.2377, -1.0637,  0.1173,  2.4312,  0.9994]],
       dtype=torch.float64)
	q_value: tensor([[-36.0392]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 33, step: 31
	action: tensor([[ 0.7773, -1.5490,  0.4643, -0.1181, -0.5997,  1.0613,  0.5582]],
       dtype=torch.float64)
	q_value: tensor([[-35.7305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16259450610885273, distance: 1.233874038366208 entropy 0.7110485434532166
epoch: 33, step: 32
	action: tensor([[ 3.2947, -4.1680,  1.1292, -1.3011,  2.9484,  4.4270,  2.1589]],
       dtype=torch.float64)
	q_value: tensor([[-34.6848]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 33, step: 33
	action: tensor([[-0.3018, -0.3674, -0.2995,  0.3615,  0.2978,  1.2685,  0.4827]],
       dtype=torch.float64)
	q_value: tensor([[-35.7305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.234524748314062, distance: 1.0012032953912573 entropy 0.7110485434532166
epoch: 33, step: 34
	action: tensor([[ 1.9652, -2.1460,  0.5777, -0.5666,  1.6038,  3.2788,  1.4675]],
       dtype=torch.float64)
	q_value: tensor([[-29.2601]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 33, step: 35
	action: tensor([[ 0.8207, -1.3420,  0.7610, -0.3661, -0.0832,  0.5910, -0.3986]],
       dtype=torch.float64)
	q_value: tensor([[-35.7305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6163401798580648, distance: 1.4548662786036 entropy 0.7110485434532166
epoch: 33, step: 36
	action: tensor([[ 4.3292, -3.7620,  0.2747, -0.8532,  3.3665,  5.1522,  1.7401]],
       dtype=torch.float64)
	q_value: tensor([[-35.9721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 33, step: 37
	action: tensor([[ 0.7488, -0.0457,  0.0689, -0.7990,  0.8460, -0.0393,  0.1986]],
       dtype=torch.float64)
	q_value: tensor([[-35.7305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2579503196674864, distance: 0.9857645236821116 entropy 0.7110485434532166
epoch: 33, step: 38
	action: tensor([[ 2.7276, -3.0795,  0.7586, -1.0585,  1.2842,  3.2362,  1.0274]],
       dtype=torch.float64)
	q_value: tensor([[-30.2978]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 33, step: 39
	action: tensor([[ 0.2000, -0.1129,  0.5883,  0.0887, -0.0120,  1.3796,  1.0194]],
       dtype=torch.float64)
	q_value: tensor([[-35.7305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 33, step: 40
	action: tensor([[ 0.4082,  0.1830,  0.0913,  0.5278,  0.5490,  1.2298, -0.1584]],
       dtype=torch.float64)
	q_value: tensor([[-35.7305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 33, step: 41
	action: tensor([[ 0.9479,  0.3011,  0.1752, -0.0684,  0.4523,  0.5832,  0.5957]],
       dtype=torch.float64)
	q_value: tensor([[-35.7305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 33, step: 42
	action: tensor([[ 0.2371, -0.2359,  0.9048, -0.0096,  0.4924,  0.6845,  0.2481]],
       dtype=torch.float64)
	q_value: tensor([[-35.7305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5225164481104165, distance: 0.7907439925648537 entropy 0.7110485434532166
epoch: 33, step: 43
	action: tensor([[ 2.7911, -2.4479,  0.1647, -0.7712,  1.8234,  4.3686,  0.8294]],
       dtype=torch.float64)
	q_value: tensor([[-29.3567]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 33, step: 44
	action: tensor([[-0.0097, -0.9815,  0.2981, -0.7679,  0.6181,  0.4426,  0.3334]],
       dtype=torch.float64)
	q_value: tensor([[-35.7305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8528626592544966, distance: 1.5576802296749306 entropy 0.7110485434532166
epoch: 33, step: 45
	action: tensor([[ 2.5638, -2.2980,  0.8400, -0.9895,  0.7576,  4.0613,  1.4675]],
       dtype=torch.float64)
	q_value: tensor([[-31.4754]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 33, step: 46
	action: tensor([[-0.2703, -0.3197, -0.5633,  0.2610,  0.0162,  0.9736, -0.1036]],
       dtype=torch.float64)
	q_value: tensor([[-35.7305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04255686594071206, distance: 1.1684404132259576 entropy 0.7110485434532166
epoch: 33, step: 47
	action: tensor([[ 1.2521, -2.1018,  0.2204, -0.3197,  0.5418,  2.2255,  1.6895]],
       dtype=torch.float64)
	q_value: tensor([[-25.3794]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 33, step: 48
	action: tensor([[ 0.7772, -0.9096, -0.4220,  0.3464, -0.0706,  0.5724,  1.3451]],
       dtype=torch.float64)
	q_value: tensor([[-35.7305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2751787578589937, distance: 0.9742538923657519 entropy 0.7110485434532166
epoch: 33, step: 49
	action: tensor([[ 2.7583, -3.5889,  1.6006, -0.7062,  1.2860,  3.7779,  1.8072]],
       dtype=torch.float64)
	q_value: tensor([[-35.3261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 33, step: 50
	action: tensor([[ 1.1073,  0.1583,  0.2690,  0.2690, -0.2562, -0.3036,  0.4583]],
       dtype=torch.float64)
	q_value: tensor([[-35.7305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 33, step: 51
	action: tensor([[-0.0100, -0.6581,  0.9538, -1.0416,  0.8879,  0.9783,  0.6677]],
       dtype=torch.float64)
	q_value: tensor([[-35.7305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5986519654257845, distance: 1.4468838071319159 entropy 0.7110485434532166
epoch: 33, step: 52
	action: tensor([[ 3.1754, -3.9876,  0.8900, -0.5992,  2.8840,  4.9064,  2.1916]],
       dtype=torch.float64)
	q_value: tensor([[-36.3074]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 33, step: 53
	action: tensor([[ 1.2060, -1.0781,  0.5182, -0.5408, -0.2724,  0.9468,  0.4090]],
       dtype=torch.float64)
	q_value: tensor([[-35.7305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.326383366381366, distance: 1.3179266097224647 entropy 0.7110485434532166
epoch: 33, step: 54
	action: tensor([[ 4.7514, -3.4629,  1.4966, -0.7023,  2.8066,  5.7869,  1.8361]],
       dtype=torch.float64)
	q_value: tensor([[-35.6439]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 33, step: 55
	action: tensor([[ 0.0059, -0.5697,  1.0452,  0.1280,  1.0868,  0.1703, -0.6897]],
       dtype=torch.float64)
	q_value: tensor([[-35.7305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04996606722549024, distance: 1.1725849753732553 entropy 0.7110485434532166
epoch: 33, step: 56
	action: tensor([[ 2.6292, -3.8234,  0.3373, -1.2576,  3.4088,  4.1458,  1.9536]],
       dtype=torch.float64)
	q_value: tensor([[-38.0266]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 33, step: 57
	action: tensor([[ 0.0609, -0.9548,  0.2622,  0.1429,  0.2783,  0.8576,  0.2737]],
       dtype=torch.float64)
	q_value: tensor([[-35.7305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.009593872922301627, distance: 1.1388416777624317 entropy 0.7110485434532166
epoch: 33, step: 58
	action: tensor([[ 2.9346, -3.2550,  0.7404, -0.9307,  1.8048,  3.8287,  1.7032]],
       dtype=torch.float64)
	q_value: tensor([[-29.5896]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 33, step: 59
	action: tensor([[ 0.4150, -0.5194, -0.1695, -0.5314,  0.6968,  0.7913,  0.1854]],
       dtype=torch.float64)
	q_value: tensor([[-35.7305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1523108379625926, distance: 1.0535981722976355 entropy 0.7110485434532166
epoch: 33, step: 60
	action: tensor([[ 1.7126, -3.1318,  1.3881, -0.3359,  1.1828,  3.4645,  2.0412]],
       dtype=torch.float64)
	q_value: tensor([[-30.5613]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 33, step: 61
	action: tensor([[-0.0532, -1.5412, -0.1828,  0.3274,  0.3201,  1.0994,  0.3417]],
       dtype=torch.float64)
	q_value: tensor([[-35.7305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14308265475075, distance: 1.2234761580723783 entropy 0.7110485434532166
epoch: 33, step: 62
	action: tensor([[ 3.6925, -2.4554,  1.8460, -1.1568,  2.2860,  3.1975,  1.9953]],
       dtype=torch.float64)
	q_value: tensor([[-32.2437]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 33, step: 63
	action: tensor([[ 1.1264, -0.9280,  0.5972, -0.6106,  0.1164,  0.5922,  0.6682]],
       dtype=torch.float64)
	q_value: tensor([[-35.7305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3878318333256183, distance: 1.3481093341455228 entropy 0.7110485434532166
epoch: 33, step: 64
	action: tensor([[ 3.8456, -4.2395,  1.0408, -1.5831,  2.5954,  4.3660,  2.6913]],
       dtype=torch.float64)
	q_value: tensor([[-34.4466]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 33, step: 65
	action: tensor([[-0.5916, -0.0030,  0.3586, -0.4888, -0.0399,  0.5486,  0.6190]],
       dtype=torch.float64)
	q_value: tensor([[-35.7305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6295200281089439, distance: 1.4607858204436825 entropy 0.7110485434532166
epoch: 33, step: 66
	action: tensor([[ 1.5143, -2.3228,  0.9136, -0.8952,  0.5452,  2.4521,  0.8639]],
       dtype=torch.float64)
	q_value: tensor([[-26.7525]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 33, step: 67
	action: tensor([[ 1.3059, -0.3522, -0.4126, -0.4454,  0.4764,  0.3735, -0.5055]],
       dtype=torch.float64)
	q_value: tensor([[-35.7305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 33, step: 68
	action: tensor([[ 0.9041,  0.0031, -1.0299,  0.5279, -0.0132,  1.8214,  0.4813]],
       dtype=torch.float64)
	q_value: tensor([[-35.7305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 33, step: 69
	action: tensor([[ 0.3893, -0.2835, -0.3145, -0.5195,  0.6012,  0.6998, -0.1573]],
       dtype=torch.float64)
	q_value: tensor([[-35.7305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36315310939512324, distance: 0.9132174670672015 entropy 0.7110485434532166
epoch: 33, step: 70
	action: tensor([[ 1.3886, -2.0956,  0.7462, -0.5282,  1.6690,  3.2617,  1.8129]],
       dtype=torch.float64)
	q_value: tensor([[-28.4669]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 33, step: 71
	action: tensor([[ 1.0409, -0.5628,  0.1216, -0.6587,  0.3615,  1.3270,  0.1693]],
       dtype=torch.float64)
	q_value: tensor([[-35.7305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24526934794929478, distance: 0.9941517640999167 entropy 0.7110485434532166
epoch: 33, step: 72
	action: tensor([[ 3.2790, -2.7291,  0.4089, -0.7495,  2.6317,  4.8414,  1.3374]],
       dtype=torch.float64)
	q_value: tensor([[-35.8779]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 33, step: 73
	action: tensor([[ 0.2900, -0.5748,  0.2547, -0.9488, -0.1760,  0.7486, -0.1705]],
       dtype=torch.float64)
	q_value: tensor([[-35.7305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3336526194707794, distance: 1.3215331279757498 entropy 0.7110485434532166
epoch: 33, step: 74
	action: tensor([[ 2.3251, -2.2390,  0.1486, -0.1537,  1.5405,  3.6053,  1.6891]],
       dtype=torch.float64)
	q_value: tensor([[-29.8935]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 33, step: 75
	action: tensor([[ 0.3512, -0.3882,  0.6900, -0.4467,  0.2366,  1.5743, -0.1524]],
       dtype=torch.float64)
	q_value: tensor([[-35.7305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5279584558909699, distance: 0.7862249189787849 entropy 0.7110485434532166
epoch: 33, step: 76
	action: tensor([[ 2.9432, -3.3296,  0.0096, -0.4634,  2.3344,  4.9835,  1.3909]],
       dtype=torch.float64)
	q_value: tensor([[-34.8930]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 33, step: 77
	action: tensor([[-0.1464, -1.0306,  0.6318,  0.4077, -0.5707,  0.6095,  0.1726]],
       dtype=torch.float64)
	q_value: tensor([[-35.7305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15802504683440532, distance: 1.2314468429496257 entropy 0.7110485434532166
epoch: 33, step: 78
	action: tensor([[ 2.5114, -3.3149,  2.2860, -0.7580,  2.9581,  3.6420,  0.9743]],
       dtype=torch.float64)
	q_value: tensor([[-32.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 33, step: 79
	action: tensor([[ 0.2464, -0.8880,  0.2499, -0.0994,  0.0966,  0.8572, -0.0363]],
       dtype=torch.float64)
	q_value: tensor([[-35.7305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04338850808727379, distance: 1.168906349721861 entropy 0.7110485434532166
epoch: 33, step: 80
	action: tensor([[ 1.7532, -2.1202,  1.2562, -1.6614,  0.8144,  3.0082,  1.2021]],
       dtype=torch.float64)
	q_value: tensor([[-29.1653]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 33, step: 81
	action: tensor([[ 0.4453, -0.7276, -0.1028, -0.3344,  1.2867,  1.0276, -0.0745]],
       dtype=torch.float64)
	q_value: tensor([[-35.7305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09054370735551764, distance: 1.0913086797539386 entropy 0.7110485434532166
epoch: 33, step: 82
	action: tensor([[ 2.1555, -3.5217,  0.7256, -0.7096,  1.7576,  4.8255,  1.6814]],
       dtype=torch.float64)
	q_value: tensor([[-37.3486]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 33, step: 83
	action: tensor([[-0.2839, -0.8157, -0.1843, -0.1766, -0.0386, -0.0247,  0.7967]],
       dtype=torch.float64)
	q_value: tensor([[-35.7305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7263449753949556, distance: 1.50355903887089 entropy 0.7110485434532166
epoch: 33, step: 84
	action: tensor([[ 2.1572, -1.5169,  0.5107, -0.1682,  1.4368,  3.0092,  0.8363]],
       dtype=torch.float64)
	q_value: tensor([[-27.8285]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 33, step: 85
	action: tensor([[ 0.9105, -1.3606,  0.1092, -0.7013,  0.5949,  0.4012, -0.1322]],
       dtype=torch.float64)
	q_value: tensor([[-35.7305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9458202998139198, distance: 1.5962762724511057 entropy 0.7110485434532166
epoch: 33, step: 86
	action: tensor([[ 2.7613, -3.5570,  1.1795, -0.5215,  1.8190,  4.0786,  2.3530]],
       dtype=torch.float64)
	q_value: tensor([[-36.7583]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 33, step: 87
	action: tensor([[-0.0729, -0.1542, -0.8093, -0.5003,  1.0766,  1.1595,  0.9454]],
       dtype=torch.float64)
	q_value: tensor([[-35.7305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5387457184004087, distance: 0.777189453438426 entropy 0.7110485434532166
epoch: 33, step: 88
	action: tensor([[ 2.0289, -1.7035,  0.1927, -1.3069,  1.8871,  2.9524,  1.4262]],
       dtype=torch.float64)
	q_value: tensor([[-36.8850]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 33, step: 89
	action: tensor([[ 0.4343, -1.1464,  1.0043, -0.2707,  0.5198,  1.9927, -0.6175]],
       dtype=torch.float64)
	q_value: tensor([[-35.7305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.014108420838790603, distance: 1.136243133837911 entropy 0.7110485434532166
epoch: 33, step: 90
	action: tensor([[ 3.8383, -5.9784,  0.9045, -1.8743,  3.0731,  5.4515,  3.0461]],
       dtype=torch.float64)
	q_value: tensor([[-45.5538]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 33, step: 91
	action: tensor([[ 0.4491, -1.0146,  0.1367, -0.8171,  1.3703,  0.8550, -0.0678]],
       dtype=torch.float64)
	q_value: tensor([[-35.7305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6000026367140052, distance: 1.4474949006819247 entropy 0.7110485434532166
epoch: 33, step: 92
	action: tensor([[ 2.9795, -3.1639,  0.4027, -0.7017,  2.5491,  4.5504,  1.6163]],
       dtype=torch.float64)
	q_value: tensor([[-40.7268]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 33, step: 93
	action: tensor([[ 1.0496, -0.5409,  0.5962,  0.0570,  0.3228,  0.7800,  0.3425]],
       dtype=torch.float64)
	q_value: tensor([[-35.7305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4035911278018304, distance: 0.8837486043972651 entropy 0.7110485434532166
epoch: 33, step: 94
	action: tensor([[ 3.6702, -3.2448,  0.7804, -1.3588,  2.1615,  4.5020,  1.5838]],
       dtype=torch.float64)
	q_value: tensor([[-32.3144]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 33, step: 95
	action: tensor([[ 0.2055,  0.0483,  0.9451, -0.2474,  0.2119,  0.7938, -0.5443]],
       dtype=torch.float64)
	q_value: tensor([[-35.7305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6092006120408515, distance: 0.7153747888231625 entropy 0.7110485434532166
epoch: 33, step: 96
	action: tensor([[ 1.9870, -3.7717,  0.9316, -0.8359,  2.6535,  4.2129,  1.5691]],
       dtype=torch.float64)
	q_value: tensor([[-29.6727]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 33, step: 97
	action: tensor([[ 0.0789, -0.4943,  0.2835, -0.1603,  0.9098,  1.4523,  0.5054]],
       dtype=torch.float64)
	q_value: tensor([[-35.7305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4329719464848195, distance: 0.86170570108235 entropy 0.7110485434532166
epoch: 33, step: 98
	action: tensor([[ 3.5296, -3.2148,  0.2186, -1.3646,  1.8008,  4.5370,  1.3992]],
       dtype=torch.float64)
	q_value: tensor([[-34.9586]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 33, step: 99
	action: tensor([[ 0.6171, -0.7559,  0.9663, -0.4076,  0.1280,  1.5636,  0.5627]],
       dtype=torch.float64)
	q_value: tensor([[-35.7305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2812974254289031, distance: 0.9701330352874905 entropy 0.7110485434532166
epoch: 33, step: 100
	action: tensor([[ 4.4310, -4.7394,  0.9153, -1.3126,  2.5986,  5.7063,  2.4526]],
       dtype=torch.float64)
	q_value: tensor([[-37.0301]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 33, step: 101
	action: tensor([[ 0.5786, -1.3517,  0.6569, -0.6031,  0.0382,  0.1673,  1.5804]],
       dtype=torch.float64)
	q_value: tensor([[-35.7305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9394410533500739, distance: 1.5936574798559113 entropy 0.7110485434532166
epoch: 33, step: 102
	action: tensor([[ 3.3776, -4.5735,  0.8258, -1.6036,  2.5382,  4.8747,  2.0978]],
       dtype=torch.float64)
	q_value: tensor([[-37.8036]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 33, step: 103
	action: tensor([[ 0.5550, -0.8311, -0.0811,  0.0281,  1.1233,  0.6017,  0.4077]],
       dtype=torch.float64)
	q_value: tensor([[-35.7305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08106110819985568, distance: 1.096983283153291 entropy 0.7110485434532166
epoch: 33, step: 104
	action: tensor([[ 1.2571, -2.8749, -0.3940, -0.9311,  1.2022,  3.9929,  0.6680]],
       dtype=torch.float64)
	q_value: tensor([[-34.9736]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 33, step: 105
	action: tensor([[ 0.1741, -1.0122,  0.5330,  0.0310,  0.0822,  0.7679,  0.9226]],
       dtype=torch.float64)
	q_value: tensor([[-35.7305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09294363430080743, distance: 1.1963426224140221 entropy 0.7110485434532166
epoch: 33, step: 106
	action: tensor([[ 2.5971, -2.8992,  0.1302, -1.9903,  2.0907,  4.2253,  2.0534]],
       dtype=torch.float64)
	q_value: tensor([[-31.4485]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 33, step: 107
	action: tensor([[ 0.5010, -0.9155, -0.8248, -0.2905,  0.0897,  0.4390,  0.4804]],
       dtype=torch.float64)
	q_value: tensor([[-35.7305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.27781714805271385, distance: 1.29357331883949 entropy 0.7110485434532166
epoch: 33, step: 108
	action: tensor([[ 1.8044, -2.1283,  0.4517, -1.3434,  1.5762,  3.4993,  0.8839]],
       dtype=torch.float64)
	q_value: tensor([[-31.0087]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 33, step: 109
	action: tensor([[ 0.4484, -0.3791,  0.0371,  0.2192,  0.8580,  0.5858,  0.2342]],
       dtype=torch.float64)
	q_value: tensor([[-35.7305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.643262088245924, distance: 0.6834886800052468 entropy 0.7110485434532166
epoch: 33, step: 110
	action: tensor([[ 2.2598, -2.3147,  0.6014, -0.4810,  1.3043,  2.5111,  1.4234]],
       dtype=torch.float64)
	q_value: tensor([[-29.3440]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 33, step: 111
	action: tensor([[ 0.4530, -0.6001,  0.1449, -0.3882,  0.1105,  0.1931,  0.3082]],
       dtype=torch.float64)
	q_value: tensor([[-35.7305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1560524417790925, distance: 1.230397559246021 entropy 0.7110485434532166
epoch: 33, step: 112
	action: tensor([[ 1.4082e+00, -1.8521e+00,  6.5808e-04,  3.4603e-02,  1.6742e+00,
          3.3274e+00,  1.3786e+00]], dtype=torch.float64)
	q_value: tensor([[-26.0185]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 33, step: 113
	action: tensor([[-0.4100, -0.8417, -0.0594, -0.7261,  0.7165,  0.8880,  0.3626]],
       dtype=torch.float64)
	q_value: tensor([[-35.7305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5946195049079717, distance: 1.4450578356710087 entropy 0.7110485434532166
epoch: 33, step: 114
	action: tensor([[ 1.9071, -2.5678,  1.1618, -1.2388,  2.5674,  2.9198,  1.5635]],
       dtype=torch.float64)
	q_value: tensor([[-33.2645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 33, step: 115
	action: tensor([[ 0.7456, -0.3477, -0.2460, -0.8910,  0.2857,  1.4362,  0.1355]],
       dtype=torch.float64)
	q_value: tensor([[-35.7305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4506792298595129, distance: 0.8481442134254608 entropy 0.7110485434532166
epoch: 33, step: 116
	action: tensor([[ 2.5666, -2.9086,  1.2814, -0.9937,  2.8390,  3.0907,  2.3200]],
       dtype=torch.float64)
	q_value: tensor([[-35.5887]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 33, step: 117
	action: tensor([[-0.4135,  0.0363, -0.0309, -1.0162, -0.0078,  1.0543,  0.1816]],
       dtype=torch.float64)
	q_value: tensor([[-35.7305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.26676875021552937, distance: 1.287968862825042 entropy 0.7110485434532166
epoch: 33, step: 118
	action: tensor([[ 2.0309, -1.8331,  0.4975, -1.5566,  2.2784,  2.3172,  1.6254]],
       dtype=torch.float64)
	q_value: tensor([[-31.4804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 33, step: 119
	action: tensor([[ 0.4119, -0.5274,  0.4475,  0.0584, -0.0697,  1.9153,  0.3885]],
       dtype=torch.float64)
	q_value: tensor([[-35.7305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7381441119783362, distance: 0.5855821975728474 entropy 0.7110485434532166
epoch: 33, step: 120
	action: tensor([[ 3.4114, -4.2332,  0.7051, -1.1721,  3.2112,  4.0260,  1.6739]],
       dtype=torch.float64)
	q_value: tensor([[-37.1522]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 33, step: 121
	action: tensor([[ 1.0690, -0.8816, -0.1516,  0.1716, -0.1583,  0.6291,  0.5360]],
       dtype=torch.float64)
	q_value: tensor([[-35.7305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2134509757989872, distance: 1.0148914448202913 entropy 0.7110485434532166
epoch: 33, step: 122
	action: tensor([[ 3.3398, -3.3637,  0.0507, -0.5685,  1.7381,  3.9836,  2.2422]],
       dtype=torch.float64)
	q_value: tensor([[-31.9610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 33, step: 123
	action: tensor([[ 0.3639, -0.7105, -0.4098,  0.2815,  0.9129,  0.2129,  0.8726]],
       dtype=torch.float64)
	q_value: tensor([[-35.7305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16724517372442216, distance: 1.0442759410111908 entropy 0.7110485434532166
epoch: 33, step: 124
	action: tensor([[ 2.1483, -2.1535, -0.0859, -0.1236,  1.4387,  2.6727,  1.8746]],
       dtype=torch.float64)
	q_value: tensor([[-32.9705]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 33, step: 125
	action: tensor([[ 0.0924, -1.3496,  0.2972,  1.0943,  0.0887,  0.7111,  1.4635]],
       dtype=torch.float64)
	q_value: tensor([[-35.7305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.499862043362866, distance: 0.8092852051824461 entropy 0.7110485434532166
epoch: 33, step: 126
	action: tensor([[ 4.8294, -3.8266,  1.4273, -0.6442,  2.1266,  4.9759,  2.2409]],
       dtype=torch.float64)
	q_value: tensor([[-38.1997]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 33, step: 127
	action: tensor([[ 0.7857,  0.0937,  0.8630, -0.5394,  0.7760,  0.2003,  0.7550]],
       dtype=torch.float64)
	q_value: tensor([[-35.7305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6241411147837184, distance: 0.7015669212423051 entropy 0.7110485434532166
LOSS epoch 33 actor 348.39367309966207 critic 236.05318119226953 
epoch: 34, step: 0
	action: tensor([[ 4.0134, -4.7154,  0.3062, -1.4813,  2.7540,  4.3772,  2.2734]],
       dtype=torch.float64)
	q_value: tensor([[-37.1793]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 34, step: 1
	action: tensor([[-0.0774,  0.5674,  0.7365, -0.1099,  0.2519, -0.1219,  0.4843]],
       dtype=torch.float64)
	q_value: tensor([[-38.2104]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 34, step: 2
	action: tensor([[-0.4573, -0.3803, -0.6535, -0.4244, -0.0698, -0.2515,  0.1174]],
       dtype=torch.float64)
	q_value: tensor([[-38.2104]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4548490808134953, distance: 1.3802751376110018 entropy 0.7110485434532166
epoch: 34, step: 3
	action: tensor([[ 0.9998, -0.9659, -0.6875, -0.4514,  0.6749,  1.0344,  0.0189]],
       dtype=torch.float64)
	q_value: tensor([[-28.7282]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10307515693591474, distance: 1.2018748437080593 entropy 0.7110485434532166
epoch: 34, step: 4
	action: tensor([[ 3.7382, -3.6705,  0.1830, -0.5498,  3.2989,  3.7656,  2.2446]],
       dtype=torch.float64)
	q_value: tensor([[-42.9652]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 34, step: 5
	action: tensor([[-0.9134, -0.2822, -0.6100, -0.6941,  0.3210,  0.8224,  0.2849]],
       dtype=torch.float64)
	q_value: tensor([[-38.2104]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7473194901683224, distance: 1.5126653328033834 entropy 0.7110485434532166
epoch: 34, step: 6
	action: tensor([[ 2.0306, -1.5732, -0.4672, -0.1706,  1.5262,  2.4550,  0.4480]],
       dtype=torch.float64)
	q_value: tensor([[-34.8158]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 34, step: 7
	action: tensor([[-4.1704e-04,  2.8758e-02,  6.1526e-02,  4.3272e-01,  2.0309e-01,
          3.8062e-01,  8.6267e-02]], dtype=torch.float64)
	q_value: tensor([[-38.2104]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 34, step: 8
	action: tensor([[ 0.3610, -1.2788,  0.2490, -0.3427,  0.7568,  0.3637, -0.0390]],
       dtype=torch.float64)
	q_value: tensor([[-38.2104]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.796688900499996, distance: 1.533886184006973 entropy 0.7110485434532166
epoch: 34, step: 9
	action: tensor([[ 3.4050, -3.8423, -0.0697, -0.5804,  2.6061,  3.3727,  1.6129]],
       dtype=torch.float64)
	q_value: tensor([[-39.3947]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 34, step: 10
	action: tensor([[ 0.8356, -0.7985, -0.3945, -0.2565,  0.9177, -0.1597, -0.8605]],
       dtype=torch.float64)
	q_value: tensor([[-38.2104]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3245425283561645, distance: 1.3170117415804492 entropy 0.7110485434532166
epoch: 34, step: 11
	action: tensor([[ 2.3994, -3.7744, -0.3331, -1.3231,  1.7074,  2.9301,  2.4118]],
       dtype=torch.float64)
	q_value: tensor([[-42.7524]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 34, step: 12
	action: tensor([[-0.3946, -0.3265,  0.5139, -0.0433,  0.5392, -0.1338, -0.0767]],
       dtype=torch.float64)
	q_value: tensor([[-38.2104]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4980504054522772, distance: 1.4006186551266828 entropy 0.7110485434532166
epoch: 34, step: 13
	action: tensor([[ 2.2636, -2.6586,  0.8154, -0.4113,  2.1634,  2.2442,  1.1864]],
       dtype=torch.float64)
	q_value: tensor([[-30.5832]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 34, step: 14
	action: tensor([[-0.2355, -0.3278,  0.5179,  1.2328,  0.2699,  0.3758,  0.6076]],
       dtype=torch.float64)
	q_value: tensor([[-38.2104]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 34, step: 15
	action: tensor([[ 4.3597e-01, -5.9356e-01, -7.0201e-06, -5.6876e-01, -3.5698e-01,
          7.5490e-01,  5.8079e-01]], dtype=torch.float64)
	q_value: tensor([[-38.2104]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0753244473151542, distance: 1.186660408882581 entropy 0.7110485434532166
epoch: 34, step: 16
	action: tensor([[ 3.2694, -3.5442, -0.0510, -1.7694,  1.5943,  3.7481,  2.2491]],
       dtype=torch.float64)
	q_value: tensor([[-32.7725]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 34, step: 17
	action: tensor([[ 0.1345,  0.3891,  0.6863, -0.1169,  0.1088,  0.1883,  0.4586]],
       dtype=torch.float64)
	q_value: tensor([[-38.2104]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 34, step: 18
	action: tensor([[-0.2109,  0.3723,  0.7606, -0.2663, -0.5170,  0.1142,  0.7868]],
       dtype=torch.float64)
	q_value: tensor([[-38.2104]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14967400887012672, distance: 1.0552355656664127 entropy 0.7110485434532166
epoch: 34, step: 19
	action: tensor([[ 2.0637, -3.0940,  0.1549, -1.3380,  1.3057,  3.7040,  2.1553]],
       dtype=torch.float64)
	q_value: tensor([[-31.0683]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 34, step: 20
	action: tensor([[-0.5715, -0.0970, -0.2470,  0.0995,  0.3711,  0.3064, -0.9333]],
       dtype=torch.float64)
	q_value: tensor([[-38.2104]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.30815401472812165, distance: 1.3088387140066668 entropy 0.7110485434532166
epoch: 34, step: 21
	action: tensor([[ 1.4272, -1.3534,  0.1105, -0.2400,  1.9976,  0.8335,  0.8154]],
       dtype=torch.float64)
	q_value: tensor([[-31.4006]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9128605989326994, distance: 1.5826990952734958 entropy 0.7110485434532166
epoch: 34, step: 22
	action: tensor([[ 6.0619, -5.6651,  0.1609, -0.8892,  4.0909,  6.1800,  4.1634]],
       dtype=torch.float64)
	q_value: tensor([[-58.2112]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 34, step: 23
	action: tensor([[-0.1045,  0.2036, -0.6682, -0.6613,  0.1576,  0.0901,  0.7762]],
       dtype=torch.float64)
	q_value: tensor([[-38.2104]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3519964537173266, distance: 0.9211818756244431 entropy 0.7110485434532166
epoch: 34, step: 24
	action: tensor([[ 1.7913, -1.3361,  0.4555, -0.8049,  1.3595,  1.8611,  1.0470]],
       dtype=torch.float64)
	q_value: tensor([[-29.5375]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5369130363531445, distance: 1.4186698545055931 entropy 0.7110485434532166
epoch: 34, step: 25
	action: tensor([[ 6.1800, -6.1558,  1.3098, -2.5357,  6.1800,  5.6394,  5.0688]],
       dtype=torch.float64)
	q_value: tensor([[-60.4699]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 34, step: 26
	action: tensor([[ 0.6450, -0.2163,  0.3200, -0.8873,  0.9973,  0.1774,  0.1251]],
       dtype=torch.float64)
	q_value: tensor([[-38.2104]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03163956253863209, distance: 1.1260954721485499 entropy 0.7110485434532166
epoch: 34, step: 27
	action: tensor([[ 2.6705, -3.5665, -0.2317, -1.3803,  2.0356,  3.6896,  2.5190]],
       dtype=torch.float64)
	q_value: tensor([[-36.3243]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 34, step: 28
	action: tensor([[-0.5797, -0.5873, -0.6011,  0.0968,  0.1688, -0.0458, -0.1806]],
       dtype=torch.float64)
	q_value: tensor([[-38.2104]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7992659184054958, distance: 1.534985827848168 entropy 0.7110485434532166
epoch: 34, step: 29
	action: tensor([[ 1.7100, -1.0866,  0.1948, -0.3671,  0.9809,  0.9640,  1.1172]],
       dtype=torch.float64)
	q_value: tensor([[-29.7238]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7263750313421653, distance: 1.5035721274178053 entropy 0.7110485434532166
epoch: 34, step: 30
	action: tensor([[ 5.5089, -6.2800,  0.6067, -1.5721,  5.5402,  6.1800,  2.9080]],
       dtype=torch.float64)
	q_value: tensor([[-51.6212]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.6327127477830172 entropy 0.7110485434532166
epoch: 34, step: 31
	action: tensor([[ 0.0269, -1.1257, -0.1666, -0.0323,  0.7626, -0.2111,  0.4844]],
       dtype=torch.float64)
	q_value: tensor([[-38.2104]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8116538002761529, distance: 1.5402609251308372 entropy 0.7110485434532166
epoch: 34, step: 32
	action: tensor([[ 3.2715, -2.8460,  0.4395, -0.6842,  2.6637,  2.5194,  1.2240]],
       dtype=torch.float64)
	q_value: tensor([[-37.1243]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 34, step: 33
	action: tensor([[ 0.1397, -0.4488,  1.0029, -0.3680,  0.0291,  0.2290,  0.7046]],
       dtype=torch.float64)
	q_value: tensor([[-38.2104]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14849055019405988, distance: 1.2263668606943146 entropy 0.7110485434532166
epoch: 34, step: 34
	action: tensor([[ 4.0297, -4.8476,  0.5554, -0.8473,  2.3801,  4.3256,  1.2829]],
       dtype=torch.float64)
	q_value: tensor([[-32.4002]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 34, step: 35
	action: tensor([[-0.1234,  0.2185, -0.7011, -0.3637, -0.8358,  1.2005, -0.1137]],
       dtype=torch.float64)
	q_value: tensor([[-38.2104]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15086757681591723, distance: 1.0544947100148045 entropy 0.7110485434532166
epoch: 34, step: 36
	action: tensor([[ 1.6930, -2.3408,  0.1854, -0.2001,  1.9521,  1.6598,  0.1647]],
       dtype=torch.float64)
	q_value: tensor([[-31.8482]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 34, step: 37
	action: tensor([[ 0.4647,  0.2795, -0.9792, -0.1819, -0.2833,  0.1046,  0.1101]],
       dtype=torch.float64)
	q_value: tensor([[-38.2104]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 34, step: 38
	action: tensor([[-0.5532, -0.5629, -0.7761,  1.1038, -0.0160, -0.4453,  0.9167]],
       dtype=torch.float64)
	q_value: tensor([[-38.2104]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7023573484303725, distance: 1.4930764935643628 entropy 0.7110485434532166
epoch: 34, step: 39
	action: tensor([[ 2.1680, -2.3252, -0.3472, -0.6208,  1.0014,  2.8392,  0.6303]],
       dtype=torch.float64)
	q_value: tensor([[-37.5566]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 34, step: 40
	action: tensor([[-0.1226, -0.1067,  0.1100,  0.2274,  0.4206,  0.0633, -0.0972]],
       dtype=torch.float64)
	q_value: tensor([[-38.2104]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2314354576715123, distance: 1.0032215798298536 entropy 0.7110485434532166
epoch: 34, step: 41
	action: tensor([[ 2.2264, -2.2790,  1.1141, -0.6210,  1.3758,  1.9389,  0.6714]],
       dtype=torch.float64)
	q_value: tensor([[-27.2000]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 34, step: 42
	action: tensor([[ 0.7607, -0.2647, -0.0805, -0.2211,  0.6018, -0.6650, -0.7615]],
       dtype=torch.float64)
	q_value: tensor([[-38.2104]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21635114953458334, distance: 1.0130186564524382 entropy 0.7110485434532166
epoch: 34, step: 43
	action: tensor([[ 2.7379, -2.9676,  1.0810, -0.2151,  0.9129,  2.2954,  1.2091]],
       dtype=torch.float64)
	q_value: tensor([[-36.4770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 34, step: 44
	action: tensor([[0.1972, 0.1065, 0.2514, 0.3107, 0.0080, 0.9531, 0.3944]],
       dtype=torch.float64)
	q_value: tensor([[-38.2104]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 34, step: 45
	action: tensor([[ 0.1140,  0.3730,  0.1186,  0.1759, -0.2968,  0.6397, -0.2580]],
       dtype=torch.float64)
	q_value: tensor([[-38.2104]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 34, step: 46
	action: tensor([[ 0.4268, -0.4886,  0.4879, -0.5262, -0.1351,  1.2016,  0.1293]],
       dtype=torch.float64)
	q_value: tensor([[-38.2104]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3131379751354134, distance: 0.9483997835367912 entropy 0.7110485434532166
epoch: 34, step: 47
	action: tensor([[ 3.8237, -4.2291,  1.4962, -2.2247,  2.6093,  4.5352,  2.0952]],
       dtype=torch.float64)
	q_value: tensor([[-35.6534]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 34, step: 48
	action: tensor([[ 0.1836,  0.7127,  0.2512,  0.4065,  0.8406,  0.8069, -0.2942]],
       dtype=torch.float64)
	q_value: tensor([[-38.2104]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 34, step: 49
	action: tensor([[-0.2260,  0.4911,  0.5616, -1.4157,  0.1528,  0.5132,  0.5291]],
       dtype=torch.float64)
	q_value: tensor([[-38.2104]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23744293465599542, distance: 1.272973267200935 entropy 0.7110485434532166
epoch: 34, step: 50
	action: tensor([[ 2.8792, -2.8310,  0.0221, -0.5592,  1.8510,  3.3102, -0.0906]],
       dtype=torch.float64)
	q_value: tensor([[-36.3344]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 34, step: 51
	action: tensor([[ 0.2015, -0.1387,  0.4922,  0.0532, -0.0512,  0.7854,  0.2565]],
       dtype=torch.float64)
	q_value: tensor([[-38.2104]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6626445782054706, distance: 0.664661526146408 entropy 0.7110485434532166
epoch: 34, step: 52
	action: tensor([[ 2.5549, -4.5032,  0.4686, -0.0661,  1.9664,  2.9184,  1.6230]],
       dtype=torch.float64)
	q_value: tensor([[-30.0554]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 34, step: 53
	action: tensor([[-0.2732, -0.0596,  0.0640, -0.7243, -0.0430, -0.5190,  0.0049]],
       dtype=torch.float64)
	q_value: tensor([[-38.2104]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.37421655427232214, distance: 1.3414802440707692 entropy 0.7110485434532166
epoch: 34, step: 54
	action: tensor([[ 0.7253, -1.8838,  0.6603, -0.2192,  0.2359,  1.4360,  0.8924]],
       dtype=torch.float64)
	q_value: tensor([[-28.3035]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 34, step: 55
	action: tensor([[ 0.9099,  0.9076,  0.0014, -0.2919, -0.4837, -0.3278,  0.3212]],
       dtype=torch.float64)
	q_value: tensor([[-38.2104]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 34, step: 56
	action: tensor([[-0.3787, -0.5515,  0.2745, -0.8286, -0.0992, -0.1422, -0.2668]],
       dtype=torch.float64)
	q_value: tensor([[-38.2104]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8894811664681077, distance: 1.5729972992981185 entropy 0.7110485434532166
epoch: 34, step: 57
	action: tensor([[ 1.4902, -2.9435,  0.2349, -0.3054,  1.3935,  1.8975,  1.3633]],
       dtype=torch.float64)
	q_value: tensor([[-31.2803]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 34, step: 58
	action: tensor([[-0.0420,  0.4429, -0.0315,  0.1200, -0.4100,  0.1309,  0.1488]],
       dtype=torch.float64)
	q_value: tensor([[-38.2104]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 34, step: 59
	action: tensor([[ 0.8629, -0.3897,  0.7708,  0.4231,  0.9691,  0.5142, -0.1096]],
       dtype=torch.float64)
	q_value: tensor([[-38.2104]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5582714184999443, distance: 0.7605616859250923 entropy 0.7110485434532166
epoch: 34, step: 60
	action: tensor([[ 4.1839, -5.2121, -0.0683, -1.8424,  2.9490,  3.5695,  2.3552]],
       dtype=torch.float64)
	q_value: tensor([[-41.3668]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 34, step: 61
	action: tensor([[ 0.4750,  0.2429,  0.5217, -0.1646, -0.2466, -0.7155,  0.3219]],
       dtype=torch.float64)
	q_value: tensor([[-38.2104]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5992646035326701, distance: 0.7244118495860569 entropy 0.7110485434532166
epoch: 34, step: 62
	action: tensor([[ 2.0708, -2.9308,  0.6467, -0.8005,  1.8444,  3.2391,  1.0383]],
       dtype=torch.float64)
	q_value: tensor([[-30.7716]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 34, step: 63
	action: tensor([[-0.3418, -0.2181, -0.8968,  0.5765,  0.4151,  0.8779,  0.7247]],
       dtype=torch.float64)
	q_value: tensor([[-38.2104]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 34, step: 64
	action: tensor([[-0.5184, -0.8733,  0.3849, -0.1154, -0.4822,  0.1091, -0.1210]],
       dtype=torch.float64)
	q_value: tensor([[-38.2104]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9549375192459686, distance: 1.600011610447915 entropy 0.7110485434532166
epoch: 34, step: 65
	action: tensor([[ 2.2524, -1.8242, -0.1743, -0.2502,  2.2899,  2.3960,  2.2680]],
       dtype=torch.float64)
	q_value: tensor([[-32.0762]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 34, step: 66
	action: tensor([[-0.2918, -0.2436, -0.8940,  0.2715,  0.0122, -0.0830,  0.0226]],
       dtype=torch.float64)
	q_value: tensor([[-38.2104]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23962358301154296, distance: 1.274094403828637 entropy 0.7110485434532166
epoch: 34, step: 67
	action: tensor([[ 0.7971, -0.4772, -0.8721, -0.7343,  0.4834,  1.2020,  0.0725]],
       dtype=torch.float64)
	q_value: tensor([[-26.6765]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41267095151722166, distance: 0.8769956389722249 entropy 0.7110485434532166
epoch: 34, step: 68
	action: tensor([[ 4.0862, -3.0910,  0.2951, -0.1836,  2.1728,  4.9478,  1.5737]],
       dtype=torch.float64)
	q_value: tensor([[-40.7153]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 34, step: 69
	action: tensor([[ 0.1596,  0.0506, -0.1556, -0.0375, -0.7710, -0.3864, -0.9291]],
       dtype=torch.float64)
	q_value: tensor([[-38.2104]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5038814475109665, distance: 0.8060266980983546 entropy 0.7110485434532166
epoch: 34, step: 70
	action: tensor([[ 1.5738, -2.2627,  0.0093, -0.3316,  2.1232,  2.5460,  0.9634]],
       dtype=torch.float64)
	q_value: tensor([[-30.8855]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 34, step: 71
	action: tensor([[ 0.1439,  0.0872, -0.1062,  0.1128,  0.6613, -0.9679,  0.0068]],
       dtype=torch.float64)
	q_value: tensor([[-38.2104]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31283496106868813, distance: 0.9486089571241448 entropy 0.7110485434532166
epoch: 34, step: 72
	action: tensor([[ 1.3028, -1.4232,  0.1217, -0.3297,  1.3575,  2.8498,  0.7465]],
       dtype=torch.float64)
	q_value: tensor([[-31.9217]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15300833560744764, distance: 1.2287765559300565 entropy 0.7110485434532166
epoch: 34, step: 73
	action: tensor([[ 6.0652, -6.0798,  0.8715, -1.6277,  3.9638,  6.1800,  2.8878]],
       dtype=torch.float64)
	q_value: tensor([[-59.0082]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.48713753367277 entropy 0.7110485434532166
epoch: 34, step: 74
	action: tensor([[ 1.1652,  0.5528, -0.0020, -0.3390, -0.2669, -0.1077,  0.2556]],
       dtype=torch.float64)
	q_value: tensor([[-38.2104]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 34, step: 75
	action: tensor([[ 0.2487,  0.4652,  0.2487, -0.9883,  0.7417, -0.6626, -0.2115]],
       dtype=torch.float64)
	q_value: tensor([[-38.2104]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2192500321798594, distance: 1.0111432354446392 entropy 0.7110485434532166
epoch: 34, step: 76
	action: tensor([[ 2.7422, -2.3133, -0.0306, -0.2058,  0.9434,  2.9213,  0.9825]],
       dtype=torch.float64)
	q_value: tensor([[-33.9093]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 34, step: 77
	action: tensor([[ 0.0910,  0.5648, -0.2277, -0.5842, -0.3419,  0.3655, -0.2033]],
       dtype=torch.float64)
	q_value: tensor([[-38.2104]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 34, step: 78
	action: tensor([[ 0.0593, -0.1753, -0.3696, -0.0791, -0.0609,  0.8812,  0.3613]],
       dtype=torch.float64)
	q_value: tensor([[-38.2104]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3445184147663938, distance: 0.9264819036282543 entropy 0.7110485434532166
epoch: 34, step: 79
	action: tensor([[ 1.3862, -3.2156,  0.5030, -0.3029,  1.3496,  2.4291,  1.1284]],
       dtype=torch.float64)
	q_value: tensor([[-28.4653]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 34, step: 80
	action: tensor([[ 0.0702,  0.1842,  0.3624, -0.0792,  0.1850, -0.2707,  0.5217]],
       dtype=torch.float64)
	q_value: tensor([[-38.2104]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36695712524703483, distance: 0.9104859652794077 entropy 0.7110485434532166
epoch: 34, step: 81
	action: tensor([[ 2.3872, -1.9183,  0.8754, -0.6560,  1.0597,  2.4280,  0.8805]],
       dtype=torch.float64)
	q_value: tensor([[-27.1058]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 34, step: 82
	action: tensor([[ 0.1349,  1.0008, -0.4010,  1.0402, -0.5695,  0.5921, -0.3329]],
       dtype=torch.float64)
	q_value: tensor([[-38.2104]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 34, step: 83
	action: tensor([[ 1.0782, -0.2073,  0.5891, -0.3848,  0.6630,  0.0652, -0.3887]],
       dtype=torch.float64)
	q_value: tensor([[-38.2104]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3849780766474239, distance: 0.8974329104306266 entropy 0.7110485434532166
epoch: 34, step: 84
	action: tensor([[ 4.1076, -4.8620, -0.0111, -1.5544,  1.9606,  3.8038,  1.8752]],
       dtype=torch.float64)
	q_value: tensor([[-36.4845]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 34, step: 85
	action: tensor([[ 0.7367, -0.6452,  0.2419,  0.2846, -0.4878,  0.1601,  0.9004]],
       dtype=torch.float64)
	q_value: tensor([[-38.2104]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3784928060173629, distance: 0.902152118425555 entropy 0.7110485434532166
epoch: 34, step: 86
	action: tensor([[ 2.9606, -4.3995, -0.3953, -1.3854,  2.7910,  4.2778,  3.0655]],
       dtype=torch.float64)
	q_value: tensor([[-34.7322]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 34, step: 87
	action: tensor([[ 0.2324, -0.6288, -0.7225, -0.7460, -0.7481,  0.4252, -0.3638]],
       dtype=torch.float64)
	q_value: tensor([[-38.2104]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2547373037569729, distance: 1.2818378705834306 entropy 0.7110485434532166
epoch: 34, step: 88
	action: tensor([[ 2.1726, -2.4188,  0.0786, -0.4358,  1.6545,  2.1989,  0.5864]],
       dtype=torch.float64)
	q_value: tensor([[-33.6260]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 34, step: 89
	action: tensor([[-0.2692, -0.3755,  0.4749, -0.0845,  0.4275,  0.2593, -0.3433]],
       dtype=torch.float64)
	q_value: tensor([[-38.2104]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2670881857966989, distance: 1.288131243349795 entropy 0.7110485434532166
epoch: 34, step: 90
	action: tensor([[ 1.2097, -2.2957,  0.5477, -0.8178,  2.2901,  2.2348,  2.5143]],
       dtype=torch.float64)
	q_value: tensor([[-30.7341]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 34, step: 91
	action: tensor([[-0.4148, -1.3451, -0.2459, -0.5524,  0.1633,  1.0643,  0.2750]],
       dtype=torch.float64)
	q_value: tensor([[-38.2104]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7851444723400609, distance: 1.5289503335235533 entropy 0.7110485434532166
epoch: 34, step: 92
	action: tensor([[ 2.8164, -3.6900, -0.2952, -0.1406,  2.3166,  3.4858,  1.8078]],
       dtype=torch.float64)
	q_value: tensor([[-38.6644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 34, step: 93
	action: tensor([[ 0.0898, -0.0755,  0.2587, -0.5869,  0.4260, -0.2658, -0.5160]],
       dtype=torch.float64)
	q_value: tensor([[-38.2104]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10331658702267332, distance: 1.2020063637003247 entropy 0.7110485434532166
epoch: 34, step: 94
	action: tensor([[ 1.6263, -1.9346,  0.8237, -0.3508,  1.6875,  2.6398,  1.4138]],
       dtype=torch.float64)
	q_value: tensor([[-30.6524]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 34, step: 95
	action: tensor([[ 1.2920, -0.4435,  0.6519, -0.6018,  0.0389,  0.1098,  0.4331]],
       dtype=torch.float64)
	q_value: tensor([[-38.2104]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05452106693835024, distance: 1.1127116151324328 entropy 0.7110485434532166
epoch: 34, step: 96
	action: tensor([[ 3.8635, -4.2915,  0.7065, -0.7890,  3.8039,  5.2964,  2.3066]],
       dtype=torch.float64)
	q_value: tensor([[-36.5985]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 34, step: 97
	action: tensor([[ 0.7743, -0.0322, -0.2345, -1.4174,  0.8544,  0.5100,  0.6570]],
       dtype=torch.float64)
	q_value: tensor([[-38.2104]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 34, step: 98
	action: tensor([[-0.2717, -0.2678,  0.5786, -1.0291,  0.4297,  0.2554, -0.0776]],
       dtype=torch.float64)
	q_value: tensor([[-38.2104]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7394502577861501, distance: 1.509255267407067 entropy 0.7110485434532166
epoch: 34, step: 99
	action: tensor([[ 2.4697, -2.5357,  0.5686, -0.1059,  1.6414,  3.9624,  1.8319]],
       dtype=torch.float64)
	q_value: tensor([[-32.2300]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 34, step: 100
	action: tensor([[-0.3930,  0.0230, -0.1270, -0.6994,  1.6566,  0.1530, -0.4902]],
       dtype=torch.float64)
	q_value: tensor([[-38.2104]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.28786638453404034, distance: 1.29864993176482 entropy 0.7110485434532166
epoch: 34, step: 101
	action: tensor([[ 1.5118, -1.2337,  0.9504,  0.0509,  0.5662,  2.1065,  1.1606]],
       dtype=torch.float64)
	q_value: tensor([[-42.8431]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.27342059843521604, distance: 1.2913460206218954 entropy 0.7110485434532166
epoch: 34, step: 102
	action: tensor([[ 6.1800, -6.2017,  0.9073, -2.2229,  5.4435,  6.1800,  4.2155]],
       dtype=torch.float64)
	q_value: tensor([[-57.9807]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3695687425605358 entropy 0.7110485434532166
epoch: 34, step: 103
	action: tensor([[ 0.6276, -1.3028,  0.1096, -0.1699,  0.2944, -0.5416,  0.7212]],
       dtype=torch.float64)
	q_value: tensor([[-38.2104]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8804761133841901, distance: 1.569244459133343 entropy 0.7110485434532166
epoch: 34, step: 104
	action: tensor([[ 2.6605, -4.0644, -0.0546, -0.7306,  2.4696,  3.4280,  1.7341]],
       dtype=torch.float64)
	q_value: tensor([[-39.5032]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 34, step: 105
	action: tensor([[ 0.2084, -0.3340,  0.4786,  0.3996,  0.5447,  0.9765, -0.3512]],
       dtype=torch.float64)
	q_value: tensor([[-38.2104]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7704351710237669, distance: 0.5482887931392416 entropy 0.7110485434532166
epoch: 34, step: 106
	action: tensor([[ 3.1160, -3.2890, -0.0539, -0.4332,  3.0183,  3.8816,  1.9554]],
       dtype=torch.float64)
	q_value: tensor([[-35.6207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 34, step: 107
	action: tensor([[ 0.5539, -0.4101, -0.1407,  0.1559,  0.2420,  0.5366,  0.8691]],
       dtype=torch.float64)
	q_value: tensor([[-38.2104]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.609181410420214, distance: 0.7153923632961591 entropy 0.7110485434532166
epoch: 34, step: 108
	action: tensor([[ 2.5794, -2.7422,  0.0146, -0.9409,  1.9022,  2.8250,  1.9939]],
       dtype=torch.float64)
	q_value: tensor([[-32.8908]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 34, step: 109
	action: tensor([[ 0.2936,  0.1183,  0.1313, -0.2426,  0.4030,  0.6626, -0.7660]],
       dtype=torch.float64)
	q_value: tensor([[-38.2104]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6907532006577903, distance: 0.6363694171228411 entropy 0.7110485434532166
epoch: 34, step: 110
	action: tensor([[ 1.5731, -2.6012,  0.3143, -1.0544,  1.6698,  2.5016,  2.2575]],
       dtype=torch.float64)
	q_value: tensor([[-31.1587]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 34, step: 111
	action: tensor([[-0.7628,  0.1813,  0.1850, -0.2888,  0.8747,  0.7783, -0.8415]],
       dtype=torch.float64)
	q_value: tensor([[-38.2104]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3104057764477659, distance: 1.309964699823098 entropy 0.7110485434532166
epoch: 34, step: 112
	action: tensor([[ 1.8786, -2.0209, -0.2578, -0.2466,  1.9100,  1.5620,  0.6271]],
       dtype=torch.float64)
	q_value: tensor([[-37.0719]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 34, step: 113
	action: tensor([[ 0.0295, -0.5743,  0.1151, -0.0237,  0.7373,  0.3564, -0.0557]],
       dtype=torch.float64)
	q_value: tensor([[-38.2104]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04177663169093848, distance: 1.1680031095686785 entropy 0.7110485434532166
epoch: 34, step: 114
	action: tensor([[ 3.6595, -3.7082,  0.6731, -0.8210,  1.9940,  2.3860,  1.8776]],
       dtype=torch.float64)
	q_value: tensor([[-31.9989]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 34, step: 115
	action: tensor([[-0.4363,  0.2092,  1.0133,  0.1846,  1.0429,  0.4814, -0.0251]],
       dtype=torch.float64)
	q_value: tensor([[-38.2104]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 34, step: 116
	action: tensor([[-0.9458, -0.6484,  0.7518,  0.4352,  0.6169,  0.2694, -0.2663]],
       dtype=torch.float64)
	q_value: tensor([[-38.2104]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7560776047876172, distance: 1.516451571384357 entropy 0.7110485434532166
epoch: 34, step: 117
	action: tensor([[ 3.4660, -4.1091,  0.4582, -0.2794,  2.2154,  4.0224,  1.4188]],
       dtype=torch.float64)
	q_value: tensor([[-36.6864]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 34, step: 118
	action: tensor([[ 1.2428e+00,  2.5901e-04,  5.2221e-01, -1.1672e+00,  5.2866e-01,
         -1.5149e-01, -1.0064e+00]], dtype=torch.float64)
	q_value: tensor([[-38.2104]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2193879662272863, distance: 1.0110539128411629 entropy 0.7110485434532166
epoch: 34, step: 119
	action: tensor([[ 3.4959, -3.4105, -0.8585, -1.1503,  2.4801,  3.4759,  2.2562]],
       dtype=torch.float64)
	q_value: tensor([[-41.0492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 34, step: 120
	action: tensor([[ 0.1568, -0.5849, -0.3493, -0.1057,  0.1244,  0.2099, -0.0073]],
       dtype=torch.float64)
	q_value: tensor([[-38.2104]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07382083141243023, distance: 1.1858304707003051 entropy 0.7110485434532166
epoch: 34, step: 121
	action: tensor([[ 1.3306, -2.2585, -0.0190, -0.2764,  1.5152,  1.5708,  1.4922]],
       dtype=torch.float64)
	q_value: tensor([[-28.2142]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 34, step: 122
	action: tensor([[ 1.3218, -0.4311, -0.1730, -0.3538,  0.0561, -0.6776, -0.2148]],
       dtype=torch.float64)
	q_value: tensor([[-38.2104]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18402450926726255, distance: 1.245194057613865 entropy 0.7110485434532166
epoch: 34, step: 123
	action: tensor([[ 2.2997, -3.6461,  0.5945, -0.4526,  1.9244,  3.6792,  1.1583]],
       dtype=torch.float64)
	q_value: tensor([[-35.4035]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 34, step: 124
	action: tensor([[ 0.3943, -1.3678,  0.5326,  0.0477,  0.8891,  0.1365, -0.4410]],
       dtype=torch.float64)
	q_value: tensor([[-38.2104]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6066623191835898, distance: 1.4505042226776943 entropy 0.7110485434532166
epoch: 34, step: 125
	action: tensor([[ 3.3310, -4.4107,  0.4665, -1.1619,  2.2066,  4.8299,  2.1183]],
       dtype=torch.float64)
	q_value: tensor([[-42.0098]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 34, step: 126
	action: tensor([[ 1.0057,  0.8263, -0.2796,  0.2663, -0.7687,  0.2203,  0.0671]],
       dtype=torch.float64)
	q_value: tensor([[-38.2104]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 34, step: 127
	action: tensor([[ 0.0674, -0.4752, -0.3026,  0.7114, -0.5251,  0.2889,  0.3658]],
       dtype=torch.float64)
	q_value: tensor([[-38.2104]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26322549716638144, distance: 0.9822544099242704 entropy 0.7110485434532166
LOSS epoch 34 actor 344.53800706677106 critic 180.03065515705072 
epoch: 35, step: 0
	action: tensor([[ 2.3859, -1.7867,  0.4202, -2.1689,  1.2935,  2.2249,  1.9367]],
       dtype=torch.float64)
	q_value: tensor([[-34.2038]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 35, step: 1
	action: tensor([[-0.3218, -1.4244,  0.2680, -0.0304, -0.9389,  0.0329, -0.0506]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9677399510971636, distance: 1.605242113331308 entropy 0.7110485434532166
epoch: 35, step: 2
	action: tensor([[ 3.2940, -3.6651,  0.3201, -2.2711,  2.2742,  2.0510,  3.0510]],
       dtype=torch.float64)
	q_value: tensor([[-43.4031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 35, step: 3
	action: tensor([[ 0.5633, -0.7835,  0.2766, -0.3690,  0.2269,  0.4541, -0.2194]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.22256917192566128, distance: 1.265299724626058 entropy 0.7110485434532166
epoch: 35, step: 4
	action: tensor([[ 3.6503, -3.7235,  0.6467, -2.1805,  1.3721,  2.6274,  2.1703]],
       dtype=torch.float64)
	q_value: tensor([[-40.2609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 35, step: 5
	action: tensor([[ 0.0582, -0.6740, -0.1907,  0.1175, -0.0462, -0.3732, -0.0364]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2496843400250226, distance: 1.2792542164626721 entropy 0.7110485434532166
epoch: 35, step: 6
	action: tensor([[ 2.2467, -1.9749,  0.4452, -1.4681,  1.9570,  2.0825,  1.0379]],
       dtype=torch.float64)
	q_value: tensor([[-34.0533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 35, step: 7
	action: tensor([[ 0.5010, -0.5425,  0.1557, -0.4861,  0.2420,  0.8430, -0.4634]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13408780942278764, distance: 1.064862713091919 entropy 0.7110485434532166
epoch: 35, step: 8
	action: tensor([[ 3.2167, -3.7069,  1.5298, -2.8110,  2.6031,  2.3896,  0.6912]],
       dtype=torch.float64)
	q_value: tensor([[-41.0244]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 35, step: 9
	action: tensor([[ 0.3238,  0.2783, -0.2356,  0.7205,  0.1517,  0.2817, -0.1433]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 35, step: 10
	action: tensor([[-0.3826,  0.0862, -0.4093, -0.3036,  0.2089,  0.4453, -0.5626]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0025405120544514936, distance: 1.1457969421281924 entropy 0.7110485434532166
epoch: 35, step: 11
	action: tensor([[ 1.3142, -1.1575,  0.0476, -0.3987,  0.7581,  0.4448,  1.4208]],
       dtype=torch.float64)
	q_value: tensor([[-31.6495]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7761382823652252, distance: 1.5250886212012815 entropy 0.7110485434532166
epoch: 35, step: 12
	action: tensor([[ 5.9665, -5.7873,  1.4382, -3.9905,  3.9351,  5.3940,  4.3503]],
       dtype=torch.float64)
	q_value: tensor([[-58.3808]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 35, step: 13
	action: tensor([[-0.2404, -0.8313,  0.2667, -0.6347, -0.6166, -0.5856,  0.2126]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8027944324571359, distance: 1.5364902097716064 entropy 0.7110485434532166
epoch: 35, step: 14
	action: tensor([[ 3.5351, -3.1285,  0.7070, -1.9985,  1.1077,  2.9953,  2.3650]],
       dtype=torch.float64)
	q_value: tensor([[-38.2630]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 35, step: 15
	action: tensor([[ 0.6719,  0.0963,  1.1403,  0.5332,  0.3461,  1.0839, -0.1419]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 35, step: 16
	action: tensor([[-0.4416, -0.3205,  0.1868, -0.6903,  0.0556, -0.5763,  0.0647]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7413924001614556, distance: 1.5100975941323773 entropy 0.7110485434532166
epoch: 35, step: 17
	action: tensor([[ 1.6347, -1.6682,  0.4789, -1.1609,  1.5369,  1.8398,  1.1031]],
       dtype=torch.float64)
	q_value: tensor([[-34.4543]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 35, step: 18
	action: tensor([[-0.0459, -0.6939, -0.9211,  0.3162,  0.0599,  0.1534, -0.6556]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.344116099627201, distance: 1.3267071961576624 entropy 0.7110485434532166
epoch: 35, step: 19
	action: tensor([[ 0.5317, -1.4177, -0.0971, -0.5038,  1.6692,  1.6879,  1.5596]],
       dtype=torch.float64)
	q_value: tensor([[-36.7975]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 35, step: 20
	action: tensor([[-2.2685e-01, -6.8388e-01, -2.6048e-01, -3.1175e-01,  3.1431e-01,
         -3.6967e-01, -6.4251e-04]], dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6262630429648917, distance: 1.459325225485818 entropy 0.7110485434532166
epoch: 35, step: 21
	action: tensor([[ 0.7067, -1.7137,  0.0097, -1.3013,  0.6915,  1.5628,  1.3172]],
       dtype=torch.float64)
	q_value: tensor([[-36.1631]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 35, step: 22
	action: tensor([[ 0.3747, -0.5796, -0.0156, -0.0283, -0.0261,  0.0736, -0.0398]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04990758315454713, distance: 1.1154230610560723 entropy 0.7110485434532166
epoch: 35, step: 23
	action: tensor([[ 2.7939, -2.7501,  0.7141, -1.5758,  0.9626,  0.8464,  2.1931]],
       dtype=torch.float64)
	q_value: tensor([[-33.0501]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 35, step: 24
	action: tensor([[-1.0343,  0.1361,  0.6065,  0.1577, -0.1573, -0.5987, -0.7710]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9208543538911136, distance: 1.5860026602507429 entropy 0.7110485434532166
epoch: 35, step: 25
	action: tensor([[ 2.2409, -1.9982, -0.6384, -1.5931,  1.6860,  1.7898,  1.8168]],
       dtype=torch.float64)
	q_value: tensor([[-40.9889]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 35, step: 26
	action: tensor([[ 0.4464, -0.1513,  0.1646, -0.5477, -0.0497,  0.0186,  0.0942]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18432290464741263, distance: 1.0335127219173894 entropy 0.7110485434532166
epoch: 35, step: 27
	action: tensor([[ 2.8847, -3.2404,  0.7609, -1.7592,  2.0438,  2.4454,  0.8499]],
       dtype=torch.float64)
	q_value: tensor([[-31.6575]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 35, step: 28
	action: tensor([[-0.6756,  0.5396, -0.3033, -0.5030,  0.1171,  1.4075, -0.7525]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06397340303758559, distance: 1.1071355290256617 entropy 0.7110485434532166
epoch: 35, step: 29
	action: tensor([[ 1.0409, -1.0127,  0.5649, -0.2258,  0.6652,  0.7365,  1.2146]],
       dtype=torch.float64)
	q_value: tensor([[-42.2429]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 35, step: 30
	action: tensor([[ 0.0954,  0.2236,  0.4531,  0.8055, -0.5563, -0.3072,  0.8353]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 35, step: 31
	action: tensor([[ 0.4770,  0.8052, -0.7438, -0.5546,  0.9184,  0.0243,  0.1828]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 35, step: 32
	action: tensor([[ 0.0896,  0.5680, -0.0123,  0.6836,  0.7603, -0.2768, -1.1590]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 35, step: 33
	action: tensor([[ 0.7335,  0.1790, -1.2726, -0.5833, -0.7913,  0.4137,  0.4689]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 35, step: 34
	action: tensor([[ 0.3667, -0.4927,  0.0875, -0.1899, -0.0178,  0.3312,  0.4384]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12360134466485895, distance: 1.0712912174636244 entropy 0.7110485434532166
epoch: 35, step: 35
	action: tensor([[ 3.6823, -3.1038,  1.0571, -2.2761,  2.4005,  2.9220,  2.4290]],
       dtype=torch.float64)
	q_value: tensor([[-33.9297]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 35, step: 36
	action: tensor([[-0.3486, -0.4758, -0.6039,  0.2206, -0.0162, -0.4454,  0.9682]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.43337167585071534, distance: 1.3700490075740652 entropy 0.7110485434532166
epoch: 35, step: 37
	action: tensor([[ 3.2709, -2.3831, -0.0313, -0.6156,  1.4856,  2.1830,  1.5444]],
       dtype=torch.float64)
	q_value: tensor([[-36.9314]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 35, step: 38
	action: tensor([[ 0.1507,  0.6570,  1.0414, -0.9969, -0.1927,  0.8582, -0.1873]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 35, step: 39
	action: tensor([[-0.0462, -1.0922, -0.2981, -0.7812,  0.2760, -0.7773,  0.9354]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6804636973582361, distance: 1.4834443565927908 entropy 0.7110485434532166
epoch: 35, step: 40
	action: tensor([[ 3.0953, -2.5266,  1.0927, -1.8349,  1.4919,  2.3149,  2.1961]],
       dtype=torch.float64)
	q_value: tensor([[-44.8426]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 35, step: 41
	action: tensor([[ 0.2894, -0.1740,  0.0223,  0.3580,  0.5945, -0.6067,  0.7357]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41987137564588195, distance: 0.871603249313451 entropy 0.7110485434532166
epoch: 35, step: 42
	action: tensor([[ 2.1685, -2.7613,  1.2213, -1.8926,  0.9759,  2.1016,  2.1995]],
       dtype=torch.float64)
	q_value: tensor([[-37.3854]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 35, step: 43
	action: tensor([[ 0.3312, -0.1837,  0.5518, -0.5809, -0.1585,  0.0824, -0.3791]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05879576241446616, distance: 1.1101933719438402 entropy 0.7110485434532166
epoch: 35, step: 44
	action: tensor([[ 2.5164, -2.4623, -0.5758, -2.0552,  1.6713,  2.5498,  1.6609]],
       dtype=torch.float64)
	q_value: tensor([[-34.2500]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 35, step: 45
	action: tensor([[-0.3200, -0.0273,  0.8245, -0.7434, -0.1625, -0.3892,  0.6317]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6948319158002301, distance: 1.489772694981766 entropy 0.7110485434532166
epoch: 35, step: 46
	action: tensor([[ 3.6080, -3.8290,  0.4470, -1.8729,  2.3186,  2.4926,  1.6529]],
       dtype=torch.float64)
	q_value: tensor([[-35.6291]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 35, step: 47
	action: tensor([[-0.0197, -0.6827,  0.0939,  0.1359,  0.2978,  0.2241,  0.1553]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1493538122084188, distance: 1.2268276729590855 entropy 0.7110485434532166
epoch: 35, step: 48
	action: tensor([[ 2.4862, -1.9255,  0.1258, -1.0714,  1.8389,  2.4929,  1.5808]],
       dtype=torch.float64)
	q_value: tensor([[-34.6658]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 35, step: 49
	action: tensor([[-0.3922, -1.0049, -0.7550, -0.6466, -0.3044, -1.2332, -0.7392]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04300499249257084, distance: 1.1686915040471624 entropy 0.7110485434532166
epoch: 35, step: 50
	action: tensor([[ 1.8076, -1.7537,  0.9684, -1.5268,  0.9221,  1.2038,  0.7266]],
       dtype=torch.float64)
	q_value: tensor([[-49.8869]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 35, step: 51
	action: tensor([[-0.1338, -0.2656,  0.3130, -0.3912, -0.1858, -0.7599,  0.5602]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.38434805293479735, distance: 1.346416237117353 entropy 0.7110485434532166
epoch: 35, step: 52
	action: tensor([[ 1.7022, -2.5867,  1.3915, -1.3277,  0.9741,  2.6173,  1.7162]],
       dtype=torch.float64)
	q_value: tensor([[-34.1863]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 35, step: 53
	action: tensor([[-0.3262,  0.4853, -0.4262,  0.0251, -0.7849,  0.1657, -0.3183]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 35, step: 54
	action: tensor([[0.4984, 0.2906, 0.5594, 0.3681, 0.1356, 0.6699, 1.0164]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 35, step: 55
	action: tensor([[-0.2544, -0.6725,  0.1818, -0.1821,  0.4310,  0.1102,  0.7002]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5976373505767527, distance: 1.4464245881056705 entropy 0.7110485434532166
epoch: 35, step: 56
	action: tensor([[ 2.3926, -3.3266,  1.0390, -1.4371,  2.3795,  3.2429,  2.1875]],
       dtype=torch.float64)
	q_value: tensor([[-36.2906]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 35, step: 57
	action: tensor([[-0.0859,  0.2457,  0.1266,  0.3223, -0.3010, -0.7213, -0.0458]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 35, step: 58
	action: tensor([[-0.5176, -0.0366,  0.7448, -0.2771, -0.1507,  0.1432, -0.5281]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5133062960086634, distance: 1.407732420749892 entropy 0.7110485434532166
epoch: 35, step: 59
	action: tensor([[ 3.3256, -3.3573,  1.1011, -1.4640,  1.2121,  1.4020,  1.4848]],
       dtype=torch.float64)
	q_value: tensor([[-35.6823]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 35, step: 60
	action: tensor([[-0.4405, -1.0133, -0.4101,  0.1501, -0.5395,  0.2099,  0.2724]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9171722111703222, distance: 1.5844818032716255 entropy 0.7110485434532166
epoch: 35, step: 61
	action: tensor([[ 2.2954, -1.9166,  0.7676, -0.9209,  1.8359,  1.8840,  1.7862]],
       dtype=torch.float64)
	q_value: tensor([[-37.8620]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 35, step: 62
	action: tensor([[ 0.0438,  0.3036, -0.1006, -0.1183, -0.6690,  0.3054, -0.3853]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 35, step: 63
	action: tensor([[-0.1098, -0.0557,  0.0521, -0.8437,  0.0171,  0.1042, -0.3921]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2037137636061015, distance: 1.255504594862124 entropy 0.7110485434532166
epoch: 35, step: 64
	action: tensor([[ 2.5162, -1.7731,  0.1333, -1.4721,  0.9483,  0.5749,  1.8381]],
       dtype=torch.float64)
	q_value: tensor([[-33.1759]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 35, step: 65
	action: tensor([[-0.1692,  0.1842, -0.1363, -0.8162, -0.1515, -0.1392,  0.7669]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0047084921087496845, distance: 1.147035158139414 entropy 0.7110485434532166
epoch: 35, step: 66
	action: tensor([[ 1.8501, -2.1364,  0.2566, -1.2493,  1.3185,  2.1585,  0.8202]],
       dtype=torch.float64)
	q_value: tensor([[-33.0763]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 35, step: 67
	action: tensor([[ 1.4124,  0.6999, -1.1504,  0.9610,  0.2113, -0.1861,  0.0563]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 35, step: 68
	action: tensor([[-0.5144, -0.3021,  0.3281,  0.5005,  0.7397,  0.5392,  0.2842]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12607500275608674, distance: 1.0697782754109684 entropy 0.7110485434532166
epoch: 35, step: 69
	action: tensor([[ 3.5530, -3.5328,  0.2821, -1.1469,  1.7693,  3.2347,  2.4211]],
       dtype=torch.float64)
	q_value: tensor([[-37.9066]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 35, step: 70
	action: tensor([[ 0.1538, -1.5143, -0.0388,  0.6118, -0.3998, -0.4016, -0.0974]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3761902253608673, distance: 1.342443225779051 entropy 0.7110485434532166
epoch: 35, step: 71
	action: tensor([[ 3.7345, -3.0549,  0.6207, -2.0032,  2.8029,  2.2346,  3.1241]],
       dtype=torch.float64)
	q_value: tensor([[-43.8563]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 35, step: 72
	action: tensor([[ 0.8359, -1.1888, -0.1646, -0.4848, -0.1604, -1.1018, -0.2381]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7758283924883027, distance: 1.5249555712732088 entropy 0.7110485434532166
epoch: 35, step: 73
	action: tensor([[ 2.6955, -3.5417,  0.2009, -1.7446,  2.2602,  3.0971,  2.0336]],
       dtype=torch.float64)
	q_value: tensor([[-49.0515]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 35, step: 74
	action: tensor([[-0.9692,  0.1765,  0.8152, -0.9408, -0.0417,  0.1491,  0.0760]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.283082355145813, distance: 1.7290895340402042 entropy 0.7110485434532166
epoch: 35, step: 75
	action: tensor([[ 1.9196, -2.5734,  0.2099, -0.8107,  2.2839,  2.6607,  2.5657]],
       dtype=torch.float64)
	q_value: tensor([[-37.5768]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 35, step: 76
	action: tensor([[-0.3659,  0.1835, -0.2353, -1.1537, -0.1183,  0.7725, -0.0465]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08997135867638373, distance: 1.1947147796099045 entropy 0.7110485434532166
epoch: 35, step: 77
	action: tensor([[ 3.3033, -2.4439,  0.6676, -1.2156,  1.8224,  1.0721,  1.2193]],
       dtype=torch.float64)
	q_value: tensor([[-38.6620]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 35, step: 78
	action: tensor([[-0.6413,  0.0954, -0.0988,  0.4458, -0.9954, -0.0746,  0.6542]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18724404127180283, distance: 1.2468858387240012 entropy 0.7110485434532166
epoch: 35, step: 79
	action: tensor([[ 2.4306, -2.7040,  0.1378, -1.3213,  1.2765,  2.2485,  1.5136]],
       dtype=torch.float64)
	q_value: tensor([[-35.5168]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 35, step: 80
	action: tensor([[-0.0565,  0.2677,  0.3412,  0.5717,  0.2146,  0.2347,  0.3317]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 35, step: 81
	action: tensor([[-0.5989, -0.5744,  0.6229, -0.6786, -0.1337, -0.0519,  0.1462]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.173620332642455, distance: 1.6871299582915755 entropy 0.7110485434532166
epoch: 35, step: 82
	action: tensor([[ 2.8959, -3.6789,  0.7457, -1.3137,  2.1751,  1.5452,  1.6667]],
       dtype=torch.float64)
	q_value: tensor([[-35.9429]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 35, step: 83
	action: tensor([[-0.2113, -0.7188, -0.1024,  0.0654,  0.5879,  1.0171,  0.0143]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.009803767303003141, distance: 1.1499400149564105 entropy 0.7110485434532166
epoch: 35, step: 84
	action: tensor([[ 3.0830, -2.6089,  0.6143, -1.5295,  1.6400,  2.7855,  2.0328]],
       dtype=torch.float64)
	q_value: tensor([[-40.5554]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 35, step: 85
	action: tensor([[ 0.3694, -0.9015, -0.7824,  0.0339,  0.2097,  0.3602,  0.6082]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2199169790765274, distance: 1.2639265340817096 entropy 0.7110485434532166
epoch: 35, step: 86
	action: tensor([[ 3.1646, -2.2779,  0.2503, -1.5850,  1.6100,  3.4947,  2.7878]],
       dtype=torch.float64)
	q_value: tensor([[-40.1173]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 35, step: 87
	action: tensor([[ 0.2203, -1.5462,  0.6065,  0.5588,  1.0578,  0.3182, -1.2398]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15914212722331111, distance: 1.2320406528742185 entropy 0.7110485434532166
epoch: 35, step: 88
	action: tensor([[ 3.8838, -4.7334,  1.3511, -2.2571,  3.1913,  4.9912,  1.7390]],
       dtype=torch.float64)
	q_value: tensor([[-59.5519]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 35, step: 89
	action: tensor([[ 0.3650,  1.0830, -0.1964, -0.2393,  0.0260,  0.0130,  0.1412]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 35, step: 90
	action: tensor([[-0.2438,  0.4510,  0.0538, -0.7069,  0.7333, -0.4098,  0.2033]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05370310302477321, distance: 1.1746698522077657 entropy 0.7110485434532166
epoch: 35, step: 91
	action: tensor([[ 1.8353, -1.9617,  0.8763, -0.0394,  1.4529,  1.8472,  1.6651]],
       dtype=torch.float64)
	q_value: tensor([[-34.4411]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 35, step: 92
	action: tensor([[-0.0329, -0.1778,  0.7223, -1.0063, -0.4502,  0.1079,  0.7173]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5190645524228317, distance: 1.4104081473417667 entropy 0.7110485434532166
epoch: 35, step: 93
	action: tensor([[ 4.5450, -4.0014,  0.7961, -3.0751,  3.1174,  3.4081,  1.8188]],
       dtype=torch.float64)
	q_value: tensor([[-38.7301]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 35, step: 94
	action: tensor([[ 0.4547, -0.1612, -0.3922, -0.8187,  0.8365, -0.3328,  0.3737]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0674864049196715, distance: 1.1050559805532225 entropy 0.7110485434532166
epoch: 35, step: 95
	action: tensor([[ 1.9999, -2.3433,  0.8986, -0.7525,  2.4266,  1.3491,  1.1735]],
       dtype=torch.float64)
	q_value: tensor([[-40.1795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 35, step: 96
	action: tensor([[ 0.2182,  0.8979,  0.1194, -0.4919,  0.1083, -0.5797, -0.9282]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 35, step: 97
	action: tensor([[-0.4479, -0.6509,  0.6572,  0.0522,  0.2019,  0.8892, -0.6165]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.22849841936941973, distance: 1.2683642550996346 entropy 0.7110485434532166
epoch: 35, step: 98
	action: tensor([[ 3.5234, -3.2166,  0.9740, -1.9091,  2.3867,  2.6474,  1.9570]],
       dtype=torch.float64)
	q_value: tensor([[-42.8719]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 35, step: 99
	action: tensor([[ 1.2151, -0.1053, -0.0185, -0.5797,  0.1010, -0.0066,  0.3689]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 35, step: 100
	action: tensor([[ 0.6901, -0.2123,  0.2121, -0.3925, -0.2443,  0.1593, -0.1993]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35046477828379385, distance: 0.9222699243549309 entropy 0.7110485434532166
epoch: 35, step: 101
	action: tensor([[ 2.4140, -2.6863, -0.1008, -1.2761,  1.5577,  1.7325,  2.2816]],
       dtype=torch.float64)
	q_value: tensor([[-33.1411]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 35, step: 102
	action: tensor([[-0.5324,  0.0884, -0.3458,  0.3885, -0.3503, -0.1539,  0.2106]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15105804002475298, distance: 1.2277368879267347 entropy 0.7110485434532166
epoch: 35, step: 103
	action: tensor([[ 1.1666, -2.2725,  0.8327, -1.1140,  0.4077,  1.0088,  0.9193]],
       dtype=torch.float64)
	q_value: tensor([[-28.8427]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 35, step: 104
	action: tensor([[ 0.7798, -0.2334, -0.7906, -0.8073, -1.2030,  0.1141,  1.3893]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18621542823814974, distance: 1.0323130541335743 entropy 0.7110485434532166
epoch: 35, step: 105
	action: tensor([[ 3.3962, -4.3890,  0.9110, -1.4657,  2.5549,  3.6834,  2.9948]],
       dtype=torch.float64)
	q_value: tensor([[-46.3390]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 35, step: 106
	action: tensor([[ 0.1587, -0.0218, -0.2321, -0.6037,  0.1801, -0.2552, -0.4787]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1674043366795529, distance: 1.0441761409367178 entropy 0.7110485434532166
epoch: 35, step: 107
	action: tensor([[ 2.1886, -1.4893,  0.3033, -0.9166,  0.6552,  0.8251,  0.7106]],
       dtype=torch.float64)
	q_value: tensor([[-32.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 35, step: 108
	action: tensor([[-1.0633, -1.1503,  0.2033, -0.1785, -0.1489,  0.3560,  0.2802]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.3764434698876364, distance: 1.7640887777633891 entropy 0.7110485434532166
epoch: 35, step: 109
	action: tensor([[ 2.9841, -3.5114,  1.1487, -1.7524,  1.4538,  1.7181,  2.0095]],
       dtype=torch.float64)
	q_value: tensor([[-41.0299]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 35, step: 110
	action: tensor([[ 0.3269,  0.4982,  0.6560,  0.2607, -0.2667,  0.6286,  0.1224]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 35, step: 111
	action: tensor([[-0.5765,  0.7295,  0.4601, -0.1378,  0.4740,  0.3750,  0.2266]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16691045393999193, distance: 1.0444857895104966 entropy 0.7110485434532166
epoch: 35, step: 112
	action: tensor([[ 2.2294, -2.8797,  0.4280, -1.3571,  0.4949,  1.8956,  1.4895]],
       dtype=torch.float64)
	q_value: tensor([[-35.8967]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 35, step: 113
	action: tensor([[-0.3242,  0.0842, -0.0851,  0.6757,  0.3132,  0.3699,  0.3221]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 35, step: 114
	action: tensor([[-0.9080,  0.2773, -0.1480,  0.0913, -0.0882, -0.3861, -0.2464]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5971752767663123, distance: 1.4462154031947054 entropy 0.7110485434532166
epoch: 35, step: 115
	action: tensor([[ 0.2815, -0.0982, -0.0057, -0.2884,  0.5011,  0.7236,  0.7944]],
       dtype=torch.float64)
	q_value: tensor([[-30.9255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5816944559886902, distance: 0.740122323471706 entropy 0.7110485434532166
epoch: 35, step: 116
	action: tensor([[ 3.6485, -3.9704,  1.5807, -2.2436,  2.4456,  2.4999,  2.2502]],
       dtype=torch.float64)
	q_value: tensor([[-38.5085]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 35, step: 117
	action: tensor([[-0.0435,  0.0759, -0.8040, -0.1937,  0.3859,  0.0693,  0.5427]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3348399932613443, distance: 0.9332967592270257 entropy 0.7110485434532166
epoch: 35, step: 118
	action: tensor([[ 0.9310, -1.7418, -0.0936, -0.6522,  1.0759,  1.3841,  1.0377]],
       dtype=torch.float64)
	q_value: tensor([[-31.5080]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 35, step: 119
	action: tensor([[ 0.0183,  0.0943, -0.7659, -0.9386,  0.0888,  0.3807, -0.2950]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.45092960823989525, distance: 0.8479509008934848 entropy 0.7110485434532166
epoch: 35, step: 120
	action: tensor([[ 1.2044, -0.9047,  0.5300, -0.8347,  1.0533,  1.4322,  0.3777]],
       dtype=torch.float64)
	q_value: tensor([[-34.8188]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.40517884493232503, distance: 1.3565084227075694 entropy 0.7110485434532166
epoch: 35, step: 121
	action: tensor([[ 5.6101, -6.2800,  1.2740, -4.1751,  4.0611,  6.1800,  5.1608]],
       dtype=torch.float64)
	q_value: tensor([[-60.7975]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 35, step: 122
	action: tensor([[ 0.0040, -0.2171, -0.0033, -0.0777,  0.0790, -0.2928, -0.0722]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.018345180497433944, distance: 1.1337990658427892 entropy 0.7110485434532166
epoch: 35, step: 123
	action: tensor([[ 1.3903, -1.7793,  1.2073, -1.1442,  1.3641,  1.8772,  0.6447]],
       dtype=torch.float64)
	q_value: tensor([[-30.2206]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 35, step: 124
	action: tensor([[ 0.2287, -0.9836, -0.2986, -0.0261, -0.2115, -0.5064,  0.1462]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5175473545661333, distance: 1.4097036338991158 entropy 0.7110485434532166
epoch: 35, step: 125
	action: tensor([[ 2.4164, -3.2850,  0.2783, -1.0657,  1.2908,  2.2559,  1.3567]],
       dtype=torch.float64)
	q_value: tensor([[-38.3875]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 35, step: 126
	action: tensor([[-0.7515, -0.1545,  1.2082, -0.2976,  0.9421,  0.2235,  1.1647]],
       dtype=torch.float64)
	q_value: tensor([[-41.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9607376471700266, distance: 1.602383399490699 entropy 0.7110485434532166
epoch: 35, step: 127
	action: tensor([[ 5.1941, -5.5630,  1.0038, -3.1655,  2.9170,  3.4140,  3.4463]],
       dtype=torch.float64)
	q_value: tensor([[-46.0053]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
LOSS epoch 35 actor 366.59261470750545 critic 114.31191396353603 
epoch: 36, step: 0
	action: tensor([[-0.4305,  0.2601,  0.4398, -0.2384,  0.3880,  0.6010,  0.1623]],
       dtype=torch.float64)
	q_value: tensor([[-43.6734]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06435132683466471, distance: 1.1069120016671514 entropy 0.6056879162788391
epoch: 36, step: 1
	action: tensor([[ 4.1920, -5.0075,  1.6042, -4.7446,  2.3996,  3.4692,  3.9636]],
       dtype=torch.float64)
	q_value: tensor([[-39.7952]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 36, step: 2
	action: tensor([[ 0.3829, -0.2359,  0.1640, -0.4787, -0.4649,  0.8139,  0.6916]],
       dtype=torch.float64)
	q_value: tensor([[-43.6734]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3337829930046138, distance: 0.9340380119387599 entropy 0.6056879162788391
epoch: 36, step: 3
	action: tensor([[ 6.1800, -6.2800,  2.8119, -5.4246,  3.6473,  4.2667,  6.0386]],
       dtype=torch.float64)
	q_value: tensor([[-44.3723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 36, step: 4
	action: tensor([[ 0.4050, -0.1726,  1.2223,  0.4460, -0.6353,  0.6628,  0.6156]],
       dtype=torch.float64)
	q_value: tensor([[-43.6734]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8528600619472394, distance: 0.43895699862410686 entropy 0.6056879162788391
epoch: 36, step: 5
	action: tensor([[ 6.0009, -6.2800,  4.8030, -6.2272,  4.0826,  5.9318,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-51.4243]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3326083281313694 entropy 0.6056879162788391
epoch: 36, step: 6
	action: tensor([[ 0.0263,  0.1525,  0.7332, -1.3487,  0.2993,  0.0095, -0.1499]],
       dtype=torch.float64)
	q_value: tensor([[-43.6734]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3497298442536334, distance: 1.3294748272806385 entropy 0.6056879162788391
epoch: 36, step: 7
	action: tensor([[ 4.7375, -5.5088,  2.3878, -5.5209,  2.1031,  4.1139,  5.0949]],
       dtype=torch.float64)
	q_value: tensor([[-45.1211]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 36, step: 8
	action: tensor([[-0.6909,  0.0908, -0.0506, -0.7324,  0.2396, -0.1146,  0.1760]],
       dtype=torch.float64)
	q_value: tensor([[-43.6734]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7101720819971993, distance: 1.4964995806913624 entropy 0.6056879162788391
epoch: 36, step: 9
	action: tensor([[ 1.9530, -2.1004,  1.4365, -1.8710,  0.8923,  1.9173,  2.6633]],
       dtype=torch.float64)
	q_value: tensor([[-36.2224]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 36, step: 10
	action: tensor([[ 0.7739, -0.6618, -0.0058,  0.3914,  0.5173,  0.5273,  0.3400]],
       dtype=torch.float64)
	q_value: tensor([[-43.6734]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5692739800089031, distance: 0.7510299343248943 entropy 0.6056879162788391
epoch: 36, step: 11
	action: tensor([[ 5.9923, -5.8529,  3.0127, -5.4564,  2.7293,  5.0379,  4.8555]],
       dtype=torch.float64)
	q_value: tensor([[-49.0381]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 36, step: 12
	action: tensor([[ 0.2127, -0.6808, -0.0790, -0.4455, -0.0420,  0.7875,  0.3308]],
       dtype=torch.float64)
	q_value: tensor([[-43.6734]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14397402164276363, distance: 1.2239530936706324 entropy 0.6056879162788391
epoch: 36, step: 13
	action: tensor([[ 5.6600, -6.2800,  2.4266, -5.0066,  2.5520,  4.4927,  4.7169]],
       dtype=torch.float64)
	q_value: tensor([[-43.9095]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 36, step: 14
	action: tensor([[ 3.8151e-01, -5.1895e-01,  1.1889e-01,  3.3641e-01, -8.1307e-02,
         -1.3591e-04,  4.0709e-01]], dtype=torch.float64)
	q_value: tensor([[-43.6734]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3722898506555419, distance: 0.9066429078091245 entropy 0.6056879162788391
epoch: 36, step: 15
	action: tensor([[ 4.0384, -4.9845,  2.2645, -4.0371,  2.0355,  3.7859,  3.9687]],
       dtype=torch.float64)
	q_value: tensor([[-40.2636]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 36, step: 16
	action: tensor([[-0.0979, -0.5776, -0.8812,  0.4463, -0.1352, -0.1975, -0.3599]],
       dtype=torch.float64)
	q_value: tensor([[-43.6734]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.277196785699523, distance: 1.2932592748420038 entropy 0.6056879162788391
epoch: 36, step: 17
	action: tensor([[ 1.3458, -1.5164,  0.1661, -1.6782,  0.3790,  1.4367,  0.5227]],
       dtype=torch.float64)
	q_value: tensor([[-38.7532]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 36, step: 18
	action: tensor([[-0.0666,  0.0073,  0.0349, -0.1001, -1.1726,  0.7133, -0.7488]],
       dtype=torch.float64)
	q_value: tensor([[-43.6734]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1281404495177657, distance: 1.0685133629991452 entropy 0.6056879162788391
epoch: 36, step: 19
	action: tensor([[ 4.2061, -4.3299,  2.1493, -3.4879,  0.5174,  3.1464,  3.2986]],
       dtype=torch.float64)
	q_value: tensor([[-42.8174]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 36, step: 20
	action: tensor([[ 0.7729, -0.1253, -0.6007, -0.5840,  0.3274, -0.0270,  0.6209]],
       dtype=torch.float64)
	q_value: tensor([[-43.6734]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 36, step: 21
	action: tensor([[-0.3216,  0.2113, -0.2224, -0.3632,  0.1352,  0.3789,  0.2173]],
       dtype=torch.float64)
	q_value: tensor([[-43.6734]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07235532658212873, distance: 1.1021672974177252 entropy 0.6056879162788391
epoch: 36, step: 22
	action: tensor([[ 2.2894, -3.1080,  0.9506, -2.9005,  0.4951,  2.0912,  1.9329]],
       dtype=torch.float64)
	q_value: tensor([[-33.7476]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 36, step: 23
	action: tensor([[-0.4253,  0.2375,  0.1699, -1.5309, -0.2075, -0.4095, -0.1006]],
       dtype=torch.float64)
	q_value: tensor([[-43.6734]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.41486441499476157, distance: 1.3611754424450377 entropy 0.6056879162788391
epoch: 36, step: 24
	action: tensor([[ 2.6456, -2.4692,  1.9300, -2.4273,  0.5731,  1.7542,  2.6229]],
       dtype=torch.float64)
	q_value: tensor([[-42.1156]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 36, step: 25
	action: tensor([[-0.6821, -0.0483,  0.5443, -0.6209, -0.4979, -0.1840, -0.2543]],
       dtype=torch.float64)
	q_value: tensor([[-43.6734]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.007302426383347, distance: 1.621298937354072 entropy 0.6056879162788391
epoch: 36, step: 26
	action: tensor([[ 3.7365, -3.5094,  1.9609, -3.0424,  2.9875,  2.7740,  3.2584]],
       dtype=torch.float64)
	q_value: tensor([[-38.8938]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 36, step: 27
	action: tensor([[-0.6401,  0.1696,  0.2755, -0.2391,  0.2031,  0.6661,  0.4064]],
       dtype=torch.float64)
	q_value: tensor([[-43.6734]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2227994743690671, distance: 1.265418894936807 entropy 0.6056879162788391
epoch: 36, step: 28
	action: tensor([[ 4.1268, -4.3849,  2.0871, -4.7025,  2.5885,  3.5201,  3.6403]],
       dtype=torch.float64)
	q_value: tensor([[-39.6432]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 36, step: 29
	action: tensor([[-0.5105, -0.1024,  0.2226, -0.6306,  0.2408,  0.1417, -0.1665]],
       dtype=torch.float64)
	q_value: tensor([[-43.6734]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6436254844961076, distance: 1.4670946266885674 entropy 0.6056879162788391
epoch: 36, step: 30
	action: tensor([[ 2.4923, -2.3042,  0.9300, -3.2780,  1.8681,  2.2977,  2.6716]],
       dtype=torch.float64)
	q_value: tensor([[-37.3949]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 36, step: 31
	action: tensor([[ 0.2640, -0.5226, -0.1879,  0.0564, -0.5870,  0.1185,  0.0140]],
       dtype=torch.float64)
	q_value: tensor([[-43.6734]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11390513826519433, distance: 1.0772011350671724 entropy 0.6056879162788391
epoch: 36, step: 32
	action: tensor([[ 4.0979, -4.0849,  0.9813, -2.8339,  0.9173,  2.2527,  2.6431]],
       dtype=torch.float64)
	q_value: tensor([[-37.0871]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 36, step: 33
	action: tensor([[ 0.5153, -0.6027, -0.2070, -0.5942,  0.0786,  0.6539,  0.1256]],
       dtype=torch.float64)
	q_value: tensor([[-43.6734]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05280943360999624, distance: 1.174171614519426 entropy 0.6056879162788391
epoch: 36, step: 34
	action: tensor([[ 5.0575, -6.2292,  2.3228, -6.1733,  2.5548,  4.5428,  4.6040]],
       dtype=torch.float64)
	q_value: tensor([[-44.6228]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 36, step: 35
	action: tensor([[-0.0738,  0.1810,  0.1889, -1.1436,  0.7113,  0.1979, -0.2265]],
       dtype=torch.float64)
	q_value: tensor([[-43.6734]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11542878138090451, distance: 1.208586159450447 entropy 0.6056879162788391
epoch: 36, step: 36
	action: tensor([[ 2.6733, -3.0063,  1.8644, -4.5305,  1.2685,  2.7397,  2.8496]],
       dtype=torch.float64)
	q_value: tensor([[-44.8891]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 36, step: 37
	action: tensor([[ 0.0174, -0.1368,  0.3959,  0.4312, -0.0546,  0.3053,  0.0890]],
       dtype=torch.float64)
	q_value: tensor([[-43.6734]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5832433470885765, distance: 0.738750799651776 entropy 0.6056879162788391
epoch: 36, step: 38
	action: tensor([[ 3.9062, -4.9828,  2.8206, -4.1622,  1.8267,  3.2735,  3.9932]],
       dtype=torch.float64)
	q_value: tensor([[-36.9629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 36, step: 39
	action: tensor([[-0.1295, -0.8239, -0.7974,  0.0969, -0.1800,  0.2229,  0.4495]],
       dtype=torch.float64)
	q_value: tensor([[-43.6734]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5761115926624665, distance: 1.4366473462143874 entropy 0.6056879162788391
epoch: 36, step: 40
	action: tensor([[ 3.0506, -3.5839,  2.0591, -2.9734,  1.4251,  1.9484,  1.8858]],
       dtype=torch.float64)
	q_value: tensor([[-41.2965]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 36, step: 41
	action: tensor([[ 0.1194,  0.3104,  0.0723, -0.9561, -0.3687, -0.1424,  0.4812]],
       dtype=torch.float64)
	q_value: tensor([[-43.6734]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20153421885994138, distance: 1.022550693737101 entropy 0.6056879162788391
epoch: 36, step: 42
	action: tensor([[ 3.5284, -4.2228,  1.7600, -3.7461,  1.3649,  2.2580,  2.6206]],
       dtype=torch.float64)
	q_value: tensor([[-36.6194]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 36, step: 43
	action: tensor([[-0.0016,  0.0482,  0.4846,  0.5090,  0.1689, -0.3835, -0.5891]],
       dtype=torch.float64)
	q_value: tensor([[-43.6734]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5235782068752941, distance: 0.7898643322970261 entropy 0.6056879162788391
epoch: 36, step: 44
	action: tensor([[ 3.6299, -3.7844,  1.5139, -3.2933,  2.1280,  2.9330,  3.3085]],
       dtype=torch.float64)
	q_value: tensor([[-42.5900]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 36, step: 45
	action: tensor([[-0.3378, -0.3684, -0.0860,  0.7281,  1.2090,  0.4944, -0.5920]],
       dtype=torch.float64)
	q_value: tensor([[-43.6734]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2191245160500358, distance: 1.0112245096620158 entropy 0.6056879162788391
epoch: 36, step: 46
	action: tensor([[ 4.1973, -4.4288,  1.8076, -3.8792,  2.4997,  2.2955,  2.8239]],
       dtype=torch.float64)
	q_value: tensor([[-51.4523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 36, step: 47
	action: tensor([[-0.3799, -0.3652, -0.0687,  0.0692,  0.2139,  0.0589,  0.7971]],
       dtype=torch.float64)
	q_value: tensor([[-43.6734]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3754806585424406, distance: 1.3420970977629567 entropy 0.6056879162788391
epoch: 36, step: 48
	action: tensor([[ 3.7422, -4.4627,  1.9501, -4.1160,  1.5835,  2.3393,  3.6891]],
       dtype=torch.float64)
	q_value: tensor([[-37.7105]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 36, step: 49
	action: tensor([[ 0.3468, -0.2734,  0.1702, -0.1939, -0.1623, -0.1422, -0.6115]],
       dtype=torch.float64)
	q_value: tensor([[-43.6734]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20105788559291116, distance: 1.0228556550162082 entropy 0.6056879162788391
epoch: 36, step: 50
	action: tensor([[ 2.9782, -3.2854,  1.1865, -3.8649,  1.3255,  3.2336,  2.4858]],
       dtype=torch.float64)
	q_value: tensor([[-38.2361]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 36, step: 51
	action: tensor([[ 0.7311, -0.1716,  0.5858, -0.3531,  0.3224, -0.7701,  0.5321]],
       dtype=torch.float64)
	q_value: tensor([[-43.6734]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2572657682464169, distance: 0.9862191097869867 entropy 0.6056879162788391
epoch: 36, step: 52
	action: tensor([[ 4.5430, -5.8810,  2.5235, -4.6739,  2.3518,  3.5908,  5.2509]],
       dtype=torch.float64)
	q_value: tensor([[-45.7183]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 36, step: 53
	action: tensor([[ 0.2013, -0.3401,  0.3435, -0.8695, -0.1337,  0.0138,  0.4679]],
       dtype=torch.float64)
	q_value: tensor([[-43.6734]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3730218076656391, distance: 1.3408969744725379 entropy 0.6056879162788391
epoch: 36, step: 54
	action: tensor([[ 4.7581, -5.9096,  2.6960, -5.0978,  1.9151,  3.5918,  5.0668]],
       dtype=torch.float64)
	q_value: tensor([[-41.2706]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 36, step: 55
	action: tensor([[-1.0181,  0.0435,  0.5290, -0.3572, -1.1946, -0.7674, -0.7461]],
       dtype=torch.float64)
	q_value: tensor([[-43.6734]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9854412189553379, distance: 1.6124461149015845 entropy 0.6056879162788391
epoch: 36, step: 56
	action: tensor([[ 3.5628, -4.4931,  2.2291, -3.6734,  1.3149,  3.1483,  3.1824]],
       dtype=torch.float64)
	q_value: tensor([[-49.0814]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 36, step: 57
	action: tensor([[ 0.0464,  0.0346, -0.0020,  0.2680,  0.5002, -0.0182, -0.5078]],
       dtype=torch.float64)
	q_value: tensor([[-43.6734]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.46849465722094075, distance: 0.8342774605219607 entropy 0.6056879162788391
epoch: 36, step: 58
	action: tensor([[ 2.5721, -3.1807,  0.6238, -2.9839,  0.9624,  1.7130,  2.4517]],
       dtype=torch.float64)
	q_value: tensor([[-38.2575]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 36, step: 59
	action: tensor([[ 0.3059, -0.0733,  0.3186, -0.4974,  0.7677,  0.2638, -0.2245]],
       dtype=torch.float64)
	q_value: tensor([[-43.6734]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27257933479593977, distance: 0.9759993099810551 entropy 0.6056879162788391
epoch: 36, step: 60
	action: tensor([[ 4.3674, -5.4462,  1.7789, -4.4193,  1.7811,  3.1130,  4.4825]],
       dtype=torch.float64)
	q_value: tensor([[-43.4302]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 36, step: 61
	action: tensor([[ 0.6428,  0.0206,  0.4847, -0.6393, -0.1366, -0.4198,  0.1703]],
       dtype=torch.float64)
	q_value: tensor([[-43.6734]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2766430029420657, distance: 0.9732693270857738 entropy 0.6056879162788391
epoch: 36, step: 62
	action: tensor([[ 4.3271, -4.9674,  2.3336, -5.0589,  2.6837,  3.8948,  4.3854]],
       dtype=torch.float64)
	q_value: tensor([[-39.5643]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 36, step: 63
	action: tensor([[-0.7877,  0.1099,  0.3378,  0.1673,  0.2622,  0.2794,  0.6350]],
       dtype=torch.float64)
	q_value: tensor([[-43.6734]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3448965818216305, distance: 1.3270923269778523 entropy 0.6056879162788391
epoch: 36, step: 64
	action: tensor([[ 4.8639, -5.2277,  2.6937, -3.8782,  1.9086,  2.8395,  3.9448]],
       dtype=torch.float64)
	q_value: tensor([[-37.3795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 36, step: 65
	action: tensor([[-0.3657, -0.8794,  0.1436, -1.2255, -0.0131, -0.4456,  0.1512]],
       dtype=torch.float64)
	q_value: tensor([[-43.6734]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7382121433995481, distance: 1.5087180392554085 entropy 0.6056879162788391
epoch: 36, step: 66
	action: tensor([[ 4.0462, -3.9918,  1.6760, -4.4141,  2.5176,  3.7027,  3.4350]],
       dtype=torch.float64)
	q_value: tensor([[-47.5564]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 36, step: 67
	action: tensor([[ 0.8406, -0.1010, -0.3444, -0.7603, -0.2293, -0.8761,  1.0679]],
       dtype=torch.float64)
	q_value: tensor([[-43.6734]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 36, step: 68
	action: tensor([[ 0.4406, -1.0296,  0.5226,  0.2263, -0.0084, -0.5042, -0.2321]],
       dtype=torch.float64)
	q_value: tensor([[-43.6734]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.44139177880295777, distance: 1.3738765587278303 entropy 0.6056879162788391
epoch: 36, step: 69
	action: tensor([[ 5.8082, -4.7521,  2.2746, -5.6316,  2.5062,  4.3993,  4.7142]],
       dtype=torch.float64)
	q_value: tensor([[-51.0777]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 36, step: 70
	action: tensor([[ 0.3779,  0.4095, -0.1902, -0.2560, -0.2204,  0.2897,  0.0832]],
       dtype=torch.float64)
	q_value: tensor([[-43.6734]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 36, step: 71
	action: tensor([[-0.4528, -0.5744,  0.8812,  0.2223, -0.1572, -0.0130,  0.5041]],
       dtype=torch.float64)
	q_value: tensor([[-43.6734]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4616402710254106, distance: 1.3834929275306964 entropy 0.6056879162788391
epoch: 36, step: 72
	action: tensor([[ 6.1800, -5.9528,  2.9807, -6.2727,  2.7966,  5.2034,  5.3278]],
       dtype=torch.float64)
	q_value: tensor([[-42.9468]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 36, step: 73
	action: tensor([[ 0.3593, -0.2226,  0.1609, -0.6254, -0.8488,  0.2195,  0.3003]],
       dtype=torch.float64)
	q_value: tensor([[-43.6734]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05214027382929376, distance: 1.1141116836396374 entropy 0.6056879162788391
epoch: 36, step: 74
	action: tensor([[ 4.7189, -6.2800,  2.5539, -4.9512,  1.5038,  4.1681,  4.5863]],
       dtype=torch.float64)
	q_value: tensor([[-40.2944]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 36, step: 75
	action: tensor([[ 0.6722, -0.3526, -0.3253,  0.1636,  0.4658, -0.6769, -0.5809]],
       dtype=torch.float64)
	q_value: tensor([[-43.6734]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3205015100277673, distance: 0.943302404018789 entropy 0.6056879162788391
epoch: 36, step: 76
	action: tensor([[ 2.0767, -2.9479,  0.4782, -2.1939,  1.1875,  2.3580,  2.3807]],
       dtype=torch.float64)
	q_value: tensor([[-44.6242]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 36, step: 77
	action: tensor([[ 0.1677, -0.3885, -0.0528, -0.2072,  0.4326,  0.2979,  0.2677]],
       dtype=torch.float64)
	q_value: tensor([[-43.6734]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10066815182599442, distance: 1.0852172288520465 entropy 0.6056879162788391
epoch: 36, step: 78
	action: tensor([[ 4.2617, -5.1340,  1.4221, -5.0075,  2.7567,  3.5434,  3.3024]],
       dtype=torch.float64)
	q_value: tensor([[-38.8103]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 36, step: 79
	action: tensor([[ 0.4328, -0.3793,  0.5576, -0.1163,  0.4432,  0.4707, -0.5738]],
       dtype=torch.float64)
	q_value: tensor([[-43.6734]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35951680459545654, distance: 0.9158209266733374 entropy 0.6056879162788391
epoch: 36, step: 80
	action: tensor([[ 4.9849, -5.9368,  2.4768, -4.6443,  1.9320,  4.6457,  4.3013]],
       dtype=torch.float64)
	q_value: tensor([[-46.9556]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 36, step: 81
	action: tensor([[-0.0601, -0.6532, -0.1859, -0.0559,  0.2898,  0.2030, -0.3181]],
       dtype=torch.float64)
	q_value: tensor([[-43.6734]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2824599960379277, distance: 1.295921236425373 entropy 0.6056879162788391
epoch: 36, step: 82
	action: tensor([[ 3.1227, -3.7030,  2.7835, -2.8251,  1.3156,  2.1887,  2.4309]],
       dtype=torch.float64)
	q_value: tensor([[-40.5291]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 36, step: 83
	action: tensor([[-0.4201,  0.4519, -0.1710,  0.0968, -0.6879,  0.6304,  0.8336]],
       dtype=torch.float64)
	q_value: tensor([[-43.6734]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 36, step: 84
	action: tensor([[ 0.1059,  0.1611, -0.1677,  0.5888, -0.7263, -0.6724,  0.4496]],
       dtype=torch.float64)
	q_value: tensor([[-43.6734]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 36, step: 85
	action: tensor([[-0.3294, -0.2589,  1.0279, -0.7591,  0.1217, -0.1891, -0.5203]],
       dtype=torch.float64)
	q_value: tensor([[-43.6734]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.839570229534631, distance: 1.5520827765357177 entropy 0.6056879162788391
epoch: 36, step: 86
	action: tensor([[ 5.3189, -4.5841,  2.4512, -4.6891,  1.7382,  3.3087,  4.0799]],
       dtype=torch.float64)
	q_value: tensor([[-45.6173]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 36, step: 87
	action: tensor([[ 0.9294,  0.0165, -0.4231, -0.2406, -0.2852,  0.3276,  0.0815]],
       dtype=torch.float64)
	q_value: tensor([[-43.6734]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 36, step: 88
	action: tensor([[-0.8636,  0.3543, -0.4326, -0.3672,  0.8267,  0.0271,  0.3085]],
       dtype=torch.float64)
	q_value: tensor([[-43.6734]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.518426665954971, distance: 1.4101119865438876 entropy 0.6056879162788391
epoch: 36, step: 89
	action: tensor([[ 1.7454, -1.3762,  0.9613, -1.6147,  0.9626,  0.7566,  1.3760]],
       dtype=torch.float64)
	q_value: tensor([[-39.5440]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.35402566047380035, distance: 1.3315888216081897 entropy 0.6056879162788391
epoch: 36, step: 90
	action: tensor([[ 5.7385, -5.3935,  6.1258, -6.0931,  6.1800,  6.1800,  5.9410]],
       dtype=torch.float64)
	q_value: tensor([[-81.2062]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 36, step: 91
	action: tensor([[-0.5872, -0.2385, -0.0292,  0.6878,  0.1160,  0.2616, -0.2090]],
       dtype=torch.float64)
	q_value: tensor([[-43.6734]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13267768066333918, distance: 1.217895048212508 entropy 0.6056879162788391
epoch: 36, step: 92
	action: tensor([[ 3.0772, -3.6699,  1.4085, -3.6997,  1.4180,  2.2610,  2.0737]],
       dtype=torch.float64)
	q_value: tensor([[-37.3072]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 36, step: 93
	action: tensor([[-0.0963,  0.0284, -0.0150,  0.3649,  0.2677,  0.5691, -0.1140]],
       dtype=torch.float64)
	q_value: tensor([[-43.6734]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 36, step: 94
	action: tensor([[ 0.0972, -0.3793,  0.5428, -0.0891,  0.0607, -0.4160, -0.3654]],
       dtype=torch.float64)
	q_value: tensor([[-43.6734]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13786977346834184, distance: 1.220683217470191 entropy 0.6056879162788391
epoch: 36, step: 95
	action: tensor([[ 2.9876, -4.2674,  2.5588, -3.8953,  1.6776,  2.7436,  3.3979]],
       dtype=torch.float64)
	q_value: tensor([[-41.6867]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 36, step: 96
	action: tensor([[ 0.1002, -0.4742, -0.1626,  0.0183, -0.1820, -0.1089,  0.4285]],
       dtype=torch.float64)
	q_value: tensor([[-43.6734]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.035697950715465554, distance: 1.1645905231987619 entropy 0.6056879162788391
epoch: 36, step: 97
	action: tensor([[ 3.3066, -2.9197,  1.5547, -3.0330,  1.1701,  2.2668,  2.7008]],
       dtype=torch.float64)
	q_value: tensor([[-36.4631]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 36, step: 98
	action: tensor([[ 0.1797,  0.6206, -0.3065,  0.1482, -0.8204,  0.4432, -0.1220]],
       dtype=torch.float64)
	q_value: tensor([[-43.6734]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 36, step: 99
	action: tensor([[ 0.1511,  0.1147,  1.0071,  0.1404, -0.8537, -0.2268,  0.1752]],
       dtype=torch.float64)
	q_value: tensor([[-43.6734]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5426541559599627, distance: 0.7738896915204543 entropy 0.6056879162788391
epoch: 36, step: 100
	action: tensor([[ 5.4869, -6.0832,  2.2517, -5.3895,  1.8794,  4.7325,  4.9237]],
       dtype=torch.float64)
	q_value: tensor([[-44.1825]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 36, step: 101
	action: tensor([[-0.6279,  0.4174,  0.3478,  0.0803, -0.2840,  0.1197,  0.2818]],
       dtype=torch.float64)
	q_value: tensor([[-43.6734]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07210147023706037, distance: 1.1848807369984906 entropy 0.6056879162788391
epoch: 36, step: 102
	action: tensor([[ 3.1516, -3.8374,  1.2636, -3.1076,  1.6977,  2.0588,  3.3229]],
       dtype=torch.float64)
	q_value: tensor([[-34.4035]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 36, step: 103
	action: tensor([[ 0.7565, -0.2081,  0.6199, -0.2930, -0.2925,  0.1462,  0.2364]],
       dtype=torch.float64)
	q_value: tensor([[-43.6734]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4591944347147754, distance: 0.8415448553633661 entropy 0.6056879162788391
epoch: 36, step: 104
	action: tensor([[ 5.1707, -6.1709,  3.5378, -6.2800,  3.3900,  4.6989,  4.8903]],
       dtype=torch.float64)
	q_value: tensor([[-41.8431]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 36, step: 105
	action: tensor([[-0.2916,  0.3020, -0.2589,  0.3607,  0.3370, -0.0230, -0.1825]],
       dtype=torch.float64)
	q_value: tensor([[-43.6734]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 36, step: 106
	action: tensor([[-0.5179, -0.3779, -0.0223, -0.3437, -0.2246, -0.1452, -0.1965]],
       dtype=torch.float64)
	q_value: tensor([[-43.6734]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6957752715435339, distance: 1.4901872464506685 entropy 0.6056879162788391
epoch: 36, step: 107
	action: tensor([[ 1.9295, -3.1007,  0.8183, -2.6068,  0.8947,  1.4656,  2.1888]],
       dtype=torch.float64)
	q_value: tensor([[-35.6102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 36, step: 108
	action: tensor([[ 0.5150, -0.1843,  0.0258, -0.9096, -0.0647, -0.0413,  0.3538]],
       dtype=torch.float64)
	q_value: tensor([[-43.6734]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.024423874872175677, distance: 1.1582346119867288 entropy 0.6056879162788391
epoch: 36, step: 109
	action: tensor([[ 3.9786, -4.4997,  2.3901, -4.4245,  2.1008,  3.6814,  3.8919]],
       dtype=torch.float64)
	q_value: tensor([[-39.8462]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 36, step: 110
	action: tensor([[ 0.0052, -0.3567, -0.2958, -0.1397, -0.0045,  0.2721,  0.3300]],
       dtype=torch.float64)
	q_value: tensor([[-43.6734]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.007518373842719028, distance: 1.1400343338652252 entropy 0.6056879162788391
epoch: 36, step: 111
	action: tensor([[ 3.5427, -3.7342,  1.2669, -3.3236,  0.3430,  2.4182,  2.4067]],
       dtype=torch.float64)
	q_value: tensor([[-34.9881]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 36, step: 112
	action: tensor([[-0.2533,  0.1116,  0.1883,  0.2072,  0.1017,  0.6393, -0.2743]],
       dtype=torch.float64)
	q_value: tensor([[-43.6734]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 36, step: 113
	action: tensor([[-0.0669, -0.6756,  0.0525,  0.9646, -0.1155,  0.2103, -0.0053]],
       dtype=torch.float64)
	q_value: tensor([[-43.6734]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3400930816353067, distance: 0.9296041075013166 entropy 0.6056879162788391
epoch: 36, step: 114
	action: tensor([[ 4.3942, -5.8119,  2.7381, -4.8108,  1.7278,  4.2494,  3.3614]],
       dtype=torch.float64)
	q_value: tensor([[-44.0676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 36, step: 115
	action: tensor([[-0.6957,  0.5969,  0.5897, -0.3446, -0.1263,  0.3216, -0.4216]],
       dtype=torch.float64)
	q_value: tensor([[-43.6734]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 36, step: 116
	action: tensor([[ 0.0448, -0.3729,  0.1341, -0.0331, -0.0282,  0.1453, -0.2873]],
       dtype=torch.float64)
	q_value: tensor([[-43.6734]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04809926152235999, distance: 1.116484055103278 entropy 0.6056879162788391
epoch: 36, step: 117
	action: tensor([[ 3.0652, -3.3767,  1.7676, -2.8211,  1.2276,  2.7507,  2.7311]],
       dtype=torch.float64)
	q_value: tensor([[-36.5243]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 36, step: 118
	action: tensor([[ 0.0632,  0.3773,  0.1055, -0.4068,  0.4525, -0.0192,  0.0402]],
       dtype=torch.float64)
	q_value: tensor([[-43.6734]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4244584266028014, distance: 0.868150546819992 entropy 0.6056879162788391
epoch: 36, step: 119
	action: tensor([[ 1.7790, -2.5185, -0.1351, -2.5507,  0.8453,  1.5151,  2.0050]],
       dtype=torch.float64)
	q_value: tensor([[-35.4472]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 36, step: 120
	action: tensor([[-0.4919, -0.6015,  0.6447, -0.2927, -0.2293, -0.3922, -0.3414]],
       dtype=torch.float64)
	q_value: tensor([[-43.6734]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9787325264709448, distance: 1.6097196281266266 entropy 0.6056879162788391
epoch: 36, step: 121
	action: tensor([[ 4.0104, -3.9681,  2.2407, -3.4505,  1.2680,  3.5171,  3.4973]],
       dtype=torch.float64)
	q_value: tensor([[-43.6038]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 36, step: 122
	action: tensor([[ 0.0703, -0.2095, -0.0469, -0.3437,  0.5719, -0.0107,  0.6122]],
       dtype=torch.float64)
	q_value: tensor([[-43.6734]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.016651349066310273, distance: 1.1347768213310614 entropy 0.6056879162788391
epoch: 36, step: 123
	action: tensor([[ 4.4742, -4.8844,  2.8451, -4.0862,  1.8832,  3.1951,  3.2838]],
       dtype=torch.float64)
	q_value: tensor([[-39.2512]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 36, step: 124
	action: tensor([[-0.3913, -0.6300, -0.2899,  0.1995,  0.8373,  0.3309, -0.7714]],
       dtype=torch.float64)
	q_value: tensor([[-43.6734]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.43365502325201577, distance: 1.3701844158040632 entropy 0.6056879162788391
epoch: 36, step: 125
	action: tensor([[ 1.8664, -2.1940,  0.9669, -2.4729,  1.2400,  2.0456,  2.1839]],
       dtype=torch.float64)
	q_value: tensor([[-47.3612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 36, step: 126
	action: tensor([[-0.3787,  0.0467,  0.7672, -0.4584,  0.0015,  0.1156,  0.6174]],
       dtype=torch.float64)
	q_value: tensor([[-43.6734]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.42702184373454366, distance: 1.3670109827841492 entropy 0.6056879162788391
epoch: 36, step: 127
	action: tensor([[ 4.8911, -5.6023,  3.4918, -5.8198,  1.8984,  4.7010,  5.7798]],
       dtype=torch.float64)
	q_value: tensor([[-40.1884]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
LOSS epoch 36 actor 432.663025828087 critic 67.55382774630304 
epoch: 37, step: 0
	action: tensor([[ 0.2973, -0.4248,  0.3226, -0.3341, -0.1661,  0.2614,  0.5662]],
       dtype=torch.float64)
	q_value: tensor([[-42.9629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.018692194457663458, distance: 1.1335986497327712 entropy 0.6056879162788391
epoch: 37, step: 1
	action: tensor([[ 6.1800, -6.2800,  6.0533, -6.2800,  5.4494,  5.0406,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-45.0499]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 37, step: 2
	action: tensor([[ 0.6225, -0.2345,  0.5285, -0.4819,  0.8293,  0.5710, -0.0390]],
       dtype=torch.float64)
	q_value: tensor([[-42.9629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34169883497319087, distance: 0.9284724145153335 entropy 0.6056879162788391
epoch: 37, step: 3
	action: tensor([[ 6.1800, -6.2590,  6.1800, -6.2800,  6.1800,  6.1800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-55.0857]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0364776646406517 entropy 0.6056879162788391
epoch: 37, step: 4
	action: tensor([[ 0.1275,  0.4174,  0.7058,  0.2094, -0.3393, -0.3818, -0.1490]],
       dtype=torch.float64)
	q_value: tensor([[-42.9629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 37, step: 5
	action: tensor([[ 0.7680, -0.5476, -0.0124, -0.5643,  0.1143, -0.0722,  0.2037]],
       dtype=torch.float64)
	q_value: tensor([[-42.9629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2207942788248578, distance: 1.2643809269484871 entropy 0.6056879162788391
epoch: 37, step: 6
	action: tensor([[ 5.9277, -6.2800,  5.5131, -5.9381,  4.3570,  5.6562,  5.4865]],
       dtype=torch.float64)
	q_value: tensor([[-47.5022]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1196823181084803 entropy 0.6056879162788391
epoch: 37, step: 7
	action: tensor([[0.5709, 0.9520, 0.1601, 0.2339, 0.2294, 0.6829, 0.2674]],
       dtype=torch.float64)
	q_value: tensor([[-42.9629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 37, step: 8
	action: tensor([[-0.2045, -0.0902,  0.2224,  0.0264,  0.1450,  0.8895, -0.2119]],
       dtype=torch.float64)
	q_value: tensor([[-42.9629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3365117902973682, distance: 0.9321231586134957 entropy 0.6056879162788391
epoch: 37, step: 9
	action: tensor([[ 6.0309, -6.2800,  6.1800, -6.2800,  3.7465,  5.8651,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-44.0560]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.049858717471162 entropy 0.6056879162788391
epoch: 37, step: 10
	action: tensor([[-0.0024, -0.2827, -0.3019,  0.3834,  0.2725, -0.0544,  0.0099]],
       dtype=torch.float64)
	q_value: tensor([[-42.9629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2068826844817392, distance: 1.019120198204589 entropy 0.6056879162788391
epoch: 37, step: 11
	action: tensor([[ 5.8809, -5.5799,  4.5003, -6.2800,  2.7055,  4.1306,  5.3631]],
       dtype=torch.float64)
	q_value: tensor([[-38.5957]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 37, step: 12
	action: tensor([[ 0.5121, -0.0850, -0.1469,  0.3129,  0.3750,  0.3140, -0.1859]],
       dtype=torch.float64)
	q_value: tensor([[-42.9629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 37, step: 13
	action: tensor([[ 0.5900,  0.2282, -0.3453, -0.1267, -0.5292,  0.5382,  0.4090]],
       dtype=torch.float64)
	q_value: tensor([[-42.9629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.844542395424199, distance: 0.4511933384678564 entropy 0.6056879162788391
epoch: 37, step: 14
	action: tensor([[ 6.1800, -6.2800,  6.1800, -6.2800,  3.9134,  6.1800,  5.8312]],
       dtype=torch.float64)
	q_value: tensor([[-39.5978]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9695408920874711 entropy 0.6056879162788391
epoch: 37, step: 15
	action: tensor([[-0.1939,  0.2589, -0.4415, -0.1699, -0.7678,  0.4141,  0.4859]],
       dtype=torch.float64)
	q_value: tensor([[-42.9629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19378417217956734, distance: 1.027501236798123 entropy 0.6056879162788391
epoch: 37, step: 16
	action: tensor([[ 6.1123, -6.1956,  4.3957, -6.0262,  2.9108,  6.1800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-35.9546]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2683784556055258 entropy 0.6056879162788391
epoch: 37, step: 17
	action: tensor([[ 0.5585,  0.7993,  0.1287, -0.4299,  0.0585, -0.4203,  0.1867]],
       dtype=torch.float64)
	q_value: tensor([[-42.9629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 37, step: 18
	action: tensor([[ 0.2666, -0.3850,  0.2318,  0.3086, -0.0806,  0.5427, -0.1869]],
       dtype=torch.float64)
	q_value: tensor([[-42.9629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5678608848320448, distance: 0.7522608882414218 entropy 0.6056879162788391
epoch: 37, step: 19
	action: tensor([[ 6.1800, -6.2800,  5.7834, -5.2973,  3.6204,  5.9789,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-44.4899]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 37, step: 20
	action: tensor([[-0.1222, -0.4334, -0.0956, -0.1546, -0.6527,  0.5369,  1.0644]],
       dtype=torch.float64)
	q_value: tensor([[-42.9629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.27664410976430465, distance: 1.2929794312709197 entropy 0.6056879162788391
epoch: 37, step: 21
	action: tensor([[ 5.9512, -6.1140,  6.1251, -6.1737,  5.5163,  6.1800,  5.7453]],
       dtype=torch.float64)
	q_value: tensor([[-46.6247]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.212428442764464 entropy 0.6056879162788391
epoch: 37, step: 22
	action: tensor([[-1.0910,  0.6401,  0.5143,  0.0968, -0.2488,  0.2836, -0.0069]],
       dtype=torch.float64)
	q_value: tensor([[-42.9629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.308505483609268, distance: 1.309014528619712 entropy 0.6056879162788391
epoch: 37, step: 23
	action: tensor([[ 6.1800, -5.4000,  6.1800, -6.2800,  4.9858,  6.1800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-41.7497]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 37, step: 24
	action: tensor([[ 0.0089, -0.4576, -0.5052, -0.0717,  0.3221,  0.9158, -0.1658]],
       dtype=torch.float64)
	q_value: tensor([[-42.9629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16566237963245178, distance: 1.045267883111258 entropy 0.6056879162788391
epoch: 37, step: 25
	action: tensor([[ 6.1800, -6.2800,  4.9159, -6.2800,  3.0249,  6.1800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-45.5654]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3014645565273524 entropy 0.6056879162788391
epoch: 37, step: 26
	action: tensor([[-0.1746,  0.1217,  0.5229, -0.6264, -0.1311, -0.1143, -0.8256]],
       dtype=torch.float64)
	q_value: tensor([[-42.9629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2279341709553071, distance: 1.2680729422643822 entropy 0.6056879162788391
epoch: 37, step: 27
	action: tensor([[ 6.1800, -6.2800,  5.3991, -5.4250,  3.9389,  6.1800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-43.7916]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.087638099463697 entropy 0.6056879162788391
epoch: 37, step: 28
	action: tensor([[-0.1796, -1.1316,  0.0500,  0.3351, -0.0597,  0.1052,  0.1548]],
       dtype=torch.float64)
	q_value: tensor([[-42.9629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6383744841319097, distance: 1.4647492392408126 entropy 0.6056879162788391
epoch: 37, step: 29
	action: tensor([[ 6.1046, -6.0773,  6.1800, -6.2791,  4.7098,  6.1800,  6.1341]],
       dtype=torch.float64)
	q_value: tensor([[-50.9642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0646158054671226 entropy 0.6056879162788391
epoch: 37, step: 30
	action: tensor([[ 0.6224, -0.0423, -0.1032, -0.1518, -0.4102,  0.1772,  0.1794]],
       dtype=torch.float64)
	q_value: tensor([[-42.9629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6379468912824235, distance: 0.6885616548066619 entropy 0.6056879162788391
epoch: 37, step: 31
	action: tensor([[ 6.1800, -6.2800,  6.1695, -6.1446,  3.8582,  5.7681,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-38.2365]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8582754280290096 entropy 0.6056879162788391
epoch: 37, step: 32
	action: tensor([[-0.0439,  0.0304, -0.3901, -0.7954, -0.1686, -0.4200,  0.0847]],
       dtype=torch.float64)
	q_value: tensor([[-42.9629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11488346208292677, distance: 1.0766063101683456 entropy 0.6056879162788391
epoch: 37, step: 33
	action: tensor([[ 3.1861, -4.3789,  3.3162, -4.9177,  1.2755,  3.5231,  3.2721]],
       dtype=torch.float64)
	q_value: tensor([[-37.1282]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 37, step: 34
	action: tensor([[0.5272, 0.1201, 0.0797, 0.5439, 0.2666, 0.2940, 0.1507]],
       dtype=torch.float64)
	q_value: tensor([[-42.9629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 37, step: 35
	action: tensor([[-0.1985,  0.4518, -0.4166, -0.1179, -0.1714, -0.0021, -0.3320]],
       dtype=torch.float64)
	q_value: tensor([[-42.9629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 37, step: 36
	action: tensor([[ 0.1862,  0.0912,  0.5702, -0.4440,  0.2358,  0.2239,  0.1991]],
       dtype=torch.float64)
	q_value: tensor([[-42.9629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3266432304075624, distance: 0.9390296569285653 entropy 0.6056879162788391
epoch: 37, step: 37
	action: tensor([[ 6.0827, -6.2800,  5.5263, -5.2589,  5.0779,  5.3265,  6.0611]],
       dtype=torch.float64)
	q_value: tensor([[-42.9240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 37, step: 38
	action: tensor([[ 0.4341, -0.5177, -0.1943, -0.1519,  0.2324,  0.6452, -0.2573]],
       dtype=torch.float64)
	q_value: tensor([[-42.9629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28058325770743864, distance: 0.970614921461818 entropy 0.6056879162788391
epoch: 37, step: 39
	action: tensor([[ 6.1677, -6.2800,  5.4508, -6.2800,  3.7182,  5.9404,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-46.7959]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0760309293605703 entropy 0.6056879162788391
epoch: 37, step: 40
	action: tensor([[-0.0051,  0.1454,  0.3606, -0.2981,  0.3973,  0.5662, -0.2047]],
       dtype=torch.float64)
	q_value: tensor([[-42.9629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.401344877154167, distance: 0.8854112685042708 entropy 0.6056879162788391
epoch: 37, step: 41
	action: tensor([[ 5.4287, -5.9535,  6.1800, -6.2800,  3.5124,  6.1800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-43.1607]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 37, step: 42
	action: tensor([[ 0.1957,  0.1764,  0.8079, -0.6232, -0.4351,  0.2510, -0.4979]],
       dtype=torch.float64)
	q_value: tensor([[-42.9629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2746239107197932, distance: 0.9746267143415679 entropy 0.6056879162788391
epoch: 37, step: 43
	action: tensor([[ 6.1800, -6.2800,  5.8979, -6.2800,  4.6472,  6.1650,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-45.4042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9872007261089297 entropy 0.6056879162788391
epoch: 37, step: 44
	action: tensor([[-0.3594, -0.1705,  0.6667, -0.1505,  1.2016,  0.3048,  0.4550]],
       dtype=torch.float64)
	q_value: tensor([[-42.9629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.29999881672206063, distance: 1.3047526024618885 entropy 0.6056879162788391
epoch: 37, step: 45
	action: tensor([[ 6.1800, -6.1094,  6.1800, -5.9264,  5.8701,  5.6535,  6.1215]],
       dtype=torch.float64)
	q_value: tensor([[-54.5367]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 37, step: 46
	action: tensor([[ 0.3624, -0.2974, -0.2741, -0.0629,  0.1403, -0.0851, -0.2605]],
       dtype=torch.float64)
	q_value: tensor([[-42.9629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2950090340717718, distance: 0.9608342358026932 entropy 0.6056879162788391
epoch: 37, step: 47
	action: tensor([[ 6.1800, -6.2800,  3.7037, -6.2350,  3.5633,  4.4522,  5.9182]],
       dtype=torch.float64)
	q_value: tensor([[-39.4094]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 37, step: 48
	action: tensor([[-0.2340,  0.1770, -0.0706,  0.3187, -0.2889, -0.1128,  0.3806]],
       dtype=torch.float64)
	q_value: tensor([[-42.9629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 37, step: 49
	action: tensor([[ 0.2963, -0.9309, -0.2286, -0.8177, -0.5423,  0.1362,  0.3599]],
       dtype=torch.float64)
	q_value: tensor([[-42.9629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7106362632282213, distance: 1.4967026596374466 entropy 0.6056879162788391
epoch: 37, step: 50
	action: tensor([[ 5.7303, -6.2800,  5.8981, -6.0048,  5.9254,  6.1800,  6.0766]],
       dtype=torch.float64)
	q_value: tensor([[-51.5675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4048923773244824 entropy 0.6056879162788391
epoch: 37, step: 51
	action: tensor([[ 0.4352, -0.3370,  0.3928, -0.2757, -0.2985, -0.5219,  0.4262]],
       dtype=torch.float64)
	q_value: tensor([[-42.9629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.003050619496468898, distance: 1.142597441321261 entropy 0.6056879162788391
epoch: 37, step: 52
	action: tensor([[ 5.8318, -6.0881,  6.0032, -6.2800,  5.3577,  6.1800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-44.9635]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2482872206547624 entropy 0.6056879162788391
epoch: 37, step: 53
	action: tensor([[-0.5512,  0.2398,  0.0242, -0.9855,  0.4350,  0.0300,  0.4300]],
       dtype=torch.float64)
	q_value: tensor([[-42.9629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5473844571757094, distance: 1.4234945488385808 entropy 0.6056879162788391
epoch: 37, step: 54
	action: tensor([[ 6.1800, -6.1471,  5.3853, -6.2800,  2.9372,  6.1800,  5.6140]],
       dtype=torch.float64)
	q_value: tensor([[-43.3797]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1684193071012734 entropy 0.6056879162788391
epoch: 37, step: 55
	action: tensor([[ 0.2582, -0.3489, -0.8511, -0.2811, -0.0779, -0.3500, -0.0067]],
       dtype=torch.float64)
	q_value: tensor([[-42.9629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2241658593891238, distance: 1.007954984112651 entropy 0.6056879162788391
epoch: 37, step: 56
	action: tensor([[ 3.7930, -4.2312,  2.3212, -4.5631,  1.8605,  3.4683,  3.5301]],
       dtype=torch.float64)
	q_value: tensor([[-39.9459]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 37, step: 57
	action: tensor([[-0.0822, -1.0153,  0.1915, -0.1871,  0.4000,  0.4161,  0.4035]],
       dtype=torch.float64)
	q_value: tensor([[-42.9629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6183257187341016, distance: 1.4557595951858324 entropy 0.6056879162788391
epoch: 37, step: 58
	action: tensor([[ 6.0856, -5.7840,  5.9464, -6.2800,  5.3080,  6.0552,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-52.5631]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 37, step: 59
	action: tensor([[-0.3338,  0.0273, -0.5573, -0.2771, -0.6731,  0.2429,  0.4551]],
       dtype=torch.float64)
	q_value: tensor([[-42.9629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09192400855547689, distance: 1.195784447989164 entropy 0.6056879162788391
epoch: 37, step: 60
	action: tensor([[ 5.6078, -6.0456,  4.4967, -6.2800,  2.0772,  5.0143,  5.8536]],
       dtype=torch.float64)
	q_value: tensor([[-36.3626]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 37, step: 61
	action: tensor([[-0.0623,  0.0806, -0.6912,  0.4709,  0.5420,  0.1772, -0.1823]],
       dtype=torch.float64)
	q_value: tensor([[-42.9629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 37, step: 62
	action: tensor([[-0.3963, -0.5238, -0.3962, -0.4104, -0.5604,  0.3219, -0.2120]],
       dtype=torch.float64)
	q_value: tensor([[-42.9629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6229690871581806, distance: 1.4578465626764494 entropy 0.6056879162788391
epoch: 37, step: 63
	action: tensor([[ 6.0227, -6.2800,  4.2932, -5.8983,  2.9441,  5.4919,  6.0796]],
       dtype=torch.float64)
	q_value: tensor([[-41.3604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4464332220543885 entropy 0.6056879162788391
epoch: 37, step: 64
	action: tensor([[-0.1745, -1.3734, -0.6478,  0.5898,  0.0986, -0.6779,  0.2611]],
       dtype=torch.float64)
	q_value: tensor([[-42.9629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8109194353396172, distance: 1.5399487164004628 entropy 0.6056879162788391
epoch: 37, step: 65
	action: tensor([[ 6.1800, -6.2800,  4.7280, -6.2800,  3.8895,  6.1800,  6.0827]],
       dtype=torch.float64)
	q_value: tensor([[-55.7175]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 37, step: 66
	action: tensor([[-0.0999,  0.4193, -0.2195, -0.4160,  0.0167,  0.0291,  0.8812]],
       dtype=torch.float64)
	q_value: tensor([[-42.9629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3732486437262197, distance: 0.9059502193622044 entropy 0.6056879162788391
epoch: 37, step: 67
	action: tensor([[ 5.8498, -6.2800,  5.4906, -5.8070,  3.2989,  6.1800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-38.3806]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2953352687301647 entropy 0.6056879162788391
epoch: 37, step: 68
	action: tensor([[-0.5727,  0.0810, -0.3874,  0.3456,  0.0891,  0.1720, -1.1345]],
       dtype=torch.float64)
	q_value: tensor([[-42.9629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2151624479066947, distance: 1.2614611020423085 entropy 0.6056879162788391
epoch: 37, step: 69
	action: tensor([[ 3.4914, -5.0701,  2.7078, -5.1087,  0.9140,  3.1629,  4.3277]],
       dtype=torch.float64)
	q_value: tensor([[-41.9701]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 37, step: 70
	action: tensor([[-0.0232, -0.0180, -0.4227, -0.3916, -0.2129,  0.4833, -0.0409]],
       dtype=torch.float64)
	q_value: tensor([[-42.9629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2343893041195676, distance: 1.001291868585148 entropy 0.6056879162788391
epoch: 37, step: 71
	action: tensor([[ 5.7230, -5.6609,  4.2069, -6.2800,  2.6219,  4.5694,  5.9917]],
       dtype=torch.float64)
	q_value: tensor([[-36.3254]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 37, step: 72
	action: tensor([[ 0.8699,  0.0074,  0.4919, -0.4735,  0.2923,  0.8466,  0.0450]],
       dtype=torch.float64)
	q_value: tensor([[-42.9629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7250514435869123, distance: 0.6000430572944855 entropy 0.6056879162788391
epoch: 37, step: 73
	action: tensor([[ 6.1800, -6.0541,  5.6073, -5.8368,  6.1580,  6.1800,  5.2728]],
       dtype=torch.float64)
	q_value: tensor([[-52.8639]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 37, step: 74
	action: tensor([[ 0.0335, -0.6743,  0.2497,  0.2372,  0.0102,  0.2173,  0.6389]],
       dtype=torch.float64)
	q_value: tensor([[-42.9629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04809784397307615, distance: 1.171541310287985 entropy 0.6056879162788391
epoch: 37, step: 75
	action: tensor([[ 6.1344, -5.7884,  6.1464, -6.2422,  5.7148,  6.1800,  5.6272]],
       dtype=torch.float64)
	q_value: tensor([[-46.8320]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 37, step: 76
	action: tensor([[ 0.7809, -0.4899,  0.8864,  0.5610,  0.5206,  0.1995,  0.2311]],
       dtype=torch.float64)
	q_value: tensor([[-42.9629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5679300582773885, distance: 0.7522006778159187 entropy 0.6056879162788391
epoch: 37, step: 77
	action: tensor([[ 5.7985, -6.2800,  6.1800, -6.2800,  6.1800,  5.8700,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-60.5690]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3093462780516556 entropy 0.6056879162788391
epoch: 37, step: 78
	action: tensor([[ 0.3349, -0.0164,  1.5388, -0.7530, -0.5178,  0.4722,  0.4993]],
       dtype=torch.float64)
	q_value: tensor([[-42.9629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.380200099742912, distance: 0.9009121505790114 entropy 0.6056879162788391
epoch: 37, step: 79
	action: tensor([[ 6.1800, -5.9470,  6.1153, -6.2556,  6.1647,  6.1800,  5.9286]],
       dtype=torch.float64)
	q_value: tensor([[-58.8936]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 37, step: 80
	action: tensor([[-0.3314, -0.1950,  0.4128, -0.7408, -0.8144, -0.7124, -0.0984]],
       dtype=torch.float64)
	q_value: tensor([[-42.9629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5281267255980704, distance: 1.4146088764333182 entropy 0.6056879162788391
epoch: 37, step: 81
	action: tensor([[ 5.8769, -6.2800,  6.1659, -6.2800,  3.1607,  6.1800,  5.2405]],
       dtype=torch.float64)
	q_value: tensor([[-44.9758]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2462100021214328 entropy 0.6056879162788391
epoch: 37, step: 82
	action: tensor([[ 0.4907,  0.7100, -0.0654, -0.3840,  0.3551,  0.1749, -0.1203]],
       dtype=torch.float64)
	q_value: tensor([[-42.9629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 37, step: 83
	action: tensor([[ 0.2201, -0.0956, -0.6695, -1.1737,  0.1664, -0.1936,  0.4413]],
       dtype=torch.float64)
	q_value: tensor([[-42.9629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19645842374988687, distance: 1.025795689023371 entropy 0.6056879162788391
epoch: 37, step: 84
	action: tensor([[ 6.1800, -6.2800,  4.6058, -5.7801,  3.4588,  5.3278,  6.0064]],
       dtype=torch.float64)
	q_value: tensor([[-44.2466]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2643988877895707 entropy 0.6056879162788391
epoch: 37, step: 85
	action: tensor([[ 0.0858, -0.0874, -0.0654, -0.5504, -0.6347, -0.0162,  0.6960]],
       dtype=torch.float64)
	q_value: tensor([[-42.9629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06575131727078831, distance: 1.1060835676740592 entropy 0.6056879162788391
epoch: 37, step: 86
	action: tensor([[ 6.1800, -5.8842,  6.1800, -5.8629,  4.2481,  5.7462,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-41.2790]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 37, step: 87
	action: tensor([[-0.3641,  0.2266, -0.3632, -0.3408, -0.4024, -0.1549,  0.3288]],
       dtype=torch.float64)
	q_value: tensor([[-42.9629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.046851236678953634, distance: 1.117215719321889 entropy 0.6056879162788391
epoch: 37, step: 88
	action: tensor([[ 4.4845, -4.2940,  2.4582, -5.3293,  2.0696,  3.8670,  3.6841]],
       dtype=torch.float64)
	q_value: tensor([[-33.1053]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 37, step: 89
	action: tensor([[ 0.2783,  0.2364,  0.7856, -0.5570,  0.2765,  0.0662,  0.4706]],
       dtype=torch.float64)
	q_value: tensor([[-42.9629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37901530464759925, distance: 0.9017728208383533 entropy 0.6056879162788391
epoch: 37, step: 90
	action: tensor([[ 6.1800, -6.2800,  6.1173, -5.6692,  4.9897,  5.8564,  5.8923]],
       dtype=torch.float64)
	q_value: tensor([[-46.6354]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 37, step: 91
	action: tensor([[ 0.2246,  0.1813,  0.0454, -0.0601, -0.1493, -0.2110,  0.1722]],
       dtype=torch.float64)
	q_value: tensor([[-42.9629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5574263988198777, distance: 0.7612888094907142 entropy 0.6056879162788391
epoch: 37, step: 92
	action: tensor([[ 6.0588, -6.2800,  4.7809, -6.2800,  2.9316,  5.6559,  6.0292]],
       dtype=torch.float64)
	q_value: tensor([[-35.0045]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3722722109236958 entropy 0.6056879162788391
epoch: 37, step: 93
	action: tensor([[ 0.3784, -0.2470,  0.6807, -0.0241, -0.3396, -0.0399,  0.0576]],
       dtype=torch.float64)
	q_value: tensor([[-42.9629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36033457617721143, distance: 0.9152360779638836 entropy 0.6056879162788391
epoch: 37, step: 94
	action: tensor([[ 6.1800, -5.8219,  5.9882, -5.7922,  5.6789,  6.1800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-45.4122]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 37, step: 95
	action: tensor([[ 0.4960, -0.4899,  0.1008, -0.2026,  0.0896,  0.2795, -0.3939]],
       dtype=torch.float64)
	q_value: tensor([[-42.9629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14350984330214178, distance: 1.0590534549261694 entropy 0.6056879162788391
epoch: 37, step: 96
	action: tensor([[ 6.1800, -5.3987,  5.5699, -5.6258,  3.4844,  6.1800,  6.0299]],
       dtype=torch.float64)
	q_value: tensor([[-45.5730]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 37, step: 97
	action: tensor([[ 0.4388,  0.1361,  0.1410,  0.6090,  0.1587, -0.3364,  0.2698]],
       dtype=torch.float64)
	q_value: tensor([[-42.9629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 37, step: 98
	action: tensor([[-0.0860, -0.1820,  0.0254,  0.0715, -0.1450,  0.6944, -0.7267]],
       dtype=torch.float64)
	q_value: tensor([[-42.9629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27410337795201123, distance: 0.9749763496319181 entropy 0.6056879162788391
epoch: 37, step: 99
	action: tensor([[ 6.1800, -6.2800,  5.6087, -6.2800,  2.4262,  6.1599,  6.1327]],
       dtype=torch.float64)
	q_value: tensor([[-43.5031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.236653262776839 entropy 0.6056879162788391
epoch: 37, step: 100
	action: tensor([[-0.7041, -0.8330,  0.0964, -0.1851, -0.1521,  0.0332,  0.5081]],
       dtype=torch.float64)
	q_value: tensor([[-42.9629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.10896698189313, distance: 1.6618490895508649 entropy 0.6056879162788391
epoch: 37, step: 101
	action: tensor([[ 6.0264, -5.9687,  6.1800, -5.8412,  3.2748,  4.6967,  5.9342]],
       dtype=torch.float64)
	q_value: tensor([[-45.3995]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 37, step: 102
	action: tensor([[ 0.0111, -0.8474, -0.9015, -0.4711,  0.4503,  0.0710,  0.0859]],
       dtype=torch.float64)
	q_value: tensor([[-42.9629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3597218869482188, distance: 1.334386800672471 entropy 0.6056879162788391
epoch: 37, step: 103
	action: tensor([[ 6.1800, -6.2531,  3.8889, -6.2800,  2.3837,  5.5331,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-49.8294]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 37, step: 104
	action: tensor([[ 0.7456, -0.2426,  0.6272, -0.0258, -0.8690, -0.2678,  0.6682]],
       dtype=torch.float64)
	q_value: tensor([[-42.9629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4478946664188217, distance: 0.8502911611832632 entropy 0.6056879162788391
epoch: 37, step: 105
	action: tensor([[ 6.1800, -6.2800,  6.1269, -6.2800,  6.1800,  5.8259,  5.7160]],
       dtype=torch.float64)
	q_value: tensor([[-51.6860]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.060639752718063 entropy 0.6056879162788391
epoch: 37, step: 106
	action: tensor([[ 0.0230, -0.0173, -0.7408,  0.6628, -0.1985,  0.7332, -0.4475]],
       dtype=torch.float64)
	q_value: tensor([[-42.9629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 37, step: 107
	action: tensor([[ 6.8957e-01, -1.3878e-01, -1.9708e-02,  1.0148e-01, -8.5056e-01,
         -6.8473e-04, -2.0399e-01]], dtype=torch.float64)
	q_value: tensor([[-42.9629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6864433593442054, distance: 0.6407884789082107 entropy 0.6056879162788391
epoch: 37, step: 108
	action: tensor([[ 6.1546, -6.1673,  6.1800, -6.2800,  3.6855,  6.1800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-42.0243]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0273509123367726 entropy 0.6056879162788391
epoch: 37, step: 109
	action: tensor([[ 0.0973,  0.0484,  0.7500, -0.5683,  0.2889, -0.1843,  0.5590]],
       dtype=torch.float64)
	q_value: tensor([[-42.9629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.027680069666475404, distance: 1.1600739118230134 entropy 0.6056879162788391
epoch: 37, step: 110
	action: tensor([[ 5.9960, -5.8408,  6.1800, -5.9291,  4.6809,  5.5387,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-45.8979]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 37, step: 111
	action: tensor([[ 0.0472,  0.0186,  0.6256, -0.1786,  0.1420,  0.5159, -0.0507]],
       dtype=torch.float64)
	q_value: tensor([[-42.9629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42083569098651874, distance: 0.8708785393795296 entropy 0.6056879162788391
epoch: 37, step: 112
	action: tensor([[ 6.1800, -5.7591,  6.1800, -6.2800,  5.6257,  5.6042,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-43.8245]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 37, step: 113
	action: tensor([[ 0.2112, -0.5533, -0.2381,  0.0065,  0.0111,  0.2312, -1.0503]],
       dtype=torch.float64)
	q_value: tensor([[-42.9629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08050750543160323, distance: 1.0973136650488131 entropy 0.6056879162788391
epoch: 37, step: 114
	action: tensor([[ 6.1800, -6.2800,  4.3394, -6.2800,  3.1012,  6.1800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-48.1611]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.246749415482634 entropy 0.6056879162788391
epoch: 37, step: 115
	action: tensor([[ 0.1606, -0.0497, -0.3938, -0.5416, -0.0398, -0.1478, -0.0777]],
       dtype=torch.float64)
	q_value: tensor([[-42.9629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25433094978549997, distance: 0.9881656475759123 entropy 0.6056879162788391
epoch: 37, step: 116
	action: tensor([[ 4.1395, -5.5608,  2.9354, -4.9502,  2.2601,  4.0331,  4.5264]],
       dtype=torch.float64)
	q_value: tensor([[-36.2574]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 37, step: 117
	action: tensor([[ 0.6890, -1.3555, -0.3628,  0.3634, -0.3943,  0.5031,  1.1596]],
       dtype=torch.float64)
	q_value: tensor([[-42.9629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17253204619783147, distance: 1.2391362263821133 entropy 0.6056879162788391
epoch: 37, step: 118
	action: tensor([[ 6.1800, -5.7339,  6.1800, -5.8100,  5.4961,  5.9206,  5.8093]],
       dtype=torch.float64)
	q_value: tensor([[-60.1507]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 37, step: 119
	action: tensor([[-0.0973, -0.1006, -0.2590, -0.6928,  0.7405,  0.3710,  0.3620]],
       dtype=torch.float64)
	q_value: tensor([[-42.9629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.041612926679503226, distance: 1.120281507018457 entropy 0.6056879162788391
epoch: 37, step: 120
	action: tensor([[ 6.1800, -6.1184,  5.8725, -6.2800,  3.1659,  5.4368,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-46.1765]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 37, step: 121
	action: tensor([[ 0.1536, -0.3435, -0.2976,  0.4258, -0.4931,  0.3961,  0.5147]],
       dtype=torch.float64)
	q_value: tensor([[-42.9629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3468105314043919, distance: 0.9248606038348436 entropy 0.6056879162788391
epoch: 37, step: 122
	action: tensor([[ 6.1800, -6.2800,  5.8217, -6.1444,  4.7448,  6.0185,  5.7334]],
       dtype=torch.float64)
	q_value: tensor([[-40.5836]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9478771310517149 entropy 0.6056879162788391
epoch: 37, step: 123
	action: tensor([[ 0.1230, -0.5221, -0.1839, -0.2535, -0.4309,  0.6064,  0.2655]],
       dtype=torch.float64)
	q_value: tensor([[-42.9629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07879800219084765, distance: 1.1885754619254387 entropy 0.6056879162788391
epoch: 37, step: 124
	action: tensor([[ 6.1800, -6.2800,  5.4834, -6.2463,  4.1115,  6.1800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-43.2604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0676256703839786 entropy 0.6056879162788391
epoch: 37, step: 125
	action: tensor([[ 1.0654, -0.3131, -0.2135, -0.8758, -0.0368,  0.3052, -0.1131]],
       dtype=torch.float64)
	q_value: tensor([[-42.9629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03668084860978649, distance: 1.1651430019023832 entropy 0.6056879162788391
epoch: 37, step: 126
	action: tensor([[ 5.3520, -6.2800,  6.1626, -5.7561,  5.7923,  6.1800,  5.5485]],
       dtype=torch.float64)
	q_value: tensor([[-49.8601]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4431287977099154 entropy 0.6056879162788391
epoch: 37, step: 127
	action: tensor([[-0.2396, -0.2467,  0.1827, -0.2785, -0.0634,  0.4007,  0.3590]],
       dtype=torch.float64)
	q_value: tensor([[-42.9629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24294254063600285, distance: 1.2757988884513978 entropy 0.6056879162788391
LOSS epoch 37 actor 412.80073164536856 critic 50.508378707720304 
epoch: 38, step: 0
	action: tensor([[ 5.9725, -6.2800,  6.1800, -6.2800,  2.9336,  5.7584,  5.5443]],
       dtype=torch.float64)
	q_value: tensor([[-41.7102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1599352971596328 entropy 0.6056879162788391
epoch: 38, step: 1
	action: tensor([[ 0.3471,  0.4581,  0.8402, -0.2944, -0.1710,  0.0922, -0.3977]],
       dtype=torch.float64)
	q_value: tensor([[-39.5865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 38, step: 2
	action: tensor([[-0.2495, -0.4813,  1.2793, -1.1704,  0.6856, -0.2771,  0.8736]],
       dtype=torch.float64)
	q_value: tensor([[-39.5865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8518339800400989, distance: 1.5572477702778948 entropy 0.6056879162788391
epoch: 38, step: 3
	action: tensor([[ 6.1242, -6.0851,  5.3444, -5.9401,  6.1800,  6.1281,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-63.7980]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 38, step: 4
	action: tensor([[ 0.0533, -0.3796, -0.0441, -0.2549,  0.1241,  0.1564,  0.9442]],
       dtype=torch.float64)
	q_value: tensor([[-39.5865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06475006352771762, distance: 1.1808113816647603 entropy 0.6056879162788391
epoch: 38, step: 5
	action: tensor([[ 5.7174, -6.2800,  5.7737, -6.2800,  2.9202,  6.1800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-46.9526]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3953240526039326 entropy 0.6056879162788391
epoch: 38, step: 6
	action: tensor([[ 0.2396, -0.0762, -0.5142,  0.2900, -0.4636, -0.1132,  0.1398]],
       dtype=torch.float64)
	q_value: tensor([[-39.5865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5368321634327686, distance: 0.7787999051453639 entropy 0.6056879162788391
epoch: 38, step: 7
	action: tensor([[ 5.5960, -6.2800,  3.4479, -6.0644,  1.8768,  4.4529,  5.5921]],
       dtype=torch.float64)
	q_value: tensor([[-36.5980]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 38, step: 8
	action: tensor([[-0.2151, -0.8669,  0.4250,  0.1618, -0.2600,  0.3242,  0.1780]],
       dtype=torch.float64)
	q_value: tensor([[-39.5865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.44327449588679513, distance: 1.3747735309491618 entropy 0.6056879162788391
epoch: 38, step: 9
	action: tensor([[ 5.4234, -6.2800,  6.1800, -5.8423,  4.3242,  5.5137,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-52.2714]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2718144724043658 entropy 0.6056879162788391
epoch: 38, step: 10
	action: tensor([[-0.0625, -0.3206,  0.3338, -0.3622,  0.6427,  0.3638, -0.4807]],
       dtype=torch.float64)
	q_value: tensor([[-39.5865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14744932064203198, distance: 1.225810818258303 entropy 0.6056879162788391
epoch: 38, step: 11
	action: tensor([[ 6.1800, -6.1275,  5.8742, -6.2800,  2.7961,  6.1800,  5.9558]],
       dtype=torch.float64)
	q_value: tensor([[-50.9474]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0745387911890962 entropy 0.6056879162788391
epoch: 38, step: 12
	action: tensor([[ 0.5595, -1.2808, -0.6099,  0.2298,  0.3923,  0.3107,  0.4072]],
       dtype=torch.float64)
	q_value: tensor([[-39.5865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.39353208122731886, distance: 1.3508750449030928 entropy 0.6056879162788391
epoch: 38, step: 13
	action: tensor([[ 6.1800, -5.9691,  6.1800, -5.6064,  4.4253,  5.9016,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-60.1962]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 38, step: 14
	action: tensor([[ 0.0853,  0.0439,  0.4690, -1.3588,  0.2116,  0.3137,  0.1210]],
       dtype=torch.float64)
	q_value: tensor([[-39.5865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.25813503722654096, distance: 1.283572257105161 entropy 0.6056879162788391
epoch: 38, step: 15
	action: tensor([[ 6.1800, -6.2800,  6.1800, -6.2800,  4.4476,  5.9855,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-51.4919]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9555439678393753 entropy 0.6056879162788391
epoch: 38, step: 16
	action: tensor([[ 0.1905, -0.3858,  0.2423,  0.3300,  0.0886, -0.1854, -0.5055]],
       dtype=torch.float64)
	q_value: tensor([[-39.5865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2988214351336965, distance: 0.9582327477564323 entropy 0.6056879162788391
epoch: 38, step: 17
	action: tensor([[ 6.1800, -6.2453,  6.0683, -5.6234,  3.7279,  5.7807,  5.8544]],
       dtype=torch.float64)
	q_value: tensor([[-48.0504]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 38, step: 18
	action: tensor([[ 0.1208,  0.4236,  0.4566,  0.1202, -0.5454, -0.0346,  0.4561]],
       dtype=torch.float64)
	q_value: tensor([[-39.5865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 38, step: 19
	action: tensor([[ 1.2980e-01, -8.6649e-01,  1.1156e-01, -5.1828e-01, -4.9345e-01,
          2.1273e-01,  5.5916e-05]], dtype=torch.float64)
	q_value: tensor([[-39.5865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6695457923696615, distance: 1.4786175648177944 entropy 0.6056879162788391
epoch: 38, step: 20
	action: tensor([[ 6.1800, -5.4541,  6.1116, -5.6794,  2.9348,  6.1800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-51.3930]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 38, step: 21
	action: tensor([[ 0.1867,  0.5548,  0.3408, -0.4853, -0.1887,  0.1368, -0.4806]],
       dtype=torch.float64)
	q_value: tensor([[-39.5865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 38, step: 22
	action: tensor([[ 0.8221, -0.3515,  0.5953, -0.2054, -0.2708, -0.2640,  0.9308]],
       dtype=torch.float64)
	q_value: tensor([[-39.5865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24726579973947238, distance: 0.9928360030663472 entropy 0.6056879162788391
epoch: 38, step: 23
	action: tensor([[ 5.8357, -6.2800,  6.1800, -6.2800,  5.4287,  6.1800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-55.6076]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3439386859116498 entropy 0.6056879162788391
epoch: 38, step: 24
	action: tensor([[ 0.1289, -0.0272, -0.7754, -0.1098, -0.4967,  0.2788, -0.2004]],
       dtype=torch.float64)
	q_value: tensor([[-39.5865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37928600466039775, distance: 0.9015762487449477 entropy 0.6056879162788391
epoch: 38, step: 25
	action: tensor([[ 4.0207, -4.3424,  2.4474, -5.9130,  0.9385,  2.8596,  4.0244]],
       dtype=torch.float64)
	q_value: tensor([[-34.2080]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 38, step: 26
	action: tensor([[-0.7271,  0.6389,  0.0205, -0.2069,  0.1059, -0.1589,  0.2766]],
       dtype=torch.float64)
	q_value: tensor([[-39.5865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2313919153307602, distance: 1.2698570728074228 entropy 0.6056879162788391
epoch: 38, step: 27
	action: tensor([[ 5.6007, -5.3257,  2.6243, -6.2753,  1.9187,  3.6857,  5.2161]],
       dtype=torch.float64)
	q_value: tensor([[-34.9357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 38, step: 28
	action: tensor([[-0.0237, -0.6365, -0.6095,  0.0136,  0.1479,  0.3491,  0.7501]],
       dtype=torch.float64)
	q_value: tensor([[-39.5865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21773075171381295, distance: 1.2627934775148584 entropy 0.6056879162788391
epoch: 38, step: 29
	action: tensor([[ 6.1800, -6.2800,  5.5185, -5.8294,  1.9752,  6.1800,  6.0104]],
       dtype=torch.float64)
	q_value: tensor([[-47.3873]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1711003210871875 entropy 0.6056879162788391
epoch: 38, step: 30
	action: tensor([[ 0.2221,  0.4552,  0.6944, -0.3288, -0.1909, -0.0686, -0.3905]],
       dtype=torch.float64)
	q_value: tensor([[-39.5865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 38, step: 31
	action: tensor([[ 1.1717,  0.0283, -0.0757, -0.5828, -0.4227,  0.5094, -0.3596]],
       dtype=torch.float64)
	q_value: tensor([[-39.5865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 38, step: 32
	action: tensor([[ 0.1119,  0.1542, -1.1126, -0.0229, -0.2115,  0.1459,  0.0543]],
       dtype=torch.float64)
	q_value: tensor([[-39.5865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 38, step: 33
	action: tensor([[ 0.0025, -0.5840, -1.0874, -0.7287,  0.1940,  0.1889, -0.0873]],
       dtype=torch.float64)
	q_value: tensor([[-39.5865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.013761366790170593, distance: 1.1364431066800118 entropy 0.6056879162788391
epoch: 38, step: 34
	action: tensor([[ 4.9401, -5.5340,  2.5886, -5.7795,  0.9074,  2.9724,  4.2363]],
       dtype=torch.float64)
	q_value: tensor([[-46.9826]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 38, step: 35
	action: tensor([[ 0.3066, -0.2875,  0.5357, -1.0674,  0.2536, -0.0337,  0.2872]],
       dtype=torch.float64)
	q_value: tensor([[-39.5865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3436519751793681, distance: 1.3264781198116609 entropy 0.6056879162788391
epoch: 38, step: 36
	action: tensor([[ 6.1800, -6.2800,  6.0621, -6.2800,  4.7672,  6.1800,  5.9831]],
       dtype=torch.float64)
	q_value: tensor([[-51.0225]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0064294673618002 entropy 0.6056879162788391
epoch: 38, step: 37
	action: tensor([[ 0.0364, -1.0795, -0.2293,  0.5204, -0.0801,  1.0715, -0.7436]],
       dtype=torch.float64)
	q_value: tensor([[-39.5865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.011753005843134545, distance: 1.1510493526356256 entropy 0.6056879162788391
epoch: 38, step: 38
	action: tensor([[ 6.1800, -5.8765,  5.8972, -6.1576,  3.4002,  6.1800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-61.2344]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 38, step: 39
	action: tensor([[-0.4280, -0.1113,  0.0822, -0.4950,  0.0564, -0.0999,  0.6196]],
       dtype=torch.float64)
	q_value: tensor([[-39.5865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5245117550183087, distance: 1.412934670597291 entropy 0.6056879162788391
epoch: 38, step: 40
	action: tensor([[ 6.1800, -6.2800,  5.3710, -6.2800,  2.3751,  5.4483,  5.7165]],
       dtype=torch.float64)
	q_value: tensor([[-40.4495]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.190471329814996 entropy 0.6056879162788391
epoch: 38, step: 41
	action: tensor([[-0.4491, -0.1980,  0.2229, -0.1792,  0.3810,  0.2269,  0.2513]],
       dtype=torch.float64)
	q_value: tensor([[-39.5865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.40094464754837, distance: 1.3544631108850387 entropy 0.6056879162788391
epoch: 38, step: 42
	action: tensor([[ 6.1800, -5.6707,  4.6288, -5.9611,  4.1396,  6.1800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-41.9140]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 38, step: 43
	action: tensor([[ 0.7734, -0.2434, -0.4460,  0.1760,  0.0762,  0.8693,  0.1364]],
       dtype=torch.float64)
	q_value: tensor([[-39.5865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 38, step: 44
	action: tensor([[-0.2879, -0.0606, -0.1845,  0.6563, -0.1038, -0.0732,  0.3842]],
       dtype=torch.float64)
	q_value: tensor([[-39.5865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15900414990311496, distance: 1.0494303490807046 entropy 0.6056879162788391
epoch: 38, step: 45
	action: tensor([[ 5.6818, -5.7727,  3.7322, -6.2058,  2.9805,  6.1459,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-38.5692]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 38, step: 46
	action: tensor([[-0.1269,  0.1079, -0.5939,  0.3125,  0.2574, -0.2909, -0.3412]],
       dtype=torch.float64)
	q_value: tensor([[-39.5865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24766724891407177, distance: 0.9925712174688059 entropy 0.6056879162788391
epoch: 38, step: 47
	action: tensor([[ 2.4310, -2.9581,  1.0739, -3.3032,  0.0998,  1.5707,  2.4438]],
       dtype=torch.float64)
	q_value: tensor([[-35.0421]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 38, step: 48
	action: tensor([[ 0.2129, -0.7864, -0.0910, -0.4684,  0.3035, -0.0892,  0.0715]],
       dtype=torch.float64)
	q_value: tensor([[-39.5865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5540356530331418, distance: 1.4265506053619157 entropy 0.6056879162788391
epoch: 38, step: 49
	action: tensor([[ 6.1800, -6.2800,  5.1979, -5.9399,  2.7715,  5.6932,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-50.3311]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2209362702684867 entropy 0.6056879162788391
epoch: 38, step: 50
	action: tensor([[ 0.2206, -0.2489,  0.4807, -0.3673,  0.4296,  0.5699,  0.3661]],
       dtype=torch.float64)
	q_value: tensor([[-39.5865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2592448221922006, distance: 0.9849043173433801 entropy 0.6056879162788391
epoch: 38, step: 51
	action: tensor([[ 6.1800, -6.2800,  5.5560, -6.2800,  5.2234,  5.7136,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-50.9388]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9377850172817496 entropy 0.6056879162788391
epoch: 38, step: 52
	action: tensor([[ 0.3508,  0.0477,  0.2377,  0.0934, -0.4973,  0.1045,  0.0056]],
       dtype=torch.float64)
	q_value: tensor([[-39.5865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.669758903607249, distance: 0.6576158195377443 entropy 0.6056879162788391
epoch: 38, step: 53
	action: tensor([[ 6.1800, -6.2800,  5.9888, -6.2800,  3.5360,  6.1800,  5.9017]],
       dtype=torch.float64)
	q_value: tensor([[-40.5382]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0803604643520517 entropy 0.6056879162788391
epoch: 38, step: 54
	action: tensor([[-0.2392, -0.7164, -0.8553, -0.4576, -0.0478, -0.1824,  0.0832]],
       dtype=torch.float64)
	q_value: tensor([[-39.5865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3616706622898189, distance: 1.335342690639511 entropy 0.6056879162788391
epoch: 38, step: 55
	action: tensor([[ 3.4067, -4.4112,  2.1876, -5.0726,  1.4301,  3.2505,  4.1055]],
       dtype=torch.float64)
	q_value: tensor([[-45.8222]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 38, step: 56
	action: tensor([[ 0.5850, -0.2126,  0.1273, -0.1858, -0.3820,  0.2229, -0.5549]],
       dtype=torch.float64)
	q_value: tensor([[-39.5865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4755191921580605, distance: 0.82874609238894 entropy 0.6056879162788391
epoch: 38, step: 57
	action: tensor([[ 6.1800, -5.5427,  6.1800, -6.2800,  3.6016,  6.1800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-45.1203]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 38, step: 58
	action: tensor([[-0.1259, -0.5640,  0.5760, -0.1449, -0.0036,  1.1760,  0.3681]],
       dtype=torch.float64)
	q_value: tensor([[-39.5865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12852398622639793, distance: 1.0682783141440642 entropy 0.6056879162788391
epoch: 38, step: 59
	action: tensor([[ 6.0262, -6.2800,  5.5697, -6.2800,  6.1800,  6.0515,  5.8343]],
       dtype=torch.float64)
	q_value: tensor([[-57.1891]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2840901981309476 entropy 0.6056879162788391
epoch: 38, step: 60
	action: tensor([[ 0.3404,  0.0519,  0.2746, -0.1919, -0.7709,  0.4382,  0.4682]],
       dtype=torch.float64)
	q_value: tensor([[-39.5865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5516041345185858, distance: 0.7662800042825301 entropy 0.6056879162788391
epoch: 38, step: 61
	action: tensor([[ 5.9775, -6.1152,  6.1800, -5.9510,  4.1744,  6.1800,  5.8433]],
       dtype=torch.float64)
	q_value: tensor([[-45.8072]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 38, step: 62
	action: tensor([[ 0.5151, -0.3389, -0.5982,  0.1159,  0.0143, -0.2176, -0.4854]],
       dtype=torch.float64)
	q_value: tensor([[-39.5865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36637368924804814, distance: 0.91090543761726 entropy 0.6056879162788391
epoch: 38, step: 63
	action: tensor([[ 5.4646, -5.9765,  2.9609, -5.8692,  1.7045,  3.1816,  4.9982]],
       dtype=torch.float64)
	q_value: tensor([[-42.4151]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 38, step: 64
	action: tensor([[ 0.5304, -0.3867,  1.0050, -0.6895,  0.3907, -0.3527, -0.2925]],
       dtype=torch.float64)
	q_value: tensor([[-39.5865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09396752471904435, distance: 1.1969028695122752 entropy 0.6056879162788391
epoch: 38, step: 65
	action: tensor([[ 6.1800, -6.2800,  6.1800, -6.0050,  4.8132,  6.1781,  5.8914]],
       dtype=torch.float64)
	q_value: tensor([[-58.6702]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9470112711404903 entropy 0.6056879162788391
epoch: 38, step: 66
	action: tensor([[-0.1613,  0.2452,  0.4328, -0.2641, -0.7112, -0.1633,  0.2332]],
       dtype=torch.float64)
	q_value: tensor([[-39.5865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12285283999470253, distance: 1.071748597980639 entropy 0.6056879162788391
epoch: 38, step: 67
	action: tensor([[ 5.6986, -6.2800,  6.1463, -6.2800,  3.3540,  5.9537,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-40.5903]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3499382706411327 entropy 0.6056879162788391
epoch: 38, step: 68
	action: tensor([[ 0.3104, -0.5601, -0.0432,  0.4205,  0.1411,  0.6086, -0.2021]],
       dtype=torch.float64)
	q_value: tensor([[-39.5865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5160458218207661, distance: 0.7960838528203227 entropy 0.6056879162788391
epoch: 38, step: 69
	action: tensor([[ 6.1800, -5.7471,  6.1800, -5.9435,  3.8118,  5.7578,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-49.4262]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 38, step: 70
	action: tensor([[ 0.4855,  0.1499, -0.1091,  0.4067,  0.0542,  0.1323,  0.1257]],
       dtype=torch.float64)
	q_value: tensor([[-39.5865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 38, step: 71
	action: tensor([[-0.2673, -0.0867,  0.3849, -0.3401,  0.0851,  0.6895, -0.3645]],
       dtype=torch.float64)
	q_value: tensor([[-39.5865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04387963348781421, distance: 1.169181420826965 entropy 0.6056879162788391
epoch: 38, step: 72
	action: tensor([[ 6.0610, -5.8076,  6.1800, -6.2800,  3.5436,  6.1800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-45.6364]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 38, step: 73
	action: tensor([[ 0.0111,  0.4263, -0.9966, -0.0706, -0.3447, -0.1135,  0.8498]],
       dtype=torch.float64)
	q_value: tensor([[-39.5865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 38, step: 74
	action: tensor([[ 0.4899, -0.2235, -0.6307, -0.2153,  0.2989,  0.3186,  0.1457]],
       dtype=torch.float64)
	q_value: tensor([[-39.5865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.47773929357543743, distance: 0.8269902115916367 entropy 0.6056879162788391
epoch: 38, step: 75
	action: tensor([[ 6.1800, -6.2800,  4.7897, -6.2800,  2.6454,  5.9763,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-42.7679]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2781262748618158 entropy 0.6056879162788391
epoch: 38, step: 76
	action: tensor([[ 0.4595,  0.3009, -0.1393,  0.0360, -0.3963, -0.1406,  0.0432]],
       dtype=torch.float64)
	q_value: tensor([[-39.5865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8175881455506473, distance: 0.48874600728243806 entropy 0.6056879162788391
epoch: 38, step: 77
	action: tensor([[ 6.1800, -6.1788,  4.8573, -6.0788,  2.4594,  5.2328,  5.7304]],
       dtype=torch.float64)
	q_value: tensor([[-35.4388]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 38, step: 78
	action: tensor([[ 0.4994, -0.3774, -0.7956, -1.0279,  0.5915,  0.3222,  0.9898]],
       dtype=torch.float64)
	q_value: tensor([[-39.5865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15884479537787866, distance: 1.0495297690444843 entropy 0.6056879162788391
epoch: 38, step: 79
	action: tensor([[ 5.9526, -6.2800,  6.1800, -6.2800,  4.9714,  5.7683,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-57.1520]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1140288301086005 entropy 0.6056879162788391
epoch: 38, step: 80
	action: tensor([[-0.4247, -0.2156,  0.2180,  0.1059,  0.4473, -0.0395, -0.6914]],
       dtype=torch.float64)
	q_value: tensor([[-39.5865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3004779304234879, distance: 1.3049930131661318 entropy 0.6056879162788391
epoch: 38, step: 81
	action: tensor([[ 6.1800, -5.5987,  4.4951, -6.2311,  2.4846,  6.1007,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-47.6415]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 38, step: 82
	action: tensor([[ 0.9196, -0.4507,  0.0608, -0.5516,  0.8633,  0.0261, -0.4333]],
       dtype=torch.float64)
	q_value: tensor([[-39.5865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.050887598886077745, distance: 1.1730994382774964 entropy 0.6056879162788391
epoch: 38, step: 83
	action: tensor([[ 6.1800, -6.2800,  6.1325, -5.9021,  3.3391,  6.0696,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-59.3947]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9234731203109243 entropy 0.6056879162788391
epoch: 38, step: 84
	action: tensor([[-0.3554, -0.3720,  0.5193, -0.5485, -0.4792,  0.6876,  0.6401]],
       dtype=torch.float64)
	q_value: tensor([[-39.5865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5714344415348882, distance: 1.4345141187174206 entropy 0.6056879162788391
epoch: 38, step: 85
	action: tensor([[ 6.1800, -6.2800,  6.1800, -6.2800,  5.4740,  5.7195,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-50.9245]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.983660574969675 entropy 0.6056879162788391
epoch: 38, step: 86
	action: tensor([[ 0.4941, -0.3989,  0.1964, -0.7557, -0.7663, -0.5669,  0.6706]],
       dtype=torch.float64)
	q_value: tensor([[-39.5865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21250745984627462, distance: 1.2600822758801087 entropy 0.6056879162788391
epoch: 38, step: 87
	action: tensor([[ 6.1800, -6.2140,  6.1800, -6.2568,  3.9038,  5.9958,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-51.9657]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9288210467255527 entropy 0.6056879162788391
epoch: 38, step: 88
	action: tensor([[ 0.3344, -0.2445,  0.2069, -0.7697,  0.1668, -0.8809,  1.0329]],
       dtype=torch.float64)
	q_value: tensor([[-39.5865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21487353809918996, distance: 1.2613111343776369 entropy 0.6056879162788391
epoch: 38, step: 89
	action: tensor([[ 6.1622, -5.8808,  6.1625, -6.1253,  3.4042,  6.1800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-52.0411]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 38, step: 90
	action: tensor([[ 0.5547, -0.5514, -0.6075, -0.3330,  0.0390,  0.5106,  0.0459]],
       dtype=torch.float64)
	q_value: tensor([[-39.5865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1307154072732304, distance: 1.0669343178077715 entropy 0.6056879162788391
epoch: 38, step: 91
	action: tensor([[ 6.1800, -6.0697,  6.1800, -5.8635,  3.7861,  5.6742,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-47.6868]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 38, step: 92
	action: tensor([[ 0.5109, -0.6281, -0.0757, -0.8995, -0.1896,  0.1253,  0.8599]],
       dtype=torch.float64)
	q_value: tensor([[-39.5865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.46177816969517016, distance: 1.3835581889149395 entropy 0.6056879162788391
epoch: 38, step: 93
	action: tensor([[ 5.9451, -6.2800,  6.1241, -5.7436,  4.8138,  6.1800,  5.3499]],
       dtype=torch.float64)
	q_value: tensor([[-53.8854]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.131898665183191 entropy 0.6056879162788391
epoch: 38, step: 94
	action: tensor([[-0.1154, -0.5588, -0.2141, -1.2419, -0.7238,  0.6987, -0.0562]],
       dtype=torch.float64)
	q_value: tensor([[-39.5865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5854510201140444, distance: 1.440897567367786 entropy 0.6056879162788391
epoch: 38, step: 95
	action: tensor([[ 6.1800, -6.0664,  6.1800, -6.0485,  3.5404,  5.4696,  5.9899]],
       dtype=torch.float64)
	q_value: tensor([[-53.3896]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 38, step: 96
	action: tensor([[ 0.1064,  0.0462,  0.8335,  0.3078, -0.2840,  0.3032,  0.0863]],
       dtype=torch.float64)
	q_value: tensor([[-39.5865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 38, step: 97
	action: tensor([[-0.1125, -0.6560,  0.8729, -0.1755, -0.9949,  0.1836, -0.2653]],
       dtype=torch.float64)
	q_value: tensor([[-39.5865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4751437962540559, distance: 1.3898690110278797 entropy 0.6056879162788391
epoch: 38, step: 98
	action: tensor([[ 6.1800, -6.2800,  6.1800, -6.0787,  5.2203,  6.0053,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-57.3435]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0097167319620397 entropy 0.6056879162788391
epoch: 38, step: 99
	action: tensor([[ 1.0345, -0.0872,  0.6538, -0.3638,  0.0943, -0.5676,  0.2140]],
       dtype=torch.float64)
	q_value: tensor([[-39.5865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41730954448030444, distance: 0.8735256166959708 entropy 0.6056879162788391
epoch: 38, step: 100
	action: tensor([[ 6.0106, -6.2800,  6.1800, -6.2800,  4.6482,  5.4576,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-54.1256]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9831868688921961 entropy 0.6056879162788391
epoch: 38, step: 101
	action: tensor([[ 0.0969, -0.5373,  0.1270, -0.2441, -0.7420,  0.3153,  0.2024]],
       dtype=torch.float64)
	q_value: tensor([[-39.5865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20040980704988387, distance: 1.2537803547946498 entropy 0.6056879162788391
epoch: 38, step: 102
	action: tensor([[ 6.0984, -6.2800,  6.1759, -5.9546,  3.2082,  5.7319,  5.8407]],
       dtype=torch.float64)
	q_value: tensor([[-46.9005]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 38, step: 103
	action: tensor([[-0.4600, -0.1218, -0.1482, -0.6383,  0.1857, -0.3349,  0.0830]],
       dtype=torch.float64)
	q_value: tensor([[-39.5865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.49458140214637636, distance: 1.3989960238670527 entropy 0.6056879162788391
epoch: 38, step: 104
	action: tensor([[ 3.6743, -5.0005,  2.5107, -5.1499,  0.8595,  4.1101,  3.3324]],
       dtype=torch.float64)
	q_value: tensor([[-39.5749]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 38, step: 105
	action: tensor([[-0.1189,  0.2932,  0.5539, -0.1204,  0.1159,  0.0281,  0.2643]],
       dtype=torch.float64)
	q_value: tensor([[-39.5865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29794165370691916, distance: 0.9588337152838727 entropy 0.6056879162788391
epoch: 38, step: 106
	action: tensor([[ 6.1800, -6.2800,  6.1800, -6.2800,  2.8135,  5.9424,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-41.0324]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0419659793812868 entropy 0.6056879162788391
epoch: 38, step: 107
	action: tensor([[ 0.7050, -0.4018, -0.4295,  0.1505,  0.7141,  0.2061,  0.3776]],
       dtype=torch.float64)
	q_value: tensor([[-39.5865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5298714233106976, distance: 0.7846301975533742 entropy 0.6056879162788391
epoch: 38, step: 108
	action: tensor([[ 5.9568, -5.9281,  5.4378, -6.0765,  3.4995,  6.1800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-52.4079]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 38, step: 109
	action: tensor([[-0.2929,  0.0926, -0.7103, -0.2770, -0.2859,  0.4684,  0.5092]],
       dtype=torch.float64)
	q_value: tensor([[-39.5865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.010287695394924468, distance: 1.1502155247335588 entropy 0.6056879162788391
epoch: 38, step: 110
	action: tensor([[ 4.1044, -5.7502,  3.2320, -6.2743,  1.3109,  3.4690,  4.6746]],
       dtype=torch.float64)
	q_value: tensor([[-36.0473]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 38, step: 111
	action: tensor([[-0.3028,  0.0372, -0.2529, -0.2477,  0.2998, -0.6583, -0.0869]],
       dtype=torch.float64)
	q_value: tensor([[-39.5865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14708415177549372, distance: 1.2256157492655222 entropy 0.6056879162788391
epoch: 38, step: 112
	action: tensor([[ 1.7747, -3.0636,  1.8752, -3.4006,  1.5707,  2.1008,  2.7385]],
       dtype=torch.float64)
	q_value: tensor([[-37.6715]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 38, step: 113
	action: tensor([[ 0.4141,  0.3667, -0.6733, -0.7351,  0.1568,  0.2531, -0.1020]],
       dtype=torch.float64)
	q_value: tensor([[-39.5865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 38, step: 114
	action: tensor([[ 0.0030, -0.7158, -0.3450,  0.5734, -0.0695,  0.0014,  0.2720]],
       dtype=torch.float64)
	q_value: tensor([[-39.5865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0773961552348934, distance: 1.1878029621019703 entropy 0.6056879162788391
epoch: 38, step: 115
	action: tensor([[ 6.1800, -6.2703,  4.0377, -6.2800,  2.3024,  6.1800,  5.7497]],
       dtype=torch.float64)
	q_value: tensor([[-46.7630]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1055099143185685 entropy 0.6056879162788391
epoch: 38, step: 116
	action: tensor([[ 0.2100, -0.6931, -0.0464,  0.6416, -0.8727,  0.8674, -0.3693]],
       dtype=torch.float64)
	q_value: tensor([[-39.5865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3311199640171887, distance: 0.9359029370133269 entropy 0.6056879162788391
epoch: 38, step: 117
	action: tensor([[ 6.1800, -5.2876,  6.1800, -6.2800,  5.2145,  5.8001,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-54.7705]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 38, step: 118
	action: tensor([[ 0.3398, -1.2363,  0.3237, -1.2565,  0.7824,  0.0376,  0.3420]],
       dtype=torch.float64)
	q_value: tensor([[-39.5865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.025420858698988, distance: 1.6285996322264227 entropy 0.6056879162788391
epoch: 38, step: 119
	action: tensor([[ 5.6166, -6.2800,  6.1800, -6.2800,  5.5742,  6.1800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-68.3189]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4200383795382197 entropy 0.6056879162788391
epoch: 38, step: 120
	action: tensor([[-3.5212e-01, -8.8761e-01, -1.1124e-01,  6.1891e-01, -6.2038e-01,
          3.5325e-01,  5.3606e-05]], dtype=torch.float64)
	q_value: tensor([[-39.5865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4354470679275899, distance: 1.3710405021059255 entropy 0.6056879162788391
epoch: 38, step: 121
	action: tensor([[ 6.1800, -6.2800,  5.7099, -6.2800,  4.1357,  6.0037,  6.0860]],
       dtype=torch.float64)
	q_value: tensor([[-51.9167]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9835335436476002 entropy 0.6056879162788391
epoch: 38, step: 122
	action: tensor([[ 0.2317, -0.4999,  0.5404,  0.6456,  0.2097, -0.2302,  0.1876]],
       dtype=torch.float64)
	q_value: tensor([[-39.5865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.47849721988242566, distance: 0.8263899125426268 entropy 0.6056879162788391
epoch: 38, step: 123
	action: tensor([[ 6.1800, -6.2800,  6.1800, -6.1549,  4.7013,  6.1800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-54.2372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9825361656597686 entropy 0.6056879162788391
epoch: 38, step: 124
	action: tensor([[ 0.5030,  0.5135, -0.7929, -0.0562,  1.1465,  0.5451, -0.6175]],
       dtype=torch.float64)
	q_value: tensor([[-39.5865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 38, step: 125
	action: tensor([[-0.8888, -0.6404,  0.9754,  0.9060, -0.5905,  0.6626, -0.0859]],
       dtype=torch.float64)
	q_value: tensor([[-39.5865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06257648665888471, distance: 1.1796055139281807 entropy 0.6056879162788391
epoch: 38, step: 126
	action: tensor([[ 6.1800, -5.8056,  6.1800, -6.1412,  5.7986,  5.7902,  5.8506]],
       dtype=torch.float64)
	q_value: tensor([[-62.4858]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 38, step: 127
	action: tensor([[ 0.9251, -0.9532,  0.2850, -0.7673,  0.4595,  0.4502,  0.0646]],
       dtype=torch.float64)
	q_value: tensor([[-39.5865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6425023148148172, distance: 1.4665932722678112 entropy 0.6056879162788391
LOSS epoch 38 actor 343.78474665406037 critic 83.52881210168874 
epoch: 39, step: 0
	action: tensor([[ 6.1800, -6.2800,  5.7242, -5.8304,  0.4305,  6.1800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-65.6150]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0389380351662885 entropy 0.5003275275230408
epoch: 39, step: 1
	action: tensor([[ 0.0314, -0.6870,  0.3670, -0.3791, -0.1272,  0.1806,  0.0192]],
       dtype=torch.float64)
	q_value: tensor([[-35.7223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5066145275280289, distance: 1.404616509418317 entropy 0.5003275275230408
epoch: 39, step: 2
	action: tensor([[ 6.1800, -5.5943,  3.6179, -5.7121,  0.1016,  6.1800,  6.1284]],
       dtype=torch.float64)
	q_value: tensor([[-49.1984]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 39, step: 3
	action: tensor([[ 0.2280, -0.0131,  0.3938,  0.1813, -0.1290, -0.0198,  0.1616]],
       dtype=torch.float64)
	q_value: tensor([[-35.7223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5926364126521757, distance: 0.7303781906258004 entropy 0.5003275275230408
epoch: 39, step: 4
	action: tensor([[ 6.1800, -6.1865,  3.7216, -6.2800,  0.2349,  6.1800,  6.1575]],
       dtype=torch.float64)
	q_value: tensor([[-43.2264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 39, step: 5
	action: tensor([[-0.1027,  0.3166,  0.5247, -0.6591, -0.2818, -0.4947, -0.0969]],
       dtype=torch.float64)
	q_value: tensor([[-35.7223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06193009501605662, distance: 1.1792466676941171 entropy 0.5003275275230408
epoch: 39, step: 6
	action: tensor([[ 6.1800, -5.6443,  2.7125, -6.2800,  0.1571,  5.4326,  6.0442]],
       dtype=torch.float64)
	q_value: tensor([[-40.7383]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 39, step: 7
	action: tensor([[-0.3449, -0.0932,  0.1500, -0.8537,  0.1624,  0.1784,  0.5447]],
       dtype=torch.float64)
	q_value: tensor([[-35.7223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5766714826242294, distance: 1.436902497248608 entropy 0.5003275275230408
epoch: 39, step: 8
	action: tensor([[ 6.1779, -6.2800,  2.2202, -6.2800,  0.7634,  4.5921,  5.6469]],
       dtype=torch.float64)
	q_value: tensor([[-44.3890]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 39, step: 9
	action: tensor([[ 0.0621, -0.4573,  0.2832, -0.2913, -0.5457,  0.2889,  0.0748]],
       dtype=torch.float64)
	q_value: tensor([[-35.7223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17651291434566185, distance: 1.2412379421383892 entropy 0.5003275275230408
epoch: 39, step: 10
	action: tensor([[ 6.1800, -6.0757,  3.9181, -6.2800,  0.2664,  6.1800,  6.1453]],
       dtype=torch.float64)
	q_value: tensor([[-46.8051]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 39, step: 11
	action: tensor([[ 0.3146, -0.2204,  0.4402, -0.3461,  0.3062, -0.0226,  0.6208]],
       dtype=torch.float64)
	q_value: tensor([[-35.7223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14616023424922886, distance: 1.05741357587353 entropy 0.5003275275230408
epoch: 39, step: 12
	action: tensor([[ 6.1800, -6.1351,  4.1904, -5.8604,  0.3541,  6.1800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-49.1081]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 39, step: 13
	action: tensor([[ 0.5688,  0.0122, -0.3610,  0.3635,  0.1581,  0.6489,  0.3158]],
       dtype=torch.float64)
	q_value: tensor([[-35.7223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 39, step: 14
	action: tensor([[ 0.0090, -0.2256,  0.0593, -0.2117, -0.4103,  0.7505, -0.1061]],
       dtype=torch.float64)
	q_value: tensor([[-35.7223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1618428790198626, distance: 1.0476577085632297 entropy 0.5003275275230408
epoch: 39, step: 15
	action: tensor([[ 6.0315, -5.2789,  3.3916, -6.2800,  0.4488,  5.5289,  5.7393]],
       dtype=torch.float64)
	q_value: tensor([[-44.1033]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 39, step: 16
	action: tensor([[-0.1654,  0.1016, -0.0500, -0.5847, -0.4161,  0.0487,  0.1743]],
       dtype=torch.float64)
	q_value: tensor([[-35.7223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.008570105379631432, distance: 1.149237368163383 entropy 0.5003275275230408
epoch: 39, step: 17
	action: tensor([[ 4.9044, -6.0113,  2.1322, -6.2800,  0.4177,  4.2020,  4.9550]],
       dtype=torch.float64)
	q_value: tensor([[-37.3336]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 39, step: 18
	action: tensor([[ 0.2363, -0.6467, -0.2603, -0.3447, -0.2392,  0.4112,  0.8126]],
       dtype=torch.float64)
	q_value: tensor([[-35.7223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20069051841665564, distance: 1.2539269421608108 entropy 0.5003275275230408
epoch: 39, step: 19
	action: tensor([[ 6.1800, -6.2163,  4.1590, -6.2056, -0.1741,  6.1800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-50.3995]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 39, step: 20
	action: tensor([[-0.6536,  0.2890, -0.0730, -0.3202, -0.8446,  0.5264,  0.0776]],
       dtype=torch.float64)
	q_value: tensor([[-35.7223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.42694265853773317, distance: 1.3669730546557597 entropy 0.5003275275230408
epoch: 39, step: 21
	action: tensor([[ 5.8160, -6.1230,  2.6707, -6.2800,  0.2987,  4.4924,  6.1465]],
       dtype=torch.float64)
	q_value: tensor([[-38.7644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 39, step: 22
	action: tensor([[ 0.2340,  0.2502, -0.1303, -0.0839,  0.1270, -0.0335, -0.1602]],
       dtype=torch.float64)
	q_value: tensor([[-35.7223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 39, step: 23
	action: tensor([[ 0.2726,  0.2508, -0.1373, -0.3438, -0.4284,  0.3251,  0.7606]],
       dtype=torch.float64)
	q_value: tensor([[-35.7223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6057757504386685, distance: 0.7185026278076498 entropy 0.5003275275230408
epoch: 39, step: 24
	action: tensor([[ 6.1800, -6.2800,  4.0675, -6.2678,  0.9176,  5.9403,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-41.3701]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.981353708977951 entropy 0.5003275275230408
epoch: 39, step: 25
	action: tensor([[ 0.3568, -0.4358,  0.3283, -0.2036,  0.3507,  0.2189, -0.2306]],
       dtype=torch.float64)
	q_value: tensor([[-35.7223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11935212230339332, distance: 1.0738851569456183 entropy 0.5003275275230408
epoch: 39, step: 26
	action: tensor([[ 5.5749, -5.6608,  3.7991, -6.2800, -0.2353,  5.8921,  5.7577]],
       dtype=torch.float64)
	q_value: tensor([[-50.2671]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 39, step: 27
	action: tensor([[-0.1499,  0.0855,  0.0998, -0.5513,  0.0048, -0.1067,  0.4773]],
       dtype=torch.float64)
	q_value: tensor([[-35.7223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08994531181073451, distance: 1.1947005045736767 entropy 0.5003275275230408
epoch: 39, step: 28
	action: tensor([[ 5.5599, -6.2800,  2.8024, -6.2800,  0.0526,  4.2604,  5.8184]],
       dtype=torch.float64)
	q_value: tensor([[-38.8157]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 39, step: 29
	action: tensor([[ 0.2737,  0.2285,  0.2677, -0.4707, -0.6398,  0.6520, -0.3630]],
       dtype=torch.float64)
	q_value: tensor([[-35.7223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5500376799701194, distance: 0.7676173229101075 entropy 0.5003275275230408
epoch: 39, step: 30
	action: tensor([[ 6.1800, -6.1564,  3.8843, -6.1995,  1.1315,  5.4335,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-45.6498]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 39, step: 31
	action: tensor([[ 0.3160,  0.2479, -0.2340, -0.7558, -0.2727, -0.3769, -0.6685]],
       dtype=torch.float64)
	q_value: tensor([[-35.7223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44476644320760217, distance: 0.8526966291973608 entropy 0.5003275275230408
epoch: 39, step: 32
	action: tensor([[ 3.4272, -4.8620,  0.7814, -5.2925, -0.1093,  2.3703,  3.4679]],
       dtype=torch.float64)
	q_value: tensor([[-39.4156]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 39, step: 33
	action: tensor([[-0.7535,  0.1781,  0.4841, -0.0138,  0.0110,  0.0617, -0.0937]],
       dtype=torch.float64)
	q_value: tensor([[-35.7223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.48567629049162275, distance: 1.3948220027640128 entropy 0.5003275275230408
epoch: 39, step: 34
	action: tensor([[ 6.1800, -6.2800,  1.8215, -6.2800, -0.1600,  4.8911,  5.5344]],
       dtype=torch.float64)
	q_value: tensor([[-40.8386]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 39, step: 35
	action: tensor([[ 0.2610,  0.1061, -0.2465, -0.3800, -0.3251, -0.3629,  0.1414]],
       dtype=torch.float64)
	q_value: tensor([[-35.7223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4152039992979344, distance: 0.8751024308574968 entropy 0.5003275275230408
epoch: 39, step: 36
	action: tensor([[ 3.0569, -4.7142,  2.0997, -4.9782,  0.3699,  2.8820,  3.5696]],
       dtype=torch.float64)
	q_value: tensor([[-36.3518]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 39, step: 37
	action: tensor([[ 0.5715, -0.1907,  0.7732, -0.2570, -0.1785,  0.1399, -0.1731]],
       dtype=torch.float64)
	q_value: tensor([[-35.7223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4321371328861653, distance: 0.8623397958636546 entropy 0.5003275275230408
epoch: 39, step: 38
	action: tensor([[ 6.1800, -6.2800,  5.3344, -6.2800,  0.9658,  6.1800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-52.2169]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0842108741017837 entropy 0.5003275275230408
epoch: 39, step: 39
	action: tensor([[ 0.9826, -0.5674, -0.1952, -0.1489, -0.0767,  0.1088,  0.9323]],
       dtype=torch.float64)
	q_value: tensor([[-35.7223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11988512982299715, distance: 1.0735601260715297 entropy 0.5003275275230408
epoch: 39, step: 40
	action: tensor([[ 5.9309, -6.2762,  4.3657, -5.6098,  1.5036,  6.0107,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-57.5417]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4704186454837402 entropy 0.5003275275230408
epoch: 39, step: 41
	action: tensor([[ 0.1406, -0.4426, -0.3004,  0.7098,  0.1168,  0.1329,  0.4542]],
       dtype=torch.float64)
	q_value: tensor([[-35.7223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38663707257941804, distance: 0.8962216991983327 entropy 0.5003275275230408
epoch: 39, step: 42
	action: tensor([[ 6.1800, -6.2071,  3.3048, -6.0783,  0.1876,  4.7840,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-46.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 39, step: 43
	action: tensor([[ 0.4823,  0.1504, -0.4584, -0.1067,  0.5877,  0.2259,  0.5221]],
       dtype=torch.float64)
	q_value: tensor([[-35.7223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 39, step: 44
	action: tensor([[-0.3933, -0.2758,  0.4363, -0.4534, -0.1277,  0.0461,  0.7407]],
       dtype=torch.float64)
	q_value: tensor([[-35.7223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6602921566492048, distance: 1.4745141732236617 entropy 0.5003275275230408
epoch: 39, step: 45
	action: tensor([[ 5.8982, -6.0875,  3.2448, -6.2314,  0.0838,  5.7164,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-45.5391]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 39, step: 46
	action: tensor([[-0.4805, -0.7992, -0.1383,  0.1911,  0.7864, -0.5835, -0.2474]],
       dtype=torch.float64)
	q_value: tensor([[-35.7223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9287366176298248, distance: 1.589253425373937 entropy 0.5003275275230408
epoch: 39, step: 47
	action: tensor([[ 2.9090, -3.6961,  1.3834, -4.3510,  0.6672,  1.8539,  3.1377]],
       dtype=torch.float64)
	q_value: tensor([[-54.2991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 39, step: 48
	action: tensor([[ 0.2776, -0.4053, -0.1524, -0.5895, -0.4404, -0.0447,  0.0436]],
       dtype=torch.float64)
	q_value: tensor([[-35.7223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12514858201616597, distance: 1.213840522804738 entropy 0.5003275275230408
epoch: 39, step: 49
	action: tensor([[ 6.1327, -5.9187,  2.8360, -6.1547,  0.6302,  4.4831,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-43.9890]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 39, step: 50
	action: tensor([[ 0.6700,  0.3496,  0.0527, -0.3615, -0.5979,  0.7520, -0.2172]],
       dtype=torch.float64)
	q_value: tensor([[-35.7223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 39, step: 51
	action: tensor([[ 0.8981, -0.0898, -0.3079, -0.6665,  0.3488, -0.3217,  0.2586]],
       dtype=torch.float64)
	q_value: tensor([[-35.7223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 39, step: 52
	action: tensor([[-0.4290,  0.0646, -0.1708, -0.3480,  0.4066,  0.7989,  0.9032]],
       dtype=torch.float64)
	q_value: tensor([[-35.7223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.02134800520861324, distance: 1.1320636251397516 entropy 0.5003275275230408
epoch: 39, step: 53
	action: tensor([[ 6.1800, -6.0267,  3.4554, -6.1780,  0.2399,  6.1308,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-47.4050]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 39, step: 54
	action: tensor([[-0.3799, -0.6123,  0.0768, -0.2102, -1.2028, -0.0404,  0.0442]],
       dtype=torch.float64)
	q_value: tensor([[-35.7223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6825817643551204, distance: 1.484378933490163 entropy 0.5003275275230408
epoch: 39, step: 55
	action: tensor([[ 6.1800, -6.2800,  4.2772, -6.2800,  0.4567,  6.1800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-51.0554]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0639276434913503 entropy 0.5003275275230408
epoch: 39, step: 56
	action: tensor([[ 0.1865,  0.4040, -0.4793, -0.0727, -0.3105,  0.4071, -0.2158]],
       dtype=torch.float64)
	q_value: tensor([[-35.7223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 39, step: 57
	action: tensor([[ 0.0411,  0.0676, -0.8587, -0.4523, -0.2939,  0.6515,  0.5517]],
       dtype=torch.float64)
	q_value: tensor([[-35.7223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.392785828690822, distance: 0.8917182244520443 entropy 0.5003275275230408
epoch: 39, step: 58
	action: tensor([[ 5.4527, -5.5905,  2.6991, -5.3504,  0.0928,  3.6383,  4.7094]],
       dtype=torch.float64)
	q_value: tensor([[-39.4755]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 39, step: 59
	action: tensor([[ 0.3307, -0.0761, -0.0626, -1.2630, -0.1340,  0.2214,  0.2810]],
       dtype=torch.float64)
	q_value: tensor([[-35.7223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05862840805447678, distance: 1.1774120203670853 entropy 0.5003275275230408
epoch: 39, step: 60
	action: tensor([[ 6.1211, -6.1051,  3.2529, -6.2476,  0.1134,  6.1800,  5.0469]],
       dtype=torch.float64)
	q_value: tensor([[-48.0540]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 39, step: 61
	action: tensor([[ 0.1955,  0.2346, -0.0668, -0.2588,  0.1981, -0.1830,  0.1230]],
       dtype=torch.float64)
	q_value: tensor([[-35.7223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.49752140446884796, distance: 0.8111767166282504 entropy 0.5003275275230408
epoch: 39, step: 62
	action: tensor([[ 3.2014, -4.7867,  2.0120, -4.9049, -0.3422,  2.3312,  4.5686]],
       dtype=torch.float64)
	q_value: tensor([[-36.7422]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 39, step: 63
	action: tensor([[ 0.9255,  0.0979,  0.4890, -1.0223, -0.4391,  0.4912, -0.2332]],
       dtype=torch.float64)
	q_value: tensor([[-35.7223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.49762258589649555, distance: 0.8110950413576905 entropy 0.5003275275230408
epoch: 39, step: 64
	action: tensor([[ 5.3761, -6.2800,  5.5452, -6.2800,  0.8184,  5.8192,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-54.8063]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.5551165888582867 entropy 0.5003275275230408
epoch: 39, step: 65
	action: tensor([[ 0.1501,  0.2380,  0.5088, -0.1864, -0.3361,  0.5681,  0.2952]],
       dtype=torch.float64)
	q_value: tensor([[-35.7223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 39, step: 66
	action: tensor([[ 0.6297, -0.1026,  0.0204, -0.5280, -0.0784,  0.4302,  0.7089]],
       dtype=torch.float64)
	q_value: tensor([[-35.7223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4706853439228944, distance: 0.8325563793656846 entropy 0.5003275275230408
epoch: 39, step: 67
	action: tensor([[ 6.1007, -5.8691,  4.6635, -6.2800,  0.2425,  6.1800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-48.9365]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 39, step: 68
	action: tensor([[-0.0590, -0.3313,  0.6028, -0.4786, -0.0625, -0.1019,  0.2787]],
       dtype=torch.float64)
	q_value: tensor([[-35.7223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4287953934777915, distance: 1.3678602021411015 entropy 0.5003275275230408
epoch: 39, step: 69
	action: tensor([[ 6.1800, -5.1172,  4.0663, -6.2800,  0.8482,  6.1800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-46.3837]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 39, step: 70
	action: tensor([[-0.1858, -0.5860, -0.3656,  0.4778,  0.0405,  0.6212,  0.2882]],
       dtype=torch.float64)
	q_value: tensor([[-35.7223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0731942350875745, distance: 1.1854844420989554 entropy 0.5003275275230408
epoch: 39, step: 71
	action: tensor([[ 6.1800, -6.2800,  3.0655, -6.2800,  0.7770,  4.5145,  5.8290]],
       dtype=torch.float64)
	q_value: tensor([[-45.5334]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 39, step: 72
	action: tensor([[-0.3241, -0.3584, -0.1282, -0.8239,  0.0606,  0.7166,  0.7009]],
       dtype=torch.float64)
	q_value: tensor([[-35.7223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4295656843052853, distance: 1.3682288722107834 entropy 0.5003275275230408
epoch: 39, step: 73
	action: tensor([[ 6.1800, -6.0073,  3.4031, -6.1049,  0.3378,  6.1800,  5.9757]],
       dtype=torch.float64)
	q_value: tensor([[-49.3742]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 39, step: 74
	action: tensor([[ 1.2255, -0.4035,  0.4897, -0.0584,  0.4267,  0.2511, -0.4312]],
       dtype=torch.float64)
	q_value: tensor([[-35.7223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29117071946464834, distance: 0.963446310342042 entropy 0.5003275275230408
epoch: 39, step: 75
	action: tensor([[ 6.1800, -6.1486,  6.1629, -6.2800, -0.2101,  6.1800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-63.2368]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9789901028998306 entropy 0.5003275275230408
epoch: 39, step: 76
	action: tensor([[-0.4547, -0.6568, -0.7221,  0.2914, -0.3745,  0.0115, -0.1992]],
       dtype=torch.float64)
	q_value: tensor([[-35.7223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.725700101285278, distance: 1.503278186296203 entropy 0.5003275275230408
epoch: 39, step: 77
	action: tensor([[ 2.9829, -2.3550,  0.7658, -3.3657, -0.3431,  2.2952,  2.4382]],
       dtype=torch.float64)
	q_value: tensor([[-43.5081]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 39, step: 78
	action: tensor([[ 0.2509, -0.6933,  0.0136, -0.1469,  0.1821,  0.5739,  0.0449]],
       dtype=torch.float64)
	q_value: tensor([[-35.7223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.019866469022251865, distance: 1.1556553920206742 entropy 0.5003275275230408
epoch: 39, step: 79
	action: tensor([[ 5.8423, -6.2800,  4.0801, -6.0868, -0.0299,  5.4994,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-50.8099]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.178801101951759 entropy 0.5003275275230408
epoch: 39, step: 80
	action: tensor([[ 0.4278, -0.1085,  0.3721, -0.2663,  0.3170,  0.0580, -0.2441]],
       dtype=torch.float64)
	q_value: tensor([[-35.7223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3945011551079023, distance: 0.8904578210676677 entropy 0.5003275275230408
epoch: 39, step: 81
	action: tensor([[ 6.1240, -6.2800,  3.1410, -6.2800,  0.4082,  5.7855,  6.0914]],
       dtype=torch.float64)
	q_value: tensor([[-47.0727]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 39, step: 82
	action: tensor([[ 0.4787, -0.1625, -0.3713, -0.1543, -0.6929,  0.5040,  0.3752]],
       dtype=torch.float64)
	q_value: tensor([[-35.7223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.49670714333073496, distance: 0.8118337021314705 entropy 0.5003275275230408
epoch: 39, step: 83
	action: tensor([[ 5.9686, -6.1166,  3.5692, -6.2296,  0.2425,  5.4611,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-43.6695]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 39, step: 84
	action: tensor([[ 0.8088, -0.3707,  0.6364, -0.7669, -1.0922,  0.3133,  0.0424]],
       dtype=torch.float64)
	q_value: tensor([[-35.7223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08377564539917104, distance: 1.0953618454767389 entropy 0.5003275275230408
epoch: 39, step: 85
	action: tensor([[ 6.1435, -6.2800,  6.1800, -6.2800,  1.1179,  6.1800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-59.6210]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1385633273658073 entropy 0.5003275275230408
epoch: 39, step: 86
	action: tensor([[ 0.2177, -0.8845, -0.5895, -0.0606, -0.8451, -0.0633,  0.2484]],
       dtype=torch.float64)
	q_value: tensor([[-35.7223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.36245005086606596, distance: 1.3357247954995546 entropy 0.5003275275230408
epoch: 39, step: 87
	action: tensor([[ 6.1800, -6.2049,  2.6338, -6.1342,  0.6199,  4.8218,  5.8765]],
       dtype=torch.float64)
	q_value: tensor([[-50.8323]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 39, step: 88
	action: tensor([[ 0.2036, -0.9291,  0.0064, -0.4911, -0.8408,  0.0865,  0.1689]],
       dtype=torch.float64)
	q_value: tensor([[-35.7223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6991852245666967, distance: 1.4916847669760225 entropy 0.5003275275230408
epoch: 39, step: 89
	action: tensor([[ 5.8186, -6.2800,  4.5381, -5.3364,  0.9159,  6.1800,  5.6972]],
       dtype=torch.float64)
	q_value: tensor([[-54.7645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 39, step: 90
	action: tensor([[ 0.1089,  0.5155, -0.1182, -0.4633,  0.0403, -0.1118,  0.2886]],
       dtype=torch.float64)
	q_value: tensor([[-35.7223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 39, step: 91
	action: tensor([[-0.0710, -0.2707,  0.0699, -0.2914,  0.3829, -0.5539,  0.2720]],
       dtype=torch.float64)
	q_value: tensor([[-35.7223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.29912503649749356, distance: 1.3043140410406506 entropy 0.5003275275230408
epoch: 39, step: 92
	action: tensor([[ 4.1581, -4.6537,  0.5857, -4.1948,  0.5054,  1.7911,  3.7853]],
       dtype=torch.float64)
	q_value: tensor([[-43.6255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 39, step: 93
	action: tensor([[ 0.0939,  0.4239, -0.3027,  0.2215,  0.2712, -0.2639,  1.5787]],
       dtype=torch.float64)
	q_value: tensor([[-35.7223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 39, step: 94
	action: tensor([[ 0.6612, -0.4197, -0.1041, -0.2056, -1.0797, -0.1007,  0.8459]],
       dtype=torch.float64)
	q_value: tensor([[-35.7223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19307619343390292, distance: 1.0279522881043421 entropy 0.5003275275230408
epoch: 39, step: 95
	action: tensor([[ 6.0963, -5.7420,  4.6996, -5.9359, -0.4556,  6.1800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-53.6445]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 39, step: 96
	action: tensor([[ 0.1273, -0.4975, -0.0986, -0.2780, -0.2807,  0.5697, -0.5522]],
       dtype=torch.float64)
	q_value: tensor([[-35.7223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.019314695723448905, distance: 1.155342730472264 entropy 0.5003275275230408
epoch: 39, step: 97
	action: tensor([[ 6.1800, -6.2800,  2.6940, -6.2800, -0.0175,  4.8040,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-46.5763]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 39, step: 98
	action: tensor([[ 0.1156, -0.0607, -0.0259, -0.2947, -0.4839,  0.6062, -0.3636]],
       dtype=torch.float64)
	q_value: tensor([[-35.7223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3176751482859026, distance: 0.9452621928946047 entropy 0.5003275275230408
epoch: 39, step: 99
	action: tensor([[ 6.1800, -5.9282,  2.4508, -6.1510,  0.8061,  4.8749,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-42.2769]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 39, step: 100
	action: tensor([[ 0.1795, -0.4864,  0.1125, -0.0494,  0.1583,  0.5204,  0.1558]],
       dtype=torch.float64)
	q_value: tensor([[-35.7223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20158552698573085, distance: 1.0225178394785226 entropy 0.5003275275230408
epoch: 39, step: 101
	action: tensor([[ 6.1800, -6.2800,  3.6442, -6.2800,  0.4661,  6.1800,  5.8610]],
       dtype=torch.float64)
	q_value: tensor([[-47.3425]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 39, step: 102
	action: tensor([[-0.3776, -0.7294,  0.1795, -0.3847, -0.0615,  0.1087,  0.1670]],
       dtype=torch.float64)
	q_value: tensor([[-35.7223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8435599644044531, distance: 1.5537649751083615 entropy 0.5003275275230408
epoch: 39, step: 103
	action: tensor([[ 6.1800, -6.0960,  3.6303, -6.2800, -0.0681,  5.6127,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-47.2696]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 39, step: 104
	action: tensor([[-0.7321, -0.7086,  0.5549, -0.7733, -0.2579, -0.1125, -0.4691]],
       dtype=torch.float64)
	q_value: tensor([[-35.7223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2809148154400405, distance: 1.7282685474070976 entropy 0.5003275275230408
epoch: 39, step: 105
	action: tensor([[ 6.1800, -6.2800,  3.3005, -6.2800,  0.9943,  5.9051,  6.0239]],
       dtype=torch.float64)
	q_value: tensor([[-53.6148]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 39, step: 106
	action: tensor([[-0.1597, -0.7935,  0.5754, -0.7315,  0.0424,  0.6790, -0.1369]],
       dtype=torch.float64)
	q_value: tensor([[-35.7223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.770006603739195, distance: 1.5224538494143731 entropy 0.5003275275230408
epoch: 39, step: 107
	action: tensor([[ 6.1130, -6.2800,  5.5890, -6.2800,  1.3357,  6.1800,  5.6006]],
       dtype=torch.float64)
	q_value: tensor([[-56.8448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1697256892951884 entropy 0.5003275275230408
epoch: 39, step: 108
	action: tensor([[ 0.3673, -1.1821,  0.6936, -0.2923,  0.1660,  0.1531,  0.2483]],
       dtype=torch.float64)
	q_value: tensor([[-35.7223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8090124148214051, distance: 1.5391376678003128 entropy 0.5003275275230408
epoch: 39, step: 109
	action: tensor([[ 6.1192, -6.2800,  6.1800, -6.2800,  0.3660,  5.7416,  5.7421]],
       dtype=torch.float64)
	q_value: tensor([[-64.2633]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1771281309692156 entropy 0.5003275275230408
epoch: 39, step: 110
	action: tensor([[ 0.9232, -0.2133, -0.3707, -0.1668, -0.4466,  0.1540,  0.7561]],
       dtype=torch.float64)
	q_value: tensor([[-35.7223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 39, step: 111
	action: tensor([[ 0.8632,  0.1067,  0.6177, -0.4259, -0.2428, -0.2083, -0.0348]],
       dtype=torch.float64)
	q_value: tensor([[-35.7223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6062787106367037, distance: 0.7180441405867546 entropy 0.5003275275230408
epoch: 39, step: 112
	action: tensor([[ 5.9093, -6.2800,  4.9689, -6.2800,  0.0266,  6.1800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-49.9519]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3767637585678858 entropy 0.5003275275230408
epoch: 39, step: 113
	action: tensor([[-0.0845, -0.6430, -0.5338, -0.1273,  0.4401,  0.1813,  0.5287]],
       dtype=torch.float64)
	q_value: tensor([[-35.7223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.32408206308253273, distance: 1.3167827980494435 entropy 0.5003275275230408
epoch: 39, step: 114
	action: tensor([[ 4.6851, -6.2800,  2.5931, -6.2800,  0.4176,  3.2192,  4.4930]],
       dtype=torch.float64)
	q_value: tensor([[-47.1142]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 39, step: 115
	action: tensor([[ 0.2799, -0.0094, -0.1756, -0.3279,  0.6861,  0.2355,  0.3618]],
       dtype=torch.float64)
	q_value: tensor([[-35.7223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4751031062628702, distance: 0.8290747614182588 entropy 0.5003275275230408
epoch: 39, step: 116
	action: tensor([[ 5.9920, -6.2800,  1.8357, -6.2800,  0.2156,  4.8580,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-45.0748]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 39, step: 117
	action: tensor([[ 0.0883,  0.0143,  0.4404, -0.9251,  0.6323,  0.3376,  0.0221]],
       dtype=torch.float64)
	q_value: tensor([[-35.7223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06702820445176583, distance: 1.1820739396350295 entropy 0.5003275275230408
epoch: 39, step: 118
	action: tensor([[ 5.4848, -6.2800,  3.3290, -6.2800,  0.5910,  6.1800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-49.8012]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4872804840625817 entropy 0.5003275275230408
epoch: 39, step: 119
	action: tensor([[-0.2568,  0.1042,  0.5826, -0.9142, -0.3880,  0.2739,  0.2055]],
       dtype=torch.float64)
	q_value: tensor([[-35.7223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4609593856913332, distance: 1.3831706492680886 entropy 0.5003275275230408
epoch: 39, step: 120
	action: tensor([[ 6.0056, -6.0699,  4.6870, -6.2553,  1.2918,  6.1291,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-45.3851]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.21035824575623 entropy 0.5003275275230408
epoch: 39, step: 121
	action: tensor([[ 1.0189,  0.5151,  0.3445,  0.4781, -0.0895,  0.0534,  0.2591]],
       dtype=torch.float64)
	q_value: tensor([[-35.7223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 39, step: 122
	action: tensor([[-0.1148, -0.2070,  0.3566,  0.3122, -0.1293,  0.3820,  0.0126]],
       dtype=torch.float64)
	q_value: tensor([[-35.7223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34513172467979325, distance: 0.9260483647410868 entropy 0.5003275275230408
epoch: 39, step: 123
	action: tensor([[ 5.6584, -6.0690,  3.6662, -6.2800, -0.2398,  5.9599,  5.8527]],
       dtype=torch.float64)
	q_value: tensor([[-44.1369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 39, step: 124
	action: tensor([[-0.6011,  0.3115,  0.5088, -0.4613,  0.8655, -0.3105, -0.1245]],
       dtype=torch.float64)
	q_value: tensor([[-35.7223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6260281903408127, distance: 1.4592198493185202 entropy 0.5003275275230408
epoch: 39, step: 125
	action: tensor([[ 3.8425, -4.1798,  1.8407, -4.6235, -0.1142,  2.6586,  3.5104]],
       dtype=torch.float64)
	q_value: tensor([[-46.7227]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 39, step: 126
	action: tensor([[-0.3450,  0.3352,  0.8387, -0.3121,  0.3962, -0.2118,  0.1510]],
       dtype=torch.float64)
	q_value: tensor([[-35.7223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.19276298484949628, distance: 1.2497805737888197 entropy 0.5003275275230408
epoch: 39, step: 127
	action: tensor([[ 6.1800, -5.6398,  3.1418, -5.2555, -0.1794,  6.0530,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-45.9650]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
LOSS epoch 39 actor 282.51135222517485 critic 124.43783735058582 
epoch: 40, step: 0
	action: tensor([[ 0.2037, -0.3954, -0.1291, -0.6035,  0.0898, -0.6539,  0.9100]],
       dtype=torch.float64)
	q_value: tensor([[-33.3404]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.27941351638807466, distance: 1.2943810928381556 entropy 0.5003275275230408
epoch: 40, step: 1
	action: tensor([[ 6.0591, -5.9610,  4.3615, -6.2800, -1.9333,  4.8352,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-48.8434]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 40, step: 2
	action: tensor([[ 0.2183, -0.7982,  0.7585, -0.5589, -0.6765,  0.0669, -0.3720]],
       dtype=torch.float64)
	q_value: tensor([[-33.3404]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6433679761800215, distance: 1.4669796966547122 entropy 0.5003275275230408
epoch: 40, step: 3
	action: tensor([[ 6.0931, -6.1483,  6.1800, -6.2612, -3.6131,  6.1800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-59.2321]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0659314780656373 entropy 0.5003275275230408
epoch: 40, step: 4
	action: tensor([[ 0.4107, -0.4014, -0.3234, -0.8070,  0.1067,  0.0231,  0.1251]],
       dtype=torch.float64)
	q_value: tensor([[-33.3404]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11575599363147071, distance: 1.2087634164863184 entropy 0.5003275275230408
epoch: 40, step: 5
	action: tensor([[ 6.1800, -6.0247,  3.4644, -5.6019, -1.3241,  4.0563,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-46.4174]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 40, step: 6
	action: tensor([[ 0.1279, -0.5760,  0.6781, -0.8074,  0.2376,  0.1750, -0.5284]],
       dtype=torch.float64)
	q_value: tensor([[-33.3404]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6004897654395371, distance: 1.4477152324079894 entropy 0.5003275275230408
epoch: 40, step: 7
	action: tensor([[ 6.0742, -6.2800,  6.1334, -6.0940, -3.6030,  6.1800,  6.0791]],
       dtype=torch.float64)
	q_value: tensor([[-56.0943]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1024338722577873 entropy 0.5003275275230408
epoch: 40, step: 8
	action: tensor([[-0.4253, -0.2567,  0.2199, -0.4905,  0.3448, -0.4299, -0.0056]],
       dtype=torch.float64)
	q_value: tensor([[-33.3404]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.789752721996404, distance: 1.5309225110542084 entropy 0.5003275275230408
epoch: 40, step: 9
	action: tensor([[ 4.6921, -5.9177,  2.4192, -6.1853, -1.3141,  3.4602,  5.2088]],
       dtype=torch.float64)
	q_value: tensor([[-44.0449]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 40, step: 10
	action: tensor([[ 0.4653, -0.6873,  0.5282,  0.1003,  0.2528, -0.1640,  0.1573]],
       dtype=torch.float64)
	q_value: tensor([[-33.3404]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0005278419267524814, distance: 1.14464623059423 entropy 0.5003275275230408
epoch: 40, step: 11
	action: tensor([[ 5.9545, -6.0001,  5.9987, -6.2800, -3.2990,  6.1795,  5.6074]],
       dtype=torch.float64)
	q_value: tensor([[-56.7700]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2382303602525835 entropy 0.5003275275230408
epoch: 40, step: 12
	action: tensor([[ 0.2159, -0.7804,  0.7976, -0.9008, -0.4276,  0.5972, -0.2604]],
       dtype=torch.float64)
	q_value: tensor([[-33.3404]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6070353464392333, distance: 1.4506725985089004 entropy 0.5003275275230408
epoch: 40, step: 13
	action: tensor([[ 6.1800, -5.9569,  6.1800, -6.0743, -3.5975,  6.1800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-61.3489]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 40, step: 14
	action: tensor([[-0.7867, -0.1689,  0.4098, -0.4569,  0.4813, -0.7509,  0.7550]],
       dtype=torch.float64)
	q_value: tensor([[-33.3404]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.190995758235414, distance: 1.6938598020561417 entropy 0.5003275275230408
epoch: 40, step: 15
	action: tensor([[ 6.1800, -5.8261,  2.9683, -6.2800, -1.9415,  4.6857,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-45.6874]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 40, step: 16
	action: tensor([[-0.2517,  0.3542,  0.4971, -0.7403,  0.3105,  0.5902, -0.0459]],
       dtype=torch.float64)
	q_value: tensor([[-33.3404]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.02199096696876046, distance: 1.1316916884272734 entropy 0.5003275275230408
epoch: 40, step: 17
	action: tensor([[ 6.1800, -6.2800,  5.3314, -6.2800, -2.5472,  6.1800,  6.0812]],
       dtype=torch.float64)
	q_value: tensor([[-46.0477]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0624545573513426 entropy 0.5003275275230408
epoch: 40, step: 18
	action: tensor([[ 0.1856,  0.0113,  0.2550, -0.3783, -0.5527,  0.7131,  0.1381]],
       dtype=torch.float64)
	q_value: tensor([[-33.3404]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4182245710712891, distance: 0.8728394777686664 entropy 0.5003275275230408
epoch: 40, step: 19
	action: tensor([[ 6.1119, -5.8418,  6.1800, -5.7884, -2.6653,  6.0079,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-46.4239]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 40, step: 20
	action: tensor([[ 0.6145, -0.4082,  0.0414, -0.3507,  0.1463,  0.2702,  0.4112]],
       dtype=torch.float64)
	q_value: tensor([[-33.3404]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19910637709252088, distance: 1.0241041147122198 entropy 0.5003275275230408
epoch: 40, step: 21
	action: tensor([[ 6.1800, -6.1048,  6.1800, -6.2800, -2.3584,  6.1800,  6.1361]],
       dtype=torch.float64)
	q_value: tensor([[-50.0543]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9232367367823465 entropy 0.5003275275230408
epoch: 40, step: 22
	action: tensor([[ 0.1086, -0.5657,  0.2569, -0.3787,  0.2159, -0.0304,  0.6165]],
       dtype=torch.float64)
	q_value: tensor([[-33.3404]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.393100884984922, distance: 1.3506660302281472 entropy 0.5003275275230408
epoch: 40, step: 23
	action: tensor([[ 6.1800, -5.8179,  4.9970, -6.1713, -2.9093,  6.0206,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-49.8058]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 40, step: 24
	action: tensor([[-0.2531, -0.7659,  0.7298, -0.8959, -0.5612,  0.3718,  0.3949]],
       dtype=torch.float64)
	q_value: tensor([[-33.3404]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0661180366130454, distance: 1.64488014274782 entropy 0.5003275275230408
epoch: 40, step: 25
	action: tensor([[ 6.1800, -6.2593,  5.7935, -5.7669, -3.9587,  6.1800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-58.1258]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1528943411782784 entropy 0.5003275275230408
epoch: 40, step: 26
	action: tensor([[ 0.3872, -0.4665, -0.2725, -0.9859, -0.1877,  0.4340, -0.2295]],
       dtype=torch.float64)
	q_value: tensor([[-33.3404]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18958877761633897, distance: 1.248116494078941 entropy 0.5003275275230408
epoch: 40, step: 27
	action: tensor([[ 5.9486, -5.7371,  4.4351, -5.5985, -2.6895,  5.2744,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-50.1971]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 40, step: 28
	action: tensor([[-0.4880, -0.3208,  0.1663, -0.3177,  0.4116,  0.1300,  0.5867]],
       dtype=torch.float64)
	q_value: tensor([[-33.3404]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6450787165171601, distance: 1.4677430584185287 entropy 0.5003275275230408
epoch: 40, step: 29
	action: tensor([[ 6.1800, -6.2800,  3.9696, -6.2800, -2.5242,  5.9181,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-43.8272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1907973189233676 entropy 0.5003275275230408
epoch: 40, step: 30
	action: tensor([[ 0.5549, -0.0306,  0.5369, -0.4839,  0.0507,  0.1022, -0.5646]],
       dtype=torch.float64)
	q_value: tensor([[-33.3404]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4148007831097713, distance: 0.8754040699449706 entropy 0.5003275275230408
epoch: 40, step: 31
	action: tensor([[ 6.1800, -5.8250,  5.1392, -6.2800, -2.5003,  5.9249,  6.0716]],
       dtype=torch.float64)
	q_value: tensor([[-49.7409]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 40, step: 32
	action: tensor([[-0.1704, -0.0134,  0.2443, -0.4479, -0.6125,  0.6905,  0.8568]],
       dtype=torch.float64)
	q_value: tensor([[-33.3404]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04742771914204802, distance: 1.1711667248155044 entropy 0.5003275275230408
epoch: 40, step: 33
	action: tensor([[ 6.1313, -5.8643,  5.6805, -5.8884, -3.6005,  6.0348,  5.8664]],
       dtype=torch.float64)
	q_value: tensor([[-48.4474]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 40, step: 34
	action: tensor([[ 0.0729, -0.6915, -0.4950, -1.2868, -0.1086,  0.0733, -0.0594]],
       dtype=torch.float64)
	q_value: tensor([[-33.3404]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3433798737526921, distance: 1.3263438012267195 entropy 0.5003275275230408
epoch: 40, step: 35
	action: tensor([[ 6.1800, -6.2800,  2.3815, -6.1357, -2.5618,  4.6181,  6.1603]],
       dtype=torch.float64)
	q_value: tensor([[-51.1677]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 40, step: 36
	action: tensor([[ 0.6634, -0.4455,  0.0122,  0.2887, -0.3315, -0.3512, -0.1896]],
       dtype=torch.float64)
	q_value: tensor([[-33.3404]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3899659601266162, distance: 0.8937863708614595 entropy 0.5003275275230408
epoch: 40, step: 37
	action: tensor([[ 6.1800, -6.0233,  5.2108, -5.5632, -3.0009,  6.1800,  5.8744]],
       dtype=torch.float64)
	q_value: tensor([[-50.7930]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 40, step: 38
	action: tensor([[ 0.0378, -0.5623,  0.2342, -0.6972, -0.7651, -0.3000,  0.5310]],
       dtype=torch.float64)
	q_value: tensor([[-33.3404]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5937263973246847, distance: 1.4446531081277656 entropy 0.5003275275230408
epoch: 40, step: 39
	action: tensor([[ 6.1800, -5.5728,  6.1800, -6.2800, -2.9135,  6.1800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-51.3836]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 40, step: 40
	action: tensor([[ 0.1840,  0.3618, -0.1596, -0.4669,  0.2479,  0.2937,  1.2885]],
       dtype=torch.float64)
	q_value: tensor([[-33.3404]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 40, step: 41
	action: tensor([[-0.1669, -0.2604, -0.2167, -0.1443,  0.0653,  0.4038,  0.2403]],
       dtype=torch.float64)
	q_value: tensor([[-33.3404]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04596636075688498, distance: 1.1703494408811355 entropy 0.5003275275230408
epoch: 40, step: 42
	action: tensor([[ 6.0986, -5.6276,  3.5257, -5.8768, -2.1027,  4.5478,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-39.3620]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 40, step: 43
	action: tensor([[ 1.0472, -0.5746,  0.3587, -0.0811,  0.2085,  0.6066,  0.0900]],
       dtype=torch.float64)
	q_value: tensor([[-33.3404]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2943297201780981, distance: 0.9612970437813341 entropy 0.5003275275230408
epoch: 40, step: 44
	action: tensor([[ 6.1800, -5.8971,  6.0249, -6.2800, -3.6462,  5.7142,  6.1588]],
       dtype=torch.float64)
	q_value: tensor([[-61.7490]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 40, step: 45
	action: tensor([[-0.0032, -0.4434, -0.1292, -0.2965, -0.0503, -0.0991,  0.3322]],
       dtype=torch.float64)
	q_value: tensor([[-33.3404]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.29297951781472054, distance: 1.301225351294503 entropy 0.5003275275230408
epoch: 40, step: 46
	action: tensor([[ 6.1800, -6.2800,  3.3016, -6.2800, -2.1311,  4.4013,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-42.9965]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 40, step: 47
	action: tensor([[-0.5783, -0.0853,  0.2182, -0.4808, -0.1862,  0.4472, -0.3148]],
       dtype=torch.float64)
	q_value: tensor([[-33.3404]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5921065405232564, distance: 1.4439187518122525 entropy 0.5003275275230408
epoch: 40, step: 48
	action: tensor([[ 6.1800, -6.2800,  4.2705, -6.2800, -1.5873,  5.3274,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-41.7510]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.001805006616898 entropy 0.5003275275230408
epoch: 40, step: 49
	action: tensor([[ 0.7895, -0.8048,  0.3919, -0.1514, -0.2882, -0.4011,  0.0869]],
       dtype=torch.float64)
	q_value: tensor([[-33.3404]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.35104819616834626, distance: 1.330123952699378 entropy 0.5003275275230408
epoch: 40, step: 50
	action: tensor([[ 6.1800, -6.2800,  6.1800, -6.2800, -3.6058,  6.1800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-59.1023]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1097778295456735 entropy 0.5003275275230408
epoch: 40, step: 51
	action: tensor([[-0.0958, -0.5301,  0.4924, -0.0997,  0.3948,  0.3807, -0.1064]],
       dtype=torch.float64)
	q_value: tensor([[-33.3404]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16738846405229513, distance: 1.2364153608077761 entropy 0.5003275275230408
epoch: 40, step: 52
	action: tensor([[ 6.1470, -6.2800,  6.1800, -6.2800, -2.7808,  6.1240,  6.1513]],
       dtype=torch.float64)
	q_value: tensor([[-51.7560]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0402916079494549 entropy 0.5003275275230408
epoch: 40, step: 53
	action: tensor([[ 0.5670,  0.2043,  0.3967, -0.8049,  0.3667, -0.6699, -0.0182]],
       dtype=torch.float64)
	q_value: tensor([[-33.3404]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2930337785091982, distance: 0.9621793350539414 entropy 0.5003275275230408
epoch: 40, step: 54
	action: tensor([[ 6.1800, -6.1829,  3.7278, -5.9670, -1.8586,  4.8678,  5.9916]],
       dtype=torch.float64)
	q_value: tensor([[-47.3225]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 40, step: 55
	action: tensor([[ 0.0293,  0.1120, -0.0247, -0.2285,  0.3494,  0.2980, -0.0955]],
       dtype=torch.float64)
	q_value: tensor([[-33.3404]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3876314844592821, distance: 0.8954949068076495 entropy 0.5003275275230408
epoch: 40, step: 56
	action: tensor([[ 5.6024, -5.7289,  2.8042, -5.3144, -1.4242,  4.0460,  5.3270]],
       dtype=torch.float64)
	q_value: tensor([[-39.5571]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 40, step: 57
	action: tensor([[ 0.3201,  0.6476, -0.3760, -0.5217, -0.0934,  0.5632,  0.7874]],
       dtype=torch.float64)
	q_value: tensor([[-33.3404]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 40, step: 58
	action: tensor([[-0.1689, -0.1008,  0.4558, -0.1046,  0.0684, -0.1868,  0.0336]],
       dtype=torch.float64)
	q_value: tensor([[-33.3404]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10232027814762246, distance: 1.201463527627984 entropy 0.5003275275230408
epoch: 40, step: 59
	action: tensor([[ 6.1800, -6.1353,  4.2581, -6.1239, -2.5250,  6.1800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-42.7415]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 40, step: 60
	action: tensor([[-0.2867, -0.1298,  0.0826, -0.0491,  0.4885, -0.3132,  0.9496]],
       dtype=torch.float64)
	q_value: tensor([[-33.3404]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2632142229634049, distance: 1.2861605862695187 entropy 0.5003275275230408
epoch: 40, step: 61
	action: tensor([[ 6.1800, -6.2309,  3.0556, -6.1524, -1.7391,  4.8938,  5.8775]],
       dtype=torch.float64)
	q_value: tensor([[-44.3240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 40, step: 62
	action: tensor([[-0.0268,  0.1529, -0.2046, -0.4414, -0.1178, -0.2091,  0.4575]],
       dtype=torch.float64)
	q_value: tensor([[-33.3404]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2284645305618067, distance: 1.0051587127963149 entropy 0.5003275275230408
epoch: 40, step: 63
	action: tensor([[ 4.4130, -6.0133,  2.4796, -6.2800, -1.4299,  3.8484,  5.5626]],
       dtype=torch.float64)
	q_value: tensor([[-35.9149]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 40, step: 64
	action: tensor([[-6.5055e-01, -7.6628e-01, -1.6059e-04, -7.1095e-01,  1.6377e-01,
         -2.0275e-01,  2.3781e-01]], dtype=torch.float64)
	q_value: tensor([[-33.3404]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9774349996008602, distance: 1.6091917657306658 entropy 0.5003275275230408
epoch: 40, step: 65
	action: tensor([[ 5.5816, -6.0144,  3.2298, -6.2800, -0.9363,  4.2139,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-48.9639]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 40, step: 66
	action: tensor([[ 0.2648, -0.2218,  0.2866,  0.1289, -0.0276, -0.2932,  0.4165]],
       dtype=torch.float64)
	q_value: tensor([[-33.3404]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3376680142147177, distance: 0.9313106249358835 entropy 0.5003275275230408
epoch: 40, step: 67
	action: tensor([[ 5.9217, -6.2800,  4.7623, -5.6655, -2.5471,  6.1632,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-46.2273]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.453164698153599 entropy 0.5003275275230408
epoch: 40, step: 68
	action: tensor([[ 0.7284, -0.0054, -0.1558, -0.5737,  0.0310,  0.1196,  0.4679]],
       dtype=torch.float64)
	q_value: tensor([[-33.3404]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4704288951403327, distance: 0.8327580384478294 entropy 0.5003275275230408
epoch: 40, step: 69
	action: tensor([[ 6.1800, -6.2800,  5.1000, -6.2229, -2.4868,  5.9771,  5.7110]],
       dtype=torch.float64)
	q_value: tensor([[-45.0679]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1144736958978758 entropy 0.5003275275230408
epoch: 40, step: 70
	action: tensor([[ 0.2825, -0.7032, -0.1833, -0.7234,  0.0052,  0.0668, -0.1594]],
       dtype=torch.float64)
	q_value: tensor([[-33.3404]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4671526511399893, distance: 1.386099301394362 entropy 0.5003275275230408
epoch: 40, step: 71
	action: tensor([[ 5.9793, -6.2494,  3.7220, -6.2800, -2.0875,  6.0303,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-49.8097]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 40, step: 72
	action: tensor([[-0.2600, -0.0442, -0.1236, -0.4001, -0.0895, -0.0218,  0.2330]],
       dtype=torch.float64)
	q_value: tensor([[-33.3404]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15399862172088175, distance: 1.2293041233192137 entropy 0.5003275275230408
epoch: 40, step: 73
	action: tensor([[ 3.7344, -5.3408,  1.9995, -6.2800, -1.8858,  3.2997,  5.3171]],
       dtype=torch.float64)
	q_value: tensor([[-36.8444]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 40, step: 74
	action: tensor([[ 0.4620, -0.6514,  0.1211,  0.0266,  0.2427,  0.7927,  1.0270]],
       dtype=torch.float64)
	q_value: tensor([[-33.3404]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.40336040100930004, distance: 0.8839195314018645 entropy 0.5003275275230408
epoch: 40, step: 75
	action: tensor([[ 6.1800, -6.2800,  6.1800, -5.8866, -4.3098,  6.1800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-60.2298]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 40, step: 76
	action: tensor([[ 0.2648,  0.6258,  0.0252, -0.3834, -0.4056,  1.0249, -0.1071]],
       dtype=torch.float64)
	q_value: tensor([[-33.3404]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 40, step: 77
	action: tensor([[ 0.5683, -0.3405,  0.5540, -0.4798,  0.3320, -0.5591,  0.7718]],
       dtype=torch.float64)
	q_value: tensor([[-33.3404]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05264403132260198, distance: 1.1740793764119721 entropy 0.5003275275230408
epoch: 40, step: 78
	action: tensor([[ 6.1800, -5.9048,  6.1800, -6.0207, -3.5041,  6.1800,  5.9662]],
       dtype=torch.float64)
	q_value: tensor([[-54.8969]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 40, step: 79
	action: tensor([[ 0.3591,  0.3328,  0.8599, -0.1927, -0.4644,  0.3428,  0.4908]],
       dtype=torch.float64)
	q_value: tensor([[-33.3404]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 40, step: 80
	action: tensor([[-0.3758, -0.0367, -0.0995, -0.9289,  0.2062,  1.2483,  0.4277]],
       dtype=torch.float64)
	q_value: tensor([[-33.3404]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.058425595105234374, distance: 1.1772992301563217 entropy 0.5003275275230408
epoch: 40, step: 81
	action: tensor([[ 5.2283, -6.2800,  6.1800, -6.2800, -2.4288,  6.1800,  5.7319]],
       dtype=torch.float64)
	q_value: tensor([[-53.1026]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.6631354677250034 entropy 0.5003275275230408
epoch: 40, step: 82
	action: tensor([[ 0.5306, -0.5200,  0.0199, -0.5614,  0.1087, -0.7475,  0.5856]],
       dtype=torch.float64)
	q_value: tensor([[-33.3404]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.31169478171546317, distance: 1.310608827157237 entropy 0.5003275275230408
epoch: 40, step: 83
	action: tensor([[ 6.1800, -5.8846,  4.0947, -6.2800, -1.8476,  5.6686,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-52.1081]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 40, step: 84
	action: tensor([[ 0.0701, -0.2030,  0.4275, -0.6897, -0.5840,  0.4888,  0.1980]],
       dtype=torch.float64)
	q_value: tensor([[-33.3404]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14062984752915186, distance: 1.2221627960331465 entropy 0.5003275275230408
epoch: 40, step: 85
	action: tensor([[ 6.1689, -5.5063,  6.1800, -6.2800, -3.2467,  6.1800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-49.0203]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 40, step: 86
	action: tensor([[ 0.8820, -0.8072,  0.1810,  0.3747,  0.5278,  0.6993,  0.0423]],
       dtype=torch.float64)
	q_value: tensor([[-33.3404]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4364231315354776, distance: 0.8590793365071588 entropy 0.5003275275230408
epoch: 40, step: 87
	action: tensor([[ 6.0744, -6.2800,  6.1174, -6.2800, -3.7110,  6.1800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-65.2747]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2396808617431183 entropy 0.5003275275230408
epoch: 40, step: 88
	action: tensor([[-0.2108, -0.0750,  0.5581,  0.0541, -0.1369,  0.4065, -0.4292]],
       dtype=torch.float64)
	q_value: tensor([[-33.3404]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17948271118087666, distance: 1.0365745968182738 entropy 0.5003275275230408
epoch: 40, step: 89
	action: tensor([[ 6.1800, -6.0430,  6.1800, -6.2800, -2.9169,  6.0539,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-46.3532]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 40, step: 90
	action: tensor([[0.6805, 0.3894, 0.5161, 0.0445, 0.1773, 0.0838, 0.1625]],
       dtype=torch.float64)
	q_value: tensor([[-33.3404]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 40, step: 91
	action: tensor([[-0.7827,  0.0327,  0.2908, -0.2843,  0.0876, -0.0376,  0.4993]],
       dtype=torch.float64)
	q_value: tensor([[-33.3404]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8293592057360928, distance: 1.5477691581181034 entropy 0.5003275275230408
epoch: 40, step: 92
	action: tensor([[ 5.9195, -6.2800,  3.9474, -6.2800, -1.1994,  4.2060,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-39.0998]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 40, step: 93
	action: tensor([[ 0.3199, -0.5857,  0.4424,  0.7490, -0.4233,  0.4840,  0.0151]],
       dtype=torch.float64)
	q_value: tensor([[-33.3404]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7085865306428696, distance: 0.6177482217747744 entropy 0.5003275275230408
epoch: 40, step: 94
	action: tensor([[ 6.0898, -6.0509,  5.8681, -5.3543, -4.3186,  6.1800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-57.8317]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 40, step: 95
	action: tensor([[ 0.8235, -0.0530,  0.5096, -0.6926,  0.2975,  0.5191, -0.1262]],
       dtype=torch.float64)
	q_value: tensor([[-33.3404]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4891197629471591, distance: 0.8179302017247131 entropy 0.5003275275230408
epoch: 40, step: 96
	action: tensor([[ 6.1782, -6.2800,  6.1638, -6.2800, -3.7624,  5.8184,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-55.0813]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0505505424759751 entropy 0.5003275275230408
epoch: 40, step: 97
	action: tensor([[ 0.3057, -0.2343,  0.6002, -0.8647,  0.6846,  0.1357,  0.3623]],
       dtype=torch.float64)
	q_value: tensor([[-33.3404]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18410297875970238, distance: 1.2452353186376386 entropy 0.5003275275230408
epoch: 40, step: 98
	action: tensor([[ 6.1800, -6.1217,  6.1800, -5.6508, -2.9860,  6.1800,  5.5068]],
       dtype=torch.float64)
	q_value: tensor([[-54.1693]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 40, step: 99
	action: tensor([[ 0.1734, -1.0734, -1.0054, -0.8676,  0.1189,  0.1743, -0.1726]],
       dtype=torch.float64)
	q_value: tensor([[-33.3404]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.42296615932897863, distance: 1.365067035684708 entropy 0.5003275275230408
epoch: 40, step: 100
	action: tensor([[ 5.2831, -5.9537,  3.2879, -6.1369, -1.5627,  4.6093,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-56.9663]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 40, step: 101
	action: tensor([[-0.0856, -0.5390, -0.1733, -0.1650,  0.3179,  0.1612,  0.4180]],
       dtype=torch.float64)
	q_value: tensor([[-33.3404]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3303293625422721, distance: 1.3198855724607896 entropy 0.5003275275230408
epoch: 40, step: 102
	action: tensor([[ 6.1800, -6.1267,  3.7458, -6.1687, -2.1903,  5.1139,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-44.9447]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 40, step: 103
	action: tensor([[ 0.0629,  0.1950, -0.0315, -0.4766, -0.0958,  0.8070,  0.4079]],
       dtype=torch.float64)
	q_value: tensor([[-33.3404]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4566278573182515, distance: 0.8435394108760476 entropy 0.5003275275230408
epoch: 40, step: 104
	action: tensor([[ 6.1800, -6.2800,  5.2468, -5.4778, -2.5997,  6.1800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-44.1618]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.135096806570047 entropy 0.5003275275230408
epoch: 40, step: 105
	action: tensor([[-0.2694, -0.7617,  0.1525, -0.1165,  0.1163, -0.5779,  0.4679]],
       dtype=torch.float64)
	q_value: tensor([[-33.3404]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7666170471919405, distance: 1.520995403912391 entropy 0.5003275275230408
epoch: 40, step: 106
	action: tensor([[ 6.1800, -6.2800,  4.0701, -6.2800, -2.1396,  5.3993,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-50.4611]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.127085810207538 entropy 0.5003275275230408
epoch: 40, step: 107
	action: tensor([[ 0.2127, -0.3486, -0.3568,  0.2718, -0.7298, -0.0651,  0.8544]],
       dtype=torch.float64)
	q_value: tensor([[-33.3404]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3005820362270476, distance: 0.95702897024525 entropy 0.5003275275230408
epoch: 40, step: 108
	action: tensor([[ 6.0602, -6.2438,  4.5680, -6.0674, -2.2429,  5.6947,  5.2855]],
       dtype=torch.float64)
	q_value: tensor([[-47.5404]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2615696149904472 entropy 0.5003275275230408
epoch: 40, step: 109
	action: tensor([[ 0.4902, -0.7338,  0.6414, -0.8063, -0.4423,  0.0573,  0.6005]],
       dtype=torch.float64)
	q_value: tensor([[-33.3404]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5555738226824254, distance: 1.4272564238352667 entropy 0.5003275275230408
epoch: 40, step: 110
	action: tensor([[ 6.1800, -5.8168,  6.1800, -6.2800, -3.4710,  5.6977,  5.8730]],
       dtype=torch.float64)
	q_value: tensor([[-59.3914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 40, step: 111
	action: tensor([[ 0.2777, -0.3474, -0.3372, -0.0996,  0.3462, -0.4736,  0.1109]],
       dtype=torch.float64)
	q_value: tensor([[-33.3404]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07553476055586494, distance: 1.1002768774849594 entropy 0.5003275275230408
epoch: 40, step: 112
	action: tensor([[ 5.1704, -5.2401,  1.7689, -5.8648, -1.6286,  3.4995,  4.3782]],
       dtype=torch.float64)
	q_value: tensor([[-43.8123]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 40, step: 113
	action: tensor([[ 0.0688, -0.2161,  0.1206, -0.5113, -0.0180,  0.1695,  0.3936]],
       dtype=torch.float64)
	q_value: tensor([[-33.3404]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06932134807777035, distance: 1.1833434517731398 entropy 0.5003275275230408
epoch: 40, step: 114
	action: tensor([[ 6.1800, -6.2800,  4.6297, -6.0012, -1.7046,  5.6942,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-43.2068]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0822325862426416 entropy 0.5003275275230408
epoch: 40, step: 115
	action: tensor([[ 0.0525, -0.2338, -0.1141, -0.0448, -1.0384, -0.1826, -0.0477]],
       dtype=torch.float64)
	q_value: tensor([[-33.3404]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1843891430198069, distance: 1.0334707570327495 entropy 0.5003275275230408
epoch: 40, step: 116
	action: tensor([[ 6.0901, -6.2258,  4.9911, -6.2800, -2.4067,  6.1800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-44.3065]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2221203909279064 entropy 0.5003275275230408
epoch: 40, step: 117
	action: tensor([[ 0.0891, -0.0631,  0.7253, -0.6409, -0.5669,  0.0872,  0.4648]],
       dtype=torch.float64)
	q_value: tensor([[-33.3404]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10029695665563265, distance: 1.2003603711505986 entropy 0.5003275275230408
epoch: 40, step: 118
	action: tensor([[ 6.1800, -6.2800,  6.1800, -6.2486, -3.8297,  6.1800,  6.1169]],
       dtype=torch.float64)
	q_value: tensor([[-49.5667]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1220327183399255 entropy 0.5003275275230408
epoch: 40, step: 119
	action: tensor([[ 0.1832, -1.0760, -0.0040, -1.0023, -0.3074,  0.7583,  0.4205]],
       dtype=torch.float64)
	q_value: tensor([[-33.3404]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.844514180340318, distance: 1.5541670329552102 entropy 0.5003275275230408
epoch: 40, step: 120
	action: tensor([[ 6.1800, -5.8854,  6.0556, -6.1585, -3.4626,  6.1800,  6.1657]],
       dtype=torch.float64)
	q_value: tensor([[-62.1585]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 40, step: 121
	action: tensor([[ 0.3284, -0.5519, -0.2536, -0.4239,  0.1137, -0.0722,  0.4220]],
       dtype=torch.float64)
	q_value: tensor([[-33.3404]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21607537912267483, distance: 1.2619348703855577 entropy 0.5003275275230408
epoch: 40, step: 122
	action: tensor([[ 6.1800, -6.2800,  4.3465, -5.7241, -0.8482,  5.0596,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-47.0756]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 40, step: 123
	action: tensor([[-0.1096, -0.6437,  0.0872, -0.2816, -0.3028, -0.1533, -0.0984]],
       dtype=torch.float64)
	q_value: tensor([[-33.3404]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5306690413914168, distance: 1.4157851165520832 entropy 0.5003275275230408
epoch: 40, step: 124
	action: tensor([[ 6.1800, -6.1680,  4.8433, -6.2800, -1.9677,  5.6059,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-46.5137]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 40, step: 125
	action: tensor([[ 0.7668, -0.6998,  0.5312, -0.2483,  0.8410,  0.4820,  0.5632]],
       dtype=torch.float64)
	q_value: tensor([[-33.3404]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04565303015017452, distance: 1.1701741322952364 entropy 0.5003275275230408
epoch: 40, step: 126
	action: tensor([[ 6.1800, -6.2800,  6.1800, -6.1229, -5.0254,  6.1800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-66.6550]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1097104807421252 entropy 0.5003275275230408
epoch: 40, step: 127
	action: tensor([[-0.3615,  0.3263,  0.1521, -0.9578, -0.3864, -0.0400,  0.1910]],
       dtype=torch.float64)
	q_value: tensor([[-33.3404]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.269105304084142, distance: 1.289156144321374 entropy 0.5003275275230408
LOSS epoch 40 actor 258.61510955851065 critic 160.84335090223047 
epoch: 41, step: 0
	action: tensor([[ 5.9063, -6.2800,  5.4936, -6.2589, -3.2350,  6.1800,  5.9560]],
       dtype=torch.float64)
	q_value: tensor([[-39.3336]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3662609311408704 entropy 0.5003275275230408
epoch: 41, step: 1
	action: tensor([[ 0.0422, -0.3568,  0.0222, -1.0332, -0.5512, -0.2549,  0.1491]],
       dtype=torch.float64)
	q_value: tensor([[-32.4094]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3924644008368585, distance: 1.350357446785882 entropy 0.5003275275230408
epoch: 41, step: 2
	action: tensor([[ 5.7770, -6.2800,  6.1800, -5.9228, -3.6442,  5.9917,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-47.2727]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2157281885055031 entropy 0.5003275275230408
epoch: 41, step: 3
	action: tensor([[ 0.0594, -1.0415, -0.1520, -0.4661,  0.3906, -0.0105, -0.2943]],
       dtype=torch.float64)
	q_value: tensor([[-32.4094]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7930160099374575, distance: 1.5323175545509857 entropy 0.5003275275230408
epoch: 41, step: 4
	action: tensor([[ 6.1800, -6.0003,  6.1800, -6.2024, -2.6144,  6.1800,  5.6016]],
       dtype=torch.float64)
	q_value: tensor([[-56.5096]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 41, step: 5
	action: tensor([[-0.5554, -0.0685,  0.2722, -0.6134, -0.1961, -0.8513,  0.1853]],
       dtype=torch.float64)
	q_value: tensor([[-32.4094]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6858472464152625, distance: 1.4858186445771528 entropy 0.5003275275230408
epoch: 41, step: 6
	action: tensor([[ 6.1636, -6.2800,  5.3996, -5.3236, -2.9932,  6.1800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-42.8775]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 41, step: 7
	action: tensor([[ 0.0788, -1.1511, -0.3412, -0.9727,  0.1271, -0.2816, -0.0524]],
       dtype=torch.float64)
	q_value: tensor([[-32.4094]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7211074858164164, distance: 1.501276512259957 entropy 0.5003275275230408
epoch: 41, step: 8
	action: tensor([[ 6.1800, -6.2800,  5.9013, -6.1576, -2.9151,  6.1800,  6.1444]],
       dtype=torch.float64)
	q_value: tensor([[-58.7155]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0793891893221847 entropy 0.5003275275230408
epoch: 41, step: 9
	action: tensor([[ 0.5697,  0.0037, -0.3600, -1.0957, -0.9810,  0.7203,  0.1273]],
       dtype=torch.float64)
	q_value: tensor([[-32.4094]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 41, step: 10
	action: tensor([[ 0.6087, -0.6545,  0.1002, -0.7968,  0.5043, -0.3070,  0.2086]],
       dtype=torch.float64)
	q_value: tensor([[-32.4094]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5321545420075309, distance: 1.416471953347477 entropy 0.5003275275230408
epoch: 41, step: 11
	action: tensor([[ 6.1800, -6.2800,  6.0788, -5.9578, -3.8561,  5.7163,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-54.9593]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 41, step: 12
	action: tensor([[ 0.5836, -0.9595,  0.3219, -0.6721, -0.3339,  0.1520,  0.2764]],
       dtype=torch.float64)
	q_value: tensor([[-32.4094]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7142999236149681, distance: 1.4983045423856765 entropy 0.5003275275230408
epoch: 41, step: 13
	action: tensor([[ 6.1800, -5.9610,  6.1800, -5.4449, -5.4557,  5.6929,  6.0778]],
       dtype=torch.float64)
	q_value: tensor([[-59.7169]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 41, step: 14
	action: tensor([[-2.8030e-01, -5.1369e-01,  3.5635e-01, -4.3868e-01, -4.2592e-01,
          2.5471e-04,  4.4477e-01]], dtype=torch.float64)
	q_value: tensor([[-32.4094]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7548007619857939, distance: 1.5159001656944369 entropy 0.5003275275230408
epoch: 41, step: 15
	action: tensor([[ 6.1800, -6.2800,  6.1800, -6.2800, -4.7308,  6.0036,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-47.8529]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2074813568919338 entropy 0.5003275275230408
epoch: 41, step: 16
	action: tensor([[ 0.0174, -0.6673,  0.1874, -0.5269, -0.0738,  0.8254, -0.1236]],
       dtype=torch.float64)
	q_value: tensor([[-32.4094]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3179988837172105, distance: 1.3137544935233307 entropy 0.5003275275230408
epoch: 41, step: 17
	action: tensor([[ 6.1800, -6.2800,  6.1800, -6.2800, -4.8222,  6.1800,  6.1014]],
       dtype=torch.float64)
	q_value: tensor([[-52.5470]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.157321764824414 entropy 0.5003275275230408
epoch: 41, step: 18
	action: tensor([[-0.4057, -0.4468,  0.7040, -0.6455, -1.0013,  0.3575,  0.6222]],
       dtype=torch.float64)
	q_value: tensor([[-32.4094]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9597917577865906, distance: 1.601996845917743 entropy 0.5003275275230408
epoch: 41, step: 19
	action: tensor([[ 6.1800, -6.1798,  6.1800, -6.2800, -5.8455,  6.1800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-56.4558]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.092725556167614 entropy 0.5003275275230408
epoch: 41, step: 20
	action: tensor([[-0.0370, -0.4061,  0.3467, -0.5420,  0.0437,  0.2534,  0.2586]],
       dtype=torch.float64)
	q_value: tensor([[-32.4094]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3686182829972895, distance: 1.3387450003332593 entropy 0.5003275275230408
epoch: 41, step: 21
	action: tensor([[ 6.1800, -6.2800,  5.8691, -5.9883, -3.9640,  5.7452,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-46.9105]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1115439555977291 entropy 0.5003275275230408
epoch: 41, step: 22
	action: tensor([[ 0.3640,  0.6481, -0.2073, -1.0602, -0.3847,  0.6705,  0.3228]],
       dtype=torch.float64)
	q_value: tensor([[-32.4094]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 41, step: 23
	action: tensor([[ 0.0845,  0.0859,  0.5068, -0.3887, -0.1687,  0.0140, -0.0547]],
       dtype=torch.float64)
	q_value: tensor([[-32.4094]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19235674704844785, distance: 1.0284104427528644 entropy 0.5003275275230408
epoch: 41, step: 24
	action: tensor([[ 5.7216, -6.0841,  6.1800, -6.2800, -4.0557,  6.1800,  5.9059]],
       dtype=torch.float64)
	q_value: tensor([[-42.8608]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3069033471976461 entropy 0.5003275275230408
epoch: 41, step: 25
	action: tensor([[ 5.9176e-04, -6.3326e-01,  2.8860e-01, -8.0487e-01,  4.7380e-01,
          5.5063e-01, -4.8316e-01]], dtype=torch.float64)
	q_value: tensor([[-32.4094]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5714739500039727, distance: 1.4345321516365768 entropy 0.5003275275230408
epoch: 41, step: 26
	action: tensor([[ 5.9821, -6.0403,  5.4947, -5.9030, -3.7135,  6.1800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-55.3525]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2428528055371404 entropy 0.5003275275230408
epoch: 41, step: 27
	action: tensor([[ 0.7733, -0.1431, -0.1889, -0.3940, -0.0198,  0.9490, -0.0147]],
       dtype=torch.float64)
	q_value: tensor([[-32.4094]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6820130517711189, distance: 0.6452995178862551 entropy 0.5003275275230408
epoch: 41, step: 28
	action: tensor([[ 6.1800, -6.2484,  6.1800, -6.1194, -4.6402,  5.8392,  5.9519]],
       dtype=torch.float64)
	q_value: tensor([[-52.5990]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1481035015007635 entropy 0.5003275275230408
epoch: 41, step: 29
	action: tensor([[ 0.4565,  0.3176,  0.0175, -0.3361, -0.1328, -0.6806,  0.0033]],
       dtype=torch.float64)
	q_value: tensor([[-32.4094]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 41, step: 30
	action: tensor([[-0.1800, -0.1675, -0.3543,  0.2678,  0.0603, -0.0908,  0.4323]],
       dtype=torch.float64)
	q_value: tensor([[-32.4094]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.035853743003689265, distance: 1.1236424889155785 entropy 0.5003275275230408
epoch: 41, step: 31
	action: tensor([[ 6.1800, -5.6824,  3.9307, -5.7607, -2.2524,  4.8917,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-38.3894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 41, step: 32
	action: tensor([[ 0.4133, -0.0243,  0.1274, -0.8558, -0.2502, -0.2945,  0.0446]],
       dtype=torch.float64)
	q_value: tensor([[-32.4094]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.061451365411652636, distance: 1.1086260631318023 entropy 0.5003275275230408
epoch: 41, step: 33
	action: tensor([[ 6.1800, -6.1976,  5.8417, -5.5684, -3.1490,  6.1800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-43.3237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 41, step: 34
	action: tensor([[ 0.5086,  0.0226,  0.1679, -0.9733, -0.0578, -0.4881,  0.3829]],
       dtype=torch.float64)
	q_value: tensor([[-32.4094]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06539925004916569, distance: 1.106291959237937 entropy 0.5003275275230408
epoch: 41, step: 35
	action: tensor([[ 5.9872, -5.9867,  6.0366, -6.2800, -2.9957,  5.4754,  6.0224]],
       dtype=torch.float64)
	q_value: tensor([[-45.9682]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 41, step: 36
	action: tensor([[-0.0579, -0.3588, -0.6998, -0.6575, -0.1434,  0.1606,  0.2873]],
       dtype=torch.float64)
	q_value: tensor([[-32.4094]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08056451653837482, distance: 1.1895482004619822 entropy 0.5003275275230408
epoch: 41, step: 37
	action: tensor([[ 5.4801, -6.2800,  3.4188, -5.8158, -2.2656,  4.7091,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-41.5564]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 41, step: 38
	action: tensor([[ 0.5073, -0.0290,  0.5669,  0.0950, -0.4581,  0.0961, -0.0648]],
       dtype=torch.float64)
	q_value: tensor([[-32.4094]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.723449483177166, distance: 0.601788564069756 entropy 0.5003275275230408
epoch: 41, step: 39
	action: tensor([[ 6.1800, -5.9268,  6.1800, -6.2800, -5.2015,  6.1800,  6.0484]],
       dtype=torch.float64)
	q_value: tensor([[-48.9378]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 41, step: 40
	action: tensor([[-0.1412, -0.2591,  0.7745, -0.6475, -0.1001,  0.2893,  0.2901]],
       dtype=torch.float64)
	q_value: tensor([[-32.4094]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.44419821519705316, distance: 1.3752133993651867 entropy 0.5003275275230408
epoch: 41, step: 41
	action: tensor([[ 6.1800, -6.2800,  5.7522, -5.7496, -5.9355,  5.9514,  6.1276]],
       dtype=torch.float64)
	q_value: tensor([[-50.0559]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.057259628418989 entropy 0.5003275275230408
epoch: 41, step: 42
	action: tensor([[ 0.4222, -0.4157,  0.3320,  0.5185,  0.0204, -0.4821,  0.4126]],
       dtype=torch.float64)
	q_value: tensor([[-32.4094]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4736396694344832, distance: 0.830229706367968 entropy 0.5003275275230408
epoch: 41, step: 43
	action: tensor([[ 6.1800, -6.2800,  6.1800, -5.6708, -5.7013,  6.0887,  6.1534]],
       dtype=torch.float64)
	q_value: tensor([[-54.7885]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 41, step: 44
	action: tensor([[-0.2753, -0.3903, -0.2153, -0.1845, -0.2379,  0.3909,  0.4948]],
       dtype=torch.float64)
	q_value: tensor([[-32.4094]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3781021272198106, distance: 1.3433754111838718 entropy 0.5003275275230408
epoch: 41, step: 45
	action: tensor([[ 6.1800, -6.1881,  5.5153, -6.2800, -2.6800,  6.1151,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-42.3607]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1077922885628737 entropy 0.5003275275230408
epoch: 41, step: 46
	action: tensor([[-0.2096,  0.0178, -0.3991, -0.5065,  0.2052,  0.4648, -0.5498]],
       dtype=torch.float64)
	q_value: tensor([[-32.4094]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0749104773003374, distance: 1.100648318443006 entropy 0.5003275275230408
epoch: 41, step: 47
	action: tensor([[ 3.4264, -5.2326,  2.6173, -5.5636, -1.7467,  2.6984,  4.6449]],
       dtype=torch.float64)
	q_value: tensor([[-38.9307]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 41, step: 48
	action: tensor([[ 0.0303, -0.4837,  0.0307, -0.0443, -0.0119, -0.1253,  0.3944]],
       dtype=torch.float64)
	q_value: tensor([[-32.4094]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16181287858171833, distance: 1.2334591937825774 entropy 0.5003275275230408
epoch: 41, step: 49
	action: tensor([[ 6.1800, -6.2066,  6.1800, -6.2800, -3.4948,  6.1800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-45.1795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1220585189388963 entropy 0.5003275275230408
epoch: 41, step: 50
	action: tensor([[-0.0689, -0.1798,  0.7369, -0.1887, -0.1851,  0.5279,  0.3455]],
       dtype=torch.float64)
	q_value: tensor([[-32.4094]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14207672541022687, distance: 1.059939112561714 entropy 0.5003275275230408
epoch: 41, step: 51
	action: tensor([[ 5.7640, -6.2800,  6.0383, -6.0686, -6.1459,  6.1800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-50.4120]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3772460646824107 entropy 0.5003275275230408
epoch: 41, step: 52
	action: tensor([[ 0.3840,  0.5333, -0.2970, -0.4237, -0.1391, -0.1409,  0.0288]],
       dtype=torch.float64)
	q_value: tensor([[-32.4094]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 41, step: 53
	action: tensor([[ 0.4987,  0.0736, -0.5392,  0.2418,  0.0799, -0.1463,  0.6311]],
       dtype=torch.float64)
	q_value: tensor([[-32.4094]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 41, step: 54
	action: tensor([[-0.0414, -0.6934, -0.2777, -0.9781, -0.2074,  0.6603,  1.2227]],
       dtype=torch.float64)
	q_value: tensor([[-32.4094]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5362426759947485, distance: 1.4183604278384323 entropy 0.5003275275230408
epoch: 41, step: 55
	action: tensor([[ 6.1800, -6.0588,  6.1800, -6.2800, -6.1450,  6.1800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-58.2677]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9694466332736308 entropy 0.5003275275230408
epoch: 41, step: 56
	action: tensor([[ 0.0099,  0.2572, -0.4588, -0.7617,  0.1866,  0.9151, -0.5746]],
       dtype=torch.float64)
	q_value: tensor([[-32.4094]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5449110890176381, distance: 0.7719778151938994 entropy 0.5003275275230408
epoch: 41, step: 57
	action: tensor([[ 5.4521, -6.0327,  4.3014, -6.2800, -1.7923,  4.5525,  5.9866]],
       dtype=torch.float64)
	q_value: tensor([[-43.3004]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 41, step: 58
	action: tensor([[-0.0535,  0.7342,  0.3973, -0.4159, -0.0287,  0.2515,  0.6429]],
       dtype=torch.float64)
	q_value: tensor([[-32.4094]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 41, step: 59
	action: tensor([[-0.2115, -0.4538,  0.2140, -0.4708,  0.2978,  0.1584, -0.0824]],
       dtype=torch.float64)
	q_value: tensor([[-32.4094]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5230901924966416, distance: 1.4122757568764137 entropy 0.5003275275230408
epoch: 41, step: 60
	action: tensor([[ 5.5798, -6.2145,  5.7759, -6.0095, -4.1119,  5.6869,  6.0890]],
       dtype=torch.float64)
	q_value: tensor([[-46.0225]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.5687742334420207 entropy 0.5003275275230408
epoch: 41, step: 61
	action: tensor([[ 0.0921, -0.9369,  0.5619, -0.5915, -0.4242,  0.0616,  0.2584]],
       dtype=torch.float64)
	q_value: tensor([[-32.4094]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8874095911823234, distance: 1.5721347672886223 entropy 0.5003275275230408
epoch: 41, step: 62
	action: tensor([[ 6.0873, -5.4037,  6.0526, -5.8983, -6.1658,  6.1800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-57.6914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 41, step: 63
	action: tensor([[-0.2330, -0.5743,  0.3065, -0.4772, -0.0486, -0.2168,  0.1141]],
       dtype=torch.float64)
	q_value: tensor([[-32.4094]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7812832384358195, distance: 1.5272958930070593 entropy 0.5003275275230408
epoch: 41, step: 64
	action: tensor([[ 6.1800, -5.6945,  5.8544, -6.1289, -3.9441,  5.6766,  5.9733]],
       dtype=torch.float64)
	q_value: tensor([[-47.4034]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 41, step: 65
	action: tensor([[ 0.1436, -0.1781, -0.0110,  0.9257, -0.4740,  0.1049,  0.0312]],
       dtype=torch.float64)
	q_value: tensor([[-32.4094]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 41, step: 66
	action: tensor([[ 0.1752, -0.0848,  0.2036, -1.2817,  0.5881,  1.0272, -0.0792]],
       dtype=torch.float64)
	q_value: tensor([[-32.4094]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10090851424886649, distance: 1.0850721973632675 entropy 0.5003275275230408
epoch: 41, step: 67
	action: tensor([[ 6.1366, -6.1883,  6.1800, -6.2800, -4.8678,  6.1800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-57.4238]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1326707368051574 entropy 0.5003275275230408
epoch: 41, step: 68
	action: tensor([[ 0.0683, -0.2447, -0.0856, -0.7521, -0.0905, -0.0690, -0.3416]],
       dtype=torch.float64)
	q_value: tensor([[-32.4094]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1754079294244717, distance: 1.2406549177681583 entropy 0.5003275275230408
epoch: 41, step: 69
	action: tensor([[ 5.9975, -6.2800,  4.7745, -5.6619, -2.2878,  4.3271,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-42.4303]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 41, step: 70
	action: tensor([[-0.6748,  0.0909,  0.1413, -0.4662, -0.7110,  1.4319, -0.5236]],
       dtype=torch.float64)
	q_value: tensor([[-32.4094]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4263864638387451, distance: 1.3667066188377448 entropy 0.5003275275230408
epoch: 41, step: 71
	action: tensor([[ 5.7959, -6.1366,  5.9951, -5.4539, -5.8791,  6.1800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-53.0433]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.214242107397369 entropy 0.5003275275230408
epoch: 41, step: 72
	action: tensor([[-0.4308, -0.4692,  0.4787,  0.4930, -0.1781,  0.2177,  0.1342]],
       dtype=torch.float64)
	q_value: tensor([[-32.4094]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1451457718957181, distance: 1.2245797688662865 entropy 0.5003275275230408
epoch: 41, step: 73
	action: tensor([[ 6.1607, -6.2800,  6.1800, -6.1009, -5.5189,  6.1800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-49.4929]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1390510052884346 entropy 0.5003275275230408
epoch: 41, step: 74
	action: tensor([[ 0.0624,  0.3085, -0.1733, -0.5609, -0.7108,  0.5114,  0.6164]],
       dtype=torch.float64)
	q_value: tensor([[-32.4094]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3950220736934218, distance: 0.8900747023872433 entropy 0.5003275275230408
epoch: 41, step: 75
	action: tensor([[ 6.1800, -6.2800,  6.1800, -6.0151, -4.8779,  6.1800,  6.0228]],
       dtype=torch.float64)
	q_value: tensor([[-42.1490]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.085125830520018 entropy 0.5003275275230408
epoch: 41, step: 76
	action: tensor([[ 0.2689, -0.5456,  0.4073, -0.6098,  0.5637, -0.0616, -0.0325]],
       dtype=torch.float64)
	q_value: tensor([[-32.4094]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4394230392316081, distance: 1.3729379765134804 entropy 0.5003275275230408
epoch: 41, step: 77
	action: tensor([[ 6.1800, -6.2800,  6.1719, -5.8701, -4.1335,  6.1800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-52.9444]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 41, step: 78
	action: tensor([[ 0.7286,  0.1300,  0.1426, -0.1902,  0.0306,  0.1533,  0.6520]],
       dtype=torch.float64)
	q_value: tensor([[-32.4094]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.788069399321491, distance: 0.5268094148088921 entropy 0.5003275275230408
epoch: 41, step: 79
	action: tensor([[ 5.7479, -6.2652,  6.1614, -6.2800, -4.9977,  6.1800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-47.1008]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3327572421661564 entropy 0.5003275275230408
epoch: 41, step: 80
	action: tensor([[ 0.6099, -0.6850,  0.2546, -0.5024,  0.1754, -0.3107,  0.1933]],
       dtype=torch.float64)
	q_value: tensor([[-32.4094]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.41804262540049697, distance: 1.3627033892939069 entropy 0.5003275275230408
epoch: 41, step: 81
	action: tensor([[ 6.1800, -6.2062,  5.7026, -5.9310, -4.0991,  6.1800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-53.9385]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1252366034594745 entropy 0.5003275275230408
epoch: 41, step: 82
	action: tensor([[-5.5225e-01,  7.0288e-02,  1.2563e-04,  2.6764e-01,  4.3437e-01,
          3.8941e-01,  1.0730e-01]], dtype=torch.float64)
	q_value: tensor([[-32.4094]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.055558995285127866, distance: 1.1757038728856828 entropy 0.5003275275230408
epoch: 41, step: 83
	action: tensor([[ 5.2340, -6.2557,  5.0228, -6.0900, -2.6944,  6.1800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-40.3515]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.6563845710188445 entropy 0.5003275275230408
epoch: 41, step: 84
	action: tensor([[-0.0538, -0.0713,  0.1913,  0.4878, -0.1251,  0.1127, -0.2450]],
       dtype=torch.float64)
	q_value: tensor([[-32.4094]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4701547527319949, distance: 0.8329735569632541 entropy 0.5003275275230408
epoch: 41, step: 85
	action: tensor([[ 6.1307, -6.2563,  5.2212, -6.2800, -4.1692,  6.1800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-43.3416]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2485640205337416 entropy 0.5003275275230408
epoch: 41, step: 86
	action: tensor([[ 0.2912, -0.4428,  0.2988, -0.8360,  0.4300,  0.4777, -0.2549]],
       dtype=torch.float64)
	q_value: tensor([[-32.4094]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2425197564774242, distance: 1.2755818899105933 entropy 0.5003275275230408
epoch: 41, step: 87
	action: tensor([[ 6.1800, -6.0150,  6.1800, -6.2800, -4.2588,  6.1800,  6.0667]],
       dtype=torch.float64)
	q_value: tensor([[-53.0280]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2560773705693273 entropy 0.5003275275230408
epoch: 41, step: 88
	action: tensor([[-2.7953e-04, -5.2924e-01,  7.6247e-02, -1.0968e+00, -7.6023e-02,
          5.0752e-01,  1.7174e-01]], dtype=torch.float64)
	q_value: tensor([[-32.4094]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.578733204009257, distance: 1.4378416671398575 entropy 0.5003275275230408
epoch: 41, step: 89
	action: tensor([[ 6.1800, -6.1563,  6.1800, -6.2800, -4.0112,  5.8277,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-51.7588]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0601372423807391 entropy 0.5003275275230408
epoch: 41, step: 90
	action: tensor([[ 0.6733,  0.2551, -0.3334, -0.2532,  0.3467,  0.6016, -0.1432]],
       dtype=torch.float64)
	q_value: tensor([[-32.4094]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 41, step: 91
	action: tensor([[ 0.0314, -0.8068, -0.2305, -0.0103, -0.3383, -0.1262, -0.0511]],
       dtype=torch.float64)
	q_value: tensor([[-32.4094]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4019955551230958, distance: 1.354971035557329 entropy 0.5003275275230408
epoch: 41, step: 92
	action: tensor([[ 6.1800, -6.2800,  6.1613, -6.2800, -3.5934,  6.1800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-48.8082]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.113350887078364 entropy 0.5003275275230408
epoch: 41, step: 93
	action: tensor([[ 0.4194, -0.3219,  0.4791, -0.3600, -0.6865,  0.0888,  0.2844]],
       dtype=torch.float64)
	q_value: tensor([[-32.4094]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10287594642448827, distance: 1.0838843454389582 entropy 0.5003275275230408
epoch: 41, step: 94
	action: tensor([[ 6.1800, -6.2800,  6.0601, -6.1459, -5.6692,  5.6500,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-51.1234]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1178741578149212 entropy 0.5003275275230408
epoch: 41, step: 95
	action: tensor([[ 0.1678, -0.4608, -0.3221, -0.4040, -0.2672,  0.0874,  0.1083]],
       dtype=torch.float64)
	q_value: tensor([[-32.4094]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09886951018924384, distance: 1.1995814876648814 entropy 0.5003275275230408
epoch: 41, step: 96
	action: tensor([[ 6.1514, -5.7949,  5.0942, -6.1044, -2.5931,  5.9529,  6.1210]],
       dtype=torch.float64)
	q_value: tensor([[-43.3508]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 41, step: 97
	action: tensor([[ 0.6051, -0.3380,  0.3564,  0.3212,  0.1341,  0.4239,  0.5900]],
       dtype=torch.float64)
	q_value: tensor([[-32.4094]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7507045845394564, distance: 0.5713652708313132 entropy 0.5003275275230408
epoch: 41, step: 98
	action: tensor([[ 5.5337, -6.2800,  6.1800, -6.2800, -6.2800,  6.1800,  6.0511]],
       dtype=torch.float64)
	q_value: tensor([[-54.9243]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.5183769254210877 entropy 0.5003275275230408
epoch: 41, step: 99
	action: tensor([[ 0.1943, -0.5483,  0.0130, -0.2294, -0.4565, -0.2366, -0.3275]],
       dtype=torch.float64)
	q_value: tensor([[-32.4094]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18814381116099144, distance: 1.2473582343775083 entropy 0.5003275275230408
epoch: 41, step: 100
	action: tensor([[ 5.5835, -6.2800,  6.1800, -5.7498, -4.0613,  6.0296,  5.9777]],
       dtype=torch.float64)
	q_value: tensor([[-47.2739]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3572707950002139 entropy 0.5003275275230408
epoch: 41, step: 101
	action: tensor([[-0.1744, -0.5063, -0.1594, -0.1813,  0.0207,  0.1654,  0.1514]],
       dtype=torch.float64)
	q_value: tensor([[-32.4094]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.33579342288776814, distance: 1.3225933771580642 entropy 0.5003275275230408
