epoch: 0, step: 0
	action: tensor([[-8.9701, -3.5313, -4.1458, -3.0375, -1.4747,  1.6134,  2.8405]],
       dtype=torch.float64)
	q_value: tensor([[-6.0163]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.0061412188246672
epoch: 0, step: 1
	action: tensor([[-5.6076, -3.3670,  5.4453,  2.5533, -1.7420, -3.5748,  7.6606]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 2
	action: tensor([[-12.1645,   1.6542,  -1.3191,  -4.9249,  -5.4037,   8.9434,   7.8700]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 3
	action: tensor([[-8.8165, -0.1239, -0.8180, -0.1637, -4.2193, 10.5345, -8.4506]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 4
	action: tensor([[-7.7422, -7.6291, -0.9871, -5.5683, -4.3503,  3.5997, -1.8776]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 5
	action: tensor([[-12.4223,  -5.8543,   2.9414,   5.1784,  -7.1633,   5.8276,   3.1645]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 6
	action: tensor([[-12.5241,  -9.8662,  -0.0629,   2.4569,  -7.4454,  11.7301,   3.0481]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 7
	action: tensor([[-9.6755, -6.0372, -2.1180,  1.5115, -8.3953,  3.7473, -0.4411]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 8
	action: tensor([[-11.1540,  -0.1515,  -2.0358,   1.6048,  -2.0991,  -0.7032,  -1.8444]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.022031730135756655, distance: 1.1568815177045788 entropy 2.998170092159314
epoch: 0, step: 9
	action: tensor([[-18.3082, -11.7225, -11.6933,   7.7016, -14.4641,  12.5769,   4.3932]],
       dtype=torch.float64)
	q_value: tensor([[-6.0800]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.4177956575782074
epoch: 0, step: 10
	action: tensor([[-10.7798,  -5.9381,   6.6093,  -0.1411,  -4.9300,  15.7649,   0.0172]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 11
	action: tensor([[-4.7326,  4.9504,  2.6276,  4.9552, -8.1756,  4.0225, 10.1014]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 12
	action: tensor([[-12.1625,  -4.2384,   0.2720,   6.1009,  -3.3807,  -2.8498,   1.4104]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 13
	action: tensor([[-2.3497, -0.6727, -3.6925,  2.3440, -2.1729,  0.6938, -0.3143]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 14
	action: tensor([[-12.7583,   4.1905,  -6.0032,   0.8070,  -8.4632,   4.5078,   3.6915]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 15
	action: tensor([[-8.7321, -5.5915, -7.2431, -1.1566, -2.8392,  4.4752, 12.0387]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 16
	action: tensor([[-8.1969, -7.8695, -4.7438, -2.1822, -1.5636, -5.9445, -2.2511]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3654083339934584, distance: 1.3371741365248417 entropy 2.998170092159314
epoch: 0, step: 17
	action: tensor([[-10.4428, -13.4541,  -1.4306,   1.3833,   5.4092,   2.7695,  10.3383]],
       dtype=torch.float64)
	q_value: tensor([[-7.5438]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.396600916891987
epoch: 0, step: 18
	action: tensor([[-9.6991, -2.7753,  1.8383,  4.3779, -2.7431, -5.2593, 12.0497]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 19
	action: tensor([[-11.4628,  -3.1111,  -6.8346,   1.8267,   0.9335,   2.8159,   3.1378]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 20
	action: tensor([[-3.0929, -7.3426, -6.0372,  3.3278,  1.3751, -8.4563,  6.2174]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 21
	action: tensor([[-10.5806,  -5.3954,   5.7612,   1.7017, -10.5888,   0.1015,   4.2911]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 22
	action: tensor([[-11.9152,   3.0408,   1.2033,   0.9553,  -9.4761,   0.8585,   3.2011]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 23
	action: tensor([[-7.6932, -9.4060, -7.6783,  0.1956, -3.8934,  8.2958,  7.9929]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 24
	action: tensor([[ 0.1443, -8.6812,  1.4045, -1.2424, -3.8488,  1.5849,  3.0546]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 25
	action: tensor([[-12.1701,  -4.1661,   3.7950,   3.8547,  -0.2469,   9.4704,   3.8220]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 26
	action: tensor([[-7.2782, -4.8174, -5.1570, -5.3472, -0.8094, -2.0764,  9.0303]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 27
	action: tensor([[-12.7313,   1.6410,  -1.3462,   5.7017,  -5.6084,  -2.6490,   2.5372]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 28
	action: tensor([[-10.3119, -10.9106,   4.5009,   0.8951,  -3.7533,   2.0839,  14.4341]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 29
	action: tensor([[-15.7938,  -4.7154,  -6.6685,   3.2118,  -2.7165,   5.0185,  13.4730]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 30
	action: tensor([[-10.2024,  -5.6991, -11.7907,   3.6849,   4.7375,   0.1251,   6.4208]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 31
	action: tensor([[-7.8553, -6.6057,  4.7413, -4.4914, -3.2501,  1.2448,  3.6369]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 32
	action: tensor([[ -8.8238, -11.7452,   0.4118,   5.4662,   2.8249, -14.0351,   0.1158]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 33
	action: tensor([[ -9.3889, -12.5335,   1.1398,   9.4940,   0.4675,   6.4338,  -0.2331]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 34
	action: tensor([[-9.7644, -4.6455, -0.8487,  9.3202, -1.6234,  3.7760,  6.4742]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 35
	action: tensor([[-19.1817,  -4.3745,   4.3296,  -2.6093,  -3.3737,  -0.0908,   5.9401]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 36
	action: tensor([[-8.8856, -2.9038, -2.5184,  4.0815, -4.4657, -9.5092, -0.0897]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 37
	action: tensor([[  2.3504,  -2.1843,  -1.2068,   4.6433, -11.7520,   5.1269,  -1.1491]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 38
	action: tensor([[-13.4239,  -3.1963,   3.7941,  -1.1302,   5.1706,  -8.1638,   0.3947]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 39
	action: tensor([[-8.9880, -5.7711, -3.8851,  7.4056, -0.5880,  6.8754,  3.3208]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 40
	action: tensor([[ -6.8893,  -8.0849,  -2.5537,  -3.4068,  -3.9186,   0.7155, -13.2818]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 41
	action: tensor([[-6.7714, -1.7504,  3.5658,  7.7806, -5.1788, -2.0861, 13.6866]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 42
	action: tensor([[-10.1177,  -7.0533,  -1.8410,  -8.9826,  -4.9319,  -5.0360,  -3.1953]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 43
	action: tensor([[-13.1472,  -5.1747,  -3.7053,   3.9882, -10.7028,   0.6842,   7.8377]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 44
	action: tensor([[ -9.2350, -12.0382,  -2.1469,  -0.5360, -10.3835,   5.7414,   4.3759]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 45
	action: tensor([[-16.8437,  -9.0212,   2.4937,  11.1979,  -0.4364,   3.3461,  -2.1999]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 46
	action: tensor([[-15.8584,  -3.1030,   6.2980,  14.7792,  -4.1431,  14.4589,   2.2683]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 47
	action: tensor([[-4.0683, -6.5410, -0.3456,  1.4924,  1.8739, -0.3050, -0.0873]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 48
	action: tensor([[-10.6548, -12.5205,   3.2863,  -0.3359,  -1.5960,  -0.4565,  17.7396]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 49
	action: tensor([[ -9.6458, -13.0040,   3.8332,  -4.0054,  -2.2754,  -4.5889,  -3.1534]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 50
	action: tensor([[-1.3352, -5.4533, -5.5747, -4.5913, -0.1858,  3.3032,  0.3034]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 51
	action: tensor([[-12.3497, -13.2754,   0.4059,  -3.7068,  -1.8185,   1.7439,   3.1148]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 52
	action: tensor([[-13.2065,  -2.4744,  -2.2685,  -4.4531,  -6.1345,   5.6039,   3.5042]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 53
	action: tensor([[ -7.1734, -12.0579,  -3.6028,  -4.6306,  -7.4224,  -3.7607,  -1.7339]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 54
	action: tensor([[-14.0567,  -9.5109,  -1.4621,   8.2432,  -3.7452,   4.0057,   1.2097]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 55
	action: tensor([[-20.4406,  -4.4896,   0.1991,   5.6514,  -7.9699,  -6.5862,   5.4110]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5323051418643325, distance: 1.4165415661788776 entropy 2.998170092159314
epoch: 0, step: 56
	action: tensor([[-15.2448,  -5.5355,  -3.1152,   2.7662,  -4.7838,  -1.3166,  11.2670]],
       dtype=torch.float64)
	q_value: tensor([[-6.4756]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.0686075130843617
epoch: 0, step: 57
	action: tensor([[-5.8770, -4.1113, -0.9662,  5.4859, -5.6561, -6.8979,  3.7618]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 58
	action: tensor([[-5.8565e+00, -9.0480e+00, -6.8575e-04,  2.4517e+00, -1.7136e+00,
         -4.3724e-01,  3.3498e+00]], dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 59
	action: tensor([[-15.8358,  -2.4730,  -3.0921,   1.8089,  -3.0471,   2.6145,   2.5705]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 60
	action: tensor([[ -4.6544,  -6.4927,  -3.4742,   2.2303, -10.8319,   0.4964,   6.8520]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 61
	action: tensor([[-7.6845, -8.5062, -1.0655,  2.0653, -6.5524, -4.9889,  1.6238]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 62
	action: tensor([[-6.8910,  1.7535,  2.3333,  1.5258, -4.0989,  1.0098, -4.0458]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 63
	action: tensor([[-3.4182, -7.2103, -5.4447,  1.8225,  6.1099, 11.9514, 10.8992]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 64
	action: tensor([[-4.8179, -7.9767, -0.1383, -4.3674, -3.5888, -5.4581,  6.2115]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 65
	action: tensor([[-15.9683,   4.7693,  -3.8503,   2.5631,  -2.4002,   1.1989,   1.3738]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 66
	action: tensor([[-17.3930,  -9.2264,   2.9150,  -7.3114,  -3.1223,   9.6292,  -0.7360]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 67
	action: tensor([[-16.6413, -10.3470,  -3.1639,   3.8266,   1.2189,  -1.6147,   8.2120]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 68
	action: tensor([[-10.8133,   2.4746,   4.8256,   6.2879,  -5.3293,   4.0248,   6.5257]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 69
	action: tensor([[-13.4212, -11.7741,  -1.9568,   0.6209,  -4.2601,  10.5954,   3.4920]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 70
	action: tensor([[-3.3855,  8.4035, -3.7199, -2.0076, -3.4100, -5.3304, -3.0148]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 71
	action: tensor([[-5.3912, -2.9004, -4.3627,  2.0742, -6.9122,  0.3818,  1.4815]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 72
	action: tensor([[-4.2031, -2.1232, -6.6071,  3.2422, -3.1624, 11.1946,  0.0333]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 73
	action: tensor([[-14.2668,   1.3196,  -1.9130,   3.5013,  -3.4692,   7.5248,  -8.8100]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 74
	action: tensor([[-11.6363,  -1.3285,  -2.4398,  -3.6592, -10.0192,  12.5232,  -2.7534]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 75
	action: tensor([[-11.7598,   1.4247,   1.5074,   2.2782,  -7.3285,  -7.0426,  -2.6866]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7794854642807191, distance: 0.5373723350914035 entropy 2.998170092159314
epoch: 0, step: 76
	action: tensor([[-11.7784, -11.1012,   1.4101,  -2.9823,  -0.2885,   3.5131,  -9.2337]],
       dtype=torch.float64)
	q_value: tensor([[-5.7979]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3067917254701027
epoch: 0, step: 77
	action: tensor([[-7.9953, -8.5650, -0.0269,  0.8982,  2.6313, -2.1925,  3.8704]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 78
	action: tensor([[ -5.7863, -12.1891,   9.4758,   6.5104,  -4.5004,  -5.2613,   4.5183]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 79
	action: tensor([[-3.2105, -5.0698, -3.8589,  2.8130, 10.6111,  7.6082, 15.2696]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 80
	action: tensor([[-10.1089,  -9.5047,  -5.5736,   0.1497,   1.9401,   9.4459,   4.3367]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 81
	action: tensor([[-11.8137,  -7.6949,   4.6662,   3.3681,  -1.5978,  -0.3634,   7.6047]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 82
	action: tensor([[-11.8235,   0.8429,   0.1402,   1.1074,  -3.1786,   5.1762,  -4.2179]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 83
	action: tensor([[-14.4226,  -6.3872,   1.3549,   0.8515,  -9.3469,  -3.7550,  -0.9559]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 84
	action: tensor([[-3.6183, -0.8289, -3.9493, -5.1408, -7.6554, -8.4735, -1.2650]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 85
	action: tensor([[-9.8125, -2.9173,  0.8053,  5.5255,  0.1262, -1.1467, -3.0387]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 86
	action: tensor([[ -8.2660, -11.9022,   1.3673,   2.1132,  -5.8095,   6.1225,   4.1960]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 87
	action: tensor([[ -6.7936, -11.8570,   5.3265,   3.8883,  -0.4396,  11.7287,   5.9709]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 88
	action: tensor([[-10.5087,   0.2846,  -3.6190,   4.1093,  -9.6934,   2.7377,   4.0202]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 89
	action: tensor([[ -6.7381, -10.0947,  -3.3979,   6.7029,  -7.4852,   7.0340,   5.6493]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 90
	action: tensor([[-4.2586,  2.5538,  2.4987,  5.8754, -3.1920, -2.3741,  3.7510]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 91
	action: tensor([[-12.3195,  -9.7676,  -1.4413,  12.7149,  -2.6193,  -3.8449,   1.1919]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 92
	action: tensor([[-18.0153,   0.6955,  -5.6718,   2.1618,  -4.3235,  -0.5942,   4.9691]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.46053514613206636, distance: 0.8405010708599167 entropy 2.998170092159314
epoch: 0, step: 93
	action: tensor([[-9.0121, -5.4333, -5.3789,  9.1141, -6.3277, -7.2331,  6.2780]],
       dtype=torch.float64)
	q_value: tensor([[-4.1080]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.9080666938582347
epoch: 0, step: 94
	action: tensor([[-11.1661,  -7.3846,   3.8632,   4.1454,  -4.6096,  -5.9162,  -1.6877]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 95
	action: tensor([[-4.3506, -2.2476, -0.0261,  5.9419, -2.3410, -4.9902,  8.1693]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 96
	action: tensor([[-12.0695,  -0.5537,   0.7911,  -5.8604,  -4.7047,  -5.9882,  -6.3514]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12257767070087877, distance: 1.2124529456109518 entropy 2.998170092159314
epoch: 0, step: 97
	action: tensor([[-7.6205, -4.6422, -0.6155,  1.8955, -0.8092, -2.2706,  2.5293]],
       dtype=torch.float64)
	q_value: tensor([[-2.6618]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5090525373972103, distance: 0.8018150490704791 entropy 2.5387925048970983
epoch: 0, step: 98
	action: tensor([[-20.5656, -27.7377, -13.1238,   3.6582,  -5.3484,   7.9479,   4.6353]],
       dtype=torch.float64)
	q_value: tensor([[-6.6254]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.4363527105295844
epoch: 0, step: 99
	action: tensor([[-16.2995, -11.4499,   5.4804,   0.1736,   6.4962,   0.2900,   1.8036]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 100
	action: tensor([[-12.9164,   0.3015,   0.7568,  -0.3208,  -4.9802,   6.5517,   0.9165]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10051515091819319, distance: 1.2004793839139702 entropy 2.998170092159314
epoch: 0, step: 101
	action: tensor([[-4.4034, -7.1639,  5.9249,  3.2934, -4.1742, -1.0863,  3.3049]],
       dtype=torch.float64)
	q_value: tensor([[-2.6429]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.499418867417685
epoch: 0, step: 102
	action: tensor([[-4.4840, -2.2586,  2.9695, 11.2647,  1.9948,  0.9254,  0.7085]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 103
	action: tensor([[ -7.4257, -14.1815,  -5.0983,  11.2255,  -2.2666,  11.2005,  -1.2089]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 104
	action: tensor([[ -8.8876, -11.9396,   9.3272,   3.5820,  -6.4055,  -6.1771,   9.6230]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 105
	action: tensor([[ -5.5236,   0.5278,  -3.9102, -10.5646,   2.2373,   5.2288, -11.9639]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 106
	action: tensor([[-14.8887,  -9.6997,  -0.6416,   1.8989,   0.7744,  -1.2896,  -4.9035]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 107
	action: tensor([[-5.6391,  1.2473, -3.9478,  3.8702, -1.5650, -2.5343, -4.2277]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 108
	action: tensor([[-8.6751, -4.5340,  0.9518, -7.9703, -6.7920, 16.1447, 14.7686]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 109
	action: tensor([[-11.4309, -11.9413,  -5.6471,   1.0703,  -8.5203,   8.3776,   3.9271]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 110
	action: tensor([[-15.7757,  -4.5303,  -4.7328,  -3.4486,  -6.1260,  -2.1335,   6.8561]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 111
	action: tensor([[-12.8163,  -9.3722,  -7.8644,  -3.5153,  -8.6232,  13.6359,   3.0736]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 112
	action: tensor([[-9.1481, -0.2040, -3.2812,  1.7591, -3.4060,  7.6119,  6.4354]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 113
	action: tensor([[-7.7726, -7.9823,  2.3818, -0.9191,  2.8506,  1.2229,  1.0228]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0275271882078822, distance: 1.6294462404911954 entropy 2.998170092159314
epoch: 0, step: 114
	action: tensor([[ -4.5129, -17.4436,   0.2062, -11.7900,  -9.0633,  -0.7780,   5.9732]],
       dtype=torch.float64)
	q_value: tensor([[-4.9211]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.2598468295464333
epoch: 0, step: 115
	action: tensor([[-8.7168, -7.5981, -6.1689, -1.8027, -2.8241,  6.6356,  5.8021]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 116
	action: tensor([[-13.0476,  -0.2405,  -0.6480,   0.5163,  -4.0593,  -3.0707,   0.9681]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 117
	action: tensor([[-11.3105,  -0.3623,  -2.2853, -10.1250,  -3.3477,  -5.0070,  12.4653]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 118
	action: tensor([[-12.2872, -10.3982,   0.1104,  -1.2906,  -8.4111,   3.5632,   0.6049]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 119
	action: tensor([[-12.0189,  -6.3641,  -2.0016,   1.9246,  -4.5017,  -4.4763,   5.8048]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 120
	action: tensor([[-8.8656, -4.9575,  4.5209, -0.2274, -6.5438, -2.4464, -8.3173]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 121
	action: tensor([[-15.2274,  -1.1166,  -3.7473,   9.9326,  -3.9455,   3.2231,   6.0245]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 122
	action: tensor([[-10.0025,  -1.2153,  -3.3091,   1.5925, -10.3631,   7.0055,   3.2001]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 123
	action: tensor([[-11.8572,  -8.9177,  -1.7940,   6.7937,   0.2103,   3.2710,   4.7241]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 124
	action: tensor([[-5.2529, -2.6664, -3.3391,  0.1036,  2.4053, -2.2166,  1.9713]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 125
	action: tensor([[-6.0400,  1.6705,  4.1397, -3.1586,  2.7378,  0.4663,  3.4425]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 126
	action: tensor([[-1.7294, -9.3552,  4.7399,  0.6359, -3.2500,  2.0803,  4.9127]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
epoch: 0, step: 127
	action: tensor([[-17.0995,  -4.4009,   2.5396,  -4.7639,  -3.2202,  -2.8778,   8.4427]],
       dtype=torch.float64)
	q_value: tensor([[-5.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.998170092159314
LOSS epoch 0 actor 678.2383538958527 critic 1925.5621417942684
epoch: 1, step: 0
	action: tensor([[-11.2024, -10.6139,   2.0569,  -0.1989,  -3.7278,  10.8555,   2.5802]],
       dtype=torch.float64)
	q_value: tensor([[-8.7827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3124125487776177
epoch: 1, step: 1
	action: tensor([[-19.8037, -11.2332,  -1.7230,  -2.8319,   5.4788,   8.7920,  13.6100]],
       dtype=torch.float64)
	q_value: tensor([[-8.7827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3124125487776177
epoch: 1, step: 2
	action: tensor([[-14.1073, -10.3174,  -3.8064,  -2.3486,  -4.9061,  -6.3810,   9.8743]],
       dtype=torch.float64)
	q_value: tensor([[-8.7827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4094627944512308, distance: 0.8793875759398948 entropy 3.3124125487776177
epoch: 1, step: 3
	action: tensor([[-16.1631,  -9.0714,   3.5552,  11.4220,  -5.4593,  -2.7524, -25.9574]],
       dtype=torch.float64)
	q_value: tensor([[-11.3444]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.6601314076901152
epoch: 1, step: 4
	action: tensor([[-7.4313,  1.7077,  0.9949,  6.4308, -8.2877, -1.4533,  3.7511]],
       dtype=torch.float64)
	q_value: tensor([[-8.7827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3124125487776177
epoch: 1, step: 5
	action: tensor([[-19.6800, -12.8475,   9.4423,  13.6673,  -5.9951,  -3.8704,  10.9595]],
       dtype=torch.float64)
	q_value: tensor([[-8.7827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3124125487776177
epoch: 1, step: 6
	action: tensor([[-17.4427,  -8.0386, -10.4767,  15.2620,  -5.2502,  -0.5135,   4.0859]],
       dtype=torch.float64)
	q_value: tensor([[-8.7827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41448609823991767, distance: 0.8756394080863851 entropy 3.3124125487776177
epoch: 1, step: 7
	action: tensor([[-10.1942,  -2.2138, -16.5587,  -1.6718,   5.5764,   4.0221, -10.3372]],
       dtype=torch.float64)
	q_value: tensor([[-7.2890]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.5262943088034215
epoch: 1, step: 8
	action: tensor([[ -6.2813,   0.8049,   4.4575,   5.3842, -14.0343,  -0.9564,   1.6795]],
       dtype=torch.float64)
	q_value: tensor([[-8.7827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3124125487776177
epoch: 1, step: 9
	action: tensor([[ -8.0081,  -4.7277,   2.8677,  17.9725,  -9.6425,  -5.3412, -10.4387]],
       dtype=torch.float64)
	q_value: tensor([[-8.7827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3124125487776177
epoch: 1, step: 10
	action: tensor([[-17.1271,  -5.2015,  -1.8186, -14.8418,  -6.2065,  12.8198,   8.6548]],
       dtype=torch.float64)
	q_value: tensor([[-8.7827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3124125487776177
epoch: 1, step: 11
	action: tensor([[-19.4331, -11.7325,  -7.4841,   1.6013,  -2.8106,  -2.5273,  -0.6107]],
       dtype=torch.float64)
	q_value: tensor([[-8.7827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3124125487776177
epoch: 1, step: 12
	action: tensor([[-15.5549,  -0.2392,  -2.5352,   4.5267,  -5.7505,  12.7786,  -2.0841]],
       dtype=torch.float64)
	q_value: tensor([[-8.7827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3124125487776177
epoch: 1, step: 13
	action: tensor([[-15.0945,   0.6798,  -9.6475,   1.9257, -14.2094,  -8.1974,  -0.5876]],
       dtype=torch.float64)
	q_value: tensor([[-8.7827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.573734355924417, distance: 1.4355634954140817 entropy 3.3124125487776177
epoch: 1, step: 14
	action: tensor([[ 2.1417, -7.2946, -1.7257, -0.6888, -0.0239,  2.7771,  7.5962]],
       dtype=torch.float64)
	q_value: tensor([[-9.9886]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.770919727272113
epoch: 1, step: 15
	action: tensor([[ -3.4336, -11.4200,  -0.2750,  -3.4939,  -5.9890,   8.5623,   0.3224]],
       dtype=torch.float64)
	q_value: tensor([[-8.7827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3124125487776177
epoch: 1, step: 16
	action: tensor([[-17.3454,   5.0328,   6.3432, -16.6456,  -0.5262,  -5.1674,   0.6010]],
       dtype=torch.float64)
	q_value: tensor([[-8.7827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3124125487776177
epoch: 1, step: 17
	action: tensor([[-15.5357,  -8.4236,  -4.3389,   5.9512,  -9.5516, -12.6790,  12.1967]],
       dtype=torch.float64)
	q_value: tensor([[-8.7827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.7938205262069125, distance: 1.9127400222788957 entropy 3.3124125487776177
epoch: 1, step: 18
	action: tensor([[-10.7458, -12.4855,  -9.7272,  -5.9126,  -8.8658, -14.3591,  -5.4031]],
       dtype=torch.float64)
	q_value: tensor([[-7.4235]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.5830795093755663
epoch: 1, step: 19
	action: tensor([[-10.5959,  -0.5617,  -5.7103, -19.5916,   0.7085,  -9.7793,  -2.7906]],
       dtype=torch.float64)
	q_value: tensor([[-8.7827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3124125487776177
epoch: 1, step: 20
	action: tensor([[-6.0848, -0.5368, -6.5110, -0.7221, -0.6519, 17.9791,  0.1922]],
       dtype=torch.float64)
	q_value: tensor([[-8.7827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4006687983717594, distance: 1.3543297558918992 entropy 3.3124125487776177
epoch: 1, step: 21
	action: tensor([[-17.5017,  -4.0360,  -2.5783,  -3.0104,  -1.7900,   3.2396,   8.7015]],
       dtype=torch.float64)
	q_value: tensor([[-3.9438]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.831854275696886
epoch: 1, step: 22
	action: tensor([[-11.8378,  -1.9276,   2.5094,   5.1116,  -7.9105,  -6.8216,  -1.7280]],
       dtype=torch.float64)
	q_value: tensor([[-8.7827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3124125487776177
epoch: 1, step: 23
	action: tensor([[-9.7404, -8.9229,  1.2481,  8.0226, -5.0258, -1.4367,  2.0850]],
       dtype=torch.float64)
	q_value: tensor([[-8.7827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3124125487776177
epoch: 1, step: 24
	action: tensor([[-19.9181,  -2.1826,   3.0791,   6.4789,  -2.9223,  11.3337,  11.1251]],
       dtype=torch.float64)
	q_value: tensor([[-8.7827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3124125487776177
epoch: 1, step: 25
	action: tensor([[-14.8167,  -5.4797, -12.2752,  -2.5869,  -3.6494,  -0.3521,   4.1476]],
       dtype=torch.float64)
	q_value: tensor([[-8.7827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3124125487776177
epoch: 1, step: 26
	action: tensor([[-11.3241,  -8.0577, -12.3621,   0.7113, -11.2921, -16.5727,   6.5350]],
       dtype=torch.float64)
	q_value: tensor([[-8.7827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3124125487776177
epoch: 1, step: 27
	action: tensor([[ -5.5380, -11.5657, -16.9190,  12.5283,  -6.7966,  -6.2756,  10.2419]],
       dtype=torch.float64)
	q_value: tensor([[-8.7827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14109072215489915, distance: 1.2224096802159783 entropy 3.3124125487776177
epoch: 1, step: 28
	action: tensor([[-14.8044,  -9.9817,  -1.7216,   7.6214,   0.5492,  -3.5729,  10.6823]],
       dtype=torch.float64)
	q_value: tensor([[-8.3876]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3410064010271343
epoch: 1, step: 29
	action: tensor([[-12.3573,  -3.4544,  -0.5424,   8.6098,  -8.1957,  -9.9150,  -4.0159]],
       dtype=torch.float64)
	q_value: tensor([[-8.7827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3124125487776177
epoch: 1, step: 30
	action: tensor([[ -9.7487,   5.5987,   0.9642,  11.3845, -16.7802,   0.7461,  -6.0767]],
       dtype=torch.float64)
	q_value: tensor([[-8.7827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3124125487776177
epoch: 1, step: 31
	action: tensor([[-23.6134,  -5.6818,  -2.0774,   9.2320, -10.5870,  -1.9916,   3.8730]],
       dtype=torch.float64)
	q_value: tensor([[-8.7827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20807721763323628, distance: 1.018352448237169 entropy 3.3124125487776177
epoch: 1, step: 32
	action: tensor([[-35.7581,  -1.5031, -13.8230, -12.3180,  -4.2427,  -7.4664,  -1.4030]],
       dtype=torch.float64)
	q_value: tensor([[-9.9888]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.750685670350159
epoch: 1, step: 33
	action: tensor([[-18.1307, -12.8544,  -9.4468,  10.4774,  -1.5613,   1.2318,   7.3240]],
       dtype=torch.float64)
	q_value: tensor([[-8.7827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3507442715253888, distance: 0.9220714778832292 entropy 3.3124125487776177
epoch: 1, step: 34
	action: tensor([[-13.2720,  -8.8522, -11.3731,   3.6267,  -6.4326,  -7.6683,   8.0199]],
       dtype=torch.float64)
	q_value: tensor([[-8.2749]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.5360138910998415
epoch: 1, step: 35
	action: tensor([[-13.4327,  -9.2331,  -4.4857,   7.9763,  -3.9921,  -0.7667,   3.9945]],
       dtype=torch.float64)
	q_value: tensor([[-8.7827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3124125487776177
epoch: 1, step: 36
	action: tensor([[-13.1738,  -3.5935,  -7.5931,  -2.0586,  -2.8326,  -7.0829,   7.5302]],
       dtype=torch.float64)
	q_value: tensor([[-8.7827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3124125487776177
epoch: 1, step: 37
	action: tensor([[-20.7776,  -3.9000,   5.2385,   3.9595,  -4.2716,  19.8275,   4.2390]],
       dtype=torch.float64)
	q_value: tensor([[-8.7827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3124125487776177
epoch: 1, step: 38
	action: tensor([[-17.1519,  -0.5650,  -5.4781,   7.2027,  -6.0142,   5.6114,   4.6689]],
       dtype=torch.float64)
	q_value: tensor([[-8.7827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3124125487776177
epoch: 1, step: 39
	action: tensor([[-18.6362,  -4.4725,  -0.3601,   0.3663,  -0.4769,  -0.0483,  -7.4564]],
       dtype=torch.float64)
	q_value: tensor([[-8.7827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3124125487776177
epoch: 1, step: 40
	action: tensor([[-10.6517,  -1.2221,   0.1066,  -5.1307,   0.2460,  -2.2646,   6.1126]],
       dtype=torch.float64)
	q_value: tensor([[-8.7827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3124125487776177
epoch: 1, step: 41
	action: tensor([[-14.5591,  -6.9206,  -0.3876,  -9.9525,  -4.3095,  -7.2207,  -1.8306]],
       dtype=torch.float64)
	q_value: tensor([[-8.7827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3124125487776177
epoch: 1, step: 42
	action: tensor([[-24.8930,  -6.8375,  -7.4430,  -4.6182,  -3.6676, -19.3670,  -8.5288]],
       dtype=torch.float64)
	q_value: tensor([[-8.7827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3124125487776177
epoch: 1, step: 43
	action: tensor([[-16.9905, -11.8685,   7.8054,  -2.2779, -14.5019,  -4.3914,  -4.2563]],
       dtype=torch.float64)
	q_value: tensor([[-8.7827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3124125487776177
epoch: 1, step: 44
	action: tensor([[-18.0657,  -9.3749,   6.7562,   9.1094,  -4.0831,  -9.3720,  -0.0608]],
       dtype=torch.float64)
	q_value: tensor([[-8.7827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3124125487776177
epoch: 1, step: 45
	action: tensor([[-11.0337, -11.9367,   4.4514,  10.4204,   5.9514,   3.7100,   6.3419]],
       dtype=torch.float64)
	q_value: tensor([[-8.7827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3124125487776177
epoch: 1, step: 46
	action: tensor([[-19.0319, -22.8123,  -5.5713,  -1.2186, -11.9193,   2.3362,  14.3421]],
       dtype=torch.float64)
	q_value: tensor([[-8.7827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2737857766462217, distance: 0.9751896168445513 entropy 3.3124125487776177
epoch: 1, step: 47
	action: tensor([[-11.3972,  -3.4059,  -0.9269,  14.9503,   6.0425,  -6.4157,   8.9430]],
       dtype=torch.float64)
	q_value: tensor([[-7.2004]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3397717844851087
epoch: 1, step: 48
	action: tensor([[ -3.3186, -14.8583,   1.4014,   0.9573,   0.4752,  11.9515,   1.3795]],
       dtype=torch.float64)
	q_value: tensor([[-8.7827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3124125487776177
epoch: 1, step: 49
	action: tensor([[-6.5147, -8.4655, -5.9099,  9.0300, -2.8544,  1.6829,  6.0403]],
       dtype=torch.float64)
	q_value: tensor([[-8.7827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3124125487776177
epoch: 1, step: 50
	action: tensor([[-15.3637, -13.9710,   5.2255,  -1.4850,  -9.4166,  -3.6100,   6.1347]],
       dtype=torch.float64)
	q_value: tensor([[-8.7827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3124125487776177
epoch: 1, step: 51
	action: tensor([[-12.2228, -17.0086,  13.1702,  11.7880, -12.8736,   1.8524,  -7.1450]],
       dtype=torch.float64)
	q_value: tensor([[-8.7827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24677993922360375, distance: 0.9931563698028635 entropy 3.3124125487776177
epoch: 1, step: 52
	action: tensor([[-18.8840, -11.4805,   2.9764,   5.3051,  -2.4121,   6.7004,  -1.2611]],
       dtype=torch.float64)
	q_value: tensor([[-7.2656]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.1501812254649386
epoch: 1, step: 53
	action: tensor([[-16.0646,  -1.8812, -10.4417, -17.1016,  -3.3367,   8.7346,   1.2049]],
       dtype=torch.float64)
	q_value: tensor([[-8.7827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3124125487776177
epoch: 1, step: 54
	action: tensor([[-15.0832,  -2.2166,   1.3148,   3.5056,  -5.6083, -17.6346,  -4.3352]],
       dtype=torch.float64)
	q_value: tensor([[-8.7827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3124125487776177
epoch: 1, step: 55
	action: tensor([[ -4.7611, -16.9747,   3.6789,  -3.4779,  -8.2275,   5.0951,  -0.8051]],
       dtype=torch.float64)
	q_value: tensor([[-8.7827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3124125487776177
epoch: 1, step: 56
	action: tensor([[-14.8967,  -7.2884,   1.5260,  -0.2587,  -0.3491,  13.3418,   4.3753]],
       dtype=torch.float64)
	q_value: tensor([[-8.7827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.5960041885986698, distance: 1.8437812224627408 entropy 3.3124125487776177
epoch: 1, step: 57
	action: tensor([[-15.8398,   1.0584,  -0.2791,   7.9531, -14.6045,  -1.9528,  -1.0152]],
       dtype=torch.float64)
	q_value: tensor([[-7.1182]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.418119320982041
epoch: 1, step: 58
	action: tensor([[-10.5009,  -8.1470,  -3.2686,   4.3070,  -5.1041,   7.8917,   5.7312]],
       dtype=torch.float64)
	q_value: tensor([[-8.7827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3124125487776177
epoch: 1, step: 59
	action: tensor([[-9.7457, -5.3604, -3.2851, -0.6591, -2.4644,  6.9645, 10.9704]],
       dtype=torch.float64)
	q_value: tensor([[-8.7827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3124125487776177
epoch: 1, step: 60
	action: tensor([[-6.6927, -5.3439, -1.3603,  3.9483, -6.3295, 14.8130,  5.7017]],
       dtype=torch.float64)
	q_value: tensor([[-8.7827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3124125487776177
epoch: 1, step: 61
	action: tensor([[-8.6078, -2.0939,  0.5024,  5.3856, -8.0918, -0.8990,  5.6055]],
       dtype=torch.float64)
	q_value: tensor([[-8.7827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3124125487776177
epoch: 1, step: 62
	action: tensor([[-17.9748,  -1.8297,  -8.2670,   3.0679,   1.0073,   1.7324,  12.8438]],
       dtype=torch.float64)
	q_value: tensor([[-8.7827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3124125487776177
epoch: 1, step: 63
	action: tensor([[ -5.0102, -14.3276,  13.1358,  12.4482,  -2.4399,  14.4146,   2.3725]],
       dtype=torch.float64)
	q_value: tensor([[-8.7827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.28855341619252806, distance: 1.2989962777319441 entropy 3.3124125487776177
epoch: 1, step: 64
	action: tensor([[-26.4174,  -2.2112,  -2.4666,  12.2812,   1.5726,   3.2080,  -3.6971]],
       dtype=torch.float64)
	q_value: tensor([[-7.4956]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.609428042361816
epoch: 1, step: 65
	action: tensor([[-15.2852,  -4.9465,   4.5922,  -3.7225,  -1.1020,  10.8494,   4.1954]],
       dtype=torch.float64)
	q_value: tensor([[-8.7827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3124125487776177
epoch: 1, step: 66
	action: tensor([[-31.0181,  -7.7813,   4.7996,   6.6456,  -0.6997,  -4.6272,  11.4292]],
       dtype=torch.float64)
	q_value: tensor([[-8.7827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3124125487776177
epoch: 1, step: 67
	action: tensor([[-20.2199,  -7.1353,   8.4854,  -2.6563,   3.6619,  -7.7135,   3.1394]],
       dtype=torch.float64)
	q_value: tensor([[-8.7827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3124125487776177
epoch: 1, step: 68
	action: tensor([[-15.3930,  -2.5903,  -2.6113,   9.7023,  -4.5967, -15.0888,   7.2185]],
       dtype=torch.float64)
	q_value: tensor([[-8.7827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3124125487776177
epoch: 1, step: 69
	action: tensor([[-15.6187,  -6.4182,  -9.3132,  -8.1358,   1.5299,  -2.9936,   2.3286]],
       dtype=torch.float64)
	q_value: tensor([[-8.7827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3124125487776177
epoch: 1, step: 70
	action: tensor([[-25.4828,  -6.8936,   1.6422,   7.6588,   4.2491,   9.0396,  13.5684]],
       dtype=torch.float64)
	q_value: tensor([[-8.7827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2552582952888458, distance: 0.9875509941607806 entropy 3.3124125487776177
epoch: 1, step: 71
	action: tensor([[-19.0542,  -8.6492,   3.1924,  -2.3077,  -8.4507,   7.6417,  -1.2782]],
       dtype=torch.float64)
	q_value: tensor([[-7.8927]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.590318729556567
epoch: 1, step: 72
	action: tensor([[-20.9784,  -1.1165,   4.5992,   0.1580,  -0.9540, -10.8939,   0.0547]],
       dtype=torch.float64)
	q_value: tensor([[-8.7827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3124125487776177
epoch: 1, step: 73
	action: tensor([[-7.9810, -6.1952, -3.4035,  1.1635, -2.2674,  6.5062,  9.6836]],
       dtype=torch.float64)
	q_value: tensor([[-8.7827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.3688623015485963, distance: 1.7612726920252235 entropy 3.3124125487776177
epoch: 1, step: 74
	action: tensor([[-10.7240, -14.3942,   5.7484,  10.5611, -12.7548,  -2.8265,   3.7561]],
       dtype=torch.float64)
	q_value: tensor([[-10.4215]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.777060773686558
epoch: 1, step: 75
	action: tensor([[-21.2628,  -7.7055,   0.6642,   4.9890,  -6.5510,   5.0933,  -1.6215]],
       dtype=torch.float64)
	q_value: tensor([[-8.7827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3124125487776177
epoch: 1, step: 76
	action: tensor([[-19.5910,  -9.5731,   5.8280,   0.2075,  -2.5802,   7.4315,  -2.1231]],
       dtype=torch.float64)
	q_value: tensor([[-8.7827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3124125487776177
epoch: 1, step: 77
	action: tensor([[-17.1548,   2.4219,   4.8297,   1.6314,  -5.2197,  -1.1029, -14.6845]],
       dtype=torch.float64)
	q_value: tensor([[-8.7827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3124125487776177
epoch: 1, step: 78
	action: tensor([[-24.1021, -10.2260,   1.7875,   9.2093,  -5.0832,  17.9881,   8.0008]],
       dtype=torch.float64)
	q_value: tensor([[-8.7827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2984571244145643, distance: 0.958481649495606 entropy 3.3124125487776177
epoch: 1, step: 79
	action: tensor([[-10.8327, -16.4118,   7.6937,  -4.7378,  -1.7707,  -6.7419,   2.3411]],
       dtype=torch.float64)
	q_value: tensor([[-7.8663]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.4531080413700073
epoch: 1, step: 80
	action: tensor([[-22.1858, -13.4307,  -3.3637,  -0.7706,  -5.4615,   9.6881,  -3.0563]],
       dtype=torch.float64)
	q_value: tensor([[-8.7827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3124125487776177
epoch: 1, step: 81
	action: tensor([[ -4.8945, -10.4374,  -1.8843,  13.6248,  -3.9910,   6.0861,  -3.7052]],
       dtype=torch.float64)
	q_value: tensor([[-8.7827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3124125487776177
epoch: 1, step: 82
	action: tensor([[-2.4510, -6.2103,  5.4324, 17.0727,  5.0045, -2.1466, -4.4588]],
       dtype=torch.float64)
	q_value: tensor([[-8.7827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3124125487776177
epoch: 1, step: 83
	action: tensor([[-15.4836, -14.3333,   0.6480,  -4.2420,  -7.0151, -12.8202,  -8.0882]],
       dtype=torch.float64)
	q_value: tensor([[-8.7827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3124125487776177
epoch: 1, step: 84
	action: tensor([[-18.7727, -12.7324,  -3.5262,  18.0539,   7.0214,   7.2015,   5.0782]],
       dtype=torch.float64)
	q_value: tensor([[-8.7827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1239399684159648, distance: 1.6677379525049725 entropy 3.3124125487776177
epoch: 1, step: 85
	action: tensor([[-14.5153,  -6.7245,   5.5765,   9.2253,   5.6739,  -5.1069,  13.5605]],
       dtype=torch.float64)
	q_value: tensor([[-7.4766]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.4028604125292823
epoch: 1, step: 86
	action: tensor([[-15.3075,  -4.7767,  12.2756,  -4.6633, -11.6371,   4.1583,   6.3686]],
       dtype=torch.float64)
	q_value: tensor([[-8.7827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3124125487776177
epoch: 1, step: 87
	action: tensor([[-12.4440,   2.1771,  -4.9736,  -2.7572, -10.1044, -17.3684,  -1.0160]],
       dtype=torch.float64)
	q_value: tensor([[-8.7827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3124125487776177
epoch: 1, step: 88
	action: tensor([[-11.8941, -13.4023,  -5.1565,  -9.4630,  -5.2282,  -1.9392,  -5.1958]],
       dtype=torch.float64)
	q_value: tensor([[-8.7827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3124125487776177
epoch: 1, step: 89
	action: tensor([[-14.4113,  -6.6061,   1.6104,  13.6997,  -0.6053,  -3.2934,   1.3737]],
       dtype=torch.float64)
	q_value: tensor([[-8.7827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3124125487776177
epoch: 1, step: 90
	action: tensor([[-17.0643, -14.1507,   1.5806,  10.5461,   0.0630,   0.2247,   2.6899]],
       dtype=torch.float64)
	q_value: tensor([[-8.7827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3124125487776177
epoch: 1, step: 91
	action: tensor([[-12.5037, -15.3215,   4.7902,  -3.0004,  -1.9288,  -4.2216,  -2.5650]],
       dtype=torch.float64)
	q_value: tensor([[-8.7827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3124125487776177
epoch: 1, step: 92
	action: tensor([[-6.5125, -1.0348, -3.0601, 17.8898, -6.9202,  1.5966, 17.7038]],
       dtype=torch.float64)
	q_value: tensor([[-8.7827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2404862891631201, distance: 1.2745376746416326 entropy 3.3124125487776177
epoch: 1, step: 93
	action: tensor([[-25.1674, -13.2869, -10.6032,  -2.2478,  -9.1331,  -8.5347, -11.0543]],
       dtype=torch.float64)
	q_value: tensor([[-9.2100]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.560201547942802, distance: 1.429377842145394 entropy 3.718208635158942
epoch: 1, step: 94
	action: tensor([[-21.6248, -16.8544,  -4.3462,  10.8548,   2.5011,  22.0812,   5.3011]],
       dtype=torch.float64)
	q_value: tensor([[-11.8738]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.8856812472197686
epoch: 1, step: 95
	action: tensor([[-7.7120e+00, -5.5866e+00, -5.2413e+00, -8.2571e+00, -7.2413e+00,
         -1.3010e+01,  7.7296e-04]], dtype=torch.float64)
	q_value: tensor([[-8.7827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8254937039427375, distance: 1.5461330474230923 entropy 3.3124125487776177
epoch: 1, step: 96
	action: tensor([[-14.6285,  -7.1845,   3.6101,   7.2452,   1.8127,  20.4872,   6.4030]],
       dtype=torch.float64)
	q_value: tensor([[-8.7022]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2076472563588188, distance: 1.7002842590720144 entropy 3.3404993669077068
epoch: 1, step: 97
	action: tensor([[-12.2282,  -3.9084,   3.3600, -10.0860, -12.1229, -21.4396,   2.5020]],
       dtype=torch.float64)
	q_value: tensor([[-8.5209]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.6457853355909977
epoch: 1, step: 98
	action: tensor([[-10.3628,  -7.8729,   1.6578,   2.1402,  -3.2685,   7.5150,   3.4374]],
       dtype=torch.float64)
	q_value: tensor([[-8.7827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3124125487776177
epoch: 1, step: 99
	action: tensor([[-20.7548,  -4.5239,  -6.9399,   4.1576,  -0.4422,  11.8919,  -1.5287]],
       dtype=torch.float64)
	q_value: tensor([[-8.7827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3124125487776177
epoch: 1, step: 100
	action: tensor([[ 1.0692,  1.5552, -5.4151,  2.3920, -4.0361,  2.8999,  6.2115]],
       dtype=torch.float64)
	q_value: tensor([[-8.7827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3124125487776177
epoch: 1, step: 101
	action: tensor([[-14.9830, -14.1317,   0.2942,   2.2729,   1.7607,  -5.7136,  11.3214]],
       dtype=torch.float64)
	q_value: tensor([[-8.7827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20770653452875032, distance: 1.2575851526468969 entropy 3.3124125487776177
epoch: 1, step: 102
	action: tensor([[-19.3573,   1.0418,  -8.1271,  12.7485,  -6.9277,  -1.3292,  -4.6353]],
       dtype=torch.float64)
	q_value: tensor([[-6.2357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.26055267412520666, distance: 1.2848049237919699 entropy 3.5547332494822674
epoch: 1, step: 103
	action: tensor([[-21.2927,   9.8661,  -4.0522,  -9.9807,  -6.2337,  -0.4839,  -9.5413]],
       dtype=torch.float64)
	q_value: tensor([[-8.2737]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.5197750551407276
epoch: 1, step: 104
	action: tensor([[-8.9955, -7.5005, -9.9161,  6.2705, -2.6137, -8.9252, -9.8163]],
       dtype=torch.float64)
	q_value: tensor([[-8.7827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3124125487776177
epoch: 1, step: 105
	action: tensor([[-14.7419,  -7.1527,  -0.5057,   5.5768,  -4.5776,   3.2485,   2.4553]],
       dtype=torch.float64)
	q_value: tensor([[-8.7827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3124125487776177
epoch: 1, step: 106
	action: tensor([[-12.3100,  -1.9329,   3.5044,   1.7499,   1.3563,  10.1348,  -6.5688]],
       dtype=torch.float64)
	q_value: tensor([[-8.7827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3124125487776177
epoch: 1, step: 107
	action: tensor([[-12.2962,  -5.2069, -11.2007,  10.3493,   7.3324,   2.1426,   0.5421]],
       dtype=torch.float64)
	q_value: tensor([[-8.7827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3124125487776177
epoch: 1, step: 108
	action: tensor([[-10.3109,  -8.0924,   2.8408,   2.3529,  -4.9112,   2.0784,  -6.9806]],
       dtype=torch.float64)
	q_value: tensor([[-8.7827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3124125487776177
epoch: 1, step: 109
	action: tensor([[-15.5682,  -6.6708,  -4.2618,  -1.1699,  -7.2717,  -3.8108,   2.8781]],
       dtype=torch.float64)
	q_value: tensor([[-8.7827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3124125487776177
epoch: 1, step: 110
	action: tensor([[-5.0410, -9.1454,  0.0518,  4.7021,  1.5605,  1.4387, 20.4592]],
       dtype=torch.float64)
	q_value: tensor([[-8.7827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3124125487776177
epoch: 1, step: 111
	action: tensor([[-15.2168,  -2.6646,   6.6501,   5.6793, -13.9936, -12.9146,   5.4502]],
       dtype=torch.float64)
	q_value: tensor([[-8.7827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3124125487776177
epoch: 1, step: 112
	action: tensor([[-6.2987, -6.6000, -2.2757,  5.5036, -5.2794, 23.4036,  6.8101]],
       dtype=torch.float64)
	q_value: tensor([[-8.7827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3124125487776177
epoch: 1, step: 113
	action: tensor([[-15.7611,  -6.0134,  -3.2427,  -2.0540,  -3.6819,  -8.8053,  16.5067]],
       dtype=torch.float64)
	q_value: tensor([[-8.7827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3124125487776177
epoch: 1, step: 114
	action: tensor([[-14.8048,  -9.3210, -12.1841,  -0.6963,   5.3444,  -8.4877,  -7.0075]],
       dtype=torch.float64)
	q_value: tensor([[-8.7827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3124125487776177
epoch: 1, step: 115
	action: tensor([[-11.2768,   2.8922,   2.4087,   9.3873,  -3.4472,   8.2047,   6.7326]],
       dtype=torch.float64)
	q_value: tensor([[-8.7827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3124125487776177
epoch: 1, step: 116
	action: tensor([[-4.6023, -4.2541,  8.8946,  2.8418,  3.7942, -1.1343,  2.7924]],
       dtype=torch.float64)
	q_value: tensor([[-8.7827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3124125487776177
epoch: 1, step: 117
	action: tensor([[ -7.3828,  -9.3084,  -4.0941,  -0.7706, -10.2566,  14.4299,  14.7516]],
       dtype=torch.float64)
	q_value: tensor([[-8.7827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3124125487776177
epoch: 1, step: 118
	action: tensor([[-14.7948, -17.8588,  -5.8596,   0.4641,  -5.4434, -11.6924,  -2.1256]],
       dtype=torch.float64)
	q_value: tensor([[-8.7827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3124125487776177
epoch: 1, step: 119
	action: tensor([[-13.7664,   2.8251,   1.8928,  -0.6754,  -1.5748,  -3.2040,  -5.0771]],
       dtype=torch.float64)
	q_value: tensor([[-8.7827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3124125487776177
epoch: 1, step: 120
	action: tensor([[-12.8416,  -6.0913,   7.5212,   6.5096,  -3.1766, -12.3061,   1.7600]],
       dtype=torch.float64)
	q_value: tensor([[-8.7827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3124125487776177
epoch: 1, step: 121
	action: tensor([[-16.9664,  -2.2153,   5.0193,  -0.6671,  -2.3594,  -0.5103,  -4.4540]],
       dtype=torch.float64)
	q_value: tensor([[-8.7827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3124125487776177
epoch: 1, step: 122
	action: tensor([[-16.7256,  -3.2146,  -5.2143,   3.7233,  -1.2289, -24.2385,   7.1529]],
       dtype=torch.float64)
	q_value: tensor([[-8.7827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3124125487776177
epoch: 1, step: 123
	action: tensor([[-15.7622, -24.4474,  -6.5607,   4.6645,  -7.9159,   3.4402,   7.7272]],
       dtype=torch.float64)
	q_value: tensor([[-8.7827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3124125487776177
epoch: 1, step: 124
	action: tensor([[-12.4925, -19.6345,   6.7580,   9.2281, -10.3038,  -5.8874,   0.9592]],
       dtype=torch.float64)
	q_value: tensor([[-8.7827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3124125487776177
epoch: 1, step: 125
	action: tensor([[-5.8723, -3.3035,  1.1800, 14.3124, -0.9476,  3.1862,  4.0626]],
       dtype=torch.float64)
	q_value: tensor([[-8.7827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.3124125487776177
epoch: 1, step: 126
	action: tensor([[ -8.1999,   1.3311, -10.0466,   4.6147,  -8.6947,   7.7223,  -1.7181]],
       dtype=torch.float64)
	q_value: tensor([[-8.7827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7601751898366402, distance: 0.5604072536499164 entropy 3.3124125487776177
epoch: 1, step: 127
	action: tensor([[ -5.4988,  22.7763,   3.0321,   5.3660,  -6.5776, -21.0236,   0.2241]],
       dtype=torch.float64)
	q_value: tensor([[-12.5033]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.7748964727287264
LOSS epoch 1 actor 453.8081533972427 critic 1668.941186320877
epoch: 2, step: 0
	action: tensor([[-20.5609,  -7.5137,  -2.3617,   2.0604,  -5.9535,  -3.6279,   7.3896]],
       dtype=torch.float64)
	q_value: tensor([[-12.6533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.603085624679822
epoch: 2, step: 1
	action: tensor([[  8.4667,  -5.1850,  33.0077,   0.8301, -14.1787,  16.2542, -12.0935]],
       dtype=torch.float64)
	q_value: tensor([[-12.6533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.603085624679822
epoch: 2, step: 2
	action: tensor([[-35.6195, -10.6843, -11.5362,   4.4232,  -4.5507, -16.0117,   8.2929]],
       dtype=torch.float64)
	q_value: tensor([[-12.6533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.603085624679822
epoch: 2, step: 3
	action: tensor([[-11.4157, -13.5978, -12.5529,  16.5679,  -0.7108, -11.0278,   0.8922]],
       dtype=torch.float64)
	q_value: tensor([[-12.6533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.603085624679822
epoch: 2, step: 4
	action: tensor([[ -6.6234, -10.1905,   6.0616,   7.5994,   5.3938, -12.3269, -16.3827]],
       dtype=torch.float64)
	q_value: tensor([[-12.6533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.603085624679822
epoch: 2, step: 5
	action: tensor([[ -4.2304, -33.6221,  -7.4829, -12.4513,  -9.9983, -11.3219,   1.4129]],
       dtype=torch.float64)
	q_value: tensor([[-12.6533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.603085624679822
epoch: 2, step: 6
	action: tensor([[-27.4935, -20.5065,  13.5078,  -0.7487, -10.1798,   1.5623,  15.6186]],
       dtype=torch.float64)
	q_value: tensor([[-12.6533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7098647882653131, distance: 1.4963651247481797 entropy 3.603085624679822
epoch: 2, step: 7
	action: tensor([[-23.2110,  -6.4463, -21.2705, -13.1475,   5.9524, -18.7231,  -9.7496]],
       dtype=torch.float64)
	q_value: tensor([[-9.8499]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.752431409617393
epoch: 2, step: 8
	action: tensor([[-15.2457,   1.2420, -11.5532,   3.4452,  -6.2078,  -8.9778,  -1.9665]],
       dtype=torch.float64)
	q_value: tensor([[-12.6533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.603085624679822
epoch: 2, step: 9
	action: tensor([[-29.0099,  -3.7716,   9.3304,   6.7820, -18.2633,  13.1837,  13.7219]],
       dtype=torch.float64)
	q_value: tensor([[-12.6533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.603085624679822
epoch: 2, step: 10
	action: tensor([[-10.8663,  -7.9573,  -6.2803,  -4.0248,  -8.9613,   0.6465,   1.2721]],
       dtype=torch.float64)
	q_value: tensor([[-12.6533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.603085624679822
epoch: 2, step: 11
	action: tensor([[-10.3873, -10.9147,  -4.5195,  16.6647,  -7.1024,  18.3592,   9.1378]],
       dtype=torch.float64)
	q_value: tensor([[-12.6533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.603085624679822
epoch: 2, step: 12
	action: tensor([[-22.2967,  -2.9902,  -5.3086,  -6.2572, -13.4828,  -0.0606,   0.5163]],
       dtype=torch.float64)
	q_value: tensor([[-12.6533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.603085624679822
epoch: 2, step: 13
	action: tensor([[-13.4191,  -9.7350,  11.6191,  28.3062, -10.2970,   2.3972,   7.2915]],
       dtype=torch.float64)
	q_value: tensor([[-12.6533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.603085624679822
epoch: 2, step: 14
	action: tensor([[ -2.0781, -10.2964,   1.8891,  23.6712,  -0.1038,  -1.4571,  -7.9803]],
       dtype=torch.float64)
	q_value: tensor([[-12.6533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.603085624679822
epoch: 2, step: 15
	action: tensor([[-13.0456,  -8.0150,   4.0857,  11.5580, -10.0433,   7.7032,  -3.1454]],
       dtype=torch.float64)
	q_value: tensor([[-12.6533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1775905474008347, distance: 1.0377691112279317 entropy 3.603085624679822
epoch: 2, step: 16
	action: tensor([[-18.1300,  -8.7323,  -1.9602,   1.7176,  -5.4290,  -2.9008,   6.7687]],
       dtype=torch.float64)
	q_value: tensor([[-12.6703]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.045905178653827
epoch: 2, step: 17
	action: tensor([[ -7.5483, -14.3226,  -8.6077,  10.1348, -16.8013, -10.3421,   9.5858]],
       dtype=torch.float64)
	q_value: tensor([[-12.6533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.603085624679822
epoch: 2, step: 18
	action: tensor([[-21.8195, -15.2758,  -3.6269,   1.6243,  -7.0168,   3.6948,  12.1921]],
       dtype=torch.float64)
	q_value: tensor([[-12.6533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.603085624679822
epoch: 2, step: 19
	action: tensor([[-12.0714, -22.4159,  -5.3987, -14.2743,  -4.7426,   2.6833,  19.0401]],
       dtype=torch.float64)
	q_value: tensor([[-12.6533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.603085624679822
epoch: 2, step: 20
	action: tensor([[-12.4686, -15.0593,  -0.8565,  -8.0619, -13.0608,  -1.3565,   5.1341]],
       dtype=torch.float64)
	q_value: tensor([[-12.6533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5887246134601396, distance: 0.7338766213565879 entropy 3.603085624679822
epoch: 2, step: 21
	action: tensor([[-25.6169,  -2.7371,   0.2308,  10.8615,  -6.8150,   7.0782,   2.6300]],
       dtype=torch.float64)
	q_value: tensor([[-12.1235]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.76429338242701
epoch: 2, step: 22
	action: tensor([[-18.0974,  -5.0211,  12.3701,  11.0010,  -5.3019,  11.9179,  -8.4671]],
       dtype=torch.float64)
	q_value: tensor([[-12.6533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09279434497918215, distance: 1.08995750882165 entropy 3.603085624679822
epoch: 2, step: 23
	action: tensor([[-17.4363, -12.2006,  -2.3713,  -0.2946,  -9.6881,  -1.6877,  -6.6112]],
       dtype=torch.float64)
	q_value: tensor([[-10.4612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.540831748363407
epoch: 2, step: 24
	action: tensor([[-10.3716,  -8.9992,   1.1954,  -9.5396, -11.3015,   8.0668,  17.0185]],
       dtype=torch.float64)
	q_value: tensor([[-12.6533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.603085624679822
epoch: 2, step: 25
	action: tensor([[-23.8817, -10.6369,   0.5204,  11.8563, -10.3911,  12.6443,  -3.3490]],
       dtype=torch.float64)
	q_value: tensor([[-12.6533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.051001122364788376, distance: 1.1147809603436192 entropy 3.603085624679822
epoch: 2, step: 26
	action: tensor([[-17.2604,  -9.2720, -20.3244,   8.4134,  -7.8280,  10.6644,   4.9014]],
       dtype=torch.float64)
	q_value: tensor([[-10.6886]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.735726477381239
epoch: 2, step: 27
	action: tensor([[-5.6704, -7.6190,  6.2987,  9.5410, -1.2798, -8.5728, -4.8448]],
       dtype=torch.float64)
	q_value: tensor([[-12.6533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4923076826232061, distance: 0.8153742443169852 entropy 3.603085624679822
epoch: 2, step: 28
	action: tensor([[-15.8664, -12.8324,  18.5482,  33.2608,   5.7352,   3.9130,   3.8874]],
       dtype=torch.float64)
	q_value: tensor([[-12.0474]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.9301349241979677
epoch: 2, step: 29
	action: tensor([[ -7.8171,  -6.7397, -15.2318,   2.5338, -13.6642,  -3.0787,  -3.8305]],
       dtype=torch.float64)
	q_value: tensor([[-12.6533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.603085624679822
epoch: 2, step: 30
	action: tensor([[-15.0701,  -0.8610,   9.4306,   4.5681,  -8.1460,  -5.3804,   7.2102]],
       dtype=torch.float64)
	q_value: tensor([[-12.6533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.169756120753918, distance: 1.6856296208948607 entropy 3.603085624679822
epoch: 2, step: 31
	action: tensor([[-44.6002,  11.5762,   9.3890,  24.5125,   2.1095, -14.0860,   4.4101]],
       dtype=torch.float64)
	q_value: tensor([[-16.4803]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8395195918279066, distance: 1.5520614143577325 entropy 4.119434642718128
epoch: 2, step: 32
	action: tensor([[-22.3891, -30.0217, -11.4342,  -8.3717, -15.7387, -10.1536, -22.3030]],
       dtype=torch.float64)
	q_value: tensor([[-12.1603]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.8586779630551837
epoch: 2, step: 33
	action: tensor([[-21.1526, -17.4445,   2.7764,  11.9675,  -8.0198, -11.5266,  10.5169]],
       dtype=torch.float64)
	q_value: tensor([[-12.6533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.603085624679822
epoch: 2, step: 34
	action: tensor([[-18.4799, -17.8410,   6.1521,   6.2555,  -8.1156,   2.9978,   9.9823]],
       dtype=torch.float64)
	q_value: tensor([[-12.6533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.603085624679822
epoch: 2, step: 35
	action: tensor([[-18.2693, -29.9784, -15.0966,   3.6729, -17.0994,  -8.1014,  12.4707]],
       dtype=torch.float64)
	q_value: tensor([[-12.6533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.603085624679822
epoch: 2, step: 36
	action: tensor([[-11.2520,  -7.4261,   2.3589, -20.9383,  -4.0261,  22.5085,  -2.0154]],
       dtype=torch.float64)
	q_value: tensor([[-12.6533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0844365827678989, distance: 1.09496669325157 entropy 3.603085624679822
epoch: 2, step: 37
	action: tensor([[ -5.5161, -26.3124,  -3.0337,   2.0114, -11.3927, -12.6215,  -8.6280]],
       dtype=torch.float64)
	q_value: tensor([[-12.6548]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8661208011104544, distance: 1.563243279163099 entropy 3.9453670039624664
epoch: 2, step: 38
	action: tensor([[-13.1394,  -5.1755, -21.7363,  18.1194,  -9.3197, -28.0888,  26.4086]],
       dtype=torch.float64)
	q_value: tensor([[-12.0766]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09691939026095875, distance: 1.0874766784538532 entropy 4.0409282244613705
epoch: 2, step: 39
	action: tensor([[-38.3848,  15.2051,  -2.9943, -13.5437, -12.9483,  26.3003,  -1.1359]],
       dtype=torch.float64)
	q_value: tensor([[-18.6011]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.276478944835589
epoch: 2, step: 40
	action: tensor([[-19.5927,   4.8591,   4.7916,   8.5485,   1.6663,  15.2913,  -5.4680]],
       dtype=torch.float64)
	q_value: tensor([[-12.6533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2629339374210018, distance: 1.286017889880433 entropy 3.603085624679822
epoch: 2, step: 41
	action: tensor([[-24.3510,   3.8878, -10.3918,  14.2377, -17.6342,   3.0831, -10.0351]],
       dtype=torch.float64)
	q_value: tensor([[-12.7929]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.9591634001968745
epoch: 2, step: 42
	action: tensor([[-17.1735, -27.4464,   8.5528,  17.5219,  -5.8771,  -2.6944,   5.3397]],
       dtype=torch.float64)
	q_value: tensor([[-12.6533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.603085624679822
epoch: 2, step: 43
	action: tensor([[-10.5137, -16.2316,  -2.6679,  -4.1673,  -1.5721,   1.2710,  -5.8248]],
       dtype=torch.float64)
	q_value: tensor([[-12.6533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.603085624679822
epoch: 2, step: 44
	action: tensor([[-39.9408, -13.9667,  -0.3909,  -1.8630,  -3.2719,   4.3521,  -9.3895]],
       dtype=torch.float64)
	q_value: tensor([[-12.6533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.603085624679822
epoch: 2, step: 45
	action: tensor([[-17.1587,  -2.2531,  -7.0583,  -8.8651, -16.1109,  11.8164,  -2.8207]],
       dtype=torch.float64)
	q_value: tensor([[-12.6533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.603085624679822
epoch: 2, step: 46
	action: tensor([[-19.9134,  -1.3905,  -6.9205,  -8.6157,  -5.8565,   2.9439,  -2.8248]],
       dtype=torch.float64)
	q_value: tensor([[-12.6533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.603085624679822
epoch: 2, step: 47
	action: tensor([[-10.6361, -27.4682, -11.6156,  -0.6549,  -7.8624,  15.0826,   2.7524]],
       dtype=torch.float64)
	q_value: tensor([[-12.6533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.603085624679822
epoch: 2, step: 48
	action: tensor([[-3.3155, -6.0616, 21.2273,  4.3243, -6.6252,  8.4191,  9.0307]],
       dtype=torch.float64)
	q_value: tensor([[-12.6533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.603085624679822
epoch: 2, step: 49
	action: tensor([[-15.3699,  -3.7685,   7.7285,  10.3299,  -4.7777,   5.6037,  -8.4584]],
       dtype=torch.float64)
	q_value: tensor([[-12.6533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.603085624679822
epoch: 2, step: 50
	action: tensor([[-21.7910,  -2.2253,  -0.8123,  11.7228,  -5.5551,   8.0645,   7.6530]],
       dtype=torch.float64)
	q_value: tensor([[-12.6533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.603085624679822
epoch: 2, step: 51
	action: tensor([[-24.8440,  -5.2555,  -8.7992,   6.3256, -10.7682,  -3.8336,  17.6077]],
       dtype=torch.float64)
	q_value: tensor([[-12.6533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.603085624679822
epoch: 2, step: 52
	action: tensor([[-18.2243,  -2.6710,  -3.0980,  19.1645, -11.0056,  -1.8531,  17.1932]],
       dtype=torch.float64)
	q_value: tensor([[-12.6533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.603085624679822
epoch: 2, step: 53
	action: tensor([[-17.1746, -15.4714,   3.4371, -10.6891,  -5.2751, -10.2405,  -1.3776]],
       dtype=torch.float64)
	q_value: tensor([[-12.6533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.603085624679822
epoch: 2, step: 54
	action: tensor([[-14.1750, -18.7154,  13.8736,   2.7016,  -4.1335,  11.3921,  -4.9826]],
       dtype=torch.float64)
	q_value: tensor([[-12.6533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.603085624679822
epoch: 2, step: 55
	action: tensor([[-10.0702, -14.7602,  -5.8407, -11.1179, -13.3487, -13.3455, -21.5610]],
       dtype=torch.float64)
	q_value: tensor([[-12.6533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.603085624679822
epoch: 2, step: 56
	action: tensor([[-19.3618, -27.4936,   9.3303,  -2.2105,  -5.1425,   5.9513,   3.3135]],
       dtype=torch.float64)
	q_value: tensor([[-12.6533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.603085624679822
epoch: 2, step: 57
	action: tensor([[-10.7182,  -7.8247,  -0.6038,   0.0120,   2.5877,  -8.4859,   5.3482]],
       dtype=torch.float64)
	q_value: tensor([[-12.6533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.603085624679822
epoch: 2, step: 58
	action: tensor([[-11.3371, -12.3504,  -1.0412,  -2.9120,  -4.8940,   7.6979,   0.9257]],
       dtype=torch.float64)
	q_value: tensor([[-12.6533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.603085624679822
epoch: 2, step: 59
	action: tensor([[ -0.0507, -15.2143,  16.8646,   8.7088,   6.1453, -14.8112,  13.0982]],
       dtype=torch.float64)
	q_value: tensor([[-12.6533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.603085624679822
epoch: 2, step: 60
	action: tensor([[-25.7466,  -2.8280,  18.6882,  10.6883, -12.9454,   2.4932,   4.2334]],
       dtype=torch.float64)
	q_value: tensor([[-12.6533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.603085624679822
epoch: 2, step: 61
	action: tensor([[-12.2958, -12.2794, -12.6229,   0.8666, -21.5671,  14.8657,  -8.4210]],
       dtype=torch.float64)
	q_value: tensor([[-12.6533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5055893279429275, distance: 1.404138531378976 entropy 3.603085624679822
epoch: 2, step: 62
	action: tensor([[-34.2960, -11.7344,   5.5659,  -6.9585,  -9.7381,  -0.2759, -11.7403]],
       dtype=torch.float64)
	q_value: tensor([[-13.5187]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.7857266688702809, distance: 1.909967354234282 entropy 3.986959649064719
epoch: 2, step: 63
	action: tensor([[-33.4754, -14.4166,  22.1189, -10.0418, -14.2962,  -2.4218,   3.2424]],
       dtype=torch.float64)
	q_value: tensor([[-9.8177]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.762029585525785
epoch: 2, step: 64
	action: tensor([[-13.1929, -19.7273, -10.9573,  -2.8933, -14.4035,   4.4733,   7.0272]],
       dtype=torch.float64)
	q_value: tensor([[-12.6533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.603085624679822
epoch: 2, step: 65
	action: tensor([[-19.3101,  -6.0921, -10.4294,  14.8516,   5.9548, -22.3044,   3.9494]],
       dtype=torch.float64)
	q_value: tensor([[-12.6533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2927475245769964, distance: 1.3011086096832338 entropy 3.603085624679822
epoch: 2, step: 66
	action: tensor([[-24.2850,   6.5253,  17.3720, -13.0773, -24.0764,  23.3322,  -6.6141]],
       dtype=torch.float64)
	q_value: tensor([[-13.7885]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07296927562688693, distance: 1.1853601869040655 entropy 3.881002162871318
epoch: 2, step: 67
	action: tensor([[-18.5857,  -0.4106, -16.5197, -10.5364, -18.8906,  -8.5559,   5.9891]],
       dtype=torch.float64)
	q_value: tensor([[-13.0685]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.8664939004753895
epoch: 2, step: 68
	action: tensor([[-21.9247,   7.2873, -13.3005,  -5.1230, -20.0986,   4.9092,  -9.3281]],
       dtype=torch.float64)
	q_value: tensor([[-12.6533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.603085624679822
epoch: 2, step: 69
	action: tensor([[ -8.5661,  -5.5237,  -4.7641,  -7.9907, -20.6480,   6.6998,   9.8267]],
       dtype=torch.float64)
	q_value: tensor([[-12.6533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8575024453167708, distance: 1.5596293175187885 entropy 3.603085624679822
epoch: 2, step: 70
	action: tensor([[-38.4574, -21.1787,   6.7672,  12.2167,  -7.1366,  -7.7482,  50.5988]],
       dtype=torch.float64)
	q_value: tensor([[-17.5974]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4281806687859191, distance: 1.3675659165116907 entropy 3.994854581751688
epoch: 2, step: 71
	action: tensor([[-18.0732,   2.0689,   4.1873,  12.2483,  -6.3288,  -2.3452,   2.3740]],
       dtype=torch.float64)
	q_value: tensor([[-9.2441]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.630583674727768
epoch: 2, step: 72
	action: tensor([[-19.3263,  -7.3732,   2.4786, -12.3862,  -9.1438,  11.2340,   6.0541]],
       dtype=torch.float64)
	q_value: tensor([[-12.6533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.603085624679822
epoch: 2, step: 73
	action: tensor([[-33.4926, -12.3069,   2.2947,  -4.1028,   8.8019,  -6.2974,  -0.9373]],
       dtype=torch.float64)
	q_value: tensor([[-12.6533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.603085624679822
epoch: 2, step: 74
	action: tensor([[-10.8820, -19.4585,  -2.7552,   3.4902,  -1.9128, -26.5120,  16.5816]],
       dtype=torch.float64)
	q_value: tensor([[-12.6533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.603085624679822
epoch: 2, step: 75
	action: tensor([[ -4.2116, -13.4959, -17.8448,  23.6965,  -9.9753,  -2.4243,  12.1967]],
       dtype=torch.float64)
	q_value: tensor([[-12.6533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.603085624679822
epoch: 2, step: 76
	action: tensor([[-16.7414,  -7.1154,   2.7872,  -9.0036, -15.5004,  11.0275,  15.5821]],
       dtype=torch.float64)
	q_value: tensor([[-12.6533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.603085624679822
epoch: 2, step: 77
	action: tensor([[ -8.2948, -21.3817, -10.9564,   7.4148, -10.0505,  16.8743,   8.1160]],
       dtype=torch.float64)
	q_value: tensor([[-12.6533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.603085624679822
epoch: 2, step: 78
	action: tensor([[-31.6044, -19.1839,  -0.9079,  11.9066,   8.8603,  18.0837,  12.5149]],
       dtype=torch.float64)
	q_value: tensor([[-12.6533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.28204700962761664, distance: 1.2957125589792637 entropy 3.603085624679822
epoch: 2, step: 79
	action: tensor([[-20.0090, -18.3152,  -1.7384, -19.9141,  -8.9864,  -2.8464,   3.9471]],
       dtype=torch.float64)
	q_value: tensor([[-9.5918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.641907456081853
epoch: 2, step: 80
	action: tensor([[-10.1312, -23.6583, -22.3421,  12.6297, -11.0711,  -4.2573,   7.6217]],
       dtype=torch.float64)
	q_value: tensor([[-12.6533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.603085624679822
epoch: 2, step: 81
	action: tensor([[-18.6852,  -5.9462,   0.8376,   8.3214,  -5.9158,   1.7263,  -4.7470]],
       dtype=torch.float64)
	q_value: tensor([[-12.6533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.603085624679822
epoch: 2, step: 82
	action: tensor([[-17.1293, -21.2770,   1.7933,  -6.2703,   4.2516,  -4.3322,  -7.4383]],
       dtype=torch.float64)
	q_value: tensor([[-12.6533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.603085624679822
epoch: 2, step: 83
	action: tensor([[-19.5111, -26.5119, -16.8300,  -2.0647,  14.3075,   8.5154,   4.4279]],
       dtype=torch.float64)
	q_value: tensor([[-12.6533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1493468242830358, distance: 1.2268239434751023 entropy 3.603085624679822
epoch: 2, step: 84
	action: tensor([[-25.6605, -28.7305,  -5.4317, -10.5576,  -6.1667,  -0.0402,  -6.4896]],
       dtype=torch.float64)
	q_value: tensor([[-14.7210]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.074232731845397
epoch: 2, step: 85
	action: tensor([[-16.8996, -12.0304,   0.2861,   1.7177, -11.4675,  -5.4032,   5.3524]],
       dtype=torch.float64)
	q_value: tensor([[-12.6533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.603085624679822
epoch: 2, step: 86
	action: tensor([[-26.9501,  -4.8365,   3.0150,   2.0875,   4.9446, -14.3322,   3.3086]],
       dtype=torch.float64)
	q_value: tensor([[-12.6533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4646891455935369, distance: 1.3849351083178 entropy 3.603085624679822
epoch: 2, step: 87
	action: tensor([[ -4.6087,  12.0097,  55.3057,  21.0342, -10.0250,  34.5561,  -9.6164]],
       dtype=torch.float64)
	q_value: tensor([[-16.8976]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.189338201543127
epoch: 2, step: 88
	action: tensor([[-29.8780,   2.7085,  -7.2364,   3.5858,  -3.3369,  29.2044,  -0.4911]],
       dtype=torch.float64)
	q_value: tensor([[-12.6533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.603085624679822
epoch: 2, step: 89
	action: tensor([[-26.8099, -14.2775,   3.9607,   3.3107,   0.6151,  -1.5033,   5.1715]],
       dtype=torch.float64)
	q_value: tensor([[-12.6533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.603085624679822
epoch: 2, step: 90
	action: tensor([[-11.6722,   2.3822,   2.7404,  29.7762, -13.4719, -12.7995,  -5.8824]],
       dtype=torch.float64)
	q_value: tensor([[-12.6533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.603085624679822
epoch: 2, step: 91
	action: tensor([[-1.9248e+01,  4.6499e-03,  1.7722e+00,  5.1505e+00, -2.9808e+00,
          1.9771e+01,  4.3237e+00]], dtype=torch.float64)
	q_value: tensor([[-12.6533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.603085624679822
epoch: 2, step: 92
	action: tensor([[-20.2779,  -5.4898,  -3.8623,   9.1708,  -6.9705,   8.2087,   9.2752]],
       dtype=torch.float64)
	q_value: tensor([[-12.6533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3889605883313477, distance: 0.8945225737805401 entropy 3.603085624679822
epoch: 2, step: 93
	action: tensor([[-33.5247, -17.1841,   4.1072,   1.9648,  -3.9475,  -2.9733,  -3.8637]],
       dtype=torch.float64)
	q_value: tensor([[-13.6446]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.052635676168197
epoch: 2, step: 94
	action: tensor([[-24.4848, -14.7894,   4.4484,  18.6177,  -8.6496,   4.8964,  -0.7496]],
       dtype=torch.float64)
	q_value: tensor([[-12.6533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.603085624679822
epoch: 2, step: 95
	action: tensor([[  1.1672,  -4.7973,   9.2184,  13.3632,  -7.5352, -12.0848,  -0.4115]],
       dtype=torch.float64)
	q_value: tensor([[-12.6533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.886828627336649, distance: 0.38496831398748854 entropy 3.603085624679822
epoch: 2, step: 96
	action: tensor([[-20.7719, -20.9435,  15.9044,   5.1800,  -1.2048,  -4.2696,  16.4754]],
       dtype=torch.float64)
	q_value: tensor([[-11.4956]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.6907003723001006
epoch: 2, step: 97
	action: tensor([[ -2.2389,  -5.4230,  15.7359,  -3.0188, -15.9352,  10.6158, -12.9563]],
       dtype=torch.float64)
	q_value: tensor([[-12.6533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.603085624679822
epoch: 2, step: 98
	action: tensor([[-20.1661, -18.9602,  12.5627,  -7.6293, -12.4142,  -2.8675,   5.3239]],
       dtype=torch.float64)
	q_value: tensor([[-12.6533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.603085624679822
epoch: 2, step: 99
	action: tensor([[-10.5685, -10.8600,  -6.2738,  16.1192,  -7.3295,  -6.1414,   0.4180]],
       dtype=torch.float64)
	q_value: tensor([[-12.6533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.603085624679822
epoch: 2, step: 100
	action: tensor([[-23.2357, -11.7847, -12.7041,  -4.4217, -11.0767,   1.9210,   2.1325]],
       dtype=torch.float64)
	q_value: tensor([[-12.6533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.603085624679822
epoch: 2, step: 101
	action: tensor([[-15.9508,  -8.7504,  -1.6291,  -2.0157,  -8.1830,  -4.0752,  -0.7074]],
       dtype=torch.float64)
	q_value: tensor([[-12.6533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.603085624679822
epoch: 2, step: 102
	action: tensor([[-12.7159,  -8.9251,   4.1458,  -1.5071, -12.1570, -11.9004,  15.2393]],
       dtype=torch.float64)
	q_value: tensor([[-12.6533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.603085624679822
epoch: 2, step: 103
	action: tensor([[-24.6226, -18.7828, -17.8993,  13.5782,   0.9890,   1.8544,  -7.9422]],
       dtype=torch.float64)
	q_value: tensor([[-12.6533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19483437332671438, distance: 1.0268317916068124 entropy 3.603085624679822
epoch: 2, step: 104
	action: tensor([[ -5.9693,  -5.1417,   5.7070,  13.4789,   0.4356, -11.5375,  13.3243]],
       dtype=torch.float64)
	q_value: tensor([[-9.3136]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.6113803312873585
epoch: 2, step: 105
	action: tensor([[-26.0879,  -2.7652, -12.7969, -15.5058,   2.8301,  15.1756,  -5.4580]],
       dtype=torch.float64)
	q_value: tensor([[-12.6533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.603085624679822
epoch: 2, step: 106
	action: tensor([[-17.5648,  -6.8230,  -3.4852,  12.1334,  -7.1183,  -1.3996,  -3.8041]],
       dtype=torch.float64)
	q_value: tensor([[-12.6533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.365199075107528, distance: 1.337071666705042 entropy 3.603085624679822
epoch: 2, step: 107
	action: tensor([[-32.8049,   6.7384,  -1.9186,  12.1552, -17.1365,  10.7859,   9.7801]],
       dtype=torch.float64)
	q_value: tensor([[-12.0859]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5744650493433379, distance: 1.4358967267068234 entropy 3.9304036899701322
epoch: 2, step: 108
	action: tensor([[-22.3531, -23.4427, -16.3309,  -6.3844, -16.7954, -14.7355,  -1.1254]],
       dtype=torch.float64)
	q_value: tensor([[-13.1788]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1470343365918645, distance: 1.2255891361629045 entropy 3.9194236548085506
epoch: 2, step: 109
	action: tensor([[-31.8974,  -0.8988,  20.4684,  -2.6836,  -2.4347,  19.9831,  11.1605]],
       dtype=torch.float64)
	q_value: tensor([[-14.5445]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.931397982904255
epoch: 2, step: 110
	action: tensor([[ 7.0081, 10.7008,  9.6910, -1.8898,  8.5786, -3.9501, -2.9049]],
       dtype=torch.float64)
	q_value: tensor([[-12.6533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.603085624679822
epoch: 2, step: 111
	action: tensor([[ -5.5738, -16.4880, -11.4926,  -3.1433, -24.9518,   1.9138,  -0.0335]],
       dtype=torch.float64)
	q_value: tensor([[-12.6533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.603085624679822
epoch: 2, step: 112
	action: tensor([[ -9.3422, -10.9283,  -0.9017,   5.7162, -16.7377,   4.6297,   5.3896]],
       dtype=torch.float64)
	q_value: tensor([[-12.6533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.603085624679822
epoch: 2, step: 113
	action: tensor([[ -1.2212,  -8.1384, -12.8470,  -3.7124,   3.6020,  18.0188,  11.0625]],
       dtype=torch.float64)
	q_value: tensor([[-12.6533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.603085624679822
epoch: 2, step: 114
	action: tensor([[-8.3691, -2.4572, -2.9495, 13.6584,  5.4979, -5.8105,  0.2782]],
       dtype=torch.float64)
	q_value: tensor([[-12.6533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.603085624679822
epoch: 2, step: 115
	action: tensor([[-14.7931,  -0.8964,  -1.9280,  25.8787,   2.2240,  16.5888,  -9.8123]],
       dtype=torch.float64)
	q_value: tensor([[-12.6533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.603085624679822
epoch: 2, step: 116
	action: tensor([[-15.3281,  -2.1904, -14.8518,  -4.4275,   0.7735,  -4.1406,  24.1779]],
       dtype=torch.float64)
	q_value: tensor([[-12.6533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.603085624679822
epoch: 2, step: 117
	action: tensor([[-17.0795, -17.5661,   4.5606,  -1.1304,   2.8925, -12.5928,   4.7023]],
       dtype=torch.float64)
	q_value: tensor([[-12.6533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.603085624679822
epoch: 2, step: 118
	action: tensor([[-21.8843, -13.7605,   3.5267,  24.6956,  -2.5145,  -8.1911,  10.2692]],
       dtype=torch.float64)
	q_value: tensor([[-12.6533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.603085624679822
epoch: 2, step: 119
	action: tensor([[-19.7778, -12.6118,  -7.0574,  -4.6834,  -9.9779,  -0.2979,   0.8358]],
       dtype=torch.float64)
	q_value: tensor([[-12.6533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.603085624679822
epoch: 2, step: 120
	action: tensor([[-18.8720,  -4.2010,  -2.9916,  -0.5388, -11.2101,  -2.7385,  -9.4463]],
       dtype=torch.float64)
	q_value: tensor([[-12.6533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.603085624679822
epoch: 2, step: 121
	action: tensor([[-12.3749, -14.9174, -16.0878,   5.5746,  -6.0243,   4.1887,   2.3651]],
       dtype=torch.float64)
	q_value: tensor([[-12.6533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.603085624679822
epoch: 2, step: 122
	action: tensor([[-17.6487,   4.5757,  -2.2953,  -1.7752,  -4.9095,   4.0533,   4.8076]],
       dtype=torch.float64)
	q_value: tensor([[-12.6533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.603085624679822
epoch: 2, step: 123
	action: tensor([[-20.4757,  -7.3569,  -2.0720,   4.6599,   6.3976, -13.3150,   2.6028]],
       dtype=torch.float64)
	q_value: tensor([[-12.6533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5712957216788075, distance: 1.434450800777245 entropy 3.603085624679822
epoch: 2, step: 124
	action: tensor([[-17.9676,  17.7048, -11.4854,  -7.8770,  -1.9839,  -6.1447,   6.7031]],
       dtype=torch.float64)
	q_value: tensor([[-12.5132]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2132560197002205, distance: 1.702442765726139 entropy 4.044855785046153
epoch: 2, step: 125
	action: tensor([[-20.8044,  -5.6702,  23.8718,  -5.5742,  -7.7800,  -3.9553,  -3.8062]],
       dtype=torch.float64)
	q_value: tensor([[-11.2950]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.7608517219460986
