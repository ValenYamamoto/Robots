epoch: 0, step: 0
	action: tensor([[-0.0574,  0.0456,  0.0565,  0.0513, -0.0180, -0.0264,  0.0306]],
       dtype=torch.float64)
	q_value: tensor([[-0.0444]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25352985261394334, distance: 0.988696314769448 entropy -8.346449374294538
epoch: 0, step: 1
	action: tensor([[-0.0462,  0.0436, -0.0112,  0.0529, -0.0189, -0.0414,  0.0279]],
       dtype=torch.float64)
	q_value: tensor([[-0.0431]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25757492754612, distance: 0.9860138341102749 entropy -8.2816375047893
epoch: 0, step: 2
	action: tensor([[-0.0334,  0.0405, -0.0002,  0.0441, -0.0189, -0.0266,  0.0278]],
       dtype=torch.float64)
	q_value: tensor([[-0.0428]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26714426003729874, distance: 0.979638715384759 entropy -8.284172458392865
epoch: 0, step: 3
	action: tensor([[-0.0621,  0.0520,  0.0294,  0.0171, -0.0189, -0.0312,  0.0279]],
       dtype=torch.float64)
	q_value: tensor([[-0.0428]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23226962854755095, distance: 1.0026770026506573 entropy -8.285666238336656
epoch: 0, step: 4
	action: tensor([[-0.0591,  0.0539,  0.0785,  0.0323, -0.0189, -0.0403,  0.0279]],
       dtype=torch.float64)
	q_value: tensor([[-0.0434]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24033674620758683, distance: 0.9973951520334797 entropy -8.283849048998508
epoch: 0, step: 5
	action: tensor([[-0.0602,  0.0508,  0.0205,  0.0097, -0.0189, -0.0362,  0.0279]],
       dtype=torch.float64)
	q_value: tensor([[-0.0431]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22409293646995077, distance: 1.008002353317536 entropy -8.280554356479024
epoch: 0, step: 6
	action: tensor([[-0.0251,  0.0528,  0.0460,  0.0245, -0.0189, -0.0363,  0.0278]],
       dtype=torch.float64)
	q_value: tensor([[-0.0434]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2665909912976142, distance: 0.9800084342736217 entropy -8.284144996467564
epoch: 0, step: 7
	action: tensor([[-0.0547,  0.0337,  0.0187,  0.0531, -0.0189, -0.0325,  0.0279]],
       dtype=torch.float64)
	q_value: tensor([[-0.0427]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2354854101746925, distance: 1.000574849353754 entropy -8.284970532663154
epoch: 0, step: 8
	action: tensor([[-0.0726,  0.0555, -0.0180,  0.0491, -0.0189, -0.0264,  0.0279]],
       dtype=torch.float64)
	q_value: tensor([[-0.0429]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2300779663963104, distance: 1.0041071682499878 entropy -8.282578936270479
epoch: 0, step: 9
	action: tensor([[-0.0595,  0.0475,  0.0359,  0.0319, -0.0189, -0.0321,  0.0278]],
       dtype=torch.float64)
	q_value: tensor([[-0.0436]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2292806956212673, distance: 1.0046269209827996 entropy -8.284567247895236
epoch: 0, step: 10
	action: tensor([[-0.0309,  0.0379,  0.0425,  0.0483, -0.0189, -0.0275,  0.0279]],
       dtype=torch.float64)
	q_value: tensor([[-0.0431]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25875076182968126, distance: 0.9852327126679183 entropy -8.283313960339495
epoch: 0, step: 11
	action: tensor([[-0.0626,  0.0378,  0.0522,  0.0359, -0.0189, -0.0396,  0.0279]],
       dtype=torch.float64)
	q_value: tensor([[-0.0428]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2157839838042691, distance: 1.0133851761926567 entropy -8.28377150742502
epoch: 0, step: 12
	action: tensor([[-0.0278,  0.0516,  0.0633, -0.0006, -0.0189, -0.0358,  0.0279]],
       dtype=torch.float64)
	q_value: tensor([[-0.0430]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24256341624915767, distance: 0.9959323330170589 entropy -8.281228622693625
epoch: 0, step: 13
	action: tensor([[-0.0583,  0.0440,  0.0065,  0.0301, -0.0188, -0.0303,  0.0279]],
       dtype=torch.float64)
	q_value: tensor([[-0.0427]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21955678024204572, distance: 1.0109445824109788 entropy -8.28485498406425
epoch: 0, step: 14
	action: tensor([[-0.0451,  0.0461,  0.0482,  0.0485, -0.0189, -0.0323,  0.0278]],
       dtype=torch.float64)
	q_value: tensor([[-0.0433]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2442811538182721, distance: 0.9948023893058199 entropy -8.284302924762242
epoch: 0, step: 15
	action: tensor([[-0.0521,  0.0406, -0.0194,  0.0506, -0.0189, -0.0347,  0.0279]],
       dtype=torch.float64)
	q_value: tensor([[-0.0430]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23059427610600514, distance: 1.0037704346575584 entropy -8.282751523939009
epoch: 0, step: 16
	action: tensor([[-0.0427,  0.0373, -0.0107,  0.0339, -0.0189, -0.0333,  0.0278]],
       dtype=torch.float64)
	q_value: tensor([[-0.0430]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23158583720254722, distance: 1.0031234284302435 entropy -8.284610168440643
epoch: 0, step: 17
	action: tensor([[-0.0242,  0.0449, -0.0226,  0.0372, -0.0189, -0.0382,  0.0278]],
       dtype=torch.float64)
	q_value: tensor([[-0.0430]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2572705276893058, distance: 0.9862159499346143 entropy -8.285298914159306
epoch: 0, step: 18
	action: tensor([[-0.0442,  0.0392,  0.0480,  0.0276, -0.0189, -0.0414,  0.0279]],
       dtype=torch.float64)
	q_value: tensor([[-0.0427]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22886443892712038, distance: 1.0048981781252664 entropy -8.2873353270922
epoch: 0, step: 19
	action: tensor([[-0.0348,  0.0385, -0.0585,  0.0216, -0.0189, -0.0364,  0.0279]],
       dtype=torch.float64)
	q_value: tensor([[-0.0427]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23402744023651345, distance: 1.0015284696161677 entropy -8.282862976624777
epoch: 0, step: 20
	action: tensor([[-0.0535,  0.0363,  0.0501,  0.0541, -0.0189, -0.0316,  0.0278]],
       dtype=torch.float64)
	q_value: tensor([[-0.0432]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23251860805894953, distance: 1.0025144024180763 entropy -8.288082027081023
epoch: 0, step: 21
	action: tensor([[-0.0821,  0.0416, -0.0019,  0.0400, -0.0189, -0.0342,  0.0279]],
       dtype=torch.float64)
	q_value: tensor([[-0.0429]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19623540096464742, distance: 1.0259380338294444 entropy -8.281376904941297
epoch: 0, step: 22
	action: tensor([[-0.0501,  0.0526,  0.0465,  0.0590, -0.0189, -0.0259,  0.0278]],
       dtype=torch.float64)
	q_value: tensor([[-0.0435]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24967568017673258, distance: 0.9912454455205847 entropy -8.28270917251916
epoch: 0, step: 23
	action: tensor([[-0.0286,  0.0584, -0.0302,  0.0190, -0.0189, -0.0385,  0.0279]],
       dtype=torch.float64)
	q_value: tensor([[-0.0431]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2537231632034932, distance: 0.9885682869819149 entropy -8.282869698257059
epoch: 0, step: 24
	action: tensor([[-0.0321,  0.0531, -0.0056,  0.0355, -0.0189, -0.0307,  0.0278]],
       dtype=torch.float64)
	q_value: tensor([[-0.0431]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2560626082211507, distance: 0.987017577940351 entropy -8.288676493547348
epoch: 0, step: 25
	action: tensor([[-0.0357,  0.0553, -0.0265,  0.0119, -0.0189, -0.0328,  0.0278]],
       dtype=torch.float64)
	q_value: tensor([[-0.0429]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2423208524315298, distance: 0.9960917904661853 entropy -8.286974591550207
epoch: 0, step: 26
	action: tensor([[-0.0573,  0.0376, -0.0087,  0.0428, -0.0189, -0.0340,  0.0278]],
       dtype=torch.float64)
	q_value: tensor([[-0.0434]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22187237502511747, distance: 1.0094437192296248 entropy -8.28834603354915
epoch: 0, step: 27
	action: tensor([[-0.0534,  0.0425, -0.0526,  0.0448, -0.0189, -0.0417,  0.0278]],
       dtype=torch.float64)
	q_value: tensor([[-0.0431]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.227396178861607, distance: 1.0058543979414976 entropy -8.283945601923481
epoch: 0, step: 28
	action: tensor([[-0.0451,  0.0468, -0.0078,  0.0377, -0.0189, -0.0230,  0.0278]],
       dtype=torch.float64)
	q_value: tensor([[-0.0432]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24145759418633872, distance: 0.9966590752244683 entropy -8.285937691060564
epoch: 0, step: 29
	action: tensor([[-0.0644,  0.0457,  0.0571,  0.0583, -0.0189, -0.0367,  0.0278]],
       dtype=torch.float64)
	q_value: tensor([[-0.0432]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22873538259149306, distance: 1.0049822638926853 entropy -8.28596764447552
epoch: 0, step: 30
	action: tensor([[-0.0415,  0.0440,  0.0434,  0.0292, -0.0189, -0.0419,  0.0279]],
       dtype=torch.float64)
	q_value: tensor([[-0.0431]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2349244847879357, distance: 1.0009418436089956 entropy -8.280573599988246
epoch: 0, step: 31
	action: tensor([[-0.0624,  0.0364, -0.0439,  0.0367, -0.0189, -0.0293,  0.0279]],
       dtype=torch.float64)
	q_value: tensor([[-0.0428]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2114992321409429, distance: 1.0161498410597816 entropy -8.283400170422963
epoch: 0, step: 32
	action: tensor([[-0.0174,  0.0389, -0.0165,  0.0319, -0.0189, -0.0313,  0.0278]],
       dtype=torch.float64)
	q_value: tensor([[-0.0435]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2595258335848655, distance: 0.984717483963206 entropy -8.285526802970095
epoch: 0, step: 33
	action: tensor([[-0.0588,  0.0411, -0.0015,  0.0226, -0.0189, -0.0324,  0.0279]],
       dtype=torch.float64)
	q_value: tensor([[-0.0426]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21503862280215225, distance: 1.0138666496525883 entropy -8.287878653544508
epoch: 0, step: 34
	action: tensor([[-0.0466,  0.0411,  0.0206,  0.0640, -0.0189, -0.0325,  0.0278]],
       dtype=torch.float64)
	q_value: tensor([[-0.0433]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24650030249961608, distance: 0.9933407098360029 entropy -8.284543283624837
epoch: 0, step: 35
	action: tensor([[-0.0607,  0.0465, -0.0075,  0.0404, -0.0189, -0.0475,  0.0279]],
       dtype=torch.float64)
	q_value: tensor([[-0.0429]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2197903502086873, distance: 1.0107932940430724 entropy -8.282963155001786
epoch: 0, step: 36
	action: tensor([[-0.0427,  0.0460,  0.0415,  0.0528, -0.0189, -0.0222,  0.0278]],
       dtype=torch.float64)
	q_value: tensor([[-0.0430]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25083699171480145, distance: 0.9904780503372576 entropy -8.28361176043606
epoch: 0, step: 37
	action: tensor([[-0.0474,  0.0594, -0.0193,  0.0429, -0.0189, -0.0238,  0.0279]],
       dtype=torch.float64)
	q_value: tensor([[-0.0430]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24763993179488764, distance: 0.9925892373809282 entropy -8.283565789811913
epoch: 0, step: 38
	action: tensor([[-0.0119,  0.0508, -0.0670,  0.0345, -0.0189, -0.0321,  0.0278]],
       dtype=torch.float64)
	q_value: tensor([[-0.0433]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27360423986599025, distance: 0.9753114966668684 entropy -8.286464720388292
epoch: 0, step: 39
	action: tensor([[-0.0449,  0.0527, -0.0124,  0.0439, -0.0189, -0.0412,  0.0278]],
       dtype=torch.float64)
	q_value: tensor([[-0.0429]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24648147164220024, distance: 0.9933531221438431 entropy -8.290144768391192
epoch: 0, step: 40
	action: tensor([[-0.0693,  0.0483,  0.0649,  0.0370, -0.0189, -0.0280,  0.0278]],
       dtype=torch.float64)
	q_value: tensor([[-0.0429]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2191846383392687, distance: 1.0111855800861813 entropy -8.285124951931412
epoch: 0, step: 41
	action: tensor([[-0.0317,  0.0402,  0.0471,  0.0245, -0.0189, -0.0300,  0.0279]],
       dtype=torch.float64)
	q_value: tensor([[-0.0433]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24353891724454124, distance: 0.9952907968056705 entropy -8.281108317336855
epoch: 0, step: 42
	action: tensor([[-0.0569,  0.0527, -0.0137,  0.0417, -0.0189, -0.0391,  0.0279]],
       dtype=torch.float64)
	q_value: tensor([[-0.0428]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23014638940108811, distance: 1.0040625497346887 entropy -8.284221153316405
epoch: 0, step: 43
	action: tensor([[-0.0574,  0.0494,  0.0269,  0.0467, -0.0189, -0.0286,  0.0278]],
       dtype=torch.float64)
	q_value: tensor([[-0.0432]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23255579407430116, distance: 1.00249011520543 entropy -8.284502641440401
epoch: 0, step: 44
	action: tensor([[-0.0229,  0.0344,  0.0103,  0.0590, -0.0189, -0.0336,  0.0279]],
       dtype=torch.float64)
	q_value: tensor([[-0.0432]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2633149635171439, distance: 0.9821947706552857 entropy -8.283226127900168
epoch: 0, step: 45
	action: tensor([[-0.0418,  0.0418, -0.0082,  0.0257, -0.0189, -0.0301,  0.0279]],
       dtype=torch.float64)
	q_value: tensor([[-0.0426]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23372268867146473, distance: 1.0017276849875847 entropy -8.284731309688835
epoch: 0, step: 46
	action: tensor([[-0.0690,  0.0460,  0.0004,  0.0497, -0.0189, -0.0299,  0.0278]],
       dtype=torch.float64)
	q_value: tensor([[-0.0431]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21973483725510246, distance: 1.0108292530466259 entropy -8.285760347532586
epoch: 0, step: 47
	action: tensor([[-0.0389,  0.0377, -0.0120,  0.0583, -0.0189, -0.0380,  0.0278]],
       dtype=torch.float64)
	q_value: tensor([[-0.0433]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24596367597262148, distance: 0.9936943649891039 entropy -8.283457891954814
epoch: 0, step: 48
	action: tensor([[-0.0600,  0.0525,  0.0342,  0.0294, -0.0189, -0.0187,  0.0278]],
       dtype=torch.float64)
	q_value: tensor([[-0.0427]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22655639758994406, distance: 1.0064009059000754 entropy -8.284679412728746
epoch: 0, step: 49
	action: tensor([[-0.0370,  0.0448, -0.0017,  0.0591, -0.0189, -0.0405,  0.0279]],
       dtype=torch.float64)
	q_value: tensor([[-0.0434]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25326822547378636, distance: 0.9888695615691865 entropy -8.284338933597002
epoch: 0, step: 50
	action: tensor([[-0.0453,  0.0462,  0.0121,  0.0401, -0.0189, -0.0361,  0.0279]],
       dtype=torch.float64)
	q_value: tensor([[-0.0428]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2379028357370211, distance: 0.9989916680929791 entropy -8.28457549201573
epoch: 0, step: 51
	action: tensor([[-0.0502,  0.0340,  0.0200,  0.0169, -0.0189, -0.0325,  0.0279]],
       dtype=torch.float64)
	q_value: tensor([[-0.0429]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21404661477272624, distance: 1.014507092816003 entropy -8.284422563547267
epoch: 0, step: 52
	action: tensor([[-0.0646,  0.0373,  0.0282,  0.0187, -0.0189, -0.0308,  0.0279]],
       dtype=torch.float64)
	q_value: tensor([[-0.0430]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20177173444686503, distance: 1.0223985959277377 entropy -8.284158557022328
epoch: 0, step: 53
	action: tensor([[-0.0465,  0.0470, -0.0110,  0.0307, -0.0189, -0.0259,  0.0279]],
       dtype=torch.float64)
	q_value: tensor([[-0.0433]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23433097944093673, distance: 1.0013300073641451 entropy -8.283230758790191
epoch: 0, step: 54
	action: tensor([[-0.0463,  0.0606, -0.0406,  0.0221, -0.0189, -0.0319,  0.0278]],
       dtype=torch.float64)
	q_value: tensor([[-0.0433]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23861432052194032, distance: 0.9985252358754709 entropy -8.286154699259203
epoch: 0, step: 55
	action: tensor([[-0.0372,  0.0519,  0.0153,  0.0425, -0.0189, -0.0286,  0.0278]],
       dtype=torch.float64)
	q_value: tensor([[-0.0436]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2536408686347993, distance: 0.9886227919284185 entropy -8.288193353290618
epoch: 0, step: 56
	action: tensor([[-0.0359,  0.0454, -0.0303,  0.0287, -0.0189, -0.0396,  0.0279]],
       dtype=torch.float64)
	q_value: tensor([[-0.0430]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24066757567022745, distance: 0.9971779481387432 entropy -8.285537207307462
epoch: 0, step: 57
	action: tensor([[-0.0587,  0.0489,  0.0027,  0.0241, -0.0189, -0.0291,  0.0278]],
       dtype=torch.float64)
	q_value: tensor([[-0.0430]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22088219348751736, distance: 1.0100857827699026 entropy -8.286967048451379
epoch: 0, step: 58
	action: tensor([[-0.0618,  0.0528,  0.0456,  0.0363, -0.0189, -0.0392,  0.0278]],
       dtype=torch.float64)
	q_value: tensor([[-0.0434]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2237342507371365, distance: 1.0082353156814967 entropy -8.284961968027003
epoch: 0, step: 59
	action: tensor([[-0.0498,  0.0419,  0.0531,  0.0498, -0.0189, -0.0353,  0.0279]],
       dtype=torch.float64)
	q_value: tensor([[-0.0431]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2362491914261461, distance: 1.000074916920807 entropy -8.282489152812794
epoch: 0, step: 60
	action: tensor([[-0.0502,  0.0522,  0.0017,  0.0472, -0.0189, -0.0264,  0.0279]],
       dtype=torch.float64)
	q_value: tensor([[-0.0429]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24167403723208492, distance: 0.9965168713642919 entropy -8.281742543056772
epoch: 0, step: 61
	action: tensor([[-0.0592,  0.0529, -0.0511,  0.0553, -0.0189, -0.0324,  0.0278]],
       dtype=torch.float64)
	q_value: tensor([[-0.0432]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23329164357578525, distance: 1.0020093905099077 entropy -8.285089813907001
epoch: 0, step: 62
	action: tensor([[-0.0268,  0.0515,  0.0191,  0.0136, -0.0189, -0.0304,  0.0278]],
       dtype=torch.float64)
	q_value: tensor([[-0.0433]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25094205791203994, distance: 0.9904085932164628 entropy -8.285875413514002
epoch: 0, step: 63
	action: tensor([[-0.0358,  0.0450,  0.0219,  0.0501, -0.0189, -0.0256,  0.0279]],
       dtype=torch.float64)
	q_value: tensor([[-0.0429]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2548150822082521, distance: 0.9878448077167261 entropy -8.286856730340595
epoch: 0, step: 64
	action: tensor([[-0.0722,  0.0537, -0.0106,  0.0177, -0.0189, -0.0285,  0.0279]],
       dtype=torch.float64)
	q_value: tensor([[-0.0429]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20649528990401655, distance: 1.0193690601529468 entropy -8.284600359076704
epoch: 0, step: 65
	action: tensor([[-0.0554,  0.0464,  0.0271,  0.0256, -0.0189, -0.0321,  0.0278]],
       dtype=torch.float64)
	q_value: tensor([[-0.0439]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22141123721978095, distance: 1.0097427856688248 entropy -8.285117769531157
epoch: 0, step: 66
	action: tensor([[-0.0241,  0.0504,  0.0214,  0.0314, -0.0189, -0.0255,  0.0279]],
       dtype=torch.float64)
	q_value: tensor([[-0.0431]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26264967714073506, distance: 0.982638171366989 entropy -8.284046544555656
epoch: 0, step: 67
	action: tensor([[-0.0431,  0.0554, -0.0054,  0.0287, -0.0189, -0.0358,  0.0279]],
       dtype=torch.float64)
	q_value: tensor([[-0.0429]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2419904157546312, distance: 0.996308973018691 entropy -8.28683613263618
epoch: 0, step: 68
	action: tensor([[-0.0601,  0.0395,  0.0076,  0.0475, -0.0189, -0.0384,  0.0278]],
       dtype=torch.float64)
	q_value: tensor([[-0.0431]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2212778053544069, distance: 1.0098293050856877 entropy -8.286180878361813
epoch: 0, step: 69
	action: tensor([[-0.0658,  0.0632,  0.0476,  0.0432, -0.0189, -0.0366,  0.0278]],
       dtype=torch.float64)
	q_value: tensor([[-0.0430]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23028438361361192, distance: 1.0039725579307794 entropy -8.282940590697537
epoch: 0, step: 70
	action: tensor([[-0.0528,  0.0489,  0.0625,  0.0340, -0.0189, -0.0209,  0.0279]],
       dtype=torch.float64)
	q_value: tensor([[-0.0433]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23376863592854646, distance: 1.0016976519115908 entropy -8.282577569526811
epoch: 0, step: 71
	action: tensor([[-0.0618,  0.0425,  0.0602,  0.0477, -0.0189, -0.0287,  0.0279]],
       dtype=torch.float64)
	q_value: tensor([[-0.0432]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2240802239164148, distance: 1.0080106108988856 entropy -8.282804179654581
epoch: 0, step: 72
	action: tensor([[-0.0395,  0.0416,  0.0129,  0.0477, -0.0189, -0.0344,  0.0279]],
       dtype=torch.float64)
	q_value: tensor([[-0.0431]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2450374486932705, distance: 0.9943044843765771 entropy -8.280928549471366
epoch: 0, step: 73
	action: tensor([[-0.0273,  0.0489, -0.0012,  0.0156, -0.0189, -0.0299,  0.0279]],
       dtype=torch.float64)
	q_value: tensor([[-0.0428]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24930442472207925, distance: 0.9914906460015296 entropy -8.284146781307923
epoch: 0, step: 74
	action: tensor([[-0.0549,  0.0305,  0.0398,  0.0329, -0.0189, -0.0271,  0.0279]],
       dtype=torch.float64)
	q_value: tensor([[-0.0430]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21685717420952721, distance: 1.012691535947554 entropy -8.287466143848128
epoch: 0, step: 75
	action: tensor([[-0.0359,  0.0396,  0.0153,  0.0324, -0.0189, -0.0274,  0.0279]],
       dtype=torch.float64)
	q_value: tensor([[-0.0429]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24196825233966435, distance: 0.9963235384333339 entropy -8.282346863748481
epoch: 0, step: 76
	action: tensor([[-0.0734,  0.0506,  0.0228,  0.0410, -0.0189, -0.0382,  0.0279]],
       dtype=torch.float64)
	q_value: tensor([[-0.0429]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21219708677107374, distance: 1.015700074952492 entropy -8.285189273549141
epoch: 0, step: 77
	action: tensor([[-0.0320,  0.0446,  0.0253,  0.0093, -0.0189, -0.0395,  0.0278]],
       dtype=torch.float64)
	q_value: tensor([[-0.0433]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23654205983972643, distance: 0.9998831538257889 entropy -8.28224926103137
epoch: 0, step: 78
	action: tensor([[-0.0313,  0.0432,  0.0502,  0.0448, -0.0189, -0.0316,  0.0279]],
       dtype=torch.float64)
	q_value: tensor([[-0.0427]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2556902411369564, distance: 0.9872645657126433 entropy -8.285476480614816
epoch: 0, step: 79
	action: tensor([[-0.0304,  0.0430,  0.0481,  0.0555, -0.0189, -0.0309,  0.0279]],
       dtype=torch.float64)
	q_value: tensor([[-0.0428]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26174011711182843, distance: 0.9832440522612215 entropy -8.283334735703924
epoch: 0, step: 80
	action: tensor([[-0.0439,  0.0322, -0.0124,  0.0536, -0.0189, -0.0418,  0.0279]],
       dtype=torch.float64)
	q_value: tensor([[-0.0427]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23400671427557918, distance: 1.0015420193832294 entropy -8.283097512929112
epoch: 0, step: 81
	action: tensor([[-0.0446,  0.0533,  0.0242,  0.0512, -0.0189, -0.0285,  0.0278]],
       dtype=torch.float64)
	q_value: tensor([[-0.0427]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25094609121472367, distance: 0.9904059267852098 entropy -8.284017370910975
epoch: 0, step: 82
	action: tensor([[-0.0723,  0.0554,  0.0661,  0.0272, -0.0189, -0.0309,  0.0279]],
       dtype=torch.float64)
	q_value: tensor([[-0.0431]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21140775870377193, distance: 1.0162087807760127 entropy -8.284345660706137
epoch: 0, step: 83
	action: tensor([[-0.0404,  0.0471,  0.0162,  0.0464, -0.0189, -0.0304,  0.0279]],
       dtype=torch.float64)
	q_value: tensor([[-0.0434]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24822841317372857, distance: 0.9922009694662062 entropy -8.281773878703882
epoch: 0, step: 84
	action: tensor([[-0.0627,  0.0466, -0.0198,  0.0593, -0.0189, -0.0332,  0.0279]],
       dtype=torch.float64)
	q_value: tensor([[-0.0430]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22852890980770413, distance: 1.0051167751845753 entropy -8.28464742816666
epoch: 0, step: 85
	action: tensor([[-0.0558,  0.0432,  0.0759,  0.0593, -0.0189, -0.0381,  0.0278]],
       dtype=torch.float64)
	q_value: tensor([[-0.0431]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23552374940141407, distance: 1.0005497603970073 entropy -8.284002102988591
epoch: 0, step: 86
	action: tensor([[-0.0323,  0.0540, -0.0249,  0.0374, -0.0189, -0.0278,  0.0280]],
       dtype=torch.float64)
	q_value: tensor([[-0.0429]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2569551924839958, distance: 0.9864252829549885 entropy -8.279768009981865
epoch: 0, step: 87
	action: tensor([[-0.0766,  0.0327,  0.0082,  0.0411, -0.0189, -0.0358,  0.0278]],
       dtype=torch.float64)
	q_value: tensor([[-0.0430]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19738562307594154, distance: 1.0252036900345192 entropy -8.287548167809053
epoch: 0, step: 88
	action: tensor([[-0.0363,  0.0438,  0.0181,  0.0239, -0.0189, -0.0279,  0.0278]],
       dtype=torch.float64)
	q_value: tensor([[-0.0432]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24050145660546896, distance: 0.9972870184246968 entropy -8.281793036106219
epoch: 0, step: 89
	action: tensor([[-0.0334,  0.0466, -0.0234,  0.0220, -0.0189, -0.0280,  0.0279]],
       dtype=torch.float64)
	q_value: tensor([[-0.0429]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24383656103631024, distance: 0.9950949696311688 entropy -8.28559662775739
epoch: 0, step: 90
	action: tensor([[-0.0667,  0.0402,  0.0395,  0.0504, -0.0189, -0.0292,  0.0278]],
       dtype=torch.float64)
	q_value: tensor([[-0.0432]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2194231899417779, distance: 1.0110311015888547 entropy -8.287835652819492
epoch: 0, step: 91
	action: tensor([[-0.0304,  0.0476,  0.0344,  0.0538, -0.0189, -0.0175,  0.0279]],
       dtype=torch.float64)
	q_value: tensor([[-0.0432]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.266642222956612, distance: 0.9799742048503707 entropy -8.281696729763636
epoch: 0, step: 92
	action: tensor([[-0.0622,  0.0437,  0.0110,  0.0430, -0.0189, -0.0253,  0.0279]],
       dtype=torch.float64)
	q_value: tensor([[-0.0430]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2225858132557157, distance: 1.0089808511411682 entropy -8.285137973813773
epoch: 0, step: 93
	action: tensor([[-0.0534,  0.0420, -0.0426,  0.0213, -0.0189, -0.0221,  0.0278]],
       dtype=torch.float64)
	q_value: tensor([[-0.0432]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21951251323573118, distance: 1.010973252565654 entropy -8.283729297005946
epoch: 0, step: 94
	action: tensor([[-0.0358,  0.0429, -0.0573,  0.0587, -0.0189, -0.0417,  0.0278]],
       dtype=torch.float64)
	q_value: tensor([[-0.0436]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2506567175494038, distance: 0.9905972145654862 entropy -8.287360347760012
epoch: 0, step: 95
	action: tensor([[-0.0673,  0.0481,  0.0038,  0.0348, -0.0189, -0.0349,  0.0278]],
       dtype=torch.float64)
	q_value: tensor([[-0.0428]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21562767668321525, distance: 1.0134861633084697 entropy -8.286888863966826
epoch: 0, step: 96
	action: tensor([[-0.0612,  0.0395,  0.0335,  0.0512, -0.0189, -0.0325,  0.0278]],
       dtype=torch.float64)
	q_value: tensor([[-0.0433]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22378544589788651, distance: 1.0082020682915558 entropy -8.283744249165403
epoch: 0, step: 97
	action: tensor([[-0.0608,  0.0492, -0.0139,  0.0295, -0.0189, -0.0386,  0.0279]],
       dtype=torch.float64)
	q_value: tensor([[-0.0431]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21700073947609677, distance: 1.0125987086948403 entropy -8.28201481424001
epoch: 0, step: 98
	action: tensor([[-0.0434,  0.0465,  0.0393,  0.0284, -0.0189, -0.0296,  0.0278]],
       dtype=torch.float64)
	q_value: tensor([[-0.0434]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23697836266193573, distance: 0.9995974051523341 entropy -8.284833941490719
epoch: 0, step: 99
	action: tensor([[-0.0447,  0.0435,  0.0042,  0.0412, -0.0189, -0.0295,  0.0279]],
       dtype=torch.float64)
	q_value: tensor([[-0.0430]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2382810219073398, distance: 0.9987437655069725 entropy -8.284328076549238
epoch: 0, step: 100
	action: tensor([[-0.0470,  0.0460,  0.0621,  0.0030, -0.0189, -0.0407,  0.0278]],
       dtype=torch.float64)
	q_value: tensor([[-0.0430]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21814315113995508, distance: 1.0118597381157186 entropy -8.284876821215459
epoch: 0, step: 101
	action: tensor([[-0.0615,  0.0413, -0.0573,  0.0236, -0.0189, -0.0279,  0.0279]],
       dtype=torch.float64)
	q_value: tensor([[-0.0428]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20975546196326045, distance: 1.0172728286874722 entropy -8.283164591249351
epoch: 0, step: 102
	action: tensor([[-0.0485,  0.0446,  0.0169,  0.0235, -0.0189, -0.0259,  0.0277]],
       dtype=torch.float64)
	q_value: tensor([[-0.0438]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22793568501962325, distance: 1.0055031445596636 entropy -8.287064989967927
epoch: 0, step: 103
	action: tensor([[-0.0569,  0.0547,  0.0099,  0.0532, -0.0189, -0.0298,  0.0279]],
       dtype=torch.float64)
	q_value: tensor([[-0.0432]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23831124570027673, distance: 0.9987239510313778 entropy -8.285116743412292
epoch: 0, step: 104
	action: tensor([[-0.0445,  0.0358,  0.0098,  0.0364, -0.0189, -0.0349,  0.0278]],
       dtype=torch.float64)
	q_value: tensor([[-0.0432]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22988075327800428, distance: 1.0042357594696152 entropy -8.284094392262592
epoch: 0, step: 105
	action: tensor([[-0.0446,  0.0510, -0.0737,  0.0153, -0.0189, -0.0285,  0.0279]],
       dtype=torch.float64)
	q_value: tensor([[-0.0428]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23124338685719992, distance: 1.003346928800548 entropy -8.283986666812988
epoch: 0, step: 106
	action: tensor([[-0.0442,  0.0416,  0.0241,  0.0772, -0.0189, -0.0222,  0.0277]],
       dtype=torch.float64)
	q_value: tensor([[-0.0437]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25763847451702915, distance: 0.9859716348799401 entropy -8.289204133051792
epoch: 0, step: 107
	action: tensor([[-0.0297,  0.0528, -0.0233,  0.0387, -0.0189, -0.0196,  0.0279]],
       dtype=torch.float64)
	q_value: tensor([[-0.0430]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2613035100360517, distance: 0.9835347546740142 entropy -8.282889234482573
epoch: 0, step: 108
	action: tensor([[-0.0528,  0.0408,  0.0489,  0.0310, -0.0189, -0.0363,  0.0278]],
       dtype=torch.float64)
	q_value: tensor([[-0.0431]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22344791305783507, distance: 1.0084212501653935 entropy -8.288058221805437
epoch: 0, step: 109
	action: tensor([[-0.0289,  0.0453,  0.0390,  0.0377, -0.0189, -0.0312,  0.0279]],
       dtype=torch.float64)
	q_value: tensor([[-0.0429]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2560722184432256, distance: 0.9870112027474557 entropy -8.282243038107877
epoch: 0, step: 110
	action: tensor([[-0.0253,  0.0424,  0.0240,  0.0312, -0.0189, -0.0236,  0.0279]],
       dtype=torch.float64)
	q_value: tensor([[-0.0428]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2560616522315994, distance: 0.9870182121188443 entropy -8.284787704958422
epoch: 0, step: 111
	action: tensor([[-0.0812,  0.0572, -0.0454,  0.0334, -0.0189, -0.0226,  0.0279]],
       dtype=torch.float64)
	q_value: tensor([[-0.0428]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20834860792628274, distance: 1.018177939650282 entropy -8.28628068169488
epoch: 0, step: 112
	action: tensor([[-0.0388,  0.0508,  0.0603,  0.0763, -0.0189, -0.0346,  0.0277]],
       dtype=torch.float64)
	q_value: tensor([[-0.0442]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26794813610903456, distance: 0.9791012806933161 entropy -8.286207752886657
epoch: 0, step: 113
	action: tensor([[-0.0942,  0.0511,  0.0349,  0.0386, -0.0189, -0.0281,  0.0280]],
       dtype=torch.float64)
	q_value: tensor([[-0.0428]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1913816976263898, distance: 1.0290310438270915 entropy -8.281492558394552
epoch: 0, step: 114
	action: tensor([[-0.0545,  0.0474,  0.0375,  0.0612, -0.0189, -0.0323,  0.0278]],
       dtype=torch.float64)
	q_value: tensor([[-0.0437]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24032242060761744, distance: 0.9974045563429071 entropy -8.28167050249398
epoch: 0, step: 115
	action: tensor([[-0.0571,  0.0389, -0.0470,  0.0165, -0.0189, -0.0348,  0.0279]],
       dtype=torch.float64)
	q_value: tensor([[-0.0431]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2081072801226257, distance: 1.0183331190166336 entropy -8.28224112588397
epoch: 0, step: 116
	action: tensor([[-0.0434,  0.0423,  0.0686, -0.0020, -0.0189, -0.0513,  0.0278]],
       dtype=torch.float64)
	q_value: tensor([[-0.0436]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21463350385800972, distance: 1.014128244437295 entropy -8.286633979366057
epoch: 0, step: 117
	action: tensor([[-0.0822,  0.0461,  0.0128,  0.0378, -0.0189, -0.0367,  0.0280]],
       dtype=torch.float64)
	q_value: tensor([[-0.0425]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19717917811650942, distance: 1.0253355307616676 entropy -8.281892991936655
epoch: 0, step: 118
	action: tensor([[-0.0611,  0.0379, -0.0435,  0.0368, -0.0189, -0.0341,  0.0278]],
       dtype=torch.float64)
	q_value: tensor([[-0.0434]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21186278164783856, distance: 1.0159155588750293 entropy -8.282136122231792
epoch: 0, step: 119
	action: tensor([[-0.0457,  0.0489,  0.0420,  0.0508, -0.0189, -0.0239,  0.0278]],
       dtype=torch.float64)
	q_value: tensor([[-0.0434]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2482631266471037, distance: 0.9921780614844786 entropy -8.28519168116597
epoch: 0, step: 120
	action: tensor([[-0.0536,  0.0428,  0.0539,  0.0359, -0.0189, -0.0222,  0.0279]],
       dtype=torch.float64)
	q_value: tensor([[-0.0431]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2288654011896415, distance: 1.004897551143316 entropy -8.283320454683269
epoch: 0, step: 121
	action: tensor([[-0.0640,  0.0544,  0.0498,  0.0707, -0.0189, -0.0251,  0.0279]],
       dtype=torch.float64)
	q_value: tensor([[-0.0431]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.241480654038424, distance: 0.9966439257843374 entropy -8.282858747603765
epoch: 0, step: 122
	action: tensor([[-0.0638,  0.0543, -0.0057,  0.0291, -0.0189, -0.0234,  0.0279]],
       dtype=torch.float64)
	q_value: tensor([[-0.0433]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2202833274218493, distance: 1.0104739075814482 entropy -8.28153072666734
epoch: 0, step: 123
	action: tensor([[-0.0539,  0.0519, -0.0470,  0.0485, -0.0189, -0.0260,  0.0278]],
       dtype=torch.float64)
	q_value: tensor([[-0.0436]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.236283755361124, distance: 1.0000522872128552 entropy -8.285438655538753
epoch: 0, step: 124
	action: tensor([[-0.0860,  0.0445,  0.0217,  0.0368, -0.0189, -0.0274,  0.0278]],
       dtype=torch.float64)
	q_value: tensor([[-0.0434]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1946376689296877, distance: 1.0269572130027391 entropy -8.286877046677615
epoch: 0, step: 125
	action: tensor([[-0.0559,  0.0382,  0.0556,  0.0370, -0.0189, -0.0278,  0.0278]],
       dtype=torch.float64)
	q_value: tensor([[-0.0436]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22239805529213563, distance: 1.0091026862942063 entropy -8.282373264475021
epoch: 0, step: 126
	action: tensor([[-0.0392,  0.0483,  0.0308,  0.0531, -0.0189, -0.0349,  0.0279]],
       dtype=torch.float64)
	q_value: tensor([[-0.0430]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2531863310547925, distance: 0.9889237849735814 entropy -8.282010262303954
epoch: 0, step: 127
	action: tensor([[-0.0485,  0.0469,  0.0151,  0.0701, -0.0189, -0.0295,  0.0279]],
       dtype=torch.float64)
	q_value: tensor([[-0.0429]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25034641629679233, distance: 0.9908022952999107 entropy -8.283730860601148
LOSS epoch 0 actor 0.17885392231371897 critic 12.672261572123404 entropy 0.01
epoch: 1, step: 0
	action: tensor([[ 0.5227,  1.6371,  0.1028,  0.8649, -4.2368, -1.5631,  1.0252]],
       dtype=torch.float64)
	q_value: tensor([[0.0833]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6778855632167738
epoch: 1, step: 1
	action: tensor([[ 2.2396,  2.4550, -2.2991,  2.0469, -5.4097, -3.2907,  2.2960]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 2
	action: tensor([[ 1.8889,  2.9500, -0.6788,  1.0750, -6.2800, -2.7612,  2.6189]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 3
	action: tensor([[ 3.1465,  2.3252, -0.9912, -0.4983, -5.9913, -3.0810,  2.3672]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 4
	action: tensor([[ 2.7803,  1.7922, -2.5585,  1.9490, -6.2800, -2.3731,  2.5042]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 5
	action: tensor([[ 1.1541,  2.6653, -1.5276,  0.3195, -5.7681, -2.1449,  2.4692]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 6
	action: tensor([[ 0.3814,  3.5827, -1.1499,  2.6875, -5.7904, -3.0500,  2.3213]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 7
	action: tensor([[-0.0911,  2.3789, -2.3453,  3.3974, -6.2800, -3.2583,  2.3402]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 8
	action: tensor([[ 0.4942,  3.6196, -0.8790,  1.2956, -6.2800, -1.0941,  2.3461]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 9
	action: tensor([[-1.0644,  2.6973, -1.6895,  2.5908, -4.5877, -3.9832,  1.9193]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 10
	action: tensor([[ 1.3454,  2.2345, -2.6744,  2.2498, -6.2800, -1.7127,  2.3348]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 11
	action: tensor([[ 1.8817,  3.2843, -2.1779,  1.3372, -6.2800, -2.3204,  2.3408]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 12
	action: tensor([[ 3.8487e-03,  4.0205e+00, -1.2771e-02,  2.4815e+00, -6.2800e+00,
         -2.1701e+00,  2.2137e+00]], dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 13
	action: tensor([[ 2.1812,  1.1885, -1.7221,  2.7167, -6.2800, -2.6020,  2.4410]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 14
	action: tensor([[-0.7338,  2.0032, -0.8665,  1.9101, -6.2800, -2.5955,  2.5863]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 15
	action: tensor([[ 0.5110,  4.4067,  0.7828,  2.7337, -6.2800, -1.4596,  2.5284]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 16
	action: tensor([[ 0.0934,  2.4643, -1.8169,  1.7598, -6.1862, -1.7962,  2.0304]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 17
	action: tensor([[ 0.0104,  2.9967, -3.2019,  1.4821, -6.2800, -2.6292,  2.0265]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 18
	action: tensor([[ 1.8979,  4.9225, -3.7978,  2.0103, -6.1898, -1.7892,  2.0972]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 19
	action: tensor([[ 0.7195,  1.9321, -2.4841,  3.1196, -6.2800, -0.7817,  2.0397]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 20
	action: tensor([[-0.9719,  2.4226, -0.4401,  2.6962, -6.0325, -1.3922,  2.2828]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 21
	action: tensor([[ 0.3442,  2.9275, -1.9547,  0.8938, -6.2800, -3.0272,  2.1395]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 22
	action: tensor([[-0.5261,  2.7604, -2.8597,  1.0351, -6.2800, -1.2661,  2.5486]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 23
	action: tensor([[ 0.6661,  3.4653, -2.1424,  1.7384, -6.0731, -0.6013,  2.5855]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 24
	action: tensor([[ 1.0012,  1.8397, -3.3452,  0.8165, -6.2800, -2.7685,  2.3927]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 25
	action: tensor([[-0.2436,  3.5661, -2.5631,  3.2032, -6.2800, -1.7709,  2.5485]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 26
	action: tensor([[-0.4157,  3.2525, -2.6070,  1.8723, -6.2800, -1.6242,  2.4011]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 27
	action: tensor([[ 1.1325,  3.7270, -1.9285,  0.1979, -6.2800, -2.8883,  2.7132]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 28
	action: tensor([[ 0.3790,  3.7868, -1.3064,  1.5672, -6.2800, -1.2497,  2.0679]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 29
	action: tensor([[ 2.0818,  2.3527, -1.7762,  1.4255, -6.2800, -1.5572,  2.3823]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 30
	action: tensor([[ 0.8469,  4.1260, -0.7360,  0.3455, -4.9321, -2.8630,  2.3835]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 31
	action: tensor([[ 1.5443,  3.2882, -2.8110,  1.8020, -6.2800, -2.7729,  2.2528]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 32
	action: tensor([[ 1.8125,  3.3198, -1.3165,  3.1657, -6.2800, -3.1145,  2.4759]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 33
	action: tensor([[ 0.9703,  3.0483, -3.7401,  2.9450, -5.8862, -2.3695,  2.3165]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 34
	action: tensor([[ 0.1053,  3.2813, -1.5074,  0.2331, -5.5071, -0.7956,  1.9384]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 35
	action: tensor([[ 1.4524,  2.3140, -3.9367,  0.7938, -6.2404, -1.9508,  2.3364]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 36
	action: tensor([[ 3.5538,  2.0443, -2.4741,  0.3957, -5.2371, -0.7190,  2.3576]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 37
	action: tensor([[-1.5825,  3.5019,  0.5265,  1.3953, -5.9087, -2.8929,  2.2785]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 38
	action: tensor([[ 0.5182,  1.7582, -1.1609,  2.0938, -6.2800, -2.0367,  2.6240]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 39
	action: tensor([[ 0.5537,  2.8231, -2.6152,  2.1092, -6.2800, -3.2397,  2.0960]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 40
	action: tensor([[ 1.7592e+00,  1.4019e+00, -3.5433e-03,  1.2497e+00, -5.8596e+00,
         -3.5790e+00,  2.1659e+00]], dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 41
	action: tensor([[ 1.3560,  3.3017, -2.6052,  1.9180, -6.2800, -2.2532,  2.0658]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 42
	action: tensor([[ 1.3336,  2.9948, -1.6294,  2.0395, -6.2800, -2.6019,  2.1173]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 43
	action: tensor([[ 1.8653,  2.8245, -1.2408,  1.4914, -6.1560, -1.7452,  2.0494]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 44
	action: tensor([[ 1.2935,  1.7116, -1.1669,  1.2749, -6.2800, -0.6033,  2.2464]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 45
	action: tensor([[ 1.2893,  3.6383, -2.6109, -0.0640, -6.2800, -0.7483,  2.1706]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 46
	action: tensor([[ 2.1294,  2.6128, -0.8230,  1.4396, -6.1601, -1.3553,  2.2969]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 47
	action: tensor([[ 1.6133,  2.0677, -1.9081,  2.8445, -6.2800, -2.1342,  2.3060]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 48
	action: tensor([[ 1.7650,  4.0985, -2.9318,  1.6337, -6.2800, -1.4039,  1.7795]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 49
	action: tensor([[ 1.0657,  3.3460, -3.6227,  3.1421, -6.2800, -1.1929,  2.2144]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 50
	action: tensor([[ 1.3427,  2.7987, -0.5818,  1.5906, -5.7500, -3.6200,  2.2871]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 51
	action: tensor([[ 0.9943,  3.4151, -1.4620,  2.2322, -6.0416, -1.8791,  2.4586]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 52
	action: tensor([[ 3.0522,  3.3897, -2.2618,  0.9360, -4.6854, -3.8947,  2.2104]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 53
	action: tensor([[ 0.1018,  1.2774, -1.2521,  3.0269, -6.0652, -3.0305,  2.1718]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 54
	action: tensor([[-0.5864,  3.7993, -1.8224,  2.1322, -5.8437, -2.5176,  2.4736]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 55
	action: tensor([[ 0.5523,  4.0232, -1.9256,  2.5865, -6.2800, -1.4804,  1.9370]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 56
	action: tensor([[ 0.4973,  3.2050, -2.6066, -0.7804, -6.2360, -2.3029,  2.1007]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 57
	action: tensor([[-1.7500,  4.8118, -1.7357,  3.5142, -6.2800, -3.2190,  1.9953]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 58
	action: tensor([[ 1.7042,  1.5144, -1.4112,  2.6297, -6.1195, -2.2610,  2.6345]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 59
	action: tensor([[ 1.7325,  4.2160, -2.1021,  1.3631, -5.9438, -1.3298,  1.9171]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 60
	action: tensor([[ 0.8427,  4.5119, -0.9843,  3.3131, -6.2800, -2.9881,  2.0716]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 61
	action: tensor([[ 3.0433,  2.6765, -2.7845,  1.2610, -6.2800, -3.1437,  2.0219]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 62
	action: tensor([[ 0.9284,  1.3508, -2.8729,  2.6033, -6.2800, -1.5586,  2.0742]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 63
	action: tensor([[-0.0835,  4.3289, -1.2238,  0.8574, -6.2800, -1.1751,  2.1939]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 64
	action: tensor([[ 1.2534,  3.3873, -0.4079,  0.7332, -6.2800, -1.8593,  2.4973]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 65
	action: tensor([[-1.6846,  2.9865, -2.3798,  2.0950, -5.9480, -0.3765,  2.3613]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 66
	action: tensor([[ 0.2940,  1.4763, -0.5293,  2.6697, -6.2800, -3.2532,  2.4416]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 67
	action: tensor([[-0.5494,  3.0138, -0.1667,  1.2330, -5.5408, -3.3327,  2.1717]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 68
	action: tensor([[ 0.7397,  3.8882, -4.1721,  0.7659, -6.2800, -2.1445,  2.4537]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 69
	action: tensor([[ 2.0118,  3.0833, -1.6695,  3.1495, -5.9775, -0.8780,  2.5019]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 70
	action: tensor([[-0.5441,  2.0363, -3.3899,  2.4691, -6.2800, -1.8028,  2.1554]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 71
	action: tensor([[ 1.2443,  2.1624, -3.8636,  2.0104, -5.5377, -2.2553,  2.4601]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 72
	action: tensor([[-1.3776,  4.2202, -2.1852,  0.2472, -6.2800, -2.2162,  2.6402]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 73
	action: tensor([[ 0.0356,  0.9067, -2.0073,  0.5738, -6.0626, -1.2391,  2.2679]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 74
	action: tensor([[ 1.1560,  5.3021, -2.8083,  3.5716, -5.1367, -1.2502,  2.2783]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 75
	action: tensor([[ 1.0590,  2.4928, -0.6959,  2.2548, -5.3710, -0.4777,  2.0678]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 76
	action: tensor([[ 0.3401,  3.7087, -1.7847,  1.2618, -6.2800, -2.3869,  2.5215]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 77
	action: tensor([[ 0.9486,  2.2546, -1.5268,  1.0589, -6.2800, -2.7543,  2.1421]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 78
	action: tensor([[-0.3882,  2.3283, -1.6430,  1.8442, -5.7896, -1.7616,  2.2353]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 79
	action: tensor([[-0.0410,  3.3247, -2.1306,  0.6740, -5.7466, -2.1624,  2.1787]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 80
	action: tensor([[ 1.6068,  2.5110, -1.5367,  1.6452, -6.2800, -1.8268,  2.4194]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 81
	action: tensor([[-0.1982,  4.2425, -0.4919,  0.8172, -5.9682, -2.4956,  2.2808]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 82
	action: tensor([[ 1.4869,  3.1245, -1.6325,  2.5816, -6.2800, -1.9301,  2.1431]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 83
	action: tensor([[ 1.1407,  3.0900, -1.4131,  1.7527, -5.8412, -0.6122,  2.2083]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 84
	action: tensor([[ 0.9536,  2.8083, -2.0626,  1.3659, -6.2800, -2.8130,  2.3631]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 85
	action: tensor([[ 1.0059,  2.5127, -3.1661,  0.3790, -6.2800, -1.8144,  1.9656]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 86
	action: tensor([[ 0.3591,  1.6245, -3.1948,  0.4640, -5.8777, -2.9743,  1.8413]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 87
	action: tensor([[ 3.1946,  2.2085, -1.5809,  1.6721, -5.8662, -2.9998,  2.2538]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 88
	action: tensor([[ 1.0466,  4.2829, -0.0365,  2.1961, -6.2800, -1.3089,  2.2232]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 89
	action: tensor([[-0.3725,  3.6395, -0.8542,  1.2416, -5.1097, -1.9732,  2.3910]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 90
	action: tensor([[ 2.7409,  2.5145, -1.3992,  1.1232, -5.5696, -2.1191,  2.1181]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 91
	action: tensor([[ 1.0976,  2.7107, -2.4781,  1.8246, -6.2800, -0.0129,  2.3884]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 92
	action: tensor([[ 2.7388,  3.6305, -2.6694,  1.6624, -6.2800, -1.8689,  2.2146]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 93
	action: tensor([[ 1.5795,  1.1650, -1.4573,  1.7340, -6.1185, -4.1259,  2.3533]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 94
	action: tensor([[ 1.2898,  4.1252, -0.8627,  2.0068, -6.2800, -1.7224,  2.4099]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 95
	action: tensor([[ 2.0475,  2.5552, -1.5777,  1.3322, -6.2800, -2.3535,  2.2423]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 96
	action: tensor([[ 0.9977,  3.7631, -1.6731,  3.0787, -6.1824, -0.9614,  2.1562]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 97
	action: tensor([[-0.0411,  2.6958,  0.1424,  2.7418, -6.2800, -1.0857,  2.3726]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 98
	action: tensor([[ 0.7751,  3.1284, -2.4981,  0.1989, -5.4438, -0.8724,  2.4864]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 99
	action: tensor([[ 0.8650,  3.1129, -0.2232,  1.1661, -6.2800, -3.9006,  2.1369]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 100
	action: tensor([[ 0.7616,  2.2257,  0.3922,  0.6900, -4.9359,  0.1107,  2.0994]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 101
	action: tensor([[ 0.0992,  3.6586, -0.8562,  2.0513, -5.9175, -4.1520,  2.3940]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 102
	action: tensor([[ 0.3534,  3.1999, -2.0594,  3.1092, -6.2800, -1.1229,  2.2385]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 103
	action: tensor([[ 0.5924,  2.9610, -0.7961,  1.4396, -6.2800, -2.0601,  2.6658]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 104
	action: tensor([[ 0.4812,  3.3105, -3.0097,  3.0033, -6.2800, -0.7568,  2.1387]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 105
	action: tensor([[ 2.2214,  3.2634, -1.0931,  2.2887, -6.2800, -2.6199,  2.4697]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 106
	action: tensor([[ 0.5730,  3.8594, -3.0535,  2.7107, -6.2800, -2.4275,  1.9889]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 107
	action: tensor([[ 0.0714,  2.2774, -0.2018,  2.8684, -5.7865, -2.5505,  2.0431]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 108
	action: tensor([[ 2.6706,  1.9508, -3.4318,  2.1835, -5.7033, -1.6028,  2.5791]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 109
	action: tensor([[ 2.3207,  2.0944, -1.8224,  2.6027, -6.2800, -2.6949,  2.2159]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 110
	action: tensor([[ 1.5869,  2.9051, -2.7913,  0.8053, -6.2800, -1.9744,  2.1988]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 111
	action: tensor([[ 0.9500,  2.1226, -0.6900,  1.1801, -6.2800, -2.7839,  2.3272]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 112
	action: tensor([[ 0.6420,  2.4959, -1.3942,  0.3542, -5.7447, -3.1507,  2.3971]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 113
	action: tensor([[-1.0188,  1.2562, -3.6363,  2.6062, -6.2800,  0.0785,  2.5301]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 114
	action: tensor([[ 1.2580,  2.9272, -2.7293,  1.1214, -6.2800, -0.8043,  2.6336]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 115
	action: tensor([[ 1.2152,  3.3094, -1.3980, -0.0666, -6.2800, -1.7949,  2.3822]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 116
	action: tensor([[ 3.0426,  2.6356, -1.2260,  2.6626, -6.2800, -1.6625,  2.2503]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 117
	action: tensor([[ 2.4830,  2.4132, -1.4767,  1.5629, -6.2800, -0.6373,  2.3069]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 118
	action: tensor([[ 1.0252,  3.4568, -2.9969,  2.7225, -6.2800, -3.1424,  2.2780]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 119
	action: tensor([[ 1.2883,  3.3375, -2.0597,  1.4790, -5.8331, -1.3638,  2.2767]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 120
	action: tensor([[-0.2037,  2.2980, -1.0829,  1.5396, -5.8903, -2.5310,  2.3851]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 121
	action: tensor([[ 0.5289,  3.8689, -1.6305,  0.8165, -5.8718, -0.6455,  2.2348]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 122
	action: tensor([[ 1.6962,  3.6594, -2.4194,  2.8026, -5.9548, -2.3294,  2.0133]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 123
	action: tensor([[ 0.9417,  3.0474, -0.3702,  2.2627, -6.2800, -2.2727,  2.1555]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 124
	action: tensor([[ 2.0173,  3.9925, -2.1488,  1.7563, -6.2800, -3.1746,  2.4261]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 125
	action: tensor([[-0.4060,  3.9239, -3.1188,  2.6710, -5.5432, -0.5891,  2.1532]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 126
	action: tensor([[-0.1488,  3.4025, -1.5398,  1.5465, -5.8036, -1.3954,  2.0545]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
epoch: 1, step: 127
	action: tensor([[ 1.0016,  3.6198, -2.2539,  1.9305, -6.2800, -4.6771,  2.0824]],
       dtype=torch.float64)
	q_value: tensor([[0.2644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.096768022039045
LOSS epoch 1 actor 1153.8332319735448 critic 2526.3649032514413 entropy 100
epoch: 2, step: 0
	action: tensor([[ 6.0447,  6.1800, -2.1492,  4.3370, -4.9260, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 2, step: 1
	action: tensor([[ 5.0639,  5.0674, -1.4461,  4.8024, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 2, step: 2
	action: tensor([[ 6.1800,  6.1800, -2.4812,  6.1800, -5.2222, -5.9576,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 2, step: 3
	action: tensor([[ 6.1800,  6.1800, -2.4866,  5.3114, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 2, step: 4
	action: tensor([[ 6.1800,  6.1800, -2.2218,  4.4009, -6.2800, -5.6260,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 2, step: 5
	action: tensor([[ 6.1800,  4.2931,  0.0852,  5.3252, -5.7649, -6.2800,  4.3059]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 2, step: 6
	action: tensor([[ 5.5685,  6.1800, -1.6443,  6.1800, -6.2800, -5.8303,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 2, step: 7
	action: tensor([[ 6.1800,  6.1800, -2.3467,  6.1800, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 2, step: 8
	action: tensor([[ 6.1800,  5.1603, -0.3471,  6.1800, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 2, step: 9
	action: tensor([[ 6.1800,  6.1800, -1.3018,  6.1800, -4.5526, -5.0970,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 2, step: 10
	action: tensor([[ 6.1800,  6.0266, -1.6677,  5.8958, -6.2316, -6.2800,  5.8704]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 2, step: 11
	action: tensor([[ 5.8239,  5.4474,  0.2195,  6.1800, -6.2800, -5.0783,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 2, step: 12
	action: tensor([[ 5.8184,  5.4457, -1.8453,  4.9802, -5.4207, -5.9902,  5.2843]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 2, step: 13
	action: tensor([[ 6.1800,  5.9042, -1.6430,  5.1622, -6.2800, -5.3179,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 2, step: 14
	action: tensor([[ 5.7772,  6.1800, -3.1462,  5.8594, -5.8133, -4.4287,  5.7162]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 2, step: 15
	action: tensor([[ 5.8205,  5.7284, -4.1236,  4.1797, -6.2800, -5.6379,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 2, step: 16
	action: tensor([[ 5.9750,  6.1800, -1.5052,  6.1800, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2035425051552715 entropy 1.4189385332046724
epoch: 2, step: 17
	action: tensor([[ 6.1800,  6.1800, -1.3379,  5.1367, -5.4520, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 2, step: 18
	action: tensor([[ 6.1800,  6.1800, -0.9017,  4.9733, -6.2452, -6.2800,  4.9269]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 2, step: 19
	action: tensor([[ 4.6834,  6.1695, -2.6888,  5.3793, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 2, step: 20
	action: tensor([[ 6.1800,  6.1800, -3.5745,  5.5019, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 2, step: 21
	action: tensor([[ 6.1800,  4.8844, -1.9362,  6.1800, -4.7596, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 2, step: 22
	action: tensor([[ 4.6779,  6.1800, -2.2076,  5.0881, -5.8299, -3.4237,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 2, step: 23
	action: tensor([[ 6.1800,  6.1800, -2.6877,  5.1320, -4.9186, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 2, step: 24
	action: tensor([[ 6.1800,  6.1800, -0.8562,  6.1800, -6.2800, -3.9149,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 2, step: 25
	action: tensor([[ 5.4185,  6.1800, -2.9348,  6.1800, -6.2800, -6.2202,  5.4119]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 2, step: 26
	action: tensor([[ 5.1440,  5.8533, -3.0225,  6.1800, -5.9557, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.936550774801619 entropy 1.4189385332046724
epoch: 2, step: 27
	action: tensor([[ 5.9955,  6.0413, -1.1945,  6.1800, -5.4737, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 2, step: 28
	action: tensor([[ 3.8915,  5.4630, -2.4339,  4.8399, -6.2800, -5.5315,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 2, step: 29
	action: tensor([[ 6.1661,  5.6801,  0.5649,  6.1800, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 2, step: 30
	action: tensor([[ 6.1800,  5.0853, -1.4749,  5.3620, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 2, step: 31
	action: tensor([[ 6.1800,  6.1800, -1.6049,  6.1800, -4.3938, -5.5561,  5.6632]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 2, step: 32
	action: tensor([[ 6.1800,  6.1800, -1.6116,  5.1243, -6.2800, -5.4448,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 2, step: 33
	action: tensor([[ 4.9463,  6.1800, -1.4539,  6.1800, -5.5289, -5.3268,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 2, step: 34
	action: tensor([[ 6.1800,  4.0079, -0.1339,  6.1800, -6.2800, -6.2800,  5.8028]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 2, step: 35
	action: tensor([[ 6.1800,  6.1800, -2.1994,  6.0960, -6.2800, -5.6027,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.136731991104666 entropy 1.4189385332046724
epoch: 2, step: 36
	action: tensor([[ 6.1800,  6.1800, -3.7751,  6.1800, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 2, step: 37
	action: tensor([[ 6.1800,  5.9348, -2.7106,  5.9531, -5.7810, -4.9874,  5.5315]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 2, step: 38
	action: tensor([[ 6.1800,  5.1080, -0.2659,  6.1800, -6.2800, -4.9912,  5.1770]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 2, step: 39
	action: tensor([[ 5.1615,  6.1800, -2.9660,  5.0821, -6.2800, -6.1130,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 2, step: 40
	action: tensor([[ 5.5125,  4.4428, -0.8304,  5.0759, -5.8986, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 2, step: 41
	action: tensor([[ 5.9076,  6.1800, -2.1261,  6.0317, -6.2800, -4.7301,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 2, step: 42
	action: tensor([[ 5.4954,  6.1800, -0.4559,  6.1800, -6.2800, -5.4153,  4.5965]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 2, step: 43
	action: tensor([[ 6.1800,  5.8434, -2.0444,  6.1800, -5.8943, -5.5139,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 2, step: 44
	action: tensor([[ 6.1800,  6.1800, -2.3705,  5.7759, -4.8676, -5.5777,  5.8611]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 2, step: 45
	action: tensor([[ 6.1800,  5.8344, -1.9500,  6.1800, -4.6611, -5.9252,  5.3365]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 2, step: 46
	action: tensor([[ 5.9240,  6.1800, -1.0661,  6.0850, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 2, step: 47
	action: tensor([[ 6.1289,  6.1800, -1.6581,  6.1800, -6.2800, -4.7674,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 2, step: 48
	action: tensor([[ 5.3233,  6.1800, -0.3101,  6.1800, -6.2800, -6.2800,  4.5426]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 2, step: 49
	action: tensor([[ 4.5854,  6.1800, -1.2130,  5.8361, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 2, step: 50
	action: tensor([[ 6.1800,  5.6946, -2.7565,  6.1800, -5.0785, -5.7360,  5.7587]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 2, step: 51
	action: tensor([[ 5.3642,  6.1800, -0.7975,  6.1800, -5.7578, -6.2800,  4.6977]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 2, step: 52
	action: tensor([[ 4.5772,  6.1800,  0.1311,  5.5526, -5.3500, -6.1936,  6.1527]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 2, step: 53
	action: tensor([[ 6.1800,  6.1800, -2.0646,  5.3470, -6.1293, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 2, step: 54
	action: tensor([[ 4.1721,  5.6614, -1.8225,  6.1800, -6.2800, -5.1324,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 2, step: 55
	action: tensor([[ 6.1736,  5.5173, -2.0774,  5.6792, -6.2676, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 2, step: 56
	action: tensor([[ 5.2805,  5.6324, -1.0359,  5.1054, -5.5108, -5.4452,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 2, step: 57
	action: tensor([[ 6.1800,  6.1800, -2.2567,  6.0384, -6.2800, -6.2068,  5.4777]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0415532014815831 entropy 1.4189385332046724
epoch: 2, step: 58
	action: tensor([[ 6.1800,  6.1800, -2.2430,  6.1800, -6.2800, -6.2800,  6.0827]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0476690355440847 entropy 1.4189385332046724
epoch: 2, step: 59
	action: tensor([[ 5.4833,  6.1800, -1.0971,  6.1800, -5.0983, -2.8815,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 2, step: 60
	action: tensor([[ 5.4143,  5.9257, -2.5063,  3.9884, -6.2800, -4.6053,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 2, step: 61
	action: tensor([[ 6.1800,  6.1800, -2.0592,  6.1800, -4.7366, -6.2800,  5.3135]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 2, step: 62
	action: tensor([[ 5.5116,  4.8863, -0.4044,  6.1800, -6.0513, -6.2800,  5.5519]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 2, step: 63
	action: tensor([[ 6.1800,  5.2110, -2.5655,  4.6752, -6.2800, -6.2800,  5.6566]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 2, step: 64
	action: tensor([[ 5.3876,  5.1945, -2.1655,  6.1800, -6.2374, -5.3536,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 2, step: 65
	action: tensor([[ 5.6295,  6.1800, -3.3122,  4.9748, -6.2800, -5.3314,  5.8135]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 2, step: 66
	action: tensor([[ 5.3725e+00,  5.8494e+00,  4.1884e-03,  6.1800e+00, -4.6573e+00,
         -5.1920e+00,  6.1800e+00]], dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 2, step: 67
	action: tensor([[ 6.1800,  6.1800, -0.6803,  6.1800, -4.7839, -6.1987,  4.7798]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0690178309747649 entropy 1.4189385332046724
epoch: 2, step: 68
	action: tensor([[ 5.9735,  5.7257, -1.6264,  6.1800, -6.2800, -6.2800,  5.1593]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 2, step: 69
	action: tensor([[ 5.1678,  6.1800, -0.4017,  5.5271, -6.2800, -5.8353,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 2, step: 70
	action: tensor([[ 6.1800,  6.1800, -1.0334,  6.1800, -5.4865, -5.4554,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 2, step: 71
	action: tensor([[ 6.1800,  6.1800, -1.0649,  5.4792, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.923405237875089 entropy 1.4189385332046724
epoch: 2, step: 72
	action: tensor([[ 6.1800,  5.4854, -1.4931,  4.8491, -5.9531, -4.6312,  5.2669]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 2, step: 73
	action: tensor([[ 5.8515,  5.2512, -1.3491,  5.8237, -6.1207, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 2, step: 74
	action: tensor([[ 4.0805,  6.1800, -2.3706,  4.6758, -6.2800, -6.2800,  4.7447]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 2, step: 75
	action: tensor([[ 4.4386,  5.5308, -2.1901,  6.1800, -4.3108, -4.5891,  4.6157]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 2, step: 76
	action: tensor([[ 6.1800,  6.1634, -1.5144,  6.1740, -6.1968, -5.7787,  5.7674]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 2, step: 77
	action: tensor([[ 5.7704,  5.9836, -3.4297,  6.1617, -5.3675, -6.2800,  5.5532]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 2, step: 78
	action: tensor([[ 5.5559,  6.1800, -0.4509,  5.1275, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 2, step: 79
	action: tensor([[ 6.1800,  6.0691, -1.2900,  6.1800, -5.8542, -5.2380,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 2, step: 80
	action: tensor([[ 5.6566,  5.0929, -1.1870,  5.3715, -4.1167, -6.0269,  4.5553]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 2, step: 81
	action: tensor([[ 5.5950,  6.1800, -3.0295,  5.1406, -6.2800, -4.2564,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 2, step: 82
	action: tensor([[ 6.1800,  6.1800, -2.1559,  6.1800, -5.7162, -6.2800,  5.2786]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 2, step: 83
	action: tensor([[ 4.4981,  6.1800,  0.2441,  6.0005, -5.2158, -5.6278,  4.2032]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 2, step: 84
	action: tensor([[ 6.1800,  6.1800, -0.9860,  6.1800, -6.2800, -6.2800,  5.7307]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 2, step: 85
	action: tensor([[ 5.3020,  6.1800, -0.1201,  5.4873, -4.8087, -6.2800,  5.7746]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 2, step: 86
	action: tensor([[ 6.1305,  6.0435, -2.2493,  5.5813, -6.2800, -5.3352,  4.4625]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 2, step: 87
	action: tensor([[ 6.1800,  5.0257, -1.7891,  4.5386, -6.2800, -5.6823,  4.9102]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 2, step: 88
	action: tensor([[ 5.9959,  6.1800, -3.7592,  6.1800, -6.2800, -5.1362,  5.8640]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1291980635128231 entropy 1.4189385332046724
epoch: 2, step: 89
	action: tensor([[ 5.5010,  6.1800, -1.3629,  4.0904, -6.2800, -6.2538,  5.1301]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 2, step: 90
	action: tensor([[ 6.1800,  6.0566, -3.5214,  6.1800, -6.2800, -5.0627,  3.8646]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 2, step: 91
	action: tensor([[ 6.1800,  6.1800, -1.6615,  3.8625, -6.2800, -6.2800,  5.9226]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 2, step: 92
	action: tensor([[ 6.1800,  6.1800, -1.3812,  6.1800, -6.1327, -5.3749,  5.8097]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 2, step: 93
	action: tensor([[ 6.1800,  6.1800, -0.8463,  6.1800, -4.6718, -6.2800,  4.8368]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 2, step: 94
	action: tensor([[ 5.6978,  6.1800, -1.6853,  5.5979, -5.6491, -6.2800,  6.1298]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 2, step: 95
	action: tensor([[ 6.1626,  6.1800, -3.2883,  3.7694, -6.2800, -5.8074,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 2, step: 96
	action: tensor([[ 4.8367,  4.4941, -2.1018,  5.5846, -6.2800, -6.2800,  5.1486]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 2, step: 97
	action: tensor([[ 6.1800,  6.1800,  0.1318,  5.2106, -6.2800, -5.9922,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 2, step: 98
	action: tensor([[ 6.1800,  6.0124, -2.9842,  4.4896, -6.2800, -4.3783,  6.1118]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 2, step: 99
	action: tensor([[ 5.4626,  6.1800, -1.4176,  6.1800, -6.2800, -5.6670,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.6425037138023455 entropy 1.4189385332046724
epoch: 2, step: 100
	action: tensor([[ 6.1800,  5.6759, -2.7742,  6.1800, -5.8447, -6.2800,  4.8682]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 2, step: 101
	action: tensor([[ 6.1800,  5.0069, -1.4428,  5.7105, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 2, step: 102
	action: tensor([[ 5.8997,  5.6675, -1.0707,  6.1800, -6.2800, -6.2800,  5.8974]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 2, step: 103
	action: tensor([[ 3.9680,  6.1800, -3.6016,  6.1800, -5.1409, -6.0498,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 2, step: 104
	action: tensor([[ 6.1800,  5.5958, -0.5360,  6.1800, -5.8132, -5.5633,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 2, step: 105
	action: tensor([[ 5.8496,  6.1800, -2.5969,  6.1800, -6.2800, -6.2800,  5.6485]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 2, step: 106
	action: tensor([[ 6.1800,  6.1800, -1.2439,  6.0064, -6.0017, -6.2800,  4.1542]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 2, step: 107
	action: tensor([[ 6.1800,  6.1800, -3.7285,  6.1800, -6.2800, -6.2800,  5.1113]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.098111629015699 entropy 1.4189385332046724
epoch: 2, step: 108
	action: tensor([[ 6.1800,  6.1800, -1.1183,  6.1800, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 2, step: 109
	action: tensor([[ 6.1143,  3.9964, -2.7981,  5.9182, -5.7254, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 2, step: 110
	action: tensor([[ 5.6191,  5.8252, -1.2187,  6.1800, -4.9360, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 2, step: 111
	action: tensor([[ 6.1800,  4.8918, -1.1864,  6.1800, -6.0958, -5.5954,  4.7206]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 2, step: 112
	action: tensor([[ 5.9068,  5.3984, -2.3514,  5.3226, -6.2800, -4.5619,  5.0885]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 2, step: 113
	action: tensor([[ 5.7681,  5.9986, -1.5133,  4.9278, -4.1988, -5.5990,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 2, step: 114
	action: tensor([[ 6.0565,  6.1800, -1.8917,  6.1800, -5.4367, -6.2800,  4.9527]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 2, step: 115
	action: tensor([[ 5.2500,  6.1800, -1.0621,  4.9470, -5.7651, -5.3773,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 2, step: 116
	action: tensor([[ 6.1745,  6.1650, -2.3094,  6.1371, -4.1342, -5.7615,  5.7086]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 2, step: 117
	action: tensor([[ 6.1800,  5.2865, -1.9349,  6.1800, -6.2800, -3.2159,  4.6515]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 2, step: 118
	action: tensor([[ 4.4404,  6.1800, -1.4056,  6.1002, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 2, step: 119
	action: tensor([[ 5.9758,  5.9326, -3.0739,  5.0363, -5.6547, -6.2800,  5.8724]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 2, step: 120
	action: tensor([[ 5.7811,  6.1800, -1.4963,  5.5747, -6.0970, -6.2497,  5.8483]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 2, step: 121
	action: tensor([[ 5.5963,  6.1800, -1.2837,  6.1800, -4.9028, -6.2800,  6.1302]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 2, step: 122
	action: tensor([[ 6.1800,  4.6306, -2.5937,  5.6940, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 2, step: 123
	action: tensor([[ 6.0025,  6.1800, -1.9244,  5.8842, -6.1692, -6.1418,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 2, step: 124
	action: tensor([[ 6.1800,  6.1800, -1.3433,  6.1800, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 2, step: 125
	action: tensor([[ 6.1800,  6.1800, -2.5799,  6.1800, -6.2800, -3.5838,  5.6973]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 2, step: 126
	action: tensor([[ 6.1800,  6.1800, -0.2968,  5.4490, -5.9137, -4.9705,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 2, step: 127
	action: tensor([[ 5.6087,  6.1800,  0.0266,  6.1800, -6.2380, -5.8120,  5.3015]],
       dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
LOSS epoch 2 actor 1109.9370927958855 critic 2503.6620073139306 entropy 100
epoch: 3, step: 0
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.0390, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 3, step: 1
	action: tensor([[ 5.1727,  5.3414,  5.0362,  6.0203, -5.4844, -5.3471,  5.7755]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 3, step: 2
	action: tensor([[ 4.4759,  6.1800,  5.8244,  6.1800, -5.3468, -5.6172,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 3, step: 3
	action: tensor([[ 6.1800,  5.3221,  6.1800,  5.3512, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 3, step: 4
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -6.2800, -5.0358,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8891390607668808 entropy 1.4189385332046724
epoch: 3, step: 5
	action: tensor([[ 5.3771,  6.1800,  6.1800,  5.9822, -6.0593, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 3, step: 6
	action: tensor([[ 6.1800,  6.1800,  5.7312,  6.1800, -5.8509, -4.5855,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 3, step: 7
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -6.2800, -4.6024,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 3, step: 8
	action: tensor([[ 5.4603,  6.1800,  6.0577,  6.0609, -5.5635, -6.1959,  5.9871]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 3, step: 9
	action: tensor([[ 6.1800,  6.1800,  6.1800,  5.8911, -4.6075, -6.2438,  6.1137]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 3, step: 10
	action: tensor([[ 5.9571,  6.1800,  6.1800,  5.6238, -6.2800, -5.5884,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 3, step: 11
	action: tensor([[ 6.1800,  6.1800,  5.8455,  6.1800, -3.3282, -5.8625,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 3, step: 12
	action: tensor([[ 6.1800,  5.3841,  6.1800,  6.1800, -5.0790, -6.2800,  5.3483]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 3, step: 13
	action: tensor([[ 6.1800,  6.0926,  6.1800,  4.9218, -6.2796, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 3, step: 14
	action: tensor([[ 6.1800,  5.7666,  5.6477,  6.1800, -6.2800, -5.9516,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.5247718135045443 entropy 1.4189385332046724
epoch: 3, step: 15
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -6.2800, -6.2800,  5.8503]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 3, step: 16
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -6.2525, -6.2800,  6.0247]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 3, step: 17
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -6.2800, -6.1454,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 3, step: 18
	action: tensor([[ 6.1800,  6.1800,  5.1115,  4.6686, -6.2800, -5.2704,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 3, step: 19
	action: tensor([[ 6.1800,  6.1800,  4.9978,  6.1800, -6.2800, -5.5348,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 3, step: 20
	action: tensor([[ 6.1800,  6.1800,  6.1800,  5.8934, -6.0102, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 3, step: 21
	action: tensor([[ 6.1800,  5.6126,  5.1307,  6.1800, -5.2873, -5.7371,  5.6010]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 3, step: 22
	action: tensor([[ 6.1800,  6.1800,  6.1800,  5.9760, -6.2800, -5.5829,  5.7914]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 3, step: 23
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -6.2800, -5.9538,  4.8081]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 3, step: 24
	action: tensor([[ 6.1800,  5.4234,  6.1800,  5.7026, -6.2800, -4.4100,  5.1606]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 3, step: 25
	action: tensor([[ 6.1800,  5.6979,  6.1800,  5.1156, -5.6078, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 3, step: 26
	action: tensor([[ 3.7816,  6.1800,  6.1800,  6.1800, -6.0320, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 3, step: 27
	action: tensor([[ 6.1800,  6.1800,  5.9992,  5.4104, -6.2800, -5.0226,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 3, step: 28
	action: tensor([[ 6.1800,  5.1751,  6.1800,  6.1800, -6.2118, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 3, step: 29
	action: tensor([[ 6.1800,  5.6460,  5.5146,  5.9579, -6.2800, -5.9936,  5.8184]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 3, step: 30
	action: tensor([[ 5.4015,  6.1800,  5.3259,  6.1800, -5.9961, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 3, step: 31
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -5.6195, -5.1916,  5.1482]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 3, step: 32
	action: tensor([[ 6.1800,  4.2288,  6.1493,  5.6103, -6.2800, -5.1542,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 3, step: 33
	action: tensor([[ 5.0408,  5.8947,  5.6605,  6.1800, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 3, step: 34
	action: tensor([[ 6.0108,  6.1800,  5.7686,  4.9692, -6.2800, -5.8275,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 3, step: 35
	action: tensor([[ 6.1800,  3.9068,  5.5576,  6.1800, -5.9462, -6.1849,  5.3478]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 3, step: 36
	action: tensor([[ 6.1800,  5.5694,  5.6607,  5.4196, -6.2800, -4.8914,  5.1782]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 3, step: 37
	action: tensor([[ 6.1800,  6.1800,  6.1800,  4.0507, -5.2030, -6.2800,  5.6648]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 3, step: 38
	action: tensor([[ 6.1800,  6.1800,  5.1079,  6.1800, -6.2800, -6.2800,  6.0485]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0491804243777838 entropy 1.4189385332046724
epoch: 3, step: 39
	action: tensor([[ 5.3305,  6.1800,  5.5138,  6.1800, -5.3825, -5.7359,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 3, step: 40
	action: tensor([[ 6.1800,  5.1134,  6.1800,  5.9383, -6.2800, -6.2800,  4.9395]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 3, step: 41
	action: tensor([[ 6.1800,  5.2418,  5.2466,  5.5224, -5.8829, -5.1123,  5.8610]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 3, step: 42
	action: tensor([[ 5.8766,  6.0936,  6.1800,  6.1800, -5.9357, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 3, step: 43
	action: tensor([[ 5.9215,  5.3936,  5.1461,  6.1800, -6.2800, -6.2800,  4.6134]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 3, step: 44
	action: tensor([[ 6.1800,  6.1800,  5.1279,  6.1800, -6.2800, -5.2069,  5.9310]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 3, step: 45
	action: tensor([[ 6.1800,  6.1800,  6.1800,  5.5977, -6.2800, -4.6702,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 3, step: 46
	action: tensor([[ 6.1800,  5.7502,  5.1388,  6.1277, -5.4487, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 3, step: 47
	action: tensor([[ 6.1800,  4.2526,  5.5243,  6.1181, -4.8779, -6.0842,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 3, step: 48
	action: tensor([[ 6.1800,  4.9082,  5.1791,  6.0706, -5.9198, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 3, step: 49
	action: tensor([[ 4.7371,  6.1800,  5.4148,  6.1800, -6.2800, -6.2800,  4.7648]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 3, step: 50
	action: tensor([[ 6.1800,  6.1800,  5.0320,  3.6923, -6.2800, -5.7542,  5.2800]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 3, step: 51
	action: tensor([[ 6.1800,  4.9001,  4.5335,  5.7383, -6.2800, -5.2967,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 3, step: 52
	action: tensor([[ 6.1800,  6.1800,  6.1800,  5.9086, -6.2800, -5.7542,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 3, step: 53
	action: tensor([[ 4.6275,  4.0772,  6.1800,  3.9235, -5.9580, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 3, step: 54
	action: tensor([[ 4.7633,  6.1800,  5.4149,  5.9077, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 3, step: 55
	action: tensor([[ 4.9569,  6.1800,  5.7151,  6.1800, -6.2800, -5.8556,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 3, step: 56
	action: tensor([[ 5.5451,  6.1800,  6.1800,  6.1800, -6.2800, -4.8803,  5.4451]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 3, step: 57
	action: tensor([[ 4.9914,  5.6347,  6.1800,  6.1800, -5.9328, -6.2800,  5.3676]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 3, step: 58
	action: tensor([[ 5.0970,  6.1800,  5.6909,  6.1800, -5.6692, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 3, step: 59
	action: tensor([[ 6.1800,  5.2891,  6.1800,  6.1800, -6.2537, -5.4713,  4.7805]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 3, step: 60
	action: tensor([[ 5.2239,  6.1800,  6.1800,  6.1323, -5.6288, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 3, step: 61
	action: tensor([[ 6.1800,  4.9857,  6.1800,  6.1800, -6.2800, -4.3909,  5.6712]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 3, step: 62
	action: tensor([[ 5.6178,  5.8360,  4.9357,  6.1800, -4.4052, -5.5306,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 3, step: 63
	action: tensor([[ 6.1800,  5.2121,  6.1800,  5.5478, -6.2800, -6.0694,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 3, step: 64
	action: tensor([[ 6.1800,  6.0661,  6.1800,  5.2724, -6.1578, -5.6315,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 3, step: 65
	action: tensor([[ 6.1800,  5.5373,  5.5709,  4.9810, -5.5441, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 3, step: 66
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -6.2800, -6.2800,  5.5075]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.137547057785543 entropy 1.4189385332046724
epoch: 3, step: 67
	action: tensor([[ 6.1800,  4.1831,  5.6845,  5.0757, -4.2885, -6.2800,  5.7694]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 3, step: 68
	action: tensor([[ 4.6490,  5.2817,  6.1800,  5.9573, -3.8830, -6.2800,  5.3867]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 3, step: 69
	action: tensor([[ 6.1800,  5.5260,  6.1800,  6.0626, -5.6087, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 3, step: 70
	action: tensor([[ 5.5291,  6.1800,  5.3412,  6.1800, -6.2800, -4.7862,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 3, step: 71
	action: tensor([[ 5.6402,  5.4508,  6.0590,  6.1800, -5.8048, -6.0956,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 3, step: 72
	action: tensor([[ 6.1800,  6.1800,  6.1800,  5.9861, -5.8270, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 3, step: 73
	action: tensor([[ 5.3587,  6.1800,  6.1800,  6.1800, -6.2800, -5.9193,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 3, step: 74
	action: tensor([[ 5.3814,  6.1800,  6.1800,  6.1800, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 3, step: 75
	action: tensor([[ 5.4187,  6.1800,  6.1800,  5.4839, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 3, step: 76
	action: tensor([[ 6.1800,  4.3835,  5.2060,  5.5641, -5.6303, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 3, step: 77
	action: tensor([[ 6.1800,  6.0377,  6.1800,  6.0139, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 3, step: 78
	action: tensor([[ 5.8795,  5.1177,  6.1800,  6.1800, -6.2800, -4.9983,  5.5020]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 3, step: 79
	action: tensor([[ 6.1800,  6.1800,  6.1800,  5.9982, -6.2800, -5.0482,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 3, step: 80
	action: tensor([[ 6.1800,  5.3133,  6.1800,  6.1800, -6.2773, -5.7113,  5.0448]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 3, step: 81
	action: tensor([[ 6.1800,  6.0101,  5.8782,  6.1800, -6.2800, -6.2800,  6.0908]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 3, step: 82
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -5.4756, -6.2800,  5.9935]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1245606767713263 entropy 1.4189385332046724
epoch: 3, step: 83
	action: tensor([[ 6.1800,  6.1800,  5.1221,  6.1800, -6.2800, -6.2800,  4.7592]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 3, step: 84
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -6.2800, -3.7863,  5.8426]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 3, step: 85
	action: tensor([[ 5.7943,  6.1800,  6.1800,  4.8064, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 3, step: 86
	action: tensor([[ 5.9677,  5.0564,  5.9291,  5.5770, -6.0087, -5.6652,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 3, step: 87
	action: tensor([[ 6.1800,  6.1800,  6.1800,  3.2726, -5.8546, -5.6185,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 3, step: 88
	action: tensor([[ 5.3950,  5.9476,  6.1800,  6.1800, -6.2800, -6.0880,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 3, step: 89
	action: tensor([[ 6.1800,  6.1800,  5.7014,  5.7269, -6.0330, -5.6662,  5.8568]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 3, step: 90
	action: tensor([[ 6.1800,  6.1800,  5.8647,  6.1800, -5.2416, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 3, step: 91
	action: tensor([[ 5.8607,  6.1800,  5.1548,  5.8029, -6.2800, -6.2800,  4.7097]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.169177813902052 entropy 1.4189385332046724
epoch: 3, step: 92
	action: tensor([[ 5.3483,  6.1800,  4.8557,  4.8877, -5.6324, -5.8663,  4.5353]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 3, step: 93
	action: tensor([[ 5.1229,  5.9670,  5.2538,  6.1800, -6.2800, -5.7683,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 3, step: 94
	action: tensor([[ 6.1800,  6.1624,  6.1800,  4.7857, -5.7554, -6.2162,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 3, step: 95
	action: tensor([[ 5.9968,  5.3328,  5.3744,  5.1990, -6.2800, -5.6053,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 3, step: 96
	action: tensor([[ 6.1800,  5.9534,  6.1800,  5.8359, -6.2800, -6.2800,  5.4630]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 3, step: 97
	action: tensor([[ 5.4016,  6.1800,  5.8019,  5.2163, -6.2800, -3.7093,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 3, step: 98
	action: tensor([[ 5.6651,  6.1800,  6.1800,  6.0702, -6.2800, -5.6320,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 3, step: 99
	action: tensor([[ 6.1800,  6.1800,  5.5035,  4.2414, -6.2800, -6.2800,  5.6755]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 3, step: 100
	action: tensor([[ 6.1800,  5.1298,  5.7956,  4.9300, -4.3828, -6.2800,  5.3793]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 3, step: 101
	action: tensor([[ 6.1800,  4.8138,  6.1652,  5.6495, -6.2800, -4.9739,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 3, step: 102
	action: tensor([[ 6.1800,  6.1800,  5.6754,  6.1800, -6.2800, -6.2287,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 3, step: 103
	action: tensor([[ 4.5628,  6.0299,  5.2802,  5.8678, -6.2133, -6.1134,  5.3779]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 3, step: 104
	action: tensor([[ 6.1800,  6.0664,  6.1800,  6.1800, -5.5259, -6.2800,  4.8452]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 3, step: 105
	action: tensor([[ 3.6273,  5.6266,  6.1800,  4.7572, -5.3256, -4.6279,  6.0952]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 3, step: 106
	action: tensor([[ 5.7775,  5.9839,  6.1800,  4.5796, -6.2800, -5.6691,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 3, step: 107
	action: tensor([[ 6.1800,  6.0156,  5.2539,  5.2432, -6.2800, -5.9241,  5.2707]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 3, step: 108
	action: tensor([[ 5.8595,  6.1800,  3.3792,  6.1800, -6.2800, -6.0519,  5.0606]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 3, step: 109
	action: tensor([[ 6.1800,  6.1800,  5.8488,  6.1800, -4.5437, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3396300316848977 entropy 1.4189385332046724
epoch: 3, step: 110
	action: tensor([[ 6.1800,  6.1800,  5.1380,  5.9308, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 3, step: 111
	action: tensor([[ 6.1498,  5.6763,  6.1800,  5.5974, -5.1866, -6.2800,  5.8830]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.6085971678102666 entropy 1.4189385332046724
epoch: 3, step: 112
	action: tensor([[ 6.1800,  6.1800,  6.1800,  5.6684, -6.0609, -4.7575,  5.1412]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 3, step: 113
	action: tensor([[ 5.2469,  6.1800,  6.1800,  5.9195, -6.2800, -5.8761,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 3, step: 114
	action: tensor([[ 6.1800,  6.1800,  5.4907,  4.3168, -6.2800, -3.3985,  4.8644]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 3, step: 115
	action: tensor([[ 6.1436,  5.7337,  5.2513,  6.1800, -5.9741, -6.2594,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 3, step: 116
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -5.0427, -6.2800,  5.4521]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 3, step: 117
	action: tensor([[ 6.1800,  5.0414,  6.1800,  5.2599, -6.2800, -5.0834,  5.7707]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 3, step: 118
	action: tensor([[ 2.9822,  5.6034,  5.1145,  6.1800, -6.2800, -3.3805,  5.5210]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 3, step: 119
	action: tensor([[ 5.6464,  4.8099,  6.1800,  4.5816, -4.8449, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 3, step: 120
	action: tensor([[ 4.6993,  6.1800,  6.1800,  5.8047, -5.3451, -6.2800,  5.7644]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 3, step: 121
	action: tensor([[ 4.8167,  5.3836,  5.2702,  6.1800, -6.2800, -6.0344,  4.8265]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 3, step: 122
	action: tensor([[ 5.8729,  6.1800,  6.1800,  5.1570, -4.2856, -5.7183,  5.8739]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 3, step: 123
	action: tensor([[ 6.1800,  6.0583,  5.7440,  5.6872, -5.0905, -6.2703,  5.6989]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1649436969763125 entropy 1.4189385332046724
epoch: 3, step: 124
	action: tensor([[ 6.1800,  6.0360,  6.0426,  6.0603, -4.0377, -5.1185,  5.7037]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 3, step: 125
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -6.2800, -6.2032,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 3, step: 126
	action: tensor([[ 5.4986,  6.1800,  5.8232,  5.3360, -6.2800, -6.1054,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 3, step: 127
	action: tensor([[ 4.1029,  5.7356,  5.6474,  6.1800, -5.3450, -5.2506,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.1346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
LOSS epoch 3 actor 1101.385766144432 critic 2486.559107323132 entropy 100
epoch: 4, step: 0
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -5.6911, -6.2800,  4.7838]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 4, step: 1
	action: tensor([[ 6.1800,  6.1800,  5.8563,  6.1800, -5.7876, -4.9474,  4.8753]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 4, step: 2
	action: tensor([[ 6.1800,  5.5586,  4.6850,  6.1772, -5.5537, -5.5629,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 4, step: 3
	action: tensor([[ 4.6003,  6.1800,  6.0474,  5.5596, -5.1550, -4.6400,  4.7881]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 4, step: 4
	action: tensor([[ 6.1800,  5.1533,  5.1631,  6.1800, -6.2800, -4.5995,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 4, step: 5
	action: tensor([[ 4.3152,  6.1800,  4.8071,  3.6521, -5.8533, -6.2464,  5.8476]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 4, step: 6
	action: tensor([[ 6.1800,  5.6733,  6.1800,  6.1800, -6.2800, -5.6970,  5.6685]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 4, step: 7
	action: tensor([[ 4.9286,  5.1878,  6.1800,  6.1800, -6.2800, -6.0618,  5.1826]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 4, step: 8
	action: tensor([[ 6.1800,  6.1800,  5.8298,  6.1800, -4.8841, -5.8218,  5.3676]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 4, step: 9
	action: tensor([[ 5.1979,  5.9440,  6.1800,  4.3201, -6.2478, -5.0401,  5.1775]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 4, step: 10
	action: tensor([[ 6.1800,  4.7862,  5.6398,  5.5788, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 4, step: 11
	action: tensor([[ 6.1800,  4.2437,  6.1800,  5.7078, -4.6819, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 4, step: 12
	action: tensor([[ 5.0163,  5.3434,  6.1800,  6.1800, -6.2800, -6.2800,  6.0344]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 4, step: 13
	action: tensor([[ 6.1800,  6.1800,  6.1800,  5.2919, -4.8648, -6.2800,  5.9724]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 4, step: 14
	action: tensor([[ 5.3423,  5.8974,  6.1800,  6.1800, -6.2026, -6.2417,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 4, step: 15
	action: tensor([[ 6.1800,  6.1566,  6.1800,  6.1800, -5.5061, -6.0791,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 4, step: 16
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -5.4969, -6.2800,  5.1697]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1334084524012369 entropy 1.4189385332046724
epoch: 4, step: 17
	action: tensor([[ 4.7861,  5.7300,  6.0562,  5.9755, -5.5240, -6.2800,  5.4522]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 4, step: 18
	action: tensor([[ 6.1800,  6.1800,  5.2768,  5.5718, -6.2800, -4.0498,  5.4163]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 4, step: 19
	action: tensor([[ 6.1800,  5.7929,  6.1800,  6.1800, -5.0073, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 4, step: 20
	action: tensor([[ 6.1800,  5.8748,  6.1800,  6.0727, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 4, step: 21
	action: tensor([[ 6.1800,  6.1800,  5.6359,  5.1815, -6.0178, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 4, step: 22
	action: tensor([[ 5.4373,  6.1194,  6.1800,  5.9970, -6.2800, -5.4145,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 4, step: 23
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -6.2800, -5.4056,  5.6435]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 4, step: 24
	action: tensor([[ 5.4780,  6.1800,  6.1800,  6.1135, -5.7604, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 4, step: 25
	action: tensor([[ 5.9802,  5.9646,  5.9685,  5.5713, -6.1154, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 4, step: 26
	action: tensor([[ 6.1800,  6.1800,  4.9748,  6.1800, -5.5830, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0945763126748915 entropy 1.4189385332046724
epoch: 4, step: 27
	action: tensor([[ 5.4451,  6.1665,  5.8470,  5.6977, -6.2800, -6.2800,  5.7605]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 4, step: 28
	action: tensor([[ 6.1800,  4.5233,  5.8746,  6.1800, -5.2346, -6.0266,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 4, step: 29
	action: tensor([[ 5.2323,  6.1800,  6.1800,  5.1769, -6.2800, -5.5431,  4.0806]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 4, step: 30
	action: tensor([[ 5.5378,  6.1800,  5.9456,  6.1800, -6.2800, -4.3895,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 4, step: 31
	action: tensor([[ 6.1800,  5.7967,  6.1800,  6.1800, -6.1821, -5.6870,  5.4132]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 4, step: 32
	action: tensor([[ 6.1800,  5.1270,  6.1800,  6.1435, -6.2800, -5.3370,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 4, step: 33
	action: tensor([[ 5.3351,  6.1800,  5.7511,  6.1800, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 4, step: 34
	action: tensor([[ 6.1051,  6.1800,  6.1800,  6.1800, -5.1723, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 4, step: 35
	action: tensor([[ 6.1800,  6.1740,  5.5499,  5.0644, -6.0958, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 4, step: 36
	action: tensor([[ 5.1889,  5.6708,  6.0361,  5.1779, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 4, step: 37
	action: tensor([[ 6.1800,  6.1800,  5.4152,  5.8262, -6.2800, -6.0025,  5.9527]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 4, step: 38
	action: tensor([[ 6.0597,  5.6714,  6.1800,  6.1800, -3.8807, -5.4464,  4.4583]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4754685077136527 entropy 1.4189385332046724
epoch: 4, step: 39
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -5.9244, -4.9186,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 4, step: 40
	action: tensor([[ 5.4036,  6.1800,  5.2904,  6.1800, -6.1391, -5.4071,  5.6870]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.6248176874921363 entropy 1.4189385332046724
epoch: 4, step: 41
	action: tensor([[ 6.1800,  5.3524,  6.1800,  5.2644, -5.5486, -6.2800,  5.6166]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 4, step: 42
	action: tensor([[ 6.0346,  6.1417,  6.1800,  6.1800, -5.5826, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 4, step: 43
	action: tensor([[ 5.9175,  6.0385,  5.9256,  5.2878, -6.2800, -6.2800,  6.1204]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 4, step: 44
	action: tensor([[ 5.8798,  5.8174,  4.4946,  5.7212, -5.4031, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 4, step: 45
	action: tensor([[ 5.9878,  6.1800,  6.1800,  5.5449, -5.1573, -5.4848,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 4, step: 46
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 4, step: 47
	action: tensor([[ 5.2420,  6.1800,  5.9951,  5.8524, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 4, step: 48
	action: tensor([[ 5.4288,  6.0259,  5.0441,  6.1800, -4.5271, -5.0362,  4.9984]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 4, step: 49
	action: tensor([[ 6.1552,  6.1800,  6.0319,  6.1800, -6.2800, -6.2800,  5.7975]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 4, step: 50
	action: tensor([[ 6.1800,  6.1800,  6.1800,  5.9654, -5.9039, -6.1450,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 4, step: 51
	action: tensor([[ 5.3293,  4.9253,  5.8338,  4.9561, -5.9516, -6.2800,  4.8969]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 4, step: 52
	action: tensor([[ 6.1800,  6.1800,  6.1608,  5.3814, -5.2830, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 4, step: 53
	action: tensor([[ 5.4617,  6.1800,  5.2030,  6.1800, -4.6796, -5.6677,  3.8273]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 4, step: 54
	action: tensor([[ 6.1800,  5.3850,  6.1800,  5.7491, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.5088575440220358 entropy 1.4189385332046724
epoch: 4, step: 55
	action: tensor([[ 4.2813,  6.1800,  6.1800,  6.1800, -6.2800, -5.9521,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 4, step: 56
	action: tensor([[ 6.1800,  5.8534,  4.3823,  5.6073, -6.2800, -4.8885,  5.2140]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 4, step: 57
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -6.2191, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 4, step: 58
	action: tensor([[ 5.3772,  6.1800,  6.1800,  6.1800, -5.9361, -5.9188,  6.1603]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.5911236138173577 entropy 1.4189385332046724
epoch: 4, step: 59
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -5.5095, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 4, step: 60
	action: tensor([[ 6.1800,  6.1800,  5.7367,  5.3562, -4.6414, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 4, step: 61
	action: tensor([[ 5.7775,  6.1800,  6.1800,  5.6123, -6.2800, -5.5383,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 4, step: 62
	action: tensor([[ 6.0547,  6.1800,  6.1800,  5.9058, -6.2800, -6.2800,  5.7134]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 4, step: 63
	action: tensor([[ 6.1800,  6.1800,  6.1800,  4.2696, -6.2800, -6.2800,  6.1717]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 4, step: 64
	action: tensor([[ 5.6021,  6.1800,  4.0671,  5.9046, -6.2800, -6.2800,  4.9072]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 4, step: 65
	action: tensor([[ 6.1800,  6.1800,  5.9091,  6.1800, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 4, step: 66
	action: tensor([[ 5.1375,  6.1800,  5.3512,  6.1800, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 4, step: 67
	action: tensor([[ 6.1800,  5.8739,  5.7717,  5.1697, -5.8406, -4.8475,  5.0011]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 4, step: 68
	action: tensor([[ 4.4980,  4.6515,  6.1800,  5.7573, -4.5600, -4.7167,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 4, step: 69
	action: tensor([[ 5.5638,  6.1800,  5.5935,  6.1800, -6.2800, -4.8868,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 4, step: 70
	action: tensor([[ 4.7895,  6.1392,  3.8638,  4.8181, -5.1963, -4.5936,  5.3160]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 4, step: 71
	action: tensor([[ 5.8030,  6.1800,  6.1800,  6.0959, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 4, step: 72
	action: tensor([[ 5.7788,  5.1029,  5.6117,  4.9907, -6.2800, -5.7831,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 4, step: 73
	action: tensor([[ 6.1800,  6.1800,  6.1800,  5.4010, -5.9588, -6.2800,  5.9441]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 4, step: 74
	action: tensor([[ 6.1800,  5.5763,  4.6424,  6.1800, -6.2800, -6.2800,  5.8802]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 4, step: 75
	action: tensor([[ 5.7355,  6.1800,  6.1800,  5.4184, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 4, step: 76
	action: tensor([[ 5.6228,  4.6633,  6.1800,  4.2328, -5.6352, -5.0652,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 4, step: 77
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -5.3746, -6.2800,  5.5334]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 4, step: 78
	action: tensor([[ 4.8485,  6.0115,  5.3633,  5.8903, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 4, step: 79
	action: tensor([[ 5.1617,  4.5655,  5.6832,  6.1800, -5.2334, -6.0281,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 4, step: 80
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -6.2800, -4.6032,  5.8643]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 4, step: 81
	action: tensor([[ 6.1800,  5.8900,  6.1800,  5.7163, -5.9758, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 4, step: 82
	action: tensor([[ 5.7657,  6.1800,  5.1821,  4.8983, -6.2007, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 4, step: 83
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -5.3134, -5.5542,  5.3742]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 4, step: 84
	action: tensor([[ 6.1800,  6.1800,  4.5011,  6.0716, -6.2800, -5.0691,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 4, step: 85
	action: tensor([[ 6.0455,  6.1800,  6.1800,  6.1800, -6.2683, -5.7140,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 4, step: 86
	action: tensor([[ 5.9591,  5.1947,  5.2459,  3.6308, -6.2800, -5.2669,  6.1750]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 4, step: 87
	action: tensor([[ 6.1800,  5.5554,  6.1800,  6.1800, -6.2800, -5.8065,  6.1500]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 4, step: 88
	action: tensor([[ 5.2842,  5.2911,  5.9145,  4.0469, -5.3598, -5.3617,  5.2863]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 4, step: 89
	action: tensor([[ 5.9227,  6.1800,  6.1800,  6.1800, -3.8119, -5.5361,  4.4492]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2013221494665112 entropy 1.4189385332046724
epoch: 4, step: 90
	action: tensor([[ 6.1800,  6.1800,  5.4350,  6.1800, -5.7512, -6.2800,  5.0617]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 4, step: 91
	action: tensor([[ 4.8024,  5.3975,  5.5699,  5.5470, -5.9209, -4.7013,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 4, step: 92
	action: tensor([[ 5.4535,  6.1800,  6.1800,  6.1800, -6.2800, -5.8504,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 4, step: 93
	action: tensor([[ 6.1800,  6.1435,  5.1858,  5.5635, -5.9584, -6.0982,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 4, step: 94
	action: tensor([[ 6.1800,  6.1800,  5.8248,  5.6267, -4.6373, -4.6577,  6.1048]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 4, step: 95
	action: tensor([[ 5.6629,  5.2812,  6.1800,  5.3204, -4.6825, -5.6683,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 4, step: 96
	action: tensor([[ 6.1102,  5.1864,  4.6060,  6.1800, -6.2800, -5.8599,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 4, step: 97
	action: tensor([[ 6.1800,  6.1800,  6.1800,  4.7931, -6.2800, -5.6561,  6.0937]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 4, step: 98
	action: tensor([[ 6.1800,  6.1800,  6.1800,  5.8548, -5.6231, -6.2800,  5.8568]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 4, step: 99
	action: tensor([[ 6.1800,  6.1800,  6.1800,  4.8679, -6.2800, -6.1078,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 4, step: 100
	action: tensor([[ 5.1276,  5.2151,  6.1800,  5.8808, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 4, step: 101
	action: tensor([[ 5.3176,  6.1800,  5.6391,  6.1800, -6.2800, -5.4936,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.5750059197761799 entropy 1.4189385332046724
epoch: 4, step: 102
	action: tensor([[ 6.1800,  5.5420,  6.1800,  6.1800, -6.2800, -5.8531,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 4, step: 103
	action: tensor([[ 6.1800,  5.6335,  6.1800,  6.1800, -5.7310, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 4, step: 104
	action: tensor([[ 6.1800,  6.0774,  6.1800,  6.1800, -4.0944, -5.2933,  5.8717]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 4, step: 105
	action: tensor([[ 6.1800,  6.1800,  6.1800,  5.8282, -5.4867, -5.5203,  5.7492]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 4, step: 106
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -6.2800, -5.9253,  5.3729]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 4, step: 107
	action: tensor([[ 6.1800,  6.1800,  5.4830,  6.1800, -6.2800, -5.5906,  6.0955]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.089628807068988 entropy 1.4189385332046724
epoch: 4, step: 108
	action: tensor([[ 6.1800,  5.7532,  6.1800,  5.5992, -5.6879, -6.1058,  4.7707]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 4, step: 109
	action: tensor([[ 5.3873,  6.1800,  6.1800,  6.1800, -5.4644, -6.2800,  4.6052]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 4, step: 110
	action: tensor([[ 6.0527,  6.0121,  5.6660,  5.2764, -5.4964, -6.2800,  5.3755]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 4, step: 111
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -3.4304, -5.9140,  6.0333]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 4, step: 112
	action: tensor([[ 5.6288,  6.0634,  5.6992,  6.1800, -5.8925, -5.2083,  5.8763]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 4, step: 113
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -5.4528, -5.0682,  5.0170]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 4, step: 114
	action: tensor([[ 6.1800,  6.1668,  6.1800,  5.5018, -6.2800, -5.5870,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 4, step: 115
	action: tensor([[ 6.0433,  6.1800,  5.8200,  5.4338, -5.1500, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 4, step: 116
	action: tensor([[ 5.2433,  6.0138,  6.1800,  6.1800, -5.4231, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 4, step: 117
	action: tensor([[ 6.1800,  4.7262,  5.6065,  6.1800, -4.5112, -5.7857,  5.6845]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 4, step: 118
	action: tensor([[ 5.8668,  6.0874,  5.6010,  6.1800, -6.1623, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 4, step: 119
	action: tensor([[ 6.1800,  5.5702,  5.9243,  6.1800, -4.4667, -5.8983,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 4, step: 120
	action: tensor([[ 6.1800,  6.1800,  6.1800,  5.4778, -6.2800, -3.7136,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 4, step: 121
	action: tensor([[ 5.5507,  5.8392,  6.1800,  5.3816, -5.5295, -6.2800,  5.4026]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 4, step: 122
	action: tensor([[ 5.2403,  6.1800,  6.1800,  6.1800, -6.2800, -5.4251,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 4, step: 123
	action: tensor([[ 6.1800,  4.6719,  6.1800,  6.1800, -6.0245, -4.8209,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 4, step: 124
	action: tensor([[ 5.1674,  6.1800,  6.1800,  6.1800, -6.2800, -4.5569,  5.8533]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 4, step: 125
	action: tensor([[ 4.8422,  6.1800,  6.0635,  5.8469, -5.9040, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 4, step: 126
	action: tensor([[ 3.8571,  4.2926,  6.1800,  6.1800, -6.2800, -4.4054,  5.5469]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 4, step: 127
	action: tensor([[ 6.1800,  5.9923,  6.1800,  5.6392, -6.2800, -6.2635,  5.8195]],
       dtype=torch.float64)
	q_value: tensor([[-0.2676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
LOSS epoch 4 actor 1094.7638655023604 critic 2473.3154376456555 entropy 100
epoch: 5, step: 0
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -5.8645, -5.4963,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 5, step: 1
	action: tensor([[ 5.2235,  6.1771,  6.1800,  6.1800, -6.2725, -5.6600,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 5, step: 2
	action: tensor([[ 6.1800,  5.4778,  5.5989,  6.1800, -5.8438, -6.2800,  3.8075]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 5, step: 3
	action: tensor([[ 6.1800,  6.0861,  5.6760,  4.8557, -5.6503, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 5, step: 4
	action: tensor([[ 5.9822,  5.5357,  5.5701,  6.1800, -5.5475, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 5, step: 5
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 5, step: 6
	action: tensor([[ 6.1800,  4.2781,  6.1800,  6.1800, -6.2800, -5.4716,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 5, step: 7
	action: tensor([[ 4.8700,  6.1800,  5.1588,  5.9009, -5.9166, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 5, step: 8
	action: tensor([[ 5.2741,  5.5243,  6.1800,  6.1800, -4.0324, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 5, step: 9
	action: tensor([[ 5.8778,  6.1800,  4.9184,  5.7540, -6.2800, -5.6800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.169900644124867 entropy 1.4189385332046724
epoch: 5, step: 10
	action: tensor([[ 6.1800,  6.1800,  5.3926,  5.2901, -6.2250, -6.2800,  6.0853]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 5, step: 11
	action: tensor([[ 5.7866,  5.7194,  6.1800,  6.1800, -5.1053, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 5, step: 12
	action: tensor([[ 6.1800,  4.9438,  4.6460,  6.1800, -5.4358, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 5, step: 13
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -5.2526, -5.9392,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 5, step: 14
	action: tensor([[ 6.1800,  4.6244,  6.1800,  4.4342, -6.2800, -6.2800,  5.1604]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 5, step: 15
	action: tensor([[ 6.1800,  5.8824,  5.2585,  6.1800, -6.2800, -6.2800,  5.4452]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2023863231898675 entropy 1.4189385332046724
epoch: 5, step: 16
	action: tensor([[ 4.9093,  5.0112,  6.1800,  4.8672, -6.1025, -5.6929,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 5, step: 17
	action: tensor([[ 6.0738,  6.1800,  6.1800,  6.1800, -6.2156, -4.9977,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 5, step: 18
	action: tensor([[ 4.8285,  6.1800,  6.1800,  6.1800, -6.2800, -5.0887,  6.1658]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 5, step: 19
	action: tensor([[ 6.1800,  5.5966,  6.1335,  5.9344, -6.2800, -6.2800,  5.1452]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 5, step: 20
	action: tensor([[ 6.1800,  4.3694,  5.0561,  6.0515, -5.5331, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 5, step: 21
	action: tensor([[ 6.1800,  5.4630,  6.1800,  5.2439, -4.5343, -6.2800,  4.8533]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 5, step: 22
	action: tensor([[ 6.1282,  6.1800,  6.1800,  6.1800, -6.2800, -5.3235,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 5, step: 23
	action: tensor([[ 6.1800,  6.0647,  6.1800,  5.2658, -5.8295, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 5, step: 24
	action: tensor([[ 6.1800,  5.7806,  5.2074,  5.0258, -6.2800, -5.0709,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 5, step: 25
	action: tensor([[ 6.1800,  6.1800,  5.7920,  5.8889, -6.2800, -4.7526,  5.1979]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 5, step: 26
	action: tensor([[ 6.1800,  6.1800,  5.9451,  6.1800, -6.2800, -4.6571,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 5, step: 27
	action: tensor([[ 4.9834,  5.6634,  6.1800,  6.1800, -6.2800, -6.0626,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 5, step: 28
	action: tensor([[ 3.8222,  6.1800,  6.1800,  5.8530, -4.7573, -3.5639,  6.0469]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 5, step: 29
	action: tensor([[ 2.9367,  5.3829,  5.9060,  5.9558, -4.0883, -6.2800,  5.1773]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 5, step: 30
	action: tensor([[ 5.6848,  4.9177,  6.1800,  6.1800, -5.2757, -5.2284,  4.7310]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 5, step: 31
	action: tensor([[ 6.1800,  5.0643,  6.1800,  4.6267, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 5, step: 32
	action: tensor([[ 5.5092,  6.1800,  6.1800,  6.1800, -6.2800, -4.9507,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 5, step: 33
	action: tensor([[ 5.7871,  6.1800,  6.1800,  6.1800, -6.0717, -6.1061,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 5, step: 34
	action: tensor([[ 5.1372,  6.1800,  6.1800,  6.1800, -6.2800, -5.9766,  6.0717]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.7002169802178007 entropy 1.4189385332046724
epoch: 5, step: 35
	action: tensor([[ 6.1800,  6.1800,  4.5601,  4.0013, -6.2025, -5.3625,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 5, step: 36
	action: tensor([[ 5.7906,  6.1800,  6.1405,  6.1800, -6.2800, -6.2800,  5.5487]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 5, step: 37
	action: tensor([[ 6.1800,  4.3965,  4.9055,  6.1800, -4.9153, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 5, step: 38
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -4.4475, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 5, step: 39
	action: tensor([[ 6.0465,  6.1800,  6.1800,  6.1800, -6.1577, -4.5469,  6.1072]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 5, step: 40
	action: tensor([[ 6.1498,  6.1800,  6.1800,  6.1800, -6.2800, -5.5658,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0449843806346495 entropy 1.4189385332046724
epoch: 5, step: 41
	action: tensor([[ 5.9526,  6.1800,  6.1800,  5.8262, -6.2800, -4.8176,  6.1397]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 5, step: 42
	action: tensor([[ 3.2543,  6.1800,  5.5949,  5.4615, -6.2800, -6.0202,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 5, step: 43
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -6.2800, -6.2082,  4.1722]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 5, step: 44
	action: tensor([[ 5.7745,  5.7860,  6.1800,  5.7579, -6.2800, -6.2800,  5.1561]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 5, step: 45
	action: tensor([[ 6.1800,  6.1800,  5.4175,  6.1800, -5.1463, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 5, step: 46
	action: tensor([[ 6.1800,  6.1800,  5.3817,  6.1800, -6.2078, -5.9662,  5.7159]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0941902208466665 entropy 1.4189385332046724
epoch: 5, step: 47
	action: tensor([[ 6.1800,  5.9141,  5.1268,  5.0728, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 5, step: 48
	action: tensor([[ 5.6977,  6.1800,  4.9541,  6.1800, -5.2657, -6.2800,  6.1096]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 5, step: 49
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -5.2185, -5.9266,  5.7422]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0312981452181098 entropy 1.4189385332046724
epoch: 5, step: 50
	action: tensor([[ 5.5483,  4.7880,  6.1800,  4.7860, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 5, step: 51
	action: tensor([[ 6.1800,  6.1800,  6.1800,  5.6396, -6.2800, -6.2800,  5.7344]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 5, step: 52
	action: tensor([[ 5.8262,  3.3202,  6.1800,  5.9445, -5.7767, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 5, step: 53
	action: tensor([[ 5.1850,  5.4991,  5.7515,  5.7642, -5.1242, -4.0963,  5.5235]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 5, step: 54
	action: tensor([[ 6.1800,  5.4945,  6.0082,  6.1800, -5.4604, -6.0783,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 5, step: 55
	action: tensor([[ 6.1800,  4.8548,  5.3773,  6.1800, -5.6776, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 5, step: 56
	action: tensor([[ 6.1800,  6.1800,  6.1017,  5.6557, -5.8082, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 5, step: 57
	action: tensor([[ 6.1800,  6.1800,  6.1800,  5.6109, -6.2800, -6.0665,  5.9384]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.463197353930384 entropy 1.4189385332046724
epoch: 5, step: 58
	action: tensor([[ 6.1800,  6.1800,  5.1441,  6.1800, -6.2355, -5.0546,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 5, step: 59
	action: tensor([[ 6.1800,  5.9261,  6.1800,  6.1800, -6.2736, -5.8578,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 5, step: 60
	action: tensor([[ 5.8800,  5.8710,  5.8801,  5.0623, -6.2800, -6.2800,  5.2806]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 5, step: 61
	action: tensor([[ 6.1800,  6.1800,  6.0451,  6.1800, -5.1732, -5.6821,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.258955539943998 entropy 1.4189385332046724
epoch: 5, step: 62
	action: tensor([[ 5.6007,  5.9988,  5.6161,  6.1800, -5.8842, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 5, step: 63
	action: tensor([[ 5.1364,  6.1800,  6.1800,  6.1800, -5.9214, -5.5203,  5.0853]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 5, step: 64
	action: tensor([[ 4.8660,  6.1800,  5.6253,  5.6216, -5.5167, -5.8376,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 5, step: 65
	action: tensor([[ 5.3285,  6.0316,  6.1800,  5.9015, -6.2800, -5.3277,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 5, step: 66
	action: tensor([[ 6.1800,  5.9160,  5.5727,  6.1800, -6.1359, -6.1111,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 5, step: 67
	action: tensor([[ 6.1800,  6.1800,  6.1800,  5.9471, -5.4917, -5.4612,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 5, step: 68
	action: tensor([[ 4.8532,  6.1800,  6.1800,  4.6871, -5.3943, -5.9524,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 5, step: 69
	action: tensor([[ 6.1800,  5.5495,  5.8706,  6.1800, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 5, step: 70
	action: tensor([[ 6.1800,  5.4199,  6.1800,  6.1800, -6.2800, -6.0378,  5.2341]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 5, step: 71
	action: tensor([[ 6.1800,  5.6959,  6.0324,  6.1800, -6.0833, -5.3294,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 5, step: 72
	action: tensor([[ 6.1571,  6.1800,  5.6832,  5.5585, -6.2800, -5.5713,  5.9558]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 5, step: 73
	action: tensor([[ 5.4907,  4.4022,  6.1800,  5.9429, -6.2800, -6.2191,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 5, step: 74
	action: tensor([[ 6.1800,  6.1800,  6.1800,  3.3143, -6.2800, -5.3385,  5.2158]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 5, step: 75
	action: tensor([[ 6.1800,  5.6759,  6.1800,  6.1800, -6.0434, -6.2800,  5.7524]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 5, step: 76
	action: tensor([[ 6.1800,  5.7550,  5.8468,  5.7415, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 5, step: 77
	action: tensor([[ 6.1800,  6.1800,  6.1800,  5.0179, -4.6557, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 5, step: 78
	action: tensor([[ 4.1677,  5.8145,  6.1800,  4.8078, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 5, step: 79
	action: tensor([[ 6.1800,  4.8635,  6.1800,  5.3871, -6.2800, -5.0399,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 5, step: 80
	action: tensor([[ 6.1800,  5.5995,  6.1800,  5.3056, -6.2800, -6.2800,  5.7974]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 5, step: 81
	action: tensor([[ 6.1800,  6.1800,  6.1800,  5.4135, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 5, step: 82
	action: tensor([[ 4.8053,  5.7341,  6.1800,  6.1800, -4.5272, -6.2800,  5.2716]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 5, step: 83
	action: tensor([[ 6.1800,  4.1615,  6.1800,  6.1800, -5.1473, -6.2800,  6.1250]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 5, step: 84
	action: tensor([[ 6.1800,  6.1800,  5.2044,  5.9540, -6.2800, -6.1600,  5.1255]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0175407036211725 entropy 1.4189385332046724
epoch: 5, step: 85
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -6.2800, -5.0822,  5.5277]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 5, step: 86
	action: tensor([[ 6.1800,  5.5040,  6.1800,  5.7782, -4.2810, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 5, step: 87
	action: tensor([[ 6.1800,  4.9706,  5.9486,  6.1800, -6.2800, -5.1875,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 5, step: 88
	action: tensor([[ 6.1800,  6.1800,  6.1800,  5.5023, -6.2800, -6.2528,  4.0399]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 5, step: 89
	action: tensor([[ 5.8365,  6.1800,  6.1800,  6.1800, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 5, step: 90
	action: tensor([[ 6.1800,  4.5863,  5.3512,  6.1011, -6.2800, -6.2800,  5.9467]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 5, step: 91
	action: tensor([[ 6.1800,  6.1800,  5.8044,  6.1800, -4.8147, -5.5587,  5.0573]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 5, step: 92
	action: tensor([[ 5.4501,  6.1800,  4.8671,  5.8056, -5.3225, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 5, step: 93
	action: tensor([[ 6.1800,  5.5928,  4.5293,  6.1800, -5.5040, -5.5408,  5.8905]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 5, step: 94
	action: tensor([[ 5.9053,  6.1037,  6.1800,  5.9228, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 5, step: 95
	action: tensor([[ 5.1917,  6.1800,  6.1800,  5.3478, -6.2800, -5.9788,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 5, step: 96
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -6.2800, -5.4441,  5.4585]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.333219622378573 entropy 1.4189385332046724
epoch: 5, step: 97
	action: tensor([[ 5.2829,  6.1800,  6.1800,  6.1800, -5.2331, -5.5895,  5.9673]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 5, step: 98
	action: tensor([[ 6.0253,  6.1800,  6.1800,  6.1437, -6.2800, -5.9255,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 5, step: 99
	action: tensor([[ 5.7980,  6.0317,  6.1800,  6.1800, -4.2791, -6.1450,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 5, step: 100
	action: tensor([[ 6.1800,  5.0789,  6.1800,  5.9090, -6.2800, -5.0306,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 5, step: 101
	action: tensor([[ 5.5954,  6.1800,  6.1800,  6.1800, -5.8214, -5.9791,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 5, step: 102
	action: tensor([[ 6.1800,  6.0586,  4.3915,  5.4159, -5.6330, -6.2800,  5.4821]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 5, step: 103
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -6.1343, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 5, step: 104
	action: tensor([[ 5.4018,  4.8082,  6.0351,  5.6976, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 5, step: 105
	action: tensor([[ 6.1800,  5.6294,  6.1071,  6.1800, -4.8798, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 5, step: 106
	action: tensor([[ 4.4436,  5.9117,  5.9772,  4.2475, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 5, step: 107
	action: tensor([[ 6.0510,  5.3969,  5.9238,  4.5572, -5.7544, -6.2800,  5.7037]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 5, step: 108
	action: tensor([[ 6.1800,  6.1800,  5.5897,  6.0367, -6.1523, -4.3123,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 5, step: 109
	action: tensor([[ 6.1800,  5.3624,  5.6064,  6.1800, -5.7967, -5.6735,  4.5523]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 5, step: 110
	action: tensor([[ 6.1800,  6.1800,  5.9052,  5.3979, -5.7703, -6.0038,  5.9494]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 5, step: 111
	action: tensor([[ 6.1800,  6.1800,  5.0396,  6.0434, -5.5317, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.03858243794063 entropy 1.4189385332046724
epoch: 5, step: 112
	action: tensor([[ 6.1800,  6.1800,  5.1529,  6.1800, -4.1960, -6.2800,  5.6687]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0686225428678782 entropy 1.4189385332046724
epoch: 5, step: 113
	action: tensor([[ 6.1800,  6.1800,  5.9158,  5.7020, -6.0437, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 5, step: 114
	action: tensor([[ 6.1800,  6.1800,  5.4225,  4.7446, -5.2054, -6.1949,  5.0519]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 5, step: 115
	action: tensor([[ 5.4082,  6.1800,  6.1800,  6.0513, -6.0062, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 5, step: 116
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -6.2800, -5.4793,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0136617923619904 entropy 1.4189385332046724
epoch: 5, step: 117
	action: tensor([[ 5.6117,  6.1800,  6.1800,  5.3693, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 5, step: 118
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -5.8013, -6.2800,  6.1374]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1339683068273552 entropy 1.4189385332046724
epoch: 5, step: 119
	action: tensor([[ 6.1800,  5.1761,  6.1800,  6.1800, -3.6700, -6.0211,  5.0298]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 5, step: 120
	action: tensor([[ 5.1491,  5.7820,  4.6614,  6.1800, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 5, step: 121
	action: tensor([[ 6.1800,  5.6598,  5.8507,  6.0836, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 5, step: 122
	action: tensor([[ 6.1800,  6.1800,  6.1800,  5.2036, -6.1794, -6.2800,  5.6820]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 5, step: 123
	action: tensor([[ 5.3655,  6.1800,  6.1800,  6.1800, -5.6359, -4.8414,  4.4979]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 5, step: 124
	action: tensor([[ 6.1800,  6.1165,  6.1800,  6.1800, -6.2800, -6.2800,  5.0096]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 5, step: 125
	action: tensor([[ 4.5476,  6.1800,  6.1800,  6.1800, -5.5979, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 5, step: 126
	action: tensor([[ 4.9421,  6.1800,  6.1800,  6.1800, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.8153435467872694 entropy 1.4189385332046724
epoch: 5, step: 127
	action: tensor([[ 6.1800,  4.3584,  5.8647,  6.1800, -6.2800, -6.2800,  5.0821]],
       dtype=torch.float64)
	q_value: tensor([[-0.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
LOSS epoch 5 actor 1084.6109492510413 critic 2453.0097265299137 entropy 100
epoch: 6, step: 0
	action: tensor([[ 6.1800,  4.4289,  5.4174,  4.8957, -6.2800, -5.9047,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 6, step: 1
	action: tensor([[ 4.6393,  6.1800,  6.1800,  5.7048, -6.2800, -5.9817,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 6, step: 2
	action: tensor([[ 6.1800,  5.5643,  6.1800,  6.1800, -5.6034, -6.2800,  4.5556]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 6, step: 3
	action: tensor([[ 5.1939,  6.1800,  6.1800,  4.4549, -6.2800, -3.8459,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 6, step: 4
	action: tensor([[ 6.1800,  5.1424,  6.1800,  6.1800, -6.2800, -4.8635,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 6, step: 5
	action: tensor([[ 5.5960,  6.1800,  4.2515,  5.3838, -5.5293, -6.2800,  5.7000]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 6, step: 6
	action: tensor([[ 5.5521,  6.1800,  6.1800,  6.1800, -6.2800, -5.2809,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 6, step: 7
	action: tensor([[ 5.5257,  6.1800,  3.9444,  5.8581, -6.0581, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 6, step: 8
	action: tensor([[ 6.1800,  6.1800,  5.8461,  6.1800, -6.2190, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.125616336887847 entropy 1.4189385332046724
epoch: 6, step: 9
	action: tensor([[ 4.5995,  5.7838,  6.1800,  5.9810, -6.2800, -5.8351,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 6, step: 10
	action: tensor([[ 6.1800,  6.1800,  5.8013,  5.9078, -5.4178, -6.2800,  5.2139]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 6, step: 11
	action: tensor([[ 6.1800,  4.6644,  6.1800,  5.6866, -5.6961, -5.4089,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 6, step: 12
	action: tensor([[ 5.7590,  6.1800,  6.1800,  5.4811, -5.2989, -5.3881,  5.6636]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 6, step: 13
	action: tensor([[ 6.1800,  6.1166,  6.1800,  6.1800, -5.9114, -6.0021,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 6, step: 14
	action: tensor([[ 6.1800,  6.1585,  6.1800,  6.0210, -6.2800, -5.0316,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 6, step: 15
	action: tensor([[ 6.1416,  6.1800,  6.0844,  5.2001, -6.2800, -6.2800,  5.8469]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 6, step: 16
	action: tensor([[ 6.1463,  6.1800,  5.1251,  6.1800, -5.7584, -6.2800,  5.9172]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0571248751849829 entropy 1.4189385332046724
epoch: 6, step: 17
	action: tensor([[ 6.1800,  5.5818,  6.1800,  6.1800, -6.2800, -6.2800,  5.8730]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 6, step: 18
	action: tensor([[ 4.6300,  6.1800,  6.1800,  6.1144, -5.4429, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 6, step: 19
	action: tensor([[ 6.1800,  6.1112,  5.7885,  5.0843, -6.2800, -4.5754,  5.7430]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 6, step: 20
	action: tensor([[ 6.1800,  6.1800,  6.1800,  3.9330, -6.1190, -5.5264,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 6, step: 21
	action: tensor([[ 6.1800,  5.7111,  6.1800,  6.1800, -3.9504, -5.7226,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 6, step: 22
	action: tensor([[ 5.4670,  5.8611,  6.1800,  6.1800, -6.2800, -5.2787,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 6, step: 23
	action: tensor([[ 5.7035,  6.1800,  6.1800,  4.7272, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 6, step: 24
	action: tensor([[ 5.3370,  6.1800,  6.1800,  5.0790, -6.2800, -6.1557,  6.0005]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 6, step: 25
	action: tensor([[ 5.5645,  6.1800,  5.1634,  6.1800, -6.2800, -6.0316,  5.6184]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 6, step: 26
	action: tensor([[ 4.7808,  6.1800,  6.0045,  6.1800, -6.2800, -5.8004,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.825514929855365 entropy 1.4189385332046724
epoch: 6, step: 27
	action: tensor([[ 6.1800,  5.1747,  6.0796,  5.8723, -5.7442, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 6, step: 28
	action: tensor([[ 5.0098,  6.0511,  6.1800,  5.1638, -6.1325, -5.9714,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 6, step: 29
	action: tensor([[ 6.0275,  6.1367,  4.7667,  5.6391, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 6, step: 30
	action: tensor([[ 4.8468,  6.1800,  6.1800,  4.8237, -5.7160, -4.7519,  6.1054]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 6, step: 31
	action: tensor([[ 5.5650,  6.1732,  4.7670,  6.0170, -5.9518, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 6, step: 32
	action: tensor([[ 6.1800,  6.1800,  6.1033,  6.1800, -5.8630, -6.0023,  5.2645]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 6, step: 33
	action: tensor([[ 5.5534,  6.1800,  6.1800,  5.0760, -6.2800, -6.2800,  5.8273]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 6, step: 34
	action: tensor([[ 6.1800,  6.1800,  5.8112,  6.1800, -5.8097, -5.2577,  5.1448]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 6, step: 35
	action: tensor([[ 6.1800,  5.3846,  6.1800,  6.1800, -5.1133, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 6, step: 36
	action: tensor([[ 6.1800,  4.0260,  6.1800,  5.4265, -5.9148, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 6, step: 37
	action: tensor([[ 5.8753,  5.6021,  6.1800,  6.1800, -6.2800, -6.2800,  6.0587]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 6, step: 38
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 6, step: 39
	action: tensor([[ 5.5715,  6.1800,  5.6952,  6.1800, -5.3128, -5.8159,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4635873827423005 entropy 1.4189385332046724
epoch: 6, step: 40
	action: tensor([[ 6.1800,  6.1800,  4.3631,  6.1800, -5.9678, -5.7631,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 6, step: 41
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -6.2800, -6.2800,  5.8041]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 6, step: 42
	action: tensor([[ 6.0125,  5.9974,  6.1800,  5.4542, -6.2800, -5.9149,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 6, step: 43
	action: tensor([[ 5.7885,  6.1134,  6.1800,  6.1800, -5.9589, -6.2800,  6.0820]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 6, step: 44
	action: tensor([[ 5.0325,  5.6288,  6.1800,  5.3768, -5.8154, -6.1463,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.69865987791639 entropy 1.4189385332046724
epoch: 6, step: 45
	action: tensor([[ 5.6227,  4.3436,  6.1800,  5.6567, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 6, step: 46
	action: tensor([[ 5.7784,  6.1800,  5.4727,  6.1800, -6.2019, -5.5858,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 6, step: 47
	action: tensor([[ 6.1800,  4.9486,  6.1800,  6.1800, -5.7981, -6.2800,  5.6078]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 6, step: 48
	action: tensor([[ 5.6419,  6.1800,  6.1800,  5.0796, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 6, step: 49
	action: tensor([[ 6.0605,  6.1800,  4.6854,  5.7738, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9395351930648702 entropy 1.4189385332046724
epoch: 6, step: 50
	action: tensor([[ 6.1800,  5.4539,  6.1800,  6.1800, -6.2800, -5.1599,  4.7438]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 6, step: 51
	action: tensor([[ 4.4038,  5.9164,  5.4207,  6.1800, -5.3491, -6.2800,  5.2674]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 6, step: 52
	action: tensor([[ 6.1800,  5.8046,  5.9329,  6.1800, -5.8983, -4.3905,  5.3834]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 6, step: 53
	action: tensor([[ 6.1800,  5.8385,  6.1800,  6.1800, -5.5842, -5.8812,  5.8868]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1991521242568695 entropy 1.4189385332046724
epoch: 6, step: 54
	action: tensor([[ 6.1800,  6.1800,  4.7296,  5.6397, -5.4470, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 6, step: 55
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 6, step: 56
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -6.2800, -4.8925,  5.5634]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 6, step: 57
	action: tensor([[ 5.2056,  6.1800,  6.1800,  4.7379, -5.4543, -6.2800,  6.1125]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 6, step: 58
	action: tensor([[ 4.9509,  5.0898,  6.0117,  5.5379, -5.4272, -4.7546,  4.7603]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.5070712716282386 entropy 1.4189385332046724
epoch: 6, step: 59
	action: tensor([[ 6.1800,  5.4057,  6.1800,  5.7501, -5.0878, -6.2800,  5.4258]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 6, step: 60
	action: tensor([[ 6.1800,  6.1800,  6.1427,  5.6884, -5.0549, -6.1026,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 6, step: 61
	action: tensor([[ 6.1800,  5.4422,  6.1800,  5.7970, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 6, step: 62
	action: tensor([[ 5.7742,  5.6673,  6.1800,  6.1800, -6.2800, -6.1258,  5.0726]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 6, step: 63
	action: tensor([[ 6.1800,  6.1070,  6.1800,  6.1800, -5.2002, -6.2393,  4.8223]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 6, step: 64
	action: tensor([[ 6.1800,  6.1800,  5.6279,  4.5910, -5.2166, -5.9075,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 6, step: 65
	action: tensor([[ 6.1800,  5.3837,  4.7949,  6.1800, -5.1699, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 6, step: 66
	action: tensor([[ 6.1800,  6.1800,  6.1800,  5.0087, -5.3724, -5.2816,  5.1021]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 6, step: 67
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -6.2800, -6.2800,  6.0660]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1391979108492825 entropy 1.4189385332046724
epoch: 6, step: 68
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 6, step: 69
	action: tensor([[ 6.1800,  6.0718,  6.1800,  6.1800, -5.8495, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 6, step: 70
	action: tensor([[ 5.7278,  4.4854,  6.1800,  5.5430, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 6, step: 71
	action: tensor([[ 6.1800,  5.9848,  6.1800,  5.7263, -6.2800, -3.8009,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 6, step: 72
	action: tensor([[ 6.0263,  4.7603,  5.5754,  5.7019, -5.5399, -6.0297,  5.5835]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 6, step: 73
	action: tensor([[ 6.1800,  5.7743,  6.1800,  4.7861, -5.8528, -5.7187,  5.4118]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 6, step: 74
	action: tensor([[ 6.1800,  5.5797,  6.1800,  5.3987, -5.2472, -5.6434,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 6, step: 75
	action: tensor([[ 6.0940,  6.1800,  5.3581,  5.8824, -6.2800, -6.2800,  6.0071]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0606497770706718 entropy 1.4189385332046724
epoch: 6, step: 76
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -4.1370, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 6, step: 77
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -5.7371, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 6, step: 78
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -6.2800, -5.1249,  4.6723]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 6, step: 79
	action: tensor([[ 6.1800,  6.1800,  4.7207,  5.9175, -5.8840, -4.9762,  5.4960]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 6, step: 80
	action: tensor([[ 6.1800,  4.3126,  6.1800,  6.1800, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 6, step: 81
	action: tensor([[ 6.1800,  6.1800,  6.0324,  6.1800, -5.1329, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 6, step: 82
	action: tensor([[ 5.7645,  5.8991,  6.1800,  5.8994, -5.1374, -5.8601,  5.8634]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 6, step: 83
	action: tensor([[ 6.1800,  6.1800,  6.1800,  4.3844, -5.9642, -5.9123,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 6, step: 84
	action: tensor([[ 6.1800,  5.9203,  5.4408,  6.1800, -6.2800, -5.2828,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 6, step: 85
	action: tensor([[ 6.1800,  6.0079,  6.1800,  6.1800, -6.2800, -5.5448,  3.9475]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 6, step: 86
	action: tensor([[ 6.1800,  4.9385,  6.0375,  5.6215, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 6, step: 87
	action: tensor([[ 6.0862,  6.1800,  6.1800,  4.8870, -6.2800, -5.3425,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 6, step: 88
	action: tensor([[ 6.1800,  6.1800,  6.1800,  5.8383, -5.7716, -6.2564,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1549631601994101 entropy 1.4189385332046724
epoch: 6, step: 89
	action: tensor([[ 6.1800,  5.1488,  6.1800,  5.0650, -6.2800, -6.2800,  5.7331]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 6, step: 90
	action: tensor([[ 5.7523,  6.1311,  6.1800,  6.1800, -6.1186, -4.9289,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 6, step: 91
	action: tensor([[ 6.1800,  6.1800,  6.1800,  5.8866, -5.4782, -5.0944,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 6, step: 92
	action: tensor([[ 5.2696,  5.9333,  6.1800,  5.6010, -6.2800, -6.2429,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 6, step: 93
	action: tensor([[ 5.6783,  6.1800,  6.1800,  6.1800, -4.8674, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 6, step: 94
	action: tensor([[ 5.1477,  6.1800,  6.1800,  6.1800, -5.6235, -6.2800,  5.4713]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 6, step: 95
	action: tensor([[ 5.4883,  5.1393,  6.1800,  6.1800, -5.2668, -6.2800,  4.4713]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 6, step: 96
	action: tensor([[ 4.8841,  6.1800,  6.1800,  6.1800, -4.7279, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 6, step: 97
	action: tensor([[ 4.8033,  6.1800,  5.5738,  5.6412, -5.3559, -6.2800,  5.9933]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.7663827495716033 entropy 1.4189385332046724
epoch: 6, step: 98
	action: tensor([[ 3.5292,  6.1800,  5.6752,  4.5255, -6.2800, -5.1618,  5.3056]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 6, step: 99
	action: tensor([[ 6.1800,  4.3207,  4.9247,  3.8712, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 6, step: 100
	action: tensor([[ 5.8403,  6.1800,  4.7582,  6.1800, -6.2800, -5.4692,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 6, step: 101
	action: tensor([[ 6.1800,  5.2084,  6.1800,  4.4805, -6.2800, -6.2800,  4.9822]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 6, step: 102
	action: tensor([[ 5.2785,  6.1800,  6.1800,  6.1800, -6.2800, -5.3725,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 6, step: 103
	action: tensor([[ 5.4633,  6.1800,  4.3438,  6.1800, -6.2800, -5.9026,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 6, step: 104
	action: tensor([[ 5.2016,  6.1800,  4.2482,  5.8179, -5.7258, -6.1434,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 6, step: 105
	action: tensor([[ 6.1614,  6.1800,  5.7608,  6.1800, -6.0895, -5.3244,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3267238723212398 entropy 1.4189385332046724
epoch: 6, step: 106
	action: tensor([[ 5.7578,  5.8276,  6.1800,  6.1800, -6.2800, -5.5158,  5.7273]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 6, step: 107
	action: tensor([[ 6.1800,  6.1800,  5.7544,  5.7340, -6.2303, -3.9638,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 6, step: 108
	action: tensor([[ 6.0270,  6.1095,  6.1800,  6.1800, -4.8421, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 6, step: 109
	action: tensor([[ 5.9188,  4.9892,  5.9866,  6.1800, -3.4806, -6.2800,  5.3818]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 6, step: 110
	action: tensor([[ 6.1800,  5.8918,  6.1800,  6.1800, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 6, step: 111
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -3.4209, -5.7834,  5.7721]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 6, step: 112
	action: tensor([[ 4.5418,  6.1800,  6.1800,  4.2465, -5.6538, -4.7573,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 6, step: 113
	action: tensor([[ 5.5938,  6.1800,  6.1800,  5.8601, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 6, step: 114
	action: tensor([[ 6.1800,  6.1800,  6.1800,  5.4331, -5.1040, -6.2800,  4.5936]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 6, step: 115
	action: tensor([[ 6.0856,  6.1800,  6.1800,  6.1800, -4.4182, -5.0981,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 6, step: 116
	action: tensor([[ 5.8877,  6.1800,  6.1800,  5.9629, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 6, step: 117
	action: tensor([[ 5.6086,  5.1625,  6.0101,  6.1800, -6.2800, -6.2800,  5.1388]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 6, step: 118
	action: tensor([[ 6.1800,  5.2059,  6.1800,  6.1800, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 6, step: 119
	action: tensor([[ 5.4327,  6.1800,  5.9153,  6.1083, -6.2800, -3.6985,  6.0944]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 6, step: 120
	action: tensor([[ 5.7217,  6.1800,  6.1800,  6.1641, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 6, step: 121
	action: tensor([[ 5.9582,  6.1800,  5.2184,  6.1800, -6.2642, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 6, step: 122
	action: tensor([[ 6.1800,  6.1800,  5.3760,  6.1461, -6.2800, -6.1917,  5.8052]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.102281586839078 entropy 1.4189385332046724
epoch: 6, step: 123
	action: tensor([[ 5.8181,  6.1800,  6.1800,  5.8238, -5.9122, -6.2800,  5.7550]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 6, step: 124
	action: tensor([[ 4.7049,  6.1800,  4.7557,  6.1800, -6.2800, -4.3086,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 6, step: 125
	action: tensor([[ 5.1158,  4.2930,  6.1800,  5.5652, -6.2800, -5.3701,  5.2846]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 6, step: 126
	action: tensor([[ 4.3360,  5.1105,  6.1800,  6.1800, -4.2086, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 6, step: 127
	action: tensor([[ 6.1800,  6.1800,  6.1800,  5.5630, -4.2490, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-0.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
LOSS epoch 6 actor 1067.0959180711468 critic 2417.979542783228 entropy 100
epoch: 7, step: 0
	action: tensor([[ 6.1800,  6.1405,  6.1800,  6.1800, -6.2800, -6.2800,  6.1508]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 7, step: 1
	action: tensor([[ 6.1800,  6.0074,  6.1800,  4.5091, -6.0574, -6.2800,  5.6798]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 7, step: 2
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -5.5764, -6.2800,  3.4986]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 7, step: 3
	action: tensor([[ 3.4424,  6.1800,  5.8810,  6.1800, -6.1285, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 7, step: 4
	action: tensor([[ 5.9064,  4.8310,  6.1800,  4.6737, -6.1075, -4.9952,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 7, step: 5
	action: tensor([[ 6.1800,  5.7071,  6.1800,  4.8364, -6.2800, -4.9018,  5.2636]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 7, step: 6
	action: tensor([[ 5.7878,  6.1800,  6.1800,  5.6308, -6.2800, -6.2800,  5.3984]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 7, step: 7
	action: tensor([[ 4.9239,  4.1508,  6.1800,  5.8552, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 7, step: 8
	action: tensor([[ 5.6059,  6.1800,  5.6886,  6.1717, -6.2800, -5.2764,  5.5868]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 7, step: 9
	action: tensor([[ 6.1800,  3.2624,  5.7752,  6.1800, -5.0340, -5.9568,  5.6301]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 7, step: 10
	action: tensor([[ 5.6858,  6.1800,  6.1160,  6.1800, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4645148998405813 entropy 1.4189385332046724
epoch: 7, step: 11
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -6.2800, -5.3418,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 7, step: 12
	action: tensor([[ 5.9259,  5.5693,  5.5073,  6.1800, -5.6280, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 7, step: 13
	action: tensor([[ 6.1800,  4.0999,  6.1800,  5.7130, -4.3989, -5.3448,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 7, step: 14
	action: tensor([[ 6.1800,  5.4863,  6.1800,  5.3685, -6.0752, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 7, step: 15
	action: tensor([[ 6.1800,  6.0266,  5.7658,  6.1741, -5.7496, -5.3163,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 7, step: 16
	action: tensor([[ 5.9576,  6.1800,  6.1800,  6.1800, -5.9136, -6.2800,  4.7404]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 7, step: 17
	action: tensor([[ 6.1800,  6.1800,  6.1800,  4.6840, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 7, step: 18
	action: tensor([[ 6.1800,  5.7562,  6.1800,  6.1800, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 7, step: 19
	action: tensor([[ 5.5868,  5.5566,  6.1800,  6.1800, -6.2800, -6.0068,  5.5166]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 7, step: 20
	action: tensor([[ 6.1800,  5.3997,  5.5701,  5.3253, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 7, step: 21
	action: tensor([[ 6.1800,  4.8816,  5.2761,  6.1800, -5.7335, -6.1641,  5.2453]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 7, step: 22
	action: tensor([[ 6.1800,  5.5882,  4.9404,  6.1800, -6.2800, -6.2800,  5.5563]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 7, step: 23
	action: tensor([[ 6.1800,  5.9073,  6.1800,  6.1800, -6.2800, -5.7911,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 7, step: 24
	action: tensor([[ 6.0355,  6.1800,  5.2866,  5.9218, -6.2800, -6.2800,  6.0920]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0921367003613904 entropy 1.4189385332046724
epoch: 7, step: 25
	action: tensor([[ 5.5024,  6.0579,  6.1240,  6.1800, -5.2520, -6.1394,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 7, step: 26
	action: tensor([[ 6.1800,  5.6134,  5.9006,  5.6267, -6.2800, -6.1588,  4.8380]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 7, step: 27
	action: tensor([[ 5.6644,  6.1800,  3.4562,  5.1152, -5.5927, -5.8055,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 7, step: 28
	action: tensor([[ 4.7989,  4.6028,  6.1800,  5.8719, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 7, step: 29
	action: tensor([[ 6.1800,  6.1800,  5.7550,  5.5380, -6.2800, -5.7196,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1266330518204632 entropy 1.4189385332046724
epoch: 7, step: 30
	action: tensor([[ 6.1800,  5.2752,  6.1800,  5.8341, -5.7128, -6.2800,  5.6878]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 7, step: 31
	action: tensor([[ 6.1800,  5.5556,  5.8028,  5.7073, -6.0387, -6.1233,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 7, step: 32
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -5.5363, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 7, step: 33
	action: tensor([[ 5.6425,  4.6057,  6.1800,  6.1800, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 7, step: 34
	action: tensor([[ 6.1800,  6.1800,  6.1800,  4.5906, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 7, step: 35
	action: tensor([[ 5.7225,  5.4496,  6.1800,  6.1800, -5.2421, -6.1832,  5.7797]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 7, step: 36
	action: tensor([[ 6.1800,  6.1800,  5.9745,  6.1800, -5.4190, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1148158635678227 entropy 1.4189385332046724
epoch: 7, step: 37
	action: tensor([[ 5.6548,  6.1800,  6.1800,  5.1652, -6.2800, -6.2338,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 7, step: 38
	action: tensor([[ 6.1800,  6.1800,  5.3982,  6.1800, -6.2800, -6.2800,  5.8347]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4218128456851804 entropy 1.4189385332046724
epoch: 7, step: 39
	action: tensor([[ 6.1800,  6.1800,  5.1895,  6.0054, -5.5227, -6.2800,  4.3416]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 7, step: 40
	action: tensor([[ 6.1800,  6.1800,  5.9211,  5.9173, -6.2800, -5.7955,  4.4836]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 7, step: 41
	action: tensor([[ 5.1478,  4.8264,  5.3276,  6.1800, -5.1560, -6.2800,  5.4543]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 7, step: 42
	action: tensor([[ 6.1800,  6.1800,  6.1800,  5.5666, -6.2800, -6.2800,  5.1573]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 7, step: 43
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -6.2800, -6.2800,  5.9978]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 7, step: 44
	action: tensor([[ 6.1800,  5.5291,  6.1800,  5.9217, -6.2800, -4.8926,  6.0121]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1648661053404872 entropy 1.4189385332046724
epoch: 7, step: 45
	action: tensor([[ 5.3477,  6.0006,  4.5604,  5.9459, -6.2800, -4.4393,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 7, step: 46
	action: tensor([[ 5.3296,  5.8489,  5.4706,  6.1800, -4.9905, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 7, step: 47
	action: tensor([[ 6.1800,  6.1800,  6.1260,  6.1800, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 7, step: 48
	action: tensor([[ 6.1800,  6.1800,  6.1062,  5.7328, -4.8510, -5.7526,  5.6243]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 7, step: 49
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 7, step: 50
	action: tensor([[ 5.6728,  6.1800,  6.1800,  5.7568, -6.2800, -5.4218,  4.9009]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 7, step: 51
	action: tensor([[ 6.1800,  6.1800,  5.4948,  5.0386, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 7, step: 52
	action: tensor([[ 4.9147,  6.1800,  4.9078,  6.1800, -6.2800, -5.0515,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 7, step: 53
	action: tensor([[ 6.1800,  6.1800,  5.3860,  6.1800, -5.2781, -6.0345,  5.2929]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 7, step: 54
	action: tensor([[ 6.1800,  5.6429,  6.1800,  6.1800, -6.2800, -5.0523,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 7, step: 55
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1363, -5.2359, -4.7515,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 7, step: 56
	action: tensor([[ 6.1800,  6.1800,  4.5462,  6.1800, -5.2310, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.073508380086522 entropy 1.4189385332046724
epoch: 7, step: 57
	action: tensor([[ 6.1800,  4.7833,  6.1800,  5.5308, -6.2800, -5.4044,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 7, step: 58
	action: tensor([[ 6.0755,  6.1800,  6.1800,  6.1800, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 7, step: 59
	action: tensor([[ 4.8694,  6.1800,  6.1800,  5.1119, -6.2800, -6.2800,  5.9875]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 7, step: 60
	action: tensor([[ 5.8126,  5.0671,  5.8978,  6.0350, -6.2585, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 7, step: 61
	action: tensor([[ 5.8144,  5.2912,  5.3809,  6.1800, -6.2800, -5.4941,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 7, step: 62
	action: tensor([[ 5.9124,  5.8346,  3.9713,  5.2766, -5.3397, -6.2800,  5.1426]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 7, step: 63
	action: tensor([[ 6.1800,  5.5647,  5.3519,  4.9877, -6.2426, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 7, step: 64
	action: tensor([[ 6.1800,  6.1800,  5.2528,  5.9161, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 7, step: 65
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -5.0226, -6.2800,  6.1727]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1347978273734065 entropy 1.4189385332046724
epoch: 7, step: 66
	action: tensor([[ 5.7429,  6.1800,  6.1800,  6.1800, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 7, step: 67
	action: tensor([[ 6.1800,  5.0291,  5.0515,  6.1800, -5.4106, -4.8240,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 7, step: 68
	action: tensor([[ 6.1800,  6.1800,  6.0591,  4.5580, -6.0566, -5.9123,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 7, step: 69
	action: tensor([[ 5.4489,  6.1800,  6.1800,  5.3484, -5.1123, -5.5273,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 7, step: 70
	action: tensor([[ 6.0701,  6.1800,  6.1800,  5.4299, -5.5160, -6.2800,  5.4666]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 7, step: 71
	action: tensor([[ 5.8034,  6.1800,  6.1800,  5.1569, -5.1851, -6.2800,  5.3702]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 7, step: 72
	action: tensor([[ 4.4079,  6.1800,  4.8967,  6.1800, -6.1740, -5.1924,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 7, step: 73
	action: tensor([[ 6.1800,  6.1800,  5.0338,  5.7045, -5.2853, -5.0267,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 7, step: 74
	action: tensor([[ 6.1800,  6.1800,  5.3444,  3.8098, -6.1139, -5.5572,  5.4238]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 7, step: 75
	action: tensor([[ 6.1800,  6.1800,  5.6819,  6.1800, -6.2800, -6.2800,  6.0362]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0712901188477266 entropy 1.4189385332046724
epoch: 7, step: 76
	action: tensor([[ 4.7281,  6.1800,  6.1800,  6.1800, -4.3109, -6.2800,  4.6772]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 7, step: 77
	action: tensor([[ 6.1800,  4.1700,  5.9814,  5.1539, -6.2800, -5.4615,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 7, step: 78
	action: tensor([[ 5.1415,  5.7806,  5.0303,  5.7856, -5.5699, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 7, step: 79
	action: tensor([[ 6.1800,  6.1800,  5.9985,  6.1800, -6.2800, -5.0752,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 7, step: 80
	action: tensor([[ 6.0902,  6.1800,  6.1800,  5.8622, -6.0760, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 7, step: 81
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -4.5154, -4.8798,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 7, step: 82
	action: tensor([[ 6.1800,  6.1800,  5.1984,  6.1800, -6.2800, -6.2800,  5.6416]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 7, step: 83
	action: tensor([[ 6.1800,  6.1800,  4.5257,  4.8179, -4.6597, -6.0279,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 7, step: 84
	action: tensor([[ 5.5271,  6.1800,  6.1800,  6.0262, -5.9690, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 7, step: 85
	action: tensor([[ 5.6944,  6.1800,  5.7802,  4.7492, -5.8143, -5.8056,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 7, step: 86
	action: tensor([[ 6.0022,  6.1800,  6.1800,  6.1800, -4.2764, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 7, step: 87
	action: tensor([[ 6.1800,  5.7039,  6.1800,  5.7218, -6.0802, -6.2800,  5.6166]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 7, step: 88
	action: tensor([[ 5.5987,  5.4207,  6.1800,  5.9362, -6.0567, -5.6381,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 7, step: 89
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -6.0198, -6.2800,  3.3007]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 7, step: 90
	action: tensor([[ 6.1800,  5.3318,  5.7504,  6.1800, -6.2800, -5.6124,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 7, step: 91
	action: tensor([[ 6.1374,  6.1800,  5.0318,  6.1800, -4.6823, -6.2800,  5.5617]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.07868385825475 entropy 1.4189385332046724
epoch: 7, step: 92
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -4.0293, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 7, step: 93
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.0180, -5.5822, -5.6922,  4.7452]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 7, step: 94
	action: tensor([[ 6.1800,  6.1064,  5.5514,  6.1800, -5.6177, -6.2800,  6.1758]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 7, step: 95
	action: tensor([[ 6.1800,  6.1800,  6.1800,  5.6824, -5.3565, -6.0886,  5.2711]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 7, step: 96
	action: tensor([[ 5.4889,  6.1800,  6.0970,  5.0323, -6.2800, -5.1794,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 7, step: 97
	action: tensor([[ 6.1800,  3.9462,  5.8332,  6.1800, -4.5527, -6.2800,  5.8916]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 7, step: 98
	action: tensor([[ 6.1800,  6.1800,  6.0851,  6.1800, -5.5218, -4.2440,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 7, step: 99
	action: tensor([[ 5.7629,  6.1670,  5.5014,  6.1800, -6.2800, -5.2734,  5.8115]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 7, step: 100
	action: tensor([[ 6.1800,  4.5384,  6.1800,  5.6567, -4.9598, -6.2800,  5.3890]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 7, step: 101
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -5.7796, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 7, step: 102
	action: tensor([[ 6.1800,  6.1800,  6.1800,  5.3077, -5.9231, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 7, step: 103
	action: tensor([[ 6.1800,  6.1800,  5.4177,  6.1800, -5.9109, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 7, step: 104
	action: tensor([[ 6.0446,  4.6346,  5.1772,  6.1800, -6.2800, -5.4547,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 7, step: 105
	action: tensor([[ 6.1800,  5.1912,  5.3273,  5.6954, -5.9878, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 7, step: 106
	action: tensor([[ 5.2357,  6.1800,  6.1800,  5.8777, -6.2800, -5.8647,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 7, step: 107
	action: tensor([[ 6.1800,  5.6771,  5.4668,  6.1800, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 7, step: 108
	action: tensor([[ 4.7777,  6.1800,  6.1800,  5.3760, -5.7875, -5.1653,  4.9233]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 7, step: 109
	action: tensor([[ 6.1800,  6.1800,  5.0749,  6.1800, -5.1265, -5.8241,  5.8125]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 7, step: 110
	action: tensor([[ 5.8473,  4.8707,  6.1800,  6.1800, -5.4785, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 7, step: 111
	action: tensor([[ 4.2833,  5.4916,  5.7159,  5.7647, -4.9129, -5.6242,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 7, step: 112
	action: tensor([[ 6.1800,  4.4947,  6.1800,  5.3654, -5.0905, -5.8198,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 7, step: 113
	action: tensor([[ 6.1800,  6.1800,  4.6384,  4.2513, -5.0899, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 7, step: 114
	action: tensor([[ 5.6011,  6.1800,  6.1800,  6.1800, -6.2665, -6.2800,  5.1896]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 7, step: 115
	action: tensor([[ 6.1800,  6.1309,  3.6707,  6.1800, -6.2543, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 7, step: 116
	action: tensor([[ 6.1800,  5.7030,  6.1800,  6.1800, -6.2800, -5.9011,  4.8864]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 7, step: 117
	action: tensor([[ 6.1800,  5.4629,  6.1800,  6.1800, -6.0740, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 7, step: 118
	action: tensor([[ 6.1800,  6.1800,  6.1800,  3.9797, -6.2800, -6.2800,  5.6089]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 7, step: 119
	action: tensor([[ 6.1800,  6.1026,  5.7012,  4.0327, -5.3088, -6.0317,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 7, step: 120
	action: tensor([[ 6.1800,  5.3345,  6.1210,  6.1800, -6.2800, -4.9063,  5.8306]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 7, step: 121
	action: tensor([[ 6.1800,  5.7754,  5.4440,  6.1800, -6.2800, -6.2800,  5.4648]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 7, step: 122
	action: tensor([[ 6.1800,  6.1572,  6.1800,  6.1800, -5.8412, -4.7543,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 7, step: 123
	action: tensor([[ 4.9271,  5.2804,  3.1034,  6.1800, -5.8589, -5.2182,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 7, step: 124
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -5.6078, -5.9284,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 7, step: 125
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -6.2800, -6.2800,  5.5586]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 7, step: 126
	action: tensor([[ 5.6755,  4.9519,  6.1800,  5.5037, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 7, step: 127
	action: tensor([[ 6.1800,  6.1800,  6.1800,  5.5028, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2152540144128907 entropy 1.4189385332046724
LOSS epoch 7 actor 1036.702067643153 critic 2357.1918419272406 entropy 100
epoch: 8, step: 0
	action: tensor([[ 6.1648,  5.4073,  6.1800,  6.1553, -6.2800, -5.6830,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 8, step: 1
	action: tensor([[ 6.1800,  5.6195,  5.5087,  6.1800, -3.8756, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 8, step: 2
	action: tensor([[ 3.8004,  5.7952,  5.7971,  5.3411, -6.2800, -5.0483,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 8, step: 3
	action: tensor([[ 5.7513,  4.8822,  6.1800,  6.1800, -6.2800, -6.1084,  6.0986]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 8, step: 4
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -5.9282, -6.0207,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 8, step: 5
	action: tensor([[ 4.8266,  6.1800,  4.3134,  6.1800, -4.0104, -6.2800,  5.1954]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 8, step: 6
	action: tensor([[ 6.1800,  6.1800,  6.1800,  5.9759, -6.2619, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 8, step: 7
	action: tensor([[ 4.2476,  5.7105,  6.1800,  5.6838, -6.2800, -5.5132,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 8, step: 8
	action: tensor([[ 6.0033,  6.1800,  6.1800,  5.4001, -6.2800, -6.2800,  5.4483]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 8, step: 9
	action: tensor([[ 5.4240,  6.1800,  6.1800,  6.1800, -6.2800, -5.8743,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 8, step: 10
	action: tensor([[ 5.2017,  5.5743,  6.0738,  6.1488, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 8, step: 11
	action: tensor([[ 6.1800,  4.4488,  6.1800,  6.1800, -6.2800, -5.0280,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 8, step: 12
	action: tensor([[ 4.4528,  5.2621,  6.1800,  6.1800, -6.1790, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 8, step: 13
	action: tensor([[ 6.1800,  5.2973,  6.1800,  4.7703, -5.4621, -5.6786,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 8, step: 14
	action: tensor([[ 6.1800,  5.5218,  6.1800,  5.8314, -6.2800, -5.4598,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 8, step: 15
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -5.9512, -5.6908,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0192428817719053 entropy 1.4189385332046724
epoch: 8, step: 16
	action: tensor([[ 5.6871,  6.1800,  4.9368,  6.1800, -6.2800, -5.0380,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 8, step: 17
	action: tensor([[ 5.6370,  5.8444,  6.1800,  6.1800, -6.2800, -5.2052,  4.3771]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 8, step: 18
	action: tensor([[ 6.1800,  6.1800,  5.2224,  6.1800, -6.2800, -4.4459,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 8, step: 19
	action: tensor([[ 5.3022,  6.1800,  6.1800,  6.1800, -6.0436, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 8, step: 20
	action: tensor([[ 3.4360,  5.9059,  5.1851,  5.0534, -6.1661, -6.2800,  5.6510]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 8, step: 21
	action: tensor([[ 5.8259,  6.0027,  5.3727,  4.4973, -6.2289, -5.9457,  4.9503]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 8, step: 22
	action: tensor([[ 6.1800,  5.6522,  5.7007,  6.1800, -3.8615, -6.1463,  5.6426]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 8, step: 23
	action: tensor([[ 5.8303,  6.1800,  6.1800,  5.4707, -6.2800, -5.7250,  5.8942]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 8, step: 24
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1378603649183099 entropy 1.4189385332046724
epoch: 8, step: 25
	action: tensor([[ 5.1878,  5.6129,  6.1800,  4.4316, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 8, step: 26
	action: tensor([[ 5.6500,  6.1800,  6.1800,  6.1800, -6.1198, -5.9404,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 8, step: 27
	action: tensor([[ 6.1800,  6.1800,  6.1800,  5.9778, -5.9209, -6.2800,  5.5833]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 8, step: 28
	action: tensor([[ 5.2826,  6.1800,  6.1800,  6.1800, -6.2688, -6.2800,  5.1574]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 8, step: 29
	action: tensor([[ 6.1800,  5.2003,  6.0589,  4.7953, -5.8268, -6.2800,  4.3339]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 8, step: 30
	action: tensor([[ 6.1800,  6.1800,  5.8930,  6.1800, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 8, step: 31
	action: tensor([[ 6.1800,  6.0245,  6.1800,  6.1271, -6.2800, -6.2800,  6.0502]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 8, step: 32
	action: tensor([[ 6.1800,  5.3348,  6.1800,  6.0287, -6.2800, -6.2800,  5.3152]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 8, step: 33
	action: tensor([[ 6.1800,  4.7720,  6.1800,  5.3191, -5.5875, -5.6398,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 8, step: 34
	action: tensor([[ 6.1800,  6.0928,  6.1800,  4.8094, -6.2800, -6.2800,  5.4829]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 8, step: 35
	action: tensor([[ 6.1068,  5.3540,  6.1800,  6.0167, -5.6957, -6.2800,  5.7475]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 8, step: 36
	action: tensor([[ 5.9478,  6.1800,  6.1800,  5.9732, -6.2800, -6.0351,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 8, step: 37
	action: tensor([[ 3.5588,  6.1800,  6.1800,  6.1800, -5.4758, -6.1379,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 8, step: 38
	action: tensor([[ 6.1800,  4.9345,  6.1800,  6.1800, -6.2800, -6.2800,  5.9755]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 8, step: 39
	action: tensor([[ 4.3425,  6.0044,  6.1800,  5.6960, -6.1225, -4.7201,  4.5364]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 8, step: 40
	action: tensor([[ 6.0714,  5.6535,  6.1800,  6.1800, -4.9619, -5.7605,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 8, step: 41
	action: tensor([[ 4.4794,  6.1800,  6.1800,  6.1800, -6.2800, -4.7335,  5.9401]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 8, step: 42
	action: tensor([[ 6.1800,  6.1800,  3.0425,  6.1800, -5.4066, -6.2035,  5.9547]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 8, step: 43
	action: tensor([[ 6.1800,  5.4402,  5.4913,  4.5002, -6.1579, -5.8453,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 8, step: 44
	action: tensor([[ 5.5889,  6.1800,  6.1800,  5.3678, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 8, step: 45
	action: tensor([[ 6.1800,  6.1800,  6.1781,  5.1051, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 8, step: 46
	action: tensor([[ 5.3398,  6.1800,  6.1800,  6.1800, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 8, step: 47
	action: tensor([[ 6.1800,  6.1800,  6.1800,  5.6622, -5.4184, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 8, step: 48
	action: tensor([[ 6.1800,  6.1800,  6.1800,  5.5143, -4.4243, -6.0092,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 8, step: 49
	action: tensor([[ 4.1106,  6.1800,  6.1800,  6.1800, -5.6586, -4.6961,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 8, step: 50
	action: tensor([[ 5.9371,  6.1800,  3.6105,  6.1800, -4.6607, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 8, step: 51
	action: tensor([[ 6.1795,  6.1800,  4.4516,  6.1800, -6.2800, -4.9604,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 8, step: 52
	action: tensor([[ 6.1552,  5.9572,  6.1800,  6.1800, -6.2800, -6.2800,  4.4498]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 8, step: 53
	action: tensor([[ 5.4803,  4.6188,  5.5344,  6.1800, -6.2800, -4.7861,  5.5498]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 8, step: 54
	action: tensor([[ 6.1800,  6.1800,  5.1768,  5.0236, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 8, step: 55
	action: tensor([[ 5.5342,  6.1800,  5.6603,  6.1800, -5.8336, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.5672992887533304 entropy 1.4189385332046724
epoch: 8, step: 56
	action: tensor([[ 5.0182,  5.3218,  5.6137,  6.1800, -6.2800, -6.2800,  5.4189]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 8, step: 57
	action: tensor([[ 6.1800,  6.1800,  5.8362,  6.1800, -6.2800, -5.2606,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 8, step: 58
	action: tensor([[ 6.0507,  6.1800,  5.5232,  6.1800, -6.2800, -5.4962,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.184164076393658 entropy 1.4189385332046724
epoch: 8, step: 59
	action: tensor([[ 6.1800,  5.2548,  6.1800,  5.4399, -5.6759, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 8, step: 60
	action: tensor([[ 5.8574,  6.1800,  6.1800,  6.1521, -5.9154, -4.4928,  6.1357]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 8, step: 61
	action: tensor([[ 5.6981,  6.1800,  4.2917,  5.1528, -5.5745, -5.8398,  6.1745]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 8, step: 62
	action: tensor([[ 6.1800,  6.1800,  6.1800,  4.6275, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 8, step: 63
	action: tensor([[ 5.4503,  5.3985,  5.8790,  5.5747, -6.2800, -6.0920,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 8, step: 64
	action: tensor([[ 4.7325,  6.1800,  6.1800,  5.4974, -5.1212, -5.1809,  5.1481]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 8, step: 65
	action: tensor([[ 4.6876,  6.1800,  6.1800,  6.1800, -6.2800, -6.2800,  5.1334]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 8, step: 66
	action: tensor([[ 6.1800,  6.1800,  5.7941,  5.1067, -6.2800, -4.9835,  5.6799]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 8, step: 67
	action: tensor([[ 5.5136,  4.2787,  6.1800,  5.9094, -5.0994, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 8, step: 68
	action: tensor([[ 6.1800,  5.0448,  5.3165,  6.1800, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 8, step: 69
	action: tensor([[ 6.1800,  6.1800,  3.8095,  6.1800, -5.7495, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 8, step: 70
	action: tensor([[ 6.1800,  6.1800,  6.1800,  5.5382, -6.2800, -5.4853,  5.8670]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 8, step: 71
	action: tensor([[ 6.1800,  6.1800,  5.9600,  4.5166, -4.1943, -6.0314,  4.6998]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 8, step: 72
	action: tensor([[ 6.1800,  6.1800,  4.2863,  3.4285, -6.0193, -5.3069,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 8, step: 73
	action: tensor([[ 6.1800,  6.1800,  5.3536,  6.1800, -6.0377, -5.4622,  5.4149]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 8, step: 74
	action: tensor([[ 6.1800,  6.1800,  5.1848,  3.3642, -5.8644, -5.9415,  3.8066]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 8, step: 75
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -4.1733, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 8, step: 76
	action: tensor([[ 5.0051,  4.7428,  6.1800,  5.2544, -6.2800, -5.3930,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 8, step: 77
	action: tensor([[ 6.1800,  6.1800,  5.9524,  6.1800, -4.5107, -6.2567,  5.0352]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1106603003286077 entropy 1.4189385332046724
epoch: 8, step: 78
	action: tensor([[ 6.1543,  6.1800,  5.9934,  6.1800, -6.2800, -6.2800,  4.8630]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1181126499681124 entropy 1.4189385332046724
epoch: 8, step: 79
	action: tensor([[ 5.9463,  6.1800,  6.1800,  5.1305, -6.2800, -6.2800,  5.6290]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 8, step: 80
	action: tensor([[ 6.1800,  6.1218,  6.1800,  6.1800, -5.2838, -6.2800,  5.5789]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 8, step: 81
	action: tensor([[ 6.1800,  6.1800,  6.1800,  5.4776, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 8, step: 82
	action: tensor([[ 5.7739,  6.1800,  3.9612,  6.1800, -5.5527, -5.7940,  6.0074]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4110090689842256 entropy 1.4189385332046724
epoch: 8, step: 83
	action: tensor([[ 5.9482,  6.1800,  6.1800,  5.7887, -6.2800, -4.9025,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 8, step: 84
	action: tensor([[ 5.9789,  6.1800,  5.1799,  5.5537, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 8, step: 85
	action: tensor([[ 6.1281,  6.1800,  6.1757,  6.1800, -5.4417, -5.9124,  4.9820]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0571567857418425 entropy 1.4189385332046724
epoch: 8, step: 86
	action: tensor([[ 5.1557,  5.9782,  4.3659,  4.9927, -5.3757, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 8, step: 87
	action: tensor([[ 3.8911,  6.1800,  6.1571,  6.1800, -5.5064, -6.2800,  5.9026]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 8, step: 88
	action: tensor([[ 5.8199,  6.1800,  5.7635,  5.4856, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2637037955174102 entropy 1.4189385332046724
epoch: 8, step: 89
	action: tensor([[ 6.1800,  6.1800,  6.1800,  4.6714, -5.2086, -6.0449,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 8, step: 90
	action: tensor([[ 5.8187,  6.1800,  6.1800,  5.6102, -6.2800, -6.2800,  6.1701]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 8, step: 91
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.0486, -6.2800, -5.3775,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0368136792648477 entropy 1.4189385332046724
epoch: 8, step: 92
	action: tensor([[ 5.1340,  5.8840,  6.1800,  5.5418, -5.0272, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 8, step: 93
	action: tensor([[ 4.1477,  6.1800,  6.1800,  6.1800, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 8, step: 94
	action: tensor([[ 4.7864,  5.2644,  6.1800,  6.1800, -5.9690, -5.8160,  5.5834]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 8, step: 95
	action: tensor([[ 6.1800,  5.6182,  6.1800,  6.1800, -6.1448, -5.1431,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 8, step: 96
	action: tensor([[ 5.7829,  6.1800,  5.8960,  5.5258, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3933319070577324 entropy 1.4189385332046724
epoch: 8, step: 97
	action: tensor([[ 5.0043,  6.1800,  6.1800,  5.3114, -5.6348, -5.6144,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 8, step: 98
	action: tensor([[ 6.1640,  6.1800,  6.1800,  5.0721, -6.2800, -6.2800,  5.7611]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 8, step: 99
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -6.2800, -4.9019,  4.7895]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 8, step: 100
	action: tensor([[ 6.1333,  6.1800,  6.1800,  5.0418, -6.0469, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 8, step: 101
	action: tensor([[ 3.6142,  5.5921,  6.1800,  6.1800, -5.6407, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 8, step: 102
	action: tensor([[ 6.1800,  4.9532,  5.5393,  4.2349, -6.2800, -5.7867,  6.0230]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 8, step: 103
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 8, step: 104
	action: tensor([[ 5.2876,  6.1800,  5.7298,  6.1800, -5.2431, -6.1415,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 8, step: 105
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1372024297505734 entropy 1.4189385332046724
epoch: 8, step: 106
	action: tensor([[ 6.1800,  5.8326,  3.7386,  6.1800, -5.9074, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 8, step: 107
	action: tensor([[ 5.4883,  5.7088,  6.1290,  4.8314, -6.2800, -6.2800,  4.1848]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 8, step: 108
	action: tensor([[ 6.1800,  5.3170,  5.9334,  5.3002, -6.1382, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 8, step: 109
	action: tensor([[ 6.1800,  6.1800,  5.0560,  3.5017, -6.2800, -5.5049,  4.3232]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 8, step: 110
	action: tensor([[ 6.1672,  6.1800,  5.9159,  6.1800, -5.4820, -6.0082,  5.1353]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 8, step: 111
	action: tensor([[ 6.1800,  6.1800,  6.1800,  5.6191, -5.5985, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 8, step: 112
	action: tensor([[ 5.6514,  5.6459,  5.4258,  5.9786, -6.2800, -5.9669,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 8, step: 113
	action: tensor([[ 5.8510,  6.1800,  5.9290,  6.1800, -5.9640, -6.0947,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 8, step: 114
	action: tensor([[ 6.1800,  5.6920,  6.1800,  6.0709, -6.2800, -6.1083,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 8, step: 115
	action: tensor([[ 5.1628,  6.1800,  6.1800,  6.1800, -6.2800, -6.2800,  4.9920]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 8, step: 116
	action: tensor([[ 6.1800,  5.0047,  6.1800,  6.1800, -6.2800, -5.4795,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 8, step: 117
	action: tensor([[ 6.1800,  4.8908,  4.3677,  6.1800, -6.2800, -6.2031,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 8, step: 118
	action: tensor([[ 6.1800,  6.1800,  4.6542,  5.2974, -6.2800, -6.0177,  5.7621]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 8, step: 119
	action: tensor([[ 5.9645,  6.1800,  6.1800,  6.1800, -6.2800, -6.2800,  5.8535]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2701169583909833 entropy 1.4189385332046724
epoch: 8, step: 120
	action: tensor([[ 5.7287,  6.1800,  5.0312,  6.1800, -6.2800, -6.2800,  5.0525]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 8, step: 121
	action: tensor([[ 5.8783,  5.5246,  6.1800,  4.5674, -5.0847, -6.2800,  5.8113]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 8, step: 122
	action: tensor([[ 6.0386,  6.1800,  5.6073,  6.1800, -4.9286, -5.8000,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0665962508890738 entropy 1.4189385332046724
epoch: 8, step: 123
	action: tensor([[ 6.1800,  6.1800,  5.8188,  5.6513, -4.5984, -5.2732,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 8, step: 124
	action: tensor([[ 6.1800,  5.4600,  5.2339,  6.1800, -4.8913, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 8, step: 125
	action: tensor([[ 6.1800,  6.1800,  5.7280,  6.1800, -6.2800, -6.2800,  5.2286]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 8, step: 126
	action: tensor([[ 5.4851,  6.1800,  6.1800,  6.1800, -5.7598, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 8, step: 127
	action: tensor([[ 5.7410,  6.1800,  5.7021,  5.6420, -6.2800, -5.9980,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-2.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
LOSS epoch 8 actor 985.2437771952916 critic 2254.275261031518 entropy 100
epoch: 9, step: 0
	action: tensor([[ 6.1800,  6.1800,  6.1626,  5.3632, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 9, step: 1
	action: tensor([[ 6.1800,  6.1800,  6.1800,  4.4968, -6.0789, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 9, step: 2
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -6.2800, -6.2800,  5.1826]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.136846822205375 entropy 1.4189385332046724
epoch: 9, step: 3
	action: tensor([[ 6.1800,  5.6705,  4.8288,  6.1800, -5.8709, -5.7047,  5.5978]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 9, step: 4
	action: tensor([[ 6.1800,  6.1538,  6.1800,  6.1800, -6.2800, -5.5721,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 9, step: 5
	action: tensor([[ 6.1800,  4.9468,  5.2465,  5.5776, -6.2800, -6.2800,  5.3531]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 9, step: 6
	action: tensor([[ 6.1800,  6.1800,  5.5819,  6.1800, -5.9136, -5.8718,  5.0106]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9991466597972437 entropy 1.4189385332046724
epoch: 9, step: 7
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 9, step: 8
	action: tensor([[ 5.8277,  6.1730,  4.8522,  5.0571, -6.2800, -5.3484,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 9, step: 9
	action: tensor([[ 6.1800,  6.1800,  4.8642,  5.8261, -5.9246, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 9, step: 10
	action: tensor([[ 6.0357,  5.6411,  6.1800,  6.1800, -5.8033, -5.8458,  6.0506]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 9, step: 11
	action: tensor([[ 6.1800,  5.8923,  6.1800,  6.1800, -6.2800, -5.7880,  5.3900]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 9, step: 12
	action: tensor([[ 6.1800,  5.6140,  6.1800,  6.1172, -4.9940, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 9, step: 13
	action: tensor([[ 6.0404,  5.5803,  6.1800,  5.0425, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 9, step: 14
	action: tensor([[ 6.1800,  5.5172,  5.5056,  5.4288, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 9, step: 15
	action: tensor([[ 6.1800,  5.6692,  6.1800,  6.1800, -4.3403, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 9, step: 16
	action: tensor([[ 6.1800,  6.0974,  6.1800,  4.9044, -5.2840, -6.2800,  6.1654]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 9, step: 17
	action: tensor([[ 5.9527,  4.4508,  6.1800,  5.0608, -4.7373, -6.2800,  5.9093]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 9, step: 18
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1200870424783083 entropy 1.4189385332046724
epoch: 9, step: 19
	action: tensor([[ 6.1800,  6.1800,  6.1800,  4.3900, -6.2415, -5.3139,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 9, step: 20
	action: tensor([[ 6.1800,  6.0803,  4.0675,  6.1800, -6.2800, -5.2111,  4.4735]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 9, step: 21
	action: tensor([[ 3.2787,  5.8133,  6.1800,  5.4970, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 9, step: 22
	action: tensor([[ 6.1800,  5.9082,  5.5624,  6.1800, -6.0396, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 9, step: 23
	action: tensor([[ 6.1800,  5.3347,  6.1327,  6.0914, -6.2800, -5.4292,  6.1175]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 9, step: 24
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -6.2800, -6.1312,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 9, step: 25
	action: tensor([[ 5.8637,  6.1800,  6.1800,  6.1800, -6.2800, -6.2800,  5.5073]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 9, step: 26
	action: tensor([[ 6.0895,  5.2258,  6.1800,  5.2328, -6.2800, -5.7850,  4.1101]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 9, step: 27
	action: tensor([[ 6.1574,  5.9072,  6.1800,  6.1800, -4.3318, -5.1333,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 9, step: 28
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -6.1197, -6.2800,  5.5353]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 9, step: 29
	action: tensor([[ 3.8005,  6.1800,  6.1800,  5.9431, -6.2800, -5.7239,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 9, step: 30
	action: tensor([[ 5.3320,  6.1800,  5.8403,  6.0776, -5.6659, -6.2800,  5.4004]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 9, step: 31
	action: tensor([[ 6.1800,  6.1800,  5.4337,  6.1800, -6.2800, -4.9467,  3.4680]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 9, step: 32
	action: tensor([[ 6.1800,  4.8530,  5.7186,  5.2938, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 9, step: 33
	action: tensor([[ 5.9686,  6.1800,  6.1343,  4.7382, -6.2800, -5.6229,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 9, step: 34
	action: tensor([[ 6.1800,  6.0906,  6.1800,  4.6114, -4.1611, -5.6637,  5.6590]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 9, step: 35
	action: tensor([[ 6.1800,  5.8185,  4.7384,  4.4589, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 9, step: 36
	action: tensor([[ 5.7901,  5.8436,  5.0764,  6.1800, -6.1919, -5.1949,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 9, step: 37
	action: tensor([[ 6.1800,  4.9077,  6.1800,  6.1800, -5.3070, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 9, step: 38
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1182, -6.2800, -6.1606,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 9, step: 39
	action: tensor([[ 5.7506,  6.1800,  6.1800,  6.1431, -5.9849, -6.2800,  5.9057]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 9, step: 40
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -6.2800, -6.0024,  6.0538]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1453615622831375 entropy 1.4189385332046724
epoch: 9, step: 41
	action: tensor([[ 6.1800,  5.7011,  6.1800,  5.8198, -4.5664, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 9, step: 42
	action: tensor([[ 6.1800,  6.1800,  4.9450,  6.1800, -6.2800, -5.4437,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 9, step: 43
	action: tensor([[ 6.1800,  6.1800,  5.3556,  6.1800, -6.2800, -5.6856,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 9, step: 44
	action: tensor([[ 6.1800,  6.1800,  5.6570,  6.1800, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 9, step: 45
	action: tensor([[ 5.2948,  6.1800,  6.0633,  6.1800, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.640067269626256 entropy 1.4189385332046724
epoch: 9, step: 46
	action: tensor([[ 6.1800,  5.8224,  6.1800,  5.1299, -5.6272, -4.6630,  6.1183]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 9, step: 47
	action: tensor([[ 6.1244,  6.1800,  6.1800,  6.1800, -6.2800, -5.5821,  5.6328]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0576241595359293 entropy 1.4189385332046724
epoch: 9, step: 48
	action: tensor([[ 5.5982,  5.8798,  6.1800,  5.1633, -6.2800, -6.2800,  5.1208]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 9, step: 49
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -5.0768, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.394264469519027 entropy 1.4189385332046724
epoch: 9, step: 50
	action: tensor([[ 6.1800,  6.1800,  6.1800,  5.1899, -6.2800, -6.2800,  4.9730]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 9, step: 51
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -6.2800, -6.2800,  5.6079]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 9, step: 52
	action: tensor([[ 6.1800,  6.1800,  6.1800,  5.8457, -5.9908, -5.4429,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 9, step: 53
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 9, step: 54
	action: tensor([[ 6.1800,  4.3396,  6.1800,  6.1800, -6.2800, -4.2388,  6.1309]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 9, step: 55
	action: tensor([[ 4.6712,  6.1800,  5.4176,  6.1800, -5.8098, -5.5741,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 9, step: 56
	action: tensor([[ 6.1800,  5.3942,  6.1800,  6.1800, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 9, step: 57
	action: tensor([[ 6.1382,  5.8856,  6.1800,  6.1800, -6.2800, -6.0139,  5.8694]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 9, step: 58
	action: tensor([[ 6.1800,  5.7684,  6.1532,  5.3390, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 9, step: 59
	action: tensor([[ 4.8755,  6.1800,  6.1800,  6.1800, -4.4814, -6.2800,  4.8535]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 9, step: 60
	action: tensor([[ 4.9917,  5.2676,  6.1800,  6.1800, -5.9842, -6.2502,  6.0590]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 9, step: 61
	action: tensor([[ 5.9611,  5.6799,  6.1800,  5.7043, -6.2774, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 9, step: 62
	action: tensor([[ 6.1800,  4.9201,  6.1800,  6.1800, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 9, step: 63
	action: tensor([[ 4.3522,  6.1800,  6.1800,  6.1800, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 9, step: 64
	action: tensor([[ 6.1800,  6.1131,  6.1439,  4.8597, -4.6672, -6.2800,  6.0769]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 9, step: 65
	action: tensor([[ 6.1800,  6.1705,  5.5693,  6.1800, -6.2800, -6.2800,  5.6477]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 9, step: 66
	action: tensor([[ 6.1369,  4.0050,  6.1800,  6.0152, -6.2800, -5.7632,  6.1458]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 9, step: 67
	action: tensor([[ 5.2463,  6.1800,  5.7010,  5.0657, -5.0512, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 9, step: 68
	action: tensor([[ 6.1800,  6.1800,  5.0601,  5.0388, -6.2800, -6.2800,  5.4490]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 9, step: 69
	action: tensor([[ 5.8984,  4.8333,  5.2530,  6.1800, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 9, step: 70
	action: tensor([[ 6.0869,  5.0677,  6.1800,  6.1800, -5.7743, -4.1384,  6.0923]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 9, step: 71
	action: tensor([[ 5.9791,  5.9166,  5.4401,  6.1800, -6.2800, -5.3518,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 9, step: 72
	action: tensor([[ 6.1800,  6.1800,  5.9019,  6.1800, -6.1307, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1068103358226768 entropy 1.4189385332046724
epoch: 9, step: 73
	action: tensor([[ 6.1800,  4.7036,  6.1800,  6.1800, -6.2800, -5.0987,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 9, step: 74
	action: tensor([[ 6.1800,  5.7912,  5.7146,  6.1604, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 9, step: 75
	action: tensor([[ 5.6333,  5.9405,  6.1800,  6.1800, -4.9192, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 9, step: 76
	action: tensor([[ 5.6548,  5.5415,  6.1800,  5.0888, -6.2800, -5.2134,  4.1786]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 9, step: 77
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -5.3042, -5.6542,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 9, step: 78
	action: tensor([[ 4.7545,  4.6518,  6.1800,  6.1800, -6.2800, -4.9006,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 9, step: 79
	action: tensor([[ 6.1800,  6.1800,  6.1800,  5.0959, -5.4141, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 9, step: 80
	action: tensor([[ 5.2414,  6.1800,  5.6705,  6.1800, -6.2800, -5.9750,  5.6323]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.6724384369346947 entropy 1.4189385332046724
epoch: 9, step: 81
	action: tensor([[ 6.1368,  6.1800,  6.1800,  6.0963, -6.2290, -4.6091,  5.8711]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 9, step: 82
	action: tensor([[ 6.1800,  5.0626,  6.1800,  6.1800, -6.2800, -5.9266,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 9, step: 83
	action: tensor([[ 5.4682,  5.4324,  5.4519,  6.1800, -5.8975, -6.2774,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 9, step: 84
	action: tensor([[ 4.5760,  5.4638,  6.1800,  4.6977, -6.1141, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 9, step: 85
	action: tensor([[ 6.1800,  6.1800,  6.0615,  6.1800, -6.0833, -4.6621,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 9, step: 86
	action: tensor([[ 6.1800,  6.1800,  6.1800,  5.4209, -5.0861, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 9, step: 87
	action: tensor([[ 6.1800,  4.3956,  6.1800,  5.9775, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 9, step: 88
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -6.2800, -6.2800,  5.4292]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 9, step: 89
	action: tensor([[ 4.9121,  5.8866,  6.1800,  6.1800, -5.5290, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 9, step: 90
	action: tensor([[ 6.1800,  6.0981,  6.1800,  6.1800, -4.7703, -6.2800,  5.7310]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 9, step: 91
	action: tensor([[ 6.1800,  6.1800,  5.8704,  4.9657, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 9, step: 92
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1054, -6.2800, -6.1020,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 9, step: 93
	action: tensor([[ 5.0854,  5.8520,  6.1343,  6.1800, -6.0527, -6.2055,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 9, step: 94
	action: tensor([[ 5.9332,  6.1800,  6.1800,  5.6695, -6.2800, -5.8597,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 9, step: 95
	action: tensor([[ 6.1800,  6.1800,  5.7068,  5.3925, -6.1385, -4.9432,  6.1760]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 9, step: 96
	action: tensor([[ 4.8027,  6.1800,  6.0102,  6.1675, -6.2800, -6.2800,  5.9515]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 9, step: 97
	action: tensor([[ 5.1809,  4.4630,  5.0196,  5.3290, -6.2800, -5.4368,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 9, step: 98
	action: tensor([[ 4.4011,  5.6152,  5.4650,  6.1099, -6.2800, -6.1723,  4.6326]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 9, step: 99
	action: tensor([[ 6.1800,  5.1746,  6.1800,  5.6404, -6.0269, -3.8010,  5.6211]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 9, step: 100
	action: tensor([[ 6.1800,  5.0977,  4.9091,  6.1800, -6.0636, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 9, step: 101
	action: tensor([[ 5.4142,  5.0675,  6.1800,  6.1800, -5.7792, -6.2800,  5.3222]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 9, step: 102
	action: tensor([[ 6.1800,  5.4649,  4.8895,  4.6366, -6.0152, -6.2800,  4.9491]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 9, step: 103
	action: tensor([[ 4.9379,  6.1800,  6.1800,  5.5486, -5.3932, -4.8240,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 9, step: 104
	action: tensor([[ 5.8419,  6.1800,  6.1271,  5.8629, -6.2800, -6.2800,  5.8758]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3452772376821331 entropy 1.4189385332046724
epoch: 9, step: 105
	action: tensor([[ 6.1800,  5.0750,  6.1800,  5.9245, -6.2800, -6.2800,  4.9365]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 9, step: 106
	action: tensor([[ 6.0120,  5.8052,  6.1800,  6.1800, -5.7837, -5.8173,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 9, step: 107
	action: tensor([[ 6.1800,  6.1800,  6.1800,  4.9496, -6.2800, -6.2800,  6.1197]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 9, step: 108
	action: tensor([[ 5.0251,  6.1800,  5.7505,  6.1800, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.7589260153017172 entropy 1.4189385332046724
epoch: 9, step: 109
	action: tensor([[ 5.5084,  6.1800,  6.1800,  6.1800, -6.1578, -6.2800,  4.9079]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 9, step: 110
	action: tensor([[ 6.1800,  6.1800,  6.1800,  5.1686, -6.2800, -6.2800,  5.9167]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 9, step: 111
	action: tensor([[ 6.1800,  5.5799,  6.1800,  6.1800, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 9, step: 112
	action: tensor([[ 5.4591,  4.4962,  4.9432,  5.8009, -6.2800, -6.2800,  5.9401]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 9, step: 113
	action: tensor([[ 6.1800,  5.7902,  4.4203,  6.1800, -6.2800, -6.2800,  5.7575]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 9, step: 114
	action: tensor([[ 6.1800,  6.1800,  6.1800,  5.0860, -5.4859, -5.8851,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 9, step: 115
	action: tensor([[ 5.1671,  6.1800,  6.1800,  4.1962, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 9, step: 116
	action: tensor([[ 6.1800,  6.1800,  6.1800,  4.6320, -6.2800, -5.1501,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 9, step: 117
	action: tensor([[ 4.5494,  5.6965,  6.1800,  6.1800, -4.0961, -5.7237,  5.9642]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 9, step: 118
	action: tensor([[ 5.3562,  5.8864,  6.1800,  5.2543, -6.2800, -6.2800,  6.0144]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 9, step: 119
	action: tensor([[ 6.1800,  6.0669,  5.8479,  5.4316, -6.2800, -6.2494,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 9, step: 120
	action: tensor([[ 6.1800,  6.1800,  4.6356,  6.1800, -5.6385, -6.1970,  5.6546]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 9, step: 121
	action: tensor([[ 5.5438,  6.1800,  5.6840,  3.8472, -6.0557, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 9, step: 122
	action: tensor([[ 4.8280,  6.1800,  6.1800,  5.0350, -6.2800, -4.8751,  5.8848]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 9, step: 123
	action: tensor([[ 6.1800,  6.1800,  5.2347,  6.1800, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 9, step: 124
	action: tensor([[ 6.1800,  6.1800,  5.6470,  6.1800, -6.2800, -5.2247,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0580985723711827 entropy 1.4189385332046724
epoch: 9, step: 125
	action: tensor([[ 6.1800,  6.1800,  6.1800,  4.8517, -6.0493, -6.2334,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 9, step: 126
	action: tensor([[ 6.1800,  6.1800,  5.1547,  6.0951, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 9, step: 127
	action: tensor([[ 6.1800,  6.1800,  5.4975,  6.1800, -5.0848, -4.2030,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-4.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
LOSS epoch 9 actor 903.1701997868886 critic 2090.1279730450005 entropy 100
epoch: 10, step: 0
	action: tensor([[ 6.1800,  4.8588,  6.1800,  6.1800, -6.2800, -6.2800,  5.9580]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 10, step: 1
	action: tensor([[ 6.1800,  6.1800,  6.1634,  6.1800, -6.2800, -5.6797,  5.9038]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.030830672711993 entropy 1.4189385332046724
epoch: 10, step: 2
	action: tensor([[ 5.3581,  6.1800,  6.1800,  5.1104, -4.3698, -6.2800,  6.1662]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 10, step: 3
	action: tensor([[ 4.1717,  6.1800,  6.1800,  5.8902, -6.2800, -6.2800,  5.9788]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 10, step: 4
	action: tensor([[ 6.1800,  4.8975,  5.7679,  4.2962, -6.2800, -5.2017,  4.4129]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 10, step: 5
	action: tensor([[ 4.9132,  5.2557,  6.1800,  5.6586, -6.2800, -6.2800,  5.5517]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 10, step: 6
	action: tensor([[ 6.1800,  5.3998,  5.3388,  6.1800, -6.0798, -5.4280,  4.6369]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 10, step: 7
	action: tensor([[ 6.1800,  5.7902,  6.0570,  6.1800, -5.7231, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2552430503024268 entropy 1.4189385332046724
epoch: 10, step: 8
	action: tensor([[ 5.6793,  6.1800,  6.1800,  6.1800, -6.2430, -5.7383,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 10, step: 9
	action: tensor([[ 6.1800,  5.7315,  6.0302,  6.1800, -6.0143, -6.2800,  5.6318]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 10, step: 10
	action: tensor([[ 4.2161,  5.9285,  5.8397,  6.0412, -6.0177, -5.4453,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 10, step: 11
	action: tensor([[ 6.1800,  5.5806,  5.0006,  6.0896, -5.3208, -6.2800,  4.9547]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 10, step: 12
	action: tensor([[ 6.1800,  5.3559,  5.5230,  6.1800, -6.2800, -5.0689,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 10, step: 13
	action: tensor([[ 6.1800,  6.1269,  6.1800,  6.1800, -5.9407, -6.2777,  5.4394]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 10, step: 14
	action: tensor([[ 5.2837,  6.1800,  5.5717,  6.1800, -5.9290, -6.2217,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.624695203576222 entropy 1.4189385332046724
epoch: 10, step: 15
	action: tensor([[ 4.2199,  5.8762,  6.1800,  5.1756, -6.2800, -6.1959,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 10, step: 16
	action: tensor([[ 5.7818,  6.1800,  5.8075,  6.1800, -6.2800, -6.2800,  4.9556]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 10, step: 17
	action: tensor([[ 6.1800,  6.1800,  6.1800,  4.9670, -5.3237, -6.2800,  5.8586]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 10, step: 18
	action: tensor([[ 4.9746,  5.8121,  5.8396,  6.0621, -4.2853, -5.9261,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 10, step: 19
	action: tensor([[ 5.8186,  5.5540,  6.1800,  3.9419, -6.2800, -5.6816,  4.9365]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 10, step: 20
	action: tensor([[ 6.1800,  6.1800,  6.1800,  5.6174, -5.3889, -6.0015,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2675762268197368 entropy 1.4189385332046724
epoch: 10, step: 21
	action: tensor([[ 6.0464,  6.1800,  6.1800,  5.5102, -6.2800, -4.4817,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 10, step: 22
	action: tensor([[ 4.1797,  6.1149,  5.9876,  5.7849, -5.4228, -6.2800,  5.8122]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 10, step: 23
	action: tensor([[ 6.1800,  6.0377,  5.0378,  6.1562, -6.2183, -6.2800,  5.0347]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 10, step: 24
	action: tensor([[ 6.1800,  5.5002,  6.1800,  5.7177, -6.2800, -4.7516,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 10, step: 25
	action: tensor([[ 5.4493,  5.2977,  4.7370,  6.1800, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 10, step: 26
	action: tensor([[ 4.5269,  5.5119,  6.1800,  6.0614, -6.2614, -5.1299,  5.8076]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 10, step: 27
	action: tensor([[ 6.1800,  5.6529,  6.1800,  6.1800, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 10, step: 28
	action: tensor([[ 6.1800,  5.1894,  6.1800,  6.1800, -5.6840, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 10, step: 29
	action: tensor([[ 6.1800,  5.7009,  6.1800,  6.1800, -6.2800, -5.9132,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 10, step: 30
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -5.3089, -6.2800,  5.4082]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 10, step: 31
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -6.2800, -5.3083,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 10, step: 32
	action: tensor([[ 5.8906,  6.0209,  6.1800,  4.6049, -6.2107, -6.2800,  6.1686]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 10, step: 33
	action: tensor([[ 6.1800,  4.3938,  5.9617,  4.1241, -4.9608, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 10, step: 34
	action: tensor([[ 6.1800,  6.1800,  6.1800,  5.1342, -6.0002, -5.7980,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 10, step: 35
	action: tensor([[ 5.9788,  6.1800,  6.1800,  6.1800, -6.2800, -5.1258,  4.8727]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 10, step: 36
	action: tensor([[ 6.1316,  5.0330,  6.1437,  6.1170, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 10, step: 37
	action: tensor([[ 5.4036,  6.0520,  6.1800,  5.2237, -6.2800, -5.4406,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 10, step: 38
	action: tensor([[ 5.5158,  4.0163,  5.6397,  6.1800, -6.2800, -6.0392,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 10, step: 39
	action: tensor([[ 6.1800,  6.1800,  5.8821,  6.1800, -6.2800, -5.2107,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 10, step: 40
	action: tensor([[ 6.1800,  5.6162,  6.0041,  6.1800, -4.5384, -5.1632,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 10, step: 41
	action: tensor([[ 6.1800,  5.7333,  6.1800,  5.2690, -6.0259, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 10, step: 42
	action: tensor([[ 6.1800,  5.7117,  5.6760,  5.1481, -6.2800, -6.2800,  5.1007]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 10, step: 43
	action: tensor([[ 5.7711,  5.4653,  6.1800,  6.1800, -6.2800, -5.6489,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 10, step: 44
	action: tensor([[ 4.4234,  6.1800,  5.6163,  5.9071, -5.0786, -4.8514,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 10, step: 45
	action: tensor([[ 6.1800,  5.1056,  6.1800,  6.1800, -6.2800, -5.3871,  4.7651]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 10, step: 46
	action: tensor([[ 4.2596,  5.7053,  6.1800,  6.1800, -5.2650, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 10, step: 47
	action: tensor([[ 5.5926,  4.4983,  6.1800,  5.5710, -6.1993, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 10, step: 48
	action: tensor([[ 6.1800,  6.1800,  6.1744,  6.1800, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 10, step: 49
	action: tensor([[ 6.1800,  6.1800,  6.1800,  4.0928, -5.9365, -5.3676,  5.6215]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 10, step: 50
	action: tensor([[ 6.0926,  6.1800,  6.1800,  6.1800, -6.2800, -4.6439,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 10, step: 51
	action: tensor([[ 6.1800,  6.1800,  5.5025,  6.1800, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1089205078654296 entropy 1.4189385332046724
epoch: 10, step: 52
	action: tensor([[ 6.0876,  6.0455,  5.2647,  6.1800, -6.2800, -4.9174,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 10, step: 53
	action: tensor([[ 5.1184,  6.1576,  6.1800,  4.1111, -4.5306, -5.7752,  4.8661]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 10, step: 54
	action: tensor([[ 4.9490,  6.1800,  5.4589,  6.1800, -6.2800, -4.4966,  5.8715]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 10, step: 55
	action: tensor([[ 6.1800,  5.0769,  4.3847,  4.3670, -6.2800, -4.1591,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 10, step: 56
	action: tensor([[ 4.8599,  4.8308,  5.9753,  6.1800, -6.2800, -4.1921,  5.4446]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 10, step: 57
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.0966, -6.0468, -5.6926,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0492000089826865 entropy 1.4189385332046724
epoch: 10, step: 58
	action: tensor([[ 5.6148,  6.1800,  6.1800,  5.9101, -5.8840, -5.6145,  6.0587]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 10, step: 59
	action: tensor([[ 6.1800,  5.0582,  5.9769,  6.1800, -5.3528, -4.5161,  4.9453]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 10, step: 60
	action: tensor([[ 6.1800,  5.6908,  6.0888,  6.1800, -4.7361, -6.2765,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 10, step: 61
	action: tensor([[ 6.1800,  6.1800,  5.0168,  6.0773, -5.2546, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 10, step: 62
	action: tensor([[ 6.1800,  6.1800,  4.6887,  6.1800, -5.4345, -6.2800,  4.0890]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 10, step: 63
	action: tensor([[ 6.1800,  4.2384,  6.1800,  6.1800, -5.5389, -6.2800,  5.9208]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 10, step: 64
	action: tensor([[ 6.1493,  6.1800,  6.1800,  6.1800, -6.2800, -6.1555,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 10, step: 65
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -6.2800, -5.0784,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 10, step: 66
	action: tensor([[ 6.1800,  5.3610,  6.1800,  5.4922, -6.2536, -6.0713,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 10, step: 67
	action: tensor([[ 6.1800,  4.6301,  6.1274,  6.1800, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 10, step: 68
	action: tensor([[ 6.1800,  5.4822,  5.6919,  6.1800, -4.6637, -5.3467,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 10, step: 69
	action: tensor([[ 5.3209,  6.1800,  6.1800,  6.1800, -4.9939, -6.2800,  6.0747]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.6360949151817183 entropy 1.4189385332046724
epoch: 10, step: 70
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -5.1860, -5.9236,  5.2620]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 10, step: 71
	action: tensor([[ 6.1800,  6.1085,  5.6773,  6.1800, -6.1559, -6.2800,  3.8715]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 10, step: 72
	action: tensor([[ 6.0358,  6.1800,  5.2288,  6.1800, -6.2800, -5.6764,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2229909198983069 entropy 1.4189385332046724
epoch: 10, step: 73
	action: tensor([[ 6.1800,  6.1800,  6.1800,  5.7695, -6.2800, -5.6269,  4.7460]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 10, step: 74
	action: tensor([[ 5.9577,  6.1800,  6.1800,  5.1486, -6.2800, -6.2042,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 10, step: 75
	action: tensor([[ 5.0008,  6.1800,  6.1800,  5.8415, -5.7554, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.7920354325106458 entropy 1.4189385332046724
epoch: 10, step: 76
	action: tensor([[ 6.1800,  6.1800,  5.8443,  5.9985, -4.1173, -4.3094,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 10, step: 77
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1369407191138456 entropy 1.4189385332046724
epoch: 10, step: 78
	action: tensor([[ 5.9432,  5.7525,  5.4624,  4.9157, -6.2800, -5.6627,  5.1386]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 10, step: 79
	action: tensor([[ 6.1800,  6.1800,  6.1800,  5.9496, -6.2800, -6.2800,  5.6212]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 10, step: 80
	action: tensor([[ 6.1796,  5.5423,  4.6044,  6.1800, -6.2800, -5.8533,  5.9970]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 10, step: 81
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -4.9958, -5.5976,  3.8301]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 10, step: 82
	action: tensor([[ 5.3909,  6.1800,  3.8851,  5.2812, -6.0629, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 10, step: 83
	action: tensor([[ 5.1701,  5.0078,  6.1800,  6.1800, -6.2800, -4.9101,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 10, step: 84
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -5.5670, -5.6089,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9138808337098078 entropy 1.4189385332046724
epoch: 10, step: 85
	action: tensor([[ 5.7275,  5.9726,  6.1800,  6.1800, -6.2800, -6.2800,  6.1236]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 10, step: 86
	action: tensor([[ 6.0686,  5.4843,  6.1800,  3.9842, -6.2800, -5.6692,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 10, step: 87
	action: tensor([[ 4.8054,  5.9228,  5.3813,  5.0416, -6.2800, -5.6727,  6.1400]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 10, step: 88
	action: tensor([[ 5.8557,  5.7675,  6.1800,  6.1800, -6.2800, -5.2812,  4.4434]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 10, step: 89
	action: tensor([[ 6.1800,  6.1800,  6.0212,  6.1800, -4.9800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 10, step: 90
	action: tensor([[ 6.1800,  4.9816,  6.1800,  6.1800, -5.8427, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 10, step: 91
	action: tensor([[ 6.1800,  5.1555,  4.6895,  5.5854, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 10, step: 92
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -6.1174, -5.8731,  5.3223]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 10, step: 93
	action: tensor([[ 5.2113,  6.1158,  6.1800,  6.1800, -5.9298, -6.1478,  5.9645]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 10, step: 94
	action: tensor([[ 6.1800,  6.1800,  5.7089,  6.1800, -6.2800, -5.5078,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 10, step: 95
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 10, step: 96
	action: tensor([[ 5.4220,  5.7333,  6.1081,  5.1080, -5.9801, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 10, step: 97
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -5.8934, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0905627527282786 entropy 1.4189385332046724
epoch: 10, step: 98
	action: tensor([[ 3.9693,  6.1800,  5.7231,  6.1800, -6.2800, -6.2800,  4.7589]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 10, step: 99
	action: tensor([[ 5.9399,  5.1209,  5.4511,  3.9942, -6.2800, -6.2800,  5.2368]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 10, step: 100
	action: tensor([[ 4.4659,  5.5414,  6.1800,  6.1800, -6.1964, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 10, step: 101
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -6.2800, -6.2800,  5.9493]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 10, step: 102
	action: tensor([[ 6.0330,  5.3643,  4.4183,  6.1800, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 10, step: 103
	action: tensor([[ 6.1800,  6.1722,  6.1800,  6.1800, -4.9346, -6.2800,  5.6666]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1338929942853988 entropy 1.4189385332046724
epoch: 10, step: 104
	action: tensor([[ 6.1800,  6.1800,  5.7340,  5.8498, -6.2800, -5.3130,  5.5590]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 10, step: 105
	action: tensor([[ 6.1385,  6.1800,  6.1800,  6.1800, -5.6829, -4.8059,  5.3208]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.810000429194945 entropy 1.4189385332046724
epoch: 10, step: 106
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -5.6668, -5.1159,  5.9899]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 10, step: 107
	action: tensor([[ 6.1800,  6.1800,  5.1988,  6.1800, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 10, step: 108
	action: tensor([[ 6.1800,  6.1800,  4.9133,  6.1800, -6.2341, -5.5061,  4.5388]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 10, step: 109
	action: tensor([[ 6.1800,  5.1477,  6.1800,  5.9028, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 10, step: 110
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -4.0555, -6.0042,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 10, step: 111
	action: tensor([[ 5.1915,  5.9890,  4.1789,  4.9010, -5.0224, -6.2800,  5.3565]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 10, step: 112
	action: tensor([[ 6.1800,  4.5896,  6.1800,  6.1800, -5.2682, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 10, step: 113
	action: tensor([[ 6.1800,  5.9501,  6.1800,  6.0298, -6.2800, -5.6243,  5.8146]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 10, step: 114
	action: tensor([[ 4.7298,  5.9643,  5.6728,  6.1800, -5.1112, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 10, step: 115
	action: tensor([[ 5.7321,  6.1800,  5.7604,  4.9407, -4.9347, -6.2800,  5.4655]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 10, step: 116
	action: tensor([[ 6.1800,  5.4707,  5.7101,  5.8709, -6.2800, -5.0528,  6.1351]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 10, step: 117
	action: tensor([[ 6.1800,  6.1800,  6.0800,  6.1800, -5.3232, -4.7723,  5.0849]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 10, step: 118
	action: tensor([[ 5.2749,  4.1360,  6.1800,  6.1800, -5.7001, -5.7394,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 10, step: 119
	action: tensor([[ 6.0346,  6.1800,  6.1800,  6.1800, -5.4184, -6.2800,  4.9424]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 10, step: 120
	action: tensor([[ 5.6460,  6.1800,  6.1367,  5.8760, -6.2800, -6.2800,  5.4731]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 10, step: 121
	action: tensor([[ 6.1800,  6.1800,  6.1800,  5.6950, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 10, step: 122
	action: tensor([[ 6.1800,  6.0519,  6.1800,  6.1800, -6.2800, -4.2486,  5.4694]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 10, step: 123
	action: tensor([[ 6.1800,  5.4558,  6.0835,  5.3126, -6.2800, -5.9316,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 10, step: 124
	action: tensor([[ 6.1800,  6.1800,  5.3031,  6.1800, -6.2800, -6.2800,  4.4069]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4355148598274419 entropy 1.4189385332046724
epoch: 10, step: 125
	action: tensor([[ 6.1800,  6.1800,  4.3721,  6.1800, -6.2800, -5.9627,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 10, step: 126
	action: tensor([[ 5.1369,  5.9066,  5.9227,  5.0256, -6.2800, -6.2699,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 10, step: 127
	action: tensor([[ 4.2599,  5.9467,  3.8141,  6.1800, -5.4483, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-7.0698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
LOSS epoch 10 actor 779.6058208249901 critic 1842.999468237404 entropy 100
epoch: 11, step: 0
	action: tensor([[ 6.1800,  4.9979,  6.1800,  5.8011, -6.2800, -6.2800,  4.5366]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 11, step: 1
	action: tensor([[ 4.3146,  5.8640,  5.3145,  5.9678, -5.6875, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 11, step: 2
	action: tensor([[ 6.0402,  6.1800,  6.1800,  6.1800, -5.6114, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4945816558528042 entropy 1.4189385332046724
epoch: 11, step: 3
	action: tensor([[ 5.3282,  6.1800,  4.5982,  5.7727, -5.9308, -6.2800,  5.0803]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 11, step: 4
	action: tensor([[ 5.4087,  5.8042,  3.5043,  6.1800, -6.2800, -5.4720,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 11, step: 5
	action: tensor([[ 5.9090,  6.1800,  6.1800,  5.3964, -6.2800, -5.5876,  5.0154]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 11, step: 6
	action: tensor([[ 6.1800,  5.2171,  5.6836,  5.5812, -6.2800, -5.9223,  5.6842]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 11, step: 7
	action: tensor([[ 6.1800,  5.6793,  6.1800,  5.1423, -5.7611, -6.2800,  5.8441]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 11, step: 8
	action: tensor([[ 5.5892,  5.9805,  5.1128,  4.6279, -4.5557, -4.4985,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 11, step: 9
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -6.2800, -6.2800,  4.8721]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 11, step: 10
	action: tensor([[ 5.8255,  5.8368,  5.6887,  5.6138, -5.6247, -5.8906,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 11, step: 11
	action: tensor([[ 5.3096,  5.7373,  6.1800,  5.3363, -5.9272, -6.0687,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 11, step: 12
	action: tensor([[ 6.1800,  6.1800,  6.1800,  4.8268, -5.8126, -5.7122,  4.2160]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 11, step: 13
	action: tensor([[ 6.0402,  5.3180,  6.0943,  6.1800, -6.2800, -5.6276,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 11, step: 14
	action: tensor([[ 5.6197,  6.1800,  6.1559,  6.1800, -5.9456, -6.2800,  3.1866]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.5374948211984474 entropy 1.4189385332046724
epoch: 11, step: 15
	action: tensor([[ 5.2173,  6.1133,  5.1005,  5.6282, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 11, step: 16
	action: tensor([[ 6.1800,  5.8468,  5.3686,  6.1800, -6.1896, -5.6336,  4.4428]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 11, step: 17
	action: tensor([[ 4.4644,  6.1800,  6.1800,  6.1800, -6.2028, -5.3850,  5.3237]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 11, step: 18
	action: tensor([[ 5.0870,  6.1800,  4.0516,  6.1109, -5.9918, -6.2800,  4.2499]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 11, step: 19
	action: tensor([[ 6.1800,  6.0868,  5.5676,  6.1800, -6.2800, -5.1015,  4.4631]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 11, step: 20
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.0254, -5.9173, -5.2272,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 11, step: 21
	action: tensor([[ 4.5373,  6.1800,  6.1800,  5.1159, -6.2800, -6.2800,  5.5846]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 11, step: 22
	action: tensor([[ 5.8545,  6.1800,  6.1800,  6.1800, -6.2800, -6.2800,  5.4547]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 11, step: 23
	action: tensor([[ 5.9039,  5.6347,  6.1800,  5.0555, -6.2800, -5.4726,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 11, step: 24
	action: tensor([[ 6.1800,  5.8429,  6.1800,  5.6716, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 11, step: 25
	action: tensor([[ 5.1983,  6.1800,  6.1800,  6.1800, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.6926570372866903 entropy 1.4189385332046724
epoch: 11, step: 26
	action: tensor([[ 5.3593,  6.1800,  6.1800,  6.1800, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 11, step: 27
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -5.2431, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 11, step: 28
	action: tensor([[ 6.1800,  5.1484,  5.6204,  6.1800, -6.2800, -6.2800,  5.0793]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 11, step: 29
	action: tensor([[ 6.1800,  4.8527,  6.1800,  6.0925, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 11, step: 30
	action: tensor([[ 4.5189,  4.3185,  6.1800,  5.4690, -6.2800, -5.1562,  5.4509]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 11, step: 31
	action: tensor([[ 6.1800,  6.1800,  5.1394,  4.3103, -6.2800, -5.4045,  5.1792]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 11, step: 32
	action: tensor([[ 6.1800,  5.8888,  6.1800,  6.0559, -5.1620, -5.4519,  5.7474]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 11, step: 33
	action: tensor([[ 5.9832,  6.1800,  6.1326,  6.1800, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 11, step: 34
	action: tensor([[ 4.8396,  6.1800,  6.1800,  4.8622, -6.2800, -5.1244,  5.2928]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 11, step: 35
	action: tensor([[ 6.1800,  5.8578,  6.1800,  6.1800, -5.6476, -6.0676,  5.7597]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 11, step: 36
	action: tensor([[ 5.8890,  4.1687,  5.3818,  6.1800, -6.1790, -5.1990,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 11, step: 37
	action: tensor([[ 6.1800,  6.1800,  5.5491,  6.1800, -6.2800, -5.9957,  6.0334]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 11, step: 38
	action: tensor([[ 6.1800,  6.1800,  4.4037,  6.1800, -6.2800, -5.6874,  4.5921]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1453391732540408 entropy 1.4189385332046724
epoch: 11, step: 39
	action: tensor([[ 6.1800,  6.1800,  5.1053,  6.1800, -6.2800, -6.2800,  4.9640]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 11, step: 40
	action: tensor([[ 6.1800,  6.1800,  5.7562,  5.5671, -5.9778, -6.2800,  5.7027]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0943671083875057 entropy 1.4189385332046724
epoch: 11, step: 41
	action: tensor([[ 6.1800,  6.1800,  5.1858,  5.9237, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 11, step: 42
	action: tensor([[ 6.1512,  6.1800,  6.1800,  4.9385, -4.3007, -6.2800,  5.6293]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 11, step: 43
	action: tensor([[ 6.1800,  6.1800,  5.1285,  4.7355, -3.9485, -6.2800,  5.1200]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 11, step: 44
	action: tensor([[ 5.8096,  6.1800,  6.1800,  6.1800, -5.0076, -5.0888,  4.9592]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 11, step: 45
	action: tensor([[ 6.1800,  4.8306,  6.1800,  5.4498, -4.2454, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 11, step: 46
	action: tensor([[ 5.1197,  5.3247,  6.1800,  6.0596, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 11, step: 47
	action: tensor([[ 4.0110,  4.8830,  6.1800,  5.0517, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 11, step: 48
	action: tensor([[ 5.7814,  6.1800,  5.7201,  6.1800, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 11, step: 49
	action: tensor([[ 6.1800,  4.8695,  6.1800,  6.1800, -6.2800, -6.2800,  4.3334]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 11, step: 50
	action: tensor([[ 6.1680,  5.8864,  6.1800,  6.1800, -5.4826, -5.0912,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9294896936078831 entropy 1.4189385332046724
epoch: 11, step: 51
	action: tensor([[ 5.8649,  6.1800,  6.1800,  6.1800, -6.2800, -6.2800,  5.9923]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 11, step: 52
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -6.2800, -5.1845,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 11, step: 53
	action: tensor([[ 6.1800,  6.1800,  5.9210,  6.1800, -6.2142, -6.0036,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 11, step: 54
	action: tensor([[ 5.2273,  6.1800,  6.1800,  5.7713, -6.2800, -6.2800,  5.7249]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 11, step: 55
	action: tensor([[ 6.1800,  6.1800,  6.1800,  5.8151, -6.2800, -6.2800,  5.6063]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 11, step: 56
	action: tensor([[ 6.1800,  6.1800,  6.1800,  5.8623, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 11, step: 57
	action: tensor([[ 6.1800,  5.0814,  6.1800,  6.1800, -6.2800, -6.0909,  5.0088]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 11, step: 58
	action: tensor([[ 5.3596,  6.1800,  6.1800,  6.1800, -6.0682, -6.2800,  5.8073]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.5775400798788404 entropy 1.4189385332046724
epoch: 11, step: 59
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -4.8259, -5.1655,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 11, step: 60
	action: tensor([[ 6.1800,  6.1800,  6.1800,  5.8227, -6.2800, -6.2761,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 11, step: 61
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -6.2800, -4.7754,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 11, step: 62
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.0328, -6.2800, -6.1637,  5.2313]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 11, step: 63
	action: tensor([[ 5.9365,  6.1800,  6.1800,  6.1800, -6.2800, -5.9705,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 11, step: 64
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -6.2243, -6.2223,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 11, step: 65
	action: tensor([[ 6.1800,  6.1800,  6.0291,  6.1800, -6.1268, -6.2800,  5.5383]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0967211173373175 entropy 1.4189385332046724
epoch: 11, step: 66
	action: tensor([[ 5.6475,  5.7184,  6.1800,  4.9939, -6.2800, -6.2800,  5.7697]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 11, step: 67
	action: tensor([[ 6.1800,  6.1734,  5.6305,  4.9677, -5.6165, -5.2582,  5.8690]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 11, step: 68
	action: tensor([[ 5.6855,  5.8464,  5.2025,  6.1800, -4.7917, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 11, step: 69
	action: tensor([[ 5.8774,  6.1800,  6.1043,  6.1800, -6.2800, -6.2463,  5.3012]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 11, step: 70
	action: tensor([[ 6.1283,  6.1800,  6.1760,  5.4751, -6.1588, -6.2800,  5.5082]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 11, step: 71
	action: tensor([[ 6.1248,  6.1800,  5.8603,  5.8449, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 11, step: 72
	action: tensor([[ 5.8635,  4.9027,  6.1800,  6.1800, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 11, step: 73
	action: tensor([[ 4.5600,  6.1800,  6.1800,  6.1800, -6.2800, -6.2800,  5.2364]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 11, step: 74
	action: tensor([[ 5.0394,  6.1800,  5.8157,  6.1800, -6.2800, -6.2800,  5.3512]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 11, step: 75
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -6.2800, -4.7849,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 11, step: 76
	action: tensor([[ 6.1800,  6.1800,  3.3351,  6.1212, -6.2011, -5.8761,  5.3063]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.113988499884573 entropy 1.4189385332046724
epoch: 11, step: 77
	action: tensor([[ 5.8055,  6.1800,  5.8068,  5.5084, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3466672390777685 entropy 1.4189385332046724
epoch: 11, step: 78
	action: tensor([[ 4.5104,  4.8349,  5.5840,  6.0279, -6.1347, -6.2800,  5.9878]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 11, step: 79
	action: tensor([[ 6.1800,  6.1800,  4.7761,  6.1800, -6.2800, -4.3103,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 11, step: 80
	action: tensor([[ 6.1800,  5.9997,  6.1800,  6.1800, -6.2800, -6.2800,  5.2437]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 11, step: 81
	action: tensor([[ 5.4168,  6.1800,  6.1800,  6.1800, -6.2800, -6.2800,  5.3098]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.5644494884103257 entropy 1.4189385332046724
epoch: 11, step: 82
	action: tensor([[ 6.1800,  4.1999,  5.1147,  6.1223, -5.8771, -6.2800,  5.3719]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 11, step: 83
	action: tensor([[ 6.1800,  6.1800,  6.1800,  4.8818, -6.2800, -5.4781,  5.9375]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 11, step: 84
	action: tensor([[ 6.1800,  5.1500,  6.1800,  6.1800, -5.6631, -6.2800,  5.9769]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 11, step: 85
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1013, -6.2800, -6.1290,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 11, step: 86
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -4.3734, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0800246682123897 entropy 1.4189385332046724
epoch: 11, step: 87
	action: tensor([[ 6.1197,  6.1116,  5.4761,  6.1800, -6.0694, -6.2800,  5.1538]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 11, step: 88
	action: tensor([[ 6.1800,  6.1800,  4.3323,  5.4926, -4.9875, -5.1355,  5.5613]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 11, step: 89
	action: tensor([[ 5.3084,  6.1800,  6.1800,  5.9884, -6.2800, -6.2111,  5.4347]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 11, step: 90
	action: tensor([[ 6.1800,  5.9045,  6.1800,  5.9758, -6.2754, -5.0916,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.12302568430181 entropy 1.4189385332046724
epoch: 11, step: 91
	action: tensor([[ 5.4615,  6.1800,  5.1077,  5.3401, -5.4447, -6.1428,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 11, step: 92
	action: tensor([[ 5.1271,  5.8453,  6.0491,  5.8903, -5.6514, -5.7755,  5.4653]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 11, step: 93
	action: tensor([[ 5.4922,  5.8360,  6.0031,  6.1800, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 11, step: 94
	action: tensor([[ 5.0981,  6.1800,  6.1800,  6.1800, -6.2800, -5.4045,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.5976921197980039 entropy 1.4189385332046724
epoch: 11, step: 95
	action: tensor([[ 6.1800,  3.8308,  6.1800,  6.1800, -5.9801, -6.2800,  5.9352]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 11, step: 96
	action: tensor([[ 5.7029,  6.1800,  6.1800,  6.1800, -6.2800, -6.2800,  4.3780]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 11, step: 97
	action: tensor([[ 5.7299,  4.7419,  5.9252,  6.1800, -5.7906, -6.2432,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 11, step: 98
	action: tensor([[ 6.0171,  5.5102,  5.5367,  6.1800, -6.1480, -6.0612,  5.5342]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 11, step: 99
	action: tensor([[ 5.4401,  5.9938,  5.0001,  5.9645, -6.0625, -4.6636,  4.4315]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 11, step: 100
	action: tensor([[ 6.1800,  5.1826,  6.1800,  6.1800, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 11, step: 101
	action: tensor([[ 6.1800,  5.7121,  4.3214,  6.1800, -6.2800, -6.2168,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 11, step: 102
	action: tensor([[ 6.0321,  6.1800,  6.1800,  5.7837, -4.6588, -5.7976,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.408087197510522 entropy 1.4189385332046724
epoch: 11, step: 103
	action: tensor([[ 6.1800,  6.1800,  5.3440,  6.1800, -5.8966, -6.2056,  4.7634]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 11, step: 104
	action: tensor([[ 4.1983,  3.4982,  6.1800,  5.7485, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 11, step: 105
	action: tensor([[ 6.1511,  6.1800,  4.6182,  4.1646, -6.2800, -5.6553,  5.6544]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 11, step: 106
	action: tensor([[ 6.1800,  5.7518,  6.1800,  6.1800, -5.3595, -4.7591,  6.0896]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 11, step: 107
	action: tensor([[ 5.1815,  6.0993,  6.1800,  6.0393, -6.2800, -5.8568,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 11, step: 108
	action: tensor([[ 5.7737,  5.7337,  6.1800,  4.1186, -6.2800, -5.6593,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 11, step: 109
	action: tensor([[ 6.1800,  6.1361,  6.0027,  6.1800, -6.2800, -6.2800,  4.5203]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 11, step: 110
	action: tensor([[ 5.2188,  5.7688,  6.1800,  6.1800, -6.2800, -5.7857,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 11, step: 111
	action: tensor([[ 6.1800,  6.1002,  6.1800,  6.1800, -6.2800, -6.2800,  5.6837]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 11, step: 112
	action: tensor([[ 5.1789,  6.1800,  5.0981,  6.0330, -6.2800, -6.2800,  5.4651]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 11, step: 113
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 11, step: 114
	action: tensor([[ 5.5617,  6.1800,  6.1800,  6.1800, -5.0121, -6.2800,  5.4473]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 11, step: 115
	action: tensor([[ 4.8644,  6.1800,  6.1800,  5.2494, -6.2800, -6.2800,  5.7560]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 11, step: 116
	action: tensor([[ 4.8976,  6.1800,  6.1800,  6.1800, -5.5976, -5.7768,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9014982557393012 entropy 1.4189385332046724
epoch: 11, step: 117
	action: tensor([[ 6.0354,  5.5351,  6.1800,  5.4647, -6.2800, -3.9481,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 11, step: 118
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -6.2800, -5.3421,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 11, step: 119
	action: tensor([[ 6.1800,  4.6054,  6.1800,  6.1800, -5.6515, -5.3661,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 11, step: 120
	action: tensor([[ 6.1800,  5.6908,  5.9776,  6.1800, -6.1521, -6.2800,  5.5274]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 11, step: 121
	action: tensor([[ 6.1800,  6.1800,  6.1800,  5.8948, -6.2800, -6.2800,  5.8621]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 11, step: 122
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -5.9426, -5.8616,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 11, step: 123
	action: tensor([[ 6.1800,  5.8816,  5.9671,  5.6870, -5.4443, -5.3982,  5.5825]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 11, step: 124
	action: tensor([[ 5.4119,  6.1800,  6.1800,  4.5727, -6.2800, -6.2800,  4.1869]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 11, step: 125
	action: tensor([[ 6.1800,  6.0631,  6.1800,  6.1800, -5.5247, -6.2149,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4432737560267495 entropy 1.4189385332046724
epoch: 11, step: 126
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -6.2800, -5.6362,  5.5773]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 11, step: 127
	action: tensor([[ 6.1800,  5.2501,  6.0584,  6.1800, -6.1965, -6.2800,  5.7454]],
       dtype=torch.float64)
	q_value: tensor([[-11.3197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
LOSS epoch 11 actor 606.1895890525832 critic 1496.1668847461008 entropy 100
epoch: 12, step: 0
	action: tensor([[ 6.1800,  6.1800,  4.6566,  5.3469, -6.2800, -5.0064,  5.9505]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 12, step: 1
	action: tensor([[ 5.1515,  6.1800,  4.4705,  6.1800, -5.9171, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 12, step: 2
	action: tensor([[ 6.1800,  6.1800,  6.1657,  6.1800, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 12, step: 3
	action: tensor([[ 6.1800,  5.4466,  5.5042,  6.1800, -5.5541, -6.2800,  5.1503]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 12, step: 4
	action: tensor([[ 6.1800,  6.1800,  6.1617,  6.1800, -6.2800, -6.2800,  5.0362]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 12, step: 5
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -4.6583, -5.7596,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 12, step: 6
	action: tensor([[ 5.3122,  6.0120,  6.1800,  5.4851, -5.3285, -6.2800,  5.0932]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 12, step: 7
	action: tensor([[ 5.9901,  6.1800,  5.7035,  4.7057, -4.5004, -5.7369,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 12, step: 8
	action: tensor([[ 4.6904,  6.1800,  6.1800,  5.1908, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 12, step: 9
	action: tensor([[ 4.5949,  5.8185,  6.1800,  6.1800, -5.9128, -5.5923,  6.0283]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 12, step: 10
	action: tensor([[ 6.1800,  3.9703,  5.1829,  5.8376, -5.9496, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 12, step: 11
	action: tensor([[ 6.1800,  5.1898,  6.1800,  6.1800, -5.6222, -6.2800,  4.9119]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 12, step: 12
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -5.9142, -5.1262,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 12, step: 13
	action: tensor([[ 4.5520,  6.1800,  6.1800,  6.1070, -5.5761, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 12, step: 14
	action: tensor([[ 5.8102,  5.2050,  4.8362,  6.1800, -6.2800, -6.2800,  5.6399]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 12, step: 15
	action: tensor([[ 5.1520,  4.4028,  6.1332,  6.1800, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 12, step: 16
	action: tensor([[ 6.1800,  6.1800,  6.1800,  5.7899, -5.3357, -5.3212,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 12, step: 17
	action: tensor([[ 6.1800,  6.1800,  5.6070,  6.1800, -5.2494, -4.6701,  5.4592]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 12, step: 18
	action: tensor([[ 6.1800,  6.1800,  6.1800,  5.9833, -6.2800, -5.9871,  5.9653]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 12, step: 19
	action: tensor([[ 6.1800,  6.1800,  6.1800,  4.9509, -6.0313, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 12, step: 20
	action: tensor([[ 5.4371,  6.1800,  6.1626,  6.1800, -4.3821, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 12, step: 21
	action: tensor([[ 6.1800,  4.6432,  6.1800,  6.1115, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 12, step: 22
	action: tensor([[ 6.1800,  6.1800,  5.7656,  6.1800, -6.2800, -6.0962,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 12, step: 23
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -5.4614, -5.6986,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 12, step: 24
	action: tensor([[ 6.0124,  6.1800,  4.8368,  6.1800, -4.9932, -4.8694,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 12, step: 25
	action: tensor([[ 5.0477,  6.1800,  5.8691,  5.1799, -6.1257, -4.5611,  5.7479]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 12, step: 26
	action: tensor([[ 4.9742,  6.1800,  6.1800,  6.1800, -5.2344, -5.9478,  5.9340]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 12, step: 27
	action: tensor([[ 6.1800,  6.1800,  5.2541,  6.1800, -6.2800, -6.2800,  4.9714]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 12, step: 28
	action: tensor([[ 4.5237,  6.1800,  6.1800,  6.1800, -6.0446, -5.9389,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 12, step: 29
	action: tensor([[ 5.3882,  6.1800,  5.1777,  5.5639, -6.2800, -5.2531,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 12, step: 30
	action: tensor([[ 6.1800,  6.1800,  5.7271,  6.1800, -6.2687, -5.7027,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3705753071652975 entropy 1.4189385332046724
epoch: 12, step: 31
	action: tensor([[ 6.1800,  4.6516,  4.9871,  6.1800, -6.0805, -5.7621,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 12, step: 32
	action: tensor([[ 5.5179,  6.0407,  5.4479,  6.1401, -6.0375, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4840544291072162 entropy 1.4189385332046724
epoch: 12, step: 33
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -5.5516, -6.1788,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 12, step: 34
	action: tensor([[ 6.0676,  6.1800,  6.1800,  5.1262, -5.4655, -6.1478,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 12, step: 35
	action: tensor([[ 6.1800,  5.5720,  6.1800,  6.1800, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 12, step: 36
	action: tensor([[ 6.1585,  6.1800,  6.1800,  5.7765, -5.0106, -4.8688,  5.1492]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 12, step: 37
	action: tensor([[ 5.3843,  6.1800,  4.4548,  5.3633, -6.2800, -5.2563,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 12, step: 38
	action: tensor([[ 3.7525,  6.1800,  6.1800,  6.1800, -5.5385, -4.4479,  5.7340]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 12, step: 39
	action: tensor([[ 6.0818,  6.1800,  6.1800,  6.1800, -5.8960, -6.2800,  6.0186]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 12, step: 40
	action: tensor([[ 6.1800,  6.1800,  5.9229,  5.3235, -6.2800, -5.4988,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 12, step: 41
	action: tensor([[ 6.1800,  6.1800,  5.8126,  5.7710, -6.1877, -6.2800,  5.2175]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 12, step: 42
	action: tensor([[ 5.7349,  5.1450,  6.1800,  6.1800, -6.2800, -6.2715,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.6902258276967068 entropy 1.4189385332046724
epoch: 12, step: 43
	action: tensor([[ 6.1800,  6.1490,  6.1174,  6.1800, -5.7791, -6.2243,  6.0288]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 12, step: 44
	action: tensor([[ 5.3586,  6.1800,  6.0513,  6.1800, -6.2800, -5.0562,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 12, step: 45
	action: tensor([[ 5.6928,  5.5259,  5.7464,  6.0985, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 12, step: 46
	action: tensor([[ 5.0155,  6.1800,  5.3843,  6.1800, -6.2800, -5.3644,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 12, step: 47
	action: tensor([[ 6.1800,  6.1800,  5.9625,  6.1800, -6.2800, -6.2800,  5.9094]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4227204083457303 entropy 1.4189385332046724
epoch: 12, step: 48
	action: tensor([[ 6.1800,  6.1728,  5.0796,  6.1800, -5.9936, -6.0526,  5.7981]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 12, step: 49
	action: tensor([[ 6.1800,  5.6182,  5.0498,  5.2858, -6.2800, -6.2800,  5.6402]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 12, step: 50
	action: tensor([[ 5.7489,  4.5513,  6.1800,  6.1800, -4.9814, -5.3199,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 12, step: 51
	action: tensor([[ 5.5701,  6.1800,  4.3992,  6.1800, -5.3024, -6.2800,  6.0468]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.5313028198860152 entropy 1.4189385332046724
epoch: 12, step: 52
	action: tensor([[ 5.4529,  6.1800,  5.7869,  5.9343, -5.9690, -5.4907,  4.2820]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 12, step: 53
	action: tensor([[ 6.1800,  5.7578,  5.6382,  6.1800, -6.2800, -4.1944,  4.5505]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 12, step: 54
	action: tensor([[ 6.1800,  3.7108,  5.9497,  6.1800, -4.1999, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 12, step: 55
	action: tensor([[ 6.1800,  5.4398,  6.1800,  6.1800, -6.0937, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 12, step: 56
	action: tensor([[ 5.6184,  6.1782,  5.2932,  6.1302, -6.2800, -6.1957,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 12, step: 57
	action: tensor([[ 6.0060,  5.5482,  5.9355,  6.1800, -5.4594, -4.7460,  5.0590]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 12, step: 58
	action: tensor([[ 6.1800,  4.1468,  4.6287,  4.5035, -4.4727, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 12, step: 59
	action: tensor([[ 6.0075,  6.1172,  6.1800,  5.0931, -4.9370, -6.1842,  5.5186]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 12, step: 60
	action: tensor([[ 5.3494,  6.1800,  4.7872,  6.0541, -6.2800, -6.2800,  4.4702]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 12, step: 61
	action: tensor([[ 6.1800,  5.2111,  6.1800,  5.6283, -6.0199, -6.2800,  5.5721]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 12, step: 62
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.104977279880815 entropy 1.4189385332046724
epoch: 12, step: 63
	action: tensor([[ 6.0506,  5.9446,  6.1800,  6.1800, -6.2800, -5.4901,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 12, step: 64
	action: tensor([[ 6.1800,  5.6432,  6.1800,  5.7472, -6.2800, -5.5693,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 12, step: 65
	action: tensor([[ 6.1800,  6.1800,  4.3979,  5.4528, -6.2800, -5.1373,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 12, step: 66
	action: tensor([[ 5.6843,  6.1800,  6.0835,  6.1585, -6.1109, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 12, step: 67
	action: tensor([[ 5.7028,  5.5402,  6.1800,  5.8007, -6.2800, -4.1930,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 12, step: 68
	action: tensor([[ 4.5822,  4.8922,  4.4880,  6.1800, -5.0184, -6.2800,  5.1737]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 12, step: 69
	action: tensor([[ 6.1800,  6.1800,  4.7472,  4.2109, -5.5734, -5.9050,  5.3632]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 12, step: 70
	action: tensor([[ 6.1800,  5.7928,  4.9058,  6.1800, -5.1689, -5.6918,  5.6543]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 12, step: 71
	action: tensor([[ 6.1800,  6.1800,  5.8297,  5.1076, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 12, step: 72
	action: tensor([[ 6.1800,  6.1800,  4.0135,  6.1800, -6.2800, -5.6291,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 12, step: 73
	action: tensor([[ 5.5302,  5.2530,  6.1800,  6.1800, -5.9267, -5.7035,  4.7035]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.6312462125340923 entropy 1.4189385332046724
epoch: 12, step: 74
	action: tensor([[ 6.1800,  5.5155,  6.1800,  6.1800, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 12, step: 75
	action: tensor([[ 6.1800,  6.1169,  6.1800,  6.1800, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 12, step: 76
	action: tensor([[ 6.1800,  6.1800,  4.9626,  5.8648, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9255519638881884 entropy 1.4189385332046724
epoch: 12, step: 77
	action: tensor([[ 4.8281,  6.1609,  6.1800,  6.1800, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 12, step: 78
	action: tensor([[ 5.9636,  6.1800,  5.9253,  6.1800, -6.2800, -6.2800,  4.5988]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 12, step: 79
	action: tensor([[ 6.1800,  5.9134,  5.6543,  5.9143, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 12, step: 80
	action: tensor([[ 4.4023,  6.1800,  5.6245,  6.1800, -5.9130, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 12, step: 81
	action: tensor([[ 6.1800,  5.9719,  6.1511,  6.1800, -6.2800, -6.2800,  5.5210]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 12, step: 82
	action: tensor([[ 6.1800,  6.1800,  6.1800,  4.4617, -6.2800, -5.7588,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 12, step: 83
	action: tensor([[ 5.9775,  5.7481,  6.1800,  5.5183, -6.2800, -5.5051,  5.6019]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 12, step: 84
	action: tensor([[ 5.3287,  4.4152,  6.1800,  6.1800, -5.4840, -6.2800,  5.6008]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 12, step: 85
	action: tensor([[ 5.7199,  5.8299,  5.5948,  6.1800, -6.1316, -6.2800,  4.9815]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 12, step: 86
	action: tensor([[ 5.6763,  4.9986,  5.8777,  6.1800, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 12, step: 87
	action: tensor([[ 6.1800,  5.9907,  5.1470,  4.7714, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 12, step: 88
	action: tensor([[ 6.1800,  6.1800,  6.1800,  4.8111, -6.2800, -4.8778,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 12, step: 89
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -6.2800, -5.8194,  6.1384]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 12, step: 90
	action: tensor([[ 6.0803,  6.1800,  4.2717,  4.6559, -5.7903, -6.2800,  5.0069]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 12, step: 91
	action: tensor([[ 5.4446,  5.5718,  6.1800,  5.8225, -5.2992, -6.2800,  5.8452]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 12, step: 92
	action: tensor([[ 6.1800,  6.1137,  6.1800,  5.5232, -5.9507, -5.2124,  5.9935]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 12, step: 93
	action: tensor([[ 5.8221,  6.1800,  6.1800,  5.0821, -5.2301, -6.0501,  5.7235]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 12, step: 94
	action: tensor([[ 6.1800,  6.1800,  4.6261,  6.1395, -5.1845, -5.2216,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 12, step: 95
	action: tensor([[ 5.5953,  6.1800,  6.1800,  5.6124, -5.5976, -6.2800,  6.0887]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 12, step: 96
	action: tensor([[ 6.0796,  5.9238,  5.2283,  6.1800, -6.1044, -6.2800,  5.8233]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 12, step: 97
	action: tensor([[ 5.9757,  6.1800,  4.7133,  5.9040, -4.7806, -5.7011,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 12, step: 98
	action: tensor([[ 5.2094,  5.6658,  6.1800,  6.1800, -6.2800, -5.8210,  6.1566]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.728859616277849 entropy 1.4189385332046724
epoch: 12, step: 99
	action: tensor([[ 6.1800,  5.1483,  6.1800,  5.6893, -5.6029, -5.8456,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 12, step: 100
	action: tensor([[ 4.9938,  6.1800,  3.5226,  5.6394, -6.2800, -5.6401,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.680672657946343 entropy 1.4189385332046724
epoch: 12, step: 101
	action: tensor([[ 5.7271,  6.1800,  5.9598,  6.1800, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 12, step: 102
	action: tensor([[ 5.2096,  5.4987,  5.8765,  4.9888, -5.8073, -6.2800,  5.5949]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4082755018332198 entropy 1.4189385332046724
epoch: 12, step: 103
	action: tensor([[ 6.1800,  6.1800,  5.8822,  6.1800, -6.2800, -5.2825,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 12, step: 104
	action: tensor([[ 6.1800,  6.1800,  6.1800,  5.6672, -4.5892, -6.2800,  4.7755]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 12, step: 105
	action: tensor([[ 5.0298,  6.1800,  6.1800,  4.8943, -5.8256, -5.6062,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 12, step: 106
	action: tensor([[ 6.1800,  4.7369,  5.3281,  6.1800, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 12, step: 107
	action: tensor([[ 5.6332,  4.7640,  5.9948,  6.0948, -3.9153, -6.1780,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 12, step: 108
	action: tensor([[ 6.1800,  5.1713,  6.1800,  5.9778, -5.8261, -5.3804,  4.7416]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3746373153871971 entropy 1.4189385332046724
epoch: 12, step: 109
	action: tensor([[ 6.1800,  4.9672,  5.3406,  6.1800, -6.2800, -6.2800,  3.9261]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 12, step: 110
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 12, step: 111
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -5.6608, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 12, step: 112
	action: tensor([[ 5.7629,  3.7927,  5.7959,  6.1800, -3.9303, -6.1199,  5.4672]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 12, step: 113
	action: tensor([[ 5.1403,  6.1800,  6.1800,  6.1800, -5.7346, -5.4499,  5.2347]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 12, step: 114
	action: tensor([[ 5.9382,  5.5480,  6.1800,  6.1800, -6.2800, -4.8691,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 12, step: 115
	action: tensor([[ 6.1800,  6.1800,  6.1563,  6.0908, -5.9152, -4.5942,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 12, step: 116
	action: tensor([[ 6.1800,  5.4292,  6.0464,  6.1800, -5.1084, -5.4517,  4.5325]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 12, step: 117
	action: tensor([[ 6.1800,  6.1800,  5.6362,  5.7212, -6.2800, -6.2800,  6.0507]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 12, step: 118
	action: tensor([[ 6.0956,  5.0250,  5.9266,  5.8177, -5.8572, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 12, step: 119
	action: tensor([[ 6.1800,  6.1800,  4.5522,  6.1800, -6.2737, -6.2800,  4.7352]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0729452152882488 entropy 1.4189385332046724
epoch: 12, step: 120
	action: tensor([[ 5.2190,  6.1800,  5.6962,  6.1800, -5.4137, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 12, step: 121
	action: tensor([[ 6.1800,  5.0770,  6.1800,  6.1800, -6.2800, -6.0518,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 12, step: 122
	action: tensor([[ 5.5582,  5.2832,  6.1087,  6.1800, -6.2800, -5.7558,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 12, step: 123
	action: tensor([[ 4.2474,  6.1800,  4.8341,  6.1800, -5.8516, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 12, step: 124
	action: tensor([[ 6.0683,  6.1707,  4.3443,  6.1800, -6.2800, -5.0553,  5.6066]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 12, step: 125
	action: tensor([[ 6.1800,  4.1544,  6.1800,  6.1800, -6.1496, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 12, step: 126
	action: tensor([[ 6.1800,  5.7287,  6.1800,  6.1800, -5.3377, -6.2351,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 12, step: 127
	action: tensor([[ 6.1800,  5.7193,  5.6891,  6.1800, -5.8519, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-17.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
LOSS epoch 12 actor 384.02121619021426 critic 1051.830139021363 entropy 100
epoch: 13, step: 0
	action: tensor([[ 6.1800,  5.8826,  6.1800,  6.1800, -5.7540, -5.7250,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1194934306387838 entropy 1.4189385332046724
epoch: 13, step: 1
	action: tensor([[ 4.9168,  6.1800,  6.1800,  6.0362, -6.2800, -6.2800,  4.4766]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 13, step: 2
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -4.8994, -6.2800,  5.4959]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 13, step: 3
	action: tensor([[ 6.1800,  5.6312,  6.1800,  3.7549, -4.8052, -6.1200,  3.3721]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 13, step: 4
	action: tensor([[ 6.1800,  6.1597,  6.1800,  6.1800, -5.6379, -5.3939,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 13, step: 5
	action: tensor([[ 6.1800,  5.5453,  6.1800,  6.1800, -6.2800, -5.6669,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 13, step: 6
	action: tensor([[ 6.1800,  4.2958,  5.9677,  6.1800, -6.2800, -6.2800,  5.7797]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 13, step: 7
	action: tensor([[ 6.1800,  5.6795,  4.4109,  4.8848, -6.2800, -6.2800,  5.2881]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 13, step: 8
	action: tensor([[ 6.1800,  5.6928,  5.4299,  6.0410, -3.6956, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 13, step: 9
	action: tensor([[ 6.1800,  5.3480,  6.1214,  5.0636, -5.5346, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 13, step: 10
	action: tensor([[ 6.1800,  6.1800,  5.0457,  5.4093, -5.5701, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 13, step: 11
	action: tensor([[ 6.1800,  5.1091,  6.1800,  5.1836, -5.1672, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 13, step: 12
	action: tensor([[ 6.1800,  4.6841,  5.0292,  6.0204, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 13, step: 13
	action: tensor([[ 5.7973,  6.1800,  5.0115,  5.8152, -6.2800, -5.7728,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 13, step: 14
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -6.2800, -5.7662,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 13, step: 15
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1310, -4.8850, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 13, step: 16
	action: tensor([[ 6.1800,  6.1472,  6.1800,  6.1800, -4.4759, -5.4245,  5.3803]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 13, step: 17
	action: tensor([[ 6.1800,  6.1800,  5.4035,  5.9575, -5.2775, -6.0617,  4.6295]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 13, step: 18
	action: tensor([[ 5.9759,  6.1800,  6.1800,  5.3488, -6.2800, -5.8811,  5.7546]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 13, step: 19
	action: tensor([[ 6.1800,  5.5969,  6.1800,  6.1800, -5.5430, -6.2800,  5.5960]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.6081632969660564 entropy 1.4189385332046724
epoch: 13, step: 20
	action: tensor([[ 5.6892,  5.6715,  6.1800,  6.1800, -4.9771, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 13, step: 21
	action: tensor([[ 6.1800,  5.2266,  6.0743,  6.1800, -6.0914, -5.8644,  4.2166]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 13, step: 22
	action: tensor([[ 4.9297,  6.1800,  5.4115,  4.9702, -4.8199, -4.9437,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 13, step: 23
	action: tensor([[ 6.0438,  6.1800,  5.2058,  6.1800, -6.2800, -4.2728,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 13, step: 24
	action: tensor([[ 6.0717,  5.8257,  5.9013,  6.1800, -3.9067, -5.7104,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 13, step: 25
	action: tensor([[ 5.5599,  5.6716,  6.1681,  6.1800, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 13, step: 26
	action: tensor([[ 4.8003,  6.0594,  5.1915,  6.1800, -5.9360, -4.7116,  5.4881]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 13, step: 27
	action: tensor([[ 6.1457,  6.1800,  6.1800,  6.1800, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 13, step: 28
	action: tensor([[ 4.8810,  6.1800,  5.3062,  6.1800, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.8103734421938196 entropy 1.4189385332046724
epoch: 13, step: 29
	action: tensor([[ 4.9307,  6.1800,  5.1330,  4.7138, -6.2800, -6.2800,  5.2312]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 13, step: 30
	action: tensor([[ 6.1800,  6.1800,  5.0405,  6.1800, -6.2800, -6.2800,  6.1753]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 13, step: 31
	action: tensor([[ 5.5710,  6.1800,  5.9801,  5.9989, -5.5877, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 13, step: 32
	action: tensor([[ 6.1800,  6.1800,  5.4478,  6.1800, -6.2800, -4.6246,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 13, step: 33
	action: tensor([[ 6.1800,  6.1800,  5.8977,  6.1800, -4.4075, -5.7674,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.011067474880751 entropy 1.4189385332046724
epoch: 13, step: 34
	action: tensor([[ 6.1800,  6.1800,  5.9489,  6.1800, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 13, step: 35
	action: tensor([[ 6.1800,  6.1800,  4.0284,  5.6668, -6.2800, -4.6463,  4.8893]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 13, step: 36
	action: tensor([[ 6.1800,  6.1800,  5.7529,  6.1800, -6.2800, -6.2800,  5.1935]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 13, step: 37
	action: tensor([[ 4.6326,  6.1800,  6.1800,  5.7478, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 13, step: 38
	action: tensor([[ 6.1800,  6.1800,  3.5149,  5.3133, -6.2800, -4.5075,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 13, step: 39
	action: tensor([[ 6.0955,  6.1800,  6.1013,  6.1800, -6.2800, -5.8816,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 13, step: 40
	action: tensor([[ 6.1800,  5.0043,  5.0798,  6.0938, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 13, step: 41
	action: tensor([[ 6.1800,  6.1800,  6.0789,  6.1800, -6.2800, -6.2800,  5.8490]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 13, step: 42
	action: tensor([[ 6.1800,  4.9519,  5.0390,  6.1800, -5.0969, -6.2800,  5.5267]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 13, step: 43
	action: tensor([[ 6.1800,  6.1800,  5.4915,  6.1312, -5.7505, -5.3462,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 13, step: 44
	action: tensor([[ 6.0696,  6.1800,  6.1800,  5.7961, -4.7970, -6.2800,  4.2787]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 13, step: 45
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -5.2353, -5.9526,  3.8824]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9951824492115013 entropy 1.4189385332046724
epoch: 13, step: 46
	action: tensor([[ 6.1800,  5.9650,  4.1453,  6.1800, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 13, step: 47
	action: tensor([[ 5.8522,  5.3658,  6.1202,  6.1800, -6.1844, -5.3981,  5.8317]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 13, step: 48
	action: tensor([[ 3.6718,  4.9668,  6.1800,  5.4359, -6.1748, -4.3361,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 13, step: 49
	action: tensor([[ 5.4033,  5.0663,  4.7827,  5.3883, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 13, step: 50
	action: tensor([[ 6.1800,  5.7383,  5.2204,  4.9062, -4.3770, -5.5433,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 13, step: 51
	action: tensor([[ 5.7139,  4.5326,  5.1237,  6.1800, -5.3725, -6.2800,  5.4265]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 13, step: 52
	action: tensor([[ 6.1800,  6.1800,  6.0831,  6.0288, -5.5167, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 13, step: 53
	action: tensor([[ 6.1800,  6.1800,  6.1800,  5.5341, -6.2399, -6.2800,  5.9725]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 13, step: 54
	action: tensor([[ 6.1800,  6.1800,  5.2763,  6.1800, -6.2800, -6.1617,  5.3637]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4356250854978512 entropy 1.4189385332046724
epoch: 13, step: 55
	action: tensor([[ 6.1800,  5.9704,  6.1800,  4.9235, -6.2800, -6.2800,  5.8855]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 13, step: 56
	action: tensor([[ 6.1335,  5.3981,  6.1800,  4.4210, -4.6836, -5.6569,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 13, step: 57
	action: tensor([[ 5.7241,  6.1800,  5.9399,  4.9032, -5.8403, -4.9157,  4.9226]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 13, step: 58
	action: tensor([[ 5.8875,  4.4639,  5.2027,  6.1800, -6.0935, -6.2800,  5.8062]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 13, step: 59
	action: tensor([[ 6.1800,  6.1800,  5.0438,  5.8712, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 13, step: 60
	action: tensor([[ 6.1800,  5.4911,  6.1800,  6.1800, -6.2800, -5.9135,  3.9122]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 13, step: 61
	action: tensor([[ 4.2063,  4.7797,  6.1800,  4.9758, -6.2800, -5.2520,  4.3379]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 13, step: 62
	action: tensor([[ 4.4984,  6.1800,  6.1800,  6.1800, -6.2800, -5.5680,  5.3235]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 13, step: 63
	action: tensor([[ 6.1800,  6.0231,  6.1800,  4.7283, -5.0515, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 13, step: 64
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 13, step: 65
	action: tensor([[ 5.7247,  6.1800,  6.1800,  6.1800, -5.0021, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 13, step: 66
	action: tensor([[ 6.1800,  5.9960,  6.1800,  5.8533, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 13, step: 67
	action: tensor([[ 6.1800,  6.1800,  5.3783,  4.9997, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 13, step: 68
	action: tensor([[ 6.1800,  5.9265,  6.1800,  5.5327, -6.0738, -6.2800,  6.1788]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 13, step: 69
	action: tensor([[ 5.5020,  6.1800,  6.1800,  6.1800, -5.9351, -3.8787,  5.9744]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 13, step: 70
	action: tensor([[ 6.1800,  5.1484,  6.1800,  6.1800, -6.2800, -5.7775,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 13, step: 71
	action: tensor([[ 6.1800,  6.1800,  6.1800,  3.9863, -5.2064, -6.2800,  4.7509]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 13, step: 72
	action: tensor([[ 5.8933,  5.8732,  6.1800,  5.7648, -5.8520, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 13, step: 73
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -5.8814, -5.7450,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0050456817521072 entropy 1.4189385332046724
epoch: 13, step: 74
	action: tensor([[ 5.8314,  6.1800,  6.1800,  6.1800, -6.2800, -6.2800,  5.6614]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 13, step: 75
	action: tensor([[ 5.6135,  6.1800,  6.1800,  6.1800, -5.7308, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 13, step: 76
	action: tensor([[ 6.1800,  6.1800,  5.7893,  6.1800, -4.9827, -6.2800,  5.3541]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0989130277869976 entropy 1.4189385332046724
epoch: 13, step: 77
	action: tensor([[ 5.6669,  5.1296,  5.3391,  6.1800, -5.0062, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 13, step: 78
	action: tensor([[ 6.0954,  5.3986,  6.1800,  6.0907, -4.1470, -5.9398,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 13, step: 79
	action: tensor([[ 6.1800,  6.1591,  6.1800,  5.7444, -6.2800, -5.1674,  5.7172]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 13, step: 80
	action: tensor([[ 6.1800,  6.1784,  6.1800,  5.7824, -6.2800, -5.6205,  4.8987]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 13, step: 81
	action: tensor([[ 4.6604,  6.1800,  6.1800,  6.1800, -6.0138, -6.2800,  4.8157]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 13, step: 82
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 13, step: 83
	action: tensor([[ 6.1800,  6.0696,  6.1800,  6.1800, -6.2800, -6.2800,  6.1753]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 13, step: 84
	action: tensor([[ 6.1800,  6.1800,  5.6479,  6.1800, -5.7037, -4.9177,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 13, step: 85
	action: tensor([[ 5.8738,  6.1387,  5.7982,  6.1800, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 13, step: 86
	action: tensor([[ 6.1800,  5.2729,  6.1800,  6.1350, -6.2800, -6.2800,  5.3245]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 13, step: 87
	action: tensor([[ 6.1800,  5.3496,  6.1800,  5.4967, -6.2800, -4.2182,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1845262533798724 entropy 1.4189385332046724
epoch: 13, step: 88
	action: tensor([[ 6.1800,  6.1800,  6.1800,  4.8652, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 13, step: 89
	action: tensor([[ 6.1800,  5.6457,  6.1800,  6.1800, -6.2800, -5.1870,  3.9712]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 13, step: 90
	action: tensor([[ 6.1800,  5.6062,  6.1800,  6.1800, -5.8767, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 13, step: 91
	action: tensor([[ 5.1673,  4.7670,  5.1011,  6.1800, -6.1785, -6.2800,  5.4588]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 13, step: 92
	action: tensor([[ 6.1800,  6.1800,  6.1800,  5.8761, -6.2800, -5.2298,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 13, step: 93
	action: tensor([[ 6.1800,  4.2799,  6.1800,  6.1800, -6.2800, -5.7991,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 13, step: 94
	action: tensor([[ 5.9057,  5.9327,  5.6430,  6.1800, -6.2800, -5.7490,  5.9384]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 13, step: 95
	action: tensor([[ 5.9124,  5.6114,  5.7816,  6.1800, -6.2800, -6.2800,  5.5500]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 13, step: 96
	action: tensor([[ 4.8947,  6.1800,  6.1800,  6.1800, -6.0036, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.8524385646379065 entropy 1.4189385332046724
epoch: 13, step: 97
	action: tensor([[ 5.5754,  6.1800,  4.7682,  6.1800, -6.2800, -5.9536,  6.1185]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 13, step: 98
	action: tensor([[ 4.4988,  3.6124,  6.1800,  6.1800, -6.2800, -4.7137,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 13, step: 99
	action: tensor([[ 5.1965,  5.9294,  4.6779,  6.1800, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 13, step: 100
	action: tensor([[ 6.1800,  6.1800,  5.7484,  6.1449, -6.2800, -5.7107,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0816069458951516 entropy 1.4189385332046724
epoch: 13, step: 101
	action: tensor([[ 5.7275,  5.9996,  4.4760,  5.1544, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 13, step: 102
	action: tensor([[ 5.6867,  6.1800,  6.1800,  5.4035, -6.2800, -5.6149,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.7078295067731666 entropy 1.4189385332046724
epoch: 13, step: 103
	action: tensor([[ 5.7043,  5.8215,  6.1800,  6.0056, -5.8316, -6.0774,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 13, step: 104
	action: tensor([[ 5.0666,  6.0575,  5.8586,  6.1800, -5.9993, -6.2800,  4.8536]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 13, step: 105
	action: tensor([[ 5.4598,  5.7465,  6.0839,  4.7068, -6.2800, -6.2800,  4.6213]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 13, step: 106
	action: tensor([[ 5.3478,  5.9533,  6.1800,  6.1800, -6.2800, -6.1539,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 13, step: 107
	action: tensor([[ 6.1800,  4.3923,  6.1800,  6.1800, -5.2987, -4.2569,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 13, step: 108
	action: tensor([[ 5.1811,  4.6060,  6.1800,  6.1800, -5.4793, -6.2800,  5.0469]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 13, step: 109
	action: tensor([[ 6.0287,  6.1800,  4.6277,  6.1800, -5.3620, -5.2711,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 13, step: 110
	action: tensor([[ 6.1800,  6.1800,  6.1800,  5.3814, -5.3236, -5.6139,  6.0520]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 13, step: 111
	action: tensor([[ 5.7084,  5.2781,  4.6130,  5.1452, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 13, step: 112
	action: tensor([[ 5.9679,  5.7312,  6.1717,  5.5912, -6.2800, -4.7759,  5.1432]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 13, step: 113
	action: tensor([[ 6.1680,  6.1800,  5.4946,  6.1800, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 13, step: 114
	action: tensor([[ 5.5620,  4.7067,  6.1800,  6.1800, -5.8474, -4.9668,  5.1182]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 13, step: 115
	action: tensor([[ 5.1756,  6.1800,  5.5342,  6.1800, -6.2800, -6.2800,  5.9447]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 13, step: 116
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -6.1195, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 13, step: 117
	action: tensor([[ 6.1800,  6.1800,  5.1630,  5.5233, -6.2800, -6.1078,  5.6473]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 13, step: 118
	action: tensor([[ 6.1800,  6.1800,  6.0682,  6.1800, -5.4530, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1219853212052266 entropy 1.4189385332046724
epoch: 13, step: 119
	action: tensor([[ 6.1800,  6.1800,  5.9880,  6.1800, -5.8596, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 13, step: 120
	action: tensor([[ 6.1800,  5.0654,  5.6034,  5.9236, -5.8679, -6.0027,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 13, step: 121
	action: tensor([[ 6.1800,  6.1800,  5.7330,  5.3540, -6.2800, -6.2800,  5.6384]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 13, step: 122
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.0517, -4.9872, -5.8910,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 13, step: 123
	action: tensor([[ 6.1800,  6.1800,  6.1800,  4.7934, -5.5992, -6.2800,  5.1283]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 13, step: 124
	action: tensor([[ 6.1800,  3.9549,  6.1800,  5.8152, -6.2800, -6.2800,  5.9957]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 13, step: 125
	action: tensor([[ 6.1800,  6.1800,  5.1519,  6.1800, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0872489566572647 entropy 1.4189385332046724
epoch: 13, step: 126
	action: tensor([[ 5.1959,  6.1800,  6.1246,  6.1800, -6.2800, -5.9158,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 13, step: 127
	action: tensor([[ 6.1800,  6.1800,  4.5935,  6.1800, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-26.3555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.06160342476489 entropy 1.4189385332046724
LOSS epoch 13 actor 208.58374823638533 critic 559.061410018423 entropy 50
epoch: 14, step: 0
	action: tensor([[ 6.0527,  6.0563,  5.6277,  6.1800, -6.2476, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.249634761090632 entropy 1.4189385332046724
epoch: 14, step: 1
	action: tensor([[ 6.0009,  6.1800,  6.1749,  5.1559, -6.2800, -6.2800,  5.4190]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 14, step: 2
	action: tensor([[ 6.1800,  6.1800,  6.1800,  5.3665, -4.8960, -6.2800,  6.1355]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 14, step: 3
	action: tensor([[ 6.1800,  4.7668,  6.1800,  6.1800, -4.5891, -6.2800,  5.9188]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 14, step: 4
	action: tensor([[ 5.2077,  6.1800,  6.1800,  6.1800, -6.2800, -6.2800,  6.1195]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 14, step: 5
	action: tensor([[ 6.1800,  6.1800,  6.1800,  5.8144, -6.2800, -6.0438,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 14, step: 6
	action: tensor([[ 6.1800,  6.1800,  5.7378,  6.1800, -5.5812, -5.5293,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 14, step: 7
	action: tensor([[ 4.4475,  5.3166,  6.1800,  5.6717, -6.2800, -6.2800,  5.6798]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 14, step: 8
	action: tensor([[ 5.4757,  6.1800,  5.7223,  5.2906, -6.0866, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 14, step: 9
	action: tensor([[ 6.1035,  6.1800,  5.5478,  6.1379, -6.2800, -5.4218,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 14, step: 10
	action: tensor([[ 5.8468,  4.8969,  6.1800,  6.1800, -6.2069, -5.6888,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 14, step: 11
	action: tensor([[ 4.8287,  5.6838,  5.9962,  6.1800, -5.1669, -5.9265,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 14, step: 12
	action: tensor([[ 6.1800,  5.0110,  6.1800,  4.4787, -6.2800, -6.2800,  5.4667]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 14, step: 13
	action: tensor([[ 5.2330,  6.1800,  6.1800,  6.1800, -6.2800, -6.2800,  4.8646]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 14, step: 14
	action: tensor([[ 5.9683,  5.2173,  6.1800,  6.1800, -4.9424, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 14, step: 15
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -6.2800, -5.0849,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 14, step: 16
	action: tensor([[ 6.0180,  6.0330,  6.1800,  6.1800, -6.2800, -6.2800,  6.1114]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.613777642913013 entropy 1.4189385332046724
epoch: 14, step: 17
	action: tensor([[ 6.1800,  5.4633,  5.4005,  5.9259, -4.6966, -5.4421,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 14, step: 18
	action: tensor([[ 6.1800,  6.1800,  6.1800,  5.8236, -6.0182, -5.4192,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 14, step: 19
	action: tensor([[ 5.6735,  6.1800,  6.1800,  6.1800, -4.9666, -6.1302,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 14, step: 20
	action: tensor([[ 4.8226,  6.1800,  6.1800,  6.1800, -4.8082, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 14, step: 21
	action: tensor([[ 6.1800,  6.1800,  5.4319,  5.2781, -5.0064, -5.8278,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 14, step: 22
	action: tensor([[ 6.1800,  6.1800,  5.9040,  5.0128, -6.2800, -5.8391,  5.3524]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 14, step: 23
	action: tensor([[ 6.1800,  6.1800,  3.7998,  6.1800, -5.3150, -6.2800,  5.7162]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 14, step: 24
	action: tensor([[ 6.1800,  5.4538,  4.0603,  6.0069, -5.7814, -5.1406,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 14, step: 25
	action: tensor([[ 5.5181,  6.1800,  6.1800,  6.1800, -4.1759, -5.1772,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 14, step: 26
	action: tensor([[ 5.9983,  5.7482,  6.1800,  6.0653, -5.7475, -5.0040,  4.5865]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 14, step: 27
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.143572285585333 entropy 1.4189385332046724
epoch: 14, step: 28
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -4.5639, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 14, step: 29
	action: tensor([[ 6.1800,  5.1657,  6.1800,  6.1800, -6.0743, -6.2800,  5.9645]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 14, step: 30
	action: tensor([[ 6.1800,  4.4787,  6.1800,  6.1800, -6.2800, -5.2105,  5.6753]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 14, step: 31
	action: tensor([[ 6.0115,  5.4701,  6.1800,  5.2476, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 14, step: 32
	action: tensor([[ 6.1800,  5.3156,  4.6121,  5.3008, -6.2800, -5.3407,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 14, step: 33
	action: tensor([[ 5.9689,  4.2093,  6.1800,  5.8894, -6.2800, -4.6271,  5.3020]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 14, step: 34
	action: tensor([[ 6.1800,  4.5284,  6.1800,  5.1907, -5.8641, -5.9468,  5.1015]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 14, step: 35
	action: tensor([[ 4.8683,  6.1800,  5.3764,  5.9248, -6.2800, -5.7916,  4.9560]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.7988523920421713 entropy 1.4189385332046724
epoch: 14, step: 36
	action: tensor([[ 6.1800,  4.3691,  5.0723,  6.1800, -6.2728, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 14, step: 37
	action: tensor([[ 5.8907,  5.1115,  6.1800,  6.1800, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 14, step: 38
	action: tensor([[ 5.5847,  5.8474,  5.9518,  6.1800, -5.9823, -6.2358,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 14, step: 39
	action: tensor([[ 6.0074,  6.1800,  6.0146,  4.0073, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 14, step: 40
	action: tensor([[ 6.1800,  5.6284,  6.1800,  6.1800, -5.7497, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 14, step: 41
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -5.9975, -5.8866,  5.1384]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 14, step: 42
	action: tensor([[ 5.5278,  6.1800,  6.1800,  5.9868, -6.2800, -6.2800,  5.9176]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 14, step: 43
	action: tensor([[ 6.1800,  6.1800,  6.1800,  5.5844, -5.3796, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2431896520696197 entropy 1.4189385332046724
epoch: 14, step: 44
	action: tensor([[ 5.9665,  6.1800,  6.1524,  4.3857, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 14, step: 45
	action: tensor([[ 6.1800,  6.1800,  4.6755,  6.1800, -5.3424, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 14, step: 46
	action: tensor([[ 6.1800,  6.1800,  5.8567,  5.1964, -6.1632, -6.1960,  6.0269]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 14, step: 47
	action: tensor([[ 5.4134,  6.0747,  6.1800,  6.1800, -4.8049, -5.4160,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 14, step: 48
	action: tensor([[ 6.1800,  5.1953,  5.2578,  6.1800, -6.0967, -5.8138,  6.0569]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 14, step: 49
	action: tensor([[ 5.3267,  5.7613,  5.6805,  5.9731, -6.2800, -5.7027,  4.7110]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 14, step: 50
	action: tensor([[ 5.3696,  5.6797,  6.1800,  6.1800, -5.1329, -6.0476,  4.3260]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 14, step: 51
	action: tensor([[ 5.9954,  6.1245,  6.1198,  5.5397, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 14, step: 52
	action: tensor([[ 5.3191,  6.1800,  3.9083,  6.1800, -6.2800, -5.3943,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 14, step: 53
	action: tensor([[ 6.1800,  6.1000,  6.1800,  6.1800, -6.2800, -4.5804,  6.1243]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 14, step: 54
	action: tensor([[ 6.1800,  6.1051,  4.8679,  6.1800, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 14, step: 55
	action: tensor([[ 6.1800,  6.1800,  5.9533,  6.1800, -5.7589, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 14, step: 56
	action: tensor([[ 5.3157,  2.7120,  5.7400,  6.1800, -6.2800, -5.6267,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 14, step: 57
	action: tensor([[ 6.1557,  5.8623,  6.1800,  4.8559, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 14, step: 58
	action: tensor([[ 5.6288,  6.1800,  5.8845,  5.8652, -6.2800, -5.2734,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 14, step: 59
	action: tensor([[ 6.1800,  5.3960,  6.1800,  5.6769, -6.2800, -6.2800,  6.0360]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.6437574922681542 entropy 1.4189385332046724
epoch: 14, step: 60
	action: tensor([[ 6.0297,  6.1800,  5.0638,  3.9602, -6.2800, -4.4702,  3.9120]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 14, step: 61
	action: tensor([[ 5.5712,  5.9356,  5.0532,  6.1800, -4.8586, -6.1812,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 14, step: 62
	action: tensor([[ 5.5949,  6.1800,  5.9518,  5.2354, -6.2800, -6.2800,  5.9447]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 14, step: 63
	action: tensor([[ 4.8922,  5.5887,  5.9861,  6.1800, -6.2800, -6.2022,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 14, step: 64
	action: tensor([[ 6.1800,  5.6916,  6.1800,  5.9121, -6.0048, -5.1784,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 14, step: 65
	action: tensor([[ 5.4122,  6.1800,  6.1800,  5.7696, -6.2800, -6.2800,  4.3338]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 14, step: 66
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -6.0427, -5.9595,  5.0924]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 14, step: 67
	action: tensor([[ 5.8029,  5.6619,  6.1800,  5.2141, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 14, step: 68
	action: tensor([[ 4.5649,  6.1800,  6.1800,  6.1800, -6.2800, -4.9414,  4.9190]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 14, step: 69
	action: tensor([[ 6.1800,  6.1800,  5.6579,  6.1800, -6.2800, -5.5371,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0860125286950602 entropy 1.4189385332046724
epoch: 14, step: 70
	action: tensor([[ 5.9545,  6.1800,  4.6218,  5.9561, -4.9342, -5.6168,  5.4013]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0449107603452645 entropy 1.4189385332046724
epoch: 14, step: 71
	action: tensor([[ 6.1800,  5.9037,  5.8917,  5.6330, -6.2800, -6.2800,  5.7603]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 14, step: 72
	action: tensor([[ 3.8824,  5.8058,  6.1800,  5.0849, -6.2800, -5.2561,  3.9248]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 14, step: 73
	action: tensor([[ 6.1800,  3.8639,  4.8978,  6.1800, -6.2800, -4.8929,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 14, step: 74
	action: tensor([[ 5.5810,  5.5038,  6.1800,  5.6584, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 14, step: 75
	action: tensor([[ 6.1800,  5.0440,  6.1800,  6.1800, -6.2800, -6.2800,  5.7387]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 14, step: 76
	action: tensor([[ 5.6640,  6.1800,  5.7618,  4.6626, -5.9683, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 14, step: 77
	action: tensor([[ 6.1800,  6.1800,  4.4138,  5.7703, -4.8393, -6.2800,  5.1171]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 14, step: 78
	action: tensor([[ 6.1800,  6.1800,  6.1800,  5.5938, -6.1634, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 14, step: 79
	action: tensor([[ 5.5207,  6.1800,  5.6951,  6.1800, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 14, step: 80
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -6.2800, -6.2800,  5.7575]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.143594515338664 entropy 1.4189385332046724
epoch: 14, step: 81
	action: tensor([[ 5.2297,  6.1800,  6.0780,  6.1800, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 14, step: 82
	action: tensor([[ 5.6422,  6.1800,  6.1800,  5.6589, -6.2800, -5.6277,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 14, step: 83
	action: tensor([[ 5.0946,  6.1800,  5.5284,  4.7043, -5.2526, -5.1800,  4.5177]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 14, step: 84
	action: tensor([[ 5.0835,  6.1800,  5.2586,  6.1800, -6.2800, -6.2800,  5.5702]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 14, step: 85
	action: tensor([[ 5.5097,  6.1800,  5.2050,  6.1800, -6.2800, -5.9775,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 14, step: 86
	action: tensor([[ 6.1800,  5.4897,  6.1800,  6.1800, -6.2800, -6.2800,  6.1575]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 14, step: 87
	action: tensor([[ 6.1800,  4.8572,  4.6818,  6.0286, -5.5202, -6.2800,  5.2192]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 14, step: 88
	action: tensor([[ 6.1800,  6.1800,  5.2643,  5.3774, -5.2438, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 14, step: 89
	action: tensor([[ 6.0961,  5.3786,  5.9989,  6.1737, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 14, step: 90
	action: tensor([[ 5.1028,  6.1073,  6.1800,  6.1800, -6.0663, -4.8903,  5.5475]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 14, step: 91
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -5.1444, -5.5513,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 14, step: 92
	action: tensor([[ 5.6370,  6.1800,  6.1638,  6.1800, -6.2800, -6.2800,  4.9918]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 14, step: 93
	action: tensor([[ 6.1800,  6.1800,  4.7564,  5.9823, -4.9795, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9526425026686776 entropy 1.4189385332046724
epoch: 14, step: 94
	action: tensor([[ 6.1800,  6.1800,  5.3048,  6.1800, -6.2800, -6.2800,  5.7143]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 14, step: 95
	action: tensor([[ 6.1800,  6.1800,  5.1658,  6.1800, -6.1338, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 14, step: 96
	action: tensor([[ 6.1800,  6.1800,  5.0659,  6.1249, -6.2800, -4.6279,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 14, step: 97
	action: tensor([[ 6.1800,  5.3840,  6.1798,  6.1800, -6.2800, -5.0967,  5.8107]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 14, step: 98
	action: tensor([[ 5.7212,  4.0382,  6.1800,  5.3156, -5.8009, -5.4784,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 14, step: 99
	action: tensor([[ 6.0594,  6.1739,  5.8131,  6.1800, -6.2119, -4.9567,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 14, step: 100
	action: tensor([[ 6.1800,  6.1800,  5.8279,  5.9699, -4.8161, -6.0477,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 14, step: 101
	action: tensor([[ 5.3820,  6.1800,  5.7173,  5.0960, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 14, step: 102
	action: tensor([[ 4.3644,  4.6891,  4.7731,  6.1800, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 14, step: 103
	action: tensor([[ 5.8250,  3.9699,  5.6655,  6.1800, -5.7870, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 14, step: 104
	action: tensor([[ 5.8986,  6.1800,  5.2013,  6.1800, -6.2800, -5.9051,  5.4213]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 14, step: 105
	action: tensor([[ 4.7364,  6.1800,  4.3022,  6.1800, -6.1028, -4.9475,  6.1087]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 14, step: 106
	action: tensor([[ 6.1800,  6.1800,  5.7371,  6.1800, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 14, step: 107
	action: tensor([[ 6.1313,  6.1800,  6.0370,  6.1800, -6.2800, -6.0491,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 14, step: 108
	action: tensor([[ 5.0205,  6.1800,  5.5206,  6.1800, -4.7087, -6.2800,  5.7702]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 14, step: 109
	action: tensor([[ 6.1800,  6.1800,  6.1800,  5.4837, -5.1658, -5.4967,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 14, step: 110
	action: tensor([[ 6.1800,  6.1800,  6.0233,  6.1800, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 14, step: 111
	action: tensor([[ 6.1800,  6.1800,  5.4777,  6.1800, -5.5330, -6.0338,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 14, step: 112
	action: tensor([[ 4.2707,  6.1800,  6.1800,  5.1901, -6.2153, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 14, step: 113
	action: tensor([[ 6.1800,  4.6862,  5.5721,  6.1800, -6.2800, -6.2800,  5.0845]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 14, step: 114
	action: tensor([[ 6.1800,  6.1800,  6.0187,  5.3549, -4.6400, -6.2800,  5.6450]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 14, step: 115
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.0890, -6.2273, -5.1993,  5.5020]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 14, step: 116
	action: tensor([[ 5.9883,  5.6088,  6.1800,  6.1800, -6.2800, -3.0494,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 14, step: 117
	action: tensor([[ 6.1800,  5.5753,  6.1800,  5.1524, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 14, step: 118
	action: tensor([[ 6.1800,  5.5667,  5.7398,  5.0891, -5.7759, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 14, step: 119
	action: tensor([[ 6.1800,  5.7104,  6.1800,  4.9307, -6.2800, -6.2800,  5.9902]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 14, step: 120
	action: tensor([[ 6.1800,  5.7969,  6.0337,  4.8596, -6.1096, -5.6225,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 14, step: 121
	action: tensor([[ 5.4063,  6.1800,  6.1800,  5.3441, -6.2703, -5.4384,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 14, step: 122
	action: tensor([[ 5.2156,  6.1800,  6.1800,  4.5094, -6.2134, -5.4752,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 14, step: 123
	action: tensor([[ 5.1425,  6.1800,  6.0579,  6.0149, -5.9502, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 14, step: 124
	action: tensor([[ 5.3194,  6.0198,  6.1800,  6.0231, -5.8257, -6.1969,  5.6223]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 14, step: 125
	action: tensor([[ 6.1800,  5.4264,  5.4205,  6.1800, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 14, step: 126
	action: tensor([[ 5.4138,  5.2157,  5.5690,  6.1800, -6.2800, -5.6397,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 14, step: 127
	action: tensor([[ 5.6102,  6.1800,  4.4712,  6.1800, -6.0316, -6.1937,  5.8132]],
       dtype=torch.float64)
	q_value: tensor([[-37.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
LOSS epoch 14 actor 72.29031722914357 critic 144.86442216492807 entropy 0.1
epoch: 15, step: 0
	action: tensor([[ 5.6017,  6.1800,  6.1800,  6.1800, -6.2800, -6.0976,  4.7507]],
       dtype=torch.float64)
	q_value: tensor([[-51.4358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4817489125398275 entropy 1.4189385332046724
epoch: 15, step: 1
	action: tensor([[ 5.5970,  6.1800,  6.1800,  5.6873, -5.3888, -5.0981,  5.2893]],
       dtype=torch.float64)
	q_value: tensor([[-51.4358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 15, step: 2
	action: tensor([[ 4.9207,  6.1800,  5.3760,  5.2235, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-51.4358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 15, step: 3
	action: tensor([[ 6.1800,  5.6386,  5.8874,  3.6200, -6.2800, -6.2800,  6.1200]],
       dtype=torch.float64)
	q_value: tensor([[-51.4358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 15, step: 4
	action: tensor([[ 6.1363,  5.9011,  6.1800,  5.0420, -6.1014, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-51.4358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 15, step: 5
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -6.2800, -6.2800,  5.6144]],
       dtype=torch.float64)
	q_value: tensor([[-51.4358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 15, step: 6
	action: tensor([[ 6.0458,  6.1800,  5.6804,  6.1800, -5.5248, -5.3402,  5.9772]],
       dtype=torch.float64)
	q_value: tensor([[-51.4358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 15, step: 7
	action: tensor([[ 6.1800,  5.3658,  6.1800,  6.1800, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-51.4358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.448907085377571 entropy 1.4189385332046724
epoch: 15, step: 8
	action: tensor([[ 5.2465,  6.1800,  5.2308,  4.4688, -6.2800, -6.2599,  4.6171]],
       dtype=torch.float64)
	q_value: tensor([[-51.4358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 15, step: 9
	action: tensor([[ 6.0526,  5.9891,  6.1800,  6.1800, -6.2800, -6.2800,  5.2428]],
       dtype=torch.float64)
	q_value: tensor([[-51.4358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 15, step: 10
	action: tensor([[ 5.2358,  5.7931,  4.8808,  5.0303, -5.9260, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-51.4358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 15, step: 11
	action: tensor([[ 5.3871,  6.1800,  6.0898,  4.6185, -6.2102, -4.5173,  5.2843]],
       dtype=torch.float64)
	q_value: tensor([[-51.4358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 15, step: 12
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -5.0506, -6.1889,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-51.4358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 15, step: 13
	action: tensor([[ 6.1800,  6.1800,  6.1800,  4.9634, -5.5417, -5.0890,  6.0218]],
       dtype=torch.float64)
	q_value: tensor([[-51.4358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 15, step: 14
	action: tensor([[ 6.1800,  6.1800,  5.8964,  5.9535, -4.9951, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-51.4358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 15, step: 15
	action: tensor([[ 5.7634,  6.1800,  6.1800,  6.1800, -4.9916, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-51.4358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3803463285870237 entropy 1.4189385332046724
epoch: 15, step: 16
	action: tensor([[ 6.1800,  6.1800,  5.2884,  6.1800, -6.2800, -5.5430,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-51.4358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 15, step: 17
	action: tensor([[ 6.1800,  4.5463,  6.1800,  6.1800, -5.2321, -6.0024,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-51.4358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 15, step: 18
	action: tensor([[ 5.4077,  6.1800,  6.1800,  6.1800, -6.2800, -6.2800,  4.8612]],
       dtype=torch.float64)
	q_value: tensor([[-51.4358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.647454850011977 entropy 1.4189385332046724
epoch: 15, step: 19
	action: tensor([[ 6.1800,  6.1800,  6.1800,  5.2101, -4.6177, -5.8175,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-51.4358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 15, step: 20
	action: tensor([[ 5.8598,  6.1800,  6.1800,  6.1800, -5.3668, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-51.4358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 15, step: 21
	action: tensor([[ 6.1800,  4.1356,  6.1800,  6.1800, -6.2800, -5.6809,  5.1159]],
       dtype=torch.float64)
	q_value: tensor([[-51.4358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 15, step: 22
	action: tensor([[ 5.5799,  5.9540,  6.1800,  6.1800, -6.1318, -6.2800,  4.3357]],
       dtype=torch.float64)
	q_value: tensor([[-51.4358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 15, step: 23
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -5.9604, -6.2800,  5.8017]],
       dtype=torch.float64)
	q_value: tensor([[-51.4358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 15, step: 24
	action: tensor([[ 4.7490,  6.0433,  3.7343,  5.5774, -4.9276, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-51.4358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 15, step: 25
	action: tensor([[ 6.1800,  6.1800,  6.1800,  5.7976, -5.7514, -5.8902,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-51.4358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 15, step: 26
	action: tensor([[ 6.1800,  6.1800,  6.0349,  4.7584, -6.2800, -5.9929,  5.6893]],
       dtype=torch.float64)
	q_value: tensor([[-51.4358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2220765601965706 entropy 1.4189385332046724
epoch: 15, step: 27
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -6.2800, -6.1226,  6.0853]],
       dtype=torch.float64)
	q_value: tensor([[-51.4358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 15, step: 28
	action: tensor([[ 6.1800,  6.1800,  6.1800,  5.7581, -6.0211, -5.7276,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-51.4358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 15, step: 29
	action: tensor([[ 5.6620,  6.1800,  6.1800,  6.1800, -5.2508, -6.2800,  6.1374]],
       dtype=torch.float64)
	q_value: tensor([[-51.4358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 15, step: 30
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.0490, -5.8100, -6.2800,  5.8444]],
       dtype=torch.float64)
	q_value: tensor([[-51.4358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 15, step: 31
	action: tensor([[ 6.1800,  6.1800,  5.6876,  5.4977, -5.9427, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-51.4358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 15, step: 32
	action: tensor([[ 6.1800,  4.9530,  6.1800,  6.1800, -5.2848, -6.2800,  4.9712]],
       dtype=torch.float64)
	q_value: tensor([[-51.4358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 15, step: 33
	action: tensor([[ 4.9086,  6.1800,  4.9785,  5.7696, -6.2800, -5.7369,  4.8235]],
       dtype=torch.float64)
	q_value: tensor([[-51.4358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.7299135920351025 entropy 1.4189385332046724
epoch: 15, step: 34
	action: tensor([[ 5.4050,  6.1800,  6.1800,  4.8247, -6.0063, -4.8477,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-51.4358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 15, step: 35
	action: tensor([[ 6.1800,  5.6775,  5.8528,  6.1800, -6.2800, -5.8479,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-51.4358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 15, step: 36
	action: tensor([[ 6.0047,  6.0859,  6.1800,  4.9665, -6.2800, -6.2800,  5.3102]],
       dtype=torch.float64)
	q_value: tensor([[-51.4358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 15, step: 37
	action: tensor([[ 5.3486,  6.1151,  6.1800,  5.9774, -6.2800, -5.8851,  5.5306]],
       dtype=torch.float64)
	q_value: tensor([[-51.4358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 15, step: 38
	action: tensor([[ 5.0884,  6.1800,  5.0871,  3.6068, -6.2800, -5.0893,  5.0709]],
       dtype=torch.float64)
	q_value: tensor([[-51.4358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 15, step: 39
	action: tensor([[ 6.1800,  6.1638,  6.1800,  6.1800, -6.2800, -6.2800,  5.4608]],
       dtype=torch.float64)
	q_value: tensor([[-51.4358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 15, step: 40
	action: tensor([[ 6.1800,  4.0781,  6.1800,  6.1800, -5.0048, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-51.4358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 15, step: 41
	action: tensor([[ 6.1800,  6.0450,  5.6178,  5.7593, -5.2351, -5.0605,  5.8722]],
       dtype=torch.float64)
	q_value: tensor([[-51.4358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 15, step: 42
	action: tensor([[ 6.1800,  5.4455,  6.1800,  6.1800, -6.0070, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-51.4358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 15, step: 43
	action: tensor([[ 5.4241,  4.3949,  4.1294,  6.1800, -6.2800, -5.1663,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-51.4358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 15, step: 44
	action: tensor([[ 6.1800,  5.7287,  5.3024,  6.1800, -6.2800, -6.2759,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-51.4358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 15, step: 45
	action: tensor([[ 6.1800,  5.3463,  6.1800,  6.1800, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-51.4358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 15, step: 46
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -6.2800, -4.3829,  4.8023]],
       dtype=torch.float64)
	q_value: tensor([[-51.4358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 15, step: 47
	action: tensor([[ 6.0268,  5.3378,  6.1473,  6.1056, -6.2800, -5.0292,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-51.4358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 15, step: 48
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -6.2800, -5.9888,  5.0465]],
       dtype=torch.float64)
	q_value: tensor([[-51.4358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 15, step: 49
	action: tensor([[ 6.1800,  5.4237,  5.6331,  6.1800, -6.2800, -6.0305,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-51.4358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 15, step: 50
	action: tensor([[ 5.2258,  6.1800,  5.1235,  6.1800, -6.2800, -6.2731,  5.3422]],
       dtype=torch.float64)
	q_value: tensor([[-51.4358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 15, step: 51
	action: tensor([[ 5.1644,  6.1800,  6.1800,  5.4069, -6.2800, -5.7971,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-51.4358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 15, step: 52
	action: tensor([[ 5.9625,  6.1264,  6.1800,  6.1800, -4.9300, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-51.4358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 15, step: 53
	action: tensor([[ 5.7857,  5.1729,  6.1800,  6.0144, -6.2800, -6.2800,  6.0883]],
       dtype=torch.float64)
	q_value: tensor([[-51.4358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 15, step: 54
	action: tensor([[ 6.1800,  6.1800,  4.4568,  5.1546, -5.6723, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-51.4358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 15, step: 55
	action: tensor([[ 6.1800,  6.1800,  6.1800,  5.4975, -5.3911, -6.2800,  5.9015]],
       dtype=torch.float64)
	q_value: tensor([[-51.4358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 15, step: 56
	action: tensor([[ 6.1800,  5.5757,  5.9372,  4.7920, -6.2800, -5.3705,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-51.4358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.347021930473223 entropy 1.4189385332046724
epoch: 15, step: 57
	action: tensor([[ 6.1800,  6.1800,  6.1800,  5.3587, -6.2800, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-51.4358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 15, step: 58
	action: tensor([[ 6.1800,  5.8775,  5.8957,  6.1800, -5.1803, -6.2800,  4.9652]],
       dtype=torch.float64)
	q_value: tensor([[-51.4358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.210329133147728 entropy 1.4189385332046724
epoch: 15, step: 59
	action: tensor([[ 6.1800,  6.1312,  6.1800,  5.9539, -6.0912, -6.2518,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-51.4358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 15, step: 60
	action: tensor([[ 6.1800,  5.6993,  6.1800,  4.0824, -6.2800, -5.7141,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-51.4358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 15, step: 61
	action: tensor([[ 5.6960,  6.1800,  5.9071,  6.1800, -6.2800, -6.2800,  5.2801]],
       dtype=torch.float64)
	q_value: tensor([[-51.4358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 15, step: 62
	action: tensor([[ 6.1800,  4.5880,  6.1800,  5.7268, -4.5116, -5.5326,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-51.4358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 15, step: 63
	action: tensor([[ 6.1800,  6.1049,  5.2890,  6.1800, -5.4318, -5.6370,  5.5640]],
       dtype=torch.float64)
	q_value: tensor([[-51.4358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 15, step: 64
	action: tensor([[ 6.1800,  4.8569,  6.1800,  6.0688, -6.2800, -5.0987,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-51.4358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 15, step: 65
	action: tensor([[ 6.1800,  4.6692,  4.1322,  6.1800, -5.7242, -6.2800,  4.7560]],
       dtype=torch.float64)
	q_value: tensor([[-51.4358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 15, step: 66
	action: tensor([[ 6.1800,  6.1800,  3.5118,  5.4457, -6.2800, -4.4111,  5.7024]],
       dtype=torch.float64)
	q_value: tensor([[-51.4358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 15, step: 67
	action: tensor([[ 5.2966,  6.1800,  6.1800,  5.8907, -5.5709, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-51.4358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 15, step: 68
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.1800, -6.2800, -6.2800,  4.9970]],
       dtype=torch.float64)
	q_value: tensor([[-51.4358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1341739507966289 entropy 1.4189385332046724
epoch: 15, step: 69
	action: tensor([[ 6.1800,  6.1800,  6.1800,  6.0504, -6.2800, -5.2157,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-51.4358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 15, step: 70
	action: tensor([[ 6.1800,  6.1800,  6.1800,  5.4779, -5.5216, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-51.4358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 15, step: 71
	action: tensor([[ 6.1800,  6.1800,  6.1800,  5.9878, -4.7950, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-51.4358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 15, step: 72
	action: tensor([[ 6.1800,  6.1800,  6.1800,  5.2398, -5.0292, -6.2800,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-51.4358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 15, step: 73
	action: tensor([[ 6.1800,  6.1800,  5.3229,  6.1800, -5.1899, -6.2800,  5.4463]],
       dtype=torch.float64)
	q_value: tensor([[-51.4358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 15, step: 74
	action: tensor([[ 6.1800,  6.1800,  5.7275,  5.7171, -6.2800, -5.6128,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-51.4358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 15, step: 75
	action: tensor([[ 6.1800,  6.0183,  5.7918,  6.1800, -6.2800, -4.5687,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-51.4358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 15, step: 76
	action: tensor([[ 6.1800,  5.9111,  6.1800,  6.1800, -4.4083, -5.5929,  5.0821]],
       dtype=torch.float64)
	q_value: tensor([[-51.4358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 15, step: 77
	action: tensor([[ 6.1800,  6.1800,  5.5053,  5.6530, -6.0724, -4.2744,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-51.4358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 15, step: 78
	action: tensor([[ 6.1800,  6.1800,  6.1800,  4.0832, -5.2855, -3.9257,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-51.4358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 15, step: 79
	action: tensor([[ 4.2914,  6.1800,  6.1800,  6.1800, -6.2800, -5.0254,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-51.4358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 15, step: 80
	action: tensor([[ 4.3563,  6.1800,  5.7695,  6.1800, -5.0197, -6.2800,  5.7159]],
       dtype=torch.float64)
	q_value: tensor([[-51.4358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 15, step: 81
	action: tensor([[ 4.5820,  6.1800,  5.8930,  5.6286, -6.2800, -4.9437,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-51.4358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
epoch: 15, step: 82
	action: tensor([[ 6.1800,  6.1800,  5.6447,  6.1406, -5.6219, -6.2255,  6.1800]],
       dtype=torch.float64)
	q_value: tensor([[-51.4358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.4189385332046724
