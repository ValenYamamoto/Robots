epoch: 0, step: 0
	action: tensor([[-0.0068,  0.1316, -0.0199,  0.0093,  0.0878, -0.0649,  0.0298]],
       dtype=torch.float64)
	q_value: tensor([[3.1864]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3321194764802662, distance: 0.9352034121942125 entropy -10.895285085956377
epoch: 0, step: 1
	action: tensor([[ 4.6000e-05,  9.6218e-02,  7.0540e-02, -5.8396e-02,  6.4597e-02,
         -8.2150e-03,  2.2130e-02]], dtype=torch.float64)
	q_value: tensor([[1.8847]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2929724017130271, distance: 0.9622211009837912 entropy -13.271383958477797
epoch: 0, step: 2
	action: tensor([[-4.7638e-05,  9.5805e-02,  5.9762e-02, -2.4617e-02,  6.4366e-02,
          2.1725e-02,  2.2064e-02]], dtype=torch.float64)
	q_value: tensor([[1.8614]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3132979561018715, distance: 0.9482893284931778 entropy -13.270849892091887
epoch: 0, step: 3
	action: tensor([[-5.5495e-05,  9.5871e-02,  5.8384e-03, -7.2685e-02,  6.4443e-02,
          3.5602e-03,  2.2025e-02]], dtype=torch.float64)
	q_value: tensor([[1.8387]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2843176355889385, distance: 0.968092489980237 entropy -13.270844014743796
epoch: 0, step: 4
	action: tensor([[-6.1384e-05,  9.6009e-02,  3.5297e-02, -2.7001e-02,  6.4483e-02,
          3.3013e-02,  2.2131e-02]], dtype=torch.float64)
	q_value: tensor([[1.8508]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30978123089243315, distance: 0.9507144081132922 entropy -13.271230875337224
epoch: 0, step: 5
	action: tensor([[-6.1537e-05,  9.5906e-02,  7.4621e-02,  7.5427e-02,  6.4474e-02,
          5.2158e-04,  2.2038e-02]], dtype=torch.float64)
	q_value: tensor([[1.8294]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3478597624160922, distance: 0.9241174944637528 entropy -13.270955113685998
epoch: 0, step: 6
	action: tensor([[2.1219e-05, 9.5822e-02, 4.4979e-02, 2.7498e-02, 6.4368e-02, 2.2137e-02,
         2.1901e-02]], dtype=torch.float64)
	q_value: tensor([[1.8233]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32600530186745713, distance: 0.9394743632368416 entropy -13.271048330936605
epoch: 0, step: 7
	action: tensor([[-1.8814e-05,  9.5898e-02,  5.1354e-02, -8.4525e-03,  6.4456e-02,
          2.7092e-02,  2.1974e-02]], dtype=torch.float64)
	q_value: tensor([[1.8216]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.309316233375346, distance: 0.9510346003677245 entropy -13.271014437048647
epoch: 0, step: 8
	action: tensor([[-4.7179e-05,  9.5867e-02,  7.1029e-02,  8.6829e-02,  6.4446e-02,
         -1.1117e-02,  2.2007e-02]], dtype=torch.float64)
	q_value: tensor([[1.8297]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3450138070577017, distance: 0.9261317345652657 entropy -13.27091145308616
epoch: 0, step: 9
	action: tensor([[3.8455e-05, 9.5844e-02, 1.4897e-02, 7.1521e-02, 6.4366e-02, 4.1494e-02,
         2.1897e-02]], dtype=torch.float64)
	q_value: tensor([[1.8249]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34683045546615043, distance: 0.9248464983408061 entropy -13.271103260809769
epoch: 0, step: 10
	action: tensor([[-2.0638e-06,  9.6029e-02,  7.3035e-02,  4.1437e-02,  6.4543e-02,
         -7.8654e-03,  2.1969e-02]], dtype=torch.float64)
	q_value: tensor([[1.7986]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.324789686977641, distance: 0.9403211981948884 entropy -13.271174250896406
epoch: 0, step: 11
	action: tensor([[ 1.1837e-05,  9.5807e-02,  5.9424e-02, -5.1627e-02,  6.4364e-02,
          5.0112e-03,  2.1936e-02]], dtype=torch.float64)
	q_value: tensor([[1.8353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28355877774780536, distance: 0.9686056016451753 entropy -13.270944068753678
epoch: 0, step: 12
	action: tensor([[-5.6139e-05,  9.5847e-02,  4.1695e-02,  1.2223e-01,  6.4409e-02,
          3.0463e-02,  2.2059e-02]], dtype=torch.float64)
	q_value: tensor([[1.8523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3686132985687943, distance: 0.909294173871106 entropy -13.270897307975018
epoch: 0, step: 13
	action: tensor([[ 2.1624e-05,  9.6012e-02,  1.5828e-02, -3.0312e-03,  6.4502e-02,
          5.0314e-02,  2.1913e-02]], dtype=torch.float64)
	q_value: tensor([[1.7971]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3166053477423928, distance: 0.9460029294661728 entropy -13.271225293663651
epoch: 0, step: 14
	action: tensor([[-6.0299e-05,  9.6024e-02,  3.9373e-02,  1.3502e-01,  6.4548e-02,
          2.5653e-03,  2.2038e-02]], dtype=torch.float64)
	q_value: tensor([[1.8149]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3675328667725617, distance: 0.9100718354925417 entropy -13.270998936986842
epoch: 0, step: 15
	action: tensor([[ 4.7781e-05,  9.6010e-02,  1.2834e-02,  1.2691e-02,  6.4471e-02,
         -5.2679e-03,  2.1905e-02]], dtype=torch.float64)
	q_value: tensor([[1.8045]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.310237949989974, distance: 0.9503998112789193 entropy -13.271346785292236
epoch: 0, step: 16
	action: tensor([[1.7682e-05, 9.5983e-02, 1.8141e-02, 5.2979e-02, 6.4487e-02, 3.5432e-03,
         2.2020e-02]], dtype=torch.float64)
	q_value: tensor([[1.8321]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3301783408115958, distance: 0.9365614691422259 entropy -13.271196400475612
epoch: 0, step: 17
	action: tensor([[2.5655e-05, 9.5988e-02, 4.8480e-02, 7.5457e-02, 6.4491e-02, 2.5129e-02,
         2.1980e-02]], dtype=torch.float64)
	q_value: tensor([[1.8198]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34723736311267106, distance: 0.9245583757974726 entropy -13.27122533904763
epoch: 0, step: 18
	action: tensor([[6.0668e-06, 9.5906e-02, 3.3694e-02, 1.2575e-01, 6.4451e-02, 2.0522e-02,
         2.1927e-02]], dtype=torch.float64)
	q_value: tensor([[1.8093]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3670997937256619, distance: 0.9103833616705359 entropy -13.27112363904878
epoch: 0, step: 19
	action: tensor([[3.3170e-05, 9.6019e-02, 1.0281e-02, 7.2791e-02, 6.4498e-02, 8.3867e-03,
         2.1915e-02]], dtype=torch.float64)
	q_value: tensor([[1.7982]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33948790136476337, distance: 0.9300302654396454 entropy -13.271300588030964
epoch: 0, step: 20
	action: tensor([[ 3.1927e-05,  9.6032e-02,  2.2510e-02,  9.2187e-03,  6.4519e-02,
         -1.5978e-02,  2.1975e-02]], dtype=torch.float64)
	q_value: tensor([[1.8124]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30641737795128277, distance: 0.9530282950600271 entropy -13.271271843307362
epoch: 0, step: 21
	action: tensor([[2.5507e-05, 9.5953e-02, 6.6929e-03, 1.0861e-02, 6.4458e-02, 3.0707e-02,
         2.2018e-02]], dtype=torch.float64)
	q_value: tensor([[1.8389]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3176757322173287, distance: 0.9452617884183372 entropy -13.27115433000327
epoch: 0, step: 22
	action: tensor([[-2.4949e-05,  9.6011e-02,  6.1704e-02,  3.3690e-02,  6.4532e-02,
          2.2959e-03,  2.2027e-02]], dtype=torch.float64)
	q_value: tensor([[1.8165]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3231650735243894, distance: 0.9414517649415006 entropy -13.27113961644568
epoch: 0, step: 23
	action: tensor([[ 1.5965e-06,  9.5842e-02,  7.1305e-02, -4.6883e-02,  6.4398e-02,
         -8.8908e-03,  2.1953e-02]], dtype=torch.float64)
	q_value: tensor([[1.8312]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2822651706765644, distance: 0.9694796649488803 entropy -13.270970042290326
epoch: 0, step: 24
	action: tensor([[-3.3729e-05,  9.5777e-02,  2.8395e-02, -4.4472e-02,  6.4354e-02,
          2.0071e-02,  2.2042e-02]], dtype=torch.float64)
	q_value: tensor([[1.8572]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29063458174572165, distance: 0.9638106027635895 entropy -13.270872783347183
epoch: 0, step: 25
	action: tensor([[-5.8389e-05,  9.5922e-02,  8.4442e-02,  8.5548e-02,  6.4475e-02,
          2.7576e-02,  2.2065e-02]], dtype=torch.float64)
	q_value: tensor([[1.8376]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35503947151398385, distance: 0.919016400200046 entropy -13.271043745571772
epoch: 0, step: 26
	action: tensor([[ 1.2854e-08,  9.5795e-02,  6.4378e-02, -4.5437e-03,  6.4371e-02,
          5.3635e-03,  2.1882e-02]], dtype=torch.float64)
	q_value: tensor([[1.8109]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3059496819979608, distance: 0.9533495634623259 entropy -13.271004243519965
epoch: 0, step: 27
	action: tensor([[-2.2156e-05,  9.5827e-02,  8.6381e-03,  9.9513e-02,  6.4403e-02,
         -1.3219e-02,  2.1993e-02]], dtype=torch.float64)
	q_value: tensor([[1.8397]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34565104399615487, distance: 0.9256811075363813 entropy -13.270910354433486
epoch: 0, step: 28
	action: tensor([[ 6.0295e-05,  9.6072e-02,  2.7375e-02, -2.5106e-02,  6.4512e-02,
          1.2826e-03,  2.1962e-02]], dtype=torch.float64)
	q_value: tensor([[1.8150]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2952069893216579, distance: 0.9606993294392862 entropy -13.271360998276583
epoch: 0, step: 29
	action: tensor([[-1.9202e-05,  9.5929e-02,  3.6378e-02,  5.7333e-02,  6.4465e-02,
          2.0282e-02,  2.2045e-02]], dtype=torch.float64)
	q_value: tensor([[1.8410]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.337031086210668, distance: 0.9317583122680508 entropy -13.271104785137338
epoch: 0, step: 30
	action: tensor([[ 4.0724e-06,  9.5935e-02,  2.6412e-02, -7.2814e-02,  6.4471e-02,
          1.1987e-02,  2.1956e-02]], dtype=torch.float64)
	q_value: tensor([[1.8138]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2759957500004032, distance: 0.9737046657787877 entropy -13.27113026118373
epoch: 0, step: 31
	action: tensor([[-7.6600e-05,  9.5906e-02,  6.0818e-02,  4.3482e-02,  6.4445e-02,
         -2.8739e-02,  2.2102e-02]], dtype=torch.float64)
	q_value: tensor([[1.8486]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31995015188119924, distance: 0.9436850332750103 entropy -13.271097189154315
epoch: 0, step: 32
	action: tensor([[4.0947e-05, 9.5847e-02, 3.1433e-02, 3.6172e-02, 6.4371e-02, 3.1597e-02,
         2.1948e-02]], dtype=torch.float64)
	q_value: tensor([[1.8415]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33001596070335937, distance: 0.9366749842360611 entropy -13.271009817457687
epoch: 0, step: 33
	action: tensor([[-2.2453e-05,  9.5979e-02,  1.0469e-01,  4.1813e-02,  6.4510e-02,
          1.9233e-02,  2.1986e-02]], dtype=torch.float64)
	q_value: tensor([[1.8150]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3332817058306706, distance: 0.9343893488354944 entropy -13.271050385745927
epoch: 0, step: 34
	action: tensor([[-2.7947e-05,  9.5745e-02,  3.4140e-02,  8.0848e-02,  6.4339e-02,
          1.1830e-02,  2.1911e-02]], dtype=torch.float64)
	q_value: tensor([[1.8303]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3451568441352997, distance: 0.9260306038763103 entropy -13.270790278968999
epoch: 0, step: 35
	action: tensor([[2.3307e-05, 9.5997e-02, 3.7046e-02, 1.0578e-01, 6.4493e-02, 3.4712e-02,
         2.1945e-02]], dtype=torch.float64)
	q_value: tensor([[1.8131]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3621208153482277, distance: 0.9139573052120056 entropy -13.271183109087188
epoch: 0, step: 36
	action: tensor([[1.6419e-05, 9.5976e-02, 2.8000e-02, 9.5772e-03, 6.4493e-02, 2.1051e-02,
         2.1920e-02]], dtype=torch.float64)
	q_value: tensor([[1.7970]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3155198341453329, distance: 0.9467539534738653 entropy -13.271215392930687
epoch: 0, step: 37
	action: tensor([[-2.5277e-05,  9.5976e-02,  2.6005e-02,  1.2903e-01,  6.4501e-02,
         -1.1435e-03,  2.2013e-02]], dtype=torch.float64)
	q_value: tensor([[1.8258]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.362747081373342, distance: 0.9135085360866197 entropy -13.271042842601783
epoch: 0, step: 38
	action: tensor([[ 4.8412e-05,  9.6075e-02,  1.9652e-02,  5.6912e-03,  6.4510e-02,
         -4.6629e-02,  2.1930e-02]], dtype=torch.float64)
	q_value: tensor([[1.8071]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29765636292825426, distance: 0.9590285129346117 entropy -13.271353156918526
epoch: 0, step: 39
	action: tensor([[ 6.0488e-05,  9.5957e-02,  4.1455e-02, -1.1011e-01,  6.4430e-02,
          2.4355e-02,  2.2036e-02]], dtype=torch.float64)
	q_value: tensor([[1.8522]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26146786956426193, distance: 0.9834253306389777 entropy -13.271152273544768
epoch: 0, step: 40
	action: tensor([[-0.0001,  0.0959,  0.0452, -0.0518,  0.0644, -0.0080,  0.0221]],
       dtype=torch.float64)
	q_value: tensor([[1.8550]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28024438941118024, distance: 0.9708434898858542 entropy -13.2708527728378
epoch: 0, step: 41
	action: tensor([[-3.5542e-05,  9.5854e-02,  5.0282e-02,  6.4248e-02,  6.4403e-02,
          2.2823e-02,  2.2068e-02]], dtype=torch.float64)
	q_value: tensor([[1.8541]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3415723114997127, distance: 0.9285616350098286 entropy -13.27105071130187
epoch: 0, step: 42
	action: tensor([[ 1.2603e-06,  9.5894e-02,  3.4755e-02,  5.2790e-02,  6.4445e-02,
         -5.3894e-03,  2.1935e-02]], dtype=torch.float64)
	q_value: tensor([[1.8130]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3287555534633755, distance: 0.9375556299955541 entropy -13.271084902570118
epoch: 0, step: 43
	action: tensor([[ 2.9372e-05,  9.5937e-02,  4.2667e-02, -7.3110e-02,  6.4449e-02,
         -1.6497e-03,  2.1964e-02]], dtype=torch.float64)
	q_value: tensor([[1.8256]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27217904326847575, distance: 0.9762678138473608 entropy -13.271159596406195
epoch: 0, step: 44
	action: tensor([[-6.2100e-05,  9.5855e-02,  3.9339e-02,  2.9616e-02,  6.4404e-02,
          1.8434e-03,  2.2095e-02]], dtype=torch.float64)
	q_value: tensor([[1.8566]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.320148647087208, distance: 0.9435473002645006 entropy -13.271033396574955
epoch: 0, step: 45
	action: tensor([[ 5.2135e-06,  9.5947e-02,  4.5956e-02,  2.5104e-02,  6.4464e-02,
         -2.4965e-06,  2.1985e-02]], dtype=torch.float64)
	q_value: tensor([[1.8305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31801679317261133, distance: 0.9450255135261189 entropy -13.271062852473637
epoch: 0, step: 46
	action: tensor([[ 2.9418e-06,  9.5925e-02,  3.9327e-02,  5.6509e-02,  6.4448e-02,
         -2.4755e-02,  2.1983e-02]], dtype=torch.float64)
	q_value: tensor([[1.8334]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32606929707318966, distance: 0.9394297610444805 entropy -13.27101601893517
epoch: 0, step: 47
	action: tensor([[ 4.9383e-05,  9.5952e-02,  7.6404e-02, -1.2153e-01,  6.4435e-02,
         -1.1572e-03,  2.1964e-02]], dtype=torch.float64)
	q_value: tensor([[1.8352]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2480629305080312, distance: 0.9923101668804004 entropy -13.27110327652397
epoch: 0, step: 48
	action: tensor([[-0.0001,  0.0957,  0.0581,  0.0465,  0.0643,  0.0051,  0.0221]],
       dtype=torch.float64)
	q_value: tensor([[1.8734]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3293473235320572, distance: 0.9371422636740181 entropy -13.270543531943769
epoch: 0, step: 49
	action: tensor([[6.5643e-06, 9.5860e-02, 6.3613e-02, 2.6323e-02, 6.4408e-02, 5.3874e-02,
         2.1944e-02]], dtype=torch.float64)
	q_value: tensor([[1.8260]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3326476629294024, distance: 0.9348335410481826 entropy -13.271010529348429
epoch: 0, step: 50
	action: tensor([[-5.5799e-05,  9.5836e-02,  2.7907e-02, -2.2316e-02,  6.4437e-02,
          2.4038e-02,  2.1957e-02]], dtype=torch.float64)
	q_value: tensor([[1.8117]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3016586086866503, distance: 0.9562921377178192 entropy -13.270849368513009
epoch: 0, step: 51
	action: tensor([[-4.8381e-05,  9.5969e-02,  1.8426e-02, -2.3995e-03,  6.4501e-02,
          3.2101e-02,  2.2047e-02]], dtype=torch.float64)
	q_value: tensor([[1.8321]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31246939340502333, distance: 0.9488612507010062 entropy -13.270998262123726
epoch: 0, step: 52
	action: tensor([[-4.2140e-05,  9.6003e-02,  3.3913e-02,  1.0172e-01,  6.4525e-02,
         -1.0979e-02,  2.2033e-02]], dtype=torch.float64)
	q_value: tensor([[1.8227]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3491926343467985, distance: 0.9231726361283258 entropy -13.271051468633283
epoch: 0, step: 53
	action: tensor([[ 4.8925e-05,  9.6018e-02,  6.0818e-02, -6.3185e-02,  6.4474e-02,
          4.0621e-03,  2.1934e-02]], dtype=torch.float64)
	q_value: tensor([[1.8186]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2780400130836067, distance: 0.972329041557637 entropy -13.271281161708275
epoch: 0, step: 54
	action: tensor([[-6.5751e-05,  9.5836e-02,  5.6669e-02, -4.4544e-02,  6.4399e-02,
          2.4094e-02,  2.2073e-02]], dtype=torch.float64)
	q_value: tensor([[1.8558]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29167178908624247, distance: 0.963105721263745 entropy -13.270874775029588
epoch: 0, step: 55
	action: tensor([[-7.3149e-05,  9.5863e-02,  4.5700e-02,  4.2401e-02,  6.4440e-02,
         -7.3958e-03,  2.2049e-02]], dtype=torch.float64)
	q_value: tensor([[1.8421]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32398826621318333, distance: 0.9408790758204967 entropy -13.270834738952175
epoch: 0, step: 56
	action: tensor([[ 2.0942e-05,  9.5931e-02,  8.4189e-03, -7.3849e-03,  6.4441e-02,
         -1.0032e-03,  2.1969e-02]], dtype=torch.float64)
	q_value: tensor([[1.8321]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3022217925130095, distance: 0.9559064546557279 entropy -13.271059155716312
epoch: 0, step: 57
	action: tensor([[-8.4445e-08,  9.6031e-02,  5.9017e-02,  3.5005e-02,  6.4515e-02,
         -3.2565e-02,  2.2052e-02]], dtype=torch.float64)
	q_value: tensor([[1.8362]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31516682442846455, distance: 0.9469980586222239 entropy -13.271154299620264
epoch: 0, step: 58
	action: tensor([[4.0739e-05, 9.5880e-02, 5.4761e-02, 4.1794e-02, 6.4388e-02, 9.8231e-03,
         2.1965e-02]], dtype=torch.float64)
	q_value: tensor([[1.8467]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32844349150615293, distance: 0.9377735397672377 entropy -13.27097622485852
epoch: 0, step: 59
	action: tensor([[-2.2260e-06,  9.5905e-02,  6.9820e-02, -5.6838e-02,  6.4440e-02,
         -2.2464e-02,  2.1959e-02]], dtype=torch.float64)
	q_value: tensor([[1.8264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27410376482666043, distance: 0.9749760898196583 entropy -13.270990007703569
epoch: 0, step: 60
	action: tensor([[-2.9138e-05,  9.5803e-02,  3.1532e-02,  3.6783e-02,  6.4347e-02,
          2.6256e-02,  2.2067e-02]], dtype=torch.float64)
	q_value: tensor([[1.8667]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3288714217078148, distance: 0.9374747074539591 entropy -13.270894961560456
epoch: 0, step: 61
	action: tensor([[-1.6706e-05,  9.5981e-02,  4.9999e-02, -8.2920e-03,  6.4507e-02,
          3.7446e-03,  2.1986e-02]], dtype=torch.float64)
	q_value: tensor([[1.8169]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3036440802120006, distance: 0.9549317412781453 entropy -13.271056965266427
epoch: 0, step: 62
	action: tensor([[-2.0966e-05,  9.5905e-02,  2.6941e-02,  2.2810e-02,  6.4446e-02,
          2.1045e-02,  2.2014e-02]], dtype=torch.float64)
	q_value: tensor([[1.8409]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3213047921611021, distance: 0.9427446676479654 entropy -13.270967628262298
epoch: 0, step: 63
	action: tensor([[-1.7208e-05,  9.5983e-02,  2.8405e-02, -6.3220e-03,  6.4505e-02,
         -8.2800e-03,  2.2002e-02]], dtype=torch.float64)
	q_value: tensor([[1.8222]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30128335456923727, distance: 0.9565490352554245 entropy -13.27107047463352
epoch: 0, step: 64
	action: tensor([[ 2.3315e-06,  9.5967e-02,  1.0930e-02, -2.2462e-02,  6.4472e-02,
         -5.1380e-03,  2.2032e-02]], dtype=torch.float64)
	q_value: tensor([[1.8421]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2946408956033717, distance: 0.9610850715551558 entropy -13.271095887091759
epoch: 0, step: 65
	action: tensor([[-5.8750e-06,  9.6013e-02,  5.3218e-02,  4.1765e-02,  6.4499e-02,
         -7.4221e-03,  2.2066e-02]], dtype=torch.float64)
	q_value: tensor([[1.8425]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3241886240553661, distance: 0.9407396355844982 entropy -13.271179130432957
epoch: 0, step: 66
	action: tensor([[ 1.7655e-05,  9.5908e-02,  1.5990e-02, -2.9133e-03,  6.4426e-02,
         -1.1486e-02,  2.1962e-02]], dtype=torch.float64)
	q_value: tensor([[1.8335]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.301796330286938, distance: 0.9561978367218096 entropy -13.271011757907958
epoch: 0, step: 67
	action: tensor([[ 1.5465e-05,  9.5974e-02,  2.6456e-02, -3.4906e-03,  6.4477e-02,
          7.7324e-03,  2.2034e-02]], dtype=torch.float64)
	q_value: tensor([[1.8388]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30639552543770954, distance: 0.9530433083396666 entropy -13.271159708755333
epoch: 0, step: 68
	action: tensor([[-1.3071e-05,  9.5941e-02,  5.4394e-02,  1.4028e-02,  6.4474e-02,
          3.1352e-02,  2.2021e-02]], dtype=torch.float64)
	q_value: tensor([[1.8328]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3209332552796733, distance: 0.9430026748408262 entropy -13.271098825618092
epoch: 0, step: 69
	action: tensor([[-3.9375e-05,  9.5862e-02,  2.4314e-02, -7.5960e-03,  6.4442e-02,
          1.6756e-02,  2.1979e-02]], dtype=torch.float64)
	q_value: tensor([[1.8228]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30652708206597923, distance: 0.9529529217327463 entropy -13.270919378989019
epoch: 0, step: 70
	action: tensor([[-2.6235e-05,  9.5948e-02,  1.8028e-03,  1.0571e-02,  6.4486e-02,
          2.1102e-02,  2.2027e-02]], dtype=torch.float64)
	q_value: tensor([[1.8295]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31505792721270953, distance: 0.9470733480184943 entropy -13.271077355805096
epoch: 0, step: 71
	action: tensor([[-1.4505e-05,  9.6060e-02,  5.2730e-02,  6.7636e-03,  6.4550e-02,
          1.0192e-02,  2.2040e-02]], dtype=torch.float64)
	q_value: tensor([[1.8215]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3123356339551653, distance: 0.9489535469458721 entropy -13.271171751729316
epoch: 0, step: 72
	action: tensor([[-2.1395e-05,  9.5900e-02,  5.8809e-02,  3.0379e-02,  6.4446e-02,
          8.3292e-03,  2.1996e-02]], dtype=torch.float64)
	q_value: tensor([[1.8349]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32292679745175845, distance: 0.9416174668583815 entropy -13.270944343652841
epoch: 0, step: 73
	action: tensor([[-5.4788e-06,  9.5853e-02,  4.2047e-02, -2.0299e-02,  6.4411e-02,
          1.0718e-02,  2.1959e-02]], dtype=torch.float64)
	q_value: tensor([[1.8290]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29961352148865816, distance: 0.9576913610145026 entropy -13.270968773679131
epoch: 0, step: 74
	action: tensor([[-3.1341e-05,  9.5888e-02,  5.1166e-02, -3.1782e-03,  6.4448e-02,
         -1.4536e-02,  2.2027e-02]], dtype=torch.float64)
	q_value: tensor([[1.8380]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30145570528205456, distance: 0.9564310531829181 entropy -13.27100130790269
epoch: 0, step: 75
	action: tensor([[ 6.1442e-06,  9.5865e-02,  3.5605e-02,  5.6698e-04,  6.4407e-02,
         -2.2098e-02,  2.2005e-02]], dtype=torch.float64)
	q_value: tensor([[1.8457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3011403636688076, distance: 0.9566469081282432 entropy -13.271012785229122
epoch: 0, step: 76
	action: tensor([[ 2.1543e-05,  9.5945e-02,  4.4988e-02,  8.3928e-02,  6.4445e-02,
         -9.5684e-03,  2.2023e-02]], dtype=torch.float64)
	q_value: tensor([[1.8472]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3424230675050943, distance: 0.9279615427371946 entropy -13.271077570909346
epoch: 0, step: 77
	action: tensor([[4.0784e-05, 9.5961e-02, 1.3523e-02, 1.0090e-01, 6.4445e-02, 1.8943e-02,
         2.1933e-02]], dtype=torch.float64)
	q_value: tensor([[1.8233]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3541878597941349, distance: 0.919622938833096 entropy -13.271194342194415
epoch: 0, step: 78
	action: tensor([[ 2.8656e-05,  9.6058e-02,  4.0771e-02, -8.4392e-02,  6.4532e-02,
          1.7986e-02,  2.1952e-02]], dtype=torch.float64)
	q_value: tensor([[1.8018]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2719058754142115, distance: 0.9764510044815962 entropy -13.271309319912863
epoch: 0, step: 79
	action: tensor([[-0.0001,  0.0959,  0.0437, -0.0975,  0.0644,  0.0158,  0.0221]],
       dtype=torch.float64)
	q_value: tensor([[1.8510]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2649004204500812, distance: 0.9811372859900699 entropy -13.27096158365605
epoch: 0, step: 80
	action: tensor([[-0.0001,  0.0958,  0.0337,  0.0360,  0.0644,  0.0009,  0.0221]],
       dtype=torch.float64)
	q_value: tensor([[1.8556]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32254718590925213, distance: 0.9418813960535078 entropy -13.270926911503395
epoch: 0, step: 81
	action: tensor([[ 1.3824e-05,  9.5932e-02,  5.3651e-02, -4.3095e-02,  6.4456e-02,
         -2.8683e-02,  2.1979e-02]], dtype=torch.float64)
	q_value: tensor([[1.8267]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27937653615397906, distance: 0.9714286161753546 entropy -13.271130137210163
epoch: 0, step: 82
	action: tensor([[-5.6147e-06,  9.5832e-02,  5.7153e-02, -1.3130e-02,  6.4367e-02,
         -1.7018e-02,  2.2056e-02]], dtype=torch.float64)
	q_value: tensor([[1.8619]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29626161428255215, distance: 0.9599802837186888 entropy -13.271050952705068
epoch: 0, step: 83
	action: tensor([[ 8.6731e-07,  9.5841e-02,  3.9937e-02,  5.3971e-02,  6.4389e-02,
         -2.0952e-03,  2.2012e-02]], dtype=torch.float64)
	q_value: tensor([[1.8501]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3303039608071733, distance: 0.9364736425075934 entropy -13.270983765085433
epoch: 0, step: 84
	action: tensor([[2.4954e-05, 9.5920e-02, 2.5774e-02, 1.3998e-01, 6.4441e-02, 2.1384e-02,
         2.1957e-02]], dtype=torch.float64)
	q_value: tensor([[1.8245]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37265580957832667, distance: 0.9063785800490575 entropy -13.27114121776084
epoch: 0, step: 85
	action: tensor([[3.5574e-05, 9.6065e-02, 6.0008e-02, 1.3528e-02, 6.4523e-02, 1.6174e-02,
         2.1918e-02]], dtype=torch.float64)
	q_value: tensor([[1.7936]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31722969944749435, distance: 0.9455706949556294 entropy -13.271344420549843
epoch: 0, step: 86
	action: tensor([[-2.4400e-05,  9.5846e-02,  4.5879e-02, -8.1048e-03,  6.4420e-02,
          1.4229e-02,  2.1976e-02]], dtype=torch.float64)
	q_value: tensor([[1.8302]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30613227579931124, distance: 0.9532241495211821 entropy -13.270923883435058
epoch: 0, step: 87
	action: tensor([[-2.8894e-05,  9.5882e-02,  2.7665e-02, -2.0564e-02,  6.4445e-02,
          2.2076e-02,  2.2011e-02]], dtype=torch.float64)
	q_value: tensor([[1.8341]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3020720527409788, distance: 0.9560090155660252 entropy -13.270980551371684
epoch: 0, step: 88
	action: tensor([[-4.1846e-05,  9.5931e-02,  2.9392e-02,  3.4752e-02,  6.4481e-02,
         -5.4572e-04,  2.2038e-02]], dtype=torch.float64)
	q_value: tensor([[1.8311]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3215617714531408, distance: 0.9425661716224082 entropy -13.2710418953172
epoch: 0, step: 89
	action: tensor([[ 1.5997e-05,  9.5945e-02,  2.1224e-02, -6.8330e-03,  6.4463e-02,
          1.4508e-02,  2.1984e-02]], dtype=torch.float64)
	q_value: tensor([[1.8271]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3063942861658139, distance: 0.9530441597465584 entropy -13.271149239747272
epoch: 0, step: 90
	action: tensor([[-2.1940e-05,  9.5956e-02,  1.5649e-02,  9.3014e-02,  6.4488e-02,
          1.9277e-03,  2.2028e-02]], dtype=torch.float64)
	q_value: tensor([[1.8299]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3469935052435529, distance: 0.924731057082965 entropy -13.271101965119254
epoch: 0, step: 91
	action: tensor([[ 4.2330e-05,  9.6037e-02,  4.2995e-02,  2.3239e-02,  6.4506e-02,
         -8.0586e-03,  2.1955e-02]], dtype=torch.float64)
	q_value: tensor([[1.8113]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31521337450348264, distance: 0.9469658729882783 entropy -13.271322865670134
epoch: 0, step: 92
	action: tensor([[ 1.5239e-05,  9.5899e-02,  4.4454e-02, -5.0817e-03,  6.4429e-02,
          4.9769e-04,  2.1982e-02]], dtype=torch.float64)
	q_value: tensor([[1.8353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30419493649609963, distance: 0.954553964478548 entropy -13.271061454570011
epoch: 0, step: 93
	action: tensor([[-1.0512e-05,  9.5886e-02, -3.7796e-03,  7.4466e-03,  6.4435e-02,
         -5.6945e-03,  2.2009e-02]], dtype=torch.float64)
	q_value: tensor([[1.8389]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3072740403428903, distance: 0.9524395578822429 entropy -13.271022699464245
epoch: 0, step: 94
	action: tensor([[ 2.2080e-05,  9.6035e-02,  3.3112e-02,  5.6519e-02,  6.4516e-02,
         -4.3207e-03,  2.2043e-02]], dtype=torch.float64)
	q_value: tensor([[1.8308]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3307210603166273, distance: 0.9361819702326364 entropy -13.271255901242267
epoch: 0, step: 95
	action: tensor([[ 2.8230e-05,  9.5978e-02,  3.0956e-02,  4.3835e-02,  6.4472e-02,
         -1.2907e-02,  2.1969e-02]], dtype=torch.float64)
	q_value: tensor([[1.8258]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32286027749521196, distance: 0.9416637209520089 entropy -13.271149985648886
epoch: 0, step: 96
	action: tensor([[ 3.3290e-05,  9.5975e-02,  1.5846e-02, -2.3285e-02,  6.4464e-02,
         -2.5652e-03,  2.1983e-02]], dtype=torch.float64)
	q_value: tensor([[1.8320]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2949593829817515, distance: 0.9608680700144094 entropy -13.27113497412903
epoch: 0, step: 97
	action: tensor([[-1.1470e-05,  9.5998e-02,  4.0496e-02,  4.0673e-02,  6.4495e-02,
          7.9749e-03,  2.2062e-02]], dtype=torch.float64)
	q_value: tensor([[1.8422]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3268338600925452, distance: 0.938896726209709 entropy -13.271148224388053
epoch: 0, step: 98
	action: tensor([[ 3.5444e-06,  9.5948e-02,  4.3226e-02, -1.7649e-02,  6.4468e-02,
          2.2179e-02,  2.1974e-02]], dtype=torch.float64)
	q_value: tensor([[1.8254]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3037486111217643, distance: 0.9548600655537446 entropy -13.271070601829766
epoch: 0, step: 99
	action: tensor([[-4.4314e-05,  9.5886e-02,  2.5898e-02,  3.4207e-02,  6.4455e-02,
          1.4117e-02,  2.2023e-02]], dtype=torch.float64)
	q_value: tensor([[1.8327]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32464266135935327, distance: 0.9404235690989787 entropy -13.27096340131108
epoch: 0, step: 100
	action: tensor([[ 7.0959e-07,  9.5956e-02,  1.7674e-02, -2.4787e-02,  6.4485e-02,
          3.0573e-02,  2.1987e-02]], dtype=torch.float64)
	q_value: tensor([[1.8205]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30218502774601597, distance: 0.9559316368803895 entropy -13.271144127653551
epoch: 0, step: 101
	action: tensor([[-5.2007e-05,  9.5959e-02,  2.8113e-02, -2.2265e-02,  6.4502e-02,
          2.9068e-03,  2.2050e-02]], dtype=torch.float64)
	q_value: tensor([[1.8271]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2967060710628412, distance: 0.95967709069736 entropy -13.271059660703326
epoch: 0, step: 102
	action: tensor([[-1.9396e-05,  9.5928e-02,  3.7085e-02, -9.7330e-02,  6.4465e-02,
          1.8000e-02,  2.2041e-02]], dtype=torch.float64)
	q_value: tensor([[1.8397]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26590227076322814, distance: 0.980468473312246 entropy -13.271095844624936
epoch: 0, step: 103
	action: tensor([[-0.0001,  0.0959,  0.0204,  0.0519,  0.0644,  0.0082,  0.0221]],
       dtype=torch.float64)
	q_value: tensor([[1.8538]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3307105449501252, distance: 0.9361893246085767 entropy -13.270969776036138
epoch: 0, step: 104
	action: tensor([[ 1.9194e-05,  9.5980e-02,  3.4201e-02,  7.5608e-02,  6.4491e-02,
         -1.1972e-02,  2.1979e-02]], dtype=torch.float64)
	q_value: tensor([[1.8182]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3374514154967361, distance: 0.9314628931356174 entropy -13.271209693180477
epoch: 0, step: 105
	action: tensor([[ 4.5743e-05,  9.5956e-02,  2.2705e-02, -2.2053e-02,  6.4446e-02,
         -3.3507e-02,  2.1946e-02]], dtype=torch.float64)
	q_value: tensor([[1.8234]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28818307057562487, distance: 0.9654745932199628 entropy -13.271228699681163
epoch: 0, step: 106
	action: tensor([[ 2.5397e-05,  9.5933e-02,  2.7696e-02, -2.7639e-02,  6.4429e-02,
          3.8745e-02,  2.2056e-02]], dtype=torch.float64)
	q_value: tensor([[1.8542]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30304820106211994, distance: 0.955340226522634 entropy -13.271195348877209
epoch: 0, step: 107
	action: tensor([[-6.6146e-05,  9.5930e-02,  4.0551e-02, -1.1159e-02,  6.4491e-02,
          2.1261e-02,  2.2044e-02]], dtype=torch.float64)
	q_value: tensor([[1.8260]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30635084982658656, distance: 0.953074000980929 entropy -13.270969791481544
epoch: 0, step: 108
	action: tensor([[-3.8252e-05,  9.5897e-02,  3.6684e-02,  4.7742e-02,  6.4460e-02,
         -2.1165e-02,  2.2018e-02]], dtype=torch.float64)
	q_value: tensor([[1.8311]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3227816579742838, distance: 0.9417183854469207 entropy -13.270986719808993
epoch: 0, step: 109
	action: tensor([[ 4.4263e-05,  9.5925e-02,  3.4382e-02, -2.3658e-02,  6.4426e-02,
          3.5155e-02,  2.1968e-02]], dtype=torch.float64)
	q_value: tensor([[1.8337]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3040596238850276, distance: 0.9546467756099728 entropy -13.271133559640479
epoch: 0, step: 110
	action: tensor([[-6.3742e-05,  9.5945e-02,  5.6977e-02,  1.9827e-02,  6.4494e-02,
          2.2837e-02,  2.2042e-02]], dtype=torch.float64)
	q_value: tensor([[1.8290]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32152883978587987, distance: 0.942589047617275 entropy -13.270932946971536
epoch: 0, step: 111
	action: tensor([[-3.0617e-05,  9.5892e-02,  4.1878e-02,  8.8584e-02,  6.4449e-02,
          1.3462e-02,  2.1977e-02]], dtype=torch.float64)
	q_value: tensor([[1.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3496926436279958, distance: 0.9228179354869205 entropy -13.270915069543166
epoch: 0, step: 112
	action: tensor([[ 2.3978e-05,  9.5973e-02,  3.5790e-02, -3.5263e-02,  6.4474e-02,
          2.7814e-02,  2.1930e-02]], dtype=torch.float64)
	q_value: tensor([[1.8120]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29700639030151765, distance: 0.9594721690738166 entropy -13.271190885087611
epoch: 0, step: 113
	action: tensor([[-6.3092e-05,  9.5933e-02,  6.0782e-02, -1.7349e-02,  6.4484e-02,
          2.5715e-02,  2.2054e-02]], dtype=torch.float64)
	q_value: tensor([[1.8352]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3049857538645294, distance: 0.9540113610218226 entropy -13.27095656189549
epoch: 0, step: 114
	action: tensor([[-5.4174e-05,  9.5834e-02,  8.8023e-02,  8.3386e-02,  6.4427e-02,
          2.8439e-02,  2.2009e-02]], dtype=torch.float64)
	q_value: tensor([[1.8340]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3544411141860776, distance: 0.9194426066816205 entropy -13.270858290769748
epoch: 0, step: 115
	action: tensor([[-3.2124e-06,  9.5782e-02,  5.9035e-02, -2.0555e-02,  6.4362e-02,
          4.0670e-02,  2.1880e-02]], dtype=torch.float64)
	q_value: tensor([[1.8116]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30716180887208744, distance: 0.9525167091436597 entropy -13.27098184627245
epoch: 0, step: 116
	action: tensor([[-7.4940e-05,  9.5840e-02,  4.8425e-02, -5.9917e-02,  6.4439e-02,
         -7.1638e-03,  2.2012e-02]], dtype=torch.float64)
	q_value: tensor([[1.8280]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27670863552211067, distance: 0.973225172106759 entropy -13.270797908867994
epoch: 0, step: 117
	action: tensor([[-4.3726e-05,  9.5842e-02,  6.9096e-02,  1.8720e-02,  6.4395e-02,
          1.9894e-02,  2.2076e-02]], dtype=torch.float64)
	q_value: tensor([[1.8563]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3206989649762141, distance: 0.9431653372971268 entropy -13.271024020695988
epoch: 0, step: 118
	action: tensor([[-2.7463e-05,  9.5818e-02,  2.9264e-02, -5.3827e-02,  6.4402e-02,
         -4.9784e-02,  2.1962e-02]], dtype=torch.float64)
	q_value: tensor([[1.8286]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26958270540632623, distance: 0.9780075718030296 entropy -13.27088157186591
epoch: 0, step: 119
	action: tensor([[ 1.6822e-05,  9.5897e-02,  1.7763e-02, -2.1717e-02,  6.4373e-02,
          9.8353e-03,  2.2100e-02]], dtype=torch.float64)
	q_value: tensor([[1.8701]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29855753573286337, distance: 0.9584130536525267 entropy -13.271214543309465
epoch: 0, step: 120
	action: tensor([[-2.4480e-05,  9.5958e-02,  5.4026e-02, -1.0478e-03,  6.4487e-02,
          3.7636e-02,  2.2048e-02]], dtype=torch.float64)
	q_value: tensor([[1.8350]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31549580689557843, distance: 0.9467705702402852 entropy -13.271130377831273
epoch: 0, step: 121
	action: tensor([[-5.5813e-05,  9.5859e-02,  6.0460e-02,  3.0444e-02,  6.4447e-02,
          3.4549e-02,  2.1995e-02]], dtype=torch.float64)
	q_value: tensor([[1.8239]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3294915478690378, distance: 0.9370414916994523 entropy -13.27087546610088
epoch: 0, step: 122
	action: tensor([[-3.2757e-05,  9.5846e-02,  1.4125e-02,  5.3222e-02,  6.4431e-02,
          1.1292e-02,  2.1956e-02]], dtype=torch.float64)
	q_value: tensor([[1.8182]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33169165920099053, distance: 0.9355028910039979 entropy -13.270919630220543
epoch: 0, step: 123
	action: tensor([[ 1.8012e-05,  9.6007e-02,  5.4547e-02,  6.5544e-03,  6.4512e-02,
         -8.1460e-03,  2.1984e-02]], dtype=torch.float64)
	q_value: tensor([[1.8155]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30775219652791097, distance: 0.9521107889988801 entropy -13.27122039665421
epoch: 0, step: 124
	action: tensor([[ 2.9959e-06,  9.5858e-02,  1.6734e-02, -5.6902e-02,  6.4408e-02,
          5.5332e-03,  2.1991e-02]], dtype=torch.float64)
	q_value: tensor([[1.8413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2816635223251974, distance: 0.9698859178107959 entropy -13.270991774434048
epoch: 0, step: 125
	action: tensor([[-5.0980e-05,  9.5952e-02,  2.1770e-02,  4.9586e-02,  6.4471e-02,
          2.0932e-02,  2.2093e-02]], dtype=torch.float64)
	q_value: tensor([[1.8456]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3328873594348206, distance: 0.9346656413910027 entropy -13.271179540791554
epoch: 0, step: 126
	action: tensor([[ 3.7099e-06,  9.5976e-02,  2.1489e-02,  1.2255e-02,  6.4501e-02,
         -2.7211e-02,  2.1979e-02]], dtype=torch.float64)
	q_value: tensor([[1.8135]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30504383391180034, distance: 0.9539714983982225 entropy -13.271180476686949
epoch: 0, step: 127
	action: tensor([[ 4.1429e-05,  9.5956e-02,  7.4364e-02, -5.4468e-02,  6.4447e-02,
          1.5137e-02,  2.2020e-02]], dtype=torch.float64)
	q_value: tensor([[1.8427]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2848351860409646, distance: 0.9677423854188534 entropy -13.271166755265892
LOSS epoch 0 actor 1.3623701509462813 critic 19.99916276698808 entropy 0.01
epoch: 1, step: 0
	action: tensor([[-1.0413e+05, -6.2800e+00, -3.4848e+04, -3.7990e+04,  6.2800e+00,
          1.7230e+03, -1.7544e+03]], dtype=torch.float64)
	q_value: tensor([[2.7449]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4675068437070693, distance: 1.3862666038364615 entropy 2.3061288912340636
epoch: 1, step: 1
	action: tensor([[ 1.8944e+05, -6.2800e+00,  1.7589e+05, -7.4807e+03,  6.2800e+00,
         -2.4150e+05, -3.1081e+03]], dtype=torch.float64)
	q_value: tensor([[5.8221]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.49672935798458184, distance: 0.8118157853227526 entropy 2.9572124600535674
epoch: 1, step: 2
	action: tensor([[-1.3357e+05, -6.2800e+00,  1.0581e+05, -2.2188e+04,  6.2800e+00,
         -1.1219e+05,  9.5248e+03]], dtype=torch.float64)
	q_value: tensor([[7.0370]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.116970461047883, distance: 1.0753363066214467 entropy 3.0388636607607404
epoch: 1, step: 3
	action: tensor([[-7.8580e+04, -6.2800e+00, -5.7190e+04, -2.3726e+05,  6.2800e+00,
          3.6197e+05, -3.1461e+02]], dtype=torch.float64)
	q_value: tensor([[6.4353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6603772388989384, distance: 1.4745519537357885 entropy 3.0423998620675627
epoch: 1, step: 4
	action: tensor([[ 5.8562e+04, -6.2800e+00, -2.4512e+05, -1.9833e+05,  6.2800e+00,
          2.8169e+05,  2.4279e+03]], dtype=torch.float64)
	q_value: tensor([[5.3440]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4297746741611408, distance: 0.8641317141444129 entropy 2.910733143792618
epoch: 1, step: 5
	action: tensor([[-1.9276e+05, -6.2800e+00, -5.5037e+04,  2.8727e+05,  6.2800e+00,
          2.4776e+05,  6.9403e+03]], dtype=torch.float64)
	q_value: tensor([[5.7404]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1434455456698318, distance: 1.0590932063180405 entropy 3.058120159129536
epoch: 1, step: 6
	action: tensor([[-1.4037e+05, -6.2800e+00,  1.9121e+05, -7.7045e+04,  6.2800e+00,
         -1.2281e+05, -3.3489e+03]], dtype=torch.float64)
	q_value: tensor([[5.1268]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07502493824532142, distance: 1.1005802249554701 entropy 2.9332484486913963
epoch: 1, step: 7
	action: tensor([[ 1.9044e+05, -6.2800e+00,  1.1749e+05,  1.5197e+05,  6.2800e+00,
          9.8886e+04,  1.3916e+04]], dtype=torch.float64)
	q_value: tensor([[6.4475]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.43616480582673367, distance: 0.859276201270588 entropy 3.077453293363949
epoch: 1, step: 8
	action: tensor([[ 3.3942e+05, -6.2800e+00, -5.3643e+04,  5.4708e+04,  6.2800e+00,
          1.3255e+05, -5.8958e+03]], dtype=torch.float64)
	q_value: tensor([[4.8869]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.48341102235780076, distance: 0.8224874141295119 entropy 2.853915887609575
epoch: 1, step: 9
	action: tensor([[-1.1964e+05, -6.2800e+00,  4.4644e+04, -3.0727e+04,  6.2800e+00,
         -1.8772e+05, -7.7139e+03]], dtype=torch.float64)
	q_value: tensor([[4.7769]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10142877133513006, distance: 1.0847582147506012 entropy 2.8374167468866247
epoch: 1, step: 10
	action: tensor([[ 2.9906e+05, -6.2800e+00,  7.6847e+04, -2.1911e+05,  6.2800e+00,
         -7.4197e+04, -3.0819e+03]], dtype=torch.float64)
	q_value: tensor([[7.9043]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38115427243506295, distance: 0.9002184130744361 entropy 3.1251468757407688
epoch: 1, step: 11
	action: tensor([[-2.0797e+04, -6.2800e+00,  2.6979e+03, -2.4885e+05,  6.2800e+00,
          4.9307e+05,  2.0776e+03]], dtype=torch.float64)
	q_value: tensor([[7.0994]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.2120070176378293
epoch: 1, step: 12
	action: tensor([[ 3.1552e+04, -6.2800e+00, -8.0293e+04, -6.3079e+03,  6.2800e+00,
         -1.5275e+05, -3.6916e+03]], dtype=torch.float64)
	q_value: tensor([[4.8893]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4860485534478367, distance: 0.8203850539475651 entropy 2.8071939296256523
epoch: 1, step: 13
	action: tensor([[ 6.1827e+04, -6.2800e+00,  2.1884e+04, -2.1356e+05,  6.2800e+00,
         -8.0868e+04,  1.1775e+04]], dtype=torch.float64)
	q_value: tensor([[7.7751]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4375886684260427, distance: 0.8581905430314618 entropy 3.091231686458637
epoch: 1, step: 14
	action: tensor([[ 6.4794e+04, -6.2800e+00, -2.1879e+04,  1.5948e+05,  6.2800e+00,
          1.4757e+04, -5.3846e+03]], dtype=torch.float64)
	q_value: tensor([[5.6086]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4049889400196999, distance: 0.8827123696009324 entropy 2.9730823557934727
epoch: 1, step: 15
	action: tensor([[-1.3919e+05, -6.2800e+00,  1.0265e+05,  8.9839e+03,  6.2800e+00,
          6.9138e+04, -1.6232e+03]], dtype=torch.float64)
	q_value: tensor([[3.7062]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09728773407529334, distance: 1.0872548786738137 entropy 2.714168461440509
epoch: 1, step: 16
	action: tensor([[ 1.4978e+05, -6.2800e+00,  6.9698e+04,  2.2336e+03,  6.2800e+00,
         -1.5390e+05,  8.2131e+02]], dtype=torch.float64)
	q_value: tensor([[4.4788]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6963692181375654, distance: 0.6305646094235112 entropy 2.8248172609919493
epoch: 1, step: 17
	action: tensor([[-8.0780e+04, -6.2800e+00,  6.3677e+04,  1.6491e+04,  6.2800e+00,
          1.5139e+05, -1.0122e+04]], dtype=torch.float64)
	q_value: tensor([[5.5973]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1472411079692615, distance: 1.0567440752666168 entropy 2.886200351117447
epoch: 1, step: 18
	action: tensor([[ 5.0320e+04, -6.2800e+00,  3.6159e+04, -3.6116e+05,  6.2800e+00,
         -3.4772e+05, -7.5754e+03]], dtype=torch.float64)
	q_value: tensor([[5.1958]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4213264892775861, distance: 0.8705094590655398 entropy 2.9191527285420653
epoch: 1, step: 19
	action: tensor([[ 1.4168e+05, -6.2800e+00,  3.1178e+03, -2.8235e+04,  6.2800e+00,
          1.4087e+05, -5.6880e+03]], dtype=torch.float64)
	q_value: tensor([[7.2002]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6008687190200135, distance: 0.7229605109945655 entropy 3.066168207054083
epoch: 1, step: 20
	action: tensor([[-7.6095e+04, -6.2800e+00, -2.1121e+04,  2.1194e+05,  6.2800e+00,
          3.5477e+05,  9.3966e+03]], dtype=torch.float64)
	q_value: tensor([[4.5780]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10779482984739475, distance: 1.0809088210923734 entropy 2.913042532969314
epoch: 1, step: 21
	action: tensor([[-1.5600e+05, -6.2800e+00, -3.7461e+03,  1.1468e+05,  6.2800e+00,
          3.1023e+04, -8.4070e+03]], dtype=torch.float64)
	q_value: tensor([[5.9779]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14393772329691534, distance: 1.0587888842288065 entropy 2.9879954295664604
epoch: 1, step: 22
	action: tensor([[ 6.5641e+04, -6.2800e+00, -1.0648e+05, -5.2275e+04,  6.2800e+00,
          9.4857e+04, -1.8703e+02]], dtype=torch.float64)
	q_value: tensor([[3.9858]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6015083350058271, distance: 0.7223809992790264 entropy 2.77292780461779
epoch: 1, step: 23
	action: tensor([[ 5.7333e+04, -6.2800e+00,  7.4296e+04,  7.6210e+04,  6.2800e+00,
          3.3587e+05,  4.2352e+03]], dtype=torch.float64)
	q_value: tensor([[5.6369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44215817349728326, distance: 0.8546970999325504 entropy 3.0576780798486616
epoch: 1, step: 24
	action: tensor([[-3.6481e+04, -6.2800e+00, -1.2116e+05, -5.2906e+04,  6.2800e+00,
         -1.1359e+05, -7.0602e+03]], dtype=torch.float64)
	q_value: tensor([[4.5078]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13753289108805633, distance: 1.062742293031332 entropy 2.8626185306548773
epoch: 1, step: 25
	action: tensor([[ 1.7157e+05, -6.2800e+00, -3.1375e+04, -9.1846e+04,  6.2800e+00,
          1.5414e+05,  3.1991e+03]], dtype=torch.float64)
	q_value: tensor([[7.4639]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7383439299428207, distance: 0.5853587308136331 entropy 3.0949405574367677
epoch: 1, step: 26
	action: tensor([[ 1.3591e+05, -6.2800e+00, -2.5024e+05,  2.0117e+05,  6.2800e+00,
          9.1438e+04, -5.0653e+03]], dtype=torch.float64)
	q_value: tensor([[5.3121]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4186313592537173, distance: 0.87253427162293 entropy 2.985671615061425
epoch: 1, step: 27
	action: tensor([[ 5.4426e+04, -6.2800e+00,  1.7499e+05,  1.5163e+05,  6.2800e+00,
          5.0645e+04, -2.5409e+03]], dtype=torch.float64)
	q_value: tensor([[3.9622]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36670348166574607, distance: 0.9106683508665502 entropy 2.7016525957969475
epoch: 1, step: 28
	action: tensor([[-1.9043e+05, -6.2800e+00, -1.2177e+04, -8.4591e+03,  6.2800e+00,
         -5.3517e+04, -1.8641e+03]], dtype=torch.float64)
	q_value: tensor([[4.5343]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.794361966078694
epoch: 1, step: 29
	action: tensor([[-1.6137e+05, -6.2800e+00, -5.4497e+04, -5.4832e+04,  6.2800e+00,
         -6.1378e+04,  2.7623e+03]], dtype=torch.float64)
	q_value: tensor([[4.8893]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.8071939296256523
epoch: 1, step: 30
	action: tensor([[-4.0621e+04, -6.2800e+00, -5.5839e+04, -2.3224e+04,  6.2800e+00,
         -1.2122e+05, -1.5627e+03]], dtype=torch.float64)
	q_value: tensor([[4.8893]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.8071939296256523
epoch: 1, step: 31
	action: tensor([[-5.4232e+04, -6.2800e+00, -9.3100e+04,  2.9204e+05,  6.2800e+00,
         -1.4758e+04,  6.7935e+02]], dtype=torch.float64)
	q_value: tensor([[4.8893]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15241804413646476, distance: 1.0535315465783468 entropy 2.8071939296256523
epoch: 1, step: 32
	action: tensor([[ 3.3668e+04, -6.2800e+00, -3.2467e+05,  2.7104e+03,  6.2800e+00,
          2.6291e+05, -1.0380e+04]], dtype=torch.float64)
	q_value: tensor([[4.4566]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39832933750613975, distance: 0.8876384597615323 entropy 2.8690238232657728
epoch: 1, step: 33
	action: tensor([[ 8.7927e+04, -6.2800e+00, -2.8991e+05, -4.3051e+04,  6.2800e+00,
          4.3587e+04,  3.9355e+03]], dtype=torch.float64)
	q_value: tensor([[3.6257]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3423361621817803, distance: 0.928022860378966 entropy 2.675133986194723
epoch: 1, step: 34
	action: tensor([[-9.0679e+04, -6.2800e+00,  6.3639e+04,  9.0065e+04,  6.2800e+00,
          1.8943e+05, -5.3365e+03]], dtype=torch.float64)
	q_value: tensor([[5.2119]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12099040831244035, distance: 1.0728858074531908 entropy 2.995622864667509
epoch: 1, step: 35
	action: tensor([[-6.6921e+04, -6.2800e+00, -1.2585e+04, -4.1352e+04,  6.2800e+00,
         -2.7211e+05,  4.5652e+03]], dtype=torch.float64)
	q_value: tensor([[5.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1306838495713859, distance: 1.0669536841291092 entropy 2.8988307648556537
epoch: 1, step: 36
	action: tensor([[-3.1552e+05, -6.2800e+00, -8.1642e+03,  3.3500e+05,  6.2800e+00,
         -1.0497e+05,  4.6507e+03]], dtype=torch.float64)
	q_value: tensor([[6.8137]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7215789170998972, distance: 1.5014821066985262 entropy 3.107906408191089
epoch: 1, step: 37
	action: tensor([[ 3.6092e+04, -6.2800e+00, -4.5456e+04, -1.2348e+05,  6.2800e+00,
          2.6341e+05, -7.4030e+03]], dtype=torch.float64)
	q_value: tensor([[5.8961]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7618808239965584, distance: 0.5584108894579947 entropy 2.9184639615943326
epoch: 1, step: 38
	action: tensor([[-1.2627e+05, -6.2800e+00, -1.8662e+05, -1.5812e+05,  6.2800e+00,
         -8.3234e+04,  4.1233e+03]], dtype=torch.float64)
	q_value: tensor([[5.1949]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.985053719021984
epoch: 1, step: 39
	action: tensor([[ 8.4404e+04, -6.2800e+00, -1.6420e+05, -8.3543e+04,  6.2800e+00,
          1.5939e+05,  4.0977e+03]], dtype=torch.float64)
	q_value: tensor([[4.8893]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.8071939296256523
epoch: 1, step: 40
	action: tensor([[ 2.0716e+04, -6.2800e+00, -2.1608e+05,  9.7474e+04,  6.2800e+00,
          9.9779e+04, -3.7304e+03]], dtype=torch.float64)
	q_value: tensor([[4.8893]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.8071939296256523
epoch: 1, step: 41
	action: tensor([[ 3.0835e+05, -6.2800e+00,  3.5996e+04,  1.4571e+05,  6.2800e+00,
         -1.1906e+05, -6.6268e+02]], dtype=torch.float64)
	q_value: tensor([[4.8893]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.8071939296256523
epoch: 1, step: 42
	action: tensor([[-1.0908e+04, -6.2800e+00, -3.8204e+04, -9.6115e+03,  6.2800e+00,
         -1.2085e+05, -2.3659e+03]], dtype=torch.float64)
	q_value: tensor([[4.8893]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.8071939296256523
epoch: 1, step: 43
	action: tensor([[ 1.4961e+05, -6.2800e+00,  4.1903e+04, -1.5646e+05,  6.2800e+00,
         -3.1812e+05, -2.9046e+03]], dtype=torch.float64)
	q_value: tensor([[4.8893]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.8071939296256523
epoch: 1, step: 44
	action: tensor([[ 2.1294e+05, -6.2800e+00,  6.4728e+04,  4.2203e+04,  6.2800e+00,
          4.4068e+04,  1.5872e+03]], dtype=torch.float64)
	q_value: tensor([[4.8893]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39504706195155026, distance: 0.8900563201908676 entropy 2.8071939296256523
epoch: 1, step: 45
	action: tensor([[ 4.5755e+04, -6.2800e+00,  6.5884e+04,  1.9941e+04,  6.2800e+00,
          9.6984e+04, -1.0719e+04]], dtype=torch.float64)
	q_value: tensor([[4.7410]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3833416517404208, distance: 0.8986260434909766 entropy 2.8440808192749683
epoch: 1, step: 46
	action: tensor([[-2.3251e+05, -6.2800e+00, -2.4024e+05,  1.8648e+04,  6.2800e+00,
          3.6640e+04, -5.7838e+03]], dtype=torch.float64)
	q_value: tensor([[3.7355]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1049080135069389, distance: 1.0826561015103346 entropy 2.7071166241009874
epoch: 1, step: 47
	action: tensor([[ 1.1182e+05, -6.2800e+00, -1.7425e+05, -3.2452e+04,  6.2800e+00,
         -7.0885e+04, -2.3094e+02]], dtype=torch.float64)
	q_value: tensor([[4.6121]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4489814651387348, distance: 0.849453865744128 entropy 2.848648914578317
epoch: 1, step: 48
	action: tensor([[-4.2353e+04, -6.2800e+00, -2.3547e+05, -1.0089e+05,  6.2800e+00,
          2.4174e+05, -7.5767e+03]], dtype=torch.float64)
	q_value: tensor([[6.5271]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3653558125858327, distance: 1.3371484185980063 entropy 3.014166872364808
epoch: 1, step: 49
	action: tensor([[-9.4886e+04, -6.2800e+00, -2.1666e+05, -1.2778e+05,  6.2800e+00,
         -1.1242e+05, -1.8345e+02]], dtype=torch.float64)
	q_value: tensor([[6.2300]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04237859988983672, distance: 1.119833910764131 entropy 3.08381944962839
epoch: 1, step: 50
	action: tensor([[ 1.5467e+05, -6.2800e+00, -1.1210e+05,  1.7693e+05,  6.2800e+00,
          1.2508e+05,  4.1500e+03]], dtype=torch.float64)
	q_value: tensor([[6.5262]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44114648084135144, distance: 0.8554717810750226 entropy 3.076509917040514
epoch: 1, step: 51
	action: tensor([[ 5.2990e+04, -6.2800e+00,  2.5885e+05,  1.9911e+05,  6.2800e+00,
         -7.1726e+04,  1.8238e+04]], dtype=torch.float64)
	q_value: tensor([[5.6363]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4844869502103727, distance: 0.8216304481346306 entropy 3.018438663164724
epoch: 1, step: 52
	action: tensor([[-5.1750e+04, -6.2800e+00,  1.6110e+05,  1.7511e+05,  6.2800e+00,
          2.2748e+05, -3.0613e+03]], dtype=torch.float64)
	q_value: tensor([[5.6678]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06606113020165294, distance: 1.1059001543000342 entropy 2.8721737559430554
epoch: 1, step: 53
	action: tensor([[-1.2237e+05, -6.2800e+00,  1.3374e+05,  5.6020e+04,  6.2800e+00,
          1.9965e+05,  1.2948e+02]], dtype=torch.float64)
	q_value: tensor([[4.4112]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0463237893874231, distance: 1.1175247953698764 entropy 2.8002945124146543
epoch: 1, step: 54
	action: tensor([[-6.6971e+04, -6.2800e+00, -2.4692e+05, -1.2473e+05,  6.2800e+00,
         -1.2031e+04,  7.2696e+03]], dtype=torch.float64)
	q_value: tensor([[4.9265]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12240379792986056, distance: 1.0720228955259548 entropy 2.855144876206545
epoch: 1, step: 55
	action: tensor([[-1.0220e+05, -6.2800e+00,  3.6254e+05, -8.7740e+04,  6.2800e+00,
          1.9865e+05, -5.9248e+03]], dtype=torch.float64)
	q_value: tensor([[6.3484]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.39280703606947487, distance: 1.3505235736825207 entropy 3.0286264862219587
epoch: 1, step: 56
	action: tensor([[-1.5232e+05, -6.2800e+00,  2.0511e+05, -1.7333e+05,  6.2800e+00,
          8.9946e+04,  4.2146e+03]], dtype=torch.float64)
	q_value: tensor([[6.2379]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8918143285429294, distance: 1.5739681809996335 entropy 3.0780598499966842
epoch: 1, step: 57
	action: tensor([[ 3.4003e+05, -6.2800e+00, -1.7940e+04,  1.9736e+05,  6.2800e+00,
         -1.5780e+05,  4.2863e+03]], dtype=torch.float64)
	q_value: tensor([[5.7009]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7817363838031948, distance: 0.5346226649692339 entropy 2.9305838966580553
epoch: 1, step: 58
	action: tensor([[ 1.2384e+05, -6.2800e+00,  1.3204e+04,  1.6840e+03,  6.2800e+00,
         -1.7766e+03,  9.9060e+02]], dtype=torch.float64)
	q_value: tensor([[6.4542]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10739866637932893, distance: 1.0811487709812193 entropy 2.85939831099114
epoch: 1, step: 59
	action: tensor([[ 1.5766e+05, -6.2800e+00, -2.0868e+05,  3.3571e+05,  6.2800e+00,
         -3.7662e+05, -1.4117e+03]], dtype=torch.float64)
	q_value: tensor([[6.2188]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.632460724777352, distance: 0.6937589047527821 entropy 2.9535852958523017
epoch: 1, step: 60
	action: tensor([[ 3.4103e+05, -6.2800e+00, -8.0201e+03, -1.6548e+04,  6.2800e+00,
          5.6916e+04,  8.9746e+03]], dtype=torch.float64)
	q_value: tensor([[6.9393]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5482883743025913, distance: 0.7691079974796862 entropy 2.875856672853215
epoch: 1, step: 61
	action: tensor([[-3.7495e+05, -6.2800e+00,  1.5284e+04,  8.9132e+04,  6.2800e+00,
          1.4628e+05, -3.3670e+03]], dtype=torch.float64)
	q_value: tensor([[5.7648]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10030149293831914, distance: 1.0854384286208119 entropy 3.0668589944096385
epoch: 1, step: 62
	action: tensor([[-3.7042e+04, -6.2800e+00,  4.5402e+05, -1.7941e+05,  6.2800e+00,
         -1.8451e+05,  2.8915e+03]], dtype=torch.float64)
	q_value: tensor([[4.8785]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09317702876677403, distance: 1.0897275978517822 entropy 2.9330207921910265
epoch: 1, step: 63
	action: tensor([[-7.6213e+04, -6.2800e+00,  8.1253e+04, -8.9039e+04,  6.2800e+00,
         -1.4610e+05, -6.6352e+03]], dtype=torch.float64)
	q_value: tensor([[6.5579]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0920805033993024, distance: 1.0903862451236035 entropy 3.0629054274607053
epoch: 1, step: 64
	action: tensor([[ 7.2306e+04, -6.2800e+00,  2.8673e+05, -2.2189e+05,  6.2800e+00,
         -6.4373e+04, -2.0384e+03]], dtype=torch.float64)
	q_value: tensor([[6.2292]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4604277804180269, distance: 0.8405847060656426 entropy 3.044407175275886
epoch: 1, step: 65
	action: tensor([[-6.0748e+04, -6.2800e+00,  6.8194e+04, -9.0250e+04,  6.2800e+00,
         -1.1535e+05, -2.5049e+03]], dtype=torch.float64)
	q_value: tensor([[6.1721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0879998912757578, distance: 1.092833849280607 entropy 3.107603550691498
epoch: 1, step: 66
	action: tensor([[ 1.9631e+05, -6.2800e+00,  4.5399e+05, -7.5529e+03,  6.2800e+00,
          1.3956e+05,  4.7823e+03]], dtype=torch.float64)
	q_value: tensor([[8.5834]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.180233239511249
epoch: 1, step: 67
	action: tensor([[-1.6867e+05, -6.2800e+00, -3.8226e+04, -4.6735e+04,  6.2800e+00,
         -9.2442e+04,  3.1073e+03]], dtype=torch.float64)
	q_value: tensor([[4.8893]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1480733370879571, distance: 1.0562282976689723 entropy 2.8071939296256523
epoch: 1, step: 68
	action: tensor([[ 2.1657e+05, -6.2800e+00,  1.6771e+05,  3.3445e+04,  6.2800e+00,
          1.3628e+05,  1.0973e+04]], dtype=torch.float64)
	q_value: tensor([[6.9677]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4114564928225918, distance: 0.8779018813663755 entropy 3.1295983743157367
epoch: 1, step: 69
	action: tensor([[-1.3014e+04, -6.2800e+00,  1.1396e+05, -1.7813e+05,  6.2800e+00,
         -2.0304e+05, -2.2193e+03]], dtype=torch.float64)
	q_value: tensor([[4.9410]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1483922529070535, distance: 1.056030581446593 entropy 2.8358406637913114
epoch: 1, step: 70
	action: tensor([[ 5.3667e+04, -6.2800e+00, -9.6603e+04, -1.2124e+05,  6.2800e+00,
         -2.1272e+04, -7.6807e+03]], dtype=torch.float64)
	q_value: tensor([[6.2272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42817039888622577, distance: 0.8653464369828731 entropy 3.042003491153603
epoch: 1, step: 71
	action: tensor([[-3.0222e+05, -6.2800e+00, -7.9984e+04, -2.2136e+05,  6.2800e+00,
          2.1367e+05, -3.5046e+03]], dtype=torch.float64)
	q_value: tensor([[6.1323]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9011089214863666, distance: 1.577829941789246 entropy 3.068183228879431
epoch: 1, step: 72
	action: tensor([[-1.7456e+05, -6.2800e+00,  1.4492e+05, -4.3220e+04,  6.2800e+00,
          2.6545e+05, -2.3297e+03]], dtype=torch.float64)
	q_value: tensor([[5.4238]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7149964444412866, distance: 1.498608892377552 entropy 2.9334793752038206
epoch: 1, step: 73
	action: tensor([[-9.9044e+03, -6.2800e+00, -1.4951e+05, -1.3567e+04,  6.2800e+00,
         -1.0075e+05,  3.6417e+03]], dtype=torch.float64)
	q_value: tensor([[5.9202]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07996996225828257, distance: 1.0976343676592024 entropy 2.9787806518785365
epoch: 1, step: 74
	action: tensor([[-1.3322e+05, -6.2800e+00, -3.3149e+04,  9.7393e+03,  6.2800e+00,
          9.0653e+04,  9.9467e+03]], dtype=torch.float64)
	q_value: tensor([[7.6596]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.051360354085642324, distance: 1.1145699471395452 entropy 3.213673203824243
epoch: 1, step: 75
	action: tensor([[ 1.3291e+05, -6.2800e+00, -1.4264e+05,  1.1447e+05,  6.2800e+00,
         -1.8274e+05,  5.9001e+03]], dtype=torch.float64)
	q_value: tensor([[4.0944]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4993136867060718, distance: 0.8097287381615003 entropy 2.7925842505272676
epoch: 1, step: 76
	action: tensor([[ 1.4949e+05, -6.2800e+00,  1.6429e+05, -3.7152e+04,  6.2800e+00,
         -2.8245e+04, -7.1371e+03]], dtype=torch.float64)
	q_value: tensor([[6.2065]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4063722339798925, distance: 0.8816856986037729 entropy 2.9150583335447413
epoch: 1, step: 77
	action: tensor([[ 8.3695e+04, -6.2800e+00, -2.8131e+05,  2.9897e+05,  6.2800e+00,
         -1.5476e+05, -1.9592e+03]], dtype=torch.float64)
	q_value: tensor([[6.6879]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4809979480222921, distance: 0.8244061648281077 entropy 3.012430855188392
epoch: 1, step: 78
	action: tensor([[-2.5124e+05, -6.2800e+00, -1.2645e+05,  1.9527e+05,  6.2800e+00,
          3.0490e+05, -7.4547e+03]], dtype=torch.float64)
	q_value: tensor([[6.6842]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08055225395588705, distance: 1.0972869634906641 entropy 2.9425745321874825
epoch: 1, step: 79
	action: tensor([[ 8.7395e+04, -6.2800e+00, -9.1936e+04,  4.2049e+04,  6.2800e+00,
          2.5984e+05,  4.7468e+03]], dtype=torch.float64)
	q_value: tensor([[5.4989]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4271200698839811, distance: 0.8661408007913377 entropy 2.9350699370128837
epoch: 1, step: 80
	action: tensor([[ 1.6670e+05, -6.2800e+00, -3.5822e+04, -1.4861e+05,  6.2800e+00,
         -5.2169e+04,  5.5762e+03]], dtype=torch.float64)
	q_value: tensor([[5.4010]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.468193858189062, distance: 0.8345135017587854 entropy 2.8745424434129143
epoch: 1, step: 81
	action: tensor([[-3.3136e+05, -6.2800e+00,  3.2166e+05, -2.8852e+05,  6.2800e+00,
         -1.1326e+05, -1.0544e+04]], dtype=torch.float64)
	q_value: tensor([[8.2719]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10495406461865187, distance: 1.0826282506530376 entropy 3.162668677776136
epoch: 1, step: 82
	action: tensor([[ 9.3917e+04, -6.2800e+00, -2.0161e+05,  2.3925e+05,  6.2800e+00,
         -1.6667e+05,  1.8197e+03]], dtype=torch.float64)
	q_value: tensor([[6.3593]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8267183446415564, distance: 0.476357478040353 entropy 3.0345570958084074
epoch: 1, step: 83
	action: tensor([[-8.7090e+04, -6.2800e+00, -1.5534e+04, -1.1468e+05,  6.2800e+00,
         -7.8508e+04, -6.8297e+03]], dtype=torch.float64)
	q_value: tensor([[5.4430]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.02975054981897607, distance: 1.1271932928539046 entropy 2.8659474378847114
epoch: 1, step: 84
	action: tensor([[-1.9864e+05, -6.2800e+00,  2.0963e+05, -3.4180e+05,  6.2800e+00,
         -7.4182e+04,  1.0208e+03]], dtype=torch.float64)
	q_value: tensor([[6.8897]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06912523308937524, distance: 1.1040845238071886 entropy 3.1363751238138633
epoch: 1, step: 85
	action: tensor([[-9.4542e+04, -6.2800e+00,  2.0414e+05,  2.3763e+05,  6.2800e+00,
          2.8379e+05, -6.3416e+03]], dtype=torch.float64)
	q_value: tensor([[8.8140]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0733154884648648, distance: 1.1015967486730365 entropy 3.1967268188274542
epoch: 1, step: 86
	action: tensor([[-7.7608e+04, -6.2800e+00,  2.5924e+05, -3.9326e+03,  6.2800e+00,
          1.3651e+05, -2.7837e+03]], dtype=torch.float64)
	q_value: tensor([[4.5554]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9299002087906834, distance: 1.5897327449443923 entropy 2.8142814318089173
epoch: 1, step: 87
	action: tensor([[ 3.4760e+04, -6.2800e+00,  5.8668e+04,  1.2215e+05,  6.2800e+00,
          8.8306e+04, -4.7815e+03]], dtype=torch.float64)
	q_value: tensor([[4.9708]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.46909509511362246, distance: 0.8338060886327816 entropy 2.8440818207617817
epoch: 1, step: 88
	action: tensor([[ 1.3927e+05, -6.2800e+00,  1.4604e+05, -1.5039e+05,  6.2800e+00,
          9.4808e+04, -2.8172e+00]], dtype=torch.float64)
	q_value: tensor([[4.2904]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7592859778906598, distance: 0.561445219166349 entropy 2.735959722915299
epoch: 1, step: 89
	action: tensor([[-3.1437e+05, -6.2800e+00, -4.9307e+04,  2.6091e+04,  6.2800e+00,
         -1.2236e+05,  3.7227e+03]], dtype=torch.float64)
	q_value: tensor([[5.3350]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9168780117583257, distance: 1.5843602253828777 entropy 2.9801959815764287
epoch: 1, step: 90
	action: tensor([[-6.7308e+04, -6.2800e+00,  2.6702e+05,  1.1896e+05,  6.2800e+00,
         -3.7099e+05, -7.6276e+03]], dtype=torch.float64)
	q_value: tensor([[6.6001]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5387559623536571, distance: 1.4195201694551605 entropy 3.103815040032383
epoch: 1, step: 91
	action: tensor([[-3.0843e+05, -6.2800e+00, -1.3166e+04, -2.4758e+05,  6.2800e+00,
         -1.7825e+05, -7.4945e+03]], dtype=torch.float64)
	q_value: tensor([[6.3506]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0653373755532386, distance: 1.1063285792194784 entropy 2.9440023176269565
epoch: 1, step: 92
	action: tensor([[ 2.8467e+05, -6.2800e+00,  2.0996e+05, -1.7166e+05,  6.2800e+00,
         -5.4706e+04,  1.0695e+02]], dtype=torch.float64)
	q_value: tensor([[7.0315]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3862479632282495, distance: 0.8965059297400538 entropy 3.1177918976027237
epoch: 1, step: 93
	action: tensor([[-3.9661e+04, -6.2800e+00, -4.3783e+05,  9.1861e+04,  6.2800e+00,
          7.3332e+04,  6.1933e+03]], dtype=torch.float64)
	q_value: tensor([[6.4029]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11306560904605123, distance: 1.0777113105370464 entropy 2.999412631422475
epoch: 1, step: 94
	action: tensor([[ 7.9246e+04, -6.2800e+00,  3.3766e+05, -6.3705e+04,  6.2800e+00,
          2.4073e+05, -2.5893e+03]], dtype=torch.float64)
	q_value: tensor([[4.6924]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.40671297433173603, distance: 0.8814326199612307 entropy 2.8758569052583387
epoch: 1, step: 95
	action: tensor([[-8.5451e+04, -6.2800e+00, -3.3447e+04,  1.4509e+05,  6.2800e+00,
         -1.2575e+04, -6.0878e+03]], dtype=torch.float64)
	q_value: tensor([[4.8629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7780575916646049, distance: 1.52591241022227 entropy 2.949794753698201
epoch: 1, step: 96
	action: tensor([[-2.0927e+05, -6.2800e+00,  6.4153e+04,  2.5693e+05,  6.2800e+00,
         -2.7120e+05, -9.1819e+02]], dtype=torch.float64)
	q_value: tensor([[5.6986]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6593771155498671, distance: 1.4741077908232583 entropy 2.902027539812189
epoch: 1, step: 97
	action: tensor([[-2.1684e+05, -6.2800e+00,  1.7691e+05, -1.4916e+05,  6.2800e+00,
          1.9454e+05,  8.1494e+02]], dtype=torch.float64)
	q_value: tensor([[6.5192]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4021095207121894, distance: 1.3550261059652577 entropy 2.975988037878956
epoch: 1, step: 98
	action: tensor([[ 8.2447e+04, -6.2800e+00, -6.2301e+05,  3.3253e+05,  6.2800e+00,
         -2.6332e+05, -8.9998e+03]], dtype=torch.float64)
	q_value: tensor([[6.5187]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3195652018382531, distance: 0.9439520873748826 entropy 3.0886058040486595
epoch: 1, step: 99
	action: tensor([[ 1.4798e+05, -6.2800e+00,  1.7321e+05, -9.4460e+04,  6.2800e+00,
          1.1871e+05,  9.6088e+02]], dtype=torch.float64)
	q_value: tensor([[5.8186]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.723579274582711, distance: 0.6016473310122967 entropy 2.901205994622477
epoch: 1, step: 100
	action: tensor([[-1.0963e+05, -6.2800e+00,  1.9050e+05, -1.1459e+05,  6.2800e+00,
         -6.5173e+04,  3.8926e+03]], dtype=torch.float64)
	q_value: tensor([[4.2427]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10075245676061306, distance: 1.0851663625828822 entropy 2.8806046359458284
epoch: 1, step: 101
	action: tensor([[-1.3302e+05, -6.2800e+00,  2.1334e+05, -5.5607e+04,  6.2800e+00,
          4.6434e+04,  1.1139e+03]], dtype=torch.float64)
	q_value: tensor([[7.1418]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6388002189858972, distance: 1.4649395358745225 entropy 3.1510549120998244
epoch: 1, step: 102
	action: tensor([[-4.5488e+04, -6.2800e+00, -5.2820e+04,  1.1586e+05,  6.2800e+00,
          3.4219e+05,  8.5363e+03]], dtype=torch.float64)
	q_value: tensor([[6.2683]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06234248357575445, distance: 1.1080996378646946 entropy 3.0255733513266145
epoch: 1, step: 103
	action: tensor([[ 1.0841e+05, -6.2800e+00,  1.1558e+05, -5.8067e+04,  6.2800e+00,
         -2.9830e+05,  6.1898e+03]], dtype=torch.float64)
	q_value: tensor([[5.4785]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3591471307618628, distance: 0.9160851851250861 entropy 2.9317658170602283
epoch: 1, step: 104
	action: tensor([[-2.6899e+05, -6.2800e+00, -4.5158e+05, -3.0042e+04,  6.2800e+00,
          2.0211e+05,  4.4094e+03]], dtype=torch.float64)
	q_value: tensor([[6.5606]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 3.1247276414092116
epoch: 1, step: 105
	action: tensor([[ 2.5120e+04, -6.2800e+00,  7.5864e+04, -8.2391e+04,  6.2800e+00,
          2.4103e+04, -1.4423e+03]], dtype=torch.float64)
	q_value: tensor([[4.8893]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.8071939296256523
epoch: 1, step: 106
	action: tensor([[-1.6358e+05, -6.2800e+00,  2.4800e+04, -3.3316e+04,  6.2800e+00,
         -5.1634e+03, -2.2013e+03]], dtype=torch.float64)
	q_value: tensor([[4.8893]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.8071939296256523
epoch: 1, step: 107
	action: tensor([[ 1.0223e+04, -6.2800e+00, -2.2738e+05,  5.8553e+04,  6.2800e+00,
         -7.9771e+04,  3.9305e+03]], dtype=torch.float64)
	q_value: tensor([[4.8893]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.8071939296256523
epoch: 1, step: 108
	action: tensor([[ 8.0402e+04, -6.2800e+00, -1.7011e+05, -1.3101e+05,  6.2800e+00,
          2.0693e+04, -1.2105e+03]], dtype=torch.float64)
	q_value: tensor([[4.8893]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.8071939296256523
epoch: 1, step: 109
	action: tensor([[-2.5152e+05, -6.2800e+00,  1.3308e+04, -1.4539e+04,  6.2800e+00,
         -1.6450e+05, -8.1338e+03]], dtype=torch.float64)
	q_value: tensor([[4.8893]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.8071939296256523
epoch: 1, step: 110
	action: tensor([[ 3.5427e+05, -6.2800e+00,  9.9579e+04, -1.1755e+05,  6.2800e+00,
         -5.4067e+04,  4.5402e+02]], dtype=torch.float64)
	q_value: tensor([[4.8893]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.8071939296256523
epoch: 1, step: 111
	action: tensor([[-9.4407e+04, -6.2800e+00,  2.0384e+05, -1.0021e+05,  6.2800e+00,
          4.9093e+04,  8.6359e+03]], dtype=torch.float64)
	q_value: tensor([[4.8893]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 2.8071939296256523
epoch: 1, step: 112
	action: tensor([[ 1.5630e+05, -6.2800e+00,  2.6974e+04, -8.4332e+04,  6.2800e+00,
          2.4372e+05, -3.5878e+03]], dtype=torch.float64)
	q_value: tensor([[4.8893]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4163386562741743, distance: 0.8742530550187575 entropy 2.8071939296256523
epoch: 1, step: 113
	action: tensor([[ 1.5611e+04, -6.2800e+00, -7.9692e+04,  1.8736e+05,  6.2800e+00,
         -1.4633e+05, -3.3034e+03]], dtype=torch.float64)
	q_value: tensor([[3.5664]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5416704365722602, distance: 0.7747215361434493 entropy 2.659646351365823
epoch: 1, step: 114
	action: tensor([[-3.0039e+05, -6.2800e+00, -2.2113e+05,  2.5492e+05,  6.2800e+00,
         -1.1421e+05,  3.5529e+02]], dtype=torch.float64)
	q_value: tensor([[6.7174]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.28393058176444064, distance: 1.296664034353311 entropy 2.9258710533076124
epoch: 1, step: 115
	action: tensor([[-2.0746e+05, -6.2800e+00, -4.6896e+04, -1.6968e+05,  6.2800e+00,
         -1.1987e+05,  1.9605e+03]], dtype=torch.float64)
	q_value: tensor([[5.8942]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09894966302594155, distance: 1.0862535780066829 entropy 2.977954600783625
epoch: 1, step: 116
	action: tensor([[ 3.1815e+05, -6.2800e+00, -3.8134e+04,  1.2508e+05,  6.2800e+00,
          2.3901e+04,  4.1977e+03]], dtype=torch.float64)
	q_value: tensor([[7.1686]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.43574421038258715, distance: 0.8595966320329408 entropy 3.084394316476524
epoch: 1, step: 117
	action: tensor([[-1.2129e+05, -6.2800e+00,  2.3581e+05, -1.4274e+04,  6.2800e+00,
          1.0881e+05,  9.3093e+03]], dtype=torch.float64)
	q_value: tensor([[3.9451]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.29260656373945704, distance: 1.301037671485421 entropy 2.768813570300179
epoch: 1, step: 118
	action: tensor([[-9.0662e+04, -6.2800e+00, -1.0031e+05, -6.8504e+04,  6.2800e+00,
         -1.1310e+05,  2.6164e+03]], dtype=torch.float64)
	q_value: tensor([[6.3792]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12708691989486054, distance: 1.0691587482657046 entropy 3.047885934687423
epoch: 1, step: 119
	action: tensor([[ 3.0308e+05, -6.2800e+00,  6.5797e+04,  1.1161e+05,  6.2800e+00,
         -1.3979e+05,  5.1317e+03]], dtype=torch.float64)
	q_value: tensor([[7.6883]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7980910661656764, distance: 0.5142028282372059 entropy 3.122623400780508
epoch: 1, step: 120
	action: tensor([[-1.0620e+05, -6.2800e+00, -4.3389e+03, -6.5031e+04,  6.2800e+00,
          1.2546e+05,  1.5303e+03]], dtype=torch.float64)
	q_value: tensor([[5.3195]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.49831667750798947, distance: 1.4007431265851573 entropy 2.802217366531996
epoch: 1, step: 121
	action: tensor([[-1.0496e+05, -6.2800e+00, -1.7877e+05, -1.9019e+05,  6.2800e+00,
          7.5620e+04, -1.5058e+04]], dtype=torch.float64)
	q_value: tensor([[6.9900]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2719485583146939, distance: 1.2905994240837626 entropy 3.0940069714078438
epoch: 1, step: 122
	action: tensor([[ 1.0229e+04, -6.2800e+00, -2.4975e+05,  1.0339e+05,  6.2800e+00,
         -9.0171e+04,  4.5977e+03]], dtype=torch.float64)
	q_value: tensor([[6.3305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5893991681884336, distance: 0.7332745392564589 entropy 3.093939099282736
epoch: 1, step: 123
	action: tensor([[-9.3796e+04, -6.2800e+00, -6.8007e+04,  2.7023e+05,  6.2800e+00,
          2.1062e+05,  3.8336e+03]], dtype=torch.float64)
	q_value: tensor([[5.5588]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10147438517392637, distance: 1.084730681811246 entropy 2.8855954566556803
epoch: 1, step: 124
	action: tensor([[ 4.7401e+04, -6.2800e+00, -5.8663e+03, -6.3558e+04,  6.2800e+00,
         -1.9881e+05, -2.6994e+03]], dtype=torch.float64)
	q_value: tensor([[5.0149]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39926292652970874, distance: 0.8869495353336052 entropy 2.909214424449307
epoch: 1, step: 125
	action: tensor([[ 2.2898e+05, -6.2800e+00, -2.6963e+05,  1.3624e+05,  6.2800e+00,
          2.9754e+05, -2.1555e+04]], dtype=torch.float64)
	q_value: tensor([[7.9305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38700986182001984, distance: 0.8959493053413004 entropy 3.0959149202026794
epoch: 1, step: 126
	action: tensor([[-1.8385e+04, -6.2800e+00, -1.2630e+05, -6.5858e+04,  6.2800e+00,
          1.8590e+05,  3.6433e+03]], dtype=torch.float64)
	q_value: tensor([[4.6383]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.43491343907616686, distance: 1.3707856355932115 entropy 2.8880335409338054
epoch: 1, step: 127
	action: tensor([[ 1.8622e+05, -6.2800e+00,  1.4159e+05, -1.2936e+05,  6.2800e+00,
         -9.4765e+04, -3.7131e+03]], dtype=torch.float64)
	q_value: tensor([[5.6301]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.392379629167394, distance: 0.8920164346870934 entropy 2.9473232793938577
LOSS epoch 1 actor 94.02871236537021 critic 1187.2598270262863 entropy 50
epoch: 2, step: 0
	action: tensor([[ 6.6700e+06, -6.2800e+00,  8.8707e+06,  1.9341e+06,  6.2800e+00,
         -3.3280e+06, -9.9979e+06]], dtype=torch.float64)
	q_value: tensor([[4.9986]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2131325513446487, distance: 1.0150968570347625 entropy 6.5204181912999175
epoch: 2, step: 1
	action: tensor([[-1.2168e+07, -6.2800e+00, -2.7355e+07,  2.5844e+07,  6.2800e+00,
         -1.2037e+06,  1.2695e+06]], dtype=torch.float64)
	q_value: tensor([[5.8061]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7534655433000736, distance: 1.5153233357086209 entropy 6.525998160497102
epoch: 2, step: 2
	action: tensor([[-9.0912e+06, -6.2800e+00,  7.1135e+06,  5.4557e+06,  6.2800e+00,
         -4.5552e+06,  7.2014e+06]], dtype=torch.float64)
	q_value: tensor([[3.6540]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3101051094917988, distance: 1.309814408325251 entropy 6.416145448148733
epoch: 2, step: 3
	action: tensor([[ 2.5831e+07, -6.2800e+00, -1.1301e+07, -1.2545e+07,  6.2800e+00,
          2.4172e+07, -1.3080e+07]], dtype=torch.float64)
	q_value: tensor([[4.1837]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7820169173256781, distance: 0.5342789801053056 entropy 6.393928379451536
epoch: 2, step: 4
	action: tensor([[-1.1946e+06, -6.2800e+00,  2.6438e+07, -1.5460e+07,  6.2800e+00,
         -1.1833e+07,  2.1697e+06]], dtype=torch.float64)
	q_value: tensor([[3.2453]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 6.3928771104592546
epoch: 2, step: 5
	action: tensor([[ 1.1993e+07, -6.2800e+00, -1.3938e+07,  3.2847e+06,  6.2800e+00,
          2.4539e+06, -1.1348e+07]], dtype=torch.float64)
	q_value: tensor([[3.4789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3810573718174429, distance: 0.9002888896907028 entropy 6.289170933770541
epoch: 2, step: 6
	action: tensor([[ 4.5152e+06, -6.2800e+00, -8.2690e+05, -4.8781e+06,  6.2800e+00,
          7.9099e+06,  9.6054e+05]], dtype=torch.float64)
	q_value: tensor([[2.5387]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7659963411814756, distance: 0.5535642274198478 entropy 6.168050507595404
epoch: 2, step: 7
	action: tensor([[-1.3083e+07, -6.2800e+00, -4.0658e+05,  3.5681e+06,  6.2800e+00,
          1.1998e+07,  1.2360e+07]], dtype=torch.float64)
	q_value: tensor([[3.4273]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10063049844293326, distance: 1.085239946646601 entropy 6.443001434569616
epoch: 2, step: 8
	action: tensor([[-7.2584e+06, -6.2800e+00, -1.3129e+07, -1.0573e+07,  6.2800e+00,
         -1.2548e+07,  1.7722e+06]], dtype=torch.float64)
	q_value: tensor([[3.3238]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13386650051278082, distance: 1.0649987826340541 entropy 6.3881138588197155
epoch: 2, step: 9
	action: tensor([[ 2.3004e+07, -6.2800e+00, -7.1891e+06, -1.3087e+07,  6.2800e+00,
          3.5490e+04, -1.3016e+06]], dtype=torch.float64)
	q_value: tensor([[5.3104]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6129503228214965, distance: 0.7119345146006544 entropy 6.586564140392336
epoch: 2, step: 10
	action: tensor([[-1.1699e+07, -6.2800e+00, -3.2398e+06,  1.1585e+07,  6.2800e+00,
         -2.2566e+05, -8.9908e+05]], dtype=torch.float64)
	q_value: tensor([[3.9474]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4234276502710639, distance: 1.3652883744001134 entropy 6.527097653580763
epoch: 2, step: 11
	action: tensor([[-2.0348e+07, -6.2800e+00, -6.2327e+06, -3.1532e+06,  6.2800e+00,
          2.7926e+07, -5.1223e+06]], dtype=torch.float64)
	q_value: tensor([[4.0242]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9229292386675736, distance: 1.5868590199701638 entropy 6.500924032288209
epoch: 2, step: 12
	action: tensor([[-1.0013e+07, -6.2800e+00,  1.9708e+07,  9.9360e+06,  6.2800e+00,
         -5.8393e+06, -8.7035e+06]], dtype=torch.float64)
	q_value: tensor([[4.2674]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6095637251328121, distance: 1.4518133351321625 entropy 6.4758723314241005
epoch: 2, step: 13
	action: tensor([[-5.1132e+05, -6.2800e+00,  8.6732e+05,  6.2989e+05,  6.2800e+00,
         -9.4449e+05, -8.1968e+05]], dtype=torch.float64)
	q_value: tensor([[4.6788]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.40518099188302426, distance: 1.3565094590011362 entropy 6.471644494476891
epoch: 2, step: 14
	action: tensor([[ 2.1441e+07, -6.2800e+00, -6.2160e+06,  9.7241e+06,  6.2800e+00,
         -3.7038e+06, -1.3288e+06]], dtype=torch.float64)
	q_value: tensor([[3.9801]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8179399737807502, distance: 0.4882744435291111 entropy 6.48313718884576
epoch: 2, step: 15
	action: tensor([[-6.2172e+05, -6.2800e+00,  8.8352e+06,  1.3997e+07,  6.2800e+00,
          6.4139e+06, -5.7564e+06]], dtype=torch.float64)
	q_value: tensor([[4.4429]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06600499221363032, distance: 1.105933390991812 entropy 6.296369859123751
epoch: 2, step: 16
	action: tensor([[-7.4540e+06, -6.2800e+00, -9.8473e+06,  5.9604e+06,  6.2800e+00,
          1.7892e+07, -5.6577e+06]], dtype=torch.float64)
	q_value: tensor([[3.0860]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06773941383167292, distance: 1.1049060588899564 entropy 6.346019081120076
epoch: 2, step: 17
	action: tensor([[-6.4089e+06, -6.2800e+00,  5.5092e+06, -1.0776e+07,  6.2800e+00,
         -2.5668e+06,  1.2908e+07]], dtype=torch.float64)
	q_value: tensor([[3.1473]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11953957049742259, distance: 1.0737708612372001 entropy 6.349064501365835
epoch: 2, step: 18
	action: tensor([[ 5.1301e+06, -6.2800e+00, -2.8151e+07, -1.4717e+07,  6.2800e+00,
         -4.8595e+06, -1.3780e+07]], dtype=torch.float64)
	q_value: tensor([[4.3409]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4983559884942319, distance: 0.8105027809817179 entropy 6.51179180408303
epoch: 2, step: 19
	action: tensor([[-2.9148e+05, -6.2800e+00, -1.2295e+06,  8.6543e+06,  6.2800e+00,
         -3.3346e+06, -5.1329e+06]], dtype=torch.float64)
	q_value: tensor([[3.7869]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.44000769638093584, distance: 1.3732167745073052 entropy 6.419702800631731
epoch: 2, step: 20
	action: tensor([[-9.0721e+06, -6.2800e+00,  2.7931e+06,  1.6470e+06,  6.2800e+00,
         -4.7281e+06,  1.4211e+06]], dtype=torch.float64)
	q_value: tensor([[3.9424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8849839350689521, distance: 1.57112420647615 entropy 6.360240240374656
epoch: 2, step: 21
	action: tensor([[-1.5057e+06, -6.2800e+00,  2.5234e+07,  1.3639e+07,  6.2800e+00,
         -1.0948e+06,  6.3892e+06]], dtype=torch.float64)
	q_value: tensor([[4.1914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.932479111951106, distance: 1.5907945610108876 entropy 6.456124970625624
epoch: 2, step: 22
	action: tensor([[-8.3431e+05, -6.2800e+00,  2.4320e+07,  3.8923e+06,  6.2800e+00,
         -1.1262e+06,  4.2503e+06]], dtype=torch.float64)
	q_value: tensor([[4.1019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.45166070214678666, distance: 1.3787618350715805 entropy 6.503192434234384
epoch: 2, step: 23
	action: tensor([[-3.6163e+06, -6.2800e+00, -4.3204e+06, -1.3946e+06,  6.2800e+00,
         -1.1760e+06,  4.6905e+06]], dtype=torch.float64)
	q_value: tensor([[4.3790]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04915510601239492, distance: 1.1158646834813706 entropy 6.527827806453326
epoch: 2, step: 24
	action: tensor([[-1.2675e+07, -6.2800e+00, -1.0128e+07, -1.8585e+07,  6.2800e+00,
         -6.4889e+06,  1.5894e+06]], dtype=torch.float64)
	q_value: tensor([[5.0714]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04848080339198424, distance: 1.1162602775122394 entropy 6.579787349839177
epoch: 2, step: 25
	action: tensor([[ 2.3228e+07, -6.2800e+00, -1.4747e+07,  8.9348e+06,  6.2800e+00,
         -1.8768e+06, -5.0667e+06]], dtype=torch.float64)
	q_value: tensor([[5.0710]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7731831943442364, distance: 0.5449972466158407 entropy 6.6917663796555775
epoch: 2, step: 26
	action: tensor([[-1.1270e+06, -6.2800e+00,  6.5584e+06,  1.5346e+07,  6.2800e+00,
          1.7538e+07, -1.2895e+07]], dtype=torch.float64)
	q_value: tensor([[3.9327]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.02967375643006842, distance: 1.1272378995682808 entropy 6.320980391485449
epoch: 2, step: 27
	action: tensor([[ 1.3083e+07, -6.2800e+00,  3.5143e+06, -1.3119e+07,  6.2800e+00,
          1.7294e+07,  1.0302e+07]], dtype=torch.float64)
	q_value: tensor([[2.6650]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5554442503173487, distance: 0.7629916919854479 entropy 6.248163149325147
epoch: 2, step: 28
	action: tensor([[ 6.4538e+06, -6.2800e+00, -2.0048e+06,  1.1803e+07,  6.2800e+00,
         -1.1393e+07,  2.4735e+05]], dtype=torch.float64)
	q_value: tensor([[2.9548]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7800077975247076, distance: 0.5367355202721866 entropy 6.329170726105381
epoch: 2, step: 29
	action: tensor([[-4.9751e+06, -6.2800e+00, -6.5607e+06,  4.2978e+06,  6.2800e+00,
         -1.4236e+07, -6.6389e+06]], dtype=torch.float64)
	q_value: tensor([[5.4887]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9108073805491466, distance: 1.5818494517171553 entropy 6.496559213968843
epoch: 2, step: 30
	action: tensor([[-2.0243e+07, -6.2800e+00,  2.1593e+07, -3.5124e+07,  6.2800e+00,
          1.1102e+07, -9.3965e+06]], dtype=torch.float64)
	q_value: tensor([[3.9378]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5992009546289931, distance: 1.4471322212405249 entropy 6.5364315493899685
epoch: 2, step: 31
	action: tensor([[-5.8390e+05, -6.2800e+00,  9.4117e+06, -1.1366e+07,  6.2800e+00,
          3.1515e+07, -7.2239e+06]], dtype=torch.float64)
	q_value: tensor([[4.2200]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8599796835414324, distance: 1.560668962466861 entropy 6.480030698682349
epoch: 2, step: 32
	action: tensor([[-1.5221e+06, -6.2800e+00, -7.7984e+06,  6.9079e+06,  6.2800e+00,
         -1.3625e+06,  8.5663e+06]], dtype=torch.float64)
	q_value: tensor([[3.7874]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5211057932901257, distance: 1.4113554462032043 entropy 6.387437750587259
epoch: 2, step: 33
	action: tensor([[ 1.4919e+07, -6.2800e+00, -5.7434e+06, -1.3802e+07,  6.2800e+00,
          2.6919e+07,  3.3801e+06]], dtype=torch.float64)
	q_value: tensor([[4.3673]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7223896654656018, distance: 0.6029405711445045 entropy 6.411098637213619
epoch: 2, step: 34
	action: tensor([[-2.6260e+07, -6.2800e+00, -4.9878e+06,  5.1843e+06,  6.2800e+00,
         -1.2687e+07,  9.2628e+06]], dtype=torch.float64)
	q_value: tensor([[4.3160]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7890636574104484, distance: 1.5306277759132045 entropy 6.619105501568931
epoch: 2, step: 35
	action: tensor([[ 2.7125e+06, -6.2800e+00,  1.1574e+07,  2.2268e+07,  6.2800e+00,
         -2.5708e+07, -6.7981e+06]], dtype=torch.float64)
	q_value: tensor([[3.7592]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22534125738060573, distance: 1.0071911627821821 entropy 6.483725867081822
epoch: 2, step: 36
	action: tensor([[-5.2495e+06, -6.2800e+00, -1.0965e+07,  2.2614e+07,  6.2800e+00,
         -3.4139e+05, -3.6203e+06]], dtype=torch.float64)
	q_value: tensor([[4.2928]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7887618605998472, distance: 1.5304986698129825 entropy 6.418704583627433
epoch: 2, step: 37
	action: tensor([[ 7.2550e+06, -6.2800e+00,  1.4867e+07,  3.3635e+07,  6.2800e+00,
          5.1167e+06, -5.8033e+05]], dtype=torch.float64)
	q_value: tensor([[4.2375]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4226216227233274, distance: 0.8695347665780275 entropy 6.446914019377297
epoch: 2, step: 38
	action: tensor([[ 9.7388e+06, -6.2800e+00, -1.6094e+04,  1.1320e+07,  6.2800e+00,
         -1.7713e+06,  1.1646e+07]], dtype=torch.float64)
	q_value: tensor([[2.7565]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3625181735344628, distance: 0.9136725922121398 entropy 6.256304194582208
epoch: 2, step: 39
	action: tensor([[ 1.5694e+06, -6.2800e+00, -1.0934e+07,  1.1799e+07,  6.2800e+00,
          8.9528e+04, -1.1029e+07]], dtype=torch.float64)
	q_value: tensor([[4.1248]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.49413746962719873, distance: 0.8139035622951203 entropy 6.500340976338028
epoch: 2, step: 40
	action: tensor([[ 3.5867e+06, -6.2800e+00,  4.1059e+05, -9.8147e+06,  6.2800e+00,
          3.2643e+06,  4.9781e+05]], dtype=torch.float64)
	q_value: tensor([[3.2134]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6509867208133985, distance: 0.6760482125712973 entropy 6.339781315024875
epoch: 2, step: 41
	action: tensor([[-5.3119e+06, -6.2800e+00, -2.2719e+06, -9.7030e+06,  6.2800e+00,
         -1.7968e+07,  2.4583e+05]], dtype=torch.float64)
	q_value: tensor([[4.3169]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.041429327039457786, distance: 1.1203888088787095 entropy 6.5894687099548035
epoch: 2, step: 42
	action: tensor([[ 1.5348e+07, -6.2800e+00,  4.6749e+05, -1.2280e+07,  6.2800e+00,
          4.1141e+06, -3.1130e+06]], dtype=torch.float64)
	q_value: tensor([[4.8598]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26352464188747204, distance: 0.9820549825199407 entropy 6.579407576790951
epoch: 2, step: 43
	action: tensor([[ 2.3522e+07, -6.2800e+00,  2.2019e+06,  4.5989e+06,  6.2800e+00,
         -8.7667e+06, -1.4572e+06]], dtype=torch.float64)
	q_value: tensor([[3.5135]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33353587743381097, distance: 0.9342112242211744 entropy 6.415554510246355
epoch: 2, step: 44
	action: tensor([[-5.5983e+05, -6.2800e+00, -5.2804e+06, -1.0562e+07,  6.2800e+00,
         -5.9666e+06,  3.3355e+05]], dtype=torch.float64)
	q_value: tensor([[5.0935]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12801903506010615, distance: 1.068587760563573 entropy 6.40948243103566
epoch: 2, step: 45
	action: tensor([[-8.4238e+06, -6.2800e+00,  8.3682e+06,  4.0986e+06,  6.2800e+00,
          1.7002e+07,  1.3309e+06]], dtype=torch.float64)
	q_value: tensor([[5.1774]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1405345277219504, distance: 1.060891354885841 entropy 6.619232696438268
epoch: 2, step: 46
	action: tensor([[-3.3025e+06, -6.2800e+00,  6.1970e+06,  7.4483e+06,  6.2800e+00,
          1.6816e+06, -7.8828e+05]], dtype=torch.float64)
	q_value: tensor([[3.1569]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0399347530538422, distance: 1.1212619066138498 entropy 6.3821213104347665
epoch: 2, step: 47
	action: tensor([[ 2.6439e+07, -6.2800e+00,  1.2686e+07, -1.9675e+07,  6.2800e+00,
          6.0078e+06, -4.5839e+06]], dtype=torch.float64)
	q_value: tensor([[3.5985]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6649992162221983, distance: 0.6623378970606367 entropy 6.409303470868464
epoch: 2, step: 48
	action: tensor([[ 9.4739e+06, -6.2800e+00, -8.2188e+06, -8.1254e+06,  6.2800e+00,
          1.1862e+07,  1.2500e+07]], dtype=torch.float64)
	q_value: tensor([[2.9745]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18911902665368674, distance: 1.0304697523877544 entropy 6.339360137613582
epoch: 2, step: 49
	action: tensor([[ 1.3190e+07, -6.2800e+00,  1.4786e+07, -7.9673e+06,  6.2800e+00,
          2.2879e+07, -5.3505e+05]], dtype=torch.float64)
	q_value: tensor([[3.8261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5779711806690047, distance: 0.7434088856347699 entropy 6.5174093811091405
epoch: 2, step: 50
	action: tensor([[ 2.3768e+07, -6.2800e+00, -9.7758e+05,  1.1191e+07,  6.2800e+00,
          5.6723e+06, -1.5833e+07]], dtype=torch.float64)
	q_value: tensor([[3.6120]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39398091030969296, distance: 0.8908402797532886 entropy 6.447748585734219
epoch: 2, step: 51
	action: tensor([[ 1.4271e+06, -6.2800e+00,  8.3805e+06,  7.0058e+05,  6.2800e+00,
         -2.3407e+07, -7.7940e+04]], dtype=torch.float64)
	q_value: tensor([[3.2332]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4790310521880705, distance: 0.8259668404106105 entropy 6.325571767272078
epoch: 2, step: 52
	action: tensor([[ 1.6178e+07, -6.2800e+00, -1.4207e+06, -1.7857e+07,  6.2800e+00,
         -6.6757e+06,  1.7883e+06]], dtype=torch.float64)
	q_value: tensor([[5.3592]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44674555935498184, distance: 0.8511755646179382 entropy 6.444443724792104
epoch: 2, step: 53
	action: tensor([[-2.4302e+07, -6.2800e+00,  1.1374e+07, -7.5445e+06,  6.2800e+00,
         -1.6858e+07, -8.1611e+06]], dtype=torch.float64)
	q_value: tensor([[3.8553]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11311166760960001, distance: 1.077683327362738 entropy 6.492158590571541
epoch: 2, step: 54
	action: tensor([[-2.4972e+07, -6.2800e+00,  1.5685e+07,  2.1667e+07,  6.2800e+00,
          6.5214e+06, -1.0473e+07]], dtype=torch.float64)
	q_value: tensor([[4.5890]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0975893387884611, distance: 1.0870732324266248 entropy 6.5576112347698325
epoch: 2, step: 55
	action: tensor([[ 1.6827e+07, -6.2800e+00, -6.6703e+06, -1.4728e+07,  6.2800e+00,
         -6.9800e+06, -8.4754e+06]], dtype=torch.float64)
	q_value: tensor([[3.5394]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4266128233655383, distance: 0.8665241705627453 entropy 6.44110666998796
epoch: 2, step: 56
	action: tensor([[-8.2812e+06, -6.2800e+00, -1.7556e+07,  1.0139e+06,  6.2800e+00,
          5.0618e+06,  3.6561e+05]], dtype=torch.float64)
	q_value: tensor([[4.2009]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11501903332421137, distance: 1.076523856368768 entropy 6.582143093382455
epoch: 2, step: 57
	action: tensor([[-3.3753e+06, -6.2800e+00, -9.3124e+06,  1.1000e+07,  6.2800e+00,
         -7.0003e+06,  2.9842e+06]], dtype=torch.float64)
	q_value: tensor([[2.9689]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.75058777392941, distance: 1.514079358667456 entropy 6.305161668123823
epoch: 2, step: 58
	action: tensor([[-8.7376e+06, -6.2800e+00,  7.5989e+06,  1.0415e+07,  6.2800e+00,
          3.0126e+07, -3.6954e+05]], dtype=torch.float64)
	q_value: tensor([[4.0502]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0883039116227764, distance: 1.0926516829538946 entropy 6.538022458991368
epoch: 2, step: 59
	action: tensor([[-7.8685e+06, -6.2800e+00,  1.6691e+07,  5.3835e+06,  6.2800e+00,
         -1.6888e+05,  7.4492e+05]], dtype=torch.float64)
	q_value: tensor([[3.5211]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2713452990820848, distance: 1.290293335307814 entropy 6.407222961531838
epoch: 2, step: 60
	action: tensor([[-1.1594e+07, -6.2800e+00,  1.1287e+07, -9.3564e+06,  6.2800e+00,
         -4.3835e+06, -5.2444e+06]], dtype=torch.float64)
	q_value: tensor([[4.3365]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10129871137555846, distance: 1.0848367163208075 entropy 6.405113352570623
epoch: 2, step: 61
	action: tensor([[-1.3364e+07, -6.2800e+00,  8.3111e+06, -7.6455e+06,  6.2800e+00,
          7.9245e+06,  1.6917e+07]], dtype=torch.float64)
	q_value: tensor([[5.2696]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3288921563502305, distance: 1.3191724181155726 entropy 6.626277979863336
epoch: 2, step: 62
	action: tensor([[ 2.4611e+07, -6.2800e+00,  3.4192e+07,  5.6693e+06,  6.2800e+00,
          2.0884e+07, -1.7072e+07]], dtype=torch.float64)
	q_value: tensor([[5.1078]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38658829032402997, distance: 0.8962573378427721 entropy 6.692352304434948
epoch: 2, step: 63
	action: tensor([[-6.8590e+06, -6.2800e+00, -3.9157e+06,  8.9606e+06,  6.2800e+00,
         -4.8657e+06, -9.6316e+06]], dtype=torch.float64)
	q_value: tensor([[3.0121]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8484696152403102, distance: 1.5558325431593227 entropy 6.302284528960259
epoch: 2, step: 64
	action: tensor([[-1.9877e+06, -6.2800e+00, -4.4117e+06, -1.2001e+07,  6.2800e+00,
          1.6134e+07,  1.4274e+06]], dtype=torch.float64)
	q_value: tensor([[4.9785]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.33045319727791966, distance: 1.3199470023149544 entropy 6.558833332897524
epoch: 2, step: 65
	action: tensor([[ 5.7647e+06, -6.2800e+00,  1.5335e+07,  9.1022e+06,  6.2800e+00,
         -2.7398e+07, -6.2238e+06]], dtype=torch.float64)
	q_value: tensor([[3.9188]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5452730032277964, distance: 0.7716707924303222 entropy 6.484044503221503
epoch: 2, step: 66
	action: tensor([[-2.6481e+07, -6.2800e+00, -1.2104e+07,  1.8191e+06,  6.2800e+00,
          8.0145e+06, -1.5256e+07]], dtype=torch.float64)
	q_value: tensor([[4.0145]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10649946041552905, distance: 1.0816932081599746 entropy 6.389529676652785
epoch: 2, step: 67
	action: tensor([[-8.5154e+06, -6.2800e+00, -1.0891e+07,  1.3225e+07,  6.2800e+00,
         -1.4788e+07, -4.5240e+05]], dtype=torch.float64)
	q_value: tensor([[2.9251]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8418695462883632, distance: 1.5530524637520837 entropy 6.3101205226074315
epoch: 2, step: 68
	action: tensor([[-1.2351e+07, -6.2800e+00, -2.5141e+07,  8.7068e+05,  6.2800e+00,
         -8.7698e+06,  2.7901e+06]], dtype=torch.float64)
	q_value: tensor([[4.3409]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7018416975693558, distance: 1.492850347012075 entropy 6.6147088292636775
epoch: 2, step: 69
	action: tensor([[ 5.1340e+05, -6.2800e+00, -1.8788e+07, -9.7441e+06,  6.2800e+00,
         -1.1717e+07, -8.6669e+06]], dtype=torch.float64)
	q_value: tensor([[5.0898]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42783703642262527, distance: 0.8655986379781442 entropy 6.527162324790119
epoch: 2, step: 70
	action: tensor([[ 2.1404e+07, -6.2800e+00,  7.6231e+06, -3.5100e+06,  6.2800e+00,
         -5.6259e+06,  4.7966e+06]], dtype=torch.float64)
	q_value: tensor([[4.3086]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3754133368081468, distance: 0.9043843668005814 entropy 6.417054315548408
epoch: 2, step: 71
	action: tensor([[-1.3843e+06, -6.2800e+00,  1.7332e+06, -7.7836e+05,  6.2800e+00,
          1.0481e+07,  5.8463e+06]], dtype=torch.float64)
	q_value: tensor([[5.1843]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4708195358142049, distance: 1.387830373631724 entropy 6.513796487387787
epoch: 2, step: 72
	action: tensor([[-1.7271e+07, -6.2800e+00, -1.3218e+07,  9.4265e+06,  6.2800e+00,
          1.2704e+07, -5.5436e+06]], dtype=torch.float64)
	q_value: tensor([[3.7253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13098772768062272, distance: 1.0667671856918666 entropy 6.414620255594477
epoch: 2, step: 73
	action: tensor([[-7.2390e+06, -6.2800e+00, -1.1661e+07, -1.8908e+07,  6.2800e+00,
         -5.4677e+06,  1.6589e+07]], dtype=torch.float64)
	q_value: tensor([[3.1257]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05947438795819626, distance: 1.1097930648668073 entropy 6.358710260100843
epoch: 2, step: 74
	action: tensor([[-9.6969e+05, -6.2800e+00, -8.7916e+06,  2.9199e+07,  6.2800e+00,
         -2.2129e+07, -8.2236e+06]], dtype=torch.float64)
	q_value: tensor([[5.4388]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7454842415469782, distance: 1.5118707310377464 entropy 6.69560411049189
epoch: 2, step: 75
	action: tensor([[-2.6293e+07, -6.2800e+00,  9.9594e+06, -6.9372e+05,  6.2800e+00,
          1.4114e+07,  2.1390e+06]], dtype=torch.float64)
	q_value: tensor([[5.6139]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6499728438568055, distance: 1.4699247129117148 entropy 6.603337760016614
epoch: 2, step: 76
	action: tensor([[ 1.4114e+07, -6.2800e+00, -2.0266e+07,  2.9897e+07,  6.2800e+00,
          1.8891e+07,  6.4625e+06]], dtype=torch.float64)
	q_value: tensor([[4.1736]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3889283283132231, distance: 0.8945461867693418 entropy 6.484957301246159
epoch: 2, step: 77
	action: tensor([[ 1.4692e+07, -6.2800e+00, -1.1694e+07,  1.4724e+07,  6.2800e+00,
          1.0493e+07,  3.3069e+06]], dtype=torch.float64)
	q_value: tensor([[3.2492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37292374477623913, distance: 0.9061850047649289 entropy 6.343266729980401
epoch: 2, step: 78
	action: tensor([[ 1.0506e+07, -6.2800e+00, -9.0205e+06,  3.5991e+06,  6.2800e+00,
         -6.8375e+04,  5.0669e+05]], dtype=torch.float64)
	q_value: tensor([[2.5852]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6565269639168634, distance: 0.6706609468845399 entropy 6.210550826083468
epoch: 2, step: 79
	action: tensor([[-1.6898e+07, -6.2800e+00, -2.5106e+07,  1.1066e+07,  6.2800e+00,
         -7.5438e+06, -3.0807e+06]], dtype=torch.float64)
	q_value: tensor([[3.8964]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5351152187549864, distance: 1.4178398609155476 entropy 6.2481329783635955
epoch: 2, step: 80
	action: tensor([[-7.6170e+06, -6.2800e+00, -1.1147e+07, -2.0550e+06,  6.2800e+00,
         -1.6767e+06,  1.3582e+06]], dtype=torch.float64)
	q_value: tensor([[4.9530]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04721776466508365, distance: 1.1170008891621837 entropy 6.49651275743954
epoch: 2, step: 81
	action: tensor([[ 2.3795e+06, -6.2800e+00,  1.4780e+07,  5.9810e+06,  6.2800e+00,
         -2.1378e+07,  1.2657e+07]], dtype=torch.float64)
	q_value: tensor([[5.2174]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2543333987274947, distance: 0.9881640248973672 entropy 6.712331269845765
epoch: 2, step: 82
	action: tensor([[ 1.5958e+07, -6.2800e+00,  1.0110e+07, -1.4583e+06,  6.2800e+00,
         -4.4634e+05,  1.0144e+07]], dtype=torch.float64)
	q_value: tensor([[4.2226]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.40713027617974396, distance: 0.881122577648419 entropy 6.425504174693148
epoch: 2, step: 83
	action: tensor([[ 4.3166e+06, -6.2800e+00,  1.0787e+07,  9.0337e+05,  6.2800e+00,
          1.0008e+07,  4.0511e+06]], dtype=torch.float64)
	q_value: tensor([[4.7248]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.46310924437310297, distance: 0.8384934151676051 entropy 6.454013824083175
epoch: 2, step: 84
	action: tensor([[-6.3810e+06, -6.2800e+00, -6.2216e+05, -7.2704e+06,  6.2800e+00,
         -1.1752e+07,  3.4179e+06]], dtype=torch.float64)
	q_value: tensor([[3.3678]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16390714813541962, distance: 1.046366792685063 entropy 6.373918631201431
epoch: 2, step: 85
	action: tensor([[-7.1521e+06, -6.2800e+00,  2.0564e+07, -1.2916e+07,  6.2800e+00,
          1.0168e+07,  9.2600e+06]], dtype=torch.float64)
	q_value: tensor([[4.3877]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2555425993471401, distance: 1.2822491490269756 entropy 6.51397103684998
epoch: 2, step: 86
	action: tensor([[-5.7584e+06, -6.2800e+00,  1.0508e+07,  2.0337e+07,  6.2800e+00,
          1.7009e+07, -1.6974e+05]], dtype=torch.float64)
	q_value: tensor([[4.3943]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15394837308352194, distance: 1.0525800290244578 entropy 6.597275185652912
epoch: 2, step: 87
	action: tensor([[ 2.2964e+06, -6.2800e+00, -5.5785e+06,  4.1817e+06,  6.2800e+00,
          1.2552e+07, -3.6474e+06]], dtype=torch.float64)
	q_value: tensor([[3.5094]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.47146403139239224, distance: 0.8319437571557914 entropy 6.459190477732376
epoch: 2, step: 88
	action: tensor([[-2.0942e+05, -6.2800e+00,  1.8037e+07, -8.8925e+06,  6.2800e+00,
         -1.2445e+06, -2.3702e+06]], dtype=torch.float64)
	q_value: tensor([[3.2723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08908447498891547, distance: 1.0921838370799717 entropy 6.363141945549982
epoch: 2, step: 89
	action: tensor([[-8.8136e+06, -6.2800e+00, -1.6531e+07, -4.0173e+06,  6.2800e+00,
         -2.5593e+07,  4.6200e+06]], dtype=torch.float64)
	q_value: tensor([[4.8568]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07422003607471739, distance: 1.1010589766626047 entropy 6.548086105280506
epoch: 2, step: 90
	action: tensor([[ 9.6411e+06, -6.2800e+00,  3.7480e+07, -1.0532e+07,  6.2800e+00,
         -4.1717e+06,  4.9150e+06]], dtype=torch.float64)
	q_value: tensor([[4.7170]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4651334481120568, distance: 0.8369112643312819 entropy 6.629489181629302
epoch: 2, step: 91
	action: tensor([[ 5.4703e+06, -6.2800e+00,  7.3231e+06, -9.9792e+06,  6.2800e+00,
         -1.1666e+07, -1.4850e+06]], dtype=torch.float64)
	q_value: tensor([[5.5011]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3671908858556946, distance: 0.9103178444350394 entropy 6.584607235917312
epoch: 2, step: 92
	action: tensor([[ 4.1647e+06, -6.2800e+00, -5.2564e+06,  1.5284e+06,  6.2800e+00,
         -7.8192e+06,  1.1106e+07]], dtype=torch.float64)
	q_value: tensor([[5.4982]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5987889673472548, distance: 0.7248416273312096 entropy 6.576209307014289
epoch: 2, step: 93
	action: tensor([[ 1.1241e+07, -6.2800e+00, -6.2146e+06, -5.2527e+06,  6.2800e+00,
          9.2876e+05,  8.9834e+04]], dtype=torch.float64)
	q_value: tensor([[4.7657]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7454883518443834, distance: 0.5773119203904097 entropy 6.339378569234746
epoch: 2, step: 94
	action: tensor([[-5.1381e+06, -6.2800e+00,  6.9269e+06,  1.7165e+06,  6.2800e+00,
         -8.9079e+06,  9.2333e+06]], dtype=torch.float64)
	q_value: tensor([[3.6047]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6437521713373102, distance: 1.46715116572512 entropy 6.4356744225593205
epoch: 2, step: 95
	action: tensor([[ 2.7740e+07, -6.2800e+00,  8.1984e+06,  2.0940e+07,  6.2800e+00,
         -8.8771e+06, -4.9009e+06]], dtype=torch.float64)
	q_value: tensor([[3.9687]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7085255549942333, distance: 0.6178128475252661 entropy 6.510673666916113
epoch: 2, step: 96
	action: tensor([[ 1.1993e+07, -6.2800e+00,  1.9196e+06, -4.6326e+06,  6.2800e+00,
          9.0504e+06, -1.0558e+07]], dtype=torch.float64)
	q_value: tensor([[4.7344]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20861352230142505, distance: 1.0180075663367092 entropy 6.4031058349128935
epoch: 2, step: 97
	action: tensor([[ 7.1442e+06, -6.2800e+00, -6.5605e+06, -1.3928e+07,  6.2800e+00,
          4.7201e+06,  7.6651e+06]], dtype=torch.float64)
	q_value: tensor([[3.0311]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1660476929783048, distance: 1.0450264927158341 entropy 6.349196514756135
epoch: 2, step: 98
	action: tensor([[ 8.8551e+06, -6.2800e+00,  1.5205e+07, -5.7035e+06,  6.2800e+00,
          3.6989e+06, -5.5927e+06]], dtype=torch.float64)
	q_value: tensor([[3.1167]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7408396479558504, distance: 0.5825604189046129 entropy 6.345689333812658
epoch: 2, step: 99
	action: tensor([[-1.7571e+07, -6.2800e+00, -4.5247e+06,  4.4959e+06,  6.2800e+00,
         -1.6451e+07,  4.3873e+06]], dtype=torch.float64)
	q_value: tensor([[3.6011]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4892946448874296, distance: 1.3965195095272467 entropy 6.44016076734176
epoch: 2, step: 100
	action: tensor([[ 1.7892e+07, -6.2800e+00,  5.6848e+06, -5.3321e+06,  6.2800e+00,
         -1.3196e+07, -9.5420e+06]], dtype=torch.float64)
	q_value: tensor([[4.3517]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38521819663957446, distance: 0.897257703163485 entropy 6.46502143121073
epoch: 2, step: 101
	action: tensor([[-7.5022e+06, -6.2800e+00, -2.0693e+06,  4.1330e+05,  6.2800e+00,
          2.2824e+06,  5.1555e+06]], dtype=torch.float64)
	q_value: tensor([[3.8080]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08840088818993497, distance: 1.0925935690557536 entropy 6.4190066587758725
epoch: 2, step: 102
	action: tensor([[-1.2784e+07, -6.2800e+00, -2.1543e+06, -5.8140e+05,  6.2800e+00,
          4.2360e+06, -1.2322e+07]], dtype=torch.float64)
	q_value: tensor([[3.2593]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7624227584438947, distance: 1.5191887630909577 entropy 6.361264859251477
epoch: 2, step: 103
	action: tensor([[ 4.9853e+05, -6.2800e+00,  8.1001e+06, -1.0398e+07,  6.2800e+00,
         -7.0765e+06,  1.1956e+06]], dtype=torch.float64)
	q_value: tensor([[4.1256]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42191245874968786, distance: 0.8700686049785917 entropy 6.489722377808732
epoch: 2, step: 104
	action: tensor([[ 1.5349e+07, -6.2800e+00,  1.8514e+07,  2.0768e+07,  6.2800e+00,
         -8.4834e+06, -1.2804e+06]], dtype=torch.float64)
	q_value: tensor([[4.1802]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.584595762730977, distance: 0.7375511667357367 entropy 6.5381662145537645
epoch: 2, step: 105
	action: tensor([[-1.0825e+07, -6.2800e+00, -1.7208e+07,  1.4533e+07,  6.2800e+00,
         -2.1435e+07, -2.3874e+06]], dtype=torch.float64)
	q_value: tensor([[4.9531]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4159079500546965, distance: 1.3616773182670676 entropy 6.372726651742336
epoch: 2, step: 106
	action: tensor([[-1.8540e+07, -6.2800e+00, -1.5098e+07, -1.6468e+07,  6.2800e+00,
         -2.6941e+07, -5.4965e+06]], dtype=torch.float64)
	q_value: tensor([[4.2937]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.058723686754295734, distance: 1.1102358793960792 entropy 6.40299788643876
epoch: 2, step: 107
	action: tensor([[ 1.0580e+07, -6.2800e+00,  8.3787e+06, -1.0365e+07,  6.2800e+00,
          6.8011e+06,  1.1699e+07]], dtype=torch.float64)
	q_value: tensor([[5.2399]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7139924938773112, distance: 0.6119915262008612 entropy 6.59757092889742
epoch: 2, step: 108
	action: tensor([[-4.9712e+06, -6.2800e+00,  1.0267e+07,  9.4279e+05,  6.2800e+00,
          1.3212e+06, -1.6549e+07]], dtype=torch.float64)
	q_value: tensor([[4.1731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.051356271164179734, distance: 1.1145723456776422 entropy 6.597445629413132
epoch: 2, step: 109
	action: tensor([[ 1.0276e+07, -6.2800e+00, -8.0357e+06, -2.3382e+06,  6.2800e+00,
         -1.6228e+07, -2.1575e+06]], dtype=torch.float64)
	q_value: tensor([[2.6473]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4211109466684736, distance: 0.8706715663744956 entropy 6.244141554505269
epoch: 2, step: 110
	action: tensor([[-1.2570e+07, -6.2800e+00,  1.8703e+07,  3.2536e+06,  6.2800e+00,
          1.6417e+07, -2.7828e+06]], dtype=torch.float64)
	q_value: tensor([[3.7965]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.01285473645369195, distance: 1.151675889574574 entropy 6.4150417434635205
epoch: 2, step: 111
	action: tensor([[ 1.2565e+06, -6.2800e+00,  1.1143e+07, -1.4950e+07,  6.2800e+00,
          2.6788e+06, -5.7854e+06]], dtype=torch.float64)
	q_value: tensor([[3.9241]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3945818954970497, distance: 0.8903984499321956 entropy 6.467790185008612
epoch: 2, step: 112
	action: tensor([[-2.0187e+07, -6.2800e+00, -1.4890e+06,  1.1228e+07,  6.2800e+00,
         -2.1768e+06,  2.3872e+06]], dtype=torch.float64)
	q_value: tensor([[3.0013]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6009787092235792, distance: 1.4479363513823142 entropy 6.351866033726864
epoch: 2, step: 113
	action: tensor([[-7.8860e+06, -6.2800e+00,  1.4531e+06,  1.0610e+07,  6.2800e+00,
          8.6658e+06,  7.1491e+06]], dtype=torch.float64)
	q_value: tensor([[4.4921]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06057929013367158, distance: 1.1091409969712214 entropy 6.447760415392829
epoch: 2, step: 114
	action: tensor([[-2.3023e+06, -6.2800e+00, -3.8111e+06, -2.3837e+07,  6.2800e+00,
          8.2461e+06,  3.6045e+06]], dtype=torch.float64)
	q_value: tensor([[3.2314]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6930329923040273, distance: 1.4889818500007157 entropy 6.369444615322779
epoch: 2, step: 115
	action: tensor([[-8.0192e+06, -6.2800e+00, -5.2983e+05, -2.9032e+07,  6.2800e+00,
         -4.6106e+06, -8.4718e+06]], dtype=torch.float64)
	q_value: tensor([[4.5133]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07048322221300951, distance: 1.10327889342246 entropy 6.559185748687663
epoch: 2, step: 116
	action: tensor([[ 1.3472e+07, -6.2800e+00,  4.2691e+06, -4.0706e+07,  6.2800e+00,
          9.5650e+06, -4.2548e+06]], dtype=torch.float64)
	q_value: tensor([[5.5941]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8325749369599327, distance: 0.4682382940588198 entropy 6.62349576527905
epoch: 2, step: 117
	action: tensor([[-2.3812e+06, -6.2800e+00, -2.3707e+07, -1.2061e+07,  6.2800e+00,
          2.8007e+07, -4.3797e+06]], dtype=torch.float64)
	q_value: tensor([[3.2188]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.40155991078239817, distance: 1.3547605030333467 entropy 6.396698858922436
epoch: 2, step: 118
	action: tensor([[ 2.0037e+07, -6.2800e+00,  9.2121e+06,  1.1944e+07,  6.2800e+00,
          1.7808e+07,  1.8423e+07]], dtype=torch.float64)
	q_value: tensor([[4.0818]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3668290470000817, distance: 0.9105780661095417 entropy 6.531049467238001
epoch: 2, step: 119
	action: tensor([[-1.5415e+07, -6.2800e+00, -1.3848e+07,  1.1068e+07,  6.2800e+00,
         -1.2359e+07, -2.7026e+06]], dtype=torch.float64)
	q_value: tensor([[2.6421]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.921065835775277, distance: 1.58608996556848 entropy 6.235447771617991
epoch: 2, step: 120
	action: tensor([[-1.4954e+07, -6.2800e+00,  2.8572e+06, -1.0952e+07,  6.2800e+00,
          2.0681e+07, -1.5465e+07]], dtype=torch.float64)
	q_value: tensor([[3.7108]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4111572952905018, distance: 1.359391048866483 entropy 6.449652247881139
epoch: 2, step: 121
	action: tensor([[ 1.0300e+07, -6.2800e+00, -5.2318e+06,  1.0620e+07,  6.2800e+00,
          9.7373e+06, -2.0355e+06]], dtype=torch.float64)
	q_value: tensor([[4.0525]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36383463448478537, distance: 0.9127286939998863 entropy 6.506980679408814
epoch: 2, step: 122
	action: tensor([[-1.1107e+07, -6.2800e+00,  3.4833e+06,  5.0288e+06,  6.2800e+00,
         -1.1483e+07, -1.5223e+06]], dtype=torch.float64)
	q_value: tensor([[2.6398]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5747512932593093, distance: 1.436027246600784 entropy 6.176800742267433
epoch: 2, step: 123
	action: tensor([[-4.8420e+06, -6.2800e+00,  2.0885e+07,  3.2410e+06,  6.2800e+00,
          1.9625e+07,  9.1430e+06]], dtype=torch.float64)
	q_value: tensor([[4.4352]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.006251222801609169, distance: 1.140761871208075 entropy 6.44813020835214
epoch: 2, step: 124
	action: tensor([[ 4.0129e+06, -6.2800e+00, -5.7889e+06,  1.0135e+07,  6.2800e+00,
         -3.0029e+06, -1.0382e+07]], dtype=torch.float64)
	q_value: tensor([[3.2391]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4671527125101105, distance: 0.835329988468067 entropy 6.364861251667542
epoch: 2, step: 125
	action: tensor([[-1.0576e+07, -6.2800e+00, -2.7659e+06, -2.2315e+06,  6.2800e+00,
         -6.1690e+06,  2.2966e+06]], dtype=torch.float64)
	q_value: tensor([[4.1870]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0827841391391152, distance: 1.0959543665424683 entropy 6.26668500085658
epoch: 2, step: 126
	action: tensor([[-4.9100e+06, -6.2800e+00,  2.8775e+06, -2.4563e+07,  6.2800e+00,
         -2.2526e+07, -5.3133e+06]], dtype=torch.float64)
	q_value: tensor([[5.1967]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.041353354737158954, distance: 1.1204332066645033 entropy 6.575261883565081
epoch: 2, step: 127
	action: tensor([[-2.2135e+07, -6.2800e+00, -1.5248e+07, -4.5535e+06,  6.2800e+00,
         -8.7761e+06, -8.1368e+06]], dtype=torch.float64)
	q_value: tensor([[4.8491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.02845321611224161, distance: 1.127946633836715 entropy 6.629480970599739
LOSS epoch 2 actor 18.945539736544287 critic 93.52060046037008 entropy 0.1
epoch: 3, step: 0
	action: tensor([[ 1.7456e+07, -6.2800e+00,  1.1191e+08, -9.0462e+07,  6.2800e+00,
         -2.3596e+08,  4.0449e+07]], dtype=torch.float64)
	q_value: tensor([[2.9905]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.373690974766015, distance: 0.9056304748355536 entropy 8.282827662943019
epoch: 3, step: 1
	action: tensor([[-1.5649e+08, -6.2800e+00, -7.7566e+06,  4.6013e+07,  6.2800e+00,
         -8.9022e+07, -1.3634e+08]], dtype=torch.float64)
	q_value: tensor([[2.8866]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6522749957506511, distance: 1.4709498238415737 entropy 8.139408599874542
epoch: 3, step: 2
	action: tensor([[ 5.4597e+07, -6.2800e+00,  2.2648e+08,  1.3115e+08,  6.2800e+00,
         -1.1108e+08, -6.7237e+07]], dtype=torch.float64)
	q_value: tensor([[2.5967]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6066356955423446, distance: 0.7177185434492473 entropy 8.096752802167092
epoch: 3, step: 3
	action: tensor([[-7.9020e+07, -6.2800e+00,  2.2995e+08, -4.9250e+07,  6.2800e+00,
          6.8803e+07, -2.3241e+07]], dtype=torch.float64)
	q_value: tensor([[2.6160]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5184393373586518, distance: 1.410117870285704 entropy 8.101174976583788
epoch: 3, step: 4
	action: tensor([[-7.3554e+07, -6.2800e+00, -1.2622e+08, -1.5525e+08,  6.2800e+00,
         -1.2270e+07, -2.3154e+08]], dtype=torch.float64)
	q_value: tensor([[2.8866]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03862946911970866, distance: 1.166237531449657 entropy 8.244048332218187
epoch: 3, step: 5
	action: tensor([[-1.2339e+08, -6.2800e+00, -1.8417e+08, -6.0895e+07,  6.2800e+00,
          5.4641e+07, -3.1533e+08]], dtype=torch.float64)
	q_value: tensor([[3.5687]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9541247145362381, distance: 1.5996789573158168 entropy 8.481231917637995
epoch: 3, step: 6
	action: tensor([[-1.4523e+07, -6.2800e+00,  2.2499e+08, -1.3126e+08,  6.2800e+00,
         -4.5754e+08,  3.1305e+08]], dtype=torch.float64)
	q_value: tensor([[2.9396]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.042176888871764406, distance: 1.1199518440854768 entropy 8.204639242990504
epoch: 3, step: 7
	action: tensor([[ 8.2701e+07, -6.2800e+00,  7.1997e+07,  7.8373e+07,  6.2800e+00,
         -1.2440e+08, -1.0202e+08]], dtype=torch.float64)
	q_value: tensor([[3.1078]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35654460421274914, distance: 0.9179434278800143 entropy 8.273411660623227
epoch: 3, step: 8
	action: tensor([[-2.8188e+08, -6.2800e+00, -8.4316e+07,  7.3282e+07,  6.2800e+00,
          9.2112e+06,  2.8583e+07]], dtype=torch.float64)
	q_value: tensor([[2.6171]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.02663386785575872, distance: 1.1290022535736473 entropy 8.125194973296697
epoch: 3, step: 9
	action: tensor([[-9.3967e+07, -6.2800e+00, -1.0628e+07, -4.0099e+06,  6.2800e+00,
         -6.2965e+07, -6.8046e+06]], dtype=torch.float64)
	q_value: tensor([[1.8098]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.013140344297772444, distance: 1.1518382546558277 entropy 7.999951110398562
epoch: 3, step: 10
	action: tensor([[-4.1895e+07, -6.2800e+00,  2.3791e+08, -1.4176e+08,  6.2800e+00,
         -1.4730e+08, -1.5422e+07]], dtype=torch.float64)
	q_value: tensor([[3.4884]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.028731795243602365, distance: 1.1606673686017293 entropy 8.326910228158594
epoch: 3, step: 11
	action: tensor([[ 1.6209e+08, -6.2800e+00,  1.2876e+08, -1.7397e+08,  6.2800e+00,
         -1.7847e+08,  9.9756e+07]], dtype=torch.float64)
	q_value: tensor([[3.4336]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.43740797466413994, distance: 0.8583283934050174 entropy 8.374297742271592
epoch: 3, step: 12
	action: tensor([[ 1.1206e+08, -6.2800e+00, -6.5421e+07,  6.4833e+07,  6.2800e+00,
          8.6846e+07, -1.0379e+08]], dtype=torch.float64)
	q_value: tensor([[2.6450]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3564967137348857, distance: 0.9179775871486784 entropy 8.147408065608175
epoch: 3, step: 13
	action: tensor([[ 1.0607e+07, -6.2800e+00,  4.1078e+07, -6.8848e+07,  6.2800e+00,
          5.6694e+07, -1.4726e+08]], dtype=torch.float64)
	q_value: tensor([[1.6987]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25980171972892463, distance: 0.9845340236674958 entropy 7.905054837139672
epoch: 3, step: 14
	action: tensor([[-1.0671e+08, -6.2800e+00,  1.0941e+08,  1.0233e+07,  6.2800e+00,
         -5.7194e+07, -4.9434e+07]], dtype=torch.float64)
	q_value: tensor([[1.9625]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4898079963614892, distance: 1.3967601749938037 entropy 8.013994370983857
epoch: 3, step: 15
	action: tensor([[-5.7975e+07, -6.2800e+00, -1.3862e+08,  5.7104e+07,  6.2800e+00,
          9.1782e+07,  1.8221e+08]], dtype=torch.float64)
	q_value: tensor([[2.7702]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.019687609288269736, distance: 1.1330235563714541 entropy 8.09885503098668
epoch: 3, step: 16
	action: tensor([[-5.5951e+06, -6.2800e+00, -7.0576e+07, -9.3352e+07,  6.2800e+00,
          1.2110e+08, -4.1084e+06]], dtype=torch.float64)
	q_value: tensor([[2.1671]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.942059995022229, distance: 1.5947331217093912 entropy 8.135019635985435
epoch: 3, step: 17
	action: tensor([[-8.4876e+07, -6.2800e+00, -8.6862e+07, -6.3897e+07,  6.2800e+00,
          1.7193e+08, -6.1023e+07]], dtype=torch.float64)
	q_value: tensor([[2.7303]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9611568424572716, distance: 1.602554680863335 entropy 8.12393338938121
epoch: 3, step: 18
	action: tensor([[-2.8775e+08, -6.2800e+00,  2.0962e+08,  1.5832e+08,  6.2800e+00,
         -2.6481e+06,  1.4264e+08]], dtype=torch.float64)
	q_value: tensor([[2.9903]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6075368958895802, distance: 1.4508989554729368 entropy 8.225301563218698
epoch: 3, step: 19
	action: tensor([[-5.6327e+07, -6.2800e+00,  7.3878e+07,  2.5859e+07,  6.2800e+00,
          2.0668e+08, -9.2260e+07]], dtype=torch.float64)
	q_value: tensor([[2.9496]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.00819554823993962, distance: 1.139645342385431 entropy 8.152731528133069
epoch: 3, step: 20
	action: tensor([[-1.7736e+07, -6.2800e+00, -2.2167e+07,  1.4643e+08,  6.2800e+00,
         -2.4804e+08,  4.8840e+07]], dtype=torch.float64)
	q_value: tensor([[1.8988]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0208556863426428, distance: 1.6267632158263436 entropy 8.064739259091539
epoch: 3, step: 21
	action: tensor([[-6.8064e+07, -6.2800e+00,  1.6688e+08, -5.3498e+07,  6.2800e+00,
         -4.6122e+07, -8.6896e+07]], dtype=torch.float64)
	q_value: tensor([[2.5379]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0076486734938465695, distance: 1.1399594957282198 entropy 8.269337217351545
epoch: 3, step: 22
	action: tensor([[-8.1703e+07, -6.2800e+00, -3.4928e+08,  1.6349e+08,  6.2800e+00,
          5.6372e+07, -1.9920e+08]], dtype=torch.float64)
	q_value: tensor([[3.2974]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.027473898941801078, distance: 1.1285149746008096 entropy 8.412207406780297
epoch: 3, step: 23
	action: tensor([[ 2.8661e+07, -6.2800e+00, -1.4648e+08,  1.1726e+08,  6.2800e+00,
          7.9455e+07, -1.7490e+07]], dtype=torch.float64)
	q_value: tensor([[1.7302]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41132516617486703, distance: 0.8779998227070834 entropy 7.997847458931722
epoch: 3, step: 24
	action: tensor([[ 2.6905e+08, -6.2800e+00, -2.1068e+08, -9.4989e+07,  6.2800e+00,
         -5.8620e+07,  1.5053e+07]], dtype=torch.float64)
	q_value: tensor([[1.9616]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38595403372029535, distance: 0.8967205750605578 entropy 8.01157072645152
epoch: 3, step: 25
	action: tensor([[-1.1419e+08, -6.2800e+00,  1.9876e+08, -1.1295e+08,  6.2800e+00,
          2.3203e+08, -1.8528e+08]], dtype=torch.float64)
	q_value: tensor([[2.5297]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3856302320374181, distance: 1.3470396161908762 entropy 8.248618917901851
epoch: 3, step: 26
	action: tensor([[-1.0247e+08, -6.2800e+00,  2.9261e+07, -7.1442e+07,  6.2800e+00,
         -2.5429e+08,  1.4035e+08]], dtype=torch.float64)
	q_value: tensor([[2.9487]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.012850036969730416, distance: 1.1516732177755218 entropy 8.363302107629458
epoch: 3, step: 27
	action: tensor([[ 1.1036e+08, -6.2800e+00, -2.2974e+08, -3.2062e+08,  6.2800e+00,
          3.5985e+07, -1.9689e+08]], dtype=torch.float64)
	q_value: tensor([[3.8601]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5321371354422244, distance: 0.7827372119790061 entropy 8.40159090001141
epoch: 3, step: 28
	action: tensor([[-1.7229e+08, -6.2800e+00,  1.0797e+08,  9.3303e+06,  6.2800e+00,
          9.9483e+07, -2.4508e+07]], dtype=torch.float64)
	q_value: tensor([[2.1316]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.027445850332843302, distance: 1.1285312482241945 entropy 8.139632410096022
epoch: 3, step: 29
	action: tensor([[-1.2805e+07, -6.2800e+00,  3.8263e+06, -2.0585e+08,  6.2800e+00,
         -9.2908e+07, -3.4109e+07]], dtype=torch.float64)
	q_value: tensor([[1.7435]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05231383322423577, distance: 1.114009678355119 entropy 8.003366943858321
epoch: 3, step: 30
	action: tensor([[ 1.2063e+08, -6.2800e+00,  3.3894e+07, -3.7258e+07,  6.2800e+00,
         -5.0277e+07, -2.4283e+08]], dtype=torch.float64)
	q_value: tensor([[2.8419]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4298819126991189, distance: 0.8640504545311233 entropy 8.21236298746131
epoch: 3, step: 31
	action: tensor([[ 3.1351e+07, -6.2800e+00,  2.4674e+08, -1.4814e+08,  6.2800e+00,
         -1.7421e+08,  2.8979e+07]], dtype=torch.float64)
	q_value: tensor([[2.6564]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4073096802094399, distance: 0.8809892524884597 entropy 8.273837859861496
epoch: 3, step: 32
	action: tensor([[ 9.6219e+07, -6.2800e+00,  2.6512e+06, -1.1707e+08,  6.2800e+00,
          6.7542e+07, -6.4264e+07]], dtype=torch.float64)
	q_value: tensor([[2.4631]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34201960459668335, distance: 0.928246179129622 entropy 8.181203814309631
epoch: 3, step: 33
	action: tensor([[-1.7659e+08, -6.2800e+00, -6.3600e+07,  8.1965e+07,  6.2800e+00,
         -1.3859e+08,  8.3213e+07]], dtype=torch.float64)
	q_value: tensor([[1.8269]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5365040901080667, distance: 1.4184811000692463 entropy 8.001555228291164
epoch: 3, step: 34
	action: tensor([[ 2.3820e+08, -6.2800e+00, -1.2097e+08,  2.5670e+08,  6.2800e+00,
         -6.1043e+07,  2.7507e+07]], dtype=torch.float64)
	q_value: tensor([[2.3912]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4606876522344592, distance: 0.840382258112497 entropy 8.128269696116117
epoch: 3, step: 35
	action: tensor([[ 1.2981e+08, -6.2800e+00,  7.4512e+07,  1.9880e+08,  6.2800e+00,
          6.2597e+07, -2.0406e+07]], dtype=torch.float64)
	q_value: tensor([[2.8847]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4206892180053816, distance: 0.870988656758817 entropy 8.130580719205083
epoch: 3, step: 36
	action: tensor([[ 3.7047e+07, -6.2800e+00,  6.1000e+07, -6.1845e+07,  6.2800e+00,
         -2.3380e+07,  8.6826e+07]], dtype=torch.float64)
	q_value: tensor([[1.9130]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37674863670536884, distance: 0.9034171107276957 entropy 8.029366308250957
epoch: 3, step: 37
	action: tensor([[-3.2119e+08, -6.2800e+00,  2.5050e+08,  5.5655e+07,  6.2800e+00,
          1.4201e+08, -2.3288e+08]], dtype=torch.float64)
	q_value: tensor([[2.5872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.029188180158854604, distance: 1.1275199137457481 entropy 8.171764526056963
epoch: 3, step: 38
	action: tensor([[ 1.2624e+07, -6.2800e+00, -6.4950e+07, -2.2695e+08,  6.2800e+00,
         -1.3374e+08,  8.6787e+07]], dtype=torch.float64)
	q_value: tensor([[2.1551]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34058124906807197, distance: 0.9292602056612356 entropy 8.093069575275809
epoch: 3, step: 39
	action: tensor([[-3.1510e+07, -6.2800e+00, -1.4504e+08,  9.9235e+06,  6.2800e+00,
          3.1663e+08, -2.3479e+08]], dtype=torch.float64)
	q_value: tensor([[2.4452]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08899906813220426, distance: 1.092235037108706 entropy 8.08916296889745
epoch: 3, step: 40
	action: tensor([[-1.8263e+08, -6.2800e+00,  9.1659e+07, -1.5078e+08,  6.2800e+00,
         -8.5623e+07,  7.8737e+07]], dtype=torch.float64)
	q_value: tensor([[1.9094]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.01157560920816525, distance: 1.1377017343017655 entropy 8.070468491342732
epoch: 3, step: 41
	action: tensor([[-2.5160e+08, -6.2800e+00,  7.6051e+07,  8.2976e+07,  6.2800e+00,
          5.1455e+08,  7.4320e+07]], dtype=torch.float64)
	q_value: tensor([[3.1415]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08449691894495692, distance: 1.0949306131769854 entropy 8.33105317860459
epoch: 3, step: 42
	action: tensor([[-2.1500e+08, -6.2800e+00,  3.0585e+08, -2.5086e+08,  6.2800e+00,
         -6.9327e+07,  3.8546e+07]], dtype=torch.float64)
	q_value: tensor([[2.0833]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.032067135489004306, distance: 1.125846834809617 entropy 8.137628991509706
epoch: 3, step: 43
	action: tensor([[ 6.1708e+07, -6.2800e+00,  9.2033e+07, -2.5215e+07,  6.2800e+00,
         -1.0662e+08, -2.0019e+06]], dtype=torch.float64)
	q_value: tensor([[2.8194]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4185954693228987, distance: 0.8725612035118955 entropy 8.221566805340268
epoch: 3, step: 44
	action: tensor([[ 1.1518e+08, -6.2800e+00,  1.2597e+08,  1.2044e+08,  6.2800e+00,
         -1.1176e+08,  7.2289e+07]], dtype=torch.float64)
	q_value: tensor([[2.4022]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.792832739580303, distance: 0.5208555003356061 entropy 8.09519234184829
epoch: 3, step: 45
	action: tensor([[ 1.0351e+08, -6.2800e+00,  1.2513e+08, -1.5607e+08,  6.2800e+00,
         -7.2178e+06, -8.9473e+07]], dtype=torch.float64)
	q_value: tensor([[2.7067]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3450158185916188, distance: 0.9261303124384741 entropy 7.972099256880237
epoch: 3, step: 46
	action: tensor([[ 6.7097e+06, -6.2800e+00, -4.1635e+07, -6.9443e+07,  6.2800e+00,
          4.4033e+07,  5.1483e+07]], dtype=torch.float64)
	q_value: tensor([[2.4316]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19887512145635222, distance: 1.024251957539449 entropy 8.115824903004738
epoch: 3, step: 47
	action: tensor([[-2.6993e+06, -6.2800e+00,  2.4802e+07, -5.0000e+07,  6.2800e+00,
          2.0409e+07,  1.6598e+08]], dtype=torch.float64)
	q_value: tensor([[1.8595]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8855739668491049, distance: 1.5713700814313152 entropy 8.01959274310467
epoch: 3, step: 48
	action: tensor([[-2.3029e+08, -6.2800e+00,  1.8936e+07, -2.3437e+08,  6.2800e+00,
         -1.9479e+08, -1.1673e+08]], dtype=torch.float64)
	q_value: tensor([[3.0526]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.041836074002167045, distance: 1.1201510783121775 entropy 8.243827691048208
epoch: 3, step: 49
	action: tensor([[-1.4778e+07, -6.2800e+00,  1.1000e+08,  1.8797e+08,  6.2800e+00,
          6.0298e+07, -7.4869e+07]], dtype=torch.float64)
	q_value: tensor([[2.8111]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.001398867977909668, distance: 1.143543580626609 entropy 8.232658397185528
epoch: 3, step: 50
	action: tensor([[ 1.4597e+08, -6.2800e+00,  1.5778e+08,  9.8957e+07,  6.2800e+00,
         -2.4012e+07,  4.1208e+07]], dtype=torch.float64)
	q_value: tensor([[1.9966]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7831977727877826, distance: 0.5328298699245773 entropy 8.030729954008327
epoch: 3, step: 51
	action: tensor([[-2.9563e+07, -6.2800e+00,  3.7555e+07,  1.2983e+07,  6.2800e+00,
          5.1973e+07, -5.5418e+07]], dtype=torch.float64)
	q_value: tensor([[2.8151]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.003219073984116
epoch: 3, step: 52
	action: tensor([[-1.3548e+08, -6.2800e+00,  1.0076e+08, -6.6536e+06,  6.2800e+00,
         -5.3075e+07, -1.6963e+08]], dtype=torch.float64)
	q_value: tensor([[2.3662]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.053392445459967575, distance: 1.1133755409141062 entropy 8.006688390151657
epoch: 3, step: 53
	action: tensor([[ 4.6175e+07, -6.2800e+00, -1.4026e+07,  7.0643e+07,  6.2800e+00,
          4.6199e+07, -3.5180e+07]], dtype=torch.float64)
	q_value: tensor([[3.0150]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.330834070848484
epoch: 3, step: 54
	action: tensor([[ 1.2676e+08, -6.2800e+00,  1.2722e+07, -1.2534e+08,  6.2800e+00,
         -3.4635e+07,  1.2790e+08]], dtype=torch.float64)
	q_value: tensor([[2.3662]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.006688390151657
epoch: 3, step: 55
	action: tensor([[-4.2180e+07, -6.2800e+00, -2.1558e+08, -1.6942e+08,  6.2800e+00,
          9.0076e+07, -3.1333e+07]], dtype=torch.float64)
	q_value: tensor([[2.3662]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1527694184690913, distance: 1.2286492407238103 entropy 8.006688390151657
epoch: 3, step: 56
	action: tensor([[ 1.1515e+08, -6.2800e+00, -1.8744e+08, -9.6923e+07,  6.2800e+00,
         -2.0504e+08,  8.7138e+07]], dtype=torch.float64)
	q_value: tensor([[2.7210]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.253550834726248
epoch: 3, step: 57
	action: tensor([[-8.1140e+06, -6.2800e+00,  6.9461e+07, -1.3047e+08,  6.2800e+00,
          1.6344e+07, -1.0268e+08]], dtype=torch.float64)
	q_value: tensor([[2.3662]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.006688390151657
epoch: 3, step: 58
	action: tensor([[ 6.3249e+07, -6.2800e+00,  7.0394e+06, -4.2139e+07,  6.2800e+00,
          5.0891e+07, -3.6346e+07]], dtype=torch.float64)
	q_value: tensor([[2.3662]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4173030124925923, distance: 0.8735305128140983 entropy 8.006688390151657
epoch: 3, step: 59
	action: tensor([[ 1.1330e+08, -6.2800e+00,  1.1473e+08, -1.1991e+08,  6.2800e+00,
         -1.8122e+07, -1.6132e+07]], dtype=torch.float64)
	q_value: tensor([[1.9236]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39522781009571495, distance: 0.889923344519201 entropy 8.01004369407398
epoch: 3, step: 60
	action: tensor([[ 3.5809e+06, -6.2800e+00,  4.6507e+07, -4.8821e+07,  6.2800e+00,
          3.0953e+08, -4.4962e+07]], dtype=torch.float64)
	q_value: tensor([[3.5168]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5632727313359952, distance: 0.7562438363708412 entropy 8.282695016264443
epoch: 3, step: 61
	action: tensor([[-2.9969e+06, -6.2800e+00,  1.3207e+08,  1.7753e+07,  6.2800e+00,
          5.6014e+07,  8.3163e+07]], dtype=torch.float64)
	q_value: tensor([[1.8184]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07742504968208552, distance: 1.0991514129463469 entropy 8.029786884649681
epoch: 3, step: 62
	action: tensor([[-2.1425e+08, -6.2800e+00, -5.1816e+07,  1.3048e+08,  6.2800e+00,
          7.1485e+07, -2.9435e+07]], dtype=torch.float64)
	q_value: tensor([[1.7633]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09641585853638446, distance: 1.0877798090349635 entropy 8.018615038767239
epoch: 3, step: 63
	action: tensor([[-1.8587e+07, -6.2800e+00,  1.7019e+07, -8.6313e+07,  6.2800e+00,
          1.1616e+08, -1.6116e+07]], dtype=torch.float64)
	q_value: tensor([[2.1197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3531434816506944, distance: 1.3311549705771197 entropy 8.115407361984449
epoch: 3, step: 64
	action: tensor([[ 2.5014e+07, -6.2800e+00, -1.9357e+08,  2.9806e+07,  6.2800e+00,
          5.9641e+07, -7.0795e+07]], dtype=torch.float64)
	q_value: tensor([[2.6242]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4352899721017366, distance: 0.8599425594940329 entropy 8.16582593284458
epoch: 3, step: 65
	action: tensor([[ 4.6284e+07, -6.2800e+00,  9.7273e+07, -1.1675e+07,  6.2800e+00,
         -1.8085e+08, -2.5241e+07]], dtype=torch.float64)
	q_value: tensor([[1.9080]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44516942786158653, distance: 0.8523871323879388 entropy 8.036179073524846
epoch: 3, step: 66
	action: tensor([[-7.6255e+07, -6.2800e+00, -1.0231e+08, -1.3879e+08,  6.2800e+00,
          7.5017e+07,  5.8424e+07]], dtype=torch.float64)
	q_value: tensor([[3.1465]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.43986406818333323, distance: 1.3731482896064813 entropy 8.249593814506962
epoch: 3, step: 67
	action: tensor([[-9.8200e+07, -6.2800e+00, -1.4835e+08, -1.6687e+08,  6.2800e+00,
          1.7308e+08, -1.5689e+08]], dtype=torch.float64)
	q_value: tensor([[2.5767]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2914997974785758, distance: 1.3004805595993767 entropy 8.198218814137443
epoch: 3, step: 68
	action: tensor([[-7.5310e+07, -6.2800e+00, -2.0678e+07,  3.4519e+06,  6.2800e+00,
          4.0549e+07,  9.8494e+07]], dtype=torch.float64)
	q_value: tensor([[3.2366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10354598810634696, distance: 1.0834795054717186 entropy 8.317272346705076
epoch: 3, step: 69
	action: tensor([[ 1.2274e+08, -6.2800e+00, -1.6959e+07,  9.5211e+07,  6.2800e+00,
         -9.9742e+07,  6.3856e+06]], dtype=torch.float64)
	q_value: tensor([[1.8556]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 7.992873471788982
epoch: 3, step: 70
	action: tensor([[-7.2493e+07, -6.2800e+00, -6.9595e+07,  7.7983e+06,  6.2800e+00,
         -5.2908e+07,  6.2724e+07]], dtype=torch.float64)
	q_value: tensor([[2.3662]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.006688390151657
epoch: 3, step: 71
	action: tensor([[ 1.0671e+08, -6.2800e+00,  2.9831e+07,  8.2533e+07,  6.2800e+00,
          1.5271e+08, -2.0614e+07]], dtype=torch.float64)
	q_value: tensor([[2.3662]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3961485448354384, distance: 0.8892456550870389 entropy 8.006688390151657
epoch: 3, step: 72
	action: tensor([[-5.6772e+07, -6.2800e+00, -4.5754e+07,  1.8197e+07,  6.2800e+00,
         -4.6182e+07,  2.6536e+06]], dtype=torch.float64)
	q_value: tensor([[1.9524]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.018123060214503
epoch: 3, step: 73
	action: tensor([[-1.6157e+08, -6.2800e+00, -4.9553e+07, -1.9323e+07,  6.2800e+00,
         -9.7648e+07, -8.1521e+07]], dtype=torch.float64)
	q_value: tensor([[2.3662]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.059099263011595515, distance: 1.110014361105918 entropy 8.006688390151657
epoch: 3, step: 74
	action: tensor([[-2.5827e+08, -6.2800e+00,  2.5738e+08, -3.6996e+08,  6.2800e+00,
          1.1503e+08, -2.2340e+08]], dtype=torch.float64)
	q_value: tensor([[3.4653]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5835259481240045, distance: 1.4400225248631646 entropy 8.327386961246392
epoch: 3, step: 75
	action: tensor([[-3.1562e+07, -6.2800e+00, -2.8820e+06,  6.0512e+07,  6.2800e+00,
         -7.9429e+07, -6.6555e+07]], dtype=torch.float64)
	q_value: tensor([[2.7592]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7946768001517055, distance: 1.5330270487667255 entropy 8.198237207734902
epoch: 3, step: 76
	action: tensor([[-1.5279e+07, -6.2800e+00,  1.8269e+08,  1.1406e+08,  6.2800e+00,
          1.7483e+08,  7.7412e+07]], dtype=torch.float64)
	q_value: tensor([[2.9954]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08301405887919544, distance: 1.0958169957517054 entropy 8.210672092783852
epoch: 3, step: 77
	action: tensor([[-7.0675e+06, -6.2800e+00,  8.3803e+07, -1.1394e+08,  6.2800e+00,
          3.9629e+07,  8.9487e+06]], dtype=torch.float64)
	q_value: tensor([[1.7763]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5385542654073712, distance: 1.4194271325236019 entropy 7.987591637873537
epoch: 3, step: 78
	action: tensor([[ 8.4583e+06, -6.2800e+00,  1.3436e+07, -4.9579e+07,  6.2800e+00,
         -1.5671e+08, -1.3438e+08]], dtype=torch.float64)
	q_value: tensor([[2.8864]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4178349518937321, distance: 0.8731317022800082 entropy 8.255363365937725
epoch: 3, step: 79
	action: tensor([[ 2.5552e+08, -6.2800e+00,  1.1458e+07, -2.7138e+07,  6.2800e+00,
         -2.5442e+07,  2.0968e+08]], dtype=torch.float64)
	q_value: tensor([[2.9745]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4290827407454366, distance: 0.8646558405784546 entropy 8.160646353783784
epoch: 3, step: 80
	action: tensor([[ 5.0335e+07, -6.2800e+00, -9.5534e+07,  2.0415e+08,  6.2800e+00,
         -1.4481e+08,  3.3488e+07]], dtype=torch.float64)
	q_value: tensor([[2.7370]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6565488522384779, distance: 0.6706395771191792 entropy 8.2279903741133
epoch: 3, step: 81
	action: tensor([[ 1.0318e+08, -6.2800e+00,  2.9380e+07, -7.8720e+07,  6.2800e+00,
         -4.2003e+06,  1.7152e+08]], dtype=torch.float64)
	q_value: tensor([[2.5439]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.46155581847620986, distance: 0.8397055766272675 entropy 8.103374058519018
epoch: 3, step: 82
	action: tensor([[-5.2696e+07, -6.2800e+00, -8.5748e+07, -2.5477e+07,  6.2800e+00,
         -9.1789e+07,  1.5590e+06]], dtype=torch.float64)
	q_value: tensor([[2.8746]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08856210980045953, distance: 1.0924969490165646 entropy 8.134808035881777
epoch: 3, step: 83
	action: tensor([[-7.9068e+07, -6.2800e+00, -5.9367e+07,  1.3928e+08,  6.2800e+00,
         -3.4794e+07,  5.5827e+07]], dtype=torch.float64)
	q_value: tensor([[2.8712]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9398349290310115, distance: 1.5938192973696743 entropy 8.221046832770226
epoch: 3, step: 84
	action: tensor([[-1.4770e+06, -6.2800e+00, -7.2469e+07, -1.4177e+08,  6.2800e+00,
         -1.0348e+08, -4.9018e+07]], dtype=torch.float64)
	q_value: tensor([[2.4920]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11973241295823145, distance: 1.073653263700839 entropy 8.161375836869615
epoch: 3, step: 85
	action: tensor([[-2.0499e+08, -6.2800e+00,  2.6566e+08, -6.1609e+07,  6.2800e+00,
         -9.3106e+07,  5.1139e+07]], dtype=torch.float64)
	q_value: tensor([[2.8625]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05290319729106452, distance: 1.1136632243246607 entropy 8.217545634286934
epoch: 3, step: 86
	action: tensor([[ 3.1994e+07, -6.2800e+00,  7.5077e+07, -1.6545e+06,  6.2800e+00,
          1.7226e+08, -3.4613e+07]], dtype=torch.float64)
	q_value: tensor([[2.9031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5397865199987596, distance: 0.7763121099798035 entropy 8.259128032130917
epoch: 3, step: 87
	action: tensor([[ 8.0124e+07, -6.2800e+00, -3.5188e+07, -3.6182e+07,  6.2800e+00,
         -1.9631e+07, -3.2307e+07]], dtype=torch.float64)
	q_value: tensor([[1.8160]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4028977861660289, distance: 0.8842621461580719 entropy 7.997123244509761
epoch: 3, step: 88
	action: tensor([[ 1.5974e+08, -6.2800e+00,  2.2931e+08,  1.7356e+08,  6.2800e+00,
         -2.0085e+08, -5.8084e+07]], dtype=torch.float64)
	q_value: tensor([[3.5527]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39906767240353436, distance: 0.8870936636839646 entropy 8.292652178514553
epoch: 3, step: 89
	action: tensor([[-1.7375e+08, -6.2800e+00, -9.9903e+07, -4.3012e+07,  6.2800e+00,
         -5.4674e+07, -7.0993e+07]], dtype=torch.float64)
	q_value: tensor([[2.5928]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0655883240490015, distance: 1.1061800496199183 entropy 8.066977357129023
epoch: 3, step: 90
	action: tensor([[ 6.0162e+06, -6.2800e+00,  3.0124e+08,  1.8350e+07,  6.2800e+00,
          1.6152e+08, -1.4613e+07]], dtype=torch.float64)
	q_value: tensor([[3.3643]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4120651707513384, distance: 0.8774477962190351 entropy 8.437152844390718
epoch: 3, step: 91
	action: tensor([[ 1.6095e+08, -6.2800e+00,  7.3917e+07, -1.1571e+07,  6.2800e+00,
          7.2683e+07,  3.1338e+07]], dtype=torch.float64)
	q_value: tensor([[1.7278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.43638503569805365, distance: 0.8591083714050798 entropy 7.91548863897004
epoch: 3, step: 92
	action: tensor([[-8.7193e+07, -6.2800e+00,  6.4210e+07,  3.6908e+07,  6.2800e+00,
          4.8951e+07,  1.6031e+08]], dtype=torch.float64)
	q_value: tensor([[1.8394]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09555233452797396, distance: 1.0882994616191615 entropy 7.986018074673374
epoch: 3, step: 93
	action: tensor([[ 6.7624e+07, -6.2800e+00,  3.5745e+07,  1.5952e+08,  6.2800e+00,
          7.8217e+07, -1.4490e+07]], dtype=torch.float64)
	q_value: tensor([[2.0704]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4135834735468067, distance: 0.8763140883760762 entropy 8.094813341840172
epoch: 3, step: 94
	action: tensor([[ 1.2799e+08, -6.2800e+00,  2.9068e+07, -3.2247e+07,  6.2800e+00,
         -4.7133e+07, -4.2934e+06]], dtype=torch.float64)
	q_value: tensor([[1.8224]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4597402348215972, distance: 0.8411200897458635 entropy 7.976383420426331
epoch: 3, step: 95
	action: tensor([[ 4.6120e+06, -6.2800e+00,  8.7278e+07,  8.3803e+06,  6.2800e+00,
         -7.3097e+06,  2.5424e+08]], dtype=torch.float64)
	q_value: tensor([[2.7591]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3078231471497036, distance: 0.9520619953565095 entropy 8.330216930969423
epoch: 3, step: 96
	action: tensor([[-2.8388e+08, -6.2800e+00, -3.0350e+07,  1.2207e+08,  6.2800e+00,
         -1.3732e+07, -2.3785e+07]], dtype=torch.float64)
	q_value: tensor([[2.7029]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8011012730496305, distance: 1.5357685151840599 entropy 8.119955180831498
epoch: 3, step: 97
	action: tensor([[-2.5220e+07, -6.2800e+00,  1.1212e+08, -1.0814e+08,  6.2800e+00,
         -3.2594e+08,  9.1978e+04]], dtype=torch.float64)
	q_value: tensor([[2.3835]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10559188217960325, distance: 1.08224243672756 entropy 8.122763284346293
epoch: 3, step: 98
	action: tensor([[-9.7494e+07, -6.2800e+00, -8.0002e+07, -2.4834e+08,  6.2800e+00,
          1.1439e+08, -1.4515e+07]], dtype=torch.float64)
	q_value: tensor([[2.9883]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9618352156856953, distance: 1.6028318224397926 entropy 8.24431459268759
epoch: 3, step: 99
	action: tensor([[-3.3487e+07, -6.2800e+00,  2.4681e+08,  6.6911e+07,  6.2800e+00,
          1.2825e+07,  3.5630e+07]], dtype=torch.float64)
	q_value: tensor([[2.6632]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06757894284808352, distance: 1.1050011491122569 entropy 8.10462814314901
epoch: 3, step: 100
	action: tensor([[ 2.4966e+08, -6.2800e+00,  4.0184e+07, -1.8907e+08,  6.2800e+00,
         -2.3065e+08, -4.8995e+07]], dtype=torch.float64)
	q_value: tensor([[1.8036]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42478066426168837, distance: 0.8679074801321934 entropy 7.999861210053468
epoch: 3, step: 101
	action: tensor([[-2.1859e+08, -6.2800e+00,  5.0624e+08,  1.1489e+07,  6.2800e+00,
         -4.1376e+08,  1.0646e+08]], dtype=torch.float64)
	q_value: tensor([[3.4848]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5421620561384068, distance: 1.421090381457017 entropy 8.37769231563847
epoch: 3, step: 102
	action: tensor([[ 1.0117e+07, -6.2800e+00,  2.0864e+08, -7.9849e+06,  6.2800e+00,
          5.8592e+06,  3.7753e+07]], dtype=torch.float64)
	q_value: tensor([[2.6267]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.46200853666156927, distance: 0.8393524946012489 entropy 8.084326415968794
epoch: 3, step: 103
	action: tensor([[ 5.9983e+07, -6.2800e+00, -3.0104e+08, -5.9156e+07,  6.2800e+00,
         -1.1462e+08,  4.8664e+06]], dtype=torch.float64)
	q_value: tensor([[2.2544]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4741431822627965, distance: 0.8298325152208902 entropy 8.099257870745971
epoch: 3, step: 104
	action: tensor([[ 9.2986e+07, -6.2800e+00,  1.7077e+08, -8.5404e+07,  6.2800e+00,
         -2.1500e+08,  1.1857e+08]], dtype=torch.float64)
	q_value: tensor([[2.6496]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.48885387711658934, distance: 0.8181430184931663 entropy 8.109026175879539
epoch: 3, step: 105
	action: tensor([[-1.7830e+08, -6.2800e+00,  6.2503e+07,  2.0318e+07,  6.2800e+00,
          4.5223e+07,  2.1378e+08]], dtype=torch.float64)
	q_value: tensor([[3.3422]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06513458415059004, distance: 1.1064485913770448 entropy 8.275860898745815
epoch: 3, step: 106
	action: tensor([[ 3.4732e+07, -6.2800e+00,  4.1206e+06, -9.2358e+07,  6.2800e+00,
          1.2503e+08,  9.0114e+07]], dtype=torch.float64)
	q_value: tensor([[1.9123]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4438655680743294, distance: 0.8533881056394341 entropy 8.041309485822584
epoch: 3, step: 107
	action: tensor([[-1.0057e+08, -6.2800e+00,  8.5710e+07, -2.8080e+08,  6.2800e+00,
          4.4217e+07, -1.3008e+08]], dtype=torch.float64)
	q_value: tensor([[2.4873]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.60245312590714, distance: 1.4486029343304492 entropy 8.169730085388267
epoch: 3, step: 108
	action: tensor([[ 1.6049e+08, -6.2800e+00, -1.8830e+08, -5.7384e+07,  6.2800e+00,
          5.5139e+06,  3.3553e+07]], dtype=torch.float64)
	q_value: tensor([[2.7798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7821711265509075, distance: 0.5340899623997047 entropy 8.19727580782715
epoch: 3, step: 109
	action: tensor([[-1.8169e+07, -6.2800e+00,  6.9693e+07,  9.1422e+07,  6.2800e+00,
         -7.9784e+06,  3.9128e+07]], dtype=torch.float64)
	q_value: tensor([[2.1940]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9992559019517102, distance: 1.6180460837822108 entropy 8.147298832335897
epoch: 3, step: 110
	action: tensor([[ 3.5433e+08, -6.2800e+00,  6.8507e+07, -1.1936e+08,  6.2800e+00,
          1.9997e+08,  2.4136e+08]], dtype=torch.float64)
	q_value: tensor([[2.5851]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.518725947320182, distance: 0.7938764467275272 entropy 8.219237190769654
epoch: 3, step: 111
	action: tensor([[-2.7701e+07, -6.2800e+00,  7.4559e+07,  5.2418e+07,  6.2800e+00,
          1.5570e+07,  7.0230e+07]], dtype=torch.float64)
	q_value: tensor([[2.1588]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08143017643248174, distance: 1.0967629734330862 entropy 8.164749698437198
epoch: 3, step: 112
	action: tensor([[ 1.0179e+07, -6.2800e+00,  3.1184e+07, -3.9336e+07,  6.2800e+00,
          4.8177e+06, -1.1045e+08]], dtype=torch.float64)
	q_value: tensor([[2.1642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6231250725092154, distance: 0.7025145382786626 entropy 8.107415983688083
epoch: 3, step: 113
	action: tensor([[-1.4266e+08, -6.2800e+00,  6.6547e+07,  1.8465e+08,  6.2800e+00,
          2.5076e+08, -7.6523e+07]], dtype=torch.float64)
	q_value: tensor([[2.4619]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0700121942018076, distance: 1.103558398567686 entropy 8.191300914354018
epoch: 3, step: 114
	action: tensor([[-4.1259e+07, -6.2800e+00,  3.8209e+07, -1.0903e+08,  6.2800e+00,
         -3.2254e+07,  9.8080e+07]], dtype=torch.float64)
	q_value: tensor([[1.8095]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.059356250731262494, distance: 1.1098627619365602 entropy 8.016444204997105
epoch: 3, step: 115
	action: tensor([[ 1.8719e+08, -6.2800e+00, -2.0383e+08,  7.5487e+07,  6.2800e+00,
         -6.1616e+07,  1.2786e+07]], dtype=torch.float64)
	q_value: tensor([[3.4745]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5334994908428861, distance: 0.7815967670314713 entropy 8.329782990179314
epoch: 3, step: 116
	action: tensor([[-8.4402e+07, -6.2800e+00,  2.9514e+07, -7.5839e+07,  6.2800e+00,
         -1.5177e+08,  5.4468e+06]], dtype=torch.float64)
	q_value: tensor([[2.5479]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03226544322339597, distance: 1.125731498519315 entropy 8.02354560988449
epoch: 3, step: 117
	action: tensor([[ 5.6873e+07, -6.2800e+00, -2.9586e+07,  2.5823e+06,  6.2800e+00,
          1.2064e+08, -9.5862e+07]], dtype=torch.float64)
	q_value: tensor([[3.6879]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4264564250117392, distance: 0.8666423400293818 entropy 8.356516267511541
epoch: 3, step: 118
	action: tensor([[ 8.2121e+07, -6.2800e+00, -1.0073e+08,  1.2303e+08,  6.2800e+00,
         -2.5357e+07, -1.9284e+07]], dtype=torch.float64)
	q_value: tensor([[1.8098]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8697689005209052, distance: 0.41296576199203205 entropy 7.962567992738441
epoch: 3, step: 119
	action: tensor([[-1.1378e+08, -6.2800e+00, -1.7474e+08, -4.4443e+07,  6.2800e+00,
          8.4035e+07, -1.0745e+07]], dtype=torch.float64)
	q_value: tensor([[2.8299]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 7.981978563071668
epoch: 3, step: 120
	action: tensor([[ 4.9708e+07, -6.2800e+00, -1.2107e+08,  4.1290e+07,  6.2800e+00,
          4.5437e+07,  3.3112e+07]], dtype=torch.float64)
	q_value: tensor([[2.3662]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38635696319722046, distance: 0.8964263182262111 entropy 8.006688390151657
epoch: 3, step: 121
	action: tensor([[ 2.9316e+07, -6.2800e+00,  1.0305e+08,  2.3804e+08,  6.2800e+00,
         -7.6107e+07, -6.7680e+07]], dtype=torch.float64)
	q_value: tensor([[1.9529]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6968121021565612, distance: 0.6301045623596615 entropy 8.009710389674018
epoch: 3, step: 122
	action: tensor([[ 4.4328e+07, -6.2800e+00,  1.9835e+08,  4.1231e+07,  6.2800e+00,
         -1.4528e+08,  7.7146e+07]], dtype=torch.float64)
	q_value: tensor([[2.5636]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19045464536206658, distance: 1.0296207486922448 entropy 8.082176719553813
epoch: 3, step: 123
	action: tensor([[-1.1476e+08, -6.2800e+00,  9.0436e+07,  2.7999e+08,  6.2800e+00,
          1.7199e+08,  1.3252e+08]], dtype=torch.float64)
	q_value: tensor([[2.6294]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08826397910937323, distance: 1.0926756118983996 entropy 8.083159669337737
epoch: 3, step: 124
	action: tensor([[-3.6941e+07, -6.2800e+00,  3.1145e+07, -7.4311e+06,  6.2800e+00,
          2.3698e+08,  7.1635e+07]], dtype=torch.float64)
	q_value: tensor([[2.0030]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4692455625811187, distance: 1.387087592983977 entropy 8.08847620311357
epoch: 3, step: 125
	action: tensor([[-1.6521e+08, -6.2800e+00,  3.5770e+06, -2.0907e+08,  6.2800e+00,
          4.0592e+07, -1.1544e+08]], dtype=torch.float64)
	q_value: tensor([[2.7724]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.28383257147600327, distance: 1.296614542253336 entropy 8.186919044815197
epoch: 3, step: 126
	action: tensor([[-4.3546e+07, -6.2800e+00,  8.9126e+07, -3.3047e+08,  6.2800e+00,
          4.4594e+07, -4.0419e+07]], dtype=torch.float64)
	q_value: tensor([[2.7253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.31989487705458575, distance: 1.3146990977402209 entropy 8.28150685299431
epoch: 3, step: 127
	action: tensor([[-1.7215e+08, -6.2800e+00,  4.1471e+07,  2.7990e+06,  6.2800e+00,
         -2.4316e+08, -3.9357e+07]], dtype=torch.float64)
	q_value: tensor([[2.6010]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3295686005179057, distance: 1.31950812367134 entropy 8.212188836581584
LOSS epoch 3 actor 99.1502806048981 critic 674.0872400802344 entropy 0.1
epoch: 4, step: 0
	action: tensor([[ 9.7797e+08, -6.2800e+00, -6.3457e+08, -2.2621e+08,  6.2800e+00,
         -3.3975e+08,  4.6022e+08]], dtype=torch.float64)
	q_value: tensor([[1.2963]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3835551855861242, distance: 0.898470443806163 entropy 9.091990762103077
epoch: 4, step: 1
	action: tensor([[-1.2562e+09, -6.2800e+00, -2.1736e+08, -8.4663e+08,  6.2800e+00,
          4.8744e+07,  1.5574e+08]], dtype=torch.float64)
	q_value: tensor([[1.4400]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.264420932306388
epoch: 4, step: 2
	action: tensor([[ 7.5737e+06, -6.2800e+00, -3.0903e+08,  9.6626e+07,  6.2800e+00,
          1.4731e+08, -5.7013e+08]], dtype=torch.float64)
	q_value: tensor([[1.4732]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.97846684042968
epoch: 4, step: 3
	action: tensor([[-3.1543e+08, -6.2800e+00, -4.0255e+08, -1.8607e+08,  6.2800e+00,
         -3.4621e+08,  3.7897e+08]], dtype=torch.float64)
	q_value: tensor([[1.4732]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11678945314950662, distance: 1.075446514934163 entropy 8.97846684042968
epoch: 4, step: 4
	action: tensor([[-3.3928e+08, -6.2800e+00,  4.6245e+08,  5.0669e+08,  6.2800e+00,
          3.7535e+08, -1.3816e+07]], dtype=torch.float64)
	q_value: tensor([[1.7810]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15108453568215807, distance: 1.0543599862743234 entropy 9.267238362859885
epoch: 4, step: 5
	action: tensor([[ 5.1620e+08, -6.2800e+00, -1.1077e+08,  5.8638e+08,  6.2800e+00,
          1.9314e+08, -1.5666e+07]], dtype=torch.float64)
	q_value: tensor([[1.0797]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.48930496076425234, distance: 0.817781935450696 entropy 9.120362487304778
epoch: 4, step: 6
	action: tensor([[ 1.0683e+09, -6.2800e+00, -1.3785e+08,  1.8992e+08,  6.2800e+00,
          7.1252e+08, -1.5192e+08]], dtype=torch.float64)
	q_value: tensor([[0.8690]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4521756950448219, distance: 0.8469881640456765 entropy 9.038259987856149
epoch: 4, step: 7
	action: tensor([[ 5.5248e+07, -6.2800e+00, -4.3559e+08,  7.1664e+08,  6.2800e+00,
         -2.6410e+08,  2.0923e+08]], dtype=torch.float64)
	q_value: tensor([[0.9397]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27096672308563363, distance: 0.977080552693893 entropy 9.021970177504661
epoch: 4, step: 8
	action: tensor([[ 2.9173e+08, -6.2800e+00,  4.0562e+08, -4.2958e+08,  6.2800e+00,
          7.1625e+07,  2.6045e+08]], dtype=torch.float64)
	q_value: tensor([[1.6813]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7199414260602064, distance: 0.6055933949707083 entropy 9.22300367899827
epoch: 4, step: 9
	action: tensor([[-5.8182e+08, -6.2800e+00, -3.0984e+07, -6.5837e+08,  6.2800e+00,
          4.9434e+08, -5.7696e+08]], dtype=torch.float64)
	q_value: tensor([[1.2088]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6467080223184405, distance: 1.4684697137543767 entropy 9.265983805428016
epoch: 4, step: 10
	action: tensor([[ 1.3051e+09, -6.2800e+00,  4.3224e+08, -2.7815e+08,  6.2800e+00,
         -3.8753e+08, -1.1252e+08]], dtype=torch.float64)
	q_value: tensor([[1.3931]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3751498956701206, distance: 0.9045750744664415 entropy 9.18556021227545
epoch: 4, step: 11
	action: tensor([[-5.2212e+07, -6.2800e+00, -6.0630e+08, -4.4690e+08,  6.2800e+00,
          1.3490e+09,  6.8713e+08]], dtype=torch.float64)
	q_value: tensor([[1.3608]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8829093923954865, distance: 1.5702594082600108 entropy 9.19766534930453
epoch: 4, step: 12
	action: tensor([[ 2.0367e+08, -6.2800e+00, -5.5306e+08, -3.8394e+08,  6.2800e+00,
         -3.2395e+08,  1.4254e+08]], dtype=torch.float64)
	q_value: tensor([[1.3940]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44618051600362263, distance: 0.8516101100288153 entropy 9.061890095026063
epoch: 4, step: 13
	action: tensor([[-1.0133e+08, -6.2800e+00, -2.8411e+08,  5.8251e+08,  6.2800e+00,
         -7.2014e+08,  1.4943e+08]], dtype=torch.float64)
	q_value: tensor([[1.1499]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4710245157775048, distance: 1.3879270773745378 entropy 9.12565204462141
epoch: 4, step: 14
	action: tensor([[-2.3089e+08, -6.2800e+00, -1.3044e+08,  1.8060e+07,  6.2800e+00,
         -6.1376e+07, -3.5480e+08]], dtype=torch.float64)
	q_value: tensor([[1.5635]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9160209403643624, distance: 1.5840059874921169 entropy 9.155553958319226
epoch: 4, step: 15
	action: tensor([[-1.3072e+08, -6.2800e+00, -2.8769e+08,  5.1285e+08,  6.2800e+00,
         -2.1921e+08,  5.2629e+08]], dtype=torch.float64)
	q_value: tensor([[1.5985]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9326562011897448, distance: 1.590867448252533 entropy 9.266022094600485
epoch: 4, step: 16
	action: tensor([[-4.9403e+08, -6.2800e+00,  4.2654e+08, -6.3580e+08,  6.2800e+00,
         -3.2010e+08,  2.3684e+08]], dtype=torch.float64)
	q_value: tensor([[1.4749]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1112519681431472, distance: 1.0788126226250279 entropy 9.221678337140636
epoch: 4, step: 17
	action: tensor([[-1.1816e+09, -6.2800e+00, -7.1223e+08, -5.8795e+08,  6.2800e+00,
         -7.5825e+08,  5.2741e+08]], dtype=torch.float64)
	q_value: tensor([[1.5925]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07575048866906153, distance: 1.1001484927206897 entropy 9.237018940716737
epoch: 4, step: 18
	action: tensor([[ 1.5437e+08, -6.2800e+00, -4.8449e+08,  6.8771e+08,  6.2800e+00,
         -7.7077e+08, -4.6679e+08]], dtype=torch.float64)
	q_value: tensor([[1.7776]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.40805566185409214, distance: 0.8804346553284229 entropy 9.307727162612833
epoch: 4, step: 19
	action: tensor([[ 3.3294e+07, -6.2800e+00,  9.2460e+08, -5.5150e+08,  6.2800e+00,
         -6.3002e+08,  2.1577e+08]], dtype=torch.float64)
	q_value: tensor([[1.5752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.47394629280998746, distance: 0.8299878521621097 entropy 9.039461352372813
epoch: 4, step: 20
	action: tensor([[-9.1315e+08, -6.2800e+00,  1.8254e+08, -5.0244e+08,  6.2800e+00,
         -2.4790e+08, -2.7636e+08]], dtype=torch.float64)
	q_value: tensor([[1.1439]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09536751290359269, distance: 1.0884106515792655 entropy 9.104829526005
epoch: 4, step: 21
	action: tensor([[ 1.4434e+09, -6.2800e+00, -8.1601e+08, -2.0417e+08,  6.2800e+00,
         -5.1383e+07,  2.3366e+08]], dtype=torch.float64)
	q_value: tensor([[1.5927]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44980601141409826, distance: 0.8488180647696056 entropy 9.201568325357565
epoch: 4, step: 22
	action: tensor([[-1.6373e+08, -6.2800e+00, -1.4349e+08, -2.0299e+08,  6.2800e+00,
         -1.1850e+08, -5.8854e+08]], dtype=torch.float64)
	q_value: tensor([[1.1564]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09567448183327965, distance: 1.088225970725113 entropy 9.16499738643378
epoch: 4, step: 23
	action: tensor([[-3.5356e+08, -6.2800e+00,  5.6321e+08, -1.3268e+09,  6.2800e+00,
         -2.8193e+08,  4.3435e+08]], dtype=torch.float64)
	q_value: tensor([[1.6440]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09060474383291417, distance: 1.0912720585574411 entropy 9.19518222089683
epoch: 4, step: 24
	action: tensor([[-2.8896e+08, -6.2800e+00,  1.1473e+09, -1.6077e+08,  6.2800e+00,
         -7.4910e+08, -2.0489e+08]], dtype=torch.float64)
	q_value: tensor([[1.7298]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07556072797372837, distance: 1.1002614244701445 entropy 9.285303084521555
epoch: 4, step: 25
	action: tensor([[-1.0495e+09, -6.2800e+00,  1.3196e+08, -4.6496e+08,  6.2800e+00,
         -9.0081e+08,  2.5813e+07]], dtype=torch.float64)
	q_value: tensor([[1.5906]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.054465022229718274, distance: 1.1127445934844422 entropy 9.211645296869559
epoch: 4, step: 26
	action: tensor([[ 5.4792e+08, -6.2800e+00,  1.8759e+08, -1.7528e+08,  6.2800e+00,
          1.7622e+08, -2.0648e+08]], dtype=torch.float64)
	q_value: tensor([[1.7605]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.368911677677115
epoch: 4, step: 27
	action: tensor([[ 5.6100e+07, -6.2800e+00, -5.0227e+08, -2.2131e+08,  6.2800e+00,
          5.7037e+08, -9.9485e+07]], dtype=torch.float64)
	q_value: tensor([[1.4732]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.97846684042968
epoch: 4, step: 28
	action: tensor([[-6.5236e+07, -6.2800e+00,  6.9368e+08,  8.3583e+08,  6.2800e+00,
          3.6605e+07,  2.8906e+08]], dtype=torch.float64)
	q_value: tensor([[1.4732]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14277259658632224, distance: 1.0595091611170722 entropy 8.97846684042968
epoch: 4, step: 29
	action: tensor([[-2.7466e+08, -6.2800e+00, -2.0090e+08, -1.3711e+06,  6.2800e+00,
         -1.4134e+09, -4.1600e+08]], dtype=torch.float64)
	q_value: tensor([[1.1600]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14425070725621914, distance: 1.0585953152322296 entropy 9.106121829773446
epoch: 4, step: 30
	action: tensor([[ 5.0647e+06, -6.2800e+00, -1.2248e+09, -4.2065e+07,  6.2800e+00,
         -3.1029e+08,  8.7523e+07]], dtype=torch.float64)
	q_value: tensor([[1.5426]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4258700102792752, distance: 0.8670852723716496 entropy 9.202246804345345
epoch: 4, step: 31
	action: tensor([[ 9.5953e+08, -6.2800e+00,  6.8883e+08,  8.3877e+07,  6.2800e+00,
          9.7992e+08,  1.1651e+09]], dtype=torch.float64)
	q_value: tensor([[1.2270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38009271750952944, distance: 0.9009901900954127 entropy 9.245638541044203
epoch: 4, step: 32
	action: tensor([[ 3.0978e+07, -6.2800e+00,  6.1466e+08, -6.3843e+06,  6.2800e+00,
         -3.7663e+08, -7.5410e+07]], dtype=torch.float64)
	q_value: tensor([[0.8058]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4155274272674022, distance: 0.8748604047951085 entropy 8.92213447929434
epoch: 4, step: 33
	action: tensor([[ 1.1384e+09, -6.2800e+00, -8.2396e+07,  1.5074e+09,  6.2800e+00,
         -6.4749e+08,  5.5792e+08]], dtype=torch.float64)
	q_value: tensor([[1.2388]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24400725627255404, distance: 0.9949826476455185 entropy 9.256094199969406
epoch: 4, step: 34
	action: tensor([[-1.5058e+08, -6.2800e+00,  4.1443e+08,  6.6276e+08,  6.2800e+00,
          1.1125e+08,  2.8232e+08]], dtype=torch.float64)
	q_value: tensor([[1.4842]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04315736579412188, distance: 1.1193784772064657 entropy 9.034394855634147
epoch: 4, step: 35
	action: tensor([[-2.4585e+08, -6.2800e+00,  3.7268e+08, -7.4354e+07,  6.2800e+00,
         -4.7280e+08,  2.3506e+08]], dtype=torch.float64)
	q_value: tensor([[0.9384]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06880413080920489, distance: 1.1042749326049037 entropy 9.114542029129968
epoch: 4, step: 36
	action: tensor([[-6.8739e+06, -6.2800e+00,  7.7037e+08, -1.3423e+09,  6.2800e+00,
          6.1234e+08,  3.1362e+08]], dtype=torch.float64)
	q_value: tensor([[1.7363]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.795201302855338, distance: 1.5332510495705274 entropy 9.361641000761997
epoch: 4, step: 37
	action: tensor([[ 2.7740e+07, -6.2800e+00,  7.0495e+06, -5.8857e+08,  6.2800e+00,
          1.8333e+08,  7.3397e+07]], dtype=torch.float64)
	q_value: tensor([[1.5504]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6532207262875885, distance: 0.6738810744088078 entropy 9.100845278678548
epoch: 4, step: 38
	action: tensor([[ 4.9421e+08, -6.2800e+00,  8.4204e+08, -1.0360e+09,  6.2800e+00,
          6.2378e+08,  1.7707e+06]], dtype=torch.float64)
	q_value: tensor([[1.5395]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18149498945843534, distance: 1.035302742452324 entropy 9.267634904848219
epoch: 4, step: 39
	action: tensor([[ 4.7206e+08, -6.2800e+00, -2.4352e+07, -1.7247e+08,  6.2800e+00,
         -1.3848e+08,  1.9871e+08]], dtype=torch.float64)
	q_value: tensor([[0.9921]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.43599577802524203, distance: 0.8594049895096825 entropy 8.992238846101746
epoch: 4, step: 40
	action: tensor([[ 1.2296e+09, -6.2800e+00,  1.9385e+08, -1.0813e+09,  6.2800e+00,
          4.5788e+08,  1.7456e+08]], dtype=torch.float64)
	q_value: tensor([[1.1889]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6954133499375529, distance: 0.6315563781547283 entropy 9.188438103384911
epoch: 4, step: 41
	action: tensor([[-2.6525e+08, -6.2800e+00, -3.1528e+07,  2.6517e+08,  6.2800e+00,
         -1.9368e+08,  3.6302e+07]], dtype=torch.float64)
	q_value: tensor([[0.8756]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.664697177023315, distance: 1.4764689383566505 entropy 8.99545455075494
epoch: 4, step: 42
	action: tensor([[ 9.2656e+08, -6.2800e+00,  3.9622e+08,  2.1289e+08,  6.2800e+00,
         -6.9366e+07,  3.2314e+08]], dtype=torch.float64)
	q_value: tensor([[1.1912]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7179354397668788, distance: 0.6077583786085474 entropy 9.105614063900854
epoch: 4, step: 43
	action: tensor([[-5.1387e+07, -6.2800e+00,  1.5055e+08,  3.5907e+08,  6.2800e+00,
         -7.9303e+08,  2.1322e+07]], dtype=torch.float64)
	q_value: tensor([[1.5156]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4362080906523771, distance: 1.3714038922811007 entropy 9.031762933643309
epoch: 4, step: 44
	action: tensor([[ 3.1004e+08, -6.2800e+00,  4.4665e+08, -5.9336e+08,  6.2800e+00,
         -4.5389e+08, -1.3360e+07]], dtype=torch.float64)
	q_value: tensor([[1.4958]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4001960862237036, distance: 0.8862603925632999 entropy 9.130566123957234
epoch: 4, step: 45
	action: tensor([[-9.3335e+06, -6.2800e+00,  2.3717e+08, -3.5736e+08,  6.2800e+00,
         -8.5954e+07, -3.8751e+08]], dtype=torch.float64)
	q_value: tensor([[1.1695]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10620902158402523, distance: 1.0818689999872841 entropy 9.15142916603632
epoch: 4, step: 46
	action: tensor([[ 5.6320e+08, -6.2800e+00,  3.7682e+08,  2.5352e+07,  6.2800e+00,
         -1.7989e+08,  7.4282e+08]], dtype=torch.float64)
	q_value: tensor([[1.7429]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7179673987304299, distance: 0.6077239469910682 entropy 9.246815067755676
epoch: 4, step: 47
	action: tensor([[-5.6461e+08, -6.2800e+00,  1.4355e+07, -4.4976e+07,  6.2800e+00,
         -7.5383e+08, -1.3800e+08]], dtype=torch.float64)
	q_value: tensor([[1.4890]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.02347426101068395, distance: 1.1308331746314786 entropy 8.946958177544909
epoch: 4, step: 48
	action: tensor([[-5.8780e+07, -6.2800e+00, -3.1506e+08,  1.0820e+08,  6.2800e+00,
          1.0207e+08,  4.9780e+08]], dtype=torch.float64)
	q_value: tensor([[1.6767]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07109836386668567, distance: 1.1029137654653962 entropy 9.324851525087052
epoch: 4, step: 49
	action: tensor([[-5.7313e+07, -6.2800e+00,  1.0477e+08,  4.1196e+08,  6.2800e+00,
         -8.0921e+08,  7.2013e+08]], dtype=torch.float64)
	q_value: tensor([[0.8217]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6244676612877293, distance: 1.4585194612187324 entropy 9.048990258369338
epoch: 4, step: 50
	action: tensor([[-2.4816e+08, -6.2800e+00,  7.6363e+08, -2.6307e+08,  6.2800e+00,
         -9.9399e+08,  2.4895e+08]], dtype=torch.float64)
	q_value: tensor([[1.6564]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07159314144690843, distance: 1.1026199939462695 entropy 9.218253444771678
epoch: 4, step: 51
	action: tensor([[ 5.7383e+08, -6.2800e+00, -1.2975e+09,  2.7244e+08,  6.2800e+00,
          2.0834e+07, -3.3227e+08]], dtype=torch.float64)
	q_value: tensor([[1.6543]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42780084110850325, distance: 0.8656260166456897 entropy 9.280791309263845
epoch: 4, step: 52
	action: tensor([[-2.2996e+06, -6.2800e+00, -3.9916e+08,  4.6889e+08,  6.2800e+00,
         -1.7525e+08,  1.2321e+07]], dtype=torch.float64)
	q_value: tensor([[0.7805]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.27209497559958695, distance: 1.290673704063304 entropy 8.930311027813593
epoch: 4, step: 53
	action: tensor([[-1.9061e+08, -6.2800e+00, -1.4281e+08, -3.6678e+08,  6.2800e+00,
         -9.8056e+08,  1.1674e+08]], dtype=torch.float64)
	q_value: tensor([[1.3896]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06822898126914057, distance: 1.1046159055411704 entropy 9.066925462198872
epoch: 4, step: 54
	action: tensor([[-6.6699e+07, -6.2800e+00,  3.1451e+07, -4.0901e+08,  6.2800e+00,
         -3.3262e+08,  3.9965e+08]], dtype=torch.float64)
	q_value: tensor([[1.8239]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0842397547743452, distance: 1.0950843849981449 entropy 9.342454956608515
epoch: 4, step: 55
	action: tensor([[-5.7928e+08, -6.2800e+00, -2.3374e+08, -4.5111e+08,  6.2800e+00,
          6.4184e+08, -6.7616e+08]], dtype=torch.float64)
	q_value: tensor([[1.7951]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.90138711191288, distance: 1.5779453799746126 entropy 9.325563859773407
epoch: 4, step: 56
	action: tensor([[ 3.5924e+08, -6.2800e+00,  8.4201e+07, -3.0963e+08,  6.2800e+00,
          2.5614e+08, -3.9388e+08]], dtype=torch.float64)
	q_value: tensor([[1.3557]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14125927697623863, distance: 1.0604439591484927 entropy 9.112233899098156
epoch: 4, step: 57
	action: tensor([[ 2.2634e+08, -6.2800e+00, -4.0388e+08, -9.2619e+08,  6.2800e+00,
          4.6590e+08,  1.2469e+09]], dtype=torch.float64)
	q_value: tensor([[1.3804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36174560344214535, distance: 0.9142260686836801 entropy 9.159208813312025
epoch: 4, step: 58
	action: tensor([[ 5.5709e+08, -6.2800e+00, -5.9930e+08, -2.3008e+08,  6.2800e+00,
          4.5744e+08,  3.4664e+08]], dtype=torch.float64)
	q_value: tensor([[1.1823]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8049988372661407, distance: 0.5053302465733452 entropy 9.098132507501257
epoch: 4, step: 59
	action: tensor([[ 4.0479e+08, -6.2800e+00,  8.7385e+04,  2.5515e+08,  6.2800e+00,
         -8.7212e+07,  8.3595e+07]], dtype=torch.float64)
	q_value: tensor([[0.8775]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6891307686789183, distance: 0.6380365571763921 entropy 9.06288517189014
epoch: 4, step: 60
	action: tensor([[-4.4545e+07, -6.2800e+00,  3.4918e+08,  4.4673e+08,  6.2800e+00,
         -2.1588e+07, -1.4861e+07]], dtype=torch.float64)
	q_value: tensor([[1.6829]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.28010446935966016, distance: 1.2947305638001638 entropy 9.140779536274554
epoch: 4, step: 61
	action: tensor([[ 2.3499e+08, -6.2800e+00,  2.9417e+07,  6.4558e+08,  6.2800e+00,
         -1.4086e+08,  5.2590e+08]], dtype=torch.float64)
	q_value: tensor([[1.3294]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.715739007124653, distance: 0.6101200927085924 entropy 9.091432547036545
epoch: 4, step: 62
	action: tensor([[-7.6357e+07, -6.2800e+00, -4.1757e+08,  4.6114e+08,  6.2800e+00,
         -4.0801e+08,  1.5217e+08]], dtype=torch.float64)
	q_value: tensor([[1.6090]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5688906387417125, distance: 1.4333525688299904 entropy 9.08058227520795
epoch: 4, step: 63
	action: tensor([[ 1.2133e+08, -6.2800e+00,  1.9070e+08, -2.5259e+08,  6.2800e+00,
          4.1171e+08, -1.0029e+08]], dtype=torch.float64)
	q_value: tensor([[1.4880]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5311262520217568, distance: 0.7835823625415198 entropy 9.150739666961096
epoch: 4, step: 64
	action: tensor([[ 6.3923e+08, -6.2800e+00,  2.9222e+08,  7.0611e+08,  6.2800e+00,
          1.9449e+08, -1.4933e+09]], dtype=torch.float64)
	q_value: tensor([[0.8886]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3348194295360519, distance: 0.9333111857632336 entropy 9.173397049897671
epoch: 4, step: 65
	action: tensor([[-2.6016e+08, -6.2800e+00, -1.0821e+08, -6.4122e+06,  6.2800e+00,
          4.4503e+08, -5.9946e+07]], dtype=torch.float64)
	q_value: tensor([[0.8999]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4783540886314752, distance: 1.3913805453569612 entropy 8.928617031774746
epoch: 4, step: 66
	action: tensor([[-3.9394e+08, -6.2800e+00, -3.3098e+08, -4.8620e+08,  6.2800e+00,
         -1.2072e+09,  2.2083e+08]], dtype=torch.float64)
	q_value: tensor([[1.7357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0025415141516895723, distance: 1.1457975147732131 entropy 9.177573428115627
epoch: 4, step: 67
	action: tensor([[-6.9325e+08, -6.2800e+00,  1.1036e+09,  8.1142e+08,  6.2800e+00,
          6.2025e+08, -5.6953e+08]], dtype=torch.float64)
	q_value: tensor([[1.6913]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04661125087122053, distance: 1.117356357936005 entropy 9.333573807258134
epoch: 4, step: 68
	action: tensor([[-4.8942e+08, -6.2800e+00, -2.4153e+08,  3.2151e+08,  6.2800e+00,
          3.3507e+08,  1.5047e+08]], dtype=torch.float64)
	q_value: tensor([[0.8369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07617151919246179, distance: 1.0998978846041196 entropy 8.912039903772836
epoch: 4, step: 69
	action: tensor([[ 9.1785e+07, -6.2800e+00,  4.2895e+08,  1.7988e+08,  6.2800e+00,
         -3.1233e+08,  1.3543e+08]], dtype=torch.float64)
	q_value: tensor([[0.8396]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42491396377896695, distance: 0.867806911232439 entropy 9.075034214519517
epoch: 4, step: 70
	action: tensor([[ 7.1648e+08, -6.2800e+00, -5.2860e+08, -9.2033e+06,  6.2800e+00,
         -1.3437e+09, -5.9605e+08]], dtype=torch.float64)
	q_value: tensor([[1.4013]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37236719734896306, distance: 0.9065870476390505 entropy 9.113332623229327
epoch: 4, step: 71
	action: tensor([[ 5.5934e+08, -6.2800e+00,  1.5158e+08, -5.8665e+07,  6.2800e+00,
          1.4151e+09,  2.5067e+08]], dtype=torch.float64)
	q_value: tensor([[1.5530]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08542592710288877, distance: 1.0943749309104485 entropy 9.320629120529457
epoch: 4, step: 72
	action: tensor([[-1.6135e+08, -6.2800e+00,  1.9469e+08, -3.7566e+08,  6.2800e+00,
          1.1698e+08, -6.5587e+08]], dtype=torch.float64)
	q_value: tensor([[0.8670]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3530968800825094, distance: 1.3311320482321944 entropy 9.08996731939592
epoch: 4, step: 73
	action: tensor([[ 7.9645e+07, -6.2800e+00, -4.2440e+08,  1.0626e+09,  6.2800e+00,
         -2.6134e+08,  1.3104e+08]], dtype=torch.float64)
	q_value: tensor([[1.8862]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7503140857521108, distance: 0.5718125918139391 entropy 9.279267223958394
epoch: 4, step: 74
	action: tensor([[ 3.1215e+08, -6.2800e+00,  5.0952e+08, -7.2896e+08,  6.2800e+00,
         -1.4144e+08,  4.1889e+08]], dtype=torch.float64)
	q_value: tensor([[1.3452]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41236654338647016, distance: 0.8772228795806728 entropy 8.980210046297199
epoch: 4, step: 75
	action: tensor([[-1.9476e+08, -6.2800e+00,  3.8275e+08,  6.7296e+08,  6.2800e+00,
          7.4753e+08,  4.6259e+08]], dtype=torch.float64)
	q_value: tensor([[1.1169]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10877480752260438, distance: 1.0803150352808752 entropy 9.068072242215504
epoch: 4, step: 76
	action: tensor([[-2.9689e+08, -6.2800e+00,  3.6576e+07,  1.9396e+08,  6.2800e+00,
          2.9938e+08,  5.6271e+07]], dtype=torch.float64)
	q_value: tensor([[1.0795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04961372550443466, distance: 1.1155955444129406 entropy 9.036297330702046
epoch: 4, step: 77
	action: tensor([[ 4.9019e+08, -6.2800e+00,  3.8034e+08, -3.1636e+08,  6.2800e+00,
         -4.0835e+08,  1.3760e+08]], dtype=torch.float64)
	q_value: tensor([[0.9384]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31683652664390805, distance: 0.9458429088868507 entropy 8.978170262203157
epoch: 4, step: 78
	action: tensor([[-1.8950e+07, -6.2800e+00, -1.9239e+08,  4.6148e+08,  6.2800e+00,
          1.3254e+08, -7.1780e+08]], dtype=torch.float64)
	q_value: tensor([[1.3969]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.243768898467964
epoch: 4, step: 79
	action: tensor([[ 6.4639e+08, -6.2800e+00,  5.7215e+07, -3.2019e+08,  6.2800e+00,
         -2.5563e+08, -7.2700e+08]], dtype=torch.float64)
	q_value: tensor([[1.4732]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.97846684042968
epoch: 4, step: 80
	action: tensor([[-1.1369e+09, -6.2800e+00, -4.4014e+08,  1.1079e+08,  6.2800e+00,
         -7.5767e+07, -4.9803e+07]], dtype=torch.float64)
	q_value: tensor([[1.4732]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09962531125187368, distance: 1.085846240643297 entropy 8.97846684042968
epoch: 4, step: 81
	action: tensor([[ 1.8272e+08, -6.2800e+00,  2.9116e+07,  8.5143e+06,  6.2800e+00,
         -4.8100e+08, -4.6471e+07]], dtype=torch.float64)
	q_value: tensor([[0.9326]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7310228780874837, distance: 0.5934913101115193 entropy 8.953606630246991
epoch: 4, step: 82
	action: tensor([[-3.2894e+08, -6.2800e+00, -5.3845e+08, -7.3055e+08,  6.2800e+00,
         -2.3094e+08,  4.5968e+08]], dtype=torch.float64)
	q_value: tensor([[1.6669]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11174110246610458, distance: 1.0785157123156073 entropy 9.124344427956073
epoch: 4, step: 83
	action: tensor([[-3.2877e+08, -6.2800e+00,  9.1609e+08,  8.7206e+08,  6.2800e+00,
          1.1668e+08, -1.3934e+08]], dtype=torch.float64)
	q_value: tensor([[1.6673]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09059036733373993, distance: 1.0912806844050789 entropy 9.322895551160343
epoch: 4, step: 84
	action: tensor([[ 2.6123e+08, -6.2800e+00,  4.2043e+08, -1.7540e+08,  6.2800e+00,
          3.1672e+07, -3.2543e+08]], dtype=torch.float64)
	q_value: tensor([[0.8689]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6775339503702813, distance: 0.6498284062306896 entropy 9.067542295734455
epoch: 4, step: 85
	action: tensor([[-4.8287e+08, -6.2800e+00, -4.0377e+08, -2.9100e+08,  6.2800e+00,
         -4.5447e+08, -3.8243e+08]], dtype=torch.float64)
	q_value: tensor([[1.1112]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12468699189122834, distance: 1.0706274757188534 entropy 9.065425997550337
epoch: 4, step: 86
	action: tensor([[ 5.4206e+08, -6.2800e+00,  6.2612e+08, -5.1949e+08,  6.2800e+00,
          6.0192e+07,  9.6114e+08]], dtype=torch.float64)
	q_value: tensor([[1.7883]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7391072745710986, distance: 0.5845042564456195 entropy 9.287251849216537
epoch: 4, step: 87
	action: tensor([[ 2.8836e+07, -6.2800e+00, -6.1878e+07,  4.3030e+08,  6.2800e+00,
          1.1611e+08, -3.5971e+08]], dtype=torch.float64)
	q_value: tensor([[1.0304]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39715254657359367, distance: 0.8885060894266689 entropy 9.057936523192769
epoch: 4, step: 88
	action: tensor([[ 4.5378e+07, -6.2800e+00, -2.8109e+07,  5.0563e+07,  6.2800e+00,
          3.7012e+07, -1.1163e+08]], dtype=torch.float64)
	q_value: tensor([[0.7640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4669920615171729, distance: 0.8354559030423175 entropy 8.929759273985711
epoch: 4, step: 89
	action: tensor([[-1.0327e+09, -6.2800e+00, -6.1833e+08,  6.5696e+08,  6.2800e+00,
         -4.7743e+08,  1.0988e+09]], dtype=torch.float64)
	q_value: tensor([[1.0304]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4098282621675353, distance: 1.358750758332986 entropy 9.163006027321623
epoch: 4, step: 90
	action: tensor([[ 1.6900e+08, -6.2800e+00,  3.2091e+08,  3.8252e+08,  6.2800e+00,
         -7.2122e+08,  2.3686e+08]], dtype=torch.float64)
	q_value: tensor([[1.1985]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42213363031429574, distance: 0.8699021484902326 entropy 9.08778746410942
epoch: 4, step: 91
	action: tensor([[ 3.7744e+08, -6.2800e+00, -5.0527e+08,  1.1288e+09,  6.2800e+00,
          3.4125e+08, -8.7957e+07]], dtype=torch.float64)
	q_value: tensor([[1.2677]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36284638508950495, distance: 0.9134373568702941 entropy 9.105310700307802
epoch: 4, step: 92
	action: tensor([[ 4.6342e+08, -6.2800e+00, -8.8795e+07,  2.7790e+08,  6.2800e+00,
         -2.8154e+08,  6.5786e+07]], dtype=torch.float64)
	q_value: tensor([[0.9846]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15957076709649065, distance: 1.049076765011737 entropy 9.01019396350361
epoch: 4, step: 93
	action: tensor([[-2.8220e+07, -6.2800e+00,  1.5193e+08, -9.7841e+07,  6.2800e+00,
         -2.7228e+08, -6.7729e+08]], dtype=torch.float64)
	q_value: tensor([[1.3902]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1373526514382174, distance: 1.0628533339570205 entropy 9.108482569661643
epoch: 4, step: 94
	action: tensor([[ 1.5098e+08, -6.2800e+00,  8.8167e+08, -6.7273e+08,  6.2800e+00,
          5.6366e+08, -1.0361e+09]], dtype=torch.float64)
	q_value: tensor([[1.7456]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1850580795543606, distance: 1.0330468611578627 entropy 9.316630031751236
epoch: 4, step: 95
	action: tensor([[ 8.5626e+08, -6.2800e+00,  1.0486e+08,  8.4272e+07,  6.2800e+00,
          5.4789e+08,  1.1792e+08]], dtype=torch.float64)
	q_value: tensor([[0.9938]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42223975557400395, distance: 0.8698222659817125 entropy 9.051686747500572
epoch: 4, step: 96
	action: tensor([[ 1.0228e+08, -6.2800e+00, -1.2080e+09,  6.8362e+07,  6.2800e+00,
          2.7424e+08, -3.1022e+08]], dtype=torch.float64)
	q_value: tensor([[0.9166]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39127242028626885, distance: 0.8928287831952456 entropy 9.070443947454619
epoch: 4, step: 97
	action: tensor([[ 1.4985e+08, -6.2800e+00, -7.8583e+08,  2.2668e+08,  6.2800e+00,
          2.7400e+08,  2.5209e+08]], dtype=torch.float64)
	q_value: tensor([[0.9609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3326278904057549, distance: 0.9348473897157901 entropy 9.062301853359044
epoch: 4, step: 98
	action: tensor([[-1.1029e+08, -6.2800e+00,  9.2420e+07,  2.5146e+08,  6.2800e+00,
         -2.7513e+08, -1.2455e+08]], dtype=torch.float64)
	q_value: tensor([[0.9038]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 8.9430541616137
epoch: 4, step: 99
	action: tensor([[ 4.1325e+08, -6.2800e+00, -5.7164e+07,  1.5471e+08,  6.2800e+00,
         -9.6557e+08,  2.9758e+08]], dtype=torch.float64)
	q_value: tensor([[1.4732]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.48132335147265504, distance: 0.8241476815795563 entropy 8.97846684042968
epoch: 4, step: 100
	action: tensor([[-6.9788e+08, -6.2800e+00,  2.7408e+08,  6.6622e+08,  6.2800e+00,
          3.2859e+08, -1.9647e+08]], dtype=torch.float64)
	q_value: tensor([[1.2728]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07106082992311713, distance: 1.1029360478488344 entropy 9.022843147125815
epoch: 4, step: 101
	action: tensor([[ 2.1295e+08, -6.2800e+00, -5.2373e+07, -7.2248e+08,  6.2800e+00,
         -1.6840e+08,  6.2630e+08]], dtype=torch.float64)
	q_value: tensor([[0.8522]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.40131022531653926, distance: 0.8854368931772875 entropy 8.970806441640718
epoch: 4, step: 102
	action: tensor([[ 2.1428e+08, -6.2800e+00, -5.5993e+07,  2.9599e+07,  6.2800e+00,
          1.4179e+08, -9.7351e+06]], dtype=torch.float64)
	q_value: tensor([[1.2729]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.40496070177666044, distance: 0.8827333153886415 entropy 9.206185775899085
epoch: 4, step: 103
	action: tensor([[ 4.2394e+08, -6.2800e+00, -3.6425e+08,  4.5094e+07,  6.2800e+00,
         -1.5417e+08,  2.8350e+08]], dtype=torch.float64)
	q_value: tensor([[0.8977]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18784810740827984, distance: 1.0312769800348518 entropy 9.013712549656734
epoch: 4, step: 104
	action: tensor([[-4.1008e+07, -6.2800e+00,  1.4685e+08, -7.2588e+08,  6.2800e+00,
         -6.9786e+07, -8.2437e+08]], dtype=torch.float64)
	q_value: tensor([[1.7557]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13766428910002115, distance: 1.0626613348462794 entropy 9.223677379097486
epoch: 4, step: 105
	action: tensor([[-1.7662e+08, -6.2800e+00,  8.5450e+08, -5.4330e+08,  6.2800e+00,
          3.8059e+08, -1.4543e+08]], dtype=torch.float64)
	q_value: tensor([[1.6583]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8792593081253521, distance: 1.568736669141372 entropy 9.269242102751543
epoch: 4, step: 106
	action: tensor([[-1.0874e+08, -6.2800e+00,  1.1028e+09,  2.9577e+08,  6.2800e+00,
          1.4777e+08,  1.7537e+08]], dtype=torch.float64)
	q_value: tensor([[1.3614]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09638910285887359, distance: 1.087795913827529 entropy 9.192596398827678
epoch: 4, step: 107
	action: tensor([[-5.3180e+07, -6.2800e+00,  7.7279e+07,  4.6858e+08,  6.2800e+00,
         -1.9957e+08,  9.9556e+07]], dtype=torch.float64)
	q_value: tensor([[0.9256]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.355470282215838, distance: 1.332298974083575 entropy 8.948276301082883
epoch: 4, step: 108
	action: tensor([[-1.1359e+09, -6.2800e+00, -1.1683e+08, -1.1742e+08,  6.2800e+00,
          1.8857e+07, -5.2247e+08]], dtype=torch.float64)
	q_value: tensor([[1.4875]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5482896927418253, distance: 1.4239108673178733 entropy 9.126124208945312
epoch: 4, step: 109
	action: tensor([[ 3.8743e+08, -6.2800e+00, -2.3641e+08,  4.3048e+08,  6.2800e+00,
         -2.2350e+08,  3.4825e+08]], dtype=torch.float64)
	q_value: tensor([[1.5640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3228847549404791, distance: 0.9416467010300286 entropy 9.106593899426853
epoch: 4, step: 110
	action: tensor([[ 7.4549e+08, -6.2800e+00,  4.2187e+08, -1.4627e+08,  6.2800e+00,
         -2.9795e+08,  6.8781e+08]], dtype=torch.float64)
	q_value: tensor([[1.4148]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4235045056732393, distance: 0.8688696991364122 entropy 9.168281347141171
epoch: 4, step: 111
	action: tensor([[ 2.4282e+08, -6.2800e+00,  5.7585e+08,  1.8353e+08,  6.2800e+00,
          1.2013e+08, -3.5578e+08]], dtype=torch.float64)
	q_value: tensor([[1.1559]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4483420667881892, distance: 0.849946573242839 entropy 9.15398227611256
epoch: 4, step: 112
	action: tensor([[ 4.6377e+08, -6.2800e+00, -3.2866e+07, -6.9692e+08,  6.2800e+00,
          2.0577e+08,  1.1057e+08]], dtype=torch.float64)
	q_value: tensor([[0.8462]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6945777226607298, distance: 0.6324221158144043 entropy 9.01978801488847
epoch: 4, step: 113
	action: tensor([[-8.2657e+08, -6.2800e+00,  3.5261e+08,  3.1806e+08,  6.2800e+00,
          4.7526e+07,  1.6909e+08]], dtype=torch.float64)
	q_value: tensor([[0.9796]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10862787783505612, distance: 1.0804040833746076 entropy 9.145621005412977
epoch: 4, step: 114
	action: tensor([[ 8.9718e+07, -6.2800e+00,  5.8563e+08, -1.5458e+08,  6.2800e+00,
         -4.8201e+08,  5.5877e+07]], dtype=torch.float64)
	q_value: tensor([[0.8355]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4149140245578913, distance: 0.8753193664645883 entropy 9.002287605612526
epoch: 4, step: 115
	action: tensor([[-3.0994e+08, -6.2800e+00, -1.0922e+09,  1.4615e+08,  6.2800e+00,
         -3.4729e+08, -6.0941e+08]], dtype=torch.float64)
	q_value: tensor([[1.6013]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.46943145783132945, distance: 1.3871753403442815 entropy 9.259247930428822
epoch: 4, step: 116
	action: tensor([[-1.7464e+08, -6.2800e+00, -5.2325e+08, -8.4114e+08,  6.2800e+00,
          5.7381e+08,  4.9733e+07]], dtype=torch.float64)
	q_value: tensor([[1.3769]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7527984213891548, distance: 1.51503504901443 entropy 9.242078153425172
epoch: 4, step: 117
	action: tensor([[ 3.6685e+08, -6.2800e+00, -1.7069e+08,  1.0864e+09,  6.2800e+00,
         -7.7540e+08, -6.4558e+07]], dtype=torch.float64)
	q_value: tensor([[1.7266]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6835095547544582, distance: 0.6437792801521233 entropy 9.219481489635395
epoch: 4, step: 118
	action: tensor([[ 2.2438e+08, -6.2800e+00,  1.9365e+08,  2.3107e+08,  6.2800e+00,
          6.2654e+08,  2.4611e+08]], dtype=torch.float64)
	q_value: tensor([[1.4382]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4571925200379393, distance: 0.8431010013318133 entropy 8.923809502533105
epoch: 4, step: 119
	action: tensor([[ 1.0493e+08, -6.2800e+00, -1.0783e+08, -1.1406e+07,  6.2800e+00,
          7.0740e+08,  4.6484e+07]], dtype=torch.float64)
	q_value: tensor([[0.8485]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7005548321108547, distance: 0.6262032941947837 entropy 9.039112966334882
epoch: 4, step: 120
	action: tensor([[ 1.3763e+08, -6.2800e+00, -8.3248e+07, -2.5771e+08,  6.2800e+00,
          5.7998e+08, -6.6905e+08]], dtype=torch.float64)
	q_value: tensor([[1.0477]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5834568086235992, distance: 0.7385615824233553 entropy 9.125639725154047
epoch: 4, step: 121
	action: tensor([[ 3.0756e+08, -6.2800e+00, -5.3969e+08,  7.7754e+08,  6.2800e+00,
          4.4477e+08,  6.0926e+08]], dtype=torch.float64)
	q_value: tensor([[1.5586]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4201752255541096, distance: 0.8713749626515122 entropy 9.277624285705793
epoch: 4, step: 122
	action: tensor([[ 9.2714e+07, -6.2800e+00, -3.8797e+07, -4.5516e+08,  6.2800e+00,
         -3.8983e+08, -8.1950e+07]], dtype=torch.float64)
	q_value: tensor([[0.8654]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3791091392991385, distance: 0.9017046865221254 entropy 9.005102487054433
epoch: 4, step: 123
	action: tensor([[-4.4843e+08, -6.2800e+00,  7.9004e+08,  1.0247e+08,  6.2800e+00,
         -1.5004e+08,  7.6705e+08]], dtype=torch.float64)
	q_value: tensor([[1.1850]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6188702985509495, distance: 1.45600451207795 entropy 9.079332174232063
epoch: 4, step: 124
	action: tensor([[-1.0639e+09, -6.2800e+00, -4.3588e+08,  6.2418e+08,  6.2800e+00,
          3.2906e+08, -2.6679e+07]], dtype=torch.float64)
	q_value: tensor([[1.2105]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1456934515668563, distance: 1.0577025732503504 entropy 9.084026695317604
epoch: 4, step: 125
	action: tensor([[ 9.3695e+08, -6.2800e+00,  2.4049e+08,  6.3400e+08,  6.2800e+00,
          5.6027e+08, -3.1719e+08]], dtype=torch.float64)
	q_value: tensor([[1.2334]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4608811803945285, distance: 0.8402314621722771 entropy 9.145959022817394
epoch: 4, step: 126
	action: tensor([[ 4.4311e+08, -6.2800e+00,  3.2642e+07,  7.0981e+08,  6.2800e+00,
          2.4143e+08,  8.9713e+08]], dtype=torch.float64)
	q_value: tensor([[0.9532]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3800315931360866, distance: 0.9010346089181145 entropy 9.094401336043983
epoch: 4, step: 127
	action: tensor([[ 1.2633e+08, -6.2800e+00, -3.7782e+08, -3.5192e+08,  6.2800e+00,
         -1.0202e+08, -2.6552e+08]], dtype=torch.float64)
	q_value: tensor([[1.0181]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4571461228975312, distance: 0.8431370331120606 entropy 9.121026058704466
LOSS epoch 4 actor 72.20417685938585 critic 579.621646173497 entropy 0.1
epoch: 5, step: 0
	action: tensor([[-2.0584e+09, -6.2800e+00,  5.1567e+08,  3.1127e+07,  6.2800e+00,
         -3.3101e+09, -3.8882e+08]], dtype=torch.float64)
	q_value: tensor([[-0.0889]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4509251515968047, distance: 1.378412484316466 entropy 9.991867238453878
epoch: 5, step: 1
	action: tensor([[-4.2519e+08, -6.2800e+00, -1.7181e+09,  3.6967e+08,  6.2800e+00,
          1.3929e+09, -4.9161e+08]], dtype=torch.float64)
	q_value: tensor([[-0.0575]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05899161268890141, distance: 1.1100778587639581 entropy 9.877170029251984
epoch: 5, step: 2
	action: tensor([[-1.0475e+09, -6.2800e+00,  4.9687e+08,  1.0538e+09,  6.2800e+00,
          9.5948e+08, -3.1622e+08]], dtype=torch.float64)
	q_value: tensor([[-0.1902]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09362646322494117, distance: 1.0894575220783238 entropy 9.759091697573016
epoch: 5, step: 3
	action: tensor([[ 1.7469e+09, -6.2800e+00,  2.4032e+07,  2.3270e+09,  6.2800e+00,
         -1.4474e+08,  1.9057e+08]], dtype=torch.float64)
	q_value: tensor([[-0.0029]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2990260429091024, distance: 0.9580929287533029 entropy 9.705979122223892
epoch: 5, step: 4
	action: tensor([[-8.5774e+08, -6.2800e+00,  8.4712e+08, -5.4583e+08,  6.2800e+00,
          5.6475e+08, -3.8849e+08]], dtype=torch.float64)
	q_value: tensor([[0.0508]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7740972007276274, distance: 1.5242120776300414 entropy 9.875915107602141
epoch: 5, step: 5
	action: tensor([[ 8.3884e+07, -6.2800e+00,  2.0224e+08, -2.1312e+08,  6.2800e+00,
         -1.0329e+09, -2.6858e+09]], dtype=torch.float64)
	q_value: tensor([[0.1224]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.40799542531815614, distance: 0.8804794509124046 entropy 9.849321968005068
epoch: 5, step: 6
	action: tensor([[-1.3014e+09, -6.2800e+00, -1.6701e+09,  2.2778e+09,  6.2800e+00,
         -1.7488e+09, -2.3055e+09]], dtype=torch.float64)
	q_value: tensor([[-0.1721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7929199919651884, distance: 1.532276525359545 entropy 9.93085128040705
epoch: 5, step: 7
	action: tensor([[-1.3363e+08, -6.2800e+00,  2.7596e+08, -1.6665e+08,  6.2800e+00,
          3.2440e+09, -4.8505e+08]], dtype=torch.float64)
	q_value: tensor([[0.1165]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.36239552254539653, distance: 1.3356980658742383 entropy 9.903722900493559
epoch: 5, step: 8
	action: tensor([[-1.0365e+09, -6.2800e+00,  7.8512e+08,  1.7889e+09,  6.2800e+00,
         -3.2452e+08, -1.5235e+09]], dtype=torch.float64)
	q_value: tensor([[0.4548]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8674695631108518, distance: 1.5638081038766982 entropy 10.058949340463554
epoch: 5, step: 9
	action: tensor([[-9.2862e+08, -6.2800e+00,  1.4696e+08, -2.0573e+09,  6.2800e+00,
         -1.2438e+09, -1.5884e+09]], dtype=torch.float64)
	q_value: tensor([[0.1679]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.121710598710709, distance: 1.0724461988313583 entropy 9.938986153460181
epoch: 5, step: 10
	action: tensor([[ 1.7286e+09, -6.2800e+00, -2.6686e+08, -1.3041e+09,  6.2800e+00,
         -1.3660e+09,  2.0491e+09]], dtype=torch.float64)
	q_value: tensor([[0.0145]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44673797820581307, distance: 0.8511813963531623 entropy 10.027188882558127
epoch: 5, step: 11
	action: tensor([[-4.7887e+08, -6.2800e+00,  1.7865e+09,  1.9024e+09,  6.2800e+00,
         -7.8229e+08, -3.2965e+07]], dtype=torch.float64)
	q_value: tensor([[-0.0937]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7827378180324776, distance: 1.5279193536819375 entropy 9.960644751785333
epoch: 5, step: 12
	action: tensor([[ 5.2258e+08, -6.2800e+00,  1.7866e+09, -1.2162e+08,  6.2800e+00,
         -4.8304e+08,  2.9756e+08]], dtype=torch.float64)
	q_value: tensor([[0.0805]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41277655180211725, distance: 0.876916794623604 entropy 9.808626279309083
epoch: 5, step: 13
	action: tensor([[-1.7421e+09, -6.2800e+00, -8.5799e+08, -1.1507e+08,  6.2800e+00,
          1.7457e+09,  8.4081e+08]], dtype=torch.float64)
	q_value: tensor([[-0.1708]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3241572542806297, distance: 1.3168201858645125 entropy 9.947660956004771
epoch: 5, step: 14
	action: tensor([[ 2.8879e+09, -6.2800e+00,  1.2930e+09, -7.6281e+08,  6.2800e+00,
          1.1031e+09,  1.1093e+09]], dtype=torch.float64)
	q_value: tensor([[0.0436]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6880620536283542, distance: 0.6391323462768498 entropy 10.044841114538873
epoch: 5, step: 15
	action: tensor([[-1.0133e+09, -6.2800e+00, -1.8278e+09, -2.1352e+09,  6.2800e+00,
         -2.2348e+09, -4.2534e+09]], dtype=torch.float64)
	q_value: tensor([[-0.0362]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06720451914260372, distance: 1.105222989392823 entropy 9.948100996760742
epoch: 5, step: 16
	action: tensor([[-1.3657e+09, -6.2800e+00, -2.4022e+09,  5.3768e+08,  6.2800e+00,
          2.4975e+09, -7.0305e+07]], dtype=torch.float64)
	q_value: tensor([[0.1984]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10520487188955474, distance: 1.0824765544937698 entropy 10.0156050255568
epoch: 5, step: 17
	action: tensor([[ 2.0410e+09, -6.2800e+00,  1.4395e+09, -1.4983e+09,  6.2800e+00,
         -5.5475e+08, -5.4605e+07]], dtype=torch.float64)
	q_value: tensor([[0.0496]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36549797588700517, distance: 0.9115346859797164 entropy 9.787844436082505
epoch: 5, step: 18
	action: tensor([[ 1.6701e+09, -6.2800e+00,  1.7459e+08,  4.5871e+07,  6.2800e+00,
         -2.5440e+09,  1.2004e+09]], dtype=torch.float64)
	q_value: tensor([[-0.0875]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7526818284510004, distance: 0.5690949172217209 entropy 10.041405685894384
epoch: 5, step: 19
	action: tensor([[-1.2188e+09, -6.2800e+00, -1.7026e+09, -1.0570e+09,  6.2800e+00,
         -2.4522e+09, -7.7472e+08]], dtype=torch.float64)
	q_value: tensor([[0.0076]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07893531430620093, distance: 1.0982513834069003 entropy 9.879065560968368
epoch: 5, step: 20
	action: tensor([[-1.3371e+09, -6.2800e+00, -1.2401e+09,  9.5063e+08,  6.2800e+00,
          4.7597e+09, -1.2039e+09]], dtype=torch.float64)
	q_value: tensor([[0.0771]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09001544133922057, distance: 1.0916255820620857 entropy 10.013309224574554
epoch: 5, step: 21
	action: tensor([[ 1.2376e+09, -6.2800e+00,  2.3756e+08, -1.2777e+09,  6.2800e+00,
          3.7262e+08, -1.8557e+09]], dtype=torch.float64)
	q_value: tensor([[-0.1360]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.48860435301258087, distance: 0.8183426888870647 entropy 9.917641656808751
epoch: 5, step: 22
	action: tensor([[-5.1642e+09, -6.2800e+00, -3.3675e+08,  1.5395e+09,  6.2800e+00,
          1.2018e+08, -3.4847e+08]], dtype=torch.float64)
	q_value: tensor([[0.0946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05944888430373174, distance: 1.1098081115527347 entropy 9.98135603546238
epoch: 5, step: 23
	action: tensor([[ 6.3558e+08, -6.2800e+00,  6.3529e+08,  1.0441e+09,  6.2800e+00,
          1.1318e+09,  9.7135e+08]], dtype=torch.float64)
	q_value: tensor([[-0.0736]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35152130836460305, distance: 0.9215195397349714 entropy 9.7173255964373
epoch: 5, step: 24
	action: tensor([[-1.6770e+09, -6.2800e+00,  5.6187e+08, -2.8339e+09,  6.2800e+00,
         -7.0827e+08,  5.1580e+07]], dtype=torch.float64)
	q_value: tensor([[-0.1024]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07512779590729102, distance: 1.1005190307348558 entropy 9.609983526143612
epoch: 5, step: 25
	action: tensor([[-1.6201e+08, -6.2800e+00, -5.9221e+08, -3.0473e+09,  6.2800e+00,
          4.7010e+07, -1.0949e+09]], dtype=torch.float64)
	q_value: tensor([[0.1999]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.44000324765828136, distance: 1.3732146533154157 entropy 9.985259881384348
epoch: 5, step: 26
	action: tensor([[ 1.0243e+09, -6.2800e+00,  3.2723e+09, -2.4011e+09,  6.2800e+00,
          1.4693e+08, -5.1229e+08]], dtype=torch.float64)
	q_value: tensor([[0.4403]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.831509571232404, distance: 0.46972568778897694 entropy 10.049518765647132
epoch: 5, step: 27
	action: tensor([[-1.0546e+08, -6.2800e+00,  1.2438e+08,  5.0569e+08,  6.2800e+00,
          1.0112e+09, -2.2622e+09]], dtype=torch.float64)
	q_value: tensor([[-0.2534]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0415324476572394, distance: 1.1203285429552885 entropy 9.863563704337768
epoch: 5, step: 28
	action: tensor([[-9.6012e+07, -6.2800e+00, -1.2632e+09, -9.0906e+08,  6.2800e+00,
         -1.0610e+09,  8.5562e+08]], dtype=torch.float64)
	q_value: tensor([[-0.0281]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04269250821160653, distance: 1.119650354926844 entropy 9.681592953841163
epoch: 5, step: 29
	action: tensor([[-3.9447e+09, -6.2800e+00, -1.3025e+09, -6.8034e+08,  6.2800e+00,
          6.3193e+08,  1.1597e+09]], dtype=torch.float64)
	q_value: tensor([[0.2219]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5951536729500395, distance: 1.4452998492296782 entropy 10.01058434940329
epoch: 5, step: 30
	action: tensor([[ 4.1596e+08, -6.2800e+00,  1.0933e+09,  8.1333e+08,  6.2800e+00,
         -1.6644e+07,  1.5901e+08]], dtype=torch.float64)
	q_value: tensor([[-0.1529]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23463827125855918, distance: 1.0011290514753155 entropy 10.015270930782945
epoch: 5, step: 31
	action: tensor([[ 1.4395e+09, -6.2800e+00,  7.6435e+08,  1.3035e+09,  6.2800e+00,
          2.2184e+09, -3.9523e+08]], dtype=torch.float64)
	q_value: tensor([[-0.0838]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.43410657273969366, distance: 0.860843130439637 entropy 9.855659836294308
epoch: 5, step: 32
	action: tensor([[-7.5011e+08, -6.2800e+00,  1.6613e+08,  1.5113e+08,  6.2800e+00,
         -1.3686e+07, -1.3486e+09]], dtype=torch.float64)
	q_value: tensor([[-0.2482]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6013852223825411, distance: 1.4481201663879566 entropy 9.867540939921236
epoch: 5, step: 33
	action: tensor([[-7.6997e+08, -6.2800e+00,  1.0547e+09, -1.4723e+08,  6.2800e+00,
         -1.2032e+09,  2.3051e+09]], dtype=torch.float64)
	q_value: tensor([[-0.0135]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0910044791408735, distance: 1.0910321915049297 entropy 9.897872444978569
epoch: 5, step: 34
	action: tensor([[ 9.3380e+07, -6.2800e+00, -2.1505e+09,  2.8414e+09,  6.2800e+00,
         -6.5135e+08,  1.0760e+09]], dtype=torch.float64)
	q_value: tensor([[0.1081]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6874200386400995, distance: 0.6397897232264943 entropy 10.009661449212105
epoch: 5, step: 35
	action: tensor([[-3.5223e+08, -6.2800e+00,  1.5494e+09,  9.8949e+08,  6.2800e+00,
          1.8114e+09, -1.4765e+08]], dtype=torch.float64)
	q_value: tensor([[0.0745]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07507244297378357, distance: 1.1005519628767402 entropy 9.786334830917209
epoch: 5, step: 36
	action: tensor([[-1.6274e+09, -6.2800e+00, -6.1026e+07,  8.9829e+08,  6.2800e+00,
          3.0967e+09,  6.7518e+08]], dtype=torch.float64)
	q_value: tensor([[-0.1764]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0472419063901115, distance: 1.1169867377128055 entropy 9.853146486693955
epoch: 5, step: 37
	action: tensor([[ 1.0063e+09, -6.2800e+00, -3.5524e+08,  1.4347e+09,  6.2800e+00,
          1.2666e+09,  9.9545e+08]], dtype=torch.float64)
	q_value: tensor([[-0.1442]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4531175487039233, distance: 0.8462597534312056 entropy 9.859621663586685
epoch: 5, step: 38
	action: tensor([[ 5.5961e+08, -6.2800e+00, -2.3441e+08, -2.5252e+08,  6.2800e+00,
         -9.7919e+08, -1.0950e+09]], dtype=torch.float64)
	q_value: tensor([[-0.2066]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3866182551965054, distance: 0.8962354467014744 entropy 9.7878822454875
epoch: 5, step: 39
	action: tensor([[ 1.5576e+09, -6.2800e+00,  1.0600e+09, -4.5374e+08,  6.2800e+00,
          1.9441e+09,  5.0059e+09]], dtype=torch.float64)
	q_value: tensor([[-0.0877]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23986151825640933, distance: 0.9977070770516333 entropy 9.950623309217672
epoch: 5, step: 40
	action: tensor([[ 7.2104e+08, -6.2800e+00, -1.1117e+09, -4.0094e+08,  6.2800e+00,
         -9.3655e+08,  7.7381e+08]], dtype=torch.float64)
	q_value: tensor([[0.0415]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36069094212354713, distance: 0.9149810974706386 entropy 9.761863446353031
epoch: 5, step: 41
	action: tensor([[ 1.6720e+09, -6.2800e+00,  1.6834e+09,  3.4855e+08,  6.2800e+00,
          1.1696e+09,  7.8252e+08]], dtype=torch.float64)
	q_value: tensor([[-0.1068]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3685562892182519, distance: 0.9093352240706831 entropy 9.856290606854776
epoch: 5, step: 42
	action: tensor([[ 1.4027e+09, -6.2800e+00,  9.3442e+08, -1.0218e+09,  6.2800e+00,
         -1.0646e+09,  9.6688e+08]], dtype=torch.float64)
	q_value: tensor([[-0.0759]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34486652172400456, distance: 0.9262358573357876 entropy 9.593780902699438
epoch: 5, step: 43
	action: tensor([[ 1.4626e+09, -6.2800e+00,  1.9274e+09, -1.2645e+09,  6.2800e+00,
         -4.1265e+07, -1.5176e+09]], dtype=torch.float64)
	q_value: tensor([[-0.2579]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39756020157666305, distance: 0.8882056276736345 entropy 10.05817381384233
epoch: 5, step: 44
	action: tensor([[-2.0097e+09, -6.2800e+00, -1.6167e+09, -1.0910e+09,  6.2800e+00,
         -1.1215e+09,  1.6185e+09]], dtype=torch.float64)
	q_value: tensor([[-0.0839]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.006218781796403561, distance: 1.1407804911855925 entropy 9.95833311906062
epoch: 5, step: 45
	action: tensor([[ 2.0775e+09, -6.2800e+00, -8.6732e+08, -2.2788e+09,  6.2800e+00,
         -3.8394e+08,  2.9701e+08]], dtype=torch.float64)
	q_value: tensor([[0.1248]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39617026133817834, distance: 0.8892296648305215 entropy 10.108700062098206
epoch: 5, step: 46
	action: tensor([[ 9.0603e+08, -6.2800e+00,  3.4221e+09,  1.0288e+09,  6.2800e+00,
         -2.6364e+09,  3.2895e+09]], dtype=torch.float64)
	q_value: tensor([[-0.1529]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2767277849496045, distance: 0.9732122887590937 entropy 9.868884840431074
epoch: 5, step: 47
	action: tensor([[ 1.2125e+09, -6.2800e+00, -1.1995e+09,  2.6543e+09,  6.2800e+00,
          7.2740e+07, -1.8923e+09]], dtype=torch.float64)
	q_value: tensor([[-0.0790]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4250718467439367, distance: 0.8676877800724278 entropy 9.883743508626534
epoch: 5, step: 48
	action: tensor([[ 1.9226e+09, -6.2800e+00,  6.7513e+08, -4.2940e+08,  6.2800e+00,
          1.5606e+08, -9.2052e+07]], dtype=torch.float64)
	q_value: tensor([[-0.1170]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7893100047532453, distance: 0.5252652256024104 entropy 9.621379975364698
epoch: 5, step: 49
	action: tensor([[ 1.6588e+08, -6.2800e+00,  1.1047e+09, -3.7314e+08,  6.2800e+00,
         -1.4690e+08, -1.7519e+09]], dtype=torch.float64)
	q_value: tensor([[-0.2600]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4439588125020826, distance: 0.853316560868324 entropy 9.855642690012829
epoch: 5, step: 50
	action: tensor([[ 1.0836e+09, -6.2800e+00, -1.2882e+09, -1.8859e+08,  6.2800e+00,
         -1.5485e+09,  3.7023e+08]], dtype=torch.float64)
	q_value: tensor([[-0.0873]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42005966462500854, distance: 0.8714617922316833 entropy 9.86938712238769
epoch: 5, step: 51
	action: tensor([[ 5.7979e+08, -6.2800e+00, -5.5001e+08, -4.5612e+09,  6.2800e+00,
         -1.5831e+09,  2.0153e+09]], dtype=torch.float64)
	q_value: tensor([[-0.0851]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3506347354218864, distance: 0.9221492560627884 entropy 10.027890844396897
epoch: 5, step: 52
	action: tensor([[ 4.8483e+08, -6.2800e+00, -1.5520e+09,  6.5797e+07,  6.2800e+00,
         -4.8515e+08,  5.3820e+08]], dtype=torch.float64)
	q_value: tensor([[-0.1462]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7213716998884478, distance: 0.604045016910229 entropy 9.888864456845528
epoch: 5, step: 53
	action: tensor([[ 1.4468e+09, -6.2800e+00, -1.0282e+09, -1.5633e+09,  6.2800e+00,
          1.1173e+07,  1.9962e+09]], dtype=torch.float64)
	q_value: tensor([[0.0905]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38298469269009716, distance: 0.8988860953436513 entropy 9.69293244744969
epoch: 5, step: 54
	action: tensor([[-2.1759e+08, -6.2800e+00, -2.1273e+09, -1.8814e+07,  6.2800e+00,
          7.2815e+08,  3.2795e+08]], dtype=torch.float64)
	q_value: tensor([[0.0127]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6817079007173861, distance: 1.4839934208016519 entropy 9.813911777885588
epoch: 5, step: 55
	action: tensor([[-1.8015e+09, -6.2800e+00, -1.3414e+09,  5.9556e+08,  6.2800e+00,
         -1.2778e+09, -5.3939e+08]], dtype=torch.float64)
	q_value: tensor([[0.3041]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9809398784994185, distance: 1.6106172298624648 entropy 9.824291713551844
epoch: 5, step: 56
	action: tensor([[-1.1710e+09, -6.2800e+00, -2.0260e+09, -4.5026e+08,  6.2800e+00,
          2.8905e+08,  1.4102e+09]], dtype=torch.float64)
	q_value: tensor([[0.1686]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6561661533838006, distance: 1.4726808709559083 entropy 9.936876879741106
epoch: 5, step: 57
	action: tensor([[-1.2594e+09, -6.2800e+00,  1.8679e+09,  8.6979e+08,  6.2800e+00,
         -2.4570e+08,  7.6411e+08]], dtype=torch.float64)
	q_value: tensor([[0.0526]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4952142146236642, distance: 1.39929216312314 entropy 9.923875416256298
epoch: 5, step: 58
	action: tensor([[-7.7517e+08, -6.2800e+00,  1.7061e+09,  2.8700e+08,  6.2800e+00,
         -4.2867e+08, -6.6439e+08]], dtype=torch.float64)
	q_value: tensor([[0.0273]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.40669232407740563, distance: 1.3572387548972278 entropy 9.850833895012135
epoch: 5, step: 59
	action: tensor([[ 1.0703e+08, -6.2800e+00, -4.9667e+08,  2.5579e+09,  6.2800e+00,
          1.5790e+09, -9.2004e+08]], dtype=torch.float64)
	q_value: tensor([[0.0275]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.335174599704088, distance: 0.9330619838428443 entropy 9.823964844902012
epoch: 5, step: 60
	action: tensor([[-6.6848e+08, -6.2800e+00, -4.2357e+08, -1.7180e+09,  6.2800e+00,
          5.9877e+08, -9.6668e+08]], dtype=torch.float64)
	q_value: tensor([[-0.1012]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4257528228902372, distance: 1.366403020375076 entropy 9.613443109292191
epoch: 5, step: 61
	action: tensor([[ 2.7140e+08, -6.2800e+00, -6.1484e+08, -2.0105e+09,  6.2800e+00,
          9.2974e+08, -2.4119e+09]], dtype=torch.float64)
	q_value: tensor([[0.3777]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22563623991705284, distance: 1.0069993802097503 entropy 9.908829038478647
epoch: 5, step: 62
	action: tensor([[-2.8026e+09, -6.2800e+00, -1.9349e+09,  3.2318e+08,  6.2800e+00,
          1.0180e+09, -1.0489e+09]], dtype=torch.float64)
	q_value: tensor([[-0.0826]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07493031745939804, distance: 1.1006365157178337 entropy 9.731113298939729
epoch: 5, step: 63
	action: tensor([[ 1.0140e+08, -6.2800e+00, -1.6916e+09,  1.3347e+08,  6.2800e+00,
          8.6305e+08,  2.2138e+09]], dtype=torch.float64)
	q_value: tensor([[-0.0143]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35216857320047446, distance: 0.9210595276225194 entropy 9.919319116464566
epoch: 5, step: 64
	action: tensor([[ 2.2793e+08, -6.2800e+00, -9.9231e+08, -1.1780e+09,  6.2800e+00,
         -1.5067e+09,  2.6622e+08]], dtype=torch.float64)
	q_value: tensor([[-0.1204]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35561992564651945, distance: 0.9186027571000078 entropy 9.8106860875038
epoch: 5, step: 65
	action: tensor([[ 1.5459e+09, -6.2800e+00, -2.5990e+09, -3.6140e+08,  6.2800e+00,
          2.1834e+09,  2.4495e+08]], dtype=torch.float64)
	q_value: tensor([[-0.1423]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23212717453288945, distance: 1.0027700227684682 entropy 9.865281609247898
epoch: 5, step: 66
	action: tensor([[-2.4947e+09, -6.2800e+00, -7.8140e+07,  9.1041e+08,  6.2800e+00,
         -3.8107e+08, -2.9943e+08]], dtype=torch.float64)
	q_value: tensor([[-0.3066]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0177639994780674, distance: 1.6255183551476642 entropy 9.907406282511973
epoch: 5, step: 67
	action: tensor([[ 1.7089e+09, -6.2800e+00, -2.8403e+09,  1.5132e+09,  6.2800e+00,
          4.1981e+08,  3.4142e+08]], dtype=torch.float64)
	q_value: tensor([[-0.0086]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3375128448434801, distance: 0.9314197110318932 entropy 10.006320343523518
epoch: 5, step: 68
	action: tensor([[-4.3475e+08, -6.2800e+00,  7.5899e+08, -4.6588e+08,  6.2800e+00,
         -7.6980e+08, -9.6493e+07]], dtype=torch.float64)
	q_value: tensor([[-0.1969]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.813277684996422
epoch: 5, step: 69
	action: tensor([[ 2.3139e+08, -6.2800e+00, -1.5516e+09, -8.2584e+08,  6.2800e+00,
         -2.2831e+08, -7.1820e+08]], dtype=torch.float64)
	q_value: tensor([[0.5094]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.71998308840981
epoch: 5, step: 70
	action: tensor([[-2.1570e+09, -6.2800e+00, -3.4709e+09,  1.0961e+09,  6.2800e+00,
         -1.1031e+09,  4.7097e+08]], dtype=torch.float64)
	q_value: tensor([[0.5094]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.323891731819554, distance: 1.316688153636069 entropy 9.71998308840981
epoch: 5, step: 71
	action: tensor([[-2.8228e+08, -6.2800e+00,  4.3202e+08,  1.0230e+09,  6.2800e+00,
          1.0721e+09, -4.6136e+08]], dtype=torch.float64)
	q_value: tensor([[0.0446]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09577085939248864, distance: 1.0881679808999047 entropy 9.907986120288145
epoch: 5, step: 72
	action: tensor([[ 7.5347e+08, -6.2800e+00,  7.5520e+08,  1.2024e+09,  6.2800e+00,
          7.4795e+08,  6.0881e+08]], dtype=torch.float64)
	q_value: tensor([[0.0294]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37912555382680746, distance: 0.9016927672338186 entropy 9.748057053749479
epoch: 5, step: 73
	action: tensor([[-5.9695e+08, -6.2800e+00, -1.8941e+09, -1.1809e+09,  6.2800e+00,
          2.0965e+09,  3.6027e+08]], dtype=torch.float64)
	q_value: tensor([[-0.2209]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7836309098361045, distance: 1.5283020239150888 entropy 9.815150026797003
epoch: 5, step: 74
	action: tensor([[ 6.8768e+08, -6.2800e+00,  7.1331e+08, -7.5684e+08,  6.2800e+00,
         -4.0690e+08,  3.3110e+08]], dtype=torch.float64)
	q_value: tensor([[0.1078]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4788041452311109, distance: 0.8261466949107653 entropy 9.853593300174424
epoch: 5, step: 75
	action: tensor([[-3.2630e+08, -6.2800e+00,  2.9635e+09,  2.1279e+09,  6.2800e+00,
          3.8662e+09, -1.6146e+09]], dtype=torch.float64)
	q_value: tensor([[-0.1540]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04891822686996117, distance: 1.116003669666461 entropy 9.907565221401592
epoch: 5, step: 76
	action: tensor([[ 6.3795e+08, -6.2800e+00,  1.0413e+09, -5.8650e+08,  6.2800e+00,
         -2.4504e+08, -1.5568e+09]], dtype=torch.float64)
	q_value: tensor([[-0.0216]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.444588949511146, distance: 0.8528329107506177 entropy 9.657123266897798
epoch: 5, step: 77
	action: tensor([[-1.7147e+08, -6.2800e+00,  3.2216e+09,  2.1436e+08,  6.2800e+00,
         -9.5106e+08, -1.8818e+08]], dtype=torch.float64)
	q_value: tensor([[-0.1204]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.28775809934151897, distance: 1.2985953346747576 entropy 9.862129145614178
epoch: 5, step: 78
	action: tensor([[-2.1364e+09, -6.2800e+00, -1.8024e+09, -2.8787e+09,  6.2800e+00,
          6.2693e+08, -1.4207e+09]], dtype=torch.float64)
	q_value: tensor([[0.0008]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9049459237289579, distance: 1.5794214038233283 entropy 9.933512848445867
epoch: 5, step: 79
	action: tensor([[-7.1377e+08, -6.2800e+00, -1.6541e+08,  3.8638e+08,  6.2800e+00,
         -1.2924e+09,  1.5939e+09]], dtype=torch.float64)
	q_value: tensor([[0.3496]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8548891890973762, distance: 1.5585318369938355 entropy 9.9181470643566
epoch: 5, step: 80
	action: tensor([[ 1.8734e+09, -6.2800e+00, -2.4058e+09, -1.9615e+09,  6.2800e+00,
         -2.4269e+08, -3.2129e+09]], dtype=torch.float64)
	q_value: tensor([[0.1092]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3719817174105894, distance: 0.9068654090269898 entropy 9.951606046402775
epoch: 5, step: 81
	action: tensor([[-2.0489e+08, -6.2800e+00,  4.3266e+09,  2.5120e+08,  6.2800e+00,
         -3.0136e+09,  1.4619e+09]], dtype=torch.float64)
	q_value: tensor([[-0.1772]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7624649921528877, distance: 1.5192069654749893 entropy 9.989223398080133
epoch: 5, step: 82
	action: tensor([[-2.1947e+08, -6.2800e+00,  2.6874e+09,  1.1343e+07,  6.2800e+00,
          8.1480e+07, -1.4573e+09]], dtype=torch.float64)
	q_value: tensor([[-0.0689]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08840934393318278, distance: 1.0925885017449157 entropy 9.95461028424234
epoch: 5, step: 83
	action: tensor([[-1.8804e+09, -6.2800e+00, -6.1546e+09,  9.7602e+08,  6.2800e+00,
         -2.7551e+09, -5.2052e+08]], dtype=torch.float64)
	q_value: tensor([[0.1704]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.542807180687495, distance: 1.4213875890137653 entropy 9.911835418878693
epoch: 5, step: 84
	action: tensor([[-4.3683e+08, -6.2800e+00,  5.4147e+08,  9.1950e+08,  6.2800e+00,
          7.5241e+08,  2.6253e+09]], dtype=torch.float64)
	q_value: tensor([[-0.0081]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.008583911023768542, distance: 1.1394221939752416 entropy 9.886240098054504
epoch: 5, step: 85
	action: tensor([[-3.8493e+08, -6.2800e+00, -7.8603e+08, -1.1178e+09,  6.2800e+00,
         -8.2416e+08,  4.2289e+08]], dtype=torch.float64)
	q_value: tensor([[0.0414]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09637881168213214, distance: 1.0878021082349723 entropy 9.8262808829521
epoch: 5, step: 86
	action: tensor([[-1.7082e+09, -6.2800e+00, -1.3220e+09, -1.9469e+09,  6.2800e+00,
         -3.4973e+08, -1.3825e+09]], dtype=torch.float64)
	q_value: tensor([[0.2132]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0471490701631565, distance: 1.11704115567528 entropy 9.926352741676721
epoch: 5, step: 87
	action: tensor([[ 5.0806e+08, -6.2800e+00, -3.0872e+08, -4.3653e+07,  6.2800e+00,
          2.1622e+08, -4.5815e+08]], dtype=torch.float64)
	q_value: tensor([[0.1614]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7245092095767962, distance: 0.6006344470086313 entropy 9.985750735562155
epoch: 5, step: 88
	action: tensor([[ 5.1103e+07, -6.2800e+00, -1.0780e+09, -1.5619e+09,  6.2800e+00,
         -1.2494e+09,  2.4428e+09]], dtype=torch.float64)
	q_value: tensor([[-0.2435]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4801536531139985, distance: 0.825076450311905 entropy 9.791329476069295
epoch: 5, step: 89
	action: tensor([[-1.7014e+09, -6.2800e+00, -1.6197e+09, -1.0862e+09,  6.2800e+00,
          2.3547e+09,  3.5677e+08]], dtype=torch.float64)
	q_value: tensor([[-0.1385]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9362871561153399, distance: 1.5923611586880748 entropy 9.855558477198455
epoch: 5, step: 90
	action: tensor([[-2.4277e+09, -6.2800e+00, -1.3415e+09,  1.4512e+09,  6.2800e+00,
         -2.4645e+09,  9.9205e+06]], dtype=torch.float64)
	q_value: tensor([[0.2941]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4999982722859484, distance: 1.4015289490451504 entropy 9.807969084951408
epoch: 5, step: 91
	action: tensor([[ 1.4243e+09, -6.2800e+00,  7.8454e+08, -7.5274e+08,  6.2800e+00,
         -8.1955e+08, -6.8142e+08]], dtype=torch.float64)
	q_value: tensor([[-0.0115]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4551283932901409, distance: 0.8447025047261749 entropy 9.824058778116626
epoch: 5, step: 92
	action: tensor([[-2.1032e+09, -6.2800e+00, -8.1509e+08, -1.4946e+09,  6.2800e+00,
         -1.0590e+09,  2.5158e+09]], dtype=torch.float64)
	q_value: tensor([[-0.1437]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.027825404575376966, distance: 1.1283110133802758 entropy 9.979659379298672
epoch: 5, step: 93
	action: tensor([[ 1.6555e+09, -6.2800e+00,  6.8268e+08,  1.5059e+09,  6.2800e+00,
          2.8089e+09, -3.9852e+09]], dtype=torch.float64)
	q_value: tensor([[0.2092]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38691078901662546, distance: 0.8960217050567072 entropy 10.072423835414005
epoch: 5, step: 94
	action: tensor([[ 1.3256e+09, -6.2800e+00,  1.2349e+09, -1.1599e+09,  6.2800e+00,
         -1.2124e+09,  1.6045e+09]], dtype=torch.float64)
	q_value: tensor([[-0.0684]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3901127570666555, distance: 0.8936788252247614 entropy 9.60034677831555
epoch: 5, step: 95
	action: tensor([[-8.6949e+08, -6.2800e+00, -2.8079e+09,  4.0379e+08,  6.2800e+00,
         -1.6072e+09, -1.6820e+09]], dtype=torch.float64)
	q_value: tensor([[-0.0810]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5613605972702855, distance: 1.4299086747981817 entropy 10.021719686899859
epoch: 5, step: 96
	action: tensor([[ 2.3066e+09, -6.2800e+00, -3.0874e+09, -5.0313e+08,  6.2800e+00,
         -4.1066e+08, -1.2569e+09]], dtype=torch.float64)
	q_value: tensor([[-0.0573]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4420051565957873, distance: 0.8548143142829835 entropy 9.868539169964228
epoch: 5, step: 97
	action: tensor([[ 2.0942e+08, -6.2800e+00,  1.8654e+09,  7.5059e+08,  6.2800e+00,
          2.7217e+09,  9.0556e+07]], dtype=torch.float64)
	q_value: tensor([[-0.0728]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4309930788065848, distance: 0.8632080221727831 entropy 9.875412630136143
epoch: 5, step: 98
	action: tensor([[-1.4875e+09, -6.2800e+00, -1.5138e+09,  7.8536e+08,  6.2800e+00,
         -7.1747e+08,  1.3613e+09]], dtype=torch.float64)
	q_value: tensor([[0.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6454035682558505, distance: 1.4678879680036407 entropy 9.842111693081241
epoch: 5, step: 99
	action: tensor([[-1.3779e+09, -6.2800e+00,  1.3996e+09,  4.9838e+08,  6.2800e+00,
          1.2019e+09,  1.7975e+07]], dtype=torch.float64)
	q_value: tensor([[0.0851]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06727797644922873, distance: 1.1051794705807652 entropy 9.792976757379778
epoch: 5, step: 100
	action: tensor([[ 1.2899e+09, -6.2800e+00,  2.0040e+09,  2.8199e+08,  6.2800e+00,
         -2.8515e+09, -6.1561e+08]], dtype=torch.float64)
	q_value: tensor([[0.0758]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5639994772201681, distance: 0.7556143520590416 entropy 9.784369207907549
epoch: 5, step: 101
	action: tensor([[ 2.4469e+09, -6.2800e+00,  6.0104e+08,  2.8669e+09,  6.2800e+00,
         -7.4666e+08,  1.1154e+09]], dtype=torch.float64)
	q_value: tensor([[0.0184]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6805397206148747, distance: 0.6467927256524043 entropy 9.879963309146108
epoch: 5, step: 102
	action: tensor([[-1.6348e+09, -6.2800e+00,  4.8559e+08,  8.2289e+08,  6.2800e+00,
         -1.1136e+09, -1.5870e+09]], dtype=torch.float64)
	q_value: tensor([[0.0469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6524516261139279, distance: 1.4710284449727902 entropy 9.837139815604443
epoch: 5, step: 103
	action: tensor([[-1.2269e+09, -6.2800e+00, -1.7444e+09, -3.4781e+08,  6.2800e+00,
          6.7410e+08, -1.8468e+08]], dtype=torch.float64)
	q_value: tensor([[0.0274]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6261827364952688, distance: 1.4592891935830417 entropy 9.828292206646244
epoch: 5, step: 104
	action: tensor([[-5.4742e+08, -6.2800e+00,  7.6172e+08, -1.0356e+09,  6.2800e+00,
         -1.0496e+09,  1.8642e+09]], dtype=torch.float64)
	q_value: tensor([[0.3650]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.031011864960509783, distance: 1.1264603842538945 entropy 9.849230028722685
epoch: 5, step: 105
	action: tensor([[-2.3923e+08, -6.2800e+00, -6.0043e+08, -1.4361e+09,  6.2800e+00,
         -1.6933e+09, -1.2599e+09]], dtype=torch.float64)
	q_value: tensor([[-0.0447]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04827010271418175, distance: 1.1163838608193821 entropy 10.072026608057937
epoch: 5, step: 106
	action: tensor([[-2.7769e+09, -6.2800e+00, -1.5456e+09, -3.2068e+09,  6.2800e+00,
         -1.0966e+09, -8.1455e+08]], dtype=torch.float64)
	q_value: tensor([[-0.0240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.008018111358770952, distance: 1.139747280863028 entropy 10.052396386469264
epoch: 5, step: 107
	action: tensor([[ 4.6588e+07, -6.2800e+00, -1.6901e+09, -2.1777e+09,  6.2800e+00,
          3.0805e+07, -1.4569e+09]], dtype=torch.float64)
	q_value: tensor([[0.2401]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.13979660735669
epoch: 5, step: 108
	action: tensor([[-1.4463e+09, -6.2800e+00,  1.7418e+09, -9.2495e+08,  6.2800e+00,
          1.5235e+08,  4.3590e+08]], dtype=torch.float64)
	q_value: tensor([[0.5094]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.71998308840981
epoch: 5, step: 109
	action: tensor([[-1.3838e+09, -6.2800e+00, -1.1545e+09, -9.8461e+07,  6.2800e+00,
          4.6363e+07, -6.2730e+08]], dtype=torch.float64)
	q_value: tensor([[0.5094]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 9.71998308840981
epoch: 5, step: 110
	action: tensor([[-3.2572e+09, -6.2800e+00, -1.2917e+09,  8.5896e+08,  6.2800e+00,
         -1.4007e+09, -1.2452e+09]], dtype=torch.float64)
	q_value: tensor([[0.5094]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4739185284381797, distance: 1.3892916722143243 entropy 9.71998308840981
epoch: 5, step: 111
	action: tensor([[-3.1103e+09, -6.2800e+00, -6.8704e+08, -4.2507e+08,  6.2800e+00,
         -2.4865e+08,  2.0004e+08]], dtype=torch.float64)
	q_value: tensor([[-0.0704]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12831224800315633, distance: 1.0684080834153158 entropy 9.973268546926153
epoch: 5, step: 112
	action: tensor([[-9.6143e+08, -6.2800e+00, -1.8953e+09, -1.9327e+08,  6.2800e+00,
         -5.2052e+08, -1.9968e+09]], dtype=torch.float64)
	q_value: tensor([[0.1150]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10347752230214713, distance: 1.0835208795290405 entropy 10.087265484116177
epoch: 5, step: 113
	action: tensor([[ 5.4500e+08, -6.2800e+00,  7.3806e+08,  2.1335e+09,  6.2800e+00,
          2.2759e+08, -2.9134e+09]], dtype=torch.float64)
	q_value: tensor([[0.1795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.429667153013062, distance: 0.8642131802336314 entropy 9.975337864690276
epoch: 5, step: 114
	action: tensor([[ 8.8045e+08, -6.2800e+00, -1.8309e+09,  1.2854e+07,  6.2800e+00,
         -1.3034e+08,  2.9406e+07]], dtype=torch.float64)
	q_value: tensor([[-0.1646]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7929559344797761, distance: 0.5207006103131115 entropy 9.718085474660954
epoch: 5, step: 115
	action: tensor([[-6.8314e+08, -6.2800e+00,  1.4954e+09,  3.5741e+09,  6.2800e+00,
         -3.3889e+08,  1.1181e+09]], dtype=torch.float64)
	q_value: tensor([[0.0696]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4487167790819211, distance: 1.3773630823477752 entropy 9.734883457853124
epoch: 5, step: 116
	action: tensor([[ 5.5759e+07, -6.2800e+00, -2.0462e+08,  5.3978e+08,  6.2800e+00,
         -2.8000e+09, -1.1207e+09]], dtype=torch.float64)
	q_value: tensor([[-0.0142]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4976834326498416, distance: 0.8110459209225446 entropy 9.862621366862063
epoch: 5, step: 117
	action: tensor([[ 2.4577e+08, -6.2800e+00, -7.3734e+08, -1.7720e+08,  6.2800e+00,
          2.0367e+09,  1.2419e+09]], dtype=torch.float64)
	q_value: tensor([[0.0566]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25594482012237063, distance: 0.9870957124203349 entropy 9.826161420216579
epoch: 5, step: 118
	action: tensor([[ 5.6266e+08, -6.2800e+00,  1.1246e+09,  1.3060e+09,  6.2800e+00,
          7.0331e+08, -3.3030e+08]], dtype=torch.float64)
	q_value: tensor([[-0.2449]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.438572477302672, distance: 0.8574396113067767 entropy 9.811518629132125
epoch: 5, step: 119
	action: tensor([[-1.1409e+09, -6.2800e+00,  2.3325e+09, -2.9507e+08,  6.2800e+00,
         -1.5020e+09, -2.6444e+08]], dtype=torch.float64)
	q_value: tensor([[-0.1413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12949833130950228, distance: 1.0676809581622948 entropy 9.660277866754999
epoch: 5, step: 120
	action: tensor([[-7.5497e+08, -6.2800e+00, -1.0354e+09,  6.2769e+08,  6.2800e+00,
         -5.6508e+08, -4.8450e+08]], dtype=torch.float64)
	q_value: tensor([[0.0944]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3652123270672729, distance: 1.3370781561530747 entropy 10.119947706936077
epoch: 5, step: 121
	action: tensor([[ 1.2949e+08, -6.2800e+00, -7.3975e+08, -3.1606e+08,  6.2800e+00,
         -1.0091e+09,  1.2699e+09]], dtype=torch.float64)
	q_value: tensor([[-0.1251]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.43976030228082674, distance: 0.8565320787877404 entropy 9.967885030100888
epoch: 5, step: 122
	action: tensor([[-1.3881e+09, -6.2800e+00,  3.5696e+08, -9.4485e+08,  6.2800e+00,
          5.4238e+08,  8.9622e+07]], dtype=torch.float64)
	q_value: tensor([[-0.0806]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.48221921531714607, distance: 1.3931982261233355 entropy 9.921868555205394
epoch: 5, step: 123
	action: tensor([[ 3.4701e+09, -6.2800e+00, -8.9427e+08, -7.3376e+08,  6.2800e+00,
         -1.0316e+09,  2.6074e+08]], dtype=torch.float64)
	q_value: tensor([[0.1371]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42740422389182253, distance: 0.8659259670245362 entropy 9.878309929501302
epoch: 5, step: 124
	action: tensor([[-5.5360e+08, -6.2800e+00, -3.2667e+08,  5.3146e+08,  6.2800e+00,
          3.7152e+08, -1.7661e+09]], dtype=torch.float64)
	q_value: tensor([[-0.0872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12936174693205127, distance: 1.067764716082839 entropy 9.829354834173058
epoch: 5, step: 125
	action: tensor([[-2.1996e+09, -6.2800e+00, -1.8105e+09,  1.9125e+09,  6.2800e+00,
          1.5252e+09,  7.2299e+08]], dtype=torch.float64)
	q_value: tensor([[-0.0075]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13023963163654972, distance: 1.067226254372589 entropy 9.7813232510412
epoch: 5, step: 126
	action: tensor([[-1.7003e+09, -6.2800e+00,  2.8353e+09,  4.5997e+07,  6.2800e+00,
          1.5989e+09,  1.0526e+09]], dtype=torch.float64)
	q_value: tensor([[0.0685]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16397812860204564, distance: 1.0463223758601625 entropy 9.864955371326804
epoch: 5, step: 127
	action: tensor([[-1.5617e+09, -6.2800e+00,  5.5195e+08, -8.2025e+08,  6.2800e+00,
          1.7395e+09, -5.8528e+08]], dtype=torch.float64)
	q_value: tensor([[-0.1030]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.28644303816489214, distance: 1.2979321012160312 entropy 9.906306970191578
LOSS epoch 5 actor 48.64433555387684 critic 394.9665612032894 entropy 0.1
epoch: 6, step: 0
	action: tensor([[ 7.6204e+07, -6.2800e+00,  1.2564e+09,  6.7170e+08,  6.2800e+00,
          3.6879e+09, -3.0617e+09]], dtype=torch.float64)
	q_value: tensor([[-0.6590]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4130060131258061, distance: 0.8767454474560379 entropy 10.564065992020469
epoch: 6, step: 1
	action: tensor([[ 9.4982e+08, -6.2800e+00,  2.0153e+09,  2.4082e+08,  6.2800e+00,
          4.4734e+09,  2.8753e+09]], dtype=torch.float64)
	q_value: tensor([[-0.6242]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.374459405446424, distance: 0.9050747365317405 entropy 10.31143791089697
epoch: 6, step: 2
	action: tensor([[ 1.4800e+09, -6.2800e+00, -1.9212e+09,  2.4506e+08,  6.2800e+00,
          1.5485e+09,  5.7039e+09]], dtype=torch.float64)
	q_value: tensor([[-0.6583]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4759260200147091, distance: 0.828424610269807 entropy 10.35748460933341
epoch: 6, step: 3
	action: tensor([[-3.2580e+08, -6.2800e+00, -3.9780e+09,  3.0444e+09,  6.2800e+00,
         -3.4908e+08,  5.0793e+09]], dtype=torch.float64)
	q_value: tensor([[-0.9522]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2593445459405561, distance: 1.2841890902912523 entropy 10.541074469571837
epoch: 6, step: 4
	action: tensor([[ 5.0592e+09, -6.2800e+00,  7.3901e+09, -3.5461e+09,  6.2800e+00,
          3.3731e+08,  6.5136e+08]], dtype=torch.float64)
	q_value: tensor([[-0.7983]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13728834649220256, distance: 1.062892947741679 entropy 10.59048340033076
epoch: 6, step: 5
	action: tensor([[ 4.0911e+09, -6.2800e+00, -1.5733e+09, -2.2542e+09,  6.2800e+00,
          2.9552e+09, -1.1875e+09]], dtype=torch.float64)
	q_value: tensor([[-0.5968]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.528973009708384, distance: 0.78537955211583 entropy 10.36250162782122
epoch: 6, step: 6
	action: tensor([[-1.9025e+08, -6.2800e+00,  9.6667e+08,  1.7769e+09,  6.2800e+00,
         -4.8456e+09,  1.2804e+08]], dtype=torch.float64)
	q_value: tensor([[-0.6715]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.49122563615400483, distance: 1.3974245666427492 entropy 10.398845508200381
epoch: 6, step: 7
	action: tensor([[-2.9311e+09, -6.2800e+00, -3.7043e+09,  1.1622e+09,  6.2800e+00,
          8.1948e+09,  2.9487e+08]], dtype=torch.float64)
	q_value: tensor([[-0.9673]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09990795543936537, distance: 1.0856757937048878 entropy 10.601017191238649
epoch: 6, step: 8
	action: tensor([[ 2.1204e+09, -6.2800e+00,  6.2921e+09,  3.8763e+09,  6.2800e+00,
         -7.7856e+08, -1.5346e+09]], dtype=torch.float64)
	q_value: tensor([[-0.7021]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7540752976109832, distance: 0.5674894217670677 entropy 10.465808573293575
epoch: 6, step: 9
	action: tensor([[-4.8457e+08, -6.2800e+00, -5.0852e+09,  1.9512e+09,  6.2800e+00,
          2.5324e+08, -1.6206e+09]], dtype=torch.float64)
	q_value: tensor([[-0.7310]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21617909510974098, distance: 1.0131298572641385 entropy 10.389077375729117
epoch: 6, step: 10
	action: tensor([[-1.8475e+09, -6.2800e+00,  1.8924e+09, -3.3551e+09,  6.2800e+00,
         -8.0176e+09,  4.5606e+09]], dtype=torch.float64)
	q_value: tensor([[-0.5949]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17698651066323312, distance: 1.0381501474669539 entropy 10.530723235240671
epoch: 6, step: 11
	action: tensor([[-1.6377e+09, -6.2800e+00, -9.7785e+09,  6.6458e+08,  6.2800e+00,
         -6.8861e+09,  1.1075e+09]], dtype=torch.float64)
	q_value: tensor([[-0.9344]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.723719355150521, distance: 1.5024152126642314 entropy 10.664971169176267
epoch: 6, step: 12
	action: tensor([[ 3.0336e+09, -6.2800e+00,  3.2924e+09,  4.8548e+09,  6.2800e+00,
          2.8789e+09, -7.2600e+08]], dtype=torch.float64)
	q_value: tensor([[-0.7933]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3630500428606318, distance: 0.9132913610856148 entropy 10.59738804693313
epoch: 6, step: 13
	action: tensor([[-4.9447e+09, -6.2800e+00, -4.2204e+09,  3.4586e+09,  6.2800e+00,
         -1.0137e+09,  6.8471e+08]], dtype=torch.float64)
	q_value: tensor([[-0.5579]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.26054404402612
epoch: 6, step: 14
	action: tensor([[-6.7274e+09, -6.2800e+00,  1.1725e+09, -6.2249e+09,  6.2800e+00,
         -4.1603e+09,  3.1636e+08]], dtype=torch.float64)
	q_value: tensor([[-0.3572]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.373342871850838
epoch: 6, step: 15
	action: tensor([[ 1.0656e+09, -6.2800e+00, -5.6613e+07,  1.1185e+09,  6.2800e+00,
         -2.8090e+09,  2.4232e+09]], dtype=torch.float64)
	q_value: tensor([[-0.3572]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.373342871850838
epoch: 6, step: 16
	action: tensor([[-3.3885e+09, -6.2800e+00, -1.8310e+09, -2.1956e+09,  6.2800e+00,
          1.4715e+09, -3.2878e+09]], dtype=torch.float64)
	q_value: tensor([[-0.3572]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8242643603540905, distance: 1.545612352978891 entropy 10.373342871850838
epoch: 6, step: 17
	action: tensor([[-5.0746e+09, -6.2800e+00, -1.9897e+09,  8.2802e+09,  6.2800e+00,
         -7.9112e+08, -2.2652e+09]], dtype=torch.float64)
	q_value: tensor([[-0.6465]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8392726534922783, distance: 1.5519572359821918 entropy 10.56963449268956
epoch: 6, step: 18
	action: tensor([[ 8.2210e+08, -6.2800e+00, -4.3488e+08, -3.9359e+09,  6.2800e+00,
         -2.5500e+09,  6.5091e+09]], dtype=torch.float64)
	q_value: tensor([[-0.6842]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4836670355118845, distance: 0.8222835831331671 entropy 10.53535886091449
epoch: 6, step: 19
	action: tensor([[ 1.3521e+09, -6.2800e+00,  3.8884e+09, -1.5174e+09,  6.2800e+00,
         -4.9811e+09, -4.6824e+07]], dtype=torch.float64)
	q_value: tensor([[-0.6849]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5051090713762902, distance: 0.805028841354789 entropy 10.507664846026197
epoch: 6, step: 20
	action: tensor([[ 7.1992e+09, -6.2800e+00,  4.0713e+09, -3.1500e+09,  6.2800e+00,
         -1.1452e+09,  3.2168e+09]], dtype=torch.float64)
	q_value: tensor([[-0.7781]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4636016235810796, distance: 0.8381088383986389 entropy 10.506321309490447
epoch: 6, step: 21
	action: tensor([[-2.9659e+09, -6.2800e+00, -3.5417e+09,  3.1465e+09,  6.2800e+00,
         -4.5626e+09,  3.6479e+09]], dtype=torch.float64)
	q_value: tensor([[-1.0723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5080128484028408, distance: 1.405268185421016 entropy 10.71977558130277
epoch: 6, step: 22
	action: tensor([[-6.1000e+09, -6.2800e+00,  4.9396e+09, -2.4909e+09,  6.2800e+00,
          2.5727e+09, -5.3187e+09]], dtype=torch.float64)
	q_value: tensor([[-1.1549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6924744803311134, distance: 1.488736230809966 entropy 10.693189550638156
epoch: 6, step: 23
	action: tensor([[ 3.6128e+09, -6.2800e+00,  4.2703e+09, -2.0539e+09,  6.2800e+00,
         -6.1910e+07,  5.7283e+09]], dtype=torch.float64)
	q_value: tensor([[-0.5751]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42048092840585605, distance: 0.8711452234756827 entropy 10.449975853798387
epoch: 6, step: 24
	action: tensor([[-4.9340e+09, -6.2800e+00, -7.4328e+08,  4.3815e+08,  6.2800e+00,
         -6.7214e+09,  5.2031e+09]], dtype=torch.float64)
	q_value: tensor([[-0.7535]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8097217281087583, distance: 1.5394393859626727 entropy 10.656873649805872
epoch: 6, step: 25
	action: tensor([[-8.0777e+08, -6.2800e+00,  4.5075e+08, -1.2628e+08,  6.2800e+00,
         -4.8959e+09,  1.8202e+09]], dtype=torch.float64)
	q_value: tensor([[-0.6644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04728635788752156, distance: 1.1169606805674726 entropy 10.536601181491827
epoch: 6, step: 26
	action: tensor([[ 1.5893e+09, -6.2800e+00, -3.3073e+09,  4.0180e+09,  6.2800e+00,
         -2.3367e+09, -5.4693e+09]], dtype=torch.float64)
	q_value: tensor([[-1.0107]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.589687996906491, distance: 0.7330165904326703 entropy 10.688173609083014
epoch: 6, step: 27
	action: tensor([[-1.8313e+09, -6.2800e+00, -2.7664e+09,  1.0844e+08,  6.2800e+00,
          3.7238e+09,  7.4003e+09]], dtype=torch.float64)
	q_value: tensor([[-0.7128]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03958749581506138, distance: 1.1214646694534747 entropy 10.402125321345068
epoch: 6, step: 28
	action: tensor([[ 2.0976e+09, -6.2800e+00, -6.4935e+09,  1.7076e+09,  6.2800e+00,
         -8.0713e+08,  1.9490e+09]], dtype=torch.float64)
	q_value: tensor([[-0.6750]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3279839379590723, distance: 0.9380943492963961 entropy 10.458131039612498
epoch: 6, step: 29
	action: tensor([[ 5.2248e+09, -6.2800e+00, -1.9601e+09,  3.0833e+08,  6.2800e+00,
          2.6090e+09, -4.1408e+09]], dtype=torch.float64)
	q_value: tensor([[-0.7640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3886193200966015, distance: 0.8947723363341512 entropy 10.446322819985982
epoch: 6, step: 30
	action: tensor([[-2.7869e+09, -6.2800e+00, -1.5456e+09, -3.4360e+09,  6.2800e+00,
          3.0390e+09, -1.1970e+09]], dtype=torch.float64)
	q_value: tensor([[-0.5751]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.672506165327337, distance: 1.4799278974831065 entropy 10.306760526274903
epoch: 6, step: 31
	action: tensor([[-3.0766e+09, -6.2800e+00, -1.3459e+09, -3.2507e+08,  6.2800e+00,
          3.6399e+09, -1.4214e+09]], dtype=torch.float64)
	q_value: tensor([[-0.5620]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6766258012846573, distance: 1.4817494197621528 entropy 10.460591618963653
epoch: 6, step: 32
	action: tensor([[ 1.4517e+09, -6.2800e+00,  4.4215e+09, -1.5423e+09,  6.2800e+00,
         -3.6056e+09, -6.8635e+08]], dtype=torch.float64)
	q_value: tensor([[-0.6973]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.40948216013789474, distance: 0.8793731567943174 entropy 10.546576574189547
epoch: 6, step: 33
	action: tensor([[ 1.9738e+08, -6.2800e+00, -5.5733e+09, -1.4627e+08,  6.2800e+00,
          4.7574e+09, -7.0555e+09]], dtype=torch.float64)
	q_value: tensor([[-0.9641]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2672881756473723, distance: 0.9795425216743764 entropy 10.613627039703955
epoch: 6, step: 34
	action: tensor([[-1.2760e+09, -6.2800e+00,  4.9453e+09, -1.9173e+09,  6.2800e+00,
          6.3689e+08,  7.4296e+08]], dtype=torch.float64)
	q_value: tensor([[-0.6830]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5786176928681552, distance: 1.4377890649112477 entropy 10.563039517303695
epoch: 6, step: 35
	action: tensor([[ 3.7733e+08, -6.2800e+00,  3.8901e+09, -2.3048e+09,  6.2800e+00,
         -1.4451e+09, -7.6152e+09]], dtype=torch.float64)
	q_value: tensor([[-0.7777]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4219325913146056, distance: 0.8700534542756833 entropy 10.618375619363832
epoch: 6, step: 36
	action: tensor([[-9.2952e+08, -6.2800e+00,  2.4343e+09, -5.6889e+09,  6.2800e+00,
         -1.2817e+09, -1.3224e+09]], dtype=torch.float64)
	q_value: tensor([[-0.8054]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08928363436570186, distance: 1.092064434930686 entropy 10.501128436864784
epoch: 6, step: 37
	action: tensor([[-9.0088e+09, -6.2800e+00, -2.1507e+09, -1.6936e+09,  6.2800e+00,
          4.8224e+09, -2.6288e+09]], dtype=torch.float64)
	q_value: tensor([[-0.9644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5566269424017343, distance: 1.427739466669559 entropy 10.72997920714617
epoch: 6, step: 38
	action: tensor([[-4.8372e+09, -6.2800e+00,  6.7519e+08,  1.3586e+09,  6.2800e+00,
          5.0797e+09,  4.8017e+09]], dtype=torch.float64)
	q_value: tensor([[-1.0094]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09130477709330409, distance: 1.0908519586051812 entropy 10.654318350952497
epoch: 6, step: 39
	action: tensor([[ 5.5409e+09, -6.2800e+00, -2.9729e+09, -1.0192e+09,  6.2800e+00,
          4.9185e+09,  1.0678e+08]], dtype=torch.float64)
	q_value: tensor([[-0.5187]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3851173749374477, distance: 0.8973312734446777 entropy 10.389076868873657
epoch: 6, step: 40
	action: tensor([[-7.5519e+08, -6.2800e+00,  2.0898e+09,  2.4368e+09,  6.2800e+00,
         -4.7274e+09,  5.6846e+08]], dtype=torch.float64)
	q_value: tensor([[-0.6049]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9608960899897143, distance: 1.602448140690425 entropy 10.421484989462744
epoch: 6, step: 41
	action: tensor([[ 8.8081e+09, -6.2800e+00,  2.7130e+09,  8.2606e+07,  6.2800e+00,
          6.0062e+08,  5.2500e+08]], dtype=torch.float64)
	q_value: tensor([[-0.6459]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4914239442880719, distance: 0.8160835953830237 entropy 10.539491122248952
epoch: 6, step: 42
	action: tensor([[-4.2744e+09, -6.2800e+00,  5.8623e+09, -5.3626e+07,  6.2800e+00,
         -6.1155e+08,  1.3435e+09]], dtype=torch.float64)
	q_value: tensor([[-0.7534]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.070318954557826, distance: 1.1033763768863578 entropy 10.485387829011637
epoch: 6, step: 43
	action: tensor([[-3.3931e+09, -6.2800e+00, -4.2900e+08, -5.0862e+09,  6.2800e+00,
          2.0944e+09,  1.3449e+09]], dtype=torch.float64)
	q_value: tensor([[-0.9404]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.45818134758242746, distance: 1.3818549627419328 entropy 10.652378933763709
epoch: 6, step: 44
	action: tensor([[ 1.8575e+08, -6.2800e+00,  3.1139e+09,  6.6495e+07,  6.2800e+00,
          4.2733e+09, -3.3557e+08]], dtype=torch.float64)
	q_value: tensor([[-0.8116]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.46071222225625974, distance: 0.8403631148037385 entropy 10.623732298668669
epoch: 6, step: 45
	action: tensor([[-2.1908e+09, -6.2800e+00, -9.1955e+09, -2.0929e+09,  6.2800e+00,
         -2.0109e+09, -2.5181e+09]], dtype=torch.float64)
	q_value: tensor([[-0.8629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10073275023729489, distance: 1.0851782529345608 entropy 10.485703704011184
epoch: 6, step: 46
	action: tensor([[-3.6162e+09, -6.2800e+00, -3.6958e+09,  6.4223e+07,  6.2800e+00,
          7.9173e+08, -1.4948e+09]], dtype=torch.float64)
	q_value: tensor([[-0.7287]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06834500125200516, distance: 1.1045471324705844 entropy 10.663870605780577
epoch: 6, step: 47
	action: tensor([[ 2.6719e+09, -6.2800e+00,  4.5279e+09,  2.3839e+09,  6.2800e+00,
          2.1267e+09, -9.5992e+07]], dtype=torch.float64)
	q_value: tensor([[-0.8563]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39626974178260443, distance: 0.8891564118915243 entropy 10.519106572029658
epoch: 6, step: 48
	action: tensor([[-3.2696e+09, -6.2800e+00, -8.2905e+09,  1.1443e+08,  6.2800e+00,
          3.1513e+09,  1.7243e+09]], dtype=torch.float64)
	q_value: tensor([[-0.6145]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08788573701829305, distance: 1.0929022416699743 entropy 10.321424485347391
epoch: 6, step: 49
	action: tensor([[-2.7845e+09, -6.2800e+00,  1.2867e+09,  2.6073e+09,  6.2800e+00,
         -5.5677e+09,  1.8988e+09]], dtype=torch.float64)
	q_value: tensor([[-0.8024]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8672453851090982, distance: 1.5637142383850235 entropy 10.586543712193606
epoch: 6, step: 50
	action: tensor([[-5.2273e+09, -6.2800e+00,  8.0282e+09, -2.2223e+09,  6.2800e+00,
          3.6993e+09,  3.2344e+09]], dtype=torch.float64)
	q_value: tensor([[-0.6623]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8467590371864853, distance: 1.5551124910207743 entropy 10.51175152385518
epoch: 6, step: 51
	action: tensor([[-5.2746e+09, -6.2800e+00, -7.2223e+09,  1.3986e+09,  6.2800e+00,
          2.3616e+09,  3.2687e+08]], dtype=torch.float64)
	q_value: tensor([[-0.5735]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06831027312640248, distance: 1.1045677186822964 entropy 10.483588653522729
epoch: 6, step: 52
	action: tensor([[-4.0811e+09, -6.2800e+00,  2.7112e+08, -4.5352e+08,  6.2800e+00,
         -3.9938e+08,  9.0162e+08]], dtype=torch.float64)
	q_value: tensor([[-0.5455]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12751465753429425, distance: 1.0688967660098028 entropy 10.3510717960913
epoch: 6, step: 53
	action: tensor([[ 5.5101e+09, -6.2800e+00,  1.5856e+09, -2.6255e+09,  6.2800e+00,
         -3.4555e+09,  6.7891e+08]], dtype=torch.float64)
	q_value: tensor([[-0.9037]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4446350728737335, distance: 0.8527974988352356 entropy 10.63595942603832
epoch: 6, step: 54
	action: tensor([[-8.4618e+08, -6.2800e+00,  3.7718e+09,  2.0901e+09,  6.2800e+00,
          6.8272e+09,  2.4272e+09]], dtype=torch.float64)
	q_value: tensor([[-0.6865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11171655049888363, distance: 1.0785306175980165 entropy 10.541271290315832
epoch: 6, step: 55
	action: tensor([[-1.1358e+09, -6.2800e+00, -6.5257e+08, -2.1958e+09,  6.2800e+00,
          3.5609e+08, -8.0841e+09]], dtype=torch.float64)
	q_value: tensor([[-0.6468]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3807492473329599, distance: 1.3446649999490776 entropy 10.607671856597532
epoch: 6, step: 56
	action: tensor([[-1.2675e+09, -6.2800e+00, -1.3645e+09, -3.2376e+09,  6.2800e+00,
         -4.0232e+08, -2.4597e+09]], dtype=torch.float64)
	q_value: tensor([[-0.7876]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08316019302239253, distance: 1.0957296756243873 entropy 10.649457452844391
epoch: 6, step: 57
	action: tensor([[ 2.6851e+09, -6.2800e+00,  5.0462e+08,  6.4808e+09,  6.2800e+00,
          1.0507e+10,  7.0434e+08]], dtype=torch.float64)
	q_value: tensor([[-0.9556]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41779355650238936, distance: 0.8731627441521915 entropy 10.66065414291539
epoch: 6, step: 58
	action: tensor([[-3.3203e+08, -6.2800e+00, -1.0067e+09, -1.8739e+08,  6.2800e+00,
          5.9977e+09,  1.0728e+08]], dtype=torch.float64)
	q_value: tensor([[-0.6701]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8188133133960365, distance: 1.5433014190717644 entropy 10.466799791563021
epoch: 6, step: 59
	action: tensor([[ 6.6576e+08, -6.2800e+00, -2.5240e+08,  3.3899e+08,  6.2800e+00,
          2.3661e+09,  8.7972e+09]], dtype=torch.float64)
	q_value: tensor([[-0.9543]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3870169778504665, distance: 0.8959441049142122 entropy 10.621668847195926
epoch: 6, step: 60
	action: tensor([[-1.2848e+09, -6.2800e+00,  6.4372e+08, -6.3461e+08,  6.2800e+00,
         -6.5005e+07,  8.9904e+08]], dtype=torch.float64)
	q_value: tensor([[-0.5689]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08059195878673464, distance: 1.0972632709731596 entropy 10.269039635826918
epoch: 6, step: 61
	action: tensor([[-3.3827e+09, -6.2800e+00, -2.0074e+09,  2.0216e+09,  6.2800e+00,
          7.4709e+08,  1.1782e+09]], dtype=torch.float64)
	q_value: tensor([[-0.7133]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07919985938629515, distance: 1.0980936540587558 entropy 10.650914457881473
epoch: 6, step: 62
	action: tensor([[-1.5611e+09, -6.2800e+00,  4.2000e+09, -1.4031e+09,  6.2800e+00,
         -1.6513e+09, -2.5850e+09]], dtype=torch.float64)
	q_value: tensor([[-0.7475]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0665683354287091, distance: 1.1055998163797323 entropy 10.459330470509887
epoch: 6, step: 63
	action: tensor([[ 7.1403e+09, -6.2800e+00, -3.3003e+08, -1.0560e+10,  6.2800e+00,
          1.1132e+09,  2.7667e+09]], dtype=torch.float64)
	q_value: tensor([[-0.7098]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6378257873551212, distance: 0.6886768044167979 entropy 10.609350385265136
epoch: 6, step: 64
	action: tensor([[ 4.8681e+07, -6.2800e+00,  5.2251e+09,  7.7610e+09,  6.2800e+00,
          4.9156e+09,  3.6464e+08]], dtype=torch.float64)
	q_value: tensor([[-0.6760]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44998380986377984, distance: 0.848680903392315 entropy 10.532213481922385
epoch: 6, step: 65
	action: tensor([[ 1.6245e+09, -6.2800e+00, -1.5264e+09,  2.8843e+09,  6.2800e+00,
         -9.1929e+08,  2.1248e+09]], dtype=torch.float64)
	q_value: tensor([[-0.5918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3743165026145293, distance: 0.9051781114031666 entropy 10.284943302755877
epoch: 6, step: 66
	action: tensor([[ 3.7528e+09, -6.2800e+00, -1.1743e+09, -2.5468e+09,  6.2800e+00,
         -3.1137e+08,  1.0521e+09]], dtype=torch.float64)
	q_value: tensor([[-0.7440]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44597478176062155, distance: 0.8517682744650366 entropy 10.425758452872346
epoch: 6, step: 67
	action: tensor([[ 7.4911e+09, -6.2800e+00,  1.3111e+09,  5.0472e+09,  6.2800e+00,
          9.9618e+09,  3.5465e+09]], dtype=torch.float64)
	q_value: tensor([[-0.9577]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37676941975773215, distance: 0.9034020478471698 entropy 10.60107494928304
epoch: 6, step: 68
	action: tensor([[-5.6449e+09, -6.2800e+00,  8.5269e+08, -2.9733e+09,  6.2800e+00,
          3.5808e+09,  1.8079e+09]], dtype=torch.float64)
	q_value: tensor([[-0.7670]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.41285616461259056, distance: 1.3602090771276822 entropy 10.465695565553357
epoch: 6, step: 69
	action: tensor([[-5.1319e+09, -6.2800e+00,  1.8088e+09, -5.2949e+09,  6.2800e+00,
         -7.9290e+09, -2.5007e+09]], dtype=torch.float64)
	q_value: tensor([[-0.6819]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.042425863112828255, distance: 1.119806275827768 entropy 10.63072359526676
epoch: 6, step: 70
	action: tensor([[ 1.3827e+09, -6.2800e+00, -9.6963e+08, -3.1495e+09,  6.2800e+00,
         -6.8999e+09, -6.5801e+08]], dtype=torch.float64)
	q_value: tensor([[-0.8800]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37103822671669096, distance: 0.9075463589160017 entropy 10.670145783639828
epoch: 6, step: 71
	action: tensor([[-2.4543e+09, -6.2800e+00,  1.0005e+10, -1.8147e+09,  6.2800e+00,
         -5.1863e+07,  5.9819e+09]], dtype=torch.float64)
	q_value: tensor([[-0.9968]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.005226276714869971, distance: 1.1413500068197338 entropy 10.583144201318623
epoch: 6, step: 72
	action: tensor([[ 6.6271e+09, -6.2800e+00, -1.0899e+10,  7.5077e+09,  6.2800e+00,
          1.9852e+09, -1.6962e+08]], dtype=torch.float64)
	q_value: tensor([[-0.8741]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.45785122088668184, distance: 0.8425892914266496 entropy 10.718625847334787
epoch: 6, step: 73
	action: tensor([[ 6.8013e+09, -6.2800e+00, -4.1870e+09, -1.6755e+08,  6.2800e+00,
         -2.2550e+09,  2.0647e+08]], dtype=torch.float64)
	q_value: tensor([[-0.7230]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4259786619719187, distance: 0.867003222363948 entropy 10.437231004933178
epoch: 6, step: 74
	action: tensor([[ 4.7863e+08, -6.2800e+00, -2.3376e+09,  1.6944e+09,  6.2800e+00,
         -5.4160e+09, -6.9995e+09]], dtype=torch.float64)
	q_value: tensor([[-0.8363]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8564979606233636, distance: 0.433496634180473 entropy 10.483786187230226
epoch: 6, step: 75
	action: tensor([[ 2.4328e+09, -6.2800e+00,  1.7166e+09,  2.8073e+07,  6.2800e+00,
         -2.5567e+09,  5.3192e+08]], dtype=torch.float64)
	q_value: tensor([[-1.0051]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7931928996281634, distance: 0.5204025500375437 entropy 10.554285184543714
epoch: 6, step: 76
	action: tensor([[-3.4721e+09, -6.2800e+00, -6.2235e+09, -1.7165e+09,  6.2800e+00,
          2.7954e+09, -4.4849e+08]], dtype=torch.float64)
	q_value: tensor([[-0.7522]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9741407563893829, distance: 1.6078508168049392 entropy 10.50699620661652
epoch: 6, step: 77
	action: tensor([[-2.5651e+09, -6.2800e+00, -5.0739e+07,  3.0818e+08,  6.2800e+00,
         -4.2999e+08,  2.4012e+09]], dtype=torch.float64)
	q_value: tensor([[-0.7853]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6270726172535803, distance: 1.4596884155583336 entropy 10.533153069574386
epoch: 6, step: 78
	action: tensor([[-2.5163e+09, -6.2800e+00,  6.1612e+08,  1.9469e+09,  6.2800e+00,
         -5.7700e+09,  7.4649e+08]], dtype=torch.float64)
	q_value: tensor([[-0.8155]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4721293058012179, distance: 1.3884481700474725 entropy 10.50222876252317
epoch: 6, step: 79
	action: tensor([[ 2.8360e+09, -6.2800e+00,  5.2931e+08, -2.6946e+09,  6.2800e+00,
          2.2523e+08,  7.1626e+08]], dtype=torch.float64)
	q_value: tensor([[-0.7819]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3507304293373684, distance: 0.9220813071524804 entropy 10.481460611502317
epoch: 6, step: 80
	action: tensor([[ 2.7262e+08, -6.2800e+00,  2.1505e+09, -2.3142e+08,  6.2800e+00,
          1.4304e+09,  7.9504e+08]], dtype=torch.float64)
	q_value: tensor([[-0.5918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5817801389290358, distance: 0.7400465187094311 entropy 10.37944602035771
epoch: 6, step: 81
	action: tensor([[ 3.5665e+08, -6.2800e+00,  3.2554e+09, -5.2494e+08,  6.2800e+00,
          3.1820e+08, -5.6912e+09]], dtype=torch.float64)
	q_value: tensor([[-0.8611]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7792068830691915, distance: 0.5377116655238396 entropy 10.620445172988616
epoch: 6, step: 82
	action: tensor([[ 5.5307e+09, -6.2800e+00, -3.6689e+09, -1.1703e+09,  6.2800e+00,
         -2.8244e+09,  2.7228e+09]], dtype=torch.float64)
	q_value: tensor([[-0.6991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3910161284412277, distance: 0.893016716733493 entropy 10.622125013663444
epoch: 6, step: 83
	action: tensor([[ 3.9459e+09, -6.2800e+00,  5.9691e+09,  7.3846e+08,  6.2800e+00,
          2.0760e+09, -5.8628e+08]], dtype=torch.float64)
	q_value: tensor([[-1.0511]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44966836676706956, distance: 0.848924234556032 entropy 10.620597195438677
epoch: 6, step: 84
	action: tensor([[ 7.7461e+08, -6.2800e+00,  1.4526e+09,  3.3526e+09,  6.2800e+00,
         -1.1159e+09, -4.4233e+08]], dtype=torch.float64)
	q_value: tensor([[-0.6395]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36560346372525865, distance: 0.9114589101612458 entropy 10.346785887359577
epoch: 6, step: 85
	action: tensor([[-3.5774e+09, -6.2800e+00,  7.6771e+08, -5.6845e+08,  6.2800e+00,
          5.7283e+08, -2.4768e+09]], dtype=torch.float64)
	q_value: tensor([[-0.8228]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7106849184700406, distance: 1.4967239446743628 entropy 10.5554298774487
epoch: 6, step: 86
	action: tensor([[-2.3877e+09, -6.2800e+00, -5.9859e+09, -7.0083e+09,  6.2800e+00,
         -7.2214e+08,  4.2790e+09]], dtype=torch.float64)
	q_value: tensor([[-0.7194]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06479817945059252, distance: 1.1066476473167928 entropy 10.545060895675238
epoch: 6, step: 87
	action: tensor([[ 1.1915e+10, -6.2800e+00,  3.7775e+09, -2.9125e+09,  6.2800e+00,
         -5.4353e+09, -3.2885e+09]], dtype=torch.float64)
	q_value: tensor([[-1.0172]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.40112064977094597, distance: 0.8855770691943912 entropy 10.718264008211765
epoch: 6, step: 88
	action: tensor([[-3.8505e+09, -6.2800e+00,  4.7505e+09,  1.7266e+09,  6.2800e+00,
         -5.5597e+09, -8.4870e+08]], dtype=torch.float64)
	q_value: tensor([[-0.8518]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6099215512552749, distance: 1.451974704284155 entropy 10.49454360718096
epoch: 6, step: 89
	action: tensor([[ 8.3470e+09, -6.2800e+00,  6.8633e+08, -1.5539e+09,  6.2800e+00,
          4.4722e+09,  4.6813e+09]], dtype=torch.float64)
	q_value: tensor([[-0.8716]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.846969738844753, distance: 0.4476569722081461 entropy 10.604365723936128
epoch: 6, step: 90
	action: tensor([[-4.3023e+09, -6.2800e+00,  1.0988e+09,  1.7881e+09,  6.2800e+00,
          4.7192e+09, -3.6696e+09]], dtype=torch.float64)
	q_value: tensor([[-0.7663]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.00496285697862775, distance: 1.1471803480062288 entropy 10.587924446779292
epoch: 6, step: 91
	action: tensor([[ 1.4261e+09, -6.2800e+00,  1.7725e+09, -2.9338e+09,  6.2800e+00,
         -2.0962e+09, -2.5363e+09]], dtype=torch.float64)
	q_value: tensor([[-0.5693]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4182927490254127, distance: 0.8727883324723413 entropy 10.418322410883738
epoch: 6, step: 92
	action: tensor([[-7.6240e+08, -6.2800e+00,  1.2622e+09,  1.2611e+09,  6.2800e+00,
          5.9723e+09,  7.1342e+09]], dtype=torch.float64)
	q_value: tensor([[-0.8554]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.005418880439812979, distance: 1.1474405973358925 entropy 10.62189480179913
epoch: 6, step: 93
	action: tensor([[-2.5346e+09, -6.2800e+00,  2.2513e+09, -2.1242e+09,  6.2800e+00,
         -1.6373e+09,  1.6612e+09]], dtype=torch.float64)
	q_value: tensor([[-0.5143]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.051551800600802444, distance: 1.1144574748746783 entropy 10.324611846191276
epoch: 6, step: 94
	action: tensor([[-1.2873e+09, -6.2800e+00,  5.9074e+09, -1.4202e+09,  6.2800e+00,
         -1.8652e+09,  2.0307e+09]], dtype=torch.float64)
	q_value: tensor([[-0.8681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.056386595629040026, distance: 1.1116133249846594 entropy 10.70293957232715
epoch: 6, step: 95
	action: tensor([[-1.0871e+09, -6.2800e+00, -2.9237e+06,  7.1785e+07,  6.2800e+00,
          1.2866e+09, -7.9327e+09]], dtype=torch.float64)
	q_value: tensor([[-0.9721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06796349991610018, distance: 1.1047732585998267 entropy 10.661725957229265
epoch: 6, step: 96
	action: tensor([[-2.8072e+09, -6.2800e+00, -9.4472e+08, -4.5592e+09,  6.2800e+00,
          2.2764e+09,  4.7376e+09]], dtype=torch.float64)
	q_value: tensor([[-0.5580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7748800507326143, distance: 1.5245483325511409 entropy 10.423534283095387
epoch: 6, step: 97
	action: tensor([[-4.2642e+09, -6.2800e+00,  1.6992e+09,  1.2703e+09,  6.2800e+00,
         -3.2731e+09,  1.1849e+09]], dtype=torch.float64)
	q_value: tensor([[-0.7237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5682112361160316, distance: 1.4330421810549308 entropy 10.514301659655974
epoch: 6, step: 98
	action: tensor([[ 1.7003e+09, -6.2800e+00, -1.9132e+09, -2.6927e+08,  6.2800e+00,
         -1.9549e+09,  1.1282e+09]], dtype=torch.float64)
	q_value: tensor([[-0.6853]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41531619833400624, distance: 0.8750184781979213 entropy 10.453430031454667
epoch: 6, step: 99
	action: tensor([[ 2.2732e+09, -6.2800e+00,  7.3803e+08,  1.6005e+09,  6.2800e+00,
         -8.9260e+08,  3.3575e+09]], dtype=torch.float64)
	q_value: tensor([[-0.7583]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7889509169378365, distance: 0.5257126508441122 entropy 10.62919383282359
epoch: 6, step: 100
	action: tensor([[ 5.1011e+09, -6.2800e+00,  5.8625e+09,  4.6268e+08,  6.2800e+00,
         -9.8429e+08, -1.3372e+09]], dtype=torch.float64)
	q_value: tensor([[-0.7106]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8020353861917717, distance: 0.5091555437485916 entropy 10.436889501149055
epoch: 6, step: 101
	action: tensor([[ 4.1446e+09, -6.2800e+00,  1.8915e+09, -2.2681e+09,  6.2800e+00,
         -1.1898e+09,  3.8183e+08]], dtype=torch.float64)
	q_value: tensor([[-0.7029]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41947479041013225, distance: 0.8719011193979915 entropy 10.35748680865484
epoch: 6, step: 102
	action: tensor([[ 2.3318e+09, -6.2800e+00, -2.6798e+09, -1.4827e+09,  6.2800e+00,
         -9.3511e+08,  8.2222e+07]], dtype=torch.float64)
	q_value: tensor([[-0.7771]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41509689800145055, distance: 0.8751825616329553 entropy 10.463275686013047
epoch: 6, step: 103
	action: tensor([[-2.1124e+09, -6.2800e+00, -6.3679e+09,  3.4605e+09,  6.2800e+00,
         -1.1809e+09,  4.6729e+09]], dtype=torch.float64)
	q_value: tensor([[-0.7716]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6815576617288697, distance: 1.483927131450765 entropy 10.609430812050155
epoch: 6, step: 104
	action: tensor([[-1.3676e+08, -6.2800e+00,  4.7832e+09, -3.4123e+09,  6.2800e+00,
          2.6297e+09, -4.0298e+09]], dtype=torch.float64)
	q_value: tensor([[-1.1884]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9701055359051176, distance: 1.606206721425441 entropy 10.716476500042075
epoch: 6, step: 105
	action: tensor([[ 1.3434e+08, -6.2800e+00,  3.6285e+09,  1.5811e+09,  6.2800e+00,
          7.6180e+08,  4.8961e+09]], dtype=torch.float64)
	q_value: tensor([[-0.7989]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33816642304628086, distance: 0.9309601505464813 entropy 10.537260036344383
epoch: 6, step: 106
	action: tensor([[-2.7777e+09, -6.2800e+00,  3.3241e+09, -2.5773e+09,  6.2800e+00,
          1.2458e+09, -7.1218e+08]], dtype=torch.float64)
	q_value: tensor([[-0.5725]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8545035082598356, distance: 1.5583697984427012 entropy 10.298345951470447
epoch: 6, step: 107
	action: tensor([[ 3.0258e+09, -6.2800e+00, -3.6560e+09,  2.9813e+09,  6.2800e+00,
         -5.1756e+09, -3.6254e+09]], dtype=torch.float64)
	q_value: tensor([[-0.7939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4221637349075662, distance: 0.8698794889336102 entropy 10.643020730694658
epoch: 6, step: 108
	action: tensor([[-1.2624e+09, -6.2800e+00, -2.6144e+09,  2.4761e+08,  6.2800e+00,
          1.3770e+09, -2.2000e+08]], dtype=torch.float64)
	q_value: tensor([[-0.7510]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.031123338170151005, distance: 1.162015715405833 entropy 10.5391886759362
epoch: 6, step: 109
	action: tensor([[-3.0926e+09, -6.2800e+00, -9.1974e+09, -4.5042e+09,  6.2800e+00,
          1.4083e+09,  2.6041e+08]], dtype=torch.float64)
	q_value: tensor([[-0.6453]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.43856300728932385, distance: 1.3725277611327644 entropy 10.430533752802324
epoch: 6, step: 110
	action: tensor([[ 2.7468e+09, -6.2800e+00, -1.7480e+09,  3.2108e+09,  6.2800e+00,
         -3.8635e+09,  3.8329e+09]], dtype=torch.float64)
	q_value: tensor([[-0.8823]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.48053354923714886, distance: 0.8247749182990869 entropy 10.605232073933994
epoch: 6, step: 111
	action: tensor([[ 4.9250e+09, -6.2800e+00, -3.2754e+09,  5.9588e+09,  6.2800e+00,
          5.5978e+09,  2.2792e+09]], dtype=torch.float64)
	q_value: tensor([[-0.8679]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31636341382955446, distance: 0.9461703656153123 entropy 10.429751002055909
epoch: 6, step: 112
	action: tensor([[-2.7091e+09, -6.2800e+00,  1.0383e+09, -1.3609e+09,  6.2800e+00,
          1.8047e+09,  1.5300e+09]], dtype=torch.float64)
	q_value: tensor([[-0.5597]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3353975750545173, distance: 1.322397394565934 entropy 10.266314829197958
epoch: 6, step: 113
	action: tensor([[-1.7260e+09, -6.2800e+00,  7.5171e+09, -5.2254e+09,  6.2800e+00,
         -3.4990e+08, -2.0549e+09]], dtype=torch.float64)
	q_value: tensor([[-0.6084]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06368709783641047, distance: 1.107304837476032 entropy 10.59291553015869
epoch: 6, step: 114
	action: tensor([[-2.5529e+09, -6.2800e+00, -8.4691e+09,  2.0023e+09,  6.2800e+00,
         -4.3294e+09, -5.1756e+09]], dtype=torch.float64)
	q_value: tensor([[-0.8791]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9746926395411089, distance: 1.6080755433768263 entropy 10.730837004735504
epoch: 6, step: 115
	action: tensor([[ 7.1394e+09, -6.2800e+00, -2.6499e+07,  5.0922e+09,  6.2800e+00,
          4.0061e+09, -2.9606e+09]], dtype=torch.float64)
	q_value: tensor([[-0.8990]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3441293837729881, distance: 0.9267567982401922 entropy 10.710254094618195
epoch: 6, step: 116
	action: tensor([[-4.1415e+08, -6.2800e+00,  2.8361e+09, -3.5011e+09,  6.2800e+00,
         -1.0511e+09, -1.5690e+09]], dtype=torch.float64)
	q_value: tensor([[-0.6624]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.043126065677813186, distance: 1.1193967855409575 entropy 10.427323558804256
epoch: 6, step: 117
	action: tensor([[-3.0511e+09, -6.2800e+00, -6.0161e+08, -5.8499e+08,  6.2800e+00,
          1.9982e+09,  3.1156e+09]], dtype=torch.float64)
	q_value: tensor([[-1.0879]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.47411562747437874, distance: 1.3893845602846417 entropy 10.710694592397802
epoch: 6, step: 118
	action: tensor([[ 4.8918e+09, -6.2800e+00, -8.1304e+08, -1.2819e+09,  6.2800e+00,
          1.1476e+10,  1.7482e+09]], dtype=torch.float64)
	q_value: tensor([[-0.6063]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7436584367149774, distance: 0.579383612804975 entropy 10.527003177008172
epoch: 6, step: 119
	action: tensor([[ 1.0580e+09, -6.2800e+00, -5.7345e+09,  2.9613e+09,  6.2800e+00,
          2.6152e+09, -1.4974e+09]], dtype=torch.float64)
	q_value: tensor([[-0.8048]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3239892453264307, distance: 0.940878394451036 entropy 10.530168822982391
epoch: 6, step: 120
	action: tensor([[-7.8289e+08, -6.2800e+00,  4.0596e+09,  1.0818e+09,  6.2800e+00,
          4.3729e+09,  2.2694e+09]], dtype=torch.float64)
	q_value: tensor([[-0.5488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.010114311512734675, distance: 1.1501168214676043 entropy 10.265155622319412
epoch: 6, step: 121
	action: tensor([[-6.2654e+07, -6.2800e+00,  4.7157e+09,  2.1828e+09,  6.2800e+00,
          4.7154e+09,  5.1063e+08]], dtype=torch.float64)
	q_value: tensor([[-0.6218]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.051796016180926996, distance: 1.1143139850134076 entropy 10.39956704625806
epoch: 6, step: 122
	action: tensor([[ 4.5801e+09, -6.2800e+00,  2.9471e+09,  5.0629e+09,  6.2800e+00,
         -2.6190e+09, -9.3512e+08]], dtype=torch.float64)
	q_value: tensor([[-0.7612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7177742188704015, distance: 0.6079320433776428 entropy 10.565877858606854
epoch: 6, step: 123
	action: tensor([[ 7.1234e+08, -6.2800e+00, -6.7586e+09, -4.4651e+08,  6.2800e+00,
          2.3063e+09,  3.7667e+09]], dtype=torch.float64)
	q_value: tensor([[-0.7097]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7507958982240361, distance: 0.5712606193949522 entropy 10.340144219865369
epoch: 6, step: 124
	action: tensor([[ 1.7797e+09, -6.2800e+00,  1.9351e+08,  4.3060e+09,  6.2800e+00,
          5.0894e+09, -1.6686e+09]], dtype=torch.float64)
	q_value: tensor([[-0.7140]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3713069256772975, distance: 0.9073524816218649 entropy 10.517489345952447
epoch: 6, step: 125
	action: tensor([[-1.6098e+09, -6.2800e+00, -6.7152e+07,  1.0365e+09,  6.2800e+00,
          2.7118e+09,  3.5377e+09]], dtype=torch.float64)
	q_value: tensor([[-0.6330]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.01827370949893803, distance: 1.133840339147226 entropy 10.31969763680468
epoch: 6, step: 126
	action: tensor([[-3.5647e+09, -6.2800e+00,  2.7679e+09,  2.9528e+09,  6.2800e+00,
          3.0582e+09, -1.7148e+09]], dtype=torch.float64)
	q_value: tensor([[-0.6143]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04903111336763755, distance: 1.1159374369366772 entropy 10.40554934304381
epoch: 6, step: 127
	action: tensor([[-3.3982e+09, -6.2800e+00,  3.3519e+09,  2.1781e+08,  6.2800e+00,
          2.4436e+09,  3.6968e+08]], dtype=torch.float64)
	q_value: tensor([[-0.8321]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0351453468441314, distance: 1.1642797940437433 entropy 10.540809765604797
LOSS epoch 6 actor 28.260241289492324 critic 171.8249842784798 entropy 0.1
epoch: 7, step: 0
	action: tensor([[ 2.4200e+09, -6.2800e+00, -1.0122e+10, -1.0142e+10,  6.2800e+00,
         -1.7832e+10, -9.3789e+08]], dtype=torch.float64)
	q_value: tensor([[-1.2343]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35896988306713673, distance: 0.9162118622214948 entropy 11.12435606999091
epoch: 7, step: 1
	action: tensor([[ 1.5910e+10, -6.2800e+00, -9.8241e+08,  3.6625e+09,  6.2800e+00,
         -1.0926e+10,  1.8613e+09]], dtype=torch.float64)
	q_value: tensor([[-1.3728]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4802721245609748, distance: 0.8249824287117967 entropy 11.063109883822351
epoch: 7, step: 2
	action: tensor([[-4.9549e+09, -6.2800e+00, -4.6878e+09,  4.4963e+09,  6.2800e+00,
          5.9565e+09, -6.8759e+09]], dtype=torch.float64)
	q_value: tensor([[-1.4239]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.017855423974187046, distance: 1.1340818619109911 entropy 10.983148866906175
epoch: 7, step: 3
	action: tensor([[-9.1953e+09, -6.2800e+00, -6.4094e+09,  9.0377e+09,  6.2800e+00,
          2.7259e+09,  4.4302e+09]], dtype=torch.float64)
	q_value: tensor([[-1.5463]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.02121566173246303, distance: 1.1321401672463525 entropy 11.181239799070013
epoch: 7, step: 4
	action: tensor([[ 2.2892e+09, -6.2800e+00,  1.9492e+09,  7.4560e+08,  6.2800e+00,
         -7.9594e+09, -4.2035e+09]], dtype=torch.float64)
	q_value: tensor([[-1.3226]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33618800151024164, distance: 0.9323505735169256 entropy 11.076818939593599
epoch: 7, step: 5
	action: tensor([[-1.4805e+09, -6.2800e+00,  7.4866e+08, -1.2903e+09,  6.2800e+00,
         -8.2195e+08, -1.6764e+09]], dtype=torch.float64)
	q_value: tensor([[-1.4451]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.02832121393915399, distance: 1.1604357261643088 entropy 10.999639199073444
epoch: 7, step: 6
	action: tensor([[-1.6820e+09, -6.2800e+00,  2.0032e+10,  1.3030e+10,  6.2800e+00,
         -1.6292e+10, -6.9379e+08]], dtype=torch.float64)
	q_value: tensor([[-1.3920]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6647921841727038, distance: 1.4765110700745245 entropy 11.342744113031825
epoch: 7, step: 7
	action: tensor([[-1.7501e+09, -6.2800e+00, -1.4442e+09,  1.0043e+10,  6.2800e+00,
          9.3175e+09, -8.2053e+09]], dtype=torch.float64)
	q_value: tensor([[-1.4425]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.030901363026322293, distance: 1.126524612337904 entropy 11.140153079124104
epoch: 7, step: 8
	action: tensor([[ 3.9381e+09, -6.2800e+00,  9.7218e+09, -1.8981e+09,  6.2800e+00,
         -5.4683e+09, -8.3279e+09]], dtype=torch.float64)
	q_value: tensor([[-1.5386]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2878990459568611, distance: 0.9656671927446128 entropy 11.176391712140349
epoch: 7, step: 9
	action: tensor([[-1.8008e+08, -6.2800e+00, -3.6664e+09,  1.1248e+10,  6.2800e+00,
         -3.8841e+09, -1.3692e+10]], dtype=torch.float64)
	q_value: tensor([[-1.3169]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7539925724268433, distance: 1.515551044641178 entropy 11.198775073687184
epoch: 7, step: 10
	action: tensor([[ 7.4489e+09, -6.2800e+00, -1.5169e+09,  6.1997e+09,  6.2800e+00,
         -1.1719e+10,  1.2086e+09]], dtype=torch.float64)
	q_value: tensor([[-1.5634]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6705698567833506, distance: 0.656807889450552 entropy 11.192264511255676
epoch: 7, step: 11
	action: tensor([[ 8.5696e+09, -6.2800e+00, -1.8786e+09, -7.4019e+09,  6.2800e+00,
         -7.3953e+08, -6.9653e+09]], dtype=torch.float64)
	q_value: tensor([[-1.2931]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3312290612493163, distance: 0.9358266089886939 entropy 11.065416303478864
epoch: 7, step: 12
	action: tensor([[-1.1752e+10, -6.2800e+00, -1.3754e+10, -2.7179e+08,  6.2800e+00,
         -1.8609e+09,  5.6638e+09]], dtype=torch.float64)
	q_value: tensor([[-1.6973]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.00016018607363110782, distance: 1.1444359043363352 entropy 11.19402957973712
epoch: 7, step: 13
	action: tensor([[-1.4114e+10, -6.2800e+00, -1.1590e+10,  1.0813e+10,  6.2800e+00,
         -2.2082e+10,  9.0379e+09]], dtype=torch.float64)
	q_value: tensor([[-1.4162]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.30084402324895465, distance: 1.3051766821660598 entropy 11.321313064974364
epoch: 7, step: 14
	action: tensor([[-1.0253e+10, -6.2800e+00, -2.8439e+09, -1.2629e+10,  6.2800e+00,
         -2.5060e+09, -7.9033e+09]], dtype=torch.float64)
	q_value: tensor([[-1.5613]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.030008931035071962, distance: 1.127043194873293 entropy 11.152397922476387
epoch: 7, step: 15
	action: tensor([[ 1.9129e+10, -6.2800e+00,  3.6111e+09, -1.8514e+10,  6.2800e+00,
         -1.5203e+10, -7.6865e+09]], dtype=torch.float64)
	q_value: tensor([[-1.6380]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33720339687997314, distance: 0.9316372188588433 entropy 11.27425526873179
epoch: 7, step: 16
	action: tensor([[-1.9459e+09, -6.2800e+00, -5.1075e+09,  5.7412e+09,  6.2800e+00,
         -7.1952e+09, -7.2944e+08]], dtype=torch.float64)
	q_value: tensor([[-1.5478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3405356229129224, distance: 1.324938966505738 entropy 11.125598521181416
epoch: 7, step: 17
	action: tensor([[ 1.5535e+10, -6.2800e+00, -3.0784e+09, -1.0114e+10,  6.2800e+00,
         -8.3442e+09, -3.7927e+09]], dtype=torch.float64)
	q_value: tensor([[-1.3627]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2592945004228323, distance: 0.9848712908282399 entropy 11.149856759414613
epoch: 7, step: 18
	action: tensor([[-1.0738e+10, -6.2800e+00,  6.2289e+09, -2.6796e+09,  6.2800e+00,
         -1.4231e+09,  5.5346e+09]], dtype=torch.float64)
	q_value: tensor([[-1.7113]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.023453569823172926, distance: 1.130845154939003 entropy 11.227744252120717
epoch: 7, step: 19
	action: tensor([[-1.3630e+10, -6.2800e+00,  1.0988e+10,  8.6453e+09,  6.2800e+00,
         -6.7930e+09,  1.1677e+09]], dtype=torch.float64)
	q_value: tensor([[-1.5007]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.880596146357824, distance: 1.5692945416820436 entropy 11.242113420798358
epoch: 7, step: 20
	action: tensor([[ 7.8589e+08, -6.2800e+00,  4.2427e+09,  8.0725e+09,  6.2800e+00,
         -1.2502e+10, -9.3727e+09]], dtype=torch.float64)
	q_value: tensor([[-1.3790]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32354057809815107, distance: 0.9411905723846566 entropy 11.15769006938505
epoch: 7, step: 21
	action: tensor([[-9.2402e+09, -6.2800e+00,  3.7199e+09, -1.1879e+10,  6.2800e+00,
          1.0158e+10,  1.7615e+09]], dtype=torch.float64)
	q_value: tensor([[-1.7731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3703291299745761, distance: 1.3395814908783015 entropy 11.234245425544582
epoch: 7, step: 22
	action: tensor([[ 1.7552e+10, -6.2800e+00, -4.8134e+09,  1.2136e+10,  6.2800e+00,
         -3.6878e+09, -1.4503e+10]], dtype=torch.float64)
	q_value: tensor([[-1.3701]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12212966470598852, distance: 1.0721903154315073 entropy 11.285864205722305
epoch: 7, step: 23
	action: tensor([[ 8.2907e+09, -6.2800e+00,  5.7193e+09, -1.3533e+10,  6.2800e+00,
         -1.2202e+10,  1.3749e+09]], dtype=torch.float64)
	q_value: tensor([[-1.5939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24351792670042083, distance: 0.9953046055467502 entropy 11.16697386472245
epoch: 7, step: 24
	action: tensor([[ 4.4500e+09, -6.2800e+00,  4.2025e+08,  1.0274e+09,  6.2800e+00,
         -7.5250e+09,  9.2003e+09]], dtype=torch.float64)
	q_value: tensor([[-1.5909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6939115173480292, distance: 0.633111478544622 entropy 11.254461278338908
epoch: 7, step: 25
	action: tensor([[-2.2760e+09, -6.2800e+00,  4.3083e+09, -4.3293e+09,  6.2800e+00,
         -4.6674e+09, -3.7032e+09]], dtype=torch.float64)
	q_value: tensor([[-1.3429]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05837794060087764, distance: 1.1104397645551276 entropy 11.107723010676938
epoch: 7, step: 26
	action: tensor([[ 1.1955e+10, -6.2800e+00, -6.1140e+09, -1.1995e+10,  6.2800e+00,
         -4.3043e+09, -8.5055e+08]], dtype=torch.float64)
	q_value: tensor([[-1.3579]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30472347689823875, distance: 0.9541913512921965 entropy 11.186099587793949
epoch: 7, step: 27
	action: tensor([[-1.0107e+10, -6.2800e+00,  1.8476e+10,  6.4277e+09,  6.2800e+00,
          5.8070e+09,  9.9197e+09]], dtype=torch.float64)
	q_value: tensor([[-1.3244]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.011776373473402435, distance: 1.1510626449808092 entropy 11.238778221895359
epoch: 7, step: 28
	action: tensor([[-5.1028e+09, -6.2800e+00,  6.5741e+09, -1.0955e+10,  6.2800e+00,
         -1.1621e+10, -7.8470e+09]], dtype=torch.float64)
	q_value: tensor([[-1.2997]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03533628637296227, distance: 1.1239439775429811 entropy 11.125441176280514
epoch: 7, step: 29
	action: tensor([[-3.8587e+09, -6.2800e+00,  3.0649e+09,  6.5826e+09,  6.2800e+00,
          3.3054e+09, -1.3177e+10]], dtype=torch.float64)
	q_value: tensor([[-1.8150]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.020621212655745236, distance: 1.1560829294804196 entropy 11.292609571486478
epoch: 7, step: 30
	action: tensor([[ 9.3423e+09, -6.2800e+00,  1.0135e+10, -4.6982e+09,  6.2800e+00,
         -2.5790e+08,  3.4767e+09]], dtype=torch.float64)
	q_value: tensor([[-0.9371]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3552661613743119, distance: 0.9188548786809355 entropy 10.938386735998202
epoch: 7, step: 31
	action: tensor([[ 9.6185e+08, -6.2800e+00,  5.7797e+09, -1.5742e+09,  6.2800e+00,
          2.0414e+08,  6.6567e+09]], dtype=torch.float64)
	q_value: tensor([[-1.2044]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12259314620656792, distance: 1.0719072406054455 entropy 11.103602386964331
epoch: 7, step: 32
	action: tensor([[-2.9634e+09, -6.2800e+00,  3.8155e+09,  7.9078e+09,  6.2800e+00,
         -1.4638e+10, -7.5127e+09]], dtype=torch.float64)
	q_value: tensor([[-1.0998]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.019247177945898, distance: 1.6261156725101726 entropy 10.972959745492687
epoch: 7, step: 33
	action: tensor([[-2.2561e+09, -6.2800e+00, -1.1053e+10, -3.4524e+09,  6.2800e+00,
          8.4106e+08, -9.3516e+09]], dtype=torch.float64)
	q_value: tensor([[-1.1648]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6114328573289616, distance: 1.4526560615337725 entropy 11.13496082423626
epoch: 7, step: 34
	action: tensor([[ 7.1222e+08, -6.2800e+00, -5.4650e+09, -7.0743e+08,  6.2800e+00,
          3.5591e+09,  6.5155e+08]], dtype=torch.float64)
	q_value: tensor([[-1.3889]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39719828865966633, distance: 0.8884723803243374 entropy 11.183902287691366
epoch: 7, step: 35
	action: tensor([[-5.4695e+09, -6.2800e+00,  4.2756e+09, -1.0867e+10,  6.2800e+00,
          1.6101e+08, -2.3334e+09]], dtype=torch.float64)
	q_value: tensor([[-1.3974]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5390349317765366, distance: 1.41964883988289 entropy 11.176079537364815
epoch: 7, step: 36
	action: tensor([[ 1.8334e+09, -6.2800e+00,  1.6001e+10, -4.8190e+09,  6.2800e+00,
          1.1981e+09, -2.3046e+09]], dtype=torch.float64)
	q_value: tensor([[-1.3900]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.47552663135561346, distance: 0.8287402149310119 entropy 11.17224230562928
epoch: 7, step: 37
	action: tensor([[ 3.4849e+09, -6.2800e+00, -1.5206e+09,  1.8265e+09,  6.2800e+00,
         -6.0473e+09,  6.2132e+09]], dtype=torch.float64)
	q_value: tensor([[-1.3315]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.052442796063393415, distance: 1.1139338775552783 entropy 11.247089461912575
epoch: 7, step: 38
	action: tensor([[-2.9872e+09, -6.2800e+00,  1.2185e+10, -4.1582e+08,  6.2800e+00,
          1.5802e+09, -3.6562e+09]], dtype=torch.float64)
	q_value: tensor([[-1.4487]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8494189086521042, distance: 1.5562319957649737 entropy 11.15126461827443
epoch: 7, step: 39
	action: tensor([[-7.1352e+08, -6.2800e+00,  7.7576e+09, -1.2938e+09,  6.2800e+00,
         -8.8058e+09, -7.7191e+09]], dtype=torch.float64)
	q_value: tensor([[-1.0581]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04087917923687434, distance: 1.1674999051989547 entropy 11.0318115666303
epoch: 7, step: 40
	action: tensor([[ 5.3114e+08, -6.2800e+00,  5.8969e+09,  7.7765e+09,  6.2800e+00,
         -7.2951e+09, -2.4923e+09]], dtype=torch.float64)
	q_value: tensor([[-1.3378]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7312520221006527, distance: 0.5932384559732357 entropy 11.316460331261748
epoch: 7, step: 41
	action: tensor([[-3.9154e+09, -6.2800e+00,  6.3743e+09, -1.1053e+08,  6.2800e+00,
         -3.8616e+09, -6.1520e+09]], dtype=torch.float64)
	q_value: tensor([[-1.3799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.02064470785174577, distance: 1.156096236199094 entropy 10.989317387900977
epoch: 7, step: 42
	action: tensor([[ 2.9010e+09, -6.2800e+00, -1.8129e+10, -1.0100e+10,  6.2800e+00,
          2.6602e+10,  2.3158e+09]], dtype=torch.float64)
	q_value: tensor([[-1.9868]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2074326400274702, distance: 1.0187668028199919 entropy 11.339652381654101
epoch: 7, step: 43
	action: tensor([[ 1.3605e+09, -6.2800e+00, -1.1784e+10,  1.3347e+09,  6.2800e+00,
          8.9216e+09,  7.4231e+09]], dtype=torch.float64)
	q_value: tensor([[-1.3189]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36961252442183634, distance: 0.9085743693435989 entropy 11.106923474691289
epoch: 7, step: 44
	action: tensor([[ 4.6499e+09, -6.2800e+00,  9.1556e+09, -9.5224e+08,  6.2800e+00,
          4.0391e+08,  1.0575e+10]], dtype=torch.float64)
	q_value: tensor([[-1.4004]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15490091454244603, distance: 1.0519873296847129 entropy 11.144780645086257
epoch: 7, step: 45
	action: tensor([[ 1.3627e+10, -6.2800e+00, -5.6796e+09, -8.0654e+08,  6.2800e+00,
          3.1103e+09, -5.3352e+08]], dtype=torch.float64)
	q_value: tensor([[-1.1001]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7431917284013305, distance: 0.5799108003978702 entropy 10.996780103514263
epoch: 7, step: 46
	action: tensor([[ 6.3712e+09, -6.2800e+00,  1.4593e+10, -8.0427e+09,  6.2800e+00,
          1.0222e+09,  2.3480e+09]], dtype=torch.float64)
	q_value: tensor([[-1.6954]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6222012059312066, distance: 0.7033750789860485 entropy 11.221773093126659
epoch: 7, step: 47
	action: tensor([[-1.0938e+10, -6.2800e+00,  1.6230e+10,  5.6376e+09,  6.2800e+00,
          1.3721e+10,  5.9135e+09]], dtype=torch.float64)
	q_value: tensor([[-1.3912]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.016665429122399766, distance: 1.1538403473893435 entropy 11.149476195939485
epoch: 7, step: 48
	action: tensor([[-1.4075e+09, -6.2800e+00,  1.0313e+10, -5.4629e+09,  6.2800e+00,
          6.2844e+09, -2.3998e+09]], dtype=torch.float64)
	q_value: tensor([[-1.3968]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.42445145337201584, distance: 1.3657792793018582 entropy 11.172372680507564
epoch: 7, step: 49
	action: tensor([[-2.9564e+09, -6.2800e+00, -1.9154e+10, -5.3710e+09,  6.2800e+00,
          1.5341e+10, -1.8465e+09]], dtype=torch.float64)
	q_value: tensor([[-1.3676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4111869887098021, distance: 1.3594053508710169 entropy 11.263148920605092
epoch: 7, step: 50
	action: tensor([[ 1.8915e+10, -6.2800e+00, -9.6864e+09, -1.2657e+10,  6.2800e+00,
          4.3217e+09,  3.4789e+09]], dtype=torch.float64)
	q_value: tensor([[-1.2164]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6298633354232426, distance: 0.6962059760065623 entropy 11.212483957049871
epoch: 7, step: 51
	action: tensor([[ 2.3276e+09, -6.2800e+00, -4.5416e+09,  4.4212e+06,  6.2800e+00,
         -3.6959e+09,  6.8929e+09]], dtype=torch.float64)
	q_value: tensor([[-1.3024]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0943976559658607, distance: 1.088993938085182 entropy 11.240891348024784
epoch: 7, step: 52
	action: tensor([[ 1.6986e+08, -6.2800e+00,  2.7890e+08,  1.0945e+10,  6.2800e+00,
         -1.5380e+10, -5.0439e+09]], dtype=torch.float64)
	q_value: tensor([[-1.4749]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4175269164478188, distance: 0.8733626676617927 entropy 11.124218090911038
epoch: 7, step: 53
	action: tensor([[-8.3296e+08, -6.2800e+00, -2.0931e+09, -8.3565e+09,  6.2800e+00,
         -1.0661e+10,  6.6273e+08]], dtype=torch.float64)
	q_value: tensor([[-1.2589]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.004181057667157706, distance: 1.146734043302392 entropy 11.005087323205661
epoch: 7, step: 54
	action: tensor([[ 1.7723e+09, -6.2800e+00,  3.2090e+09,  5.0856e+09,  6.2800e+00,
         -1.0976e+10, -3.2070e+09]], dtype=torch.float64)
	q_value: tensor([[-1.5692]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13646146632186507, distance: 1.0634021993575005 entropy 11.338090186408605
epoch: 7, step: 55
	action: tensor([[ 1.4448e+08, -6.2800e+00,  1.0415e+10,  7.9246e+09,  6.2800e+00,
          2.8018e+09,  5.9677e+08]], dtype=torch.float64)
	q_value: tensor([[-1.4596]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34541456682847493, distance: 0.9258483597880994 entropy 11.163944281933803
epoch: 7, step: 56
	action: tensor([[ 1.2175e+09, -6.2800e+00,  4.6539e+08,  1.1906e+10,  6.2800e+00,
         -5.9045e+09,  2.1094e+09]], dtype=torch.float64)
	q_value: tensor([[-1.1665]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6291165302621047, distance: 0.6969079710683187 entropy 11.006987965713408
epoch: 7, step: 57
	action: tensor([[-5.9304e+09, -6.2800e+00, -1.8084e+10,  9.1202e+09,  6.2800e+00,
          5.0172e+09, -2.4604e+09]], dtype=torch.float64)
	q_value: tensor([[-1.3786]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.013990854916952067, distance: 1.1363108793649659 entropy 11.12119015395476
epoch: 7, step: 58
	action: tensor([[-1.3671e+09, -6.2800e+00,  4.2069e+09,  1.2984e+10,  6.2800e+00,
          3.7125e+08, -8.5280e+08]], dtype=torch.float64)
	q_value: tensor([[-1.0253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06943193546290205, distance: 1.1834046398692397 entropy 11.03244099826043
epoch: 7, step: 59
	action: tensor([[-4.1778e+09, -6.2800e+00,  1.7293e+10, -2.0217e+10,  6.2800e+00,
         -2.3737e+09,  9.5511e+09]], dtype=torch.float64)
	q_value: tensor([[-1.5123]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09746626710153161, distance: 1.1988153174561582 entropy 11.164401686596154
epoch: 7, step: 60
	action: tensor([[ 4.8368e+09, -6.2800e+00,  4.7878e+09, -5.4799e+09,  6.2800e+00,
         -7.7802e+09,  1.3203e+10]], dtype=torch.float64)
	q_value: tensor([[-1.2668]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3882801545586684, distance: 0.8950204909404743 entropy 11.253215842160577
epoch: 7, step: 61
	action: tensor([[ 7.9866e+09, -6.2800e+00, -3.2323e+09, -9.0087e+09,  6.2800e+00,
          4.7333e+09,  3.6726e+09]], dtype=torch.float64)
	q_value: tensor([[-1.1999]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10573830253546124, distance: 1.0821538480813877 entropy 11.087111470939032
epoch: 7, step: 62
	action: tensor([[-1.6664e+10, -6.2800e+00, -1.0005e+09,  1.2464e+10,  6.2800e+00,
          6.6935e+09,  4.9303e+09]], dtype=torch.float64)
	q_value: tensor([[-1.3756]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.012984631068703512, distance: 1.1368905350173681 entropy 11.12449897905788
epoch: 7, step: 63
	action: tensor([[ 9.1191e+09, -6.2800e+00, -7.1707e+08, -6.7079e+09,  6.2800e+00,
         -8.6634e+09,  5.5103e+09]], dtype=torch.float64)
	q_value: tensor([[-1.1278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3246914889960617, distance: 0.9403895726590538 entropy 11.111559176998611
epoch: 7, step: 64
	action: tensor([[ 6.9351e+08, -6.2800e+00, -1.6535e+09, -7.4769e+09,  6.2800e+00,
         -8.7940e+09,  2.4267e+09]], dtype=torch.float64)
	q_value: tensor([[-1.6006]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3544135973276712, distance: 0.9194622020299137 entropy 11.209612112380224
epoch: 7, step: 65
	action: tensor([[-1.9644e+10, -6.2800e+00, -3.1698e+09,  4.5531e+07,  6.2800e+00,
          4.1731e+09,  6.8154e+09]], dtype=torch.float64)
	q_value: tensor([[-1.8923]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03812778531230987, distance: 1.1223165910007258 entropy 11.309950708044829
epoch: 7, step: 66
	action: tensor([[-1.0902e+09, -6.2800e+00, -6.1833e+09,  1.4778e+09,  6.2800e+00,
         -6.8187e+09,  7.9521e+09]], dtype=torch.float64)
	q_value: tensor([[-1.1347]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6497477255238215, distance: 1.46982443299195 entropy 11.155607821014867
epoch: 7, step: 67
	action: tensor([[-2.4520e+09, -6.2800e+00, -7.8880e+09, -4.5259e+09,  6.2800e+00,
          3.5942e+09,  6.1263e+09]], dtype=torch.float64)
	q_value: tensor([[-1.6399]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.44294248905877787, distance: 1.3746153973256603 entropy 11.197573841535306
epoch: 7, step: 68
	action: tensor([[ 1.3834e+09, -6.2800e+00,  1.1498e+10, -5.7619e+09,  6.2800e+00,
         -8.3910e+09, -2.5465e+09]], dtype=torch.float64)
	q_value: tensor([[-1.2016]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 11.220263101262463
epoch: 7, step: 69
	action: tensor([[ 6.4466e+08, -6.2800e+00,  1.1239e+10, -4.9878e+09,  6.2800e+00,
          2.9492e+09,  1.8042e+09]], dtype=torch.float64)
	q_value: tensor([[-0.8391]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.9621225645809
epoch: 7, step: 70
	action: tensor([[ 8.4902e+09, -6.2800e+00,  7.2580e+08, -3.3053e+09,  6.2800e+00,
          6.1947e+09,  6.8689e+09]], dtype=torch.float64)
	q_value: tensor([[-0.8391]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 10.9621225645809
epoch: 7, step: 71
	action: tensor([[-7.5158e+09, -6.2800e+00, -3.4763e+08, -1.4826e+08,  6.2800e+00,
         -5.9744e+09,  1.3216e+07]], dtype=torch.float64)
	q_value: tensor([[-0.8391]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07178699052646831, distance: 1.1025048757589568 entropy 10.9621225645809
