epoch: 0, step: 0
	action: tensor([[ 0.0047,  0.1114, -0.0047, -0.0034, -0.0335, -0.0875,  0.0757]],
       dtype=torch.float64)
	q_value: tensor([[-0.0809]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35211838914278726, distance: 0.9210952017249456 entropy tensor([[ -3.4480, -21.6069, -21.6069,  -2.5708, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 1
	action: tensor([[ 0.0280,  0.0696, -0.0145,  0.0312, -0.0440, -0.0081,  0.0310]],
       dtype=torch.float64)
	q_value: tensor([[-0.0674]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.377518415398734, distance: 0.9028590324395588 entropy tensor([[ -2.4011, -21.6069, -21.6069,  -4.1194,  -3.3452, -21.6069,  -3.7741]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 2
	action: tensor([[-0.0205,  0.0673, -0.0130,  0.0315, -0.0316, -0.0090,  0.0286]],
       dtype=torch.float64)
	q_value: tensor([[-0.0657]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3261423659553684, distance: 0.9393788322598652 entropy tensor([[ -2.2662, -21.6069, -21.6069,  -4.5013,  -3.6642, -21.6069,  -4.1771]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 3
	action: tensor([[ 0.0081,  0.0674, -0.0136,  0.0373, -0.0308, -0.0092,  0.0274]],
       dtype=torch.float64)
	q_value: tensor([[-0.0674]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3582997980579128, distance: 0.9166906068701658 entropy tensor([[ -2.3325, -21.6069, -21.6069,  -4.5233,  -3.6328, -21.6069,  -4.2035]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 4
	action: tensor([[-0.0251,  0.0672, -0.0132,  0.0295, -0.0300, -0.0089,  0.0271]],
       dtype=torch.float64)
	q_value: tensor([[-0.0663]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32040909643927007, distance: 0.9433665476465595 entropy tensor([[ -2.2923, -21.6069, -21.6069,  -4.5619,  -3.6494, -21.6069,  -4.2486]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 5
	action: tensor([[-0.0297,  0.0673, -0.0136,  0.0292, -0.0339, -0.0093,  0.0290]],
       dtype=torch.float64)
	q_value: tensor([[-0.0675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3154150026700455, distance: 0.9468264506905693 entropy tensor([[ -2.3401, -21.6069, -21.6069,  -4.5328,  -3.6451, -21.6069,  -4.2191]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 6
	action: tensor([[-0.0460,  0.0673, -0.0137,  0.0340, -0.0427, -0.0093,  0.0302]],
       dtype=torch.float64)
	q_value: tensor([[-0.0676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3003104570188715, distance: 0.9572147561115425 entropy tensor([[ -2.3455, -21.6069, -21.6069,  -4.5530,  -3.6507, -21.6069,  -4.2070]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 7
	action: tensor([[-0.0104,  0.0675, -0.0142,  0.0359, -0.0397, -0.0092,  0.0294]],
       dtype=torch.float64)
	q_value: tensor([[-0.0680]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3387443648885582, distance: 0.9305535843380682 entropy tensor([[ -2.3645, -21.6069, -21.6069,  -4.6023,  -3.6403, -21.6069,  -4.2040]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 8
	action: tensor([[-0.0421,  0.0674, -0.0137,  0.0309, -0.0326, -0.0089,  0.0192]],
       dtype=torch.float64)
	q_value: tensor([[-0.0669]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3032273915506194, distance: 0.9552174067760552 entropy tensor([[ -2.3131, -21.6069, -21.6069,  -4.5467,  -3.6464, -21.6069,  -4.1835]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 9
	action: tensor([[-0.0170,  0.0671, -0.0139,  0.0331, -0.0266, -0.0093,  0.0212]],
       dtype=torch.float64)
	q_value: tensor([[-0.0676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.330295878659705, distance: 0.9364792933512602 entropy tensor([[ -2.3612, -21.6069, -21.6069,  -4.6241,  -3.6791, -21.6069,  -4.2654]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 10
	action: tensor([[-2.2534e-05,  6.7131e-02, -1.3359e-02,  2.8364e-02, -4.7758e-02,
         -9.2714e-03,  2.4028e-02]], dtype=torch.float64)
	q_value: tensor([[-0.0671]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3459327614787654, distance: 0.9254818188972759 entropy tensor([[ -2.3284, -21.6069, -21.6069,  -4.5695,  -3.6793, -21.6069,  -4.2908]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 11
	action: tensor([[ 0.0007,  0.0671, -0.0134,  0.0312, -0.0356, -0.0092,  0.0221]],
       dtype=torch.float64)
	q_value: tensor([[-0.0667]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34784653983242586, distance: 0.9241268629693642 entropy tensor([[ -2.2973, -21.6069, -21.6069,  -4.5992,  -3.7236, -21.6069,  -4.1986]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 12
	action: tensor([[-0.0386,  0.0671, -0.0132,  0.0268, -0.0333, -0.0092,  0.0289]],
       dtype=torch.float64)
	q_value: tensor([[-0.0667]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3048965749952808, distance: 0.9540725647496219 entropy tensor([[ -2.3017, -21.6069, -21.6069,  -4.5689,  -3.6943, -21.6069,  -4.2411]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 13
	action: tensor([[ 0.0062,  0.0674, -0.0138,  0.0318, -0.0440, -0.0096,  0.0210]],
       dtype=torch.float64)
	q_value: tensor([[-0.0679]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35392644051188327, distance: 0.9198090477879088 entropy tensor([[ -2.3609, -21.6069, -21.6069,  -4.5572,  -3.6564, -21.6069,  -4.2348]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 14
	action: tensor([[ 0.0030,  0.0670, -0.0133,  0.0283, -0.0333, -0.0091,  0.0249]],
       dtype=torch.float64)
	q_value: tensor([[-0.0664]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3489910625456236, distance: 0.923315590229829 entropy tensor([[ -2.2891, -21.6069, -21.6069,  -4.6061,  -3.7253, -21.6069,  -4.2326]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 15
	action: tensor([[-0.0263,  0.0671, -0.0132,  0.0318, -0.0309, -0.0092,  0.0217]],
       dtype=torch.float64)
	q_value: tensor([[-0.0667]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.320012125907843, distance: 0.9436420325346586 entropy tensor([[ -2.2997, -21.6069, -21.6069,  -4.5365,  -3.6729, -21.6069,  -4.2078]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 16
	action: tensor([[-0.0042,  0.0672, -0.0136,  0.0289, -0.0374, -0.0092,  0.0287]],
       dtype=torch.float64)
	q_value: tensor([[-0.0673]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34182177972380223, distance: 0.9283857094130685 entropy tensor([[ -2.3387, -21.6069, -21.6069,  -4.5742,  -3.6708, -21.6069,  -4.2493]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 17
	action: tensor([[ 0.0133,  0.0672, -0.0134,  0.0316, -0.0374, -0.0092,  0.0319]],
       dtype=torch.float64)
	q_value: tensor([[-0.0669]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36098819005260907, distance: 0.9147683616628803 entropy tensor([[ -2.3081, -21.6069, -21.6069,  -4.5396,  -3.6689, -21.6069,  -4.1873]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 18
	action: tensor([[-0.0258,  0.0672, -0.0132,  0.0318, -0.0290, -0.0090,  0.0223]],
       dtype=torch.float64)
	q_value: tensor([[-0.0662]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3206766279412214, distance: 0.943180843930322 entropy tensor([[ -2.2846, -21.6069, -21.6069,  -4.5235,  -3.6499, -21.6069,  -4.1802]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 19
	action: tensor([[ 0.0140,  0.0672, -0.0135,  0.0301, -0.0286, -0.0094,  0.0249]],
       dtype=torch.float64)
	q_value: tensor([[-0.0674]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36102201637534037, distance: 0.9147441495443771 entropy tensor([[ -2.3407, -21.6069, -21.6069,  -4.5637,  -3.6757, -21.6069,  -4.2690]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 20
	action: tensor([[-0.0021,  0.0670, -0.0129,  0.0323, -0.0296, -0.0093,  0.0314]],
       dtype=torch.float64)
	q_value: tensor([[-0.0662]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3454051498967682, distance: 0.9258550194378383 entropy tensor([[ -2.2875, -21.6069, -21.6069,  -4.5645,  -3.6862, -21.6069,  -4.2676]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 21
	action: tensor([[ 0.0223,  0.0673, -0.0132,  0.0303, -0.0401, -0.0092,  0.0221]],
       dtype=torch.float64)
	q_value: tensor([[-0.0670]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36958973946064944, distance: 0.9085907891214995 entropy tensor([[ -2.3098, -21.6069, -21.6069,  -4.5215,  -3.6398, -21.6069,  -4.2314]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 22
	action: tensor([[-0.0745,  0.0669, -0.0131,  0.0282, -0.0360, -0.0090,  0.0209]],
       dtype=torch.float64)
	q_value: tensor([[-0.0658]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26667593693593716, distance: 0.9799516788672554 entropy tensor([[ -2.2701, -21.6069, -21.6069,  -4.5713,  -3.7062, -21.6069,  -4.2220]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 23
	action: tensor([[ 0.0191,  0.0671, -0.0143,  0.0299, -0.0421, -0.0097,  0.0241]],
       dtype=torch.float64)
	q_value: tensor([[-0.0685]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3659388905554014, distance: 0.9112179187793668 entropy tensor([[ -2.4128, -21.6069, -21.6069,  -4.7221,  -3.6433, -21.6069,  -4.2968]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 24
	action: tensor([[ 0.0171,  0.0670, -0.0131,  0.0304, -0.0409, -0.0092,  0.0316]],
       dtype=torch.float64)
	q_value: tensor([[-0.0660]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36419547765755145, distance: 0.9124698001239048 entropy tensor([[ -2.2748, -21.6069, -21.6069,  -4.5856,  -3.7125, -21.6069,  -4.2223]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 25
	action: tensor([[-0.0004,  0.0672, -0.0132,  0.0356, -0.0398, -0.0090,  0.0261]],
       dtype=torch.float64)
	q_value: tensor([[-0.0661]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3487743834478011, distance: 0.9234692337002238 entropy tensor([[ -2.2783, -21.6069, -21.6069,  -4.5229,  -3.6577, -21.6069,  -4.1741]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 26
	action: tensor([[ 0.0175,  0.0672, -0.0134,  0.0305, -0.0369, -0.0090,  0.0274]],
       dtype=torch.float64)
	q_value: tensor([[-0.0667]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3648905169036477, distance: 0.9119709237268242 entropy tensor([[ -2.3000, -21.6069, -21.6069,  -4.5699,  -3.6739, -21.6069,  -4.2197]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 27
	action: tensor([[ 0.0038,  0.0671, -0.0130,  0.0286, -0.0303, -0.0091,  0.0268]],
       dtype=torch.float64)
	q_value: tensor([[-0.0661]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34987541141658296, distance: 0.9226882481599584 entropy tensor([[ -2.2798, -21.6069, -21.6069,  -4.5578,  -3.6812, -21.6069,  -4.2232]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 28
	action: tensor([[ 0.0170,  0.0671, -0.0131,  0.0300, -0.0251, -0.0092,  0.0195]],
       dtype=torch.float64)
	q_value: tensor([[-0.0667]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36394527724563874, distance: 0.912649319029625 entropy tensor([[ -2.3007, -21.6069, -21.6069,  -4.5229,  -3.6572, -21.6069,  -4.2110]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 29
	action: tensor([[-0.0326,  0.0668, -0.0128,  0.0293, -0.0379, -0.0092,  0.0258]],
       dtype=torch.float64)
	q_value: tensor([[-0.0659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3120924613170655, distance: 0.9491213171198724 entropy tensor([[ -2.2827, -21.6069, -21.6069,  -4.5715,  -3.6916, -21.6069,  -4.2865]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 30
	action: tensor([[ 0.0043,  0.0673, -0.0138,  0.0344, -0.0398, -0.0093,  0.0245]],
       dtype=torch.float64)
	q_value: tensor([[-0.0675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3530088569736547, distance: 0.9204619937011502 entropy tensor([[ -2.3471, -21.6069, -21.6069,  -4.5831,  -3.6662, -21.6069,  -4.2081]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 31
	action: tensor([[-0.0139,  0.0671, -0.0133,  0.0321, -0.0346, -0.0090,  0.0231]],
       dtype=torch.float64)
	q_value: tensor([[-0.0665]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33315281282645737, distance: 0.9344796646638371 entropy tensor([[ -2.2932, -21.6069, -21.6069,  -4.5871,  -3.6960, -21.6069,  -4.2319]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 32
	action: tensor([[-0.0079,  0.0672, -0.0134,  0.0333, -0.0311, -0.0092,  0.0275]],
       dtype=torch.float64)
	q_value: tensor([[-0.0671]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33996646965232635, distance: 0.9296932817251126 entropy tensor([[ -2.3220, -21.6069, -21.6069,  -4.5604,  -3.6804, -21.6069,  -4.2446]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 33
	action: tensor([[-0.0093,  0.0673, -0.0133,  0.0355, -0.0368, -0.0092,  0.0297]],
       dtype=torch.float64)
	q_value: tensor([[-0.0670]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33957032227292805, distance: 0.9299722374862669 entropy tensor([[ -2.3157, -21.6069, -21.6069,  -4.5444,  -3.6589, -21.6069,  -4.2459]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 34
	action: tensor([[-0.0091,  0.0673, -0.0135,  0.0324, -0.0261, -0.0090,  0.0262]],
       dtype=torch.float64)
	q_value: tensor([[-0.0670]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3384319559348783, distance: 0.9307733775069797 entropy tensor([[ -2.3143, -21.6069, -21.6069,  -4.5479,  -3.6536, -21.6069,  -4.2216]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 35
	action: tensor([[-0.0189,  0.0672, -0.0133,  0.0313, -0.0389, -0.0091,  0.0264]],
       dtype=torch.float64)
	q_value: tensor([[-0.0670]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32767861656427777, distance: 0.9383074303267078 entropy tensor([[ -2.3186, -21.6069, -21.6069,  -4.5284,  -3.6416, -21.6069,  -4.2366]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 36
	action: tensor([[-0.0386,  0.0673, -0.0137,  0.0331, -0.0302, -0.0091,  0.0253]],
       dtype=torch.float64)
	q_value: tensor([[-0.0671]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30774952579571924, distance: 0.9521126256464426 entropy tensor([[ -2.3255, -21.6069, -21.6069,  -4.5706,  -3.6687, -21.6069,  -4.1980]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 37
	action: tensor([[-0.0042,  0.0673, -0.0138,  0.0313, -0.0518, -0.0093,  0.0286]],
       dtype=torch.float64)
	q_value: tensor([[-0.0678]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3430340236689813, distance: 0.927530357012179 entropy tensor([[ -2.3576, -21.6069, -21.6069,  -4.5718,  -3.6475, -21.6069,  -4.2654]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 38
	action: tensor([[ 0.0055,  0.0673, -0.0137,  0.0273, -0.0268, -0.0089,  0.0290]],
       dtype=torch.float64)
	q_value: tensor([[-0.0667]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35127540335873286, distance: 0.9216942446191748 entropy tensor([[ -2.3007, -21.6069, -21.6069,  -4.5874,  -3.6887, -21.6069,  -4.1475]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 39
	action: tensor([[-0.0052,  0.0672, -0.0131,  0.0337, -0.0361, -0.0093,  0.0247]],
       dtype=torch.float64)
	q_value: tensor([[-0.0667]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34286922863716907, distance: 0.9276466817656118 entropy tensor([[ -2.3010, -21.6069, -21.6069,  -4.5064,  -3.6448, -21.6069,  -4.2093]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 40
	action: tensor([[-0.0124,  0.0672, -0.0134,  0.0281, -0.0304, -0.0091,  0.0258]],
       dtype=torch.float64)
	q_value: tensor([[-0.0668]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3330471111189067, distance: 0.9345537237467841 entropy tensor([[ -2.3087, -21.6069, -21.6069,  -4.5726,  -3.6840, -21.6069,  -4.2419]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 41
	action: tensor([[-0.0068,  0.0672, -0.0134,  0.0279, -0.0389, -0.0093,  0.0249]],
       dtype=torch.float64)
	q_value: tensor([[-0.0672]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33876245288326556, distance: 0.9305408570639848 entropy tensor([[ -2.3229, -21.6069, -21.6069,  -4.5313,  -3.6606, -21.6069,  -4.2141]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 42
	action: tensor([[-0.0134,  0.0671, -0.0134,  0.0282, -0.0418, -0.0093,  0.0244]],
       dtype=torch.float64)
	q_value: tensor([[-0.0669]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3320320130095973, distance: 0.93526464579481 entropy tensor([[ -2.3099, -21.6069, -21.6069,  -4.5674,  -3.6917, -21.6069,  -4.2014]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 43
	action: tensor([[-0.0164,  0.0672, -0.0136,  0.0320, -0.0246, -0.0092,  0.0249]],
       dtype=torch.float64)
	q_value: tensor([[-0.0670]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33053394256100155, distance: 0.9363128304886956 entropy tensor([[ -2.3173, -21.6069, -21.6069,  -4.5812,  -3.6910, -21.6069,  -4.1900]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 44
	action: tensor([[-0.0066,  0.0672, -0.0134,  0.0294, -0.0251, -0.0092,  0.0267]],
       dtype=torch.float64)
	q_value: tensor([[-0.0672]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3396843576552321, distance: 0.9298919455782353 entropy tensor([[ -2.3291, -21.6069, -21.6069,  -4.5297,  -3.6438, -21.6069,  -4.2495]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 45
	action: tensor([[ 0.0091,  0.0672, -0.0132,  0.0292, -0.0385, -0.0094,  0.0346]],
       dtype=torch.float64)
	q_value: tensor([[-0.0671]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3555866006026326, distance: 0.9186265102254003 entropy tensor([[ -2.3184, -21.6069, -21.6069,  -4.5318,  -3.6625, -21.6069,  -4.2631]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 46
	action: tensor([[-0.0054,  0.0673, -0.0132,  0.0319, -0.0250, -0.0092,  0.0261]],
       dtype=torch.float64)
	q_value: tensor([[-0.0666]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3420319574195205, distance: 0.9282374657131601 entropy tensor([[ -2.2929, -21.6069, -21.6069,  -4.5140,  -3.6545, -21.6069,  -4.1772]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 47
	action: tensor([[-0.0236,  0.0672, -0.0132,  0.0367, -0.0328, -0.0093,  0.0316]],
       dtype=torch.float64)
	q_value: tensor([[-0.0670]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3251066183883482, distance: 0.9401004874790582 entropy tensor([[ -2.3158, -21.6069, -21.6069,  -4.5304,  -3.6493, -21.6069,  -4.2633]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 48
	action: tensor([[ 0.0188,  0.0675, -0.0137,  0.0316, -0.0334, -0.0090,  0.0275]],
       dtype=torch.float64)
	q_value: tensor([[-0.0674]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3667791829829744, distance: 0.9106139207125846 entropy tensor([[ -2.3350, -21.6069, -21.6069,  -4.5311,  -3.6293, -21.6069,  -4.2355]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 49
	action: tensor([[-0.0118,  0.0671, -0.0129,  0.0341, -0.0285, -0.0091,  0.0275]],
       dtype=torch.float64)
	q_value: tensor([[-0.0660]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3361943545826741, distance: 0.9323461119337425 entropy tensor([[ -2.2795, -21.6069, -21.6069,  -4.5605,  -3.6747, -21.6069,  -4.2401]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 50
	action: tensor([[-0.0136,  0.0673, -0.0135,  0.0300, -0.0309, -0.0090,  0.0239]],
       dtype=torch.float64)
	q_value: tensor([[-0.0671]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3327085208014874, distance: 0.9347909149315135 entropy tensor([[ -2.3206, -21.6069, -21.6069,  -4.5262,  -3.6311, -21.6069,  -4.2252]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 51
	action: tensor([[ 0.0021,  0.0672, -0.0134,  0.0321, -0.0324, -0.0092,  0.0254]],
       dtype=torch.float64)
	q_value: tensor([[-0.0670]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3497228391130759, distance: 0.9227965108033476 entropy tensor([[ -2.3226, -21.6069, -21.6069,  -4.5516,  -3.6746, -21.6069,  -4.2330]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 52
	action: tensor([[-0.0085,  0.0671, -0.0132,  0.0367, -0.0382, -0.0091,  0.0276]],
       dtype=torch.float64)
	q_value: tensor([[-0.0666]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34082434028255626, distance: 0.9290889064570259 entropy tensor([[ -2.2996, -21.6069, -21.6069,  -4.5522,  -3.6714, -21.6069,  -4.2271]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 53
	action: tensor([[ 0.0311,  0.0673, -0.0136,  0.0294, -0.0362, -0.0088,  0.0248]],
       dtype=torch.float64)
	q_value: tensor([[-0.0668]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3780673257549301, distance: 0.9024608696767665 entropy tensor([[ -2.3097, -21.6069, -21.6069,  -4.5601,  -3.6541, -21.6069,  -4.2072]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 54
	action: tensor([[-0.0180,  0.0669, -0.0129,  0.0294, -0.0313, -0.0091,  0.0288]],
       dtype=torch.float64)
	q_value: tensor([[-0.0656]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3276674095820923, distance: 0.9383152506572032 entropy tensor([[ -2.2622, -21.6069, -21.6069,  -4.5320,  -3.6876, -21.6069,  -4.2282]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 55
	action: tensor([[-0.0013,  0.0673, -0.0135,  0.0251, -0.0295, -0.0093,  0.0318]],
       dtype=torch.float64)
	q_value: tensor([[-0.0674]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3433207076205337, distance: 0.9273279591005699 entropy tensor([[ -2.3302, -21.6069, -21.6069,  -4.5198,  -3.6396, -21.6069,  -4.2016]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 56
	action: tensor([[-0.0156,  0.0672, -0.0132,  0.0351, -0.0411, -0.0094,  0.0248]],
       dtype=torch.float64)
	q_value: tensor([[-0.0671]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.332766752752256, distance: 0.9347501262448131 entropy tensor([[ -2.3107, -21.6069, -21.6069,  -4.4985,  -3.6503, -21.6069,  -4.1913]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 57
	action: tensor([[-0.0101,  0.0672, -0.0137,  0.0312, -0.0360, -0.0089,  0.0276]],
       dtype=torch.float64)
	q_value: tensor([[-0.0669]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33685349846053236, distance: 0.9318830977440064 entropy tensor([[ -2.3177, -21.6069, -21.6069,  -4.5919,  -3.6730, -21.6069,  -4.2066]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 58
	action: tensor([[-0.0321,  0.0673, -0.0135,  0.0341, -0.0284, -0.0091,  0.0219]],
       dtype=torch.float64)
	q_value: tensor([[-0.0670]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3150303125956553, distance: 0.9470924392710766 entropy tensor([[ -2.3156, -21.6069, -21.6069,  -4.5458,  -3.6631, -21.6069,  -4.2009]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 59
	action: tensor([[ 0.0060,  0.0672, -0.0137,  0.0306, -0.0427, -0.0093,  0.0263]],
       dtype=torch.float64)
	q_value: tensor([[-0.0675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35309174285512845, distance: 0.9204030317291286 entropy tensor([[ -2.3484, -21.6069, -21.6069,  -4.5802,  -3.6670, -21.6069,  -4.2844]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 60
	action: tensor([[-0.0083,  0.0671, -0.0133,  0.0337, -0.0291, -0.0091,  0.0271]],
       dtype=torch.float64)
	q_value: tensor([[-0.0666]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3396659957905108, distance: 0.929904874570308 entropy tensor([[ -2.2918, -21.6069, -21.6069,  -4.5742,  -3.7009, -21.6069,  -4.2064]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 61
	action: tensor([[-0.0280,  0.0673, -0.0134,  0.0303, -0.0406, -0.0091,  0.0259]],
       dtype=torch.float64)
	q_value: tensor([[-0.0670]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31783906579010657, distance: 0.9451486441018524 entropy tensor([[ -2.3159, -21.6069, -21.6069,  -4.5271,  -3.6367, -21.6069,  -4.2220]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 62
	action: tensor([[-0.0075,  0.0673, -0.0138,  0.0283, -0.0257, -0.0092,  0.0246]],
       dtype=torch.float64)
	q_value: tensor([[-0.0674]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33820754048929036, distance: 0.9309312314217824 entropy tensor([[ -2.3382, -21.6069, -21.6069,  -4.5893,  -3.6737, -21.6069,  -4.1945]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 63
	action: tensor([[-0.0178,  0.0672, -0.0132,  0.0300, -0.0256, -0.0093,  0.0238]],
       dtype=torch.float64)
	q_value: tensor([[-0.0670]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3282090807766196, distance: 0.9379371931585994 entropy tensor([[ -2.3184, -21.6069, -21.6069,  -4.5299,  -3.6612, -21.6069,  -4.2360]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
LOSS epoch 0 actor 0.1526112830765049 critic 20.756106362338713
epoch: 1, step: 0
	action: tensor([[ 0.0222, -0.0007, -0.0867,  0.0501, -0.0225,  0.2132,  0.0166]],
       dtype=torch.float64)
	q_value: tensor([[0.1642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3747935341174713, distance: 0.9048329841857385 entropy tensor([[ -2.1918,  -4.1450, -21.6069,  -3.8770,  -2.1773, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 1
	action: tensor([[ 0.0064,  0.0043, -0.0793,  0.0466, -0.0169,  0.2103,  0.0146]],
       dtype=torch.float64)
	q_value: tensor([[0.1681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3611181485240461, distance: 0.9146753368226971 entropy tensor([[ -2.0925, -21.6069, -21.6069,  -3.4987,  -2.1616, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 2
	action: tensor([[ 0.0078,  0.0039, -0.0789,  0.0413, -0.0356,  0.2103,  0.0146]],
       dtype=torch.float64)
	q_value: tensor([[0.1674]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3588536310592564, distance: 0.9162949368206736 entropy tensor([[ -2.1043, -21.6069, -21.6069,  -3.5635,  -2.1599, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 3
	action: tensor([[ 0.0306,  0.0041, -0.0792,  0.0648, -0.0590,  0.2097,  0.0145]],
       dtype=torch.float64)
	q_value: tensor([[0.1677]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38994559844203425, distance: 0.8938012871145854 entropy tensor([[ -2.1010, -21.6069, -21.6069,  -3.5795,  -2.1728, -21.6069,  -7.2158]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 4
	action: tensor([[-0.0170,  0.0057, -0.0810,  0.0484, -0.0182,  0.2103,  0.0154]],
       dtype=torch.float64)
	q_value: tensor([[0.1693]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33832229037404615, distance: 0.9308505196510302 entropy tensor([[ -2.0847, -21.6069, -21.6069,  -3.5248,  -2.1707, -21.6069,  -6.5198]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 5
	action: tensor([[ 0.0304,  0.0036, -0.0781,  0.0475, -0.0015,  0.2098,  0.0146]],
       dtype=torch.float64)
	q_value: tensor([[0.1666]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3861605891955532, distance: 0.8965697409749674 entropy tensor([[ -2.1143, -21.6069, -21.6069,  -3.6723,  -2.1631, -21.6069,  -7.2618]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 6
	action: tensor([[ 0.0047,  0.0042, -0.0796,  0.0460,  0.0467,  0.2111,  0.0148]],
       dtype=torch.float64)
	q_value: tensor([[0.1680]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36306957027816955, distance: 0.9132773612743008 entropy tensor([[ -2.0942, -21.6069, -21.6069,  -3.4463,  -2.1504, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 7
	action: tensor([[ 0.0176,  0.0029, -0.0779,  0.0419,  0.0221,  0.2114,  0.0139]],
       dtype=torch.float64)
	q_value: tensor([[0.1661]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3724490498622265, distance: 0.906527929594561 entropy tensor([[ -2.1078, -21.6069, -21.6069,  -3.4964,  -2.1368, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 8
	action: tensor([[-0.0184,  0.0033, -0.0786,  0.0469,  0.0154,  0.2111,  0.0142]],
       dtype=torch.float64)
	q_value: tensor([[0.1669]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3371857092639151, distance: 0.9316496497700906 entropy tensor([[ -2.1018, -21.6069, -21.6069,  -3.4672,  -2.1496, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 9
	action: tensor([[ 0.0033,  0.0028, -0.0773,  0.0570,  0.0363,  0.2105,  0.0138]],
       dtype=torch.float64)
	q_value: tensor([[0.1655]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36465141377040045, distance: 0.9121425749081244 entropy tensor([[ -2.1117, -21.6069, -21.6069,  -3.6130,  -2.1494, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 10
	action: tensor([[-0.0058,  0.0034, -0.0783,  0.0429, -0.0079,  0.2116,  0.0141]],
       dtype=torch.float64)
	q_value: tensor([[0.1663]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3473738532500009, distance: 0.924461709957654 entropy tensor([[ -2.1061, -21.6069, -21.6069,  -3.5088,  -2.1346, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 11
	action: tensor([[ 0.0129,  0.0033, -0.0781,  0.0670, -0.0440,  0.2101,  0.0142]],
       dtype=torch.float64)
	q_value: tensor([[0.1665]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3735720951272453, distance: 0.9057164195617774 entropy tensor([[ -2.1096, -21.6069, -21.6069,  -3.6021,  -2.1614, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 12
	action: tensor([[-0.0633,  0.0050, -0.0800,  0.0486,  0.0217,  0.2104,  0.0151]],
       dtype=torch.float64)
	q_value: tensor([[0.1683]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29151446390153146, distance: 0.9632126719474879 entropy tensor([[ -2.0929, -21.6069, -21.6069,  -3.5757,  -2.1637, -21.6069,  -7.5104]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 13
	action: tensor([[ 0.0239,  0.0015, -0.0749,  0.0530,  0.0214,  0.2097,  0.0129]],
       dtype=torch.float64)
	q_value: tensor([[0.1633]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38217735776054473, distance: 0.899473977769284 entropy tensor([[ -2.1188, -21.6069, -21.6069,  -3.7882,  -2.1466, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 14
	action: tensor([[ 0.0041,  0.0039, -0.0794,  0.0501, -0.0016,  0.2116,  0.0146]],
       dtype=torch.float64)
	q_value: tensor([[0.1674]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36117299338430586, distance: 0.914636075793783 entropy tensor([[ -2.0983, -21.6069, -21.6069,  -3.4448,  -2.1427, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 15
	action: tensor([[ 0.0061,  0.0036, -0.0786,  0.0566, -0.0168,  0.2107,  0.0144]],
       dtype=torch.float64)
	q_value: tensor([[0.1670]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36451326183472155, distance: 0.9122417389069063 entropy tensor([[ -2.1052, -21.6069, -21.6069,  -3.5545,  -2.1546, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 16
	action: tensor([[-0.0248,  0.0041, -0.0791,  0.0642, -0.0087,  0.2106,  0.0146]],
       dtype=torch.float64)
	q_value: tensor([[0.1674]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33634662115540714, distance: 0.9322391730351944 entropy tensor([[ -2.1032, -21.6069, -21.6069,  -3.5667,  -2.1565, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 17
	action: tensor([[-0.0548,  0.0035, -0.0777,  0.0505, -0.0203,  0.2103,  0.0142]],
       dtype=torch.float64)
	q_value: tensor([[0.1661]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29779828777245654, distance: 0.9589316110465336 entropy tensor([[ -2.1095, -21.6069, -21.6069,  -3.6791,  -2.1506, -21.6069, -11.0011]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 18
	action: tensor([[-0.0160,  0.0027, -0.0764,  0.0454,  0.0366,  0.2089,  0.0136]],
       dtype=torch.float64)
	q_value: tensor([[0.1651]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33961585550967555, distance: 0.9299401785272491 entropy tensor([[ -2.1195, -21.6069, -21.6069,  -3.8365,  -2.1652, -21.6069,  -6.6668]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 19
	action: tensor([[ 0.0170,  0.0025, -0.0771,  0.0546,  0.0317,  0.2109,  0.0136]],
       dtype=torch.float64)
	q_value: tensor([[0.1651]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37725204753564867, distance: 0.9030521842653466 entropy tensor([[ -2.1136, -21.6069, -21.6069,  -3.5769,  -2.1418, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 20
	action: tensor([[ 0.0310,  0.0036, -0.0788,  0.0474,  0.0444,  0.2117,  0.0144]],
       dtype=torch.float64)
	q_value: tensor([[0.1670]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38993288841858986, distance: 0.8938105979040125 entropy tensor([[ -2.1014, -21.6069, -21.6069,  -3.4635,  -2.1385, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 21
	action: tensor([[-0.0224,  0.0033, -0.0787,  0.0473,  0.0086,  0.2120,  0.0142]],
       dtype=torch.float64)
	q_value: tensor([[0.1673]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33294688660503946, distance: 0.934623939856236 entropy tensor([[ -2.0973, -21.6069, -21.6069,  -3.3962,  -2.1373, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 22
	action: tensor([[-0.0032,  0.0028, -0.0771,  0.0504,  0.0418,  0.2103,  0.0138]],
       dtype=torch.float64)
	q_value: tensor([[0.1655]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35559706176479444, distance: 0.9186190538783569 entropy tensor([[ -2.1117, -21.6069, -21.6069,  -3.6431,  -2.1539, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 23
	action: tensor([[ 0.0459,  0.0030, -0.0778,  0.0550,  0.0179,  0.2113,  0.0139]],
       dtype=torch.float64)
	q_value: tensor([[0.1658]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4057198043808721, distance: 0.8821700761143946 entropy tensor([[ -2.1102, -21.6069, -21.6069,  -3.5266,  -2.1365, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 24
	action: tensor([[ 0.0325,  0.0044, -0.0800,  0.0500, -0.0401,  0.2120,  0.0149]],
       dtype=torch.float64)
	q_value: tensor([[0.1684]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38774434620761666, distance: 0.8954123815181949 entropy tensor([[ -2.0877, -21.6069, -21.6069,  -3.3781,  -2.1410, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 25
	action: tensor([[ 0.0209,  0.0050, -0.0803,  0.0659,  0.0244,  0.2103,  0.0150]],
       dtype=torch.float64)
	q_value: tensor([[0.1689]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38643718930085336, distance: 0.8963677180815834 entropy tensor([[ -2.0880, -21.6069, -21.6069,  -3.4864,  -2.1663, -21.6069,  -7.0951]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 26
	action: tensor([[ 0.0074,  0.0042, -0.0793,  0.0683,  0.0259,  0.2120,  0.0146]],
       dtype=torch.float64)
	q_value: tensor([[0.1675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3737846730880886, distance: 0.905562729340574 entropy tensor([[ -2.0986, -21.6069, -21.6069,  -3.4665,  -2.1330, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 27
	action: tensor([[-0.0408,  0.0039, -0.0787,  0.0566,  0.0122,  0.2118,  0.0144]],
       dtype=torch.float64)
	q_value: tensor([[0.1669]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31793174488897735, distance: 0.9450844374787167 entropy tensor([[ -2.1025, -21.6069, -21.6069,  -3.5094,  -2.1317, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 28
	action: tensor([[ 0.0450,  0.0027, -0.0766,  0.0417,  0.0289,  0.2102,  0.0137]],
       dtype=torch.float64)
	q_value: tensor([[0.1648]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39972093749052084, distance: 0.8866113590450061 entropy tensor([[ -2.1148, -21.6069, -21.6069,  -3.7113,  -2.1477, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 29
	action: tensor([[ 0.0008,  0.0037, -0.0794,  0.0434, -0.0119,  0.2118,  0.0146]],
       dtype=torch.float64)
	q_value: tensor([[0.1680]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35424116984355614, distance: 0.9195849818537896 entropy tensor([[ -2.0901, -21.6069, -21.6069,  -3.3567,  -2.1454, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 30
	action: tensor([[-0.0089,  0.0035, -0.0785,  0.0479, -0.0102,  0.2102,  0.0144]],
       dtype=torch.float64)
	q_value: tensor([[0.1669]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34580726281428875, distance: 0.9255706027061653 entropy tensor([[ -2.1072, -21.6069, -21.6069,  -3.5820,  -2.1625, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 31
	action: tensor([[ 0.0010,  0.0035, -0.0782,  0.0496,  0.0188,  0.2102,  0.0143]],
       dtype=torch.float64)
	q_value: tensor([[0.1666]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3584938999136206, distance: 0.9165519558267385 entropy tensor([[ -2.1102, -21.6069, -21.6069,  -3.6147,  -2.1587, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 32
	action: tensor([[ 0.0857,  0.0034, -0.0784,  0.0605,  0.0237,  0.2110,  0.0143]],
       dtype=torch.float64)
	q_value: tensor([[0.1664]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44684159065822704, distance: 0.8511016898868445 entropy tensor([[ -2.1084, -21.6069, -21.6069,  -3.5349,  -2.1446, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 33
	action: tensor([[-0.0126,  0.0052, -0.0810,  0.0692, -0.0101,  0.2133,  0.0150]],
       dtype=torch.float64)
	q_value: tensor([[0.1701]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35217297532133063, distance: 0.9210563982418838 entropy tensor([[ -2.0730, -21.6069, -21.6069,  -3.2589,  -2.1380, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 34
	action: tensor([[-0.0289,  0.0038, -0.0781,  0.0412,  0.0051,  0.2107,  0.0144]],
       dtype=torch.float64)
	q_value: tensor([[0.1667]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32348856874500775, distance: 0.9412267532481468 entropy tensor([[ -2.1053, -21.6069, -21.6069,  -3.6402,  -2.1507, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 35
	action: tensor([[-0.0035,  0.0026, -0.0769,  0.0483, -0.0013,  0.2099,  0.0138]],
       dtype=torch.float64)
	q_value: tensor([[0.1653]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35160512972614144, distance: 0.9214599807087912 entropy tensor([[ -2.1144, -21.6069, -21.6069,  -3.6696,  -2.1571, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 36
	action: tensor([[-0.0205,  0.0036, -0.0786,  0.0496, -0.0025,  0.2104,  0.0144]],
       dtype=torch.float64)
	q_value: tensor([[0.1666]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3349553852739985, distance: 0.9332158015164757 entropy tensor([[ -2.1104, -21.6069, -21.6069,  -3.5827,  -2.1554, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 37
	action: tensor([[ 0.0015,  0.0033, -0.0777,  0.0380,  0.0071,  0.2101,  0.0142]],
       dtype=torch.float64)
	q_value: tensor([[0.1660]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35347755458827357, distance: 0.9201285297067098 entropy tensor([[ -2.1139, -21.6069, -21.6069,  -3.6553,  -2.1561, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 38
	action: tensor([[-0.0250,  0.0032, -0.0783,  0.0474,  0.0074,  0.2104,  0.0141]],
       dtype=torch.float64)
	q_value: tensor([[0.1664]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32981133029056064, distance: 0.9368180156698475 entropy tensor([[ -2.1090, -21.6069, -21.6069,  -3.5448,  -2.1553, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 39
	action: tensor([[ 0.0324,  0.0028, -0.0771,  0.0565, -0.0325,  0.2102,  0.0138]],
       dtype=torch.float64)
	q_value: tensor([[0.1654]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3896169884576741, distance: 0.8940419808005746 entropy tensor([[ -2.1126, -21.6069, -21.6069,  -3.6513,  -2.1532, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 40
	action: tensor([[ 0.0729,  0.0052, -0.0806,  0.0438, -0.0018,  0.2106,  0.0151]],
       dtype=torch.float64)
	q_value: tensor([[0.1688]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42738011647104446, distance: 0.8659441954352258 entropy tensor([[ -2.0881, -21.6069, -21.6069,  -3.4769,  -2.1586, -21.6069,  -7.9007]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 41
	action: tensor([[-0.0177,  0.0052, -0.0812,  0.0483,  0.0166,  0.2115,  0.0151]],
       dtype=torch.float64)
	q_value: tensor([[0.1696]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33967593476476476, distance: 0.9298978763426492 entropy tensor([[ -2.0797, -21.6069, -21.6069,  -3.3082,  -2.1502, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 42
	action: tensor([[ 0.0432,  0.0030, -0.0774,  0.0445, -0.0293,  0.2105,  0.0140]],
       dtype=torch.float64)
	q_value: tensor([[0.1657]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3957878590177766, distance: 0.8895111925903499 entropy tensor([[ -2.1121, -21.6069, -21.6069,  -3.6149,  -2.1477, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 43
	action: tensor([[-0.0074,  0.0049, -0.0805,  0.0569,  0.0309,  0.2105,  0.0149]],
       dtype=torch.float64)
	q_value: tensor([[0.1689]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3544272675428207, distance: 0.919452467227097 entropy tensor([[ -2.0879, -21.6069, -21.6069,  -3.4239,  -2.1631, -21.6069,  -8.7536]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 44
	action: tensor([[-0.0088,  0.0032, -0.0778,  0.0507, -0.0265,  0.2112,  0.0140]],
       dtype=torch.float64)
	q_value: tensor([[0.1660]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3460522200957641, distance: 0.9253973001963645 entropy tensor([[ -2.1091, -21.6069, -21.6069,  -3.5575,  -2.1368, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 45
	action: tensor([[ 0.0262,  0.0038, -0.0785,  0.0527, -0.0069,  0.2099,  0.0144]],
       dtype=torch.float64)
	q_value: tensor([[0.1670]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3838236249219693, distance: 0.8982747968754913 entropy tensor([[ -2.1099, -21.6069, -21.6069,  -3.6416,  -2.1664, -21.6069,  -8.0374]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 46
	action: tensor([[ 0.0095,  0.0043, -0.0797,  0.0620, -0.0290,  0.2111,  0.0148]],
       dtype=torch.float64)
	q_value: tensor([[0.1680]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3698263192392254, distance: 0.9084202855685738 entropy tensor([[ -2.0957, -21.6069, -21.6069,  -3.4727,  -2.1496, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 47
	action: tensor([[ 0.0140,  0.0047, -0.0797,  0.0486, -0.0428,  0.2105,  0.0149]],
       dtype=torch.float64)
	q_value: tensor([[0.1679]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.368241012214927, distance: 0.9095622092337566 entropy tensor([[ -2.0981, -21.6069, -21.6069,  -3.5728,  -2.1591, -21.6069,  -8.1672]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 48
	action: tensor([[-0.0438,  0.0046, -0.0797,  0.0540,  0.0364,  0.2098,  0.0148]],
       dtype=torch.float64)
	q_value: tensor([[0.1682]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31518759552152376, distance: 0.9469836972166609 entropy tensor([[ -2.0961, -21.6069, -21.6069,  -3.5652,  -2.1712, -21.6069,  -6.9550]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 49
	action: tensor([[ 0.0011,  0.0019, -0.0757,  0.0512,  0.0208,  0.2105,  0.0132]],
       dtype=torch.float64)
	q_value: tensor([[0.1638]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.358649972228975, distance: 0.9164404548702357 entropy tensor([[ -2.1161, -21.6069, -21.6069,  -3.6937,  -2.1371, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 50
	action: tensor([[ 0.0166,  0.0034, -0.0784,  0.0509,  0.0088,  0.2111,  0.0142]],
       dtype=torch.float64)
	q_value: tensor([[0.1663]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37440403312147896, distance: 0.9051147938708146 entropy tensor([[ -2.1082, -21.6069, -21.6069,  -3.5312,  -2.1444, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 51
	action: tensor([[-0.0367,  0.0039, -0.0791,  0.0564, -0.0087,  0.2111,  0.0146]],
       dtype=torch.float64)
	q_value: tensor([[0.1673]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3205722846659954, distance: 0.9432532768861372 entropy tensor([[ -2.1017, -21.6069, -21.6069,  -3.4939,  -2.1485, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 52
	action: tensor([[-0.0198,  0.0030, -0.0769,  0.0550, -0.0019,  0.2098,  0.0139]],
       dtype=torch.float64)
	q_value: tensor([[0.1655]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3375442561169393, distance: 0.9313976295274979 entropy tensor([[ -2.1124, -21.6069, -21.6069,  -3.7282,  -2.1579, -21.6069,  -8.4393]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 53
	action: tensor([[-0.0460,  0.0033, -0.0777,  0.0495,  0.0204,  0.2104,  0.0141]],
       dtype=torch.float64)
	q_value: tensor([[0.1659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3093593178189643, distance: 0.95100493741804 entropy tensor([[ -2.1114, -21.6069, -21.6069,  -3.6461,  -2.1524, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 54
	action: tensor([[ 0.0210,  0.0023, -0.0761,  0.0470,  0.0250,  0.2100,  0.0135]],
       dtype=torch.float64)
	q_value: tensor([[0.1642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37750510989410124, distance: 0.9028686816654496 entropy tensor([[ -2.1190, -21.6069, -21.6069,  -3.7117,  -2.1467, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 55
	action: tensor([[ 0.0213,  0.0036, -0.0791,  0.0443,  0.0373,  0.2114,  0.0145]],
       dtype=torch.float64)
	q_value: tensor([[0.1671]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37840557226202765, distance: 0.9022154285180561 entropy tensor([[ -2.1005, -21.6069, -21.6069,  -3.4504,  -2.1451, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 56
	action: tensor([[-0.0309,  0.0034, -0.0787,  0.0500, -0.0178,  0.2115,  0.0143]],
       dtype=torch.float64)
	q_value: tensor([[0.1670]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3232975560225303, distance: 0.9413596213620073 entropy tensor([[ -2.1013, -21.6069, -21.6069,  -3.4376,  -2.1406, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 57
	action: tensor([[ 0.0107,  0.0032, -0.0775,  0.0515,  0.0345,  0.2096,  0.0140]],
       dtype=torch.float64)
	q_value: tensor([[0.1660]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36992823366318484, distance: 0.9083468257618557 entropy tensor([[ -2.1161, -21.6069, -21.6069,  -3.7223,  -2.1642, -21.6069,  -7.2888]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 58
	action: tensor([[-0.0346,  0.0034, -0.0786,  0.0473,  0.0044,  0.2115,  0.0142]],
       dtype=torch.float64)
	q_value: tensor([[0.1666]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3198070304768831, distance: 0.9437843307177987 entropy tensor([[ -2.1045, -21.6069, -21.6069,  -3.4816,  -2.1374, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 59
	action: tensor([[ 0.0036,  0.0026, -0.0766,  0.0443, -0.0335,  0.2099,  0.0137]],
       dtype=torch.float64)
	q_value: tensor([[0.1651]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3552296760096102, distance: 0.9188808772241107 entropy tensor([[ -2.1137, -21.6069, -21.6069,  -3.6988,  -2.1567, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 60
	action: tensor([[ 0.0470,  0.0041, -0.0791,  0.0399,  0.0186,  0.2098,  0.0147]],
       dtype=torch.float64)
	q_value: tensor([[0.1675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4009758157146939, distance: 0.8856841474942047 entropy tensor([[ -2.1038, -21.6069, -21.6069,  -3.5922,  -2.1701, -21.6069,  -7.5778]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 61
	action: tensor([[ 6.3463e-05,  3.9388e-03, -7.9629e-02,  3.7290e-02,  3.9891e-02,
          2.1158e-01,  1.4689e-02]], dtype=torch.float64)
	q_value: tensor([[0.1683]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35433637037764076, distance: 0.9195171947759191 entropy tensor([[ -2.0887, -21.6069, -21.6069,  -3.3620,  -2.1477, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 62
	action: tensor([[-0.0706,  0.0026, -0.0774,  0.0492,  0.0120,  0.2110,  0.0137]],
       dtype=torch.float64)
	q_value: tensor([[0.1657]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28187010328816753, distance: 0.9697464466992944 entropy tensor([[ -2.1097, -21.6069, -21.6069,  -3.5159,  -2.1440, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 63
	action: tensor([[-0.0181,  0.0014, -0.0747,  0.0462,  0.0160,  0.2092,  0.0128]],
       dtype=torch.float64)
	q_value: tensor([[0.1631]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3358745629005383, distance: 0.9325706660879397 entropy tensor([[ -2.1202, -21.6069, -21.6069,  -3.8403,  -2.1522, -21.6069,  -7.9494]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
LOSS epoch 1 actor 0.1051909976975099 critic 22.80017976605357
epoch: 2, step: 0
	action: tensor([[-0.0194,  0.0653, -0.1329,  0.0246,  0.0779,  0.2518, -0.0872]],
       dtype=torch.float64)
	q_value: tensor([[0.4232]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3765763968189487, distance: 0.9035419349323379 entropy tensor([[ -2.0722,  -2.5499, -21.6069,  -2.7160,  -1.5740, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 1
	action: tensor([[-0.0133,  0.0343, -0.1237,  0.0563,  0.1429,  0.2532, -0.0943]],
       dtype=torch.float64)
	q_value: tensor([[0.4248]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3805472900041833, distance: 0.9006597854686722 entropy tensor([[ -2.0022,  -2.6941, -21.6069,  -2.8975,  -1.5768, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 2
	action: tensor([[ 0.0518,  0.0460, -0.1257,  0.0388,  0.1443,  0.2541, -0.0959]],
       dtype=torch.float64)
	q_value: tensor([[0.4254]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44666670298559197, distance: 0.851236222263203 entropy tensor([[ -1.9898,  -2.7271, -21.6069,  -2.8048,  -1.5395, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 3
	action: tensor([[-0.0347,  0.0296, -0.1294,  0.0523,  0.0085,  0.2543, -0.0954]],
       dtype=torch.float64)
	q_value: tensor([[0.4273]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3432667436681647, distance: 0.927366060845483 entropy tensor([[ -1.9901,  -2.6856, -21.6069,  -2.7056,  -1.5724, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 4
	action: tensor([[-0.0262,  0.0246, -0.1250,  0.0432,  0.0960,  0.2513, -0.0933]],
       dtype=torch.float64)
	q_value: tensor([[0.4234]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35222501898262637, distance: 0.9210194006266537 entropy tensor([[ -1.9967,  -2.7397, -21.6069,  -2.9169,  -1.5902, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 5
	action: tensor([[-0.0155,  0.0353, -0.1249,  0.0395,  0.0190,  0.2522, -0.0950]],
       dtype=torch.float64)
	q_value: tensor([[0.4236]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3632325799188275, distance: 0.9131604862326764 entropy tensor([[ -1.9995,  -2.7242, -21.6069,  -2.8456,  -1.5594, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 6
	action: tensor([[-0.0491,  0.0093, -0.1264,  0.0495,  0.0430,  0.2514, -0.0927]],
       dtype=torch.float64)
	q_value: tensor([[0.4238]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3161431945821278, distance: 0.9463227478551367 entropy tensor([[ -2.0021,  -2.7198, -21.6069,  -2.8771,  -1.5974, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 7
	action: tensor([[ 0.0184,  0.0637, -0.1243,  0.0418, -0.0199,  0.2510, -0.0941]],
       dtype=torch.float64)
	q_value: tensor([[0.4223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4133632486919836, distance: 0.876478619915452 entropy tensor([[ -2.0059,  -2.7398, -21.6069,  -2.8951,  -1.5694, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 8
	action: tensor([[-0.0115,  0.0200, -0.1289,  0.0562,  0.0882,  0.2528, -0.0911]],
       dtype=torch.float64)
	q_value: tensor([[0.4259]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3685211675005614, distance: 0.9093605129209739 entropy tensor([[ -1.9994,  -2.6853, -21.6069,  -2.8442,  -1.6243, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 9
	action: tensor([[-0.0054,  0.0431, -0.1266,  0.0218,  0.0410,  0.2522, -0.0946]],
       dtype=torch.float64)
	q_value: tensor([[0.4240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3738607991035948, distance: 0.9055076851931199 entropy tensor([[ -1.9992,  -2.7217, -21.6069,  -2.8097,  -1.5637, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 10
	action: tensor([[ 0.0413,  0.0200, -0.1259,  0.0461,  0.0413,  0.2516, -0.0933]],
       dtype=torch.float64)
	q_value: tensor([[0.4241]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41469364700036504, distance: 0.8754841991482711 entropy tensor([[ -2.0019,  -2.7050, -21.6069,  -2.8587,  -1.5969, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 11
	action: tensor([[-0.0472,  0.0329, -0.1306,  0.0635,  0.0928,  0.2516, -0.0928]],
       dtype=torch.float64)
	q_value: tensor([[0.4257]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34199748206827363, distance: 0.9282617836825904 entropy tensor([[ -1.9933,  -2.6738, -21.6069,  -2.7214,  -1.5986, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 12
	action: tensor([[-0.0286,  0.0557, -0.1236,  0.0578,  0.0528,  0.2531, -0.0952]],
       dtype=torch.float64)
	q_value: tensor([[0.4236]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3719442455418146, distance: 0.9068924635217885 entropy tensor([[ -1.9987,  -2.7362, -21.6069,  -2.9005,  -1.5488, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 13
	action: tensor([[-0.0116,  0.0625, -0.1249,  0.0316,  0.0782,  0.2533, -0.0939]],
       dtype=torch.float64)
	q_value: tensor([[0.4241]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38645744643623936, distance: 0.8963529209069669 entropy tensor([[ -2.0017,  -2.7366, -21.6069,  -2.9002,  -1.5764, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 14
	action: tensor([[-0.0507,  0.0438, -0.1250,  0.0332,  0.0747,  0.2531, -0.0941]],
       dtype=torch.float64)
	q_value: tensor([[0.4247]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3339901090415657, distance: 0.9338928120360656 entropy tensor([[ -2.0027,  -2.7123, -21.6069,  -2.8683,  -1.5791, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 15
	action: tensor([[ 0.0218,  0.0531, -0.1223,  0.0347,  0.0715,  0.2525, -0.0948]],
       dtype=torch.float64)
	q_value: tensor([[0.4235]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4150955761512918, distance: 0.8751835505656096 entropy tensor([[ -2.0041,  -2.7299, -21.6069,  -2.9402,  -1.5640, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 16
	action: tensor([[ 0.0536,  0.0479, -0.1282,  0.0413,  0.1307,  0.2528, -0.0934]],
       dtype=torch.float64)
	q_value: tensor([[0.4253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4490256226386733, distance: 0.8494198283132884 entropy tensor([[ -2.0007,  -2.7009, -21.6069,  -2.7855,  -1.5929, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 17
	action: tensor([[ 0.0429,  0.0353, -0.1298,  0.0458,  0.0457,  0.2541, -0.0948]],
       dtype=torch.float64)
	q_value: tensor([[0.4272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.426449903711587, distance: 0.8666472669599634 entropy tensor([[ -1.9957,  -2.6751, -21.6069,  -2.7044,  -1.5778, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 18
	action: tensor([[ 0.0511,  0.0482, -0.1303,  0.0459, -0.0040,  0.2521, -0.0927]],
       dtype=torch.float64)
	q_value: tensor([[0.4261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4384093938059911, distance: 0.8575641368163585 entropy tensor([[ -1.9950,  -2.6823, -21.6069,  -2.7323,  -1.6031, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 19
	action: tensor([[-0.0357,  0.0401, -0.1311,  0.0346,  0.0512,  0.2525, -0.0913]],
       dtype=torch.float64)
	q_value: tensor([[0.4267]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3454533291645192, distance: 0.9258209465775867 entropy tensor([[ -1.9974,  -2.6644, -21.6069,  -2.7468,  -1.6221, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 20
	action: tensor([[-0.0320,  0.0359, -0.1234,  0.0255,  0.0389,  0.2520, -0.0943]],
       dtype=torch.float64)
	q_value: tensor([[0.4237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34300841042028285, distance: 0.9275484377309793 entropy tensor([[ -1.9999,  -2.7135, -21.6069,  -2.9163,  -1.5748, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 21
	action: tensor([[-0.0118,  0.0241, -0.1243,  0.0585,  0.0797,  0.2514, -0.0936]],
       dtype=torch.float64)
	q_value: tensor([[0.4232]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37123352606502014, distance: 0.9074054465576371 entropy tensor([[ -2.0048,  -2.7200, -21.6069,  -2.9052,  -1.5860, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 22
	action: tensor([[-0.0362,  0.0278, -0.1270,  0.0472,  0.0452,  0.2523, -0.0942]],
       dtype=torch.float64)
	q_value: tensor([[0.4239]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34144275043657013, distance: 0.9286529886368714 entropy tensor([[ -2.0028,  -2.7266, -21.6069,  -2.8146,  -1.5673, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 23
	action: tensor([[ 0.0092,  0.0305, -0.1244,  0.0514,  0.0526,  0.2518, -0.0942]],
       dtype=torch.float64)
	q_value: tensor([[0.4231]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39230607747705204, distance: 0.8920704217934938 entropy tensor([[ -2.0002,  -2.7330, -21.6069,  -2.8972,  -1.5741, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 24
	action: tensor([[-0.0042,  0.0398, -0.1282,  0.0252,  0.1309,  0.2520, -0.0934]],
       dtype=torch.float64)
	q_value: tensor([[0.4246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38060074957705226, distance: 0.900620920575899 entropy tensor([[ -1.9987,  -2.7121, -21.6069,  -2.7952,  -1.5871, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 25
	action: tensor([[ 0.0325,  0.0255, -0.1247,  0.0686,  0.0522,  0.2533, -0.0960]],
       dtype=torch.float64)
	q_value: tensor([[0.4255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4189941192644967, distance: 0.8722620089738132 entropy tensor([[ -1.9918,  -2.6923, -21.6069,  -2.8132,  -1.5544, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 26
	action: tensor([[ 0.0104,  0.0547, -0.1305,  0.0572,  0.0385,  0.2525, -0.0931]],
       dtype=torch.float64)
	q_value: tensor([[0.4256]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4091998498252437, distance: 0.8795833337187587 entropy tensor([[ -1.9964,  -2.7037, -21.6069,  -2.7380,  -1.5910, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 27
	action: tensor([[ 0.0270,  0.0330, -0.1280,  0.0327, -0.0312,  0.2529, -0.0929]],
       dtype=torch.float64)
	q_value: tensor([[0.4253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3986554185363921, distance: 0.887397895189763 entropy tensor([[ -1.9992,  -2.7105, -21.6069,  -2.8263,  -1.5968, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 28
	action: tensor([[-0.0037,  0.0472, -0.1297,  0.0358,  0.1179,  0.2515, -0.0910]],
       dtype=torch.float64)
	q_value: tensor([[0.4255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38838168671321993, distance: 0.8949462109162507 entropy tensor([[ -2.0002,  -2.6642, -21.6069,  -2.7935,  -1.6240, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 29
	action: tensor([[-0.0253,  0.0429, -0.1254,  0.0503,  0.0825,  0.2534, -0.0951]],
       dtype=torch.float64)
	q_value: tensor([[0.4252]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3669022609326196, distance: 0.9105254192468861 entropy tensor([[ -1.9980,  -2.6957, -21.6069,  -2.8207,  -1.5618, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 30
	action: tensor([[ 0.0107,  0.0543, -0.1249,  0.0245,  0.0158,  0.2529, -0.0946]],
       dtype=torch.float64)
	q_value: tensor([[0.4240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3967191062685308, distance: 0.8888254448041799 entropy tensor([[ -2.0005,  -2.7351, -21.6069,  -2.8702,  -1.5661, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 31
	action: tensor([[ 0.0534,  0.0332, -0.1275,  0.0552,  0.1405,  0.2520, -0.0921]],
       dtype=torch.float64)
	q_value: tensor([[0.4249]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4450413243701188, distance: 0.8524855295122865 entropy tensor([[ -2.0062,  -2.6985, -21.6069,  -2.8435,  -1.6137, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 32
	action: tensor([[ 0.0045,  0.0067, -0.1301,  0.0507,  0.0070,  0.2543, -0.0954]],
       dtype=torch.float64)
	q_value: tensor([[0.4274]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36837955047668614, distance: 0.90946247493733 entropy tensor([[ -1.9859,  -2.6814, -21.6069,  -2.6907,  -1.5678, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 33
	action: tensor([[ 0.0582,  0.0618, -0.1284,  0.0532,  0.0577,  0.2507, -0.0927]],
       dtype=torch.float64)
	q_value: tensor([[0.4242]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4606312225338166, distance: 0.8404262226898199 entropy tensor([[ -1.9917,  -2.7094, -21.6069,  -2.8007,  -1.5960, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 34
	action: tensor([[ 0.0863,  0.0388, -0.1311,  0.0701,  0.0036,  0.2537, -0.0925]],
       dtype=torch.float64)
	q_value: tensor([[0.4270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4756375540253708, distance: 0.8286525737430833 entropy tensor([[ -2.0014,  -2.6711, -21.6069,  -2.7214,  -1.6062, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 35
	action: tensor([[ 0.0067,  0.0700, -0.1345,  0.0223,  0.0752,  0.2531, -0.0910]],
       dtype=torch.float64)
	q_value: tensor([[0.4286]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4054542941426925, distance: 0.882367120398416 entropy tensor([[ -1.9890,  -2.6535, -21.6069,  -2.6554,  -1.6197, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 36
	action: tensor([[ 0.0030,  0.0303, -0.1251,  0.0199,  0.1147,  0.2533, -0.0943]],
       dtype=torch.float64)
	q_value: tensor([[0.4256]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3792797902114339, distance: 0.9015807619230003 entropy tensor([[ -2.0016,  -2.6782, -21.6069,  -2.8543,  -1.5859, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 37
	action: tensor([[-0.0077,  0.0410, -0.1255,  0.0505, -0.0125,  0.2523, -0.0956]],
       dtype=torch.float64)
	q_value: tensor([[0.4251]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3764122497825504, distance: 0.9036608781105249 entropy tensor([[ -1.9926,  -2.6927, -21.6069,  -2.7963,  -1.5651, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 38
	action: tensor([[ 0.0089,  0.0368, -0.1274,  0.0330,  0.0609,  0.2518, -0.0919]],
       dtype=torch.float64)
	q_value: tensor([[0.4243]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38969125313202035, distance: 0.8939875905611687 entropy tensor([[ -1.9945,  -2.7133, -21.6069,  -2.8763,  -1.6080, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 39
	action: tensor([[-0.0097,  0.0496, -0.1271,  0.0403,  0.0549,  0.2518, -0.0937]],
       dtype=torch.float64)
	q_value: tensor([[0.4247]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3810702828228947, distance: 0.9002794997288138 entropy tensor([[ -1.9968,  -2.6972, -21.6069,  -2.8052,  -1.5874, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 40
	action: tensor([[-0.0772,  0.0430, -0.1257,  0.0084,  0.1203,  0.2525, -0.0938]],
       dtype=torch.float64)
	q_value: tensor([[0.4244]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2992572518026665, distance: 0.957934907279581 entropy tensor([[ -1.9998,  -2.7135, -21.6069,  -2.8636,  -1.5850, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 41
	action: tensor([[-0.0224,  0.0494, -0.1184,  0.0456,  0.1411,  0.2530, -0.0967]],
       dtype=torch.float64)
	q_value: tensor([[0.4247]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3771947261481461, distance: 0.9030937444285466 entropy tensor([[ -1.9884,  -2.7373, -21.6069,  -3.0088,  -1.5370, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 42
	action: tensor([[-0.0071,  0.0438, -0.1245,  0.0443, -0.0023,  0.2544, -0.0958]],
       dtype=torch.float64)
	q_value: tensor([[0.4253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3778665572973028, distance: 0.9026065213400626 entropy tensor([[ -1.9931,  -2.7325, -21.6069,  -2.8425,  -1.5429, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 43
	action: tensor([[-0.0287,  0.0456, -0.1270,  0.0587,  0.1513,  0.2517, -0.0923]],
       dtype=torch.float64)
	q_value: tensor([[0.4244]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3724035348169096, distance: 0.9065608033490153 entropy tensor([[ -1.9970,  -2.7167, -21.6069,  -2.8778,  -1.6063, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 44
	action: tensor([[ 0.0103,  0.0162, -0.1238,  0.0309,  0.0970,  0.2550, -0.0964]],
       dtype=torch.float64)
	q_value: tensor([[0.4258]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.380747024908202, distance: 0.9005145704293914 entropy tensor([[ -1.9820,  -2.7320, -21.6069,  -2.8522,  -1.5264, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 45
	action: tensor([[ 0.0052,  0.0424, -0.1273,  0.0507, -0.0006,  0.2516, -0.0949]],
       dtype=torch.float64)
	q_value: tensor([[0.4246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39111871283463806, distance: 0.8929414984553552 entropy tensor([[ -1.9928,  -2.7087, -21.6069,  -2.7676,  -1.5743, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 46
	action: tensor([[ 0.0037,  0.0143, -0.1282,  0.0287,  0.0206,  0.2520, -0.0920]],
       dtype=torch.float64)
	q_value: tensor([[0.4246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3651465586896354, distance: 0.9117870765249237 entropy tensor([[ -1.9952,  -2.7024, -21.6069,  -2.8451,  -1.6071, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 47
	action: tensor([[ 0.0310,  0.0295, -0.1278,  0.0250,  0.0974,  0.2505, -0.0926]],
       dtype=torch.float64)
	q_value: tensor([[0.4241]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.40659672779901124, distance: 0.881518968105449 entropy tensor([[ -1.9974,  -2.6911, -21.6069,  -2.8102,  -1.5970, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 48
	action: tensor([[-0.0140,  0.0417, -0.1284,  0.0380,  0.1037,  0.2522, -0.0943]],
       dtype=torch.float64)
	q_value: tensor([[0.4254]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37441885975668976, distance: 0.9051040681891475 entropy tensor([[ -1.9955,  -2.6742, -21.6069,  -2.7394,  -1.5833, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 49
	action: tensor([[ 0.0398,  0.0189, -0.1250,  0.0278,  0.0556,  0.2529, -0.0950]],
       dtype=torch.float64)
	q_value: tensor([[0.4245]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4072221346406979, distance: 0.881054315013295 entropy tensor([[ -1.9976,  -2.7114, -21.6069,  -2.8422,  -1.5635, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 50
	action: tensor([[-0.0084,  0.0276, -0.1296,  0.0436,  0.0606,  0.2513, -0.0934]],
       dtype=torch.float64)
	q_value: tensor([[0.4256]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36957667796358196, distance: 0.9086002016383952 entropy tensor([[ -1.9924,  -2.6751, -21.6069,  -2.7256,  -1.5963, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 51
	action: tensor([[ 0.0029,  0.0486, -0.1263,  0.0257,  0.0440,  0.2518, -0.0941]],
       dtype=torch.float64)
	q_value: tensor([[0.4240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38738575895276317, distance: 0.8956745566948585 entropy tensor([[ -1.9968,  -2.7100, -21.6069,  -2.8328,  -1.5782, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 52
	action: tensor([[ 0.0420,  0.0247, -0.1266,  0.0436, -0.0401,  0.2519, -0.0932]],
       dtype=torch.float64)
	q_value: tensor([[0.4245]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41140811134197197, distance: 0.8779379647820234 entropy tensor([[ -2.0027,  -2.7028, -21.6069,  -2.8413,  -1.5992, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 53
	action: tensor([[-0.0396,  0.0225, -0.1315,  0.0605,  0.0137,  0.2515, -0.0906]],
       dtype=torch.float64)
	q_value: tensor([[0.4262]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3360190089372511, distance: 0.9324692443876665 entropy tensor([[ -1.9967,  -2.6543, -21.6069,  -2.7482,  -1.6243, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 54
	action: tensor([[-0.0111,  0.0613, -0.1255,  0.0455,  0.1070,  0.2513, -0.0930]],
       dtype=torch.float64)
	q_value: tensor([[0.4231]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39269208169270065, distance: 0.8917870573979353 entropy tensor([[ -2.0016,  -2.7357, -21.6069,  -2.9031,  -1.5841, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 55
	action: tensor([[ 0.0335,  0.0530, -0.1253,  0.0271,  0.1093,  0.2541, -0.0947]],
       dtype=torch.float64)
	q_value: tensor([[0.4250]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42668118630677887, distance: 0.8664725127047012 entropy tensor([[ -2.0029,  -2.7101, -21.6069,  -2.8494,  -1.5643, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 56
	action: tensor([[-0.0476,  0.0080, -0.1277,  0.0249,  0.0250,  0.2534, -0.0948]],
       dtype=torch.float64)
	q_value: tensor([[0.4261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30659433506468337, distance: 0.9529067119285387 entropy tensor([[ -1.9990,  -2.6783, -21.6069,  -2.7597,  -1.5801, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 57
	action: tensor([[-0.0532,  0.0743, -0.1235,  0.0696,  0.0696,  0.2500, -0.0940]],
       dtype=torch.float64)
	q_value: tensor([[0.4222]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3622132543783515, distance: 0.9138910792009157 entropy tensor([[ -2.0056,  -2.7341, -21.6069,  -2.9168,  -1.5809, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 58
	action: tensor([[ 0.0443,  0.0526, -0.1231,  0.0622,  0.1001,  0.2547, -0.0941]],
       dtype=torch.float64)
	q_value: tensor([[0.4240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44898958611400774, distance: 0.849447606046114 entropy tensor([[ -2.0064,  -2.7429, -21.6069,  -2.9562,  -1.5632, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 59
	action: tensor([[-0.0263,  0.0544, -0.1301,  0.0500,  0.0880,  0.2541, -0.0941]],
       dtype=torch.float64)
	q_value: tensor([[0.4267]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37294507274198574, distance: 0.9061695941614173 entropy tensor([[ -2.0008,  -2.7005, -21.6069,  -2.7304,  -1.5816, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 60
	action: tensor([[ 0.0303,  0.0318, -0.1243,  0.0405,  0.0622,  0.2535, -0.0947]],
       dtype=torch.float64)
	q_value: tensor([[0.4246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41152237405371217, distance: 0.8778527440683677 entropy tensor([[ -1.9991,  -2.7268, -21.6069,  -2.8864,  -1.5642, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 61
	action: tensor([[ 0.0523,  0.0554, -0.1294,  0.0576,  0.0196,  0.2520, -0.0933]],
       dtype=torch.float64)
	q_value: tensor([[0.4254]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4498885215953683, distance: 0.8487544156205854 entropy tensor([[ -1.9976,  -2.6957, -21.6069,  -2.7489,  -1.5937, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 62
	action: tensor([[-0.0737,  0.0415, -0.1316,  0.0203,  0.0590,  0.2531, -0.0915]],
       dtype=torch.float64)
	q_value: tensor([[0.4268]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30129695287412894, distance: 0.9565397271125613 entropy tensor([[ -1.9983,  -2.6783, -21.6069,  -2.7402,  -1.6159, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 63
	action: tensor([[-0.0403,  0.0346, -0.1198,  0.0372, -0.0250,  0.2520, -0.0951]],
       dtype=torch.float64)
	q_value: tensor([[0.4233]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3329957995835552, distance: 0.9345896726608335 entropy tensor([[ -2.0078,  -2.7228, -21.6069,  -3.0171,  -1.5644, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
LOSS epoch 2 actor 0.07159941777850944 critic 26.174273956226706
epoch: 3, step: 0
	action: tensor([[ 0.0084,  0.0884, -0.1259,  0.0405,  0.0590,  0.3815,  0.0110]],
       dtype=torch.float64)
	q_value: tensor([[0.7574]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4509999041466347, distance: 0.8478966187956799 entropy tensor([[ -1.4942,  -1.7714, -21.6069,  -2.2044,  -1.5719, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 1
	action: tensor([[-0.0366,  0.1266, -0.1231,  0.0772, -0.0912,  0.3838,  0.0107]],
       dtype=torch.float64)
	q_value: tensor([[0.7871]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8478966187956799 entropy tensor([[ -1.5053,  -1.8114, -21.6069,  -2.1964,  -1.4996, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 2
	action: tensor([[ 0.0652, -0.1587,  0.2565,  0.0190,  0.4566,  0.7821, -0.1430]],
       dtype=torch.float64)
	q_value: tensor([[1.4874]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5694247984938324, distance: 0.7508984364485212 entropy tensor([[ -0.7532,  -2.4029, -21.6069,  -1.5005,  -0.7039, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 3
	action: tensor([[-0.0473,  0.0763, -0.1331,  0.0988,  0.0666,  0.4325, -0.0086]],
       dtype=torch.float64)
	q_value: tensor([[0.9284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4286990786017102, distance: 0.8649463204985213 entropy tensor([[ -1.2984,  -2.3663, -21.6069,  -2.1330,  -1.1340, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 4
	action: tensor([[-0.0774,  0.1389, -0.1188,  0.0837,  0.0370,  0.3866,  0.0083]],
       dtype=torch.float64)
	q_value: tensor([[0.7974]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8649463204985213 entropy tensor([[ -1.4808,  -1.8702, -21.6069,  -2.2759,  -1.4568, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 5
	action: tensor([[-0.0472, -0.1803,  0.2565,  0.0767,  0.6319,  0.7821, -0.1430]],
       dtype=torch.float64)
	q_value: tensor([[1.4874]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.46875857842046664, distance: 0.834070302822458 entropy tensor([[ -0.7532,  -2.4029, -21.6069,  -1.5005,  -0.7039, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 6
	action: tensor([[ 0.1424,  0.0921, -0.1222,  0.1286,  0.0043,  0.4499, -0.0168]],
       dtype=torch.float64)
	q_value: tensor([[0.9476]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.834070302822458 entropy tensor([[ -1.1968,  -2.4265, -21.6069,  -2.2450,  -1.0395, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 7
	action: tensor([[-0.0329, -0.1839,  0.2565, -0.0766,  0.4688,  0.7821, -0.1430]],
       dtype=torch.float64)
	q_value: tensor([[1.4874]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39060550047423115, distance: 0.8933177393568087 entropy tensor([[ -0.7532,  -2.4029, -21.6069,  -1.5005,  -0.7039, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 8
	action: tensor([[ 0.0016,  0.1288, -0.1205,  0.1186, -0.0605,  0.4282, -0.0128]],
       dtype=torch.float64)
	q_value: tensor([[0.9271]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8933177393568087 entropy tensor([[ -1.2807,  -2.4013, -21.6069,  -2.2140,  -1.0785, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 9
	action: tensor([[-0.0131, -0.1774,  0.2565,  0.0502,  0.5279,  0.7821, -0.1430]],
       dtype=torch.float64)
	q_value: tensor([[1.4874]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.48685306098833847, distance: 0.8197427126798873 entropy tensor([[ -0.7532,  -2.4029, -21.6069,  -1.5005,  -0.7039, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 10
	action: tensor([[-0.1362,  0.0723, -0.1253,  0.1077, -0.0858,  0.4386, -0.0133]],
       dtype=torch.float64)
	q_value: tensor([[0.9328]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31663129290736336, distance: 0.9459849717349066 entropy tensor([[ -1.2500,  -2.4111, -21.6069,  -2.2224,  -1.0760, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 11
	action: tensor([[ 0.0560,  0.1120, -0.1164,  0.0489,  0.1335,  0.3852,  0.0068]],
       dtype=torch.float64)
	q_value: tensor([[0.7921]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9459849717349066 entropy tensor([[ -1.4920,  -1.8996, -21.6069,  -2.3254,  -1.4472, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 12
	action: tensor([[ 0.0416, -0.1958,  0.2565,  0.0823,  0.3953,  0.7821, -0.1430]],
       dtype=torch.float64)
	q_value: tensor([[1.4874]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5492948304358962, distance: 0.7682506970745385 entropy tensor([[ -0.7532,  -2.4029, -21.6069,  -1.5005,  -0.7039, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 13
	action: tensor([[ 0.0871,  0.1143, -0.1335,  0.0723,  0.0819,  0.4287, -0.0087]],
       dtype=torch.float64)
	q_value: tensor([[0.9174]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7682506970745385 entropy tensor([[ -1.3085,  -2.3788, -21.6069,  -2.1380,  -1.1343, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 14
	action: tensor([[ 0.1874, -0.1744,  0.2565,  0.0087,  0.3340,  0.7821, -0.1430]],
       dtype=torch.float64)
	q_value: tensor([[1.4874]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6092424893501094, distance: 0.7153364587044463 entropy tensor([[ -0.7532,  -2.4029, -21.6069,  -1.5005,  -0.7039, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 15
	action: tensor([[ 0.0334,  0.0963, -0.1397,  0.0974, -0.0728,  0.4217, -0.0039]],
       dtype=torch.float64)
	q_value: tensor([[0.9135]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7153364587044463 entropy tensor([[ -1.3700,  -2.2822, -21.6069,  -2.0555,  -1.1955, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 16
	action: tensor([[ 0.0286, -0.1826,  0.2565,  0.1451,  0.3898,  0.7821, -0.1430]],
       dtype=torch.float64)
	q_value: tensor([[1.4874]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5695895135884954, distance: 0.7507547958746723 entropy tensor([[ -0.7532,  -2.4029, -21.6069,  -1.5005,  -0.7039, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 17
	action: tensor([[ 0.1096,  0.1023, -0.1331,  0.1045,  0.0322,  0.4303, -0.0086]],
       dtype=torch.float64)
	q_value: tensor([[0.9145]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7507547958746723 entropy tensor([[ -1.3043,  -2.3861, -21.6069,  -2.1474,  -1.1372, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 18
	action: tensor([[ 0.0231, -0.2060,  0.2565,  0.0324,  0.4660,  0.7821, -0.1430]],
       dtype=torch.float64)
	q_value: tensor([[1.4874]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4952728443479547, distance: 0.8129896730771626 entropy tensor([[ -0.7532,  -2.4029, -21.6069,  -1.5005,  -0.7039, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 19
	action: tensor([[ 0.0170,  0.0869, -0.1296,  0.0752,  0.1559,  0.4320, -0.0109]],
       dtype=torch.float64)
	q_value: tensor([[0.9274]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5014984995556037, distance: 0.8079601258982518 entropy tensor([[ -1.2729,  -2.4044, -21.6069,  -2.1508,  -1.1070, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 20
	action: tensor([[ 0.0805,  0.1281, -0.1204,  0.0615,  0.1037,  0.3899,  0.0081]],
       dtype=torch.float64)
	q_value: tensor([[0.8059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5656437849228714, distance: 0.7541881655216999 entropy tensor([[ -1.4767,  -1.8384, -21.6069,  -2.2373,  -1.4583, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 21
	action: tensor([[-0.0137,  0.1196, -0.1284,  0.0560, -0.0326,  0.3873,  0.0118]],
       dtype=torch.float64)
	q_value: tensor([[0.7944]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.45359387395963713, distance: 0.8458911342673147 entropy tensor([[ -1.5059,  -1.7905, -21.6069,  -2.1678,  -1.5155, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 22
	action: tensor([[ 0.0189, -0.0187, -0.1245,  0.0887,  0.1070,  0.3828,  0.0114]],
       dtype=torch.float64)
	q_value: tensor([[0.7836]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4326480109255446, distance: 0.861951806421439 entropy tensor([[ -1.5198,  -1.8188, -21.6069,  -2.2322,  -1.5172, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 23
	action: tensor([[-0.0041,  0.1221, -0.1266,  0.0462,  0.0050,  0.3814,  0.0089]],
       dtype=torch.float64)
	q_value: tensor([[0.7899]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4650841965352487, distance: 0.8369497956718831 entropy tensor([[ -1.4645,  -1.8476, -21.6069,  -2.1411,  -1.4784, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 24
	action: tensor([[-0.0377,  0.0954, -0.1234,  0.0691, -0.0576,  0.3841,  0.0114]],
       dtype=torch.float64)
	q_value: tensor([[0.7852]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41744879554560443, distance: 0.8734212331079642 entropy tensor([[ -1.5169,  -1.8061, -21.6069,  -2.2317,  -1.5138, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 25
	action: tensor([[-0.0417,  0.1262, -0.1243,  0.0785,  0.1343,  0.3817,  0.0111]],
       dtype=torch.float64)
	q_value: tensor([[0.7813]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8734212331079642 entropy tensor([[ -1.5154,  -1.8331, -21.6069,  -2.2379,  -1.5101, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 26
	action: tensor([[-0.1230, -0.1667,  0.2565,  0.0017,  0.3392,  0.7821, -0.1430]],
       dtype=torch.float64)
	q_value: tensor([[1.4874]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3361311696141638, distance: 0.9323904840013345 entropy tensor([[ -0.7532,  -2.4029, -21.6069,  -1.5005,  -0.7039, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 27
	action: tensor([[-0.0201,  0.0762, -0.1179,  0.0758, -0.0023,  0.4214, -0.0124]],
       dtype=torch.float64)
	q_value: tensor([[0.9029]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41166637436538056, distance: 0.8777453323356988 entropy tensor([[ -1.3213,  -2.3145, -21.6069,  -2.3411,  -1.1026, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 28
	action: tensor([[ 0.0570,  0.1142, -0.1239,  0.0579,  0.0544,  0.3828,  0.0101]],
       dtype=torch.float64)
	q_value: tensor([[0.7898]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8777453323356988 entropy tensor([[ -1.4976,  -1.8642, -21.6069,  -2.2232,  -1.4969, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 29
	action: tensor([[ 0.0327, -0.1785,  0.2565, -0.0200,  0.5222,  0.7821, -0.1430]],
       dtype=torch.float64)
	q_value: tensor([[1.4874]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5073773665872761, distance: 0.8031818280520914 entropy tensor([[ -0.7532,  -2.4029, -21.6069,  -1.5005,  -0.7039, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 30
	action: tensor([[ 0.0716,  0.0789, -0.1279,  0.0891,  0.1985,  0.4363, -0.0117]],
       dtype=torch.float64)
	q_value: tensor([[0.9372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8031818280520914 entropy tensor([[ -1.2607,  -2.4096, -21.6069,  -2.1444,  -1.0914, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 31
	action: tensor([[ 0.0873, -0.1929,  0.2565, -0.0310,  0.4075,  0.7821, -0.1430]],
       dtype=torch.float64)
	q_value: tensor([[1.4874]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5177386579725558, distance: 0.7946903117183134 entropy tensor([[ -0.7532,  -2.4029, -21.6069,  -1.5005,  -0.7039, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 32
	action: tensor([[ 0.1009,  0.0914, -0.1331,  0.0371,  0.2143,  0.4262, -0.0075]],
       dtype=torch.float64)
	q_value: tensor([[0.9214]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5762078905166457, distance: 0.7449602953255854 entropy tensor([[ -1.3297,  -2.3726, -21.6069,  -2.1118,  -1.1311, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 33
	action: tensor([[ 0.0257,  0.1070, -0.1248,  0.0923,  0.0660,  0.3917,  0.0083]],
       dtype=torch.float64)
	q_value: tensor([[0.8148]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7449602953255854 entropy tensor([[ -1.4783,  -1.8094, -21.6069,  -2.1631,  -1.4796, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 34
	action: tensor([[ 0.0855, -0.2003,  0.2565,  0.0376,  0.4042,  0.7821, -0.1430]],
       dtype=torch.float64)
	q_value: tensor([[1.4874]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5517796244419592, distance: 0.7661300390513934 entropy tensor([[ -0.7532,  -2.4029, -21.6069,  -1.5005,  -0.7039, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 35
	action: tensor([[-0.0325,  0.0894, -0.1353,  0.0589,  0.0063,  0.4283, -0.0073]],
       dtype=torch.float64)
	q_value: tensor([[0.9209]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.43494391598003823, distance: 0.8602060069232496 entropy tensor([[ -1.3215,  -2.3709, -21.6069,  -2.1111,  -1.1304, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 36
	action: tensor([[ 0.0093,  0.0351, -0.1202,  0.0777,  0.0478,  0.3843,  0.0091]],
       dtype=torch.float64)
	q_value: tensor([[0.7930]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4518466304627208, distance: 0.8472425083328501 entropy tensor([[ -1.4995,  -1.8581, -21.6069,  -2.2550,  -1.4865, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 37
	action: tensor([[ 0.0537,  0.0861, -0.1270,  0.0856,  0.1382,  0.3809,  0.0110]],
       dtype=torch.float64)
	q_value: tensor([[0.7854]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5324567637950561, distance: 0.7824697962800462 entropy tensor([[ -1.4927,  -1.8382, -21.6069,  -2.1629,  -1.4967, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 38
	action: tensor([[ 0.0357,  0.0237, -0.1261,  0.0396,  0.1443,  0.3874,  0.0101]],
       dtype=torch.float64)
	q_value: tensor([[0.7959]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4675039070230841, distance: 0.8350546641213643 entropy tensor([[ -1.4827,  -1.7902, -21.6069,  -2.1714,  -1.4867, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 39
	action: tensor([[ 0.0128,  0.0717, -0.1239,  0.0612,  0.0047,  0.3840,  0.0083]],
       dtype=torch.float64)
	q_value: tensor([[0.7953]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.46527641670750775, distance: 0.836799404630668 entropy tensor([[ -1.4786,  -1.8207, -21.6069,  -2.1508,  -1.4734, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 40
	action: tensor([[-0.0050,  0.0344, -0.1273,  0.0254,  0.1017,  0.3809,  0.0119]],
       dtype=torch.float64)
	q_value: tensor([[0.7836]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42339517670101623, distance: 0.868952083239034 entropy tensor([[ -1.5068,  -1.8272, -21.6069,  -2.1672,  -1.5152, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 41
	action: tensor([[-0.0595,  0.0843, -0.1211,  0.0546, -0.0213,  0.3825,  0.0087]],
       dtype=torch.float64)
	q_value: tensor([[0.7878]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3944342195456447, distance: 0.8905070380465975 entropy tensor([[ -1.4937,  -1.8316, -21.6069,  -2.1837,  -1.4793, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 42
	action: tensor([[ 0.0735,  0.1097, -0.1213,  0.0538,  0.0962,  0.3825,  0.0101]],
       dtype=torch.float64)
	q_value: tensor([[0.7815]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8905070380465975 entropy tensor([[ -1.5126,  -1.8347, -21.6069,  -2.2584,  -1.4910, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 43
	action: tensor([[-0.0122, -0.1864,  0.2565, -0.0518,  0.5534,  0.7821, -0.1430]],
       dtype=torch.float64)
	q_value: tensor([[1.4874]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4252612691870723, distance: 0.8675448290980963 entropy tensor([[ -0.7532,  -2.4029, -21.6069,  -1.5005,  -0.7039, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 44
	action: tensor([[ 0.0170,  0.0911, -0.1220,  0.0587, -0.0902,  0.4370, -0.0141]],
       dtype=torch.float64)
	q_value: tensor([[0.9387]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4628383539069708, distance: 0.8387049211637849 entropy tensor([[ -1.2431,  -2.4409, -21.6069,  -2.1794,  -1.0557, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 45
	action: tensor([[ 0.0049,  0.1078, -0.1287,  0.0101,  0.1044,  0.3807,  0.0110]],
       dtype=torch.float64)
	q_value: tensor([[0.7895]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8387049211637849 entropy tensor([[ -1.5167,  -1.8764, -21.6069,  -2.1900,  -1.5319, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 46
	action: tensor([[-0.1187, -0.1896,  0.2565,  0.0673,  0.6568,  0.7821, -0.1430]],
       dtype=torch.float64)
	q_value: tensor([[1.4874]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3803202266949295, distance: 0.9008248408884021 entropy tensor([[ -0.7532,  -2.4029, -21.6069,  -1.5005,  -0.7039, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 47
	action: tensor([[-0.0006,  0.0428, -0.1166,  0.0499, -0.0071,  0.4526, -0.0197]],
       dtype=torch.float64)
	q_value: tensor([[0.9549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41202481743388697, distance: 0.8774779078233875 entropy tensor([[ -1.1838,  -2.4653, -21.6069,  -2.2896,  -1.0035, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 48
	action: tensor([[ 0.0391,  0.1400, -0.1248,  0.0497,  0.0375,  0.3813,  0.0093]],
       dtype=torch.float64)
	q_value: tensor([[0.7945]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8774779078233875 entropy tensor([[ -1.4939,  -1.8940, -21.6069,  -2.1921,  -1.4937, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 49
	action: tensor([[-0.0066, -0.1517,  0.2565,  0.0774,  0.5934,  0.7821, -0.1430]],
       dtype=torch.float64)
	q_value: tensor([[1.4874]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.522627763487219, distance: 0.7906518144198058 entropy tensor([[ -0.7532,  -2.4029, -21.6069,  -1.5005,  -0.7039, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 50
	action: tensor([[-0.0151,  0.0941, -0.1244,  0.0425,  0.2288,  0.4458, -0.0147]],
       dtype=torch.float64)
	q_value: tensor([[0.9418]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.47391931185718283, distance: 0.8300091366579959 entropy tensor([[ -1.2217,  -2.3893, -21.6069,  -2.2414,  -1.0711, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 51
	action: tensor([[ 0.0446,  0.0494, -0.1141,  0.1000,  0.0931,  0.3945,  0.0050]],
       dtype=torch.float64)
	q_value: tensor([[0.8168]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5012657802708438, distance: 0.8081486970088094 entropy tensor([[ -1.4647,  -1.8355, -21.6069,  -2.2933,  -1.4107, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 52
	action: tensor([[-0.0219,  0.0778, -0.1290,  0.0101,  0.0127,  0.3836,  0.0111]],
       dtype=torch.float64)
	q_value: tensor([[0.7915]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4091698125008211, distance: 0.8796056932206463 entropy tensor([[ -1.4837,  -1.8312, -21.6069,  -2.1564,  -1.4975, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 53
	action: tensor([[ 0.0877,  0.1481, -0.1209,  0.0514,  0.0379,  0.3822,  0.0099]],
       dtype=torch.float64)
	q_value: tensor([[0.7839]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8796056932206463 entropy tensor([[ -1.5158,  -1.8268, -21.6069,  -2.2139,  -1.4997, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 54
	action: tensor([[-0.0160, -0.1422,  0.2565, -0.0684,  0.5544,  0.7821, -0.1430]],
       dtype=torch.float64)
	q_value: tensor([[1.4874]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4400053852787399, distance: 0.8563447086195198 entropy tensor([[ -0.7532,  -2.4029, -21.6069,  -1.5005,  -0.7039, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 55
	action: tensor([[ 0.0220,  0.0557, -0.1190,  0.0527,  0.2217,  0.4364, -0.0143]],
       dtype=torch.float64)
	q_value: tensor([[0.9391]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.48965774523607075, distance: 0.817499427704611 entropy tensor([[ -1.2557,  -2.4210, -21.6069,  -2.2280,  -1.0620, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 56
	action: tensor([[ 0.0087,  0.0078, -0.1198,  0.0479,  0.0184,  0.3913,  0.0062]],
       dtype=torch.float64)
	q_value: tensor([[0.8136]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.414580514602922, distance: 0.8755688051330327 entropy tensor([[ -1.4597,  -1.8437, -21.6069,  -2.2196,  -1.4323, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 57
	action: tensor([[ 0.0156,  0.0736, -0.1271,  0.0529,  0.1621,  0.3786,  0.0106]],
       dtype=torch.float64)
	q_value: tensor([[0.7841]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.473504500677279, distance: 0.8303363005653718 entropy tensor([[ -1.4964,  -1.8600, -21.6069,  -2.1370,  -1.5020, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 58
	action: tensor([[ 0.1101, -0.0100, -0.1208,  0.0953,  0.0956,  0.3882,  0.0081]],
       dtype=torch.float64)
	q_value: tensor([[0.7952]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5240618556447829, distance: 0.7894633074907571 entropy tensor([[ -1.4767,  -1.7956, -21.6069,  -2.2054,  -1.4604, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 59
	action: tensor([[ 0.0065,  0.0837, -0.1361,  0.1008, -0.0014,  0.3806,  0.0117]],
       dtype=torch.float64)
	q_value: tensor([[0.7942]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7894633074907571 entropy tensor([[ -1.4855,  -1.8443, -21.6069,  -2.0572,  -1.5128, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 60
	action: tensor([[ 0.0495, -0.1885,  0.2565,  0.0434,  0.4353,  0.7821, -0.1430]],
       dtype=torch.float64)
	q_value: tensor([[1.4874]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5489577837750967, distance: 0.7685379002820817 entropy tensor([[ -0.7532,  -2.4029, -21.6069,  -1.5005,  -0.7039, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 61
	action: tensor([[ 0.0140,  0.0980, -0.1331,  0.0957,  0.1316,  0.4295, -0.0090]],
       dtype=torch.float64)
	q_value: tensor([[0.9251]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7685379002820817 entropy tensor([[ -1.2958,  -2.3572, -21.6069,  -2.1144,  -1.1356, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 62
	action: tensor([[ 0.0131, -0.2156,  0.2565,  0.0926,  0.3901,  0.7821, -0.1430]],
       dtype=torch.float64)
	q_value: tensor([[1.4874]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4982444441128604, distance: 0.8105928867181725 entropy tensor([[ -0.7532,  -2.4029, -21.6069,  -1.5005,  -0.7039, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 63
	action: tensor([[-4.0011e-04,  9.6563e-02, -1.3067e-01,  1.2567e-01,  1.0703e-01,
          4.2787e-01, -9.4146e-03]], dtype=torch.float64)
	q_value: tensor([[0.9131]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8105928867181725 entropy tensor([[ -1.3039,  -2.3979, -21.6069,  -2.1593,  -1.1177, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
LOSS epoch 3 actor 384.0214872816478 critic 2083.1673550492765
epoch: 4, step: 0
	action: tensor([[-0.0122, -0.0631,  0.6817,  0.1322,  0.4012,  0.5300, -0.5222]],
       dtype=torch.float64)
	q_value: tensor([[0.9064]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5281917786491167, distance: 0.7860305856167008 entropy tensor([[ -0.5429,  -1.0593, -21.6069,  -0.4876,  -0.2581, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 1
	action: tensor([[ 0.0736,  0.0344, -0.1331,  0.1494,  0.0118,  0.3204, -0.0123]],
       dtype=torch.float64)
	q_value: tensor([[0.5436]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5048753159288555, distance: 0.8052189414862306 entropy tensor([[ -1.1841,  -1.6824, -21.6069,  -1.6903,  -1.1335, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 2
	action: tensor([[ 0.0291,  0.0289, -0.0786,  0.0237,  0.0237,  0.2741,  0.0097]],
       dtype=torch.float64)
	q_value: tensor([[0.5136]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41347406990133506, distance: 0.8763958284707003 entropy tensor([[ -1.3992,  -1.4768, -21.6069,  -1.7498,  -1.4992, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 3
	action: tensor([[ 0.0628,  0.1586, -0.0753,  0.1051,  0.0390,  0.2747,  0.0064]],
       dtype=torch.float64)
	q_value: tensor([[0.5019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8763958284707003 entropy tensor([[ -1.4320,  -1.4537, -21.6069,  -1.7918,  -1.4939, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 4
	action: tensor([[ 0.1560, -0.3487,  0.6817,  0.0078,  0.8107,  0.5300, -0.5222]],
       dtype=torch.float64)
	q_value: tensor([[0.9064]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31659316108908997, distance: 0.9460113642397631 entropy tensor([[ -0.5429,  -1.0593, -21.6069,  -0.4876,  -0.2581, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 5
	action: tensor([[ 0.1502,  0.0327, -0.1546,  0.1135,  0.1083,  0.3397, -0.0199]],
       dtype=torch.float64)
	q_value: tensor([[0.5903]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5781939401432551, distance: 0.7432126629800089 entropy tensor([[ -1.0545,  -1.6641, -21.6069,  -1.5620,  -0.9598, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 6
	action: tensor([[-0.0396,  0.0783, -0.0814,  0.0270,  0.0943,  0.2740,  0.0100]],
       dtype=torch.float64)
	q_value: tensor([[0.5206]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3913028422914061, distance: 0.89280647273807 entropy tensor([[ -1.4103,  -1.4708, -21.6069,  -1.7111,  -1.5116, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 7
	action: tensor([[ 0.0471,  0.0459, -0.0579,  0.1035,  0.1980,  0.2812, -0.0037]],
       dtype=torch.float64)
	q_value: tensor([[0.4993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4973236258730116, distance: 0.8113363429383419 entropy tensor([[ -1.4366,  -1.4227, -21.6069,  -1.8274,  -1.4423, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 8
	action: tensor([[ 0.0238,  0.0456, -0.0764,  0.1276,  0.0540,  0.2818,  0.0048]],
       dtype=torch.float64)
	q_value: tensor([[0.5085]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4712141083867054, distance: 0.8321404299681194 entropy tensor([[ -1.3814,  -1.4470, -21.6069,  -1.7771,  -1.4475, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 9
	action: tensor([[ 0.0464,  0.1605, -0.0764,  0.0899, -0.0007,  0.2781,  0.0083]],
       dtype=torch.float64)
	q_value: tensor([[0.5055]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8321404299681194 entropy tensor([[ -1.4023,  -1.4529, -21.6069,  -1.7751,  -1.4797, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 10
	action: tensor([[ 0.0969, -0.0619,  0.6817, -0.1319,  0.2203,  0.5300, -0.5222]],
       dtype=torch.float64)
	q_value: tensor([[0.9064]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.45723401560347543, distance: 0.8430687747848441 entropy tensor([[ -0.5429,  -1.0593, -21.6069,  -0.4876,  -0.2581, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 11
	action: tensor([[-0.0019,  0.1063, -0.1379,  0.1045, -0.0931,  0.3085, -0.0081]],
       dtype=torch.float64)
	q_value: tensor([[0.5377]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8430687747848441 entropy tensor([[ -1.2237,  -1.6220, -21.6069,  -1.6969,  -1.2056, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 12
	action: tensor([[-0.1518, -0.3008,  0.6817,  0.1621,  0.6222,  0.5300, -0.5222]],
       dtype=torch.float64)
	q_value: tensor([[0.9064]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22075781044449816, distance: 1.0101664076306993 entropy tensor([[ -0.5429,  -1.0593, -21.6069,  -0.4876,  -0.2581, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 13
	action: tensor([[ 0.0148,  0.1362, -0.1274,  0.1212,  0.0278,  0.3357, -0.0305]],
       dtype=torch.float64)
	q_value: tensor([[0.5505]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.47340623479608557, distance: 0.8304137845339236 entropy tensor([[ -1.1060,  -1.7088, -21.6069,  -1.6466,  -0.9509, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 14
	action: tensor([[ 7.2976e-02,  1.3523e-01, -6.4010e-02,  1.4432e-01,  2.4013e-02,
          2.7993e-01,  1.9065e-04]], dtype=torch.float64)
	q_value: tensor([[0.5111]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8304137845339236 entropy tensor([[ -1.4368,  -1.4213, -21.6069,  -1.7951,  -1.4877, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 15
	action: tensor([[-0.0321, -0.2841,  0.6817, -0.0603,  0.3460,  0.5300, -0.5222]],
       dtype=torch.float64)
	q_value: tensor([[0.9064]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20400762426560415, distance: 1.0209656889859482 entropy tensor([[ -0.5429,  -1.0593, -21.6069,  -0.4876,  -0.2581, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 16
	action: tensor([[ 0.0564,  0.1327, -0.1309,  0.0042,  0.1298,  0.3119, -0.0182]],
       dtype=torch.float64)
	q_value: tensor([[0.5367]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.503465100050646, distance: 0.8063648398485 entropy tensor([[ -1.2015,  -1.6669, -21.6069,  -1.6960,  -1.0638, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 17
	action: tensor([[ 0.0973,  0.0483, -0.0596,  0.0620,  0.1102,  0.2808, -0.0035]],
       dtype=torch.float64)
	q_value: tensor([[0.5085]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5119483088555997, distance: 0.7994468658727927 entropy tensor([[ -1.4419,  -1.4163, -21.6069,  -1.7761,  -1.4831, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 18
	action: tensor([[-0.0252,  0.1470, -0.0836,  0.1071,  0.0928,  0.2765,  0.0106]],
       dtype=torch.float64)
	q_value: tensor([[0.5088]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4595264465257811, distance: 0.8412864947386022 entropy tensor([[ -1.4061,  -1.4662, -21.6069,  -1.7490,  -1.5042, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 19
	action: tensor([[ 0.0801,  0.1342, -0.0603,  0.0818,  0.0184,  0.2856, -0.0015]],
       dtype=torch.float64)
	q_value: tensor([[0.5042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8412864947386022 entropy tensor([[ -1.4289,  -1.4087, -21.6069,  -1.8055,  -1.4533, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 20
	action: tensor([[ 0.1117, -0.0703,  0.6817,  0.1814,  0.8433,  0.5300, -0.5222]],
       dtype=torch.float64)
	q_value: tensor([[0.9064]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6023723318700457, distance: 0.7215974526611277 entropy tensor([[ -0.5429,  -1.0593, -21.6069,  -0.4876,  -0.2581, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 21
	action: tensor([[-0.0354,  0.0552, -0.1437,  0.0776,  0.1629,  0.3518, -0.0250]],
       dtype=torch.float64)
	q_value: tensor([[0.5822]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41528516564092766, distance: 0.8750416991418702 entropy tensor([[ -1.0162,  -1.6881, -21.6069,  -1.6226,  -0.9991, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 22
	action: tensor([[-0.0014,  0.1454, -0.0517,  0.0243,  0.0611,  0.2799, -0.0085]],
       dtype=torch.float64)
	q_value: tensor([[0.5111]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4695403652161473, distance: 0.8334563585797585 entropy tensor([[ -1.4050,  -1.4193, -21.6069,  -1.8187,  -1.3899, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 23
	action: tensor([[ 0.0812,  0.0571, -0.0653,  0.0559,  0.0521,  0.2826,  0.0005]],
       dtype=torch.float64)
	q_value: tensor([[0.5007]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5073069349427485, distance: 0.8032392425833572 entropy tensor([[ -1.4485,  -1.4276, -21.6069,  -1.8122,  -1.4849, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 24
	action: tensor([[-0.0363,  0.1681, -0.0820,  0.0434,  0.0343,  0.2753,  0.0106]],
       dtype=torch.float64)
	q_value: tensor([[0.5063]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8032392425833572 entropy tensor([[ -1.4231,  -1.4609, -21.6069,  -1.7629,  -1.5123, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 25
	action: tensor([[ 0.2987, -0.2574,  0.6817, -0.1556,  0.1972,  0.5300, -0.5222]],
       dtype=torch.float64)
	q_value: tensor([[0.9064]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4222994036765544, distance: 0.8697773644958442 entropy tensor([[ -0.5429,  -1.0593, -21.6069,  -0.4876,  -0.2581, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 26
	action: tensor([[ 0.0038,  0.1797, -0.1592,  0.1259,  0.0462,  0.3028, -0.0024]],
       dtype=torch.float64)
	q_value: tensor([[0.5440]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8697773644958442 entropy tensor([[ -1.1784,  -1.6109, -21.6069,  -1.5201,  -1.1679, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 27
	action: tensor([[ 0.0574, -0.2762,  0.6817, -0.3881,  0.5436,  0.5300, -0.5222]],
       dtype=torch.float64)
	q_value: tensor([[0.9064]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03206390956108107, distance: 1.1258487109199593 entropy tensor([[ -0.5429,  -1.0593, -21.6069,  -0.4876,  -0.2581, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 28
	action: tensor([[ 0.1663,  0.0981, -0.1304,  0.1667,  0.0203,  0.3268, -0.0245]],
       dtype=torch.float64)
	q_value: tensor([[0.5697]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6283345909525487, distance: 0.6976422351409223 entropy tensor([[ -1.0605,  -1.6330, -21.6069,  -1.6463,  -0.9630, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 29
	action: tensor([[-0.0337,  0.0911, -0.0865,  0.0387,  0.0710,  0.2754,  0.0143]],
       dtype=torch.float64)
	q_value: tensor([[0.5236]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.40053625380946234, distance: 0.8860090439078792 entropy tensor([[ -1.4053,  -1.4886, -21.6069,  -1.6976,  -1.5432, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 30
	action: tensor([[-0.0304,  0.0962, -0.0593,  0.0646,  0.0064,  0.2812, -0.0025]],
       dtype=torch.float64)
	q_value: tensor([[0.5002]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41532087348172375, distance: 0.8750149798550504 entropy tensor([[ -1.4432,  -1.4206, -21.6069,  -1.8243,  -1.4536, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 31
	action: tensor([[-0.0439,  0.1108, -0.0668,  0.0455,  0.0153,  0.2800,  0.0020]],
       dtype=torch.float64)
	q_value: tensor([[0.5002]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8750149798550504 entropy tensor([[ -1.4430,  -1.4289, -21.6069,  -1.8305,  -1.4786, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 32
	action: tensor([[ 0.2736, -0.1743,  0.6817, -0.1048,  0.7511,  0.5300, -0.5222]],
       dtype=torch.float64)
	q_value: tensor([[0.9064]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.47902944028032535, distance: 0.8259681182039637 entropy tensor([[ -0.5429,  -1.0593, -21.6069,  -0.4876,  -0.2581, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 33
	action: tensor([[-0.1007,  0.0939, -0.1621,  0.0580,  0.0749,  0.3345, -0.0120]],
       dtype=torch.float64)
	q_value: tensor([[0.5982]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35598145193331177, distance: 0.9183450321557254 entropy tensor([[ -1.0485,  -1.6583, -21.6069,  -1.5662,  -1.0262, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 34
	action: tensor([[-0.0785,  0.0720, -0.0383,  0.0777,  0.0183,  0.2817, -0.0158]],
       dtype=torch.float64)
	q_value: tensor([[0.5055]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37488578503766445, distance: 0.9047662264350489 entropy tensor([[ -1.4451,  -1.3936, -21.6069,  -1.8793,  -1.3888, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 35
	action: tensor([[ 0.0060,  0.0766, -0.0640,  0.0506,  0.0624,  0.2808, -0.0003]],
       dtype=torch.float64)
	q_value: tensor([[0.4971]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4529582423408285, distance: 0.8463830018064485 entropy tensor([[ -1.4353,  -1.4258, -21.6069,  -1.8587,  -1.4492, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 36
	action: tensor([[-0.0299,  0.0992, -0.0706, -0.0243,  0.0600,  0.2789,  0.0040]],
       dtype=torch.float64)
	q_value: tensor([[0.5017]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4020326961490771, distance: 0.8849024799772605 entropy tensor([[ -1.4303,  -1.4366, -21.6069,  -1.8006,  -1.4824, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 37
	action: tensor([[-0.0568,  0.1690, -0.0586,  0.0420, -0.0377,  0.2804, -0.0040]],
       dtype=torch.float64)
	q_value: tensor([[0.4979]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8849024799772605 entropy tensor([[ -1.4545,  -1.4266, -21.6069,  -1.8383,  -1.4644, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 38
	action: tensor([[ 0.3978, -0.1955,  0.6817,  0.0438,  0.4646,  0.5300, -0.5222]],
       dtype=torch.float64)
	q_value: tensor([[0.9064]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6419271259577912, distance: 0.6847663401087248 entropy tensor([[ -0.5429,  -1.0593, -21.6069,  -0.4876,  -0.2581, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 39
	action: tensor([[ 1.0584e-02,  5.6319e-02, -1.7158e-01,  3.6058e-02,  1.2635e-01,
          3.1226e-01,  1.0515e-04]], dtype=torch.float64)
	q_value: tensor([[0.5740]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44570910416263576, distance: 0.8519724787311539 entropy tensor([[ -1.1637,  -1.6952, -21.6069,  -1.4845,  -1.1561, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 40
	action: tensor([[ 1.4454e-04,  5.2134e-02, -5.4134e-02,  1.5033e-02,  1.7163e-01,
          2.7821e-01, -5.4140e-03]], dtype=torch.float64)
	q_value: tensor([[0.5083]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4328373901318101, distance: 0.8618079368517781 entropy tensor([[ -1.4252,  -1.4241, -21.6069,  -1.7885,  -1.4418, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 41
	action: tensor([[ 0.0207,  0.0760, -0.0650,  0.0529,  0.0194,  0.2810, -0.0018]],
       dtype=torch.float64)
	q_value: tensor([[0.5008]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.47028375442875014, distance: 0.8328721485387357 entropy tensor([[ -1.4051,  -1.4409, -21.6069,  -1.8060,  -1.4375, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 42
	action: tensor([[ 0.0213,  0.1095, -0.0739, -0.0382,  0.0539,  0.2775,  0.0062]],
       dtype=torch.float64)
	q_value: tensor([[0.5025]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4588153252746583, distance: 0.8418397688470437 entropy tensor([[ -1.4346,  -1.4434, -21.6069,  -1.7972,  -1.4972, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 43
	action: tensor([[-0.0210,  0.1164, -0.0640,  0.0266,  0.0164,  0.2794, -0.0009]],
       dtype=torch.float64)
	q_value: tensor([[0.5002]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44308091825974905, distance: 0.8539899158839557 entropy tensor([[ -1.4607,  -1.4282, -21.6069,  -1.8022,  -1.4944, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 44
	action: tensor([[-1.2821e-02,  7.0926e-02, -6.3855e-02,  7.2694e-02,  8.1237e-02,
          2.8074e-01,  1.6734e-05]], dtype=torch.float64)
	q_value: tensor([[0.4997]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4467789544553642, distance: 0.8511498752422267 entropy tensor([[ -1.4545,  -1.4257, -21.6069,  -1.8290,  -1.4849, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 45
	action: tensor([[ 0.0468,  0.1445, -0.0686,  0.0604,  0.0363,  0.2801,  0.0029]],
       dtype=torch.float64)
	q_value: tensor([[0.5015]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8511498752422267 entropy tensor([[ -1.4224,  -1.4324, -21.6069,  -1.8064,  -1.4639, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 46
	action: tensor([[ 0.0680, -0.2410,  0.6817,  0.1801,  1.1231,  0.5300, -0.5222]],
       dtype=torch.float64)
	q_value: tensor([[0.9064]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4295831636544669, distance: 0.8642768115315682 entropy tensor([[ -0.5429,  -1.0593, -21.6069,  -0.4876,  -0.2581, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 47
	action: tensor([[ 0.0616,  0.0965, -0.1421,  0.1435,  0.1584,  0.3751, -0.0419]],
       dtype=torch.float64)
	q_value: tensor([[0.6103]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8642768115315682 entropy tensor([[ -0.9326,  -1.5585, -21.6069,  -1.5638,  -0.8894, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 48
	action: tensor([[-0.2152, -0.0315,  0.6817,  0.0518,  0.2309,  0.5300, -0.5222]],
       dtype=torch.float64)
	q_value: tensor([[0.9064]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26566475637452147, distance: 0.9806270738141843 entropy tensor([[ -0.5429,  -1.0593, -21.6069,  -0.4876,  -0.2581, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 49
	action: tensor([[ 0.0195,  0.1135, -0.1085,  0.1304,  0.1482,  0.3203, -0.0197]],
       dtype=torch.float64)
	q_value: tensor([[0.5268]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9806270738141843 entropy tensor([[ -1.2286,  -1.5815, -21.6069,  -1.8338,  -1.1224, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 50
	action: tensor([[ 0.0610, -0.2561,  0.6817,  0.2210,  0.3668,  0.5300, -0.5222]],
       dtype=torch.float64)
	q_value: tensor([[0.9064]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5178747704601473, distance: 0.7945781578931232 entropy tensor([[ -0.5429,  -1.0593, -21.6069,  -0.4876,  -0.2581, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 51
	action: tensor([[ 0.0048,  0.0685, -0.1472,  0.2353,  0.2041,  0.3132, -0.0091]],
       dtype=torch.float64)
	q_value: tensor([[0.5422]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7945781578931232 entropy tensor([[ -1.1981,  -1.7534, -21.6069,  -1.5888,  -1.1239, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 52
	action: tensor([[ 0.0432, -0.2665,  0.6817,  0.1347,  0.5055,  0.5300, -0.5222]],
       dtype=torch.float64)
	q_value: tensor([[0.9064]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42921030396412196, distance: 0.864559237750182 entropy tensor([[ -0.5429,  -1.0593, -21.6069,  -0.4876,  -0.2581, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 53
	action: tensor([[ 0.0362,  0.0276, -0.1436,  0.1104,  0.0887,  0.3205, -0.0148]],
       dtype=torch.float64)
	q_value: tensor([[0.5517]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4627937389718775, distance: 0.8387397505200445 entropy tensor([[ -1.1606,  -1.7437, -21.6069,  -1.6124,  -1.0672, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 54
	action: tensor([[ 0.0250,  0.1519, -0.0694,  0.0139, -0.0051,  0.2758,  0.0037]],
       dtype=torch.float64)
	q_value: tensor([[0.5103]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.48365960109324957, distance: 0.8222895029357064 entropy tensor([[ -1.4053,  -1.4451, -21.6069,  -1.7673,  -1.4679, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 55
	action: tensor([[ 2.7144e-04,  1.1860e-01, -6.8471e-02,  1.0988e-01,  2.6199e-02,
          2.8090e-01,  2.2015e-03]], dtype=torch.float64)
	q_value: tensor([[0.5015]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8222895029357064 entropy tensor([[ -1.4697,  -1.4251, -21.6069,  -1.7936,  -1.5202, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 56
	action: tensor([[ 0.0069, -0.1564,  0.6817,  0.2533,  0.5449,  0.5300, -0.5222]],
       dtype=torch.float64)
	q_value: tensor([[0.9064]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.555797480338406, distance: 0.7626885071438909 entropy tensor([[ -0.5429,  -1.0593, -21.6069,  -0.4876,  -0.2581, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 57
	action: tensor([[-0.0423,  0.0443, -0.1418,  0.0234,  0.0390,  0.3287, -0.0149]],
       dtype=torch.float64)
	q_value: tensor([[0.5536]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35593713959803597, distance: 0.9183766254329004 entropy tensor([[ -1.1345,  -1.7398, -21.6069,  -1.6151,  -1.0709, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 58
	action: tensor([[-0.0315,  0.1233, -0.0527,  0.0688,  0.1522,  0.2764, -0.0070]],
       dtype=torch.float64)
	q_value: tensor([[0.5036]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4394958911956993, distance: 0.8567341796577942 entropy tensor([[ -1.4456,  -1.4223, -21.6069,  -1.8400,  -1.4429, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 59
	action: tensor([[-0.0787,  0.0686, -0.0611,  0.1023,  0.0370,  0.2860, -0.0026]],
       dtype=torch.float64)
	q_value: tensor([[0.5009]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36334231720512755, distance: 0.9130817981055424 entropy tensor([[ -1.4170,  -1.4198, -21.6069,  -1.8218,  -1.4306, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 60
	action: tensor([[-0.0129,  0.0759, -0.0624,  0.0654,  0.1850,  0.2808, -0.0010]],
       dtype=torch.float64)
	q_value: tensor([[0.4993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4320396943229705, distance: 0.862413776353874 entropy tensor([[ -1.4287,  -1.4205, -21.6069,  -1.8533,  -1.4382, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 61
	action: tensor([[ 0.0917,  0.1413, -0.0632,  0.1012, -0.0110,  0.2835, -0.0023]],
       dtype=torch.float64)
	q_value: tensor([[0.5029]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.862413776353874 entropy tensor([[ -1.3995,  -1.4291, -21.6069,  -1.8122,  -1.4201, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 62
	action: tensor([[ 0.0950, -0.4008,  0.6817, -0.2349,  0.7128,  0.5300, -0.5222]],
       dtype=torch.float64)
	q_value: tensor([[0.9064]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04545945392737083, distance: 1.1180310980013541 entropy tensor([[ -0.5429,  -1.0593, -21.6069,  -0.4876,  -0.2581, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 63
	action: tensor([[-0.0766,  0.0962, -0.1422,  0.1213,  0.0008,  0.3340, -0.0243]],
       dtype=torch.float64)
	q_value: tensor([[0.5888]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1180310980013541 entropy tensor([[ -1.0335,  -1.6475, -21.6069,  -1.5745,  -0.9158, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
LOSS epoch 4 actor 319.2008743118267 critic 2001.1705159955472
epoch: 5, step: 0
	action: tensor([[ 0.5153,  0.1946,  1.2785,  0.0163,  0.6688,  0.3798, -0.7444]],
       dtype=torch.float64)
	q_value: tensor([[0.3590]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1180310980013541 entropy tensor([[ -0.0624,  -0.3129, -21.6069,   0.1101,   0.1565,  -2.7059, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 1
	action: tensor([[ 0.2019, -0.0377,  1.2785, -0.1307,  1.1923,  0.3458, -0.7444]],
       dtype=torch.float64)
	q_value: tensor([[0.3590]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37937216853062306, distance: 0.9015136708140887 entropy tensor([[ -0.0624,  -0.3129, -21.6069,   0.1101,   0.1565,  -2.7059, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 2
	action: tensor([[-0.0371, -0.0075, -0.0535,  0.1922, -0.0926, -0.0344, -0.1612]],
       dtype=torch.float64)
	q_value: tensor([[0.1428]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34331917505872833, distance: 0.9273290412015273 entropy tensor([[ -1.0889,  -1.4779, -21.6069,  -1.0149,  -0.6479, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 3
	action: tensor([[ 0.0517,  0.0686,  0.0119,  0.0614,  0.1653,  0.1049, -0.0324]],
       dtype=torch.float64)
	q_value: tensor([[0.2056]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.46369209695594027, distance: 0.838038154242892 entropy tensor([[ -1.4321,  -1.3185, -21.6069,  -1.4994,  -1.3996, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 4
	action: tensor([[ 0.0341,  0.0378, -0.0077,  0.1580,  0.0476,  0.0815, -0.0242]],
       dtype=torch.float64)
	q_value: tensor([[0.2044]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4587728253038964, distance: 0.841872823610818 entropy tensor([[ -1.4951,  -1.4986, -21.6069,  -1.5234,  -1.3966, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 5
	action: tensor([[-0.0105,  0.1320, -0.0156,  0.0483,  0.0585,  0.0859, -0.0165]],
       dtype=torch.float64)
	q_value: tensor([[0.2072]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42855985387885764, distance: 0.8650517067945183 entropy tensor([[ -1.4817,  -1.4657, -21.6069,  -1.5473,  -1.4407, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 6
	action: tensor([[-0.0335,  0.1491,  0.0163, -0.0083,  0.1190,  0.0933, -0.0338]],
       dtype=torch.float64)
	q_value: tensor([[0.2031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3972951221047216, distance: 0.8884010158149316 entropy tensor([[ -1.4878,  -1.4199, -21.6069,  -1.5438,  -1.3874, -21.6069,  -4.2399]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 7
	action: tensor([[-0.0462,  0.1555,  0.0285,  0.0333,  0.0258,  0.0911, -0.0433]],
       dtype=torch.float64)
	q_value: tensor([[0.2000]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4030709127167992, distance: 0.8841339433560617 entropy tensor([[ -1.4850,  -1.4259, -21.6069,  -1.5357,  -1.3645, -21.6069,  -4.0576]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 8
	action: tensor([[ 0.0557,  0.0602,  0.0201,  0.0494,  0.1042,  0.0932, -0.0424]],
       dtype=torch.float64)
	q_value: tensor([[0.1994]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.452821557212551, distance: 0.8464887348167173 entropy tensor([[ -1.4971,  -1.3995, -21.6069,  -1.5619,  -1.4049, -21.6069,  -3.9335]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 9
	action: tensor([[ 0.0105,  0.0211, -0.0121,  0.0920,  0.1061,  0.0829, -0.0207]],
       dtype=torch.float64)
	q_value: tensor([[0.2030]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3977270503859116, distance: 0.8880826225915753 entropy tensor([[ -1.5042,  -1.4988, -21.6069,  -1.5312,  -1.4306, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 10
	action: tensor([[-0.0135,  0.0215, -0.0041,  0.1328, -0.0092,  0.0855, -0.0230]],
       dtype=torch.float64)
	q_value: tensor([[0.2034]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3908100331336456, distance: 0.8931678134986586 entropy tensor([[ -1.4878,  -1.4632, -21.6069,  -1.5451,  -1.4004, -21.6069,  -6.6893]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 11
	action: tensor([[-0.0141, -0.0047, -0.0074, -0.0235,  0.0806,  0.0871, -0.0201]],
       dtype=torch.float64)
	q_value: tensor([[0.2046]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30307188779060523, distance: 0.9553239922027987 entropy tensor([[ -1.4916,  -1.4275, -21.6069,  -1.5761,  -1.4349, -21.6069,  -4.7557]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 12
	action: tensor([[-0.0435,  0.1073,  0.0108,  0.0603, -0.0061,  0.0851, -0.0337]],
       dtype=torch.float64)
	q_value: tensor([[0.1985]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3846257801084605, distance: 0.8976899071597231 entropy tensor([[ -1.5103,  -1.4496, -21.6069,  -1.5478,  -1.3833, -21.6069,  -4.4751]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 13
	action: tensor([[-0.0116,  0.1086,  0.0131,  0.0633,  0.1166,  0.0930, -0.0304]],
       dtype=torch.float64)
	q_value: tensor([[0.2002]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42386371589014527, distance: 0.8685989637107202 entropy tensor([[ -1.5036,  -1.3988, -21.6069,  -1.5728,  -1.4123, -21.6069,  -4.0603]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 14
	action: tensor([[ 0.0347,  0.1133,  0.0087,  0.0002,  0.0263,  0.0887, -0.0321]],
       dtype=torch.float64)
	q_value: tensor([[0.2019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4425597842081832, distance: 0.8543893807042747 entropy tensor([[ -1.4892,  -1.4486, -21.6069,  -1.5509,  -1.3833, -21.6069,  -4.8213]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 15
	action: tensor([[-0.0505,  0.0711,  0.0024, -0.0859,  0.0408,  0.0885, -0.0256]],
       dtype=torch.float64)
	q_value: tensor([[0.2020]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2908647275563173, distance: 0.9636542412035503 entropy tensor([[ -1.5198,  -1.4555, -21.6069,  -1.5437,  -1.4325, -21.6069,  -4.6949]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 16
	action: tensor([[-0.0493,  0.0173,  0.0354, -0.0471,  0.0519,  0.0907, -0.0431]],
       dtype=torch.float64)
	q_value: tensor([[0.1957]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2719159828986394, distance: 0.9764442268568073 entropy tensor([[ -1.5150,  -1.3976, -21.6069,  -1.5568,  -1.3769, -21.6069,  -3.6664]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 17
	action: tensor([[-0.0028,  0.0375,  0.0151,  0.1339, -0.0172,  0.0852, -0.0383]],
       dtype=torch.float64)
	q_value: tensor([[0.1947]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41394369925698205, distance: 0.8760448946005875 entropy tensor([[ -1.5195,  -1.4311, -21.6069,  -1.5733,  -1.3979, -21.6069,  -4.0780]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 18
	action: tensor([[ 0.1185,  0.0871, -0.0116, -0.0454,  0.0600,  0.0868, -0.0193]],
       dtype=torch.float64)
	q_value: tensor([[0.2039]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4838292533077859, distance: 0.8221544034002375 entropy tensor([[ -1.4978,  -1.4387, -21.6069,  -1.5721,  -1.4524, -21.6069,  -5.0807]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 19
	action: tensor([[-0.0034,  0.0646, -0.0079,  0.1012,  0.0920,  0.0840, -0.0212]],
       dtype=torch.float64)
	q_value: tensor([[0.2043]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4163614473738352, distance: 0.8742359857171532 entropy tensor([[ -1.5220,  -1.5132, -21.6069,  -1.5065,  -1.4421, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 20
	action: tensor([[-0.0730,  0.1189,  0.0004,  0.0726,  0.0859,  0.0886, -0.0262]],
       dtype=torch.float64)
	q_value: tensor([[0.2035]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3686306095424404, distance: 0.9092817085504562 entropy tensor([[ -1.4873,  -1.4437, -21.6069,  -1.5521,  -1.3984, -21.6069,  -5.2447]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 21
	action: tensor([[-0.0027,  0.0083,  0.0267,  0.0486,  0.0830,  0.0935, -0.0391]],
       dtype=torch.float64)
	q_value: tensor([[0.2007]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36065692973257335, distance: 0.9150054364664101 entropy tensor([[ -1.4799,  -1.3990, -21.6069,  -1.5630,  -1.3544, -21.6069,  -3.9848]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 22
	action: tensor([[ 0.0139,  0.0476, -0.0058,  0.0926,  0.0707,  0.0826, -0.0251]],
       dtype=torch.float64)
	q_value: tensor([[0.1997]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41836685132549845, distance: 0.8727327394892167 entropy tensor([[ -1.5063,  -1.4741, -21.6069,  -1.5644,  -1.4160, -21.6069,  -5.4647]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 23
	action: tensor([[ 0.0510, -0.0069, -0.0052,  0.0940,  0.1226,  0.0870, -0.0214]],
       dtype=torch.float64)
	q_value: tensor([[0.2036]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4209443509979113, distance: 0.8707968405339302 entropy tensor([[ -1.4918,  -1.4564, -21.6069,  -1.5512,  -1.4186, -21.6069,  -5.6366]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 24
	action: tensor([[-0.0304,  0.0496, -0.0156, -0.0264,  0.0910,  0.0811, -0.0191]],
       dtype=torch.float64)
	q_value: tensor([[0.2051]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3226448431989706, distance: 0.9418135057904736 entropy tensor([[ -1.4933,  -1.5017, -21.6069,  -1.5334,  -1.4151, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 25
	action: tensor([[-0.0050,  0.0389,  0.0232, -0.0224,  0.1509,  0.0894, -0.0383]],
       dtype=torch.float64)
	q_value: tensor([[0.1989]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34655168785332446, distance: 0.9250438358850979 entropy tensor([[ -1.5064,  -1.4246, -21.6069,  -1.5410,  -1.3697, -21.6069,  -4.1027]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 26
	action: tensor([[-0.0935,  0.0265,  0.0105,  0.0391,  0.0505,  0.0835, -0.0334]],
       dtype=torch.float64)
	q_value: tensor([[0.1991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2699106452438247, distance: 0.977787996188938 entropy tensor([[ -1.5086,  -1.4704, -21.6069,  -1.5407,  -1.3732, -21.6069,  -5.3949]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 27
	action: tensor([[-0.0648,  0.0946,  0.0220, -0.0279,  0.0882,  0.0899, -0.0382]],
       dtype=torch.float64)
	q_value: tensor([[0.1966]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31944794151602907, distance: 0.9440334201916918 entropy tensor([[ -1.4994,  -1.3972, -21.6069,  -1.5811,  -1.3697, -21.6069,  -3.9417]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 28
	action: tensor([[-0.0235,  0.1192,  0.0311,  0.0248,  0.0446,  0.0901, -0.0439]],
       dtype=torch.float64)
	q_value: tensor([[0.1966]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4007331941626261, distance: 0.8858634927642557 entropy tensor([[ -1.5006,  -1.4149, -21.6069,  -1.5648,  -1.3678, -21.6069,  -3.8202]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 29
	action: tensor([[-0.0506,  0.1004,  0.0119,  0.0344,  0.1177,  0.0901, -0.0315]],
       dtype=torch.float64)
	q_value: tensor([[0.1994]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36599145394810284, distance: 0.9111801482101872 entropy tensor([[ -1.5082,  -1.4284, -21.6069,  -1.5615,  -1.4115, -21.6069,  -4.2679]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 30
	action: tensor([[ 0.0193,  0.0723,  0.0227,  0.0915,  0.0696,  0.0902, -0.0374]],
       dtype=torch.float64)
	q_value: tensor([[0.1998]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4426806049590427, distance: 0.8542967846216405 entropy tensor([[ -1.4918,  -1.4277, -21.6069,  -1.5581,  -1.3599, -21.6069,  -4.2251]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 31
	action: tensor([[-0.0337,  0.0758, -0.0090,  0.1056,  0.0229,  0.0859, -0.0197]],
       dtype=torch.float64)
	q_value: tensor([[0.2024]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.393532276559615, distance: 0.8911699616747721 entropy tensor([[ -1.4999,  -1.4642, -21.6069,  -1.5512,  -1.4347, -21.6069,  -5.7303]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 32
	action: tensor([[ 0.1265,  0.0705,  0.0059,  0.0299,  0.1146,  0.0913, -0.0262]],
       dtype=torch.float64)
	q_value: tensor([[0.2029]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5144004900635561, distance: 0.7974359546506461 entropy tensor([[ -1.4883,  -1.4057, -21.6069,  -1.5719,  -1.4078, -21.6069,  -4.3809]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 33
	action: tensor([[-0.0284,  0.1679, -0.0211, -0.0124,  0.1009,  0.0811, -0.0156]],
       dtype=torch.float64)
	q_value: tensor([[0.2065]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4085880867582069, distance: 0.8800386123544326 entropy tensor([[ -1.5058,  -1.5295, -21.6069,  -1.5022,  -1.4481, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 34
	action: tensor([[-0.0185,  0.0607,  0.0381,  0.1005,  0.1764,  0.0957, -0.0434]],
       dtype=torch.float64)
	q_value: tensor([[0.2020]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4070339623358701, distance: 0.8811941455263347 entropy tensor([[ -1.4767,  -1.4055, -21.6069,  -1.5217,  -1.3495, -21.6069,  -3.9411]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 35
	action: tensor([[-0.0358,  0.0763, -0.0001, -0.0150,  0.0070,  0.0836, -0.0298]],
       dtype=torch.float64)
	q_value: tensor([[0.2011]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3394929335217828, distance: 0.9300267226830086 entropy tensor([[ -1.4880,  -1.4588, -21.6069,  -1.5615,  -1.3687, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 36
	action: tensor([[-0.0505,  0.0981,  0.0204,  0.0597,  0.1618,  0.0915, -0.0306]],
       dtype=torch.float64)
	q_value: tensor([[0.1985]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3774188844559717, distance: 0.9029312103329132 entropy tensor([[ -1.5139,  -1.4073, -21.6069,  -1.5642,  -1.3969, -21.6069,  -3.9272]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 37
	action: tensor([[ 0.0323,  0.1378,  0.0188,  0.0244,  0.0265,  0.0888, -0.0367]],
       dtype=torch.float64)
	q_value: tensor([[0.1999]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.46584207964881896, distance: 0.8363566790083313 entropy tensor([[ -1.4877,  -1.4391, -21.6069,  -1.5648,  -1.3493, -21.6069,  -4.5701]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 38
	action: tensor([[ 0.0331,  0.0782,  0.0019,  0.0168,  0.0790,  0.0895, -0.0294]],
       dtype=torch.float64)
	q_value: tensor([[0.2024]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.426627510295979, distance: 0.8665130727720782 entropy tensor([[ -1.5125,  -1.4484, -21.6069,  -1.5415,  -1.4360, -21.6069,  -4.6503]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 39
	action: tensor([[ 0.0075,  0.1333, -0.0004,  0.0574, -0.0196,  0.0864, -0.0241]],
       dtype=torch.float64)
	q_value: tensor([[0.2022]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.449892629964858, distance: 0.8487512462600169 entropy tensor([[ -1.5060,  -1.4732, -21.6069,  -1.5388,  -1.4156, -21.6069,  -5.2336]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 40
	action: tensor([[-0.0403,  0.1224,  0.0051,  0.0080,  0.0201,  0.0925, -0.0339]],
       dtype=torch.float64)
	q_value: tensor([[0.2036]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3774292453242225, distance: 0.9029236971029114 entropy tensor([[ -1.5098,  -1.4184, -21.6069,  -1.5625,  -1.4351, -21.6069,  -4.2597]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 41
	action: tensor([[-0.0385,  0.0459,  0.0221,  0.0481, -0.0229,  0.0924, -0.0388]],
       dtype=torch.float64)
	q_value: tensor([[0.1994]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3464916715784154, distance: 0.9250863154580203 entropy tensor([[ -1.5068,  -1.4034, -21.6069,  -1.5642,  -1.3960, -21.6069,  -3.8579]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 42
	action: tensor([[-0.0243,  0.0010,  0.0040, -0.0059,  0.0860,  0.0882, -0.0236]],
       dtype=torch.float64)
	q_value: tensor([[0.1989]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3055632516684593, distance: 0.9536149274504784 entropy tensor([[ -1.5145,  -1.4171, -21.6069,  -1.5792,  -1.4302, -21.6069,  -4.2371]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 43
	action: tensor([[-0.0611,  0.0378,  0.0086,  0.0259,  0.0077,  0.0849, -0.0361]],
       dtype=torch.float64)
	q_value: tensor([[0.1985]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3055451109125128, distance: 0.9536273830000527 entropy tensor([[ -1.5108,  -1.4493, -21.6069,  -1.5571,  -1.3868, -21.6069,  -4.4734]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 44
	action: tensor([[-0.0071,  0.1278,  0.0153,  0.0618, -0.0008,  0.0900, -0.0360]],
       dtype=torch.float64)
	q_value: tensor([[0.1974]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4360633544668907, distance: 0.8593535029528849 entropy tensor([[ -1.5082,  -1.4061, -21.6069,  -1.5803,  -1.3963, -21.6069,  -3.9799]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 45
	action: tensor([[-0.0051,  0.0867,  0.0056,  0.0522,  0.0932,  0.0915, -0.0304]],
       dtype=torch.float64)
	q_value: tensor([[0.2018]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4107035348490561, distance: 0.878463277361114 entropy tensor([[ -1.5069,  -1.4205, -21.6069,  -1.5614,  -1.4327, -21.6069,  -4.2889]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 46
	action: tensor([[-0.0592,  0.0943,  0.0063,  0.0504,  0.0263,  0.0882, -0.0293]],
       dtype=torch.float64)
	q_value: tensor([[0.2017]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35709899298411574, distance: 0.9175479014815867 entropy tensor([[ -1.4941,  -1.4511, -21.6069,  -1.5491,  -1.3936, -21.6069,  -4.8223]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 47
	action: tensor([[-0.0216,  0.0307,  0.0178,  0.0238,  0.0717,  0.0922, -0.0363]],
       dtype=torch.float64)
	q_value: tensor([[0.1993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3444486939649001, distance: 0.9265311752971775 entropy tensor([[ -1.4999,  -1.3988, -21.6069,  -1.5745,  -1.3917, -21.6069,  -3.9376]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 48
	action: tensor([[-0.0200,  0.0258,  0.0048,  0.0629,  0.0595,  0.0855, -0.0297]],
       dtype=torch.float64)
	q_value: tensor([[0.1990]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35811401954134714, distance: 0.9168232927384098 entropy tensor([[ -1.5076,  -1.4499, -21.6069,  -1.5660,  -1.3997, -21.6069,  -4.5676]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 49
	action: tensor([[ 0.0239, -0.0369,  0.0018,  0.0583,  0.0842,  0.0868, -0.0249]],
       dtype=torch.float64)
	q_value: tensor([[0.2004]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35674396609714965, distance: 0.9178012135867576 entropy tensor([[ -1.4982,  -1.4422, -21.6069,  -1.5664,  -1.4055, -21.6069,  -4.7532]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 50
	action: tensor([[ 0.0149,  0.0740, -0.0116,  0.0032,  0.0969,  0.0810, -0.0218]],
       dtype=torch.float64)
	q_value: tensor([[0.2021]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3981143277799146, distance: 0.8877970463993761 entropy tensor([[ -1.5022,  -1.4895, -21.6069,  -1.5475,  -1.4163, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 51
	action: tensor([[-0.0205,  0.0593,  0.0094,  0.0667,  0.0494,  0.0885, -0.0306]],
       dtype=torch.float64)
	q_value: tensor([[0.2018]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38228988528871033, distance: 0.8993920609005835 entropy tensor([[ -1.5000,  -1.4587, -21.6069,  -1.5375,  -1.3890, -21.6069,  -4.9350]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 52
	action: tensor([[ 0.0165,  0.0392,  0.0024,  0.0943, -0.1039,  0.0881, -0.0285]],
       dtype=torch.float64)
	q_value: tensor([[0.2007]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41389350330415364, distance: 0.8760824105923054 entropy tensor([[ -1.5004,  -1.4350, -21.6069,  -1.5696,  -1.4125, -21.6069,  -4.5696]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 53
	action: tensor([[-0.0085,  0.1103, -0.0110,  0.0506,  0.0014,  0.0884, -0.0186]],
       dtype=torch.float64)
	q_value: tensor([[0.2056]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41760535183198577, distance: 0.8733038625044647 entropy tensor([[ -1.5099,  -1.4224, -21.6069,  -1.5656,  -1.4678, -21.6069,  -4.7616]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 54
	action: tensor([[ 0.0368,  0.0651,  0.0100,  0.1377,  0.0382,  0.0923, -0.0333]],
       dtype=torch.float64)
	q_value: tensor([[0.2030]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4734888979128977, distance: 0.8303486040377549 entropy tensor([[ -1.5018,  -1.4111, -21.6069,  -1.5614,  -1.4119, -21.6069,  -4.1597]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 55
	action: tensor([[-0.0086,  0.1373, -0.0164,  0.0549,  0.0062,  0.0853, -0.0168]],
       dtype=torch.float64)
	q_value: tensor([[0.2058]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.43487225650746963, distance: 0.8602605501414295 entropy tensor([[ -1.4928,  -1.4697, -21.6069,  -1.5478,  -1.4563, -21.6069,  -6.1419]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 56
	action: tensor([[-0.0035,  0.0722,  0.0132,  0.0058, -0.0152,  0.0941, -0.0315]],
       dtype=torch.float64)
	q_value: tensor([[0.2035]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38091202483350417, distance: 0.9003945914048604 entropy tensor([[ -1.4995,  -1.4061, -21.6069,  -1.5579,  -1.4095, -21.6069,  -4.0840]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 57
	action: tensor([[-0.0377, -0.0161,  0.0049, -0.0339,  0.1533,  0.0883, -0.0263]],
       dtype=torch.float64)
	q_value: tensor([[0.2001]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2672879785172305, distance: 0.9795426534433317 entropy tensor([[ -1.5221,  -1.4302, -21.6069,  -1.5648,  -1.4280, -21.6069,  -4.2175]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 58
	action: tensor([[ 0.0262,  0.0907,  0.0176,  0.1563,  0.0402,  0.0831, -0.0398]],
       dtype=torch.float64)
	q_value: tensor([[0.1972]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.48491530476114786, distance: 0.8212890190639188 entropy tensor([[ -1.5034,  -1.4456, -21.6069,  -1.5439,  -1.3514, -21.6069,  -4.7516]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 59
	action: tensor([[-0.0045,  0.1324, -0.0143, -0.0688,  0.0554,  0.0878, -0.0170]],
       dtype=torch.float64)
	q_value: tensor([[0.2053]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38812230638525635, distance: 0.8951359590245073 entropy tensor([[ -1.4877,  -1.4562, -21.6069,  -1.5526,  -1.4550, -21.6069,  -6.1978]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 60
	action: tensor([[ 0.0165,  0.0659,  0.0325,  0.1096,  0.0855,  0.0922, -0.0420]],
       dtype=torch.float64)
	q_value: tensor([[0.2003]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4454971192553693, distance: 0.8521353787929508 entropy tensor([[ -1.5008,  -1.4180, -21.6069,  -1.5308,  -1.3765, -21.6069,  -3.9674]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 61
	action: tensor([[ 0.0084,  0.0835, -0.0123, -0.0119,  0.1035,  0.0845, -0.0205]],
       dtype=torch.float64)
	q_value: tensor([[0.2024]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3926249888675968, distance: 0.8918363164795711 entropy tensor([[ -1.4976,  -1.4694, -21.6069,  -1.5554,  -1.4360, -21.6069,  -6.3659]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 62
	action: tensor([[ 0.0658,  0.1025,  0.0146,  0.0587,  0.0506,  0.0887, -0.0282]],
       dtype=torch.float64)
	q_value: tensor([[0.2014]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4903895997895411, distance: 0.8169130512864863 entropy tensor([[ -1.4998,  -1.4541, -21.6069,  -1.5348,  -1.3810, -21.6069,  -4.6689]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 63
	action: tensor([[-0.0007,  0.0415, -0.0119,  0.0510,  0.0125,  0.0863, -0.0188]],
       dtype=torch.float64)
	q_value: tensor([[0.2047]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38090638293671575, distance: 0.9003986941511452 entropy tensor([[ -1.5061,  -1.4819, -21.6069,  -1.5309,  -1.4537, -21.6069,  -6.2200]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
LOSS epoch 5 actor 19.86659766114111 critic 66.02270987622312
epoch: 6, step: 0
	action: tensor([[-0.0178, -0.0189,  0.1323,  0.1020,  0.1012,  0.0060, -0.1246]],
       dtype=torch.float64)
	q_value: tensor([[0.0622]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3349960100991555, distance: 0.9331872979342225 entropy tensor([[ -1.3986,  -1.2523, -21.6069,  -1.2825,  -1.1712, -21.6069,  -2.4477]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 1
	action: tensor([[ 0.0226,  0.1157,  0.1091,  0.0105, -0.0644, -0.0001, -0.1060]],
       dtype=torch.float64)
	q_value: tensor([[0.0491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41774539754959994, distance: 0.8731988565435033 entropy tensor([[ -1.4304,  -1.3174, -21.6069,  -1.3074,  -1.1984, -21.6069,  -2.8559]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 2
	action: tensor([[ 0.0022,  0.0153,  0.1217, -0.0817,  0.1097,  0.0132, -0.0893]],
       dtype=torch.float64)
	q_value: tensor([[0.0561]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2826921386846285, distance: 0.9691912587271377 entropy tensor([[ -1.4338,  -1.2531, -21.6069,  -1.3017,  -1.2223, -21.6069,  -2.6428]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 3
	action: tensor([[-0.0016, -0.0593,  0.1334,  0.0263,  0.1191,  0.0031, -0.0925]],
       dtype=torch.float64)
	q_value: tensor([[0.0475]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.278522911374668, distance: 0.9720038056843979 entropy tensor([[ -1.3878,  -1.2895, -21.6069,  -1.2881,  -1.1625, -21.6069,  -2.6091]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 4
	action: tensor([[ 0.0458,  0.0525,  0.1080,  0.1179,  0.1588, -0.0016, -0.1037]],
       dtype=torch.float64)
	q_value: tensor([[0.0475]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.45063166010812505, distance: 0.8481809361730245 entropy tensor([[ -1.4329,  -1.3189, -21.6069,  -1.3089,  -1.1869, -21.6069,  -2.8485]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 5
	action: tensor([[-0.0100,  0.1314,  0.1028, -0.0575, -0.0505, -0.0003, -0.1090]],
       dtype=torch.float64)
	q_value: tensor([[0.0546]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3630793896708806, distance: 0.9132703213660079 entropy tensor([[ -1.4290,  -1.3471, -21.6069,  -1.2991,  -1.2135, -21.6069,  -2.9530]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 6
	action: tensor([[-0.0076,  0.1123,  0.1479,  0.0228,  0.0599,  0.0178, -0.1371]],
       dtype=torch.float64)
	q_value: tensor([[0.0531]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3966668702556744, distance: 0.888863924136828 entropy tensor([[ -1.4012,  -1.2157, -21.6069,  -1.2866,  -1.1913, -21.6069,  -2.5139]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 7
	action: tensor([[-0.0638,  0.0438,  0.1254, -0.0247,  0.1295,  0.0042, -0.1133]],
       dtype=torch.float64)
	q_value: tensor([[0.0497]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26310305606968143, distance: 0.9823360246587894 entropy tensor([[ -1.4112,  -1.2900, -21.6069,  -1.3119,  -1.2033, -21.6069,  -2.6463]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 8
	action: tensor([[ 0.0376,  0.0762,  0.1513, -0.0164,  0.0604,  0.0082, -0.1331]],
       dtype=torch.float64)
	q_value: tensor([[0.0462]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3945227808423081, distance: 0.8904419193228126 entropy tensor([[ -1.3502,  -1.2652, -21.6069,  -1.2785,  -1.1505, -21.6069,  -2.5476]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 9
	action: tensor([[-0.0071,  0.0641,  0.1120, -0.0504,  0.1033,  0.0010, -0.0984]],
       dtype=torch.float64)
	q_value: tensor([[0.0492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.322677381106226, distance: 0.9417908847118406 entropy tensor([[ -1.4409,  -1.3128, -21.6069,  -1.3186,  -1.2168, -21.6069,  -2.7646]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 10
	action: tensor([[-0.0909, -0.0371,  0.1397,  0.1375,  0.0483,  0.0079, -0.1224]],
       dtype=torch.float64)
	q_value: tensor([[0.0495]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26644141825598755, distance: 0.9801083617136656 entropy tensor([[ -1.3806,  -1.2826, -21.6069,  -1.2892,  -1.1715, -21.6069,  -2.5947]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 11
	action: tensor([[-0.0426,  0.1165,  0.1208,  0.0894, -0.0042,  0.0044, -0.0810]],
       dtype=torch.float64)
	q_value: tensor([[0.0483]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3922860238256125, distance: 0.8920851406519956 entropy tensor([[ -1.4195,  -1.2756, -21.6069,  -1.3112,  -1.1890, -21.6069,  -2.7172]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 12
	action: tensor([[-0.0535,  0.0897,  0.1294,  0.0004,  0.1264,  0.0137, -0.1268]],
       dtype=torch.float64)
	q_value: tensor([[0.0559]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3221317854891561, distance: 0.9421701231500496 entropy tensor([[ -1.4043,  -1.2561, -21.6069,  -1.2998,  -1.2013, -21.6069,  -2.6057]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 13
	action: tensor([[ 0.0198, -0.0346,  0.1473,  0.0044,  0.1848,  0.0070, -0.1312]],
       dtype=torch.float64)
	q_value: tensor([[0.0473]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30766467120523455, distance: 0.9521709778284241 entropy tensor([[ -1.3604,  -1.2724, -21.6069,  -1.2888,  -1.1616, -21.6069,  -2.5553]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 14
	action: tensor([[ 0.1101, -0.0914,  0.1104,  0.0696, -0.0022, -0.0060, -0.1220]],
       dtype=torch.float64)
	q_value: tensor([[0.0449]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3813243427309231, distance: 0.9000947062076154 entropy tensor([[ -1.4216,  -1.3371, -21.6069,  -1.2908,  -1.1670, -21.6069,  -2.8952]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 15
	action: tensor([[-0.0742,  0.1302,  0.0732,  0.1422,  0.0959, -0.0038, -0.0903]],
       dtype=torch.float64)
	q_value: tensor([[0.0537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3873461483818892, distance: 0.895703512605985 entropy tensor([[ -1.5003,  -1.3539, -21.6069,  -1.3083,  -1.2361, -21.6069,  -3.2908]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 16
	action: tensor([[ 0.0487, -0.0157,  0.1502,  0.0167,  0.0670,  0.0167, -0.0950]],
       dtype=torch.float64)
	q_value: tensor([[0.0568]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3595628685819615, distance: 0.9157879928495812 entropy tensor([[ -1.3412,  -1.2442, -21.6069,  -1.2541,  -1.1670, -21.6069,  -2.6422]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 17
	action: tensor([[-0.0968,  0.0326,  0.0919,  0.0702,  0.0012, -0.0045, -0.0918]],
       dtype=torch.float64)
	q_value: tensor([[0.0494]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26656811689309945, distance: 0.98002371697011 entropy tensor([[ -1.4724,  -1.3497, -21.6069,  -1.3363,  -1.2241, -21.6069,  -2.9451]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 18
	action: tensor([[-0.0486,  0.0741,  0.1457,  0.0574,  0.0806,  0.0166, -0.0907]],
       dtype=torch.float64)
	q_value: tensor([[0.0524]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34705080742353744, distance: 0.9246904830087308 entropy tensor([[ -1.3752,  -1.2227, -21.6069,  -1.2851,  -1.1816, -21.6069,  -2.5391]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 19
	action: tensor([[-0.0174, -0.0561,  0.1279, -0.0954,  0.0334,  0.0055, -0.0992]],
       dtype=torch.float64)
	q_value: tensor([[0.0505]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19657813679001068, distance: 1.0257192737523813 entropy tensor([[ -1.3946,  -1.2854, -21.6069,  -1.3197,  -1.1891, -21.6069,  -2.6354]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 20
	action: tensor([[-0.0773,  0.0956,  0.1288, -0.0579,  0.1378,  0.0036, -0.1189]],
       dtype=torch.float64)
	q_value: tensor([[0.0445]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26805278800897914, distance: 0.9790312935267006 entropy tensor([[ -1.3965,  -1.2525, -21.6069,  -1.2979,  -1.1684, -21.6069,  -2.6367]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 21
	action: tensor([[-0.0562,  0.1042,  0.1681,  0.0372,  0.0798,  0.0122, -0.0866]],
       dtype=torch.float64)
	q_value: tensor([[0.0455]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34786474860678807, distance: 0.9241139616079382 entropy tensor([[ -1.3308,  -1.2495, -21.6069,  -1.2575,  -1.1339, -21.6069,  -2.4667]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 22
	action: tensor([[-0.0064,  0.0818,  0.1325,  0.0397,  0.0046,  0.0068, -0.0602]],
       dtype=torch.float64)
	q_value: tensor([[0.0497]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3830177460311074, distance: 0.8988620184802393 entropy tensor([[ -1.3902,  -1.2784, -21.6069,  -1.3238,  -1.1831, -21.6069,  -2.5835]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 23
	action: tensor([[ 0.0409,  0.0225,  0.1165,  0.1883,  0.1320,  0.0088, -0.0967]],
       dtype=torch.float64)
	q_value: tensor([[0.0549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.46483765886067896, distance: 0.8371426446076359 entropy tensor([[ -1.4345,  -1.2902, -21.6069,  -1.3294,  -1.2120, -21.6069,  -2.6579]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 24
	action: tensor([[ 0.0456,  0.1132,  0.0891, -0.0440,  0.0083, -0.0032, -0.0597]],
       dtype=torch.float64)
	q_value: tensor([[0.0564]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4118295003408944, distance: 0.8776236386369702 entropy tensor([[ -1.4492,  -1.3575, -21.6069,  -1.3075,  -1.2192, -21.6069,  -3.0217]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 25
	action: tensor([[ 0.0526,  0.0643,  0.1287, -0.0503,  0.0070,  0.0128, -0.0987]],
       dtype=torch.float64)
	q_value: tensor([[0.0564]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3843959495809406, distance: 0.8978575265350697 entropy tensor([[ -1.4164,  -1.2790, -21.6069,  -1.3107,  -1.1981, -21.6069,  -2.6345]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 26
	action: tensor([[-0.0021,  0.0360,  0.1127,  0.0172,  0.1479,  0.0039, -0.1031]],
       dtype=torch.float64)
	q_value: tensor([[0.0519]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3433449901635808, distance: 0.9273108136769848 entropy tensor([[ -1.4429,  -1.2960, -21.6069,  -1.3248,  -1.2140, -21.6069,  -2.7174]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 27
	action: tensor([[-0.0564, -0.0583,  0.1271, -0.0940,  0.1154,  0.0033, -0.1290]],
       dtype=torch.float64)
	q_value: tensor([[0.0496]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15455656486676994, distance: 1.0522016327221866 entropy tensor([[ -1.3919,  -1.3098, -21.6069,  -1.2904,  -1.1751, -21.6069,  -2.7365]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 28
	action: tensor([[-0.0914,  0.0869,  0.1471, -0.0280,  0.1734,  0.0038, -0.1180]],
       dtype=torch.float64)
	q_value: tensor([[0.0409]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2619866036983406, distance: 0.9830798982090082 entropy tensor([[ -1.3513,  -1.2510, -21.6069,  -1.2649,  -1.1321, -21.6069,  -2.5977]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 29
	action: tensor([[ 0.1079,  0.0274,  0.1642, -0.0205,  0.1008,  0.0095, -0.1162]],
       dtype=torch.float64)
	q_value: tensor([[0.0439]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42394897309205115, distance: 0.8685346932775545 entropy tensor([[ -1.3295,  -1.2593, -21.6069,  -1.2699,  -1.1275, -21.6069,  -2.5115]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 30
	action: tensor([[-0.0732,  0.0864,  0.0861,  0.2323,  0.1704, -0.0079, -0.0910]],
       dtype=torch.float64)
	q_value: tensor([[0.0494]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.40384001836418515, distance: 0.8835641842368711 entropy tensor([[ -1.4824,  -1.3748, -21.6069,  -1.3171,  -1.2231, -21.6069,  -3.1084]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 31
	action: tensor([[-0.1539,  0.1163,  0.1368, -0.0396,  0.0743,  0.0094, -0.1126]],
       dtype=torch.float64)
	q_value: tensor([[0.0552]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20860752650137038, distance: 1.0180114227067842 entropy tensor([[ -1.3423,  -1.2661, -21.6069,  -1.2534,  -1.1594, -21.6069,  -2.7612]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 32
	action: tensor([[-0.0042,  0.0117,  0.1898, -0.0963,  0.1205,  0.0203, -0.1372]],
       dtype=torch.float64)
	q_value: tensor([[0.0444]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26474241902449114, distance: 0.9812427225629381 entropy tensor([[ -1.3092,  -1.1995, -21.6069,  -1.2560,  -1.1212, -21.6069,  -2.3478]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 33
	action: tensor([[-0.0165, -0.0789,  0.1242,  0.0058,  0.0175, -0.0043, -0.1031]],
       dtype=torch.float64)
	q_value: tensor([[0.0413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2343183465654669, distance: 1.0013382678688612 entropy tensor([[ -1.4027,  -1.2891, -21.6069,  -1.2996,  -1.1567, -21.6069,  -2.6930]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 34
	action: tensor([[-0.0667,  0.0350,  0.1116, -0.0241,  0.0563,  0.0037, -0.0337]],
       dtype=torch.float64)
	q_value: tensor([[0.0478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25396346487851473, distance: 0.9884091143198481 entropy tensor([[ -1.4389,  -1.2852, -21.6069,  -1.3077,  -1.1959, -21.6069,  -2.7630]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 35
	action: tensor([[ 0.0092,  0.0531,  0.1456,  0.0495,  0.1378,  0.0129, -0.1032]],
       dtype=torch.float64)
	q_value: tensor([[0.0514]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38665984748368853, distance: 0.896205060148294 entropy tensor([[ -1.3754,  -1.2510, -21.6069,  -1.3052,  -1.1647, -21.6069,  -2.4916]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 36
	action: tensor([[ 0.0076,  0.1265,  0.1119, -0.0151,  0.0633, -0.0010, -0.0995]],
       dtype=torch.float64)
	q_value: tensor([[0.0500]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39747894071803136, distance: 0.8882655290267651 entropy tensor([[ -1.4212,  -1.3288, -21.6069,  -1.3170,  -1.1994, -21.6069,  -2.8023]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 37
	action: tensor([[-0.1406,  0.1092,  0.1370, -0.0654,  0.1344,  0.0109, -0.1336]],
       dtype=torch.float64)
	q_value: tensor([[0.0533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2057890801332538, distance: 1.0198225724505692 entropy tensor([[ -1.3920,  -1.2794, -21.6069,  -1.2939,  -1.1880, -21.6069,  -2.6061]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 38
	action: tensor([[ 0.0200,  0.0939,  0.1932,  0.1556,  0.0037,  0.0173, -0.1589]],
       dtype=torch.float64)
	q_value: tensor([[0.0410]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4808656628036484, distance: 0.8245112220249295 entropy tensor([[ -1.2932,  -1.2172, -21.6069,  -1.2365,  -1.1078, -21.6069,  -2.3732]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 39
	action: tensor([[ 0.0079,  0.0970,  0.0903, -0.0250,  0.0724, -0.0012, -0.0880]],
       dtype=torch.float64)
	q_value: tensor([[0.0518]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37338722792747947, distance: 0.9058500541792222 entropy tensor([[ -1.4700,  -1.3271, -21.6069,  -1.3303,  -1.2395, -21.6069,  -2.9255]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 40
	action: tensor([[-0.0064,  0.0038,  0.1391, -0.0249, -0.0744,  0.0115, -0.0941]],
       dtype=torch.float64)
	q_value: tensor([[0.0537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2948217528700098, distance: 0.9609618503750492 entropy tensor([[ -1.3851,  -1.2800, -21.6069,  -1.2893,  -1.1822, -21.6069,  -2.6010]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 41
	action: tensor([[-0.0109,  0.0501,  0.1143, -0.0217,  0.0105,  0.0065, -0.1041]],
       dtype=torch.float64)
	q_value: tensor([[0.0503]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32490118215970787, distance: 0.9402435589693385 entropy tensor([[ -1.4503,  -1.2529, -21.6069,  -1.3214,  -1.2138, -21.6069,  -2.6572]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 42
	action: tensor([[ 0.1238, -0.0104,  0.1288,  0.0445,  0.0233,  0.0088, -0.0944]],
       dtype=torch.float64)
	q_value: tensor([[0.0508]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44504682291106556, distance: 0.8524813062805332 entropy tensor([[ -1.4177,  -1.2635, -21.6069,  -1.3123,  -1.1990, -21.6069,  -2.6028]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 43
	action: tensor([[-0.0206,  0.0769,  0.0732,  0.0295,  0.0775, -0.0045, -0.0695]],
       dtype=torch.float64)
	q_value: tensor([[0.0551]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35632516192507535, distance: 0.9180999410087813 entropy tensor([[ -1.4969,  -1.3779, -21.6069,  -1.3303,  -1.2531, -21.6069,  -3.2230]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 44
	action: tensor([[ 0.0084,  0.0774,  0.1400,  0.0358, -0.0505,  0.0135, -0.1132]],
       dtype=torch.float64)
	q_value: tensor([[0.0554]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3945430179452788, distance: 0.8904270384034195 entropy tensor([[ -1.3748,  -1.2715, -21.6069,  -1.2846,  -1.1790, -21.6069,  -2.6072]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 45
	action: tensor([[ 0.0467,  0.0944,  0.1104,  0.0754,  0.1226,  0.0071, -0.0884]],
       dtype=torch.float64)
	q_value: tensor([[0.0532]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.46018527778964824, distance: 0.840773578930746 entropy tensor([[ -1.4551,  -1.2803, -21.6069,  -1.3191,  -1.2306, -21.6069,  -2.7045]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 46
	action: tensor([[-0.0952,  0.1052,  0.1092,  0.0599,  0.1700,  0.0022, -0.0880]],
       dtype=torch.float64)
	q_value: tensor([[0.0557]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31438727988940496, distance: 0.9475368885374486 entropy tensor([[ -1.4199,  -1.3353, -21.6069,  -1.3014,  -1.2149, -21.6069,  -2.8365]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 47
	action: tensor([[ 0.0293,  0.0705,  0.1610,  0.1094,  0.1072,  0.0127, -0.1265]],
       dtype=torch.float64)
	q_value: tensor([[0.0506]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4489936895251624, distance: 0.8494444430938507 entropy tensor([[ -1.3258,  -1.2562, -21.6069,  -1.2705,  -1.1405, -21.6069,  -2.5494]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 48
	action: tensor([[-0.0343, -0.0931,  0.0985,  0.0729,  0.0648, -0.0029, -0.0874]],
       dtype=torch.float64)
	q_value: tensor([[0.0515]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24286298770633885, distance: 0.9957353644439426 entropy tensor([[ -1.4474,  -1.3462, -21.6069,  -1.3206,  -1.2189, -21.6069,  -2.9135]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 49
	action: tensor([[-0.0393, -0.0033,  0.1134,  0.0218,  0.0938,  0.0035, -0.0760]],
       dtype=torch.float64)
	q_value: tensor([[0.0498]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27933301917833253, distance: 0.9714579470252146 entropy tensor([[ -1.4279,  -1.2906, -21.6069,  -1.2934,  -1.1875, -21.6069,  -2.7817]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 50
	action: tensor([[-0.0166,  0.0813,  0.1288,  0.0346,  0.1217,  0.0064, -0.0814]],
       dtype=torch.float64)
	q_value: tensor([[0.0497]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3698161362196779, distance: 0.9084276251516429 entropy tensor([[ -1.3917,  -1.2870, -21.6069,  -1.3059,  -1.1795, -21.6069,  -2.6672]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 51
	action: tensor([[-0.0571, -0.0885,  0.1274, -0.0203,  0.0933,  0.0055, -0.0816]],
       dtype=torch.float64)
	q_value: tensor([[0.0517]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17238916081185274, distance: 1.041045660671371 entropy tensor([[ -1.3914,  -1.3000, -21.6069,  -1.3100,  -1.1860, -21.6069,  -2.6753]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 52
	action: tensor([[ 0.0756,  0.0555,  0.1274,  0.1043,  0.0192,  0.0025, -0.1320]],
       dtype=torch.float64)
	q_value: tensor([[0.0448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.47583671553457096, distance: 0.8284951908371818 entropy tensor([[ -1.3928,  -1.2734, -21.6069,  -1.3028,  -1.1591, -21.6069,  -2.6664]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 53
	action: tensor([[ 0.0196, -0.0335,  0.0878,  0.0062,  0.0312,  0.0011, -0.0466]],
       dtype=torch.float64)
	q_value: tensor([[0.0560]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30726054494563526, distance: 0.9524488353515037 entropy tensor([[ -1.4687,  -1.3521, -21.6069,  -1.3141,  -1.2541, -21.6069,  -3.0432]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 54
	action: tensor([[ 0.0725,  0.0069,  0.1104, -0.0382,  0.1263,  0.0056, -0.0864]],
       dtype=torch.float64)
	q_value: tensor([[0.0530]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3661822942197589, distance: 0.9110430026184354 entropy tensor([[ -1.4345,  -1.3072, -21.6069,  -1.3180,  -1.2005, -21.6069,  -2.7408]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 55
	action: tensor([[ 0.0098, -0.0635,  0.1066,  0.0476, -0.0029, -0.0016, -0.0854]],
       dtype=torch.float64)
	q_value: tensor([[0.0509]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2968233009463316, distance: 0.9595971045653322 entropy tensor([[ -1.4356,  -1.3459, -21.6069,  -1.2999,  -1.1945, -21.6069,  -2.8844]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 56
	action: tensor([[ 0.0085,  0.1016,  0.1024,  0.0108, -0.0252,  0.0040, -0.0989]],
       dtype=torch.float64)
	q_value: tensor([[0.0523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39565144520565565, distance: 0.8896116000105113 entropy tensor([[ -1.4566,  -1.3037, -21.6069,  -1.3099,  -1.2123, -21.6069,  -2.8184]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 57
	action: tensor([[-6.0671e-05,  3.2632e-02,  1.2582e-01,  8.5743e-02,  1.3084e-01,
          1.2292e-02, -7.3648e-02]], dtype=torch.float64)
	q_value: tensor([[0.0552]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3813975651940491, distance: 0.9000414399275681 entropy tensor([[ -1.4272,  -1.2614, -21.6069,  -1.3049,  -1.2111, -21.6069,  -2.6107]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 58
	action: tensor([[ 0.0578, -0.0241,  0.1101,  0.0135,  0.1383,  0.0007, -0.0929]],
       dtype=torch.float64)
	q_value: tensor([[0.0525]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3552703337414752, distance: 0.9188519055113138 entropy tensor([[ -1.4219,  -1.3279, -21.6069,  -1.3175,  -1.2031, -21.6069,  -2.8081]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 59
	action: tensor([[ 0.0465, -0.0745,  0.1029,  0.0402, -0.0392, -0.0022, -0.0696]],
       dtype=torch.float64)
	q_value: tensor([[0.0505]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31994476317741016, distance: 0.9436887721399294 entropy tensor([[ -1.4386,  -1.3536, -21.6069,  -1.2965,  -1.1965, -21.6069,  -2.9742]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 60
	action: tensor([[-0.0532, -0.0668,  0.0915,  0.0347,  0.1297,  0.0028, -0.0547]],
       dtype=torch.float64)
	q_value: tensor([[0.0538]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22365628217372058, distance: 1.0082859482679518 entropy tensor([[ -1.4760,  -1.3134, -21.6069,  -1.3174,  -1.2242, -21.6069,  -2.9153]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 61
	action: tensor([[ 0.0250,  0.0149,  0.1292, -0.0341,  0.2414,  0.0047, -0.1118]],
       dtype=torch.float64)
	q_value: tensor([[0.0493]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3282505021238551, distance: 0.9379082770033391 entropy tensor([[ -1.3833,  -1.2813, -21.6069,  -1.2880,  -1.1634, -21.6069,  -2.6559]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 62
	action: tensor([[-0.0701,  0.0532,  0.1283,  0.0712, -0.1006, -0.0030, -0.0896]],
       dtype=torch.float64)
	q_value: tensor([[0.0469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31174016415381245, distance: 0.9493643221236673 entropy tensor([[ -1.3865,  -1.3351, -21.6069,  -1.2588,  -1.1374, -21.6069,  -2.7837]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 63
	action: tensor([[-0.0650,  0.0212,  0.1301, -0.0518,  0.1006,  0.0168, -0.0728]],
       dtype=torch.float64)
	q_value: tensor([[0.0536]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2342702044039633, distance: 1.0013697468984768 entropy tensor([[ -1.4221,  -1.2212, -21.6069,  -1.2906,  -1.2007, -21.6069,  -2.5611]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
LOSS epoch 6 actor 0.11961073745175 critic 20.131176913199297
epoch: 7, step: 0
	action: tensor([[ 0.0190,  0.1097,  0.4100, -0.1392,  0.0450,  0.0148, -0.2837]],
       dtype=torch.float64)
	q_value: tensor([[0.0136]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3232981085971227, distance: 0.94135923701914 entropy tensor([[ -0.9202,  -0.9155, -21.6069,  -0.8054,  -0.7468,  -2.4608,  -1.6267]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 1
	action: tensor([[ 0.0404,  0.1781,  0.3903,  0.0160,  0.1179,  0.0283, -0.3439]],
       dtype=torch.float64)
	q_value: tensor([[-0.0051]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.48694691423580294, distance: 0.8196677448412727 entropy tensor([[ -0.9802,  -0.9531, -21.6069,  -0.8434,  -0.7075,  -2.6421,  -1.8110]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 2
	action: tensor([[-0.0318, -0.0108,  0.3782,  0.1479,  0.0502,  0.0264, -0.2420]],
       dtype=torch.float64)
	q_value: tensor([[-0.0055]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37449548051414394, distance: 0.905048638222972 entropy tensor([[ -0.9971,  -1.0063, -21.6069,  -0.8703,  -0.7462,  -2.7782,  -1.8445]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 3
	action: tensor([[ 0.1363, -0.0076,  0.3469, -0.0435,  0.0572,  0.0034, -0.2712]],
       dtype=torch.float64)
	q_value: tensor([[-0.0012]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.40584820113090736, distance: 0.8820747726782018 entropy tensor([[ -1.0627,  -0.9991, -21.6069,  -0.9519,  -0.7885,  -3.5895,  -1.8727]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 4
	action: tensor([[-0.0696, -0.0370,  0.3330,  0.0242, -0.0833, -0.0171, -0.1878]],
       dtype=torch.float64)
	q_value: tensor([[0.0019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22000562249117106, distance: 1.0106538374295881 entropy tensor([[ -1.0766,  -1.0355, -21.6069,  -0.9182,  -0.7806,  -3.3576,  -1.9195]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 5
	action: tensor([[ 0.0535,  0.2719,  0.3690, -0.1035, -0.0597,  0.0357, -0.2954]],
       dtype=torch.float64)
	q_value: tensor([[0.0061]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4898805180710726, distance: 0.8173209822182782 entropy tensor([[ -1.0188,  -0.9273, -21.6069,  -0.9194,  -0.7746,  -3.1708,  -1.7822]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 6
	action: tensor([[-0.0899, -0.0675,  0.3963,  0.1033,  0.1883,  0.0357, -0.2839]],
       dtype=torch.float64)
	q_value: tensor([[0.0036]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24907412699553466, distance: 0.9916427186522374 entropy tensor([[ -0.9657,  -0.9283, -21.6069,  -0.8358,  -0.7374,  -2.4579,  -1.7434]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 7
	action: tensor([[-0.0099,  0.1135,  0.3781, -0.0624,  0.1435,  0.0073, -0.2809]],
       dtype=torch.float64)
	q_value: tensor([[-0.0138]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3430207965524036, distance: 0.9275396942464129 entropy tensor([[ -1.0107,  -0.9795, -21.6069,  -0.8933,  -0.7197,  -3.3512,  -1.8797]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 8
	action: tensor([[-0.1352,  0.0337,  0.3967,  0.1045,  0.1510,  0.0285, -0.2494]],
       dtype=torch.float64)
	q_value: tensor([[-0.0052]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27005228875420306, distance: 0.9776931420170831 entropy tensor([[ -0.9582,  -0.9709, -21.6069,  -0.8487,  -0.7129,  -2.6394,  -1.8144]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 9
	action: tensor([[-0.1146,  0.0920,  0.3930,  0.0406,  0.0685,  0.0223, -0.1488]],
       dtype=torch.float64)
	q_value: tensor([[-0.0115]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2873720242558411, distance: 0.9660244689390269 entropy tensor([[ -0.9766,  -0.9511, -21.6069,  -0.8941,  -0.7240,  -3.0575,  -1.8122]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 10
	action: tensor([[ 0.0188,  0.1553,  0.3878,  0.1163,  0.1431,  0.0211, -0.2869]],
       dtype=torch.float64)
	q_value: tensor([[-0.0019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.507667929367722, distance: 0.8029449234193743 entropy tensor([[ -0.9833,  -0.9416, -21.6069,  -0.9060,  -0.7516,  -2.9311,  -1.7425]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 11
	action: tensor([[ 0.0224,  0.0656,  0.3632,  0.1483,  0.2287, -0.0041, -0.2323]],
       dtype=torch.float64)
	q_value: tensor([[-0.0030]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4669572487461662, distance: 0.8354831859944549 entropy tensor([[ -1.0264,  -1.0131, -21.6069,  -0.9058,  -0.7764,  -2.9918,  -1.8573]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 12
	action: tensor([[-0.2064, -0.0459,  0.3567,  0.0341,  0.2601,  0.0192, -0.1867]],
       dtype=torch.float64)
	q_value: tensor([[-0.0005]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07989527874204805, distance: 1.0976789170365246 entropy tensor([[ -1.0355,  -1.0311, -21.6069,  -0.9143,  -0.7713,  -3.0917,  -1.8727]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 13
	action: tensor([[-0.0499, -0.0201,  0.4326, -0.0837,  0.2012, -0.0157, -0.2175]],
       dtype=torch.float64)
	q_value: tensor([[-0.0124]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17946452165929483, distance: 1.0365860863333414 entropy tensor([[ -0.8873,  -0.8968, -21.6069,  -0.8254,  -0.6412,  -2.8162,  -1.7897]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 14
	action: tensor([[-0.0502, -0.0602,  0.3944,  0.0844,  0.1392, -0.0099, -0.2105]],
       dtype=torch.float64)
	q_value: tensor([[-0.0114]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26644605250854037, distance: 0.9801052657940186 entropy tensor([[ -0.9720,  -0.9605, -21.6069,  -0.8492,  -0.6814,  -2.9615,  -1.8579]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 15
	action: tensor([[-0.0102, -0.0013,  0.3629,  0.0449,  0.0417,  0.0168, -0.1967]],
       dtype=torch.float64)
	q_value: tensor([[-0.0042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3325260341292944, distance: 0.9349187265606269 entropy tensor([[ -1.0309,  -0.9881, -21.6069,  -0.9265,  -0.7548,  -3.4084,  -1.8816]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 16
	action: tensor([[-0.2113,  0.0653,  0.3505,  0.0366,  0.3772,  0.0111, -0.2718]],
       dtype=torch.float64)
	q_value: tensor([[0.0022]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15347128782109898, distance: 1.0528767601164826 entropy tensor([[ -1.0515,  -0.9899, -21.6069,  -0.9452,  -0.7887,  -3.3902,  -1.8474]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 17
	action: tensor([[ 0.0846, -0.0136,  0.4723,  0.1704, -0.0851,  0.0261, -0.2628]],
       dtype=torch.float64)
	q_value: tensor([[-0.0142]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.500360534910191, distance: 0.8088817940811863 entropy tensor([[ -0.8112,  -0.8565, -21.6069,  -0.7504,  -0.5826,  -2.4684,  -1.8275]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 18
	action: tensor([[-0.0529, -0.0301,  0.3068,  0.0918,  0.1466, -0.0114, -0.1564]],
       dtype=torch.float64)
	q_value: tensor([[-0.0009]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28721627730542354, distance: 0.9661300269233251 entropy tensor([[ -1.1596,  -1.0067, -21.6069,  -0.9939,  -0.8016,  -5.2619,  -1.9652]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 19
	action: tensor([[-0.0081, -0.0888,  0.3672,  0.1015,  0.1858,  0.0136, -0.2620]],
       dtype=torch.float64)
	q_value: tensor([[0.0046]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30672823666234394, distance: 0.9528147009325888 entropy tensor([[ -1.0080,  -0.9878, -21.6069,  -0.9185,  -0.7749,  -3.1362,  -1.8205]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 20
	action: tensor([[-0.1327,  0.0869,  0.3556, -0.0361,  0.1253,  0.0042, -0.1924]],
       dtype=torch.float64)
	q_value: tensor([[-0.0061]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20858979879941564, distance: 1.018022824699239 entropy tensor([[ -1.0474,  -1.0183, -21.6069,  -0.9088,  -0.7520,  -3.4980,  -1.9189]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 21
	action: tensor([[-0.0399, -0.0394,  0.4193, -0.0557,  0.1134,  0.0239, -0.2890]],
       dtype=torch.float64)
	q_value: tensor([[-0.0047]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20879927237699492, distance: 1.017888088632165 entropy tensor([[ -0.9175,  -0.9018, -21.6069,  -0.8447,  -0.7045,  -2.5995,  -1.7460]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 22
	action: tensor([[-0.0368,  0.0199,  0.3805,  0.0946,  0.1712,  0.0011, -0.2014]],
       dtype=torch.float64)
	q_value: tensor([[-0.0150]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3479370466625592, distance: 0.9240627348919772 entropy tensor([[ -1.0099,  -0.9695, -21.6069,  -0.8697,  -0.7089,  -3.1657,  -1.8697]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 23
	action: tensor([[-0.0903,  0.1030,  0.3641,  0.1505,  0.2985, -0.0188, -0.2230]],
       dtype=torch.float64)
	q_value: tensor([[-0.0021]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3745348692580627, distance: 0.9050201417939594 entropy tensor([[ -1.0237,  -0.9986, -21.6069,  -0.9244,  -0.7646,  -3.1805,  -1.8527]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 24
	action: tensor([[-0.0403, -0.0033,  0.4058, -0.0740,  0.0433,  0.0253, -0.2531]],
       dtype=torch.float64)
	q_value: tensor([[-0.0037]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2241904197300003, distance: 1.007939029727154 entropy tensor([[ -0.9347,  -0.9534, -21.6069,  -0.8528,  -0.7074,  -2.6985,  -1.8126]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 25
	action: tensor([[ 0.1633,  0.0409,  0.3783,  0.0523,  0.1072,  0.0184, -0.1302]],
       dtype=torch.float64)
	q_value: tensor([[-0.0091]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5267953829846872, distance: 0.7871929209807154 entropy tensor([[ -1.0161,  -0.9522, -21.6069,  -0.8818,  -0.7263,  -3.0512,  -1.8357]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 26
	action: tensor([[-0.0237,  0.0918,  0.3018,  0.1077,  0.1764, -0.0099, -0.2222]],
       dtype=torch.float64)
	q_value: tensor([[0.0115]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.40998417940321086, distance: 0.8789992848282864 entropy tensor([[ -1.1342,  -1.0768, -21.6069,  -0.9803,  -0.8181,  -3.8660,  -1.9056]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 27
	action: tensor([[-0.1307,  0.0098,  0.3779,  0.0763,  0.1336,  0.0405, -0.2419]],
       dtype=torch.float64)
	q_value: tensor([[0.0023]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24305915163762704, distance: 0.9956063653237391 entropy tensor([[ -0.9835,  -0.9921, -21.6069,  -0.8845,  -0.7733,  -2.8114,  -1.8120]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 28
	action: tensor([[ 0.0657,  0.1074,  0.3907,  0.0876,  0.1741,  0.0079, -0.2246]],
       dtype=torch.float64)
	q_value: tensor([[-0.0107]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5026236798854682, distance: 0.807047777180604 entropy tensor([[ -0.9816,  -0.9509, -21.6069,  -0.8923,  -0.7258,  -3.1016,  -1.7995]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 29
	action: tensor([[-0.0977, -0.0531,  0.3444,  0.0816,  0.1492, -0.0143, -0.2110]],
       dtype=torch.float64)
	q_value: tensor([[0.0001]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21643206475004895, distance: 1.0129663557702997 entropy tensor([[ -1.0596,  -1.0442, -21.6069,  -0.9327,  -0.7821,  -3.1389,  -1.8644]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 30
	action: tensor([[ 0.0535, -0.0348,  0.3829,  0.0507,  0.0286,  0.0105, -0.2189]],
       dtype=torch.float64)
	q_value: tensor([[-0.0035]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37214901147582347, distance: 0.9067446137333308 entropy tensor([[ -0.9898,  -0.9621, -21.6069,  -0.9011,  -0.7435,  -3.1272,  -1.8411]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 31
	action: tensor([[-0.0338,  0.0627,  0.3309, -0.2219,  0.1228,  0.0079, -0.2183]],
       dtype=torch.float64)
	q_value: tensor([[0.0025]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18459384369273213, distance: 1.03334105949806 entropy tensor([[ -1.0899,  -1.0143, -21.6069,  -0.9617,  -0.7969,  -3.7537,  -1.8947]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 32
	action: tensor([[ 0.1280, -0.1224,  0.4246, -0.1642,  0.0363,  0.0349, -0.2933]],
       dtype=torch.float64)
	q_value: tensor([[-0.0044]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23229585231149252, distance: 1.0026598780255656 entropy tensor([[ -0.9027,  -0.8983, -21.6069,  -0.7692,  -0.6658,  -2.3720,  -1.7453]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 33
	action: tensor([[ 1.6175e-01,  1.1954e-01,  3.3925e-01,  9.2092e-02,  1.6556e-01,
         -1.0967e-04, -1.6805e-01]], dtype=torch.float64)
	q_value: tensor([[-0.0138]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.591527942415096, distance: 0.7313712254850163 entropy tensor([[ -1.0712,  -0.9965, -21.6069,  -0.8671,  -0.7227,  -3.4568,  -1.9032]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 34
	action: tensor([[-0.2108, -0.1425,  0.3187, -0.1044,  0.0921, -0.0080, -0.1804]],
       dtype=torch.float64)
	q_value: tensor([[0.0110]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10590274866228255, distance: 1.2034142838276884 entropy tensor([[ -1.0940,  -1.0816, -21.6069,  -0.9425,  -0.8129,  -3.2502,  -1.8833]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 35
	action: tensor([[ 0.0276,  0.1431,  0.4398,  0.0284,  0.2397,  0.0325, -0.3042]],
       dtype=torch.float64)
	q_value: tensor([[-0.0139]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4630842730972924, distance: 0.8385129144853504 entropy tensor([[ -0.8865,  -0.8495, -21.6069,  -0.7901,  -0.6356,  -2.6619,  -1.7225]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 36
	action: tensor([[-0.1161,  0.0411,  0.3783,  0.0484,  0.1383, -0.0090, -0.2671]],
       dtype=torch.float64)
	q_value: tensor([[-0.0118]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24512205402833365, distance: 0.9942487691487962 entropy tensor([[ -1.0009,  -1.0282, -21.6069,  -0.8648,  -0.7148,  -2.8438,  -1.8583]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 37
	action: tensor([[ 0.0489, -0.0153,  0.4029,  0.1138,  0.3341,  0.0153, -0.2940]],
       dtype=torch.float64)
	q_value: tensor([[-0.0099]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42528454014516615, distance: 0.8675272656360905 entropy tensor([[ -0.9550,  -0.9387, -21.6069,  -0.8736,  -0.7215,  -2.8451,  -1.8304]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 38
	action: tensor([[ 0.0117,  0.0461,  0.3614,  0.0296, -0.0472, -0.0271, -0.2054]],
       dtype=torch.float64)
	q_value: tensor([[-0.0092]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36493201454522295, distance: 0.9119411294458905 entropy tensor([[ -1.0384,  -1.0500, -21.6069,  -0.8875,  -0.7174,  -3.1861,  -1.9313]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 39
	action: tensor([[ 0.0275,  0.0916,  0.3517,  0.1130,  0.1965,  0.0185, -0.2178]],
       dtype=torch.float64)
	q_value: tensor([[0.0076]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.47392213008194417, distance: 0.8300069134672512 entropy tensor([[ -1.0496,  -0.9626, -21.6069,  -0.9457,  -0.8043,  -3.2314,  -1.8360]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 40
	action: tensor([[ 0.0077, -0.0187,  0.3548, -0.1240,  0.1192,  0.0052, -0.1685]],
       dtype=torch.float64)
	q_value: tensor([[0.0005]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22363627748567338, distance: 1.0082989388515418 entropy tensor([[ -1.0345,  -1.0314, -21.6069,  -0.9149,  -0.7789,  -3.0684,  -1.8521]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 41
	action: tensor([[-0.0633, -0.0139,  0.3762, -0.0122, -0.1352,  0.0169, -0.2459]],
       dtype=torch.float64)
	q_value: tensor([[-0.0015]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23027027574296388, distance: 1.0039817586341166 entropy tensor([[ -0.9943,  -0.9681, -21.6069,  -0.8561,  -0.7217,  -2.9102,  -1.8245]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 42
	action: tensor([[-0.0689, -0.0558,  0.3715, -0.0482,  0.0930,  0.0138, -0.2044]],
       dtype=torch.float64)
	q_value: tensor([[-0.0006]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16740683470754814, distance: 1.0441745745202065 entropy tensor([[ -1.0352,  -0.9059, -21.6069,  -0.9093,  -0.7589,  -3.1680,  -1.7853]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 43
	action: tensor([[ 0.0585, -0.0554,  0.3818, -0.0381,  0.0167, -0.0050, -0.2571]],
       dtype=torch.float64)
	q_value: tensor([[-0.0070]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2974740398109883, distance: 0.9591529831459402 entropy tensor([[ -0.9968,  -0.9527, -21.6069,  -0.8794,  -0.7244,  -3.1151,  -1.8240]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 44
	action: tensor([[ 0.0983, -0.1124,  0.3447,  0.0793,  0.2981, -0.0153, -0.2001]],
       dtype=torch.float64)
	q_value: tensor([[-0.0021]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36325374754056683, distance: 0.9131453083232924 entropy tensor([[ -1.0643,  -0.9912, -21.6069,  -0.9193,  -0.7708,  -3.4167,  -1.8982]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 45
	action: tensor([[-0.0134, -0.0539,  0.3395,  0.1087,  0.1777, -0.0030, -0.2285]],
       dtype=torch.float64)
	q_value: tensor([[0.0013]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3252490016306899, distance: 0.9400013150617555 entropy tensor([[ -1.0640,  -1.0680, -21.6069,  -0.8989,  -0.7472,  -3.5928,  -1.9547]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 46
	action: tensor([[-0.0441,  0.0551,  0.3579,  0.0803,  0.3508,  0.0034, -0.2438]],
       dtype=torch.float64)
	q_value: tensor([[-0.0010]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35550260824655633, distance: 0.9186863748177706 entropy tensor([[ -1.0332,  -1.0136, -21.6069,  -0.9130,  -0.7678,  -3.2950,  -1.8931]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 47
	action: tensor([[-0.0637,  0.0507,  0.4015,  0.0255,  0.2859,  0.0225, -0.2425]],
       dtype=torch.float64)
	q_value: tensor([[-0.0047]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3048190212524252, distance: 0.9541257869263216 entropy tensor([[ -0.9481,  -0.9861, -21.6069,  -0.8435,  -0.6886,  -2.7915,  -1.8522]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 48
	action: tensor([[-1.6351e-02, -7.2404e-02,  3.9802e-01,  1.3806e-05,  1.8265e-02,
         -1.6321e-02, -2.4566e-01]], dtype=torch.float64)
	q_value: tensor([[-0.0109]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23191530246820924, distance: 1.002908355767527 entropy tensor([[ -0.9603,  -0.9841, -21.6069,  -0.8580,  -0.6863,  -2.9023,  -1.8508]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 49
	action: tensor([[-0.0419, -0.1253,  0.3586,  0.1990, -0.0846,  0.0056, -0.2458]],
       dtype=torch.float64)
	q_value: tensor([[-0.0044]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30845947812455465, distance: 0.9516242706362662 entropy tensor([[ -1.0470,  -0.9660, -21.6069,  -0.9202,  -0.7641,  -3.4049,  -1.8842]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 50
	action: tensor([[ 0.1261,  0.1082,  0.3382,  0.1545,  0.0905,  0.0072, -0.2399]],
       dtype=torch.float64)
	q_value: tensor([[0.0055]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5902846277775061, distance: 0.7324834602508339 entropy tensor([[ -1.0868,  -0.9717, -21.6069,  -0.9595,  -0.7920,  -4.1594,  -1.8831]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 51
	action: tensor([[-0.2059, -0.0587,  0.3204, -0.0058,  0.0100, -0.0005, -0.1460]],
       dtype=torch.float64)
	q_value: tensor([[0.0076]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.035619301686238725, distance: 1.1237790927823215 entropy tensor([[ -1.0964,  -1.0607, -21.6069,  -0.9510,  -0.8265,  -3.4553,  -1.9098]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 52
	action: tensor([[ 0.0941,  0.0962,  0.4161, -0.0480,  0.1043,  0.0407, -0.2865]],
       dtype=torch.float64)
	q_value: tensor([[-0.0025]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.45109598759267, distance: 0.8478224180855919 entropy tensor([[ -0.9347,  -0.8670, -21.6069,  -0.8558,  -0.7001,  -2.8267,  -1.6843]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 53
	action: tensor([[-0.0641, -0.1005,  0.3502, -0.0462,  0.2717,  0.0027, -0.2480]],
       dtype=torch.float64)
	q_value: tensor([[-0.0060]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1348803558840327, distance: 1.0643752812341778 entropy tensor([[ -1.0556,  -1.0284, -21.6069,  -0.8980,  -0.7517,  -3.0880,  -1.8656]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 54
	action: tensor([[ 0.0620, -0.0707,  0.4027, -0.0539,  0.0949,  0.0118, -0.2661]],
       dtype=torch.float64)
	q_value: tensor([[-0.0127]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2833132836914525, distance: 0.9687715374849779 entropy tensor([[ -0.9453,  -0.9618, -21.6069,  -0.8294,  -0.6644,  -3.0263,  -1.8846]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 55
	action: tensor([[-0.0315,  0.0677,  0.3484, -0.0028, -0.0496, -0.0126, -0.2672]],
       dtype=torch.float64)
	q_value: tensor([[-0.0088]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3203442411524201, distance: 0.9434115606448951 entropy tensor([[ -1.0574,  -1.0129, -21.6069,  -0.9005,  -0.7475,  -3.4406,  -1.8986]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 56
	action: tensor([[ 0.0621, -0.2666,  0.3771,  0.0084,  0.1975,  0.0011, -0.2001]],
       dtype=torch.float64)
	q_value: tensor([[0.0004]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1566856500666417, distance: 1.050875914759734 entropy tensor([[ -1.0076,  -0.9427, -21.6069,  -0.9055,  -0.7718,  -2.8828,  -1.8005]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 57
	action: tensor([[-0.0115, -0.0255,  0.3364, -0.0540,  0.1616, -0.0162, -0.2296]],
       dtype=torch.float64)
	q_value: tensor([[-0.0057]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23789066928842917, distance: 0.9989996422285851 entropy tensor([[ -1.0614,  -1.0270, -21.6069,  -0.8784,  -0.7216,  -4.1976,  -1.9679]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 58
	action: tensor([[ 0.0510,  0.0466,  0.3817,  0.0195,  0.2511, -0.0121, -0.2581]],
       dtype=torch.float64)
	q_value: tensor([[-0.0041]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4012202319521838, distance: 0.8855034388693036 entropy tensor([[ -0.9853,  -0.9817, -21.6069,  -0.8656,  -0.7265,  -2.9428,  -1.8515]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 59
	action: tensor([[-0.1307,  0.0335,  0.3670, -0.1540,  0.1266, -0.0164, -0.2735]],
       dtype=torch.float64)
	q_value: tensor([[-0.0049]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08747289393740065, distance: 1.0931495495473846 entropy tensor([[ -1.0154,  -1.0392, -21.6069,  -0.8833,  -0.7356,  -2.9857,  -1.8797]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 60
	action: tensor([[ 0.0617,  0.0806,  0.4445, -0.1737,  0.1074,  0.0078, -0.3377]],
       dtype=torch.float64)
	q_value: tensor([[-0.0122]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3188565750610546, distance: 0.9444434905643515 entropy tensor([[ -0.8743,  -0.8657, -21.6069,  -0.7808,  -0.6409,  -2.4207,  -1.7903]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 61
	action: tensor([[-0.2135, -0.0200,  0.3890, -0.0521, -0.0207, -0.0043, -0.2101]],
       dtype=torch.float64)
	q_value: tensor([[-0.0137]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0223842059493905, distance: 1.131464149614248 entropy tensor([[ -0.9771,  -0.9802, -21.6069,  -0.8188,  -0.6810,  -2.6657,  -1.8530]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 62
	action: tensor([[-0.0623, -0.0684,  0.4288, -0.0936, -0.1062,  0.0431, -0.2887]],
       dtype=torch.float64)
	q_value: tensor([[-0.0106]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1408834465160127, distance: 1.0606759870084317 entropy tensor([[ -0.9340,  -0.8451, -21.6069,  -0.8400,  -0.6750,  -2.7005,  -1.7123]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 63
	action: tensor([[ 0.0077, -0.0034,  0.3783, -0.0363,  0.0666,  0.0126, -0.2384]],
       dtype=torch.float64)
	q_value: tensor([[-0.0117]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29488951431943866, distance: 0.9609156792598448 entropy tensor([[ -1.0466,  -0.8991, -21.6069,  -0.8750,  -0.7115,  -3.1681,  -1.8113]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
LOSS epoch 7 actor 0.13108994510663316 critic 16.107300194516547
epoch: 8, step: 0
	action: tensor([[ 0.0291, -0.0377,  0.6810,  0.1846,  0.0565, -0.0694, -0.3878]],
       dtype=torch.float64)
	q_value: tensor([[0.0122]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41906561350348637, distance: 0.8722083403026704 entropy tensor([[ -0.4891,  -0.5854, -21.6069,  -0.4386,  -0.2537,  -1.2223,  -1.2140]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 1
	action: tensor([[ 0.0642,  0.0829,  0.6975, -0.0883,  0.1632,  0.0517, -0.3147]],
       dtype=torch.float64)
	q_value: tensor([[0.0066]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3880343276268675, distance: 0.8952003102179064 entropy tensor([[ -0.5142,  -0.5671, -21.6069,  -0.4424,  -0.2212,  -1.2854,  -1.2824]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 2
	action: tensor([[-0.0654, -0.0537,  0.7239,  0.0509, -0.1155,  0.0582, -0.2583]],
       dtype=torch.float64)
	q_value: tensor([[-0.0057]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2616808028772345, distance: 0.9832835500033886 entropy tensor([[ -0.4629,  -0.5505, -21.6069,  -0.3766,  -0.1551,  -1.1972,  -1.2499]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 3
	action: tensor([[ 0.2008, -0.0483,  0.6916,  0.2224,  0.4075,  0.0459, -0.3133]],
       dtype=torch.float64)
	q_value: tensor([[0.0043]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6132809660345853, distance: 0.7116303590574188 entropy tensor([[ -0.5410,  -0.5163, -21.6069,  -0.4485,  -0.2156,  -1.3001,  -1.1915]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 4
	action: tensor([[ 0.0285, -0.3816,  0.6797, -0.0521, -0.2319, -0.0131, -0.3939]],
       dtype=torch.float64)
	q_value: tensor([[0.0077]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03395952392796753, distance: 1.1636127257472058 entropy tensor([[ -0.5464,  -0.6181, -21.6069,  -0.4169,  -0.1755,  -1.3078,  -1.3406]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 5
	action: tensor([[ 0.0447,  0.1950,  0.7139, -0.0265,  0.1898, -0.0967, -0.3863]],
       dtype=torch.float64)
	q_value: tensor([[0.0078]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4351000361256542, distance: 0.8600871649460432 entropy tensor([[ -0.5092,  -0.5099, -21.6069,  -0.3710,  -0.1651,  -1.3077,  -1.2175]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 6
	action: tensor([[-0.1842,  0.0462,  0.7720, -0.0201,  0.4063,  0.1193, -0.4116]],
       dtype=torch.float64)
	q_value: tensor([[0.0042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17391415034796331, distance: 1.0400860817997795 entropy tensor([[ -0.3893,  -0.5140, -21.6069,  -0.3482,  -0.1302,  -1.0745,  -1.2259]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 7
	action: tensor([[ 0.0427,  0.1269,  0.8383,  0.1576,  0.0594, -0.0411, -0.5226]],
       dtype=torch.float64)
	q_value: tensor([[-0.0211]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5323729439943348, distance: 0.782539932599045 entropy tensor([[-3.4477e-01, -4.5783e-01, -2.1607e+01, -2.7627e-01, -3.4879e-03,
         -1.0356e+00, -1.2570e+00]], dtype=torch.float64,
       grad_fn=<AddBackward0>)
epoch: 8, step: 8
	action: tensor([[ 1.7856e-03, -1.4339e-01,  7.5488e-01, -1.2064e-01,  4.9191e-01,
          3.3124e-04, -4.5835e-01]], dtype=torch.float64)
	q_value: tensor([[-0.0023]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1049324768673997, distance: 1.0826413066133667 entropy tensor([[ -0.4553,  -0.5271, -21.6069,  -0.3646,  -0.1365,  -1.1657,  -1.2497]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 9
	action: tensor([[-0.2109, -0.0293,  0.8432, -0.1055,  0.6773,  0.1457, -0.4163]],
       dtype=torch.float64)
	q_value: tensor([[-0.0206]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.021098995727716008, distance: 1.1322076378470338 entropy tensor([[-3.1706e-01, -4.5349e-01, -2.1607e+01, -2.4357e-01,  1.1959e-02,
         -1.0009e+00, -1.3131e+00]], dtype=torch.float64,
       grad_fn=<AddBackward0>)
epoch: 8, step: 10
	action: tensor([[ 0.0414,  0.1588,  0.9350, -0.1420, -0.0722,  0.0228, -0.5468]],
       dtype=torch.float64)
	q_value: tensor([[-0.0277]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36796198444507533, distance: 0.9097630493506355 entropy tensor([[ -0.2264,  -0.3888, -21.6069,  -0.1618,   0.1297,  -0.8789,  -1.2283]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 11
	action: tensor([[-0.2175, -0.0751,  0.8077, -0.1934,  0.1440, -0.0283, -0.3087]],
       dtype=torch.float64)
	q_value: tensor([[-0.0179]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1551132006742808, distance: 1.2298976360837888 entropy tensor([[ -0.3926,  -0.4491, -21.6069,  -0.3007,  -0.0555,  -1.0536,  -1.1862]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 12
	action: tensor([[-0.0425, -0.2181,  0.8415, -0.1232,  0.3235,  0.1115, -0.3743]],
       dtype=torch.float64)
	q_value: tensor([[-0.0129]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03422816856654498, distance: 1.124589334497017 entropy tensor([[ -0.3415,  -0.3972, -21.6069,  -0.2780,  -0.0305,  -1.0315,  -1.2107]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 13
	action: tensor([[-0.1737, -0.0661,  0.7909, -0.3277,  0.1762,  0.0931, -0.3579]],
       dtype=torch.float64)
	q_value: tensor([[-0.0233]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1517757119583334, distance: 1.2281195684595625 entropy tensor([[ -0.4113,  -0.4873, -21.6069,  -0.2982,  -0.0231,  -1.1307,  -1.2940]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 14
	action: tensor([[ 0.2174,  0.0481,  0.8545,  0.1067,  0.4251, -0.0992, -0.3521]],
       dtype=torch.float64)
	q_value: tensor([[-0.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5799833013513738, distance: 0.7416345807577431 entropy tensor([[-3.2800e-01, -3.9700e-01, -2.1607e+01, -2.4434e-01,  4.6978e-03,
         -9.8969e-01, -1.1826e+00]], dtype=torch.float64,
       grad_fn=<AddBackward0>)
epoch: 8, step: 15
	action: tensor([[-0.0770,  0.0075,  0.7547, -0.2752,  0.1252,  0.0088, -0.4648]],
       dtype=torch.float64)
	q_value: tensor([[0.0078]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.028824271993621475, distance: 1.1277312189907196 entropy tensor([[ -0.4320,  -0.5513, -21.6069,  -0.3307,  -0.1005,  -1.1392,  -1.2888]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 16
	action: tensor([[ 0.1574, -0.0165,  0.8368,  0.1476, -0.0127,  0.0052, -0.4723]],
       dtype=torch.float64)
	q_value: tensor([[-0.0218]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5440004045650024, distance: 0.772749836681158 entropy tensor([[ -0.3332,  -0.4201, -21.6069,  -0.2649,  -0.0328,  -0.9977,  -1.2136]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 17
	action: tensor([[-0.1476,  0.1738,  0.6965, -0.0158,  0.1950,  0.0519, -0.4418]],
       dtype=torch.float64)
	q_value: tensor([[-0.0034]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2794068258157728, distance: 0.9714082001363845 entropy tensor([[ -0.5384,  -0.5797, -21.6069,  -0.4054,  -0.1769,  -1.2965,  -1.2973]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 18
	action: tensor([[-0.3268,  0.1805,  0.8094, -0.0815,  0.2310,  0.0151, -0.3287]],
       dtype=torch.float64)
	q_value: tensor([[-0.0124]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.005049138793094987, distance: 1.1414516215708985 entropy tensor([[ -0.3670,  -0.4720, -21.6069,  -0.3229,  -0.0777,  -1.0352,  -1.2191]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 19
	action: tensor([[-0.2992,  0.2210,  0.8878,  0.1442, -0.3320, -0.0867, -0.5433]],
       dtype=torch.float64)
	q_value: tensor([[-0.0092]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1414516215708985 entropy tensor([[-2.7808e-01, -3.7243e-01, -2.1607e+01, -2.5220e-01,  4.1879e-03,
         -9.1247e-01, -1.1626e+00]], dtype=torch.float64,
       grad_fn=<AddBackward0>)
epoch: 8, step: 20
	action: tensor([[-0.0218, -0.9402,  2.9078,  0.2461,  1.4404,  0.9033, -2.7291]],
       dtype=torch.float64)
	q_value: tensor([[0.0263]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8118904529924083, distance: 1.540361522449346 entropy tensor([[  0.9751,   0.8761, -21.6069,   1.1115,   1.1265,   0.7613,  -0.2400]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 21
	action: tensor([[-0.1187, -0.0428,  2.0668, -0.6971, -0.7668,  0.1163, -1.0437]],
       dtype=torch.float64)
	q_value: tensor([[-0.0965]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06213988033751816, distance: 1.1793631426041256 entropy tensor([[ 5.7126e-01,  3.2484e-01, -2.1607e+01,  7.3781e-01,  9.9904e-01,
          1.8808e-02, -9.0819e-01]], dtype=torch.float64,
       grad_fn=<AddBackward0>)
epoch: 8, step: 22
	action: tensor([[-0.0139,  0.2461,  1.3186,  0.3247,  0.6516,  0.2502, -0.7145]],
       dtype=torch.float64)
	q_value: tensor([[0.0048]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6671690297618007, distance: 0.6601894178737806 entropy tensor([[ -0.0667,   0.0255, -21.6069,   0.1724,   0.4606,  -0.5044,  -1.0186]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 23
	action: tensor([[-0.1772, -0.1331,  0.9804,  0.1033,  0.0456, -0.3508, -0.2916]],
       dtype=torch.float64)
	q_value: tensor([[-0.0167]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07109746147224727, distance: 1.1843257945200742 entropy tensor([[ -0.2160,  -0.3361, -21.6069,  -0.0934,   0.2111,  -0.8387,  -1.1611]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 24
	action: tensor([[-0.0721,  0.2712,  0.8468,  0.1065,  0.4858,  0.1424, -0.3768]],
       dtype=torch.float64)
	q_value: tensor([[0.0317]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1843257945200742 entropy tensor([[ -0.3463,  -0.3837, -21.6069,  -0.3047,  -0.0477,  -1.0278,  -1.2468]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 25
	action: tensor([[-0.2261,  1.0742,  2.9078, -0.1319,  0.7847,  0.9971, -2.3637]],
       dtype=torch.float64)
	q_value: tensor([[0.0263]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30805628695268594, distance: 0.951901644545324 entropy tensor([[  0.9751,   0.8761, -21.6069,   1.1115,   1.1265,   0.7613,  -0.2400]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 26
	action: tensor([[-0.7347,  0.1129,  2.9078, -1.8281,  1.4379,  0.3114, -2.4965]],
       dtype=torch.float64)
	q_value: tensor([[0.0263]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07220422529069859, distance: 1.1022570581448545 entropy tensor([[  0.9751,   0.8761, -21.6069,   1.1115,   1.1265,   0.7613,  -0.2400]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 27
	action: tensor([[-0.3873,  0.0116,  2.9078,  0.1189,  0.7747,  0.3591, -2.2530]],
       dtype=torch.float64)
	q_value: tensor([[0.0263]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3448226163318129, distance: 1.3270558333228835 entropy tensor([[  0.9751,   0.8761, -21.6069,   1.1115,   1.1265,   0.7613,  -0.2400]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 28
	action: tensor([[ 0.2808, -0.3873,  1.9122,  0.6727, -0.0880, -0.1947, -1.1208]],
       dtype=torch.float64)
	q_value: tensor([[-0.0541]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28685611316615656, distance: 0.9663740851372906 entropy tensor([[  0.4172,   0.2631, -21.6069,   0.5668,   0.8507,  -0.0836,  -0.8650]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 29
	action: tensor([[ 0.0670,  0.1810,  1.0709,  0.0437,  0.7928, -0.1243, -0.5004]],
       dtype=torch.float64)
	q_value: tensor([[0.0164]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4552555855824376, distance: 0.8446039072608761 entropy tensor([[ -0.1917,  -0.1900, -21.6069,   0.0679,   0.3022,  -0.7694,  -1.2070]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 30
	action: tensor([[-0.2239,  0.1742,  0.9716, -0.1710,  0.1424,  0.2050, -0.6310]],
       dtype=torch.float64)
	q_value: tensor([[0.0003]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06943517256339449, distance: 1.1039007032344628 entropy tensor([[ -0.1793,  -0.3496, -21.6069,  -0.1309,   0.1363,  -0.8148,  -1.2402]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 31
	action: tensor([[-0.2422,  0.2663,  0.9194,  0.2327,  0.2748, -0.1823, -0.5549]],
       dtype=torch.float64)
	q_value: tensor([[-0.0389]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1039007032344628 entropy tensor([[ -0.2899,  -0.3568, -21.6069,  -0.1978,   0.0857,  -0.8892,  -1.1430]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 32
	action: tensor([[-0.3970, -0.5409,  2.9078,  0.8736,  0.6828,  0.4038, -2.3251]],
       dtype=torch.float64)
	q_value: tensor([[0.0263]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1039007032344628 entropy tensor([[  0.9751,   0.8761, -21.6069,   1.1115,   1.1265,   0.7613,  -0.2400]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 33
	action: tensor([[ 1.0754, -0.4372,  2.9078, -0.1894,  1.3861,  0.6772, -2.3299]],
       dtype=torch.float64)
	q_value: tensor([[0.0263]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42302587539000447, distance: 0.8692303099933487 entropy tensor([[  0.9751,   0.8761, -21.6069,   1.1115,   1.1265,   0.7613,  -0.2400]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 34
	action: tensor([[ 0.2399, -0.2818,  2.9078, -0.7517,  1.0368,  0.6878, -2.0476]],
       dtype=torch.float64)
	q_value: tensor([[0.0263]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38443191481152705, distance: 0.8978312985362378 entropy tensor([[  0.9751,   0.8761, -21.6069,   1.1115,   1.1265,   0.7613,  -0.2400]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 35
	action: tensor([[ 0.4501, -0.9188,  2.9078, -0.0686,  0.0156, -0.0884, -2.3171]],
       dtype=torch.float64)
	q_value: tensor([[0.0263]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2469827944184566, distance: 1.2778707337385482 entropy tensor([[  0.9751,   0.8761, -21.6069,   1.1115,   1.1265,   0.7613,  -0.2400]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 36
	action: tensor([[ 0.1556, -0.4456,  1.6846,  0.1212,  1.1524,  0.1798, -0.8169]],
       dtype=torch.float64)
	q_value: tensor([[-0.0342]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.046727419195452535, distance: 1.1172882821536785 entropy tensor([[  0.2753,   0.1464, -21.6069,   0.4999,   0.7584,  -0.1975,  -1.0058]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 37
	action: tensor([[-0.5791, -0.3020,  1.1538, -0.3727,  0.3963, -0.1928, -0.7143]],
       dtype=torch.float64)
	q_value: tensor([[-0.0363]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0014010908508162, distance: 1.6189139276382287 entropy tensor([[ -0.0227,  -0.2157, -21.6069,   0.1285,   0.4213,  -0.6238,  -1.1663]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 38
	action: tensor([[ 0.1000, -0.1688,  1.2215,  0.0438,  0.5568, -0.0474, -0.6910]],
       dtype=torch.float64)
	q_value: tensor([[-0.0331]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25828260411584014, distance: 0.9855437898775211 entropy tensor([[ 2.0986e-03, -8.7218e-02, -2.1607e+01,  6.8112e-02,  3.3470e-01,
         -5.1815e-01, -1.1660e+00]], dtype=torch.float64,
       grad_fn=<AddBackward0>)
epoch: 8, step: 39
	action: tensor([[-0.1655, -0.3297,  0.9333,  0.1156,  0.6145, -0.0110, -0.5768]],
       dtype=torch.float64)
	q_value: tensor([[-0.0269]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07994212071760098, distance: 1.189205566375996 entropy tensor([[ -0.2581,  -0.3853, -21.6069,  -0.1353,   0.1273,  -0.8891,  -1.2905]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 40
	action: tensor([[-0.3254, -0.0202,  0.9348,  0.1422,  0.7670,  0.0170, -0.3255]],
       dtype=torch.float64)
	q_value: tensor([[-0.0247]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.005846682757555399, distance: 1.1409940410039074 entropy tensor([[ -0.2492,  -0.3511, -21.6069,  -0.1730,   0.1130,  -0.9117,  -1.3379]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 41
	action: tensor([[-0.3107, -0.0521,  0.9787,  0.0802,  0.2238,  0.0830, -0.5965]],
       dtype=torch.float64)
	q_value: tensor([[-0.0051]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.02428932381614657, distance: 1.1581585463992725 entropy tensor([[ -0.1775,  -0.3196, -21.6069,  -0.1643,   0.1599,  -0.8163,  -1.1838]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 42
	action: tensor([[ 0.0625, -0.2179,  0.9056,  0.0485,  0.1827, -0.0923, -0.5315]],
       dtype=torch.float64)
	q_value: tensor([[-0.0345]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16400984836018928, distance: 1.0463025262553536 entropy tensor([[ -0.3175,  -0.3708, -21.6069,  -0.2235,   0.0549,  -0.9654,  -1.2263]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 43
	action: tensor([[-0.1654,  0.2860,  0.7892,  0.1166,  0.7686, -0.0720, -0.2620]],
       dtype=torch.float64)
	q_value: tensor([[-0.0101]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0463025262553536 entropy tensor([[ -0.4197,  -0.4883, -21.6069,  -0.3033,  -0.0612,  -1.1384,  -1.3294]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 44
	action: tensor([[-0.8806,  0.5639,  2.9078, -1.4740,  0.3224,  0.1430, -2.0450]],
       dtype=torch.float64)
	q_value: tensor([[0.0263]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0463025262553536 entropy tensor([[  0.9751,   0.8761, -21.6069,   1.1115,   1.1265,   0.7613,  -0.2400]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 45
	action: tensor([[-0.3131, -0.9799,  2.9078, -1.9309,  2.0413,  0.6969, -2.2824]],
       dtype=torch.float64)
	q_value: tensor([[0.0263]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03486806493188499, distance: 1.124216710319959 entropy tensor([[  0.9751,   0.8761, -21.6069,   1.1115,   1.1265,   0.7613,  -0.2400]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 46
	action: tensor([[-0.1090, -0.4898,  2.9078,  1.0353,  1.6526,  0.6584, -2.5345]],
       dtype=torch.float64)
	q_value: tensor([[0.0263]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6233098822636296, distance: 1.4579996157747748 entropy tensor([[  0.9751,   0.8761, -21.6069,   1.1115,   1.1265,   0.7613,  -0.2400]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 47
	action: tensor([[-0.2569,  0.7587,  2.0585,  0.0647,  0.3916, -0.4838, -1.0192]],
       dtype=torch.float64)
	q_value: tensor([[-0.0696]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4579996157747748 entropy tensor([[  0.5827,   0.3664, -21.6069,   0.7155,   0.9627,  -0.0221,  -0.8728]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 48
	action: tensor([[ 1.4232, -1.3859,  2.9078,  1.2019,  2.2419,  0.6862, -2.2245]],
       dtype=torch.float64)
	q_value: tensor([[0.0263]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4579996157747748 entropy tensor([[  0.9751,   0.8761, -21.6069,   1.1115,   1.1265,   0.7613,  -0.2400]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 49
	action: tensor([[ 0.3935,  0.4032,  2.9078,  0.5898,  0.4041,  0.4945, -2.2402]],
       dtype=torch.float64)
	q_value: tensor([[0.0263]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6168053738853992, distance: 0.7083801748147069 entropy tensor([[  0.9751,   0.8761, -21.6069,   1.1115,   1.1265,   0.7613,  -0.2400]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 50
	action: tensor([[-1.3693, -0.6925,  2.9078,  0.7840,  1.4141,  0.4276, -2.1729]],
       dtype=torch.float64)
	q_value: tensor([[0.0263]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.3423521547224264, distance: 1.751389684854469 entropy tensor([[  0.9751,   0.8761, -21.6069,   1.1115,   1.1265,   0.7613,  -0.2400]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 51
	action: tensor([[ 0.3472,  0.2413,  2.3175, -0.2468,  1.3767,  0.0430, -1.1902]],
       dtype=torch.float64)
	q_value: tensor([[-0.0424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.751389684854469 entropy tensor([[  0.6686,   0.4323, -21.6069,   0.7248,   1.0306,   0.1834,  -0.8471]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 52
	action: tensor([[-0.5801,  0.0993,  2.9078, -0.4849,  1.1894,  1.3162, -2.5779]],
       dtype=torch.float64)
	q_value: tensor([[0.0263]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5077685122784668, distance: 1.4051543363607186 entropy tensor([[  0.9751,   0.8761, -21.6069,   1.1115,   1.1265,   0.7613,  -0.2400]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 53
	action: tensor([[ 0.0244, -0.3983,  2.2739,  0.1102,  1.3607, -0.3831, -1.1788]],
       dtype=torch.float64)
	q_value: tensor([[-0.0769]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11799564074290769, distance: 1.0747119033291634 entropy tensor([[  0.5894,   0.4318, -21.6069,   0.7917,   1.0637,   0.1950,  -0.7415]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 54
	action: tensor([[-0.0127, -0.0160,  1.5363,  0.1197,  0.7172,  0.2139, -0.8392]],
       dtype=torch.float64)
	q_value: tensor([[-0.0007]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32815932417686, distance: 0.9379719269575257 entropy tensor([[  0.2357,   0.0644, -21.6069,   0.3159,   0.6212,  -0.2559,  -1.0824]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 55
	action: tensor([[-0.0060,  0.0370,  1.0641,  0.0449,  0.4690, -0.0846, -0.5650]],
       dtype=torch.float64)
	q_value: tensor([[-0.0349]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2880878464312181, distance: 0.9655391698082422 entropy tensor([[-1.3755e-01, -2.7555e-01, -2.1607e+01,  9.5256e-05,  3.0368e-01,
         -7.3651e-01, -1.1665e+00]], dtype=torch.float64,
       grad_fn=<AddBackward0>)
epoch: 8, step: 56
	action: tensor([[-0.0786, -0.1055,  0.8944,  0.0352,  0.2824,  0.0664, -0.4448]],
       dtype=torch.float64)
	q_value: tensor([[-0.0142]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14694741905707442, distance: 1.0569260301568317 entropy tensor([[ -0.2882,  -0.4217, -21.6069,  -0.2051,   0.0608,  -0.9326,  -1.2626]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 57
	action: tensor([[ 2.2296e-01,  1.2423e-01,  7.9436e-01,  1.3596e-01,  7.7867e-01,
         -7.9170e-04, -4.2946e-01]], dtype=torch.float64)
	q_value: tensor([[-0.0227]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6399729429517483, distance: 0.6866323540632865 entropy tensor([[ -0.4218,  -0.4812, -21.6069,  -0.3171,  -0.0416,  -1.1382,  -1.2867]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 58
	action: tensor([[-0.0887, -0.4020,  0.8412,  0.0025,  0.2563, -0.0203, -0.4216]],
       dtype=torch.float64)
	q_value: tensor([[0.0011]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17602478000537514, distance: 1.2409804210710067 entropy tensor([[-3.2335e-01, -4.8705e-01, -2.1607e+01, -2.2475e-01,  7.6974e-03,
         -9.9531e-01, -1.2859e+00]], dtype=torch.float64,
       grad_fn=<AddBackward0>)
epoch: 8, step: 59
	action: tensor([[ 0.0891, -0.1828,  0.8072, -0.1359,  0.2604,  0.0483, -0.5130]],
       dtype=torch.float64)
	q_value: tensor([[-0.0151]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11558739017397601, distance: 1.0761781156135493 entropy tensor([[ -0.3937,  -0.4461, -21.6069,  -0.2939,  -0.0290,  -1.1306,  -1.3231]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 60
	action: tensor([[-0.0533, -0.0924,  0.7830, -0.1358,  0.3880,  0.0691, -0.4541]],
       dtype=torch.float64)
	q_value: tensor([[-0.0247]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04824153310793722, distance: 1.1164006168381106 entropy tensor([[ -0.4042,  -0.5087, -21.6069,  -0.2843,  -0.0474,  -1.1133,  -1.3022]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 61
	action: tensor([[ 0.0258,  0.2608,  0.8269, -0.0637,  0.4474,  0.1199, -0.4132]],
       dtype=torch.float64)
	q_value: tensor([[-0.0257]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1164006168381106 entropy tensor([[-3.5100e-01, -4.6553e-01, -2.1607e+01, -2.6639e-01, -3.8352e-03,
         -1.0454e+00, -1.2857e+00]], dtype=torch.float64,
       grad_fn=<AddBackward0>)
epoch: 8, step: 62
	action: tensor([[ 0.1148, -0.1905,  2.9078, -1.9696,  2.1817,  1.4304, -2.1753]],
       dtype=torch.float64)
	q_value: tensor([[0.0263]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23020528257553918, distance: 1.0040241440163638 entropy tensor([[  0.9751,   0.8761, -21.6069,   1.1115,   1.1265,   0.7613,  -0.2400]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 63
	action: tensor([[-0.1264, -0.9126,  2.9078, -1.2062,  1.1782, -0.0346, -2.5853]],
       dtype=torch.float64)
	q_value: tensor([[0.0263]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03810314699589579, distance: 1.1223309649548694 entropy tensor([[  0.9751,   0.8761, -21.6069,   1.1115,   1.1265,   0.7613,  -0.2400]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
LOSS epoch 8 actor 332.23187402122807 critic 2374.9975860922864
