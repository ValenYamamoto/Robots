epoch: 0, step: 0
	action: tensor([[ 0.0102,  0.0183, -0.0741,  0.2021,  0.1519,  0.0533,  0.0223]],
       dtype=torch.float64)
	q_value: tensor([[-0.1624]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.4018573838451769, distance: 0.8850321885102984 entropy tensor([[-21.6069,  -1.3902,  -3.4663, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 1
	action: tensor([[-0.0621,  0.0271,  0.0532,  0.0259,  0.0227,  0.0243,  0.0461]],
       dtype=torch.float64)
	q_value: tensor([[-0.0593]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.25446481348853867, distance: 0.9880769450542959 entropy tensor([[ -1.3974,  -3.3245, -21.6069, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 2
	action: tensor([[0.0425, 0.0389, 0.0573, 0.0263, 0.0113, 0.0248, 0.0343]],
       dtype=torch.float64)
	q_value: tensor([[-0.0723]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.37216162801243247, distance: 0.9067355032642109 entropy tensor([[ -1.4443, -21.6069, -21.6069, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 3
	action: tensor([[-0.0050,  0.0394,  0.0612,  0.0240,  0.0136,  0.0276,  0.0298]],
       dtype=torch.float64)
	q_value: tensor([[-0.0709]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.3239544698056348, distance: 0.9409025945951651 entropy tensor([[ -1.3234, -21.6069, -21.6069, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 4
	action: tensor([[-0.0760,  0.0391,  0.0596,  0.0247,  0.0126,  0.0257,  0.0314]],
       dtype=torch.float64)
	q_value: tensor([[-0.0715]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.2476104161181618, distance: 0.9926087072178854 entropy tensor([[ -1.3618, -21.6069, -21.6069, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 5
	action: tensor([[-0.0174,  0.0379,  0.0576,  0.0264,  0.0111,  0.0237,  0.0344]],
       dtype=torch.float64)
	q_value: tensor([[-0.0725]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.3100881187497405, distance: 0.9505030293743566 entropy tensor([[ -1.4264, -21.6069, -21.6069, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 6
	action: tensor([[-0.1009,  0.0389,  0.0593,  0.0249,  0.0122,  0.0257,  0.0319]],
       dtype=torch.float64)
	q_value: tensor([[-0.0715]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.21998438680082322, distance: 1.0106675950837993 entropy tensor([[ -1.3659, -21.6069, -21.6069, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 7
	action: tensor([[-0.1542,  0.0374,  0.0572,  0.0271,  0.0105,  0.0230,  0.0354]],
       dtype=torch.float64)
	q_value: tensor([[-0.0733]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.1585732143996894, distance: 1.0496991844606904 entropy tensor([[ -1.4474, -21.6069, -21.6069, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 8
	action: tensor([[-0.1299,  0.0358,  0.0560,  0.0284,  0.0095,  0.0212,  0.0374]],
       dtype=torch.float64)
	q_value: tensor([[-0.0747]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.1854623060155518, distance: 1.032790624080238 entropy tensor([[ -1.4994, -21.6069, -21.6069, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 9
	action: tensor([[-0.1114,  0.0366,  0.0562,  0.0280,  0.0101,  0.0222,  0.0366]],
       dtype=torch.float64)
	q_value: tensor([[-0.0739]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.2070665546445245, distance: 1.0190020588985549 entropy tensor([[ -1.4845, -21.6069, -21.6069, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 10
	action: tensor([[0.0430, 0.0372, 0.0569, 0.0274, 0.0103, 0.0229, 0.0360]],
       dtype=torch.float64)
	q_value: tensor([[-0.0734]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.37157671268722836, distance: 0.9071577775682499 entropy tensor([[ -1.4593, -21.6069, -21.6069, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 11
	action: tensor([[-0.0801,  0.0394,  0.0612,  0.0240,  0.0135,  0.0278,  0.0298]],
       dtype=torch.float64)
	q_value: tensor([[-0.0708]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.24353080451595566, distance: 0.9952961338300286 entropy tensor([[ -1.3232, -21.6069, -21.6069, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 12
	action: tensor([[-0.0017,  0.0378,  0.0575,  0.0265,  0.0111,  0.0235,  0.0345]],
       dtype=torch.float64)
	q_value: tensor([[-0.0727]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.32634160776019694, distance: 0.9392399473074896 entropy tensor([[ -1.4313, -21.6069, -21.6069, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 13
	action: tensor([[0.0015, 0.0391, 0.0598, 0.0247, 0.0124, 0.0262, 0.0313]],
       dtype=torch.float64)
	q_value: tensor([[-0.0712]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.33030806380366484, distance: 0.9364707737778041 entropy tensor([[ -1.3534, -21.6069, -21.6069, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 14
	action: tensor([[0.0363, 0.0391, 0.0598, 0.0246, 0.0127, 0.0261, 0.0312]],
       dtype=torch.float64)
	q_value: tensor([[-0.0714]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.3656979686045375, distance: 0.9113910184582533 entropy tensor([[ -1.3566, -21.6069, -21.6069, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 15
	action: tensor([[-0.1748,  0.0394,  0.0611,  0.0241,  0.0132,  0.0272,  0.0297]],
       dtype=torch.float64)
	q_value: tensor([[-0.0709]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.13561222167883014, distance: 1.0639249708726555 entropy tensor([[ -1.3215, -21.6069, -21.6069, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 16
	action: tensor([[-0.0395,  0.0352,  0.0559,  0.0287,  0.0092,  0.0201,  0.0379]],
       dtype=torch.float64)
	q_value: tensor([[-0.0754]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.2849389507330532, distance: 0.9676721770337428 entropy tensor([[ -1.5172, -21.6069, -21.6069, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 17
	action: tensor([[-0.0224,  0.0386,  0.0584,  0.0256,  0.0117,  0.0253,  0.0331]],
       dtype=torch.float64)
	q_value: tensor([[-0.0716]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.3052398514222815, distance: 0.9538369515627462 entropy tensor([[ -1.3905, -21.6069, -21.6069, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 18
	action: tensor([[-0.0672,  0.0388,  0.0590,  0.0251,  0.0123,  0.0255,  0.0322]],
       dtype=torch.float64)
	q_value: tensor([[-0.0718]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.2571956847882716, distance: 0.9862656378803218 entropy tensor([[ -1.3764, -21.6069, -21.6069, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 19
	action: tensor([[-0.0385,  0.0381,  0.0578,  0.0262,  0.0113,  0.0241,  0.0341]],
       dtype=torch.float64)
	q_value: tensor([[-0.0724]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.28784085354223976, distance: 0.9657066487777425 entropy tensor([[ -1.4179, -21.6069, -21.6069, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 20
	action: tensor([[-0.0418,  0.0386,  0.0585,  0.0255,  0.0119,  0.0251,  0.0329]],
       dtype=torch.float64)
	q_value: tensor([[-0.0718]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.28459900286528905, distance: 0.9679021706862664 entropy tensor([[ -1.3892, -21.6069, -21.6069, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 21
	action: tensor([[0.0229, 0.0386, 0.0586, 0.0256, 0.0116, 0.0249, 0.0329]],
       dtype=torch.float64)
	q_value: tensor([[-0.0720]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.35193575346184836, distance: 0.9212250194205874 entropy tensor([[ -1.3872, -21.6069, -21.6069, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 22
	action: tensor([[-0.0424,  0.0393,  0.0605,  0.0244,  0.0131,  0.0269,  0.0304]],
       dtype=torch.float64)
	q_value: tensor([[-0.0711]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.2843229321565035, distance: 0.9680889076810048 entropy tensor([[ -1.3386, -21.6069, -21.6069, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 23
	action: tensor([[-0.0068,  0.0386,  0.0586,  0.0255,  0.0116,  0.0247,  0.0329]],
       dtype=torch.float64)
	q_value: tensor([[-0.0721]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.3213821126411017, distance: 0.9426909649360583 entropy tensor([[ -1.3879, -21.6069, -21.6069, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 24
	action: tensor([[-0.0995,  0.0391,  0.0596,  0.0247,  0.0123,  0.0259,  0.0315]],
       dtype=torch.float64)
	q_value: tensor([[-0.0713]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.22161238890391166, distance: 1.0096123415936233 entropy tensor([[ -1.3567, -21.6069, -21.6069, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 25
	action: tensor([[0.0140, 0.0374, 0.0572, 0.0270, 0.0105, 0.0230, 0.0353]],
       dtype=torch.float64)
	q_value: tensor([[-0.0733]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.34231800275846225, distance: 0.9280356725807863 entropy tensor([[ -1.4458, -21.6069, -21.6069, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 26
	action: tensor([[-0.0322,  0.0392,  0.0602,  0.0245,  0.0128,  0.0268,  0.0309]],
       dtype=torch.float64)
	q_value: tensor([[-0.0711]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.29518903655316286, distance: 0.9607115650194941 entropy tensor([[ -1.3467, -21.6069, -21.6069, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 27
	action: tensor([[-0.1103,  0.0387,  0.0587,  0.0253,  0.0121,  0.0250,  0.0325]],
       dtype=torch.float64)
	q_value: tensor([[-0.0720]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.20933523144255872, distance: 1.0175432717296342 entropy tensor([[ -1.3845, -21.6069, -21.6069, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 28
	action: tensor([[-0.0360,  0.0372,  0.0567,  0.0274,  0.0104,  0.0227,  0.0358]],
       dtype=torch.float64)
	q_value: tensor([[-0.0736]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.2900673573288388, distance: 0.9641958679943041 entropy tensor([[ -1.4628, -21.6069, -21.6069, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 29
	action: tensor([[-0.0344,  0.0386,  0.0587,  0.0254,  0.0117,  0.0253,  0.0328]],
       dtype=torch.float64)
	q_value: tensor([[-0.0718]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.2925131520809554, distance: 0.9625335555131492 entropy tensor([[ -1.3818, -21.6069, -21.6069, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 30
	action: tensor([[-0.0419,  0.0387,  0.0588,  0.0254,  0.0118,  0.0251,  0.0326]],
       dtype=torch.float64)
	q_value: tensor([[-0.0720]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.2845287522609954, distance: 0.9679496923164358 entropy tensor([[ -1.3809, -21.6069, -21.6069, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 31
	action: tensor([[-0.0181,  0.0386,  0.0586,  0.0255,  0.0116,  0.0249,  0.0329]],
       dtype=torch.float64)
	q_value: tensor([[-0.0720]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.3096159044282669, distance: 0.9508282624791494 entropy tensor([[ -1.3868, -21.6069, -21.6069, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 32
	action: tensor([[-0.0376,  0.0389,  0.0591,  0.0249,  0.0124,  0.0256,  0.0320]],
       dtype=torch.float64)
	q_value: tensor([[-0.0716]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.2892102202241582, distance: 0.9647777533045852 entropy tensor([[ -1.3713, -21.6069, -21.6069, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 33
	action: tensor([[-0.1007,  0.0386,  0.0585,  0.0255,  0.0120,  0.0250,  0.0328]],
       dtype=torch.float64)
	q_value: tensor([[-0.0719]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.22001663687644712, distance: 1.0106467016281224 entropy tensor([[ -1.3887, -21.6069, -21.6069, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 34
	action: tensor([[0.0470, 0.0374, 0.0569, 0.0271, 0.0106, 0.0230, 0.0355]],
       dtype=torch.float64)
	q_value: tensor([[-0.0732]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.37559287330222735, distance: 0.9042543754975196 entropy tensor([[ -1.4525, -21.6069, -21.6069, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 35
	action: tensor([[-0.0314,  0.0395,  0.0613,  0.0239,  0.0136,  0.0278,  0.0296]],
       dtype=torch.float64)
	q_value: tensor([[-0.0707]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.29625754305531815, distance: 0.9599830605263079 entropy tensor([[ -1.3200, -21.6069, -21.6069, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 36
	action: tensor([[-0.0904,  0.0387,  0.0587,  0.0252,  0.0122,  0.0250,  0.0324]],
       dtype=torch.float64)
	q_value: tensor([[-0.0720]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.2315664695149251, distance: 1.003136070088694 entropy tensor([[ -1.3835, -21.6069, -21.6069, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 37
	action: tensor([[0.0153, 0.0376, 0.0574, 0.0268, 0.0107, 0.0234, 0.0350]],
       dtype=torch.float64)
	q_value: tensor([[-0.0729]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.34373944492106456, distance: 0.9270322525567818 entropy tensor([[ -1.4359, -21.6069, -21.6069, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 38
	action: tensor([[-0.0703,  0.0392,  0.0602,  0.0245,  0.0129,  0.0268,  0.0308]],
       dtype=torch.float64)
	q_value: tensor([[-0.0711]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.2541561661725149, distance: 0.9882814530007245 entropy tensor([[ -1.3458, -21.6069, -21.6069, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 39
	action: tensor([[-0.0424,  0.0380,  0.0577,  0.0263,  0.0113,  0.0239,  0.0342]],
       dtype=torch.float64)
	q_value: tensor([[-0.0725]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.2836382028772697, distance: 0.9685519100406086 entropy tensor([[ -1.4210, -21.6069, -21.6069, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 40
	action: tensor([[0.0082, 0.0385, 0.0586, 0.0256, 0.0116, 0.0250, 0.0330]],
       dtype=torch.float64)
	q_value: tensor([[-0.0719]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.3369348799648513, distance: 0.9318259155263315 entropy tensor([[ -1.3871, -21.6069, -21.6069, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 41
	action: tensor([[0.0062, 0.0392, 0.0602, 0.0245, 0.0126, 0.0264, 0.0309]],
       dtype=torch.float64)
	q_value: tensor([[-0.0711]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.3352569873029261, distance: 0.9330041678221891 entropy tensor([[ -1.3445, -21.6069, -21.6069, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 42
	action: tensor([[-0.0260,  0.0392,  0.0601,  0.0245,  0.0126,  0.0262,  0.0309]],
       dtype=torch.float64)
	q_value: tensor([[-0.0712]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.30162682611330305, distance: 0.9563138986212839 entropy tensor([[ -1.3462, -21.6069, -21.6069, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 43
	action: tensor([[-0.0217,  0.0388,  0.0589,  0.0251,  0.0123,  0.0252,  0.0322]],
       dtype=torch.float64)
	q_value: tensor([[-0.0718]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.30589542682656345, distance: 0.9533868252658456 entropy tensor([[ -1.3778, -21.6069, -21.6069, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 44
	action: tensor([[-0.0789,  0.0389,  0.0591,  0.0250,  0.0121,  0.0254,  0.0320]],
       dtype=torch.float64)
	q_value: tensor([[-0.0716]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.24433246126539643, distance: 0.9947686190501264 entropy tensor([[ -1.3688, -21.6069, -21.6069, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 45
	action: tensor([[-0.1569,  0.0378,  0.0575,  0.0265,  0.0111,  0.0237,  0.0346]],
       dtype=torch.float64)
	q_value: tensor([[-0.0726]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.15557677680981208, distance: 1.051566585676669 entropy tensor([[ -1.4302, -21.6069, -21.6069, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 46
	action: tensor([[-0.0347,  0.0357,  0.0559,  0.0285,  0.0095,  0.0210,  0.0375]],
       dtype=torch.float64)
	q_value: tensor([[-0.0748]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.29050789141629996, distance: 0.9638966656221669 entropy tensor([[ -1.5033, -21.6069, -21.6069, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 47
	action: tensor([[-0.1626,  0.0387,  0.0588,  0.0254,  0.0117,  0.0254,  0.0328]],
       dtype=torch.float64)
	q_value: tensor([[-0.0716]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.14944868178056925, distance: 1.055375369348503 entropy tensor([[ -1.3807, -21.6069, -21.6069, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 48
	action: tensor([[-0.2080,  0.0355,  0.0559,  0.0286,  0.0094,  0.0208,  0.0376]],
       dtype=torch.float64)
	q_value: tensor([[-0.0751]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.09395866226711413, distance: 1.0892578527926216 entropy tensor([[ -1.5083, -21.6069, -21.6069, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 0, step: 49
	action: tensor([[-0.0546,  0.0340,  0.0549,  0.0289,  0.0086,  0.0187,  0.0398]],
       dtype=torch.float64)
	q_value: tensor([[-0.0758]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.26764882468183826, distance: 0.9793014210570389 entropy tensor([[ -1.5519, -21.6069, -21.6069, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
LOSS epoch 0 actor 0.14163128423769825 critic 12.009932753094677
epoch: 1, step: 0
	action: tensor([[ 0.0825,  0.1933,  0.0056,  0.0404, -0.0257,  0.0490,  0.1536]],
       dtype=torch.float64)
	q_value: tensor([[0.2452]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.5173107719111715, distance: 0.7950427777848789 entropy tensor([[ -1.3131,  -3.3359, -21.6069, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 1
	action: tensor([[-0.0597,  0.1776,  0.0181,  0.0429, -0.0226,  0.0460,  0.1557]],
       dtype=torch.float64)
	q_value: tensor([[0.2499]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.3666982313236702, distance: 0.9106721258047596 entropy tensor([[ -1.1880,  -2.7654,  -3.7875, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 2
	action: tensor([[ 0.0467,  0.1863,  0.0083,  0.0473, -0.0210,  0.0435,  0.1617]],
       dtype=torch.float64)
	q_value: tensor([[0.2513]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.48100282936695216, distance: 0.824402287945466 entropy tensor([[ -1.3053,  -3.0541,  -3.4582, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 3
	action: tensor([[-0.0162,  0.2091,  0.0177,  0.0430, -0.0220,  0.0463,  0.1583]],
       dtype=torch.float64)
	q_value: tensor([[0.2499]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.824402287945466 entropy tensor([[ -1.2292,  -2.8212,  -3.6902, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 4
	action: tensor([[ 0.2289,  0.2390, -0.1052,  0.4332, -0.0563, -0.0803,  0.3861]],
       dtype=torch.float64)
	q_value: tensor([[0.5491]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.7241921018373865, distance: 0.6009800321610526 entropy tensor([[-21.6069,  -0.3694,  -1.0183, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 5
	action: tensor([[-0.1453,  0.1427,  0.0262,  0.0546, -0.0209,  0.0442,  0.1893]],
       dtype=torch.float64)
	q_value: tensor([[0.2549]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.2670778431835298, distance: 0.9796831054629606 entropy tensor([[ -1.0109,  -1.9942,  -3.0488, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 6
	action: tensor([[ 0.0024,  0.1896, -0.0121,  0.0487, -0.0183,  0.0420,  0.1653]],
       dtype=torch.float64)
	q_value: tensor([[0.2526]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.44821879481892446, distance: 0.8500415313038395 entropy tensor([[ -1.4153,  -3.0606,  -3.2670, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 7
	action: tensor([[-0.0373,  0.1819,  0.0176,  0.0467, -0.0207,  0.0446,  0.1611]],
       dtype=torch.float64)
	q_value: tensor([[0.2507]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.40423285219130545, distance: 0.8832730282484801 entropy tensor([[ -1.2626,  -2.9046,  -3.5830, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 8
	action: tensor([[ 0.0273,  0.1941,  0.0247,  0.0463, -0.0210,  0.0440,  0.1615]],
       dtype=torch.float64)
	q_value: tensor([[0.2509]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.47622215614329, distance: 0.8281905201103528 entropy tensor([[ -1.2932,  -2.9694,  -3.4371, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 9
	action: tensor([[-0.0043,  0.2001,  0.0182,  0.0430, -0.0221,  0.0452,  0.1588]],
       dtype=torch.float64)
	q_value: tensor([[0.2498]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.4470571882790806, distance: 0.8509358121025469 entropy tensor([[ -1.2388,  -2.8311,  -3.5710, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 10
	action: tensor([[-0.0752,  0.1890,  0.0113,  0.0453, -0.0212,  0.0435,  0.1604]],
       dtype=torch.float64)
	q_value: tensor([[0.2502]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.36759570221318183, distance: 0.9100266266728513 entropy tensor([[ -1.2575,  -2.8798,  -3.5587, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 11
	action: tensor([[ 0.0413,  0.1985,  0.0166,  0.0490, -0.0200,  0.0422,  0.1634]],
       dtype=torch.float64)
	q_value: tensor([[0.2517]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.4925862884864851, distance: 0.8151504875141806 entropy tensor([[ -1.3154,  -3.0810,  -3.3959, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 12
	action: tensor([[ 0.1378,  0.1934,  0.0245,  0.0429, -0.0221,  0.0455,  0.1586]],
       dtype=torch.float64)
	q_value: tensor([[0.2496]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.5767015096403015, distance: 0.7445263162810356 entropy tensor([[ -1.2217,  -2.8119,  -3.5725, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 13
	action: tensor([[ 0.0196,  0.1933,  0.0304,  0.0410, -0.0236,  0.0457,  0.1520]],
       dtype=torch.float64)
	q_value: tensor([[0.2498]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.4666150989025033, distance: 0.8357512832375384 entropy tensor([[ -1.1583,  -2.6261,  -3.4981, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 14
	action: tensor([[-0.0404,  0.2108,  0.0273,  0.0429, -0.0229,  0.0452,  0.1581]],
       dtype=torch.float64)
	q_value: tensor([[0.2496]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.41673719434190826, distance: 0.8739545234942265 entropy tensor([[ -1.2324,  -2.8364,  -3.5817, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 15
	action: tensor([[ 0.0601,  0.1946,  0.0205,  0.0467, -0.0206,  0.0421,  0.1617]],
       dtype=torch.float64)
	q_value: tensor([[0.2508]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.5075809348457887, distance: 0.8030158600173604 entropy tensor([[ -1.2682,  -2.9424,  -3.3696, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 16
	action: tensor([[-0.0374,  0.2135,  0.0087,  0.0421, -0.0222,  0.0459,  0.1574]],
       dtype=torch.float64)
	q_value: tensor([[0.2493]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.42038586039098325, distance: 0.8712166746624503 entropy tensor([[ -1.2133,  -2.7754,  -3.6037, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 17
	action: tensor([[ 0.0089,  0.1714,  0.0086,  0.0482, -0.0196,  0.0414,  0.1624]],
       dtype=torch.float64)
	q_value: tensor([[0.2512]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.44464146698392626, distance: 0.852792589543517 entropy tensor([[ -1.2752,  -2.9797,  -3.4659, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 18
	action: tensor([[ 0.0278,  0.2121,  0.0176,  0.0440, -0.0224,  0.0465,  0.1593]],
       dtype=torch.float64)
	q_value: tensor([[0.2501]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.48598369790223894, distance: 0.8204368145197006 entropy tensor([[ -1.2630,  -2.8703,  -3.5950, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 19
	action: tensor([[ 0.1206,  0.2056,  0.0146,  0.0441, -0.0215,  0.0439,  0.1592]],
       dtype=torch.float64)
	q_value: tensor([[0.2500]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.5680540069003086, distance: 0.7520927775471248 entropy tensor([[ -1.2207,  -2.8336,  -3.5805, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 20
	action: tensor([[-0.0002,  0.2083,  0.0233,  0.0420, -0.0223,  0.0445,  0.1540]],
       dtype=torch.float64)
	q_value: tensor([[0.2495]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.455346759474447, distance: 0.844533223610177 entropy tensor([[ -1.1657,  -2.6630,  -3.5939, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 21
	action: tensor([[-0.0300,  0.1763,  0.0167,  0.0449, -0.0216,  0.0433,  0.1600]],
       dtype=torch.float64)
	q_value: tensor([[0.2499]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.407429020686359, distance: 0.8809005527349429 entropy tensor([[ -1.2421,  -2.8674,  -3.5338, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 22
	action: tensor([[ 0.1146,  0.1834,  0.0042,  0.0459, -0.0216,  0.0447,  0.1608]],
       dtype=torch.float64)
	q_value: tensor([[0.2505]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.5510489289825555, distance: 0.7667542629883611 entropy tensor([[ -1.2883,  -2.9659,  -3.5041, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 23
	action: tensor([[ 0.1256,  0.1752,  0.0267,  0.0420, -0.0231,  0.0470,  0.1541]],
       dtype=torch.float64)
	q_value: tensor([[0.2501]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.5559439363958381, distance: 0.7625627654433867 entropy tensor([[ -1.1824,  -2.7122,  -3.6572, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 24
	action: tensor([[-0.0092,  0.2166,  0.0287,  0.0406, -0.0243,  0.0474,  0.1522]],
       dtype=torch.float64)
	q_value: tensor([[0.2500]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.45118203149555114, distance: 0.8477559649287468 entropy tensor([[ -1.1737,  -2.6588,  -3.5625, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 25
	action: tensor([[-0.0077,  0.1986,  0.0183,  0.0455, -0.0208,  0.0420,  0.1605]],
       dtype=torch.float64)
	q_value: tensor([[0.2503]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.44311042698367364, distance: 0.8539672909755005 entropy tensor([[ -1.2453,  -2.8759,  -3.5273, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 26
	action: tensor([[ 0.1213,  0.1698,  0.0159,  0.0454, -0.0212,  0.0436,  0.1607]],
       dtype=torch.float64)
	q_value: tensor([[0.2500]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.5494462479067557, distance: 0.7681216367073505 entropy tensor([[ -1.2627,  -2.8826,  -3.5245, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 27
	action: tensor([[ 0.0260,  0.1968,  0.0210,  0.0410, -0.0237,  0.0478,  0.1532]],
       dtype=torch.float64)
	q_value: tensor([[0.2499]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.47496209841535153, distance: 0.8291861149036804 entropy tensor([[ -1.1913,  -2.6884,  -3.6297, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 28
	action: tensor([[-0.0855,  0.1710,  0.0179,  0.0436, -0.0221,  0.0447,  0.1586]],
       dtype=torch.float64)
	q_value: tensor([[0.2500]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.3455592865442041, distance: 0.9257460080466663 entropy tensor([[ -1.2332,  -2.8463,  -3.6571, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 29
	action: tensor([[-0.0140,  0.1886,  0.0087,  0.0481, -0.0209,  0.0432,  0.1627]],
       dtype=torch.float64)
	q_value: tensor([[0.2515]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.43191317523287054, distance: 0.8625098268431525 entropy tensor([[ -1.3262,  -3.1006,  -3.4186, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 30
	action: tensor([[-0.0044,  0.1923,  0.0233,  0.0459, -0.0213,  0.0444,  0.1610]],
       dtype=torch.float64)
	q_value: tensor([[0.2504]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.4436751689163658, distance: 0.8535341768613135 entropy tensor([[ -1.2675,  -2.8966,  -3.5083, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 31
	action: tensor([[-0.0270,  0.1941, -0.0008,  0.0445, -0.0216,  0.0444,  0.1603]],
       dtype=torch.float64)
	q_value: tensor([[0.2502]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.4203443479080988, distance: 0.871247872762428 entropy tensor([[ -1.2654,  -2.8685,  -3.4977, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 32
	action: tensor([[ 0.0441,  0.1883,  0.0290,  0.0477, -0.0203,  0.0430,  0.1619]],
       dtype=torch.float64)
	q_value: tensor([[0.2510]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.48971245404099484, distance: 0.8174556084686184 entropy tensor([[ -1.2785,  -2.9619,  -3.5381, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 33
	action: tensor([[-0.0575,  0.1818,  0.0237,  0.0416, -0.0230,  0.0467,  0.1576]],
       dtype=torch.float64)
	q_value: tensor([[0.2495]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.3817469198104142, distance: 0.8997872556399208 entropy tensor([[ -1.2254,  -2.7855,  -3.5188, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 34
	action: tensor([[-0.1326,  0.1803,  0.0070,  0.0468, -0.0214,  0.0438,  0.1614]],
       dtype=torch.float64)
	q_value: tensor([[0.2512]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.30037931091655234, distance: 0.9571676569451639 entropy tensor([[ -1.2966,  -3.0284,  -3.4034, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 35
	action: tensor([[ 0.1394,  0.2011,  0.0122,  0.0512, -0.0191,  0.0401,  0.1658]],
       dtype=torch.float64)
	q_value: tensor([[0.2524]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.5839253973971847, distance: 0.7381460444204426 entropy tensor([[ -1.3576,  -3.1739,  -3.4121, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 36
	action: tensor([[ 0.0652,  0.1731,  0.0202,  0.0415, -0.0224,  0.0449,  0.1535]],
       dtype=torch.float64)
	q_value: tensor([[0.2494]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.49881673869439, distance: 0.8101304800087411 entropy tensor([[ -1.1632,  -2.6260,  -3.4985, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 37
	action: tensor([[ 0.0322,  0.2206,  0.0215,  0.0414, -0.0235,  0.0479,  0.1557]],
       dtype=torch.float64)
	q_value: tensor([[0.2496]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.4942280457673117, distance: 0.8138306931469612 entropy tensor([[ -1.2144,  -2.7731,  -3.6973, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 38
	action: tensor([[ 0.0235,  0.2064,  0.0193,  0.0442, -0.0210,  0.0428,  0.1591]],
       dtype=torch.float64)
	q_value: tensor([[0.2500]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.47814498698048347, distance: 0.8266689451712006 entropy tensor([[ -1.2148,  -2.8255,  -3.6015, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 39
	action: tensor([[ 0.0067,  0.1955,  0.0213,  0.0441, -0.0215,  0.0440,  0.1593]],
       dtype=torch.float64)
	q_value: tensor([[0.2497]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.45572697221744385, distance: 0.8442383953340157 entropy tensor([[ -1.2344,  -2.8492,  -3.6015, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 40
	action: tensor([[ 0.0603,  0.2015,  0.0121,  0.0443, -0.0217,  0.0444,  0.1597]],
       dtype=torch.float64)
	q_value: tensor([[0.2500]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.510815266023223, distance: 0.8003743111052106 entropy tensor([[ -1.2540,  -2.8660,  -3.5570, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 41
	action: tensor([[-0.0859,  0.1844,  0.0214,  0.0429, -0.0217,  0.0451,  0.1577]],
       dtype=torch.float64)
	q_value: tensor([[0.2497]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.35316715917525665, distance: 0.9203493800466169 entropy tensor([[ -1.2083,  -2.7978,  -3.6696, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 42
	action: tensor([[-0.0191,  0.1958,  0.0210,  0.0485, -0.0206,  0.0424,  0.1630]],
       dtype=torch.float64)
	q_value: tensor([[0.2515]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.43139377729835837, distance: 0.8629040302045188 entropy tensor([[ -1.3172,  -3.0987,  -3.3773, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 43
	action: tensor([[-0.0227,  0.1773,  0.0279,  0.0455, -0.0213,  0.0438,  0.1611]],
       dtype=torch.float64)
	q_value: tensor([[0.2501]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.4163334181935754, distance: 0.8742569780108667 entropy tensor([[ -1.2654,  -2.8873,  -3.4367, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 44
	action: tensor([[-0.0801,  0.1823,  0.0301,  0.0447, -0.0219,  0.0450,  0.1602]],
       dtype=torch.float64)
	q_value: tensor([[0.2503]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.3591977487221245, distance: 0.9160490057761251 entropy tensor([[ -1.2844,  -2.9246,  -3.4664, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 45
	action: tensor([[-0.0362,  0.1884,  0.0240,  0.0475, -0.0208,  0.0429,  0.1625]],
       dtype=torch.float64)
	q_value: tensor([[0.2512]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.40937928421913916, distance: 0.8794497526019012 entropy tensor([[ -1.3132,  -3.0589,  -3.3334, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 46
	action: tensor([[-0.0100,  0.1820,  0.0244,  0.0459, -0.0213,  0.0438,  0.1612]],
       dtype=torch.float64)
	q_value: tensor([[0.2504]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.43208900428685937, distance: 0.8623763384248535 entropy tensor([[ -1.2801,  -2.9398,  -3.4013, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 47
	action: tensor([[-0.0134,  0.1875,  0.0063,  0.0444, -0.0219,  0.0449,  0.1600]],
       dtype=torch.float64)
	q_value: tensor([[0.2501]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.43066163011192127, distance: 0.8634593965486718 entropy tensor([[ -1.2723,  -2.8875,  -3.5172, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 48
	action: tensor([[ 0.0061,  0.1957,  0.0211,  0.0462, -0.0211,  0.0441,  0.1608]],
       dtype=torch.float64)
	q_value: tensor([[0.2506]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.456082615012563, distance: 0.8439625261003741 entropy tensor([[ -1.2722,  -2.9186,  -3.5717, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 1, step: 49
	action: tensor([[-0.0154,  0.1877,  0.0212,  0.0443, -0.0216,  0.0444,  0.1600]],
       dtype=torch.float64)
	q_value: tensor([[0.2501]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.42913839803678155, distance: 0.8646136929868456 entropy tensor([[ -1.2553,  -2.8597,  -3.5298, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
LOSS epoch 1 actor 25.329335389013686 critic 192.64128125465106
epoch: 2, step: 0
	action: tensor([[-0.1458,  0.2470, -0.0071,  0.1881,  0.0447, -0.0060,  0.1042]],
       dtype=torch.float64)
	q_value: tensor([[0.5196]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.36692386833296553, distance: 0.9105098811601776 entropy tensor([[ -1.1720,  -2.0299,  -2.3182, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 1
	action: tensor([[ 0.0621,  0.2658, -0.0296,  0.1993,  0.0537, -0.0213,  0.1223]],
       dtype=torch.float64)
	q_value: tensor([[0.5080]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.5806244001255487, distance: 0.7410683620901296 entropy tensor([[ -1.1648,  -1.8417,  -2.0152, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 2
	action: tensor([[-0.1312,  0.2555, -0.0621,  0.1893,  0.0592, -0.0186,  0.1178]],
       dtype=torch.float64)
	q_value: tensor([[0.5108]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.3783258423693816, distance: 0.9022732887799304 entropy tensor([[ -0.9850,  -1.7341,  -2.1955, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 3
	action: tensor([[-0.0792,  0.2774, -0.0746,  0.2027,  0.0551, -0.0222,  0.1267]],
       dtype=torch.float64)
	q_value: tensor([[0.5119]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.4451381243167548, distance: 0.8524111778928218 entropy tensor([[ -1.1768,  -1.8277,  -2.0306, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 4
	action: tensor([[ 0.1430,  0.2167, -0.0123,  0.2021,  0.0578, -0.0225,  0.1270]],
       dtype=torch.float64)
	q_value: tensor([[0.5142]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.6323214688825753, distance: 0.6938903204601821 entropy tensor([[ -1.1040,  -1.7898,  -2.0463, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 5
	action: tensor([[-0.0645,  0.2276, -0.0181,  0.1802,  0.0576, -0.0151,  0.1143]],
       dtype=torch.float64)
	q_value: tensor([[0.5069]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.4367933931698865, distance: 0.8587970889573318 entropy tensor([[ -0.9598,  -1.7057,  -2.2461, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 6
	action: tensor([[-0.1798,  0.2760,  0.0108,  0.1939,  0.0549, -0.0170,  0.1178]],
       dtype=torch.float64)
	q_value: tensor([[0.5060]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.34691696012163775, distance: 0.9247852537700472 entropy tensor([[ -1.1290,  -1.8241,  -2.1393, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 7
	action: tensor([[ 0.0446,  0.3234, -0.0362,  0.2036,  0.0539, -0.0237,  0.1260]],
       dtype=torch.float64)
	q_value: tensor([[0.5106]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.5878491879405334, distance: 0.7346572574350609 entropy tensor([[ -1.1987,  -1.8015,  -1.9254, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 8
	action: tensor([[-0.0544,  0.2977,  0.0166,  0.1954,  0.0603, -0.0224,  0.1219]],
       dtype=torch.float64)
	q_value: tensor([[0.5174]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.4876000015542866, distance: 0.8191458835669333 entropy tensor([[ -0.9653,  -1.7192,  -2.1014, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 9
	action: tensor([[-0.2865,  0.2653,  0.0073,  0.1967,  0.0578, -0.0211,  0.1199]],
       dtype=torch.float64)
	q_value: tensor([[0.5113]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.22126027895789047, distance: 1.009840668938775 entropy tensor([[ -1.0922,  -1.7498,  -2.0392, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 10
	action: tensor([[-0.0013,  0.2686,  0.0171,  0.2113,  0.0511, -0.0286,  0.1334]],
       dtype=torch.float64)
	q_value: tensor([[0.5124]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.5304940410672133, distance: 0.784110460432604 entropy tensor([[ -1.2852,  -1.8409,  -1.8366, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 11
	action: tensor([[ 0.1038,  0.2951,  0.0119,  0.1914,  0.0588, -0.0184,  0.1184]],
       dtype=torch.float64)
	q_value: tensor([[0.5087]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.6315462749983785, distance: 0.6946214160586597 entropy tensor([[ -1.0298,  -1.7205,  -2.0996, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 12
	action: tensor([[ 0.1348,  0.2574, -0.0133,  0.1858,  0.0593, -0.0193,  0.1149]],
       dtype=torch.float64)
	q_value: tensor([[0.5126]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.6386683942035614, distance: 0.6878752265080741 entropy tensor([[ -0.9549,  -1.6746,  -2.2010, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 13
	action: tensor([[ 0.0073,  0.2727, -0.0032,  0.1831,  0.0584, -0.0174,  0.1136]],
       dtype=torch.float64)
	q_value: tensor([[0.5097]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.5313551244021302, distance: 0.7833910933010241 entropy tensor([[ -0.9547,  -1.7053,  -2.2733, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 14
	action: tensor([[ 0.0744,  0.2968, -0.0239,  0.1918,  0.0579, -0.0184,  0.1161]],
       dtype=torch.float64)
	q_value: tensor([[0.5095]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.6032608643528539, distance: 0.7207907666437582 entropy tensor([[ -1.0517,  -1.7587,  -2.1634, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 15
	action: tensor([[-0.0250,  0.2607,  0.0028,  0.1902,  0.0599, -0.0207,  0.1173]],
       dtype=torch.float64)
	q_value: tensor([[0.5132]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.49708119953420693, distance: 0.8115319614322685 entropy tensor([[ -0.9723,  -1.7149,  -2.1864, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 16
	action: tensor([[-0.0639,  0.2143, -0.0171,  0.1929,  0.0574, -0.0181,  0.1171]],
       dtype=torch.float64)
	q_value: tensor([[0.5075]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.43463606645833963, distance: 0.8604403004355392 entropy tensor([[ -1.0813,  -1.7689,  -2.1385, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 17
	action: tensor([[ 0.1796,  0.2515, -0.0501,  0.1926,  0.0557, -0.0168,  0.1183]],
       dtype=torch.float64)
	q_value: tensor([[0.5036]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.6718205914904254, distance: 0.6555598651637624 entropy tensor([[ -1.1203,  -1.8032,  -2.1310, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 18
	action: tensor([[-0.0584,  0.3667,  0.0037,  0.1840,  0.0581, -0.0176,  0.1154]],
       dtype=torch.float64)
	q_value: tensor([[0.5124]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.5082872358714932, distance: 0.8024397506706102 entropy tensor([[ -0.9100,  -1.6973,  -2.3348, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 19
	action: tensor([[-0.0664,  0.3598,  0.0106,  0.2034,  0.0601, -0.0258,  0.1236]],
       dtype=torch.float64)
	q_value: tensor([[0.5200]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.5027405637184836, distance: 0.8069529431722133 entropy tensor([[ -1.0730,  -1.7363,  -1.9522, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 20
	action: tensor([[ 0.0338,  0.2909,  0.0200,  0.2028,  0.0606, -0.0259,  0.1249]],
       dtype=torch.float64)
	q_value: tensor([[0.5184]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.5708751782903855, distance: 0.7496326800769214 entropy tensor([[ -1.0641,  -1.7124,  -1.9288, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 21
	action: tensor([[-0.1122,  0.2126, -0.0261,  0.1900,  0.0595, -0.0193,  0.1171]],
       dtype=torch.float64)
	q_value: tensor([[0.5109]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.3808718281152994, distance: 0.9004238217579306 entropy tensor([[ -1.0058,  -1.7015,  -2.1151, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 22
	action: tensor([[-0.0837,  0.3253, -0.0311,  0.1959,  0.0545, -0.0185,  0.1212]],
       dtype=torch.float64)
	q_value: tensor([[0.5051]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.46580150618987615, distance: 0.8363884423106461 entropy tensor([[ -1.1687,  -1.8288,  -2.0677, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 23
	action: tensor([[-0.0649,  0.2821, -0.0065,  0.2032,  0.0581, -0.0247,  0.1259]],
       dtype=torch.float64)
	q_value: tensor([[0.5166]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.4696988830250166, distance: 0.8333318179561396 entropy tensor([[ -1.0942,  -1.7677,  -1.9962, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 24
	action: tensor([[ 0.0620,  0.2552, -0.0060,  0.1975,  0.0578, -0.0209,  0.1218]],
       dtype=torch.float64)
	q_value: tensor([[0.5106]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.5780036319520482, distance: 0.7433803034209098 entropy tensor([[ -1.0944,  -1.7479,  -2.0450, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 25
	action: tensor([[-0.0804,  0.1927, -0.0318,  0.1871,  0.0587, -0.0174,  0.1160]],
       dtype=torch.float64)
	q_value: tensor([[0.5086]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.40265824062481215, distance: 0.8844395025713734 entropy tensor([[ -0.9966,  -1.7237,  -2.2033, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 26
	action: tensor([[-0.0352,  0.1922, -0.0202,  0.1926,  0.0544, -0.0158,  0.1187]],
       dtype=torch.float64)
	q_value: tensor([[0.5033]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.45246627832761854, distance: 0.8467634996367024 entropy tensor([[ -1.1499,  -1.8335,  -2.1327, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 27
	action: tensor([[-0.1315,  0.2525, -0.0042,  0.1893,  0.0557, -0.0145,  0.1163]],
       dtype=torch.float64)
	q_value: tensor([[0.5022]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.3841518803731069, distance: 0.8980354961722072 entropy tensor([[ -1.0990,  -1.8099,  -2.1677, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 28
	action: tensor([[-0.0521,  0.2533,  0.0369,  0.1991,  0.0546, -0.0210,  0.1220]],
       dtype=torch.float64)
	q_value: tensor([[0.5086]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.4734718754586037, distance: 0.8303620267907237 entropy tensor([[ -1.1718,  -1.8146,  -2.0154, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 29
	action: tensor([[-0.0755,  0.2294, -0.0102,  0.1923,  0.0563, -0.0175,  0.1168]],
       dtype=torch.float64)
	q_value: tensor([[0.5050]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.431036846864415, distance: 0.8631748225193318 entropy tensor([[ -1.0909,  -1.7709,  -2.0960, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 30
	action: tensor([[-0.1754,  0.3505,  0.0020,  0.1941,  0.0556, -0.0177,  0.1189]],
       dtype=torch.float64)
	q_value: tensor([[0.5050]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.3860151198337821, distance: 0.8966759704704972 entropy tensor([[ -1.1212,  -1.8023,  -2.1034, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 31
	action: tensor([[ 0.0081,  0.1449, -0.0067,  0.2107,  0.0570, -0.0280,  0.1297]],
       dtype=torch.float64)
	q_value: tensor([[0.5199]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.476448443571584, distance: 0.8280115994562378 entropy tensor([[ -1.1790,  -1.7604,  -1.8997, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 32
	action: tensor([[-0.0462,  0.2317, -0.0304,  0.1828,  0.0550, -0.0116,  0.1149]],
       dtype=torch.float64)
	q_value: tensor([[0.4970]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.4576834238531605, distance: 0.842719673575427 entropy tensor([[ -1.0786,  -1.7673,  -2.1779, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 33
	action: tensor([[-0.0365,  0.2550, -0.0013,  0.1936,  0.0560, -0.0173,  0.1181]],
       dtype=torch.float64)
	q_value: tensor([[0.5067]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.48429149603793076, distance: 0.8217861918936077 entropy tensor([[ -1.1039,  -1.8160,  -2.1664, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 34
	action: tensor([[-0.0743,  0.2943,  0.0291,  0.1935,  0.0571, -0.0180,  0.1178]],
       dtype=torch.float64)
	q_value: tensor([[0.5076]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.4679095777215697, distance: 0.8347365192967843 entropy tensor([[ -1.0807,  -1.7747,  -2.1287, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 35
	action: tensor([[-0.0584,  0.2685, -0.0591,  0.1972,  0.0570, -0.0210,  0.1195]],
       dtype=torch.float64)
	q_value: tensor([[0.5105]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.46257515563317475, distance: 0.8389103701710613 entropy tensor([[ -1.1092,  -1.7604,  -2.0260, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 36
	action: tensor([[-0.0201,  0.2503, -0.0349,  0.1990,  0.0577, -0.0211,  0.1236]],
       dtype=torch.float64)
	q_value: tensor([[0.5113]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.49559691414060525, distance: 0.8127286333290824 entropy tensor([[ -1.0864,  -1.7905,  -2.0943, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 37
	action: tensor([[ 0.0154,  0.2779,  0.0042,  0.1939,  0.0584, -0.0186,  0.1197]],
       dtype=torch.float64)
	q_value: tensor([[0.5086]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.5451347808414179, distance: 0.771788065074072 entropy tensor([[ -1.0660,  -1.7646,  -2.1462, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 38
	action: tensor([[-0.2157,  0.3303, -0.0294,  0.1912,  0.0589, -0.0189,  0.1169]],
       dtype=torch.float64)
	q_value: tensor([[0.5102]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.3283787154528848, distance: 0.9378187658973155 entropy tensor([[ -1.0343,  -1.7336,  -2.1411, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 39
	action: tensor([[-0.1146,  0.2969, -0.0350,  0.2146,  0.0549, -0.0291,  0.1331]],
       dtype=torch.float64)
	q_value: tensor([[0.5207]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.4250824492809042, distance: 0.8676797793038736 entropy tensor([[ -1.2208,  -1.7926,  -1.8978, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 40
	action: tensor([[ 0.0329,  0.2081, -0.0042,  0.2034,  0.0578, -0.0240,  0.1283]],
       dtype=torch.float64)
	q_value: tensor([[0.5138]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.5310110898824265, distance: 0.7836785861851773 entropy tensor([[ -1.1209,  -1.7514,  -1.9615, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 41
	action: tensor([[-0.0023,  0.2158, -0.0161,  0.1856,  0.0572, -0.0149,  0.1153]],
       dtype=torch.float64)
	q_value: tensor([[0.5039]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.49530386418980443, distance: 0.8129646900756614 entropy tensor([[ -1.0391,  -1.7503,  -2.2008, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 42
	action: tensor([[-0.0974,  0.2850, -0.0378,  0.1891,  0.0563, -0.0152,  0.1152]],
       dtype=torch.float64)
	q_value: tensor([[0.5047]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.43152241846167116, distance: 0.8628064132132008 entropy tensor([[ -1.0733,  -1.7927,  -2.2119, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 43
	action: tensor([[ 0.0786,  0.1843, -0.0207,  0.2015,  0.0566, -0.0225,  0.1241]],
       dtype=torch.float64)
	q_value: tensor([[0.5129]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.5597563902458966, distance: 0.7592822085994421 entropy tensor([[ -1.1300,  -1.8002,  -2.0415, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 44
	action: tensor([[-0.0519,  0.2803, -0.0069,  0.1818,  0.0565, -0.0133,  0.1143]],
       dtype=torch.float64)
	q_value: tensor([[0.5028]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.4770348811237667, distance: 0.8275477357342585 entropy tensor([[ -1.0104,  -1.7560,  -2.2211, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 45
	action: tensor([[-0.0507,  0.2352,  0.0015,  0.1966,  0.0567, -0.0199,  0.1188]],
       dtype=torch.float64)
	q_value: tensor([[0.5111]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.4615106743116847, distance: 0.839740777130377 entropy tensor([[ -1.0968,  -1.7906,  -2.0945, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 46
	action: tensor([[ 0.1132,  0.3432, -0.0254,  0.1926,  0.0563, -0.0172,  0.1176]],
       dtype=torch.float64)
	q_value: tensor([[0.5045]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.6536663683563982, distance: 0.6734479367877 entropy tensor([[ -1.0963,  -1.7843,  -2.1230, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 47
	action: tensor([[-0.0431,  0.2147, -0.0262,  0.1913,  0.0610, -0.0245,  0.1185]],
       dtype=torch.float64)
	q_value: tensor([[0.5192]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.4532415021314796, distance: 0.8461638436600094 entropy tensor([[ -0.9279,  -1.6506,  -2.1765, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 48
	action: tensor([[ 0.0316,  0.2113, -0.0375,  0.1920,  0.0560, -0.0161,  0.1180]],
       dtype=torch.float64)
	q_value: tensor([[0.5041]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.5254241528978945, distance: 0.7883326412354276 entropy tensor([[ -1.1080,  -1.7953,  -2.1411, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 2, step: 49
	action: tensor([[-0.0113,  0.2215, -0.0162,  0.1878,  0.0571, -0.0153,  0.1161]],
       dtype=torch.float64)
	q_value: tensor([[0.5056]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.489952793884168, distance: 0.8172630794786256 entropy tensor([[ -1.0361,  -1.7805,  -2.2282, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
LOSS epoch 2 actor 0.08203951872690243 critic 35.79500438193378
epoch: 3, step: 0
	action: tensor([[-0.0690,  0.2641, -0.1214,  0.1800,  0.0974,  0.0485,  0.0338]],
       dtype=torch.float64)
	q_value: tensor([[0.8142]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.4484979048727278, distance: 0.8498265138785744 entropy tensor([[ -0.9201,  -1.3869,  -1.9288, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 1
	action: tensor([[-0.0007,  0.3894, -0.0960,  0.1872,  0.0961,  0.0380,  0.0424]],
       dtype=torch.float64)
	q_value: tensor([[0.8214]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.5672880855979905, distance: 0.7527592830322036 entropy tensor([[ -0.9209,  -1.4438,  -1.8919, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 2
	action: tensor([[-0.1922,  0.1957, -0.0154,  0.1895,  0.1037,  0.0325,  0.0484]],
       dtype=torch.float64)
	q_value: tensor([[0.8336]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.2961187466175309, distance: 0.9600777227559035 entropy tensor([[ -0.8125,  -1.3347,  -1.8052, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 3
	action: tensor([[ 0.1120,  0.1882, -0.1608,  0.1827,  0.0878,  0.0403,  0.0340]],
       dtype=torch.float64)
	q_value: tensor([[0.8097]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.5809703154151763, distance: 0.7407626698182075 entropy tensor([[ -1.0512,  -1.4973,  -1.7603, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 4
	action: tensor([[-0.0886,  0.2282, -0.0292,  0.1835,  0.1009,  0.0432,  0.0448]],
       dtype=torch.float64)
	q_value: tensor([[0.8111]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.42332650414986317, distance: 0.8690038269787151 entropy tensor([[ -0.8205,  -1.3915,  -2.2346, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 5
	action: tensor([[-0.0318,  0.2303,  0.0330,  0.1813,  0.0960,  0.0425,  0.0339]],
       dtype=torch.float64)
	q_value: tensor([[0.8111]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.4885176660636974, distance: 0.8184120448008858 entropy tensor([[ -0.9723,  -1.4415,  -1.8890, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 6
	action: tensor([[8.3023e-02, 3.1859e-01, 1.8051e-04, 1.7673e-01, 9.9754e-02, 4.4878e-02,
         2.8449e-02]], dtype=torch.float64)
	q_value: tensor([[0.8036]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.6270688303474659, distance: 0.6988291864640543 entropy tensor([[ -0.9192,  -1.4081,  -1.9814, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 7
	action: tensor([[-0.0386,  0.2165, -0.0459,  0.1768,  0.1068,  0.0407,  0.0348]],
       dtype=torch.float64)
	q_value: tensor([[0.8142]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.4639455410854275, distance: 0.8378401141354365 entropy tensor([[ -0.7950,  -1.3034,  -1.9926, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 8
	action: tensor([[-0.0211,  0.3414, -0.0558,  0.1797,  0.0980,  0.0439,  0.0330]],
       dtype=torch.float64)
	q_value: tensor([[0.8080]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.5356486635500091, distance: 0.77979427638619 entropy tensor([[ -0.9403,  -1.4331,  -1.9904, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 9
	action: tensor([[0.0774, 0.3633, 0.0121, 0.1853, 0.1019, 0.0367, 0.0407]],
       dtype=torch.float64)
	q_value: tensor([[0.8238]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.6395760889206683, distance: 0.6870106836389592 entropy tensor([[ -0.8545,  -1.3700,  -1.8546, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 10
	action: tensor([[ 0.0459,  0.2608, -0.0481,  0.1777,  0.1094,  0.0380,  0.0374]],
       dtype=torch.float64)
	q_value: tensor([[0.8202]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.5652758337695787, distance: 0.7545075412446416 entropy tensor([[ -0.7835,  -1.2657,  -1.8830, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 11
	action: tensor([[-0.1034,  0.3418, -0.0403,  0.1786,  0.1037,  0.0427,  0.0349]],
       dtype=torch.float64)
	q_value: tensor([[0.8115]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.45704349591670723, distance: 0.8432167272273545 entropy tensor([[ -0.8544,  -1.3690,  -2.0487, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 12
	action: tensor([[-0.0395,  0.2662, -0.0139,  0.1893,  0.0959,  0.0363,  0.0412]],
       dtype=torch.float64)
	q_value: tensor([[0.8262]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.4933161595371798, distance: 0.8145640145522468 entropy tensor([[ -0.9197,  -1.3918,  -1.7893, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 13
	action: tensor([[ 0.0031,  0.2552, -0.0242,  0.1805,  0.1001,  0.0423,  0.0339]],
       dtype=torch.float64)
	q_value: tensor([[0.8114]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.5269473431676017, distance: 0.7870665152134718 entropy tensor([[ -0.9004,  -1.4134,  -1.9128, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 14
	action: tensor([[ 0.0708,  0.3348, -0.0327,  0.1792,  0.1018,  0.0435,  0.0329]],
       dtype=torch.float64)
	q_value: tensor([[0.8093]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.6191646776808284, distance: 0.7061960830150688 entropy tensor([[ -0.8804,  -1.4022,  -2.0075, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 15
	action: tensor([[-0.1911,  0.3508,  0.0006,  0.1797,  0.1071,  0.0383,  0.0377]],
       dtype=torch.float64)
	q_value: tensor([[0.8186]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.3753306294727048, distance: 0.9044442437956809 entropy tensor([[ -0.7953,  -1.3043,  -1.9672, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 16
	action: tensor([[ 0.0364,  0.3395, -0.0970,  0.1952,  0.0902,  0.0331,  0.0433]],
       dtype=torch.float64)
	q_value: tensor([[0.8276]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.584577284044403, distance: 0.7375675710282986 entropy tensor([[ -0.9725,  -1.3948,  -1.7333, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 17
	action: tensor([[ 0.0666,  0.2156, -0.0731,  0.1863,  0.1053,  0.0357,  0.0450]],
       dtype=torch.float64)
	q_value: tensor([[0.8251]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.5632602948065658, distance: 0.7562546039395809 entropy tensor([[ -0.7940,  -1.3479,  -1.9205, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 18
	action: tensor([[-0.0343,  0.3559, -0.0426,  0.1782,  0.1023,  0.0446,  0.0370]],
       dtype=torch.float64)
	q_value: tensor([[0.8094]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.5299854992052586, distance: 0.7845349971906579 entropy tensor([[ -0.8559,  -1.3693,  -2.0967, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 19
	action: tensor([[-0.0124,  0.3367, -0.0463,  0.1861,  0.1019,  0.0362,  0.0408]],
       dtype=torch.float64)
	q_value: tensor([[0.8262]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.54362887344515, distance: 0.7730645761669045 entropy tensor([[ -0.8676,  -1.3614,  -1.8144, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 20
	action: tensor([[-0.0983,  0.2130, -0.0117,  0.1838,  0.1031,  0.0376,  0.0400]],
       dtype=torch.float64)
	q_value: tensor([[0.8225]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.40675274366342096, distance: 0.881403077284448 entropy tensor([[ -0.8495,  -1.3635,  -1.8467, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 21
	action: tensor([[-0.0646,  0.3253, -0.0522,  0.1797,  0.0947,  0.0431,  0.0320]],
       dtype=torch.float64)
	q_value: tensor([[0.8069]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.4875542691164828, distance: 0.819182437726232 entropy tensor([[ -0.9808,  -1.4454,  -1.8872, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 22
	action: tensor([[ 0.2183,  0.1908, -0.0191,  0.1868,  0.0981,  0.0376,  0.0396]],
       dtype=torch.float64)
	q_value: tensor([[0.8225]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.6861250051955294, distance: 0.6411136927464559 entropy tensor([[ -0.8891,  -1.4059,  -1.8305, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 23
	action: tensor([[-0.1734,  0.3678, -0.0027,  0.1714,  0.1066,  0.0471,  0.0372]],
       dtype=torch.float64)
	q_value: tensor([[0.8025]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.4000083081993461, distance: 0.8863991105665362 entropy tensor([[ -0.7917,  -1.2588,  -2.2678, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 24
	action: tensor([[0.0489, 0.2036, 0.0034, 0.1958, 0.0917, 0.0329, 0.0437]],
       dtype=torch.float64)
	q_value: tensor([[0.8299]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.5525904425440258, distance: 0.7654367714956859 entropy tensor([[ -0.9575,  -1.3760,  -1.7481, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 25
	action: tensor([[-0.3248,  0.2029, -0.0179,  0.1741,  0.1021,  0.0477,  0.0306]],
       dtype=torch.float64)
	q_value: tensor([[0.8017]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.14350350739757378, distance: 1.0590573721044119 entropy tensor([[ -0.8569,  -1.3697,  -2.1093, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 26
	action: tensor([[-0.0088,  0.2432, -0.1096,  0.1940,  0.0749,  0.0303,  0.0413]],
       dtype=torch.float64)
	q_value: tensor([[0.8162]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.500848383640924, distance: 0.8084868009350111 entropy tensor([[ -1.1263,  -1.5343,  -1.6713, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 27
	action: tensor([[-0.1561,  0.3663,  0.0135,  0.1842,  0.0991,  0.0415,  0.0413]],
       dtype=torch.float64)
	q_value: tensor([[0.8146]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.4220764993242204, distance: 0.8699451490395962 entropy tensor([[ -0.8544,  -1.4415,  -1.9785, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 28
	action: tensor([[-0.2185,  0.3432, -0.0686,  0.1933,  0.0941,  0.0351,  0.0419]],
       dtype=torch.float64)
	q_value: tensor([[0.8277]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.3363208685089232, distance: 0.9322572603332715 entropy tensor([[ -0.9306,  -1.3676,  -1.7414, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 29
	action: tensor([[-0.0800,  0.2908, -0.0077,  0.2003,  0.0853,  0.0297,  0.0505]],
       dtype=torch.float64)
	q_value: tensor([[0.8329]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.466840571728523, distance: 0.8355746198919971 entropy tensor([[ -0.9804,  -1.4033,  -1.6943, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 30
	action: tensor([[0.0793, 0.3137, 0.0312, 0.1836, 0.0984, 0.0403, 0.0366]],
       dtype=torch.float64)
	q_value: tensor([[0.8161]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.6264699641246826, distance: 0.6993900638476306 entropy tensor([[ -0.9030,  -1.4071,  -1.8173, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 31
	action: tensor([[-0.0073,  0.1893, -0.0489,  0.1752,  0.1076,  0.0422,  0.0330]],
       dtype=torch.float64)
	q_value: tensor([[0.8126]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.4809515477165702, distance: 0.8244430161690646 entropy tensor([[ -0.8034,  -1.2993,  -1.9605, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 32
	action: tensor([[-0.1289,  0.2524, -0.0388,  0.1782,  0.0980,  0.0456,  0.0318]],
       dtype=torch.float64)
	q_value: tensor([[0.8034]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.39041293084122297, distance: 0.8934588731226993 entropy tensor([[ -0.9240,  -1.4206,  -2.0807, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 33
	action: tensor([[-0.0307,  0.3766,  0.0014,  0.1851,  0.0928,  0.0397,  0.0362]],
       dtype=torch.float64)
	q_value: tensor([[0.8152]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.5470930245994103, distance: 0.770124957808133 entropy tensor([[ -0.9856,  -1.4629,  -1.8305, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 34
	action: tensor([[-0.0353,  0.2827, -0.0448,  0.1856,  0.1035,  0.0371,  0.0391]],
       dtype=torch.float64)
	q_value: tensor([[0.8237]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.49999617720171985, distance: 0.8091766753167261 entropy tensor([[ -0.8360,  -1.3426,  -1.7766, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 35
	action: tensor([[0.1383, 0.2563, 0.0217, 0.1823, 0.1005, 0.0401, 0.0368]],
       dtype=torch.float64)
	q_value: tensor([[0.8161]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.6529672306018948, distance: 0.6741273329815364 entropy tensor([[ -0.8973,  -1.4042,  -1.9001, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 36
	action: tensor([[-0.0732,  0.3642, -0.0281,  0.1722,  0.1073,  0.0446,  0.0329]],
       dtype=torch.float64)
	q_value: tensor([[0.8070]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.49603966379480446, distance: 0.8123718608099918 entropy tensor([[ -0.7989,  -1.2869,  -2.1021, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 37
	action: tensor([[ 0.0442,  0.3701, -0.0222,  0.1884,  0.0985,  0.0359,  0.0409]],
       dtype=torch.float64)
	q_value: tensor([[0.8270]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.609912185575819, distance: 0.7147232093786084 entropy tensor([[ -0.8964,  -1.3710,  -1.8040, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 38
	action: tensor([[ 0.1322,  0.3101, -0.0669,  0.1818,  0.1075,  0.0365,  0.0403]],
       dtype=torch.float64)
	q_value: tensor([[0.8231]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.6585031674277337, distance: 0.6687288091602899 entropy tensor([[ -0.7899,  -1.2947,  -1.8500, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 39
	action: tensor([[-0.0069,  0.2978, -0.0139,  0.1788,  0.1086,  0.0377,  0.0406]],
       dtype=torch.float64)
	q_value: tensor([[0.8190]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.535586435268693, distance: 0.7798465252203889 entropy tensor([[ -0.7775,  -1.2824,  -2.0902, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 40
	action: tensor([[-0.0029,  0.2711, -0.0397,  0.1807,  0.1028,  0.0413,  0.0340]],
       dtype=torch.float64)
	q_value: tensor([[0.8155]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.5262601895872345, distance: 0.7876379519287731 entropy tensor([[ -0.8851,  -1.3785,  -1.9110, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 41
	action: tensor([[ 0.0356,  0.2232, -0.0171,  0.1804,  0.1017,  0.0417,  0.0351]],
       dtype=torch.float64)
	q_value: tensor([[0.8127]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.5438267924893736, distance: 0.7728969266537681 entropy tensor([[ -0.8798,  -1.3985,  -1.9761, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 42
	action: tensor([[-0.1509,  0.2283, -0.0203,  0.1765,  0.1020,  0.0457,  0.0313]],
       dtype=torch.float64)
	q_value: tensor([[0.8049]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.3566176986705204, distance: 0.9178912886927748 entropy tensor([[ -0.8710,  -1.3825,  -2.0988, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 43
	action: tensor([[-0.1148,  0.3209, -0.0259,  0.1836,  0.0907,  0.0401,  0.0344]],
       dtype=torch.float64)
	q_value: tensor([[0.8114]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.4392624946421917, distance: 0.8569125351685293 entropy tensor([[ -1.0141,  -1.4773,  -1.8201, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 44
	action: tensor([[ 0.0284,  0.3399, -0.1456,  0.1882,  0.0948,  0.0377,  0.0392]],
       dtype=torch.float64)
	q_value: tensor([[0.8218]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.5708017672286914, distance: 0.7496967977587335 entropy tensor([[ -0.9221,  -1.4141,  -1.7898, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 45
	action: tensor([[ 0.0759,  0.3518, -0.0413,  0.1902,  0.1033,  0.0335,  0.0490]],
       dtype=torch.float64)
	q_value: tensor([[0.8298]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.6293978174570756, distance: 0.6966436448214267 entropy tensor([[ -0.8138,  -1.3649,  -1.9374, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 46
	action: tensor([[ 0.0165,  0.1586, -0.0144,  0.1808,  0.1090,  0.0369,  0.0407]],
       dtype=torch.float64)
	q_value: tensor([[0.8237]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.4932695938496847, distance: 0.8146014440683197 entropy tensor([[ -0.7861,  -1.2817,  -1.9169, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 47
	action: tensor([[-0.0736,  0.1310, -0.0464,  0.1745,  0.0983,  0.0484,  0.0286]],
       dtype=torch.float64)
	q_value: tensor([[0.7981]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.38285486284310766, distance: 0.8989806603442922 entropy tensor([[ -0.9274,  -1.3889,  -2.1174, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 48
	action: tensor([[ 0.0138,  0.3442, -0.0509,  0.1771,  0.0916,  0.0460,  0.0300]],
       dtype=torch.float64)
	q_value: tensor([[0.7973]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.5693760376226092, distance: 0.7509409533251117 entropy tensor([[ -0.9839,  -1.4628,  -2.0286, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 3, step: 49
	action: tensor([[0.1828, 0.4112, 0.0070, 0.1841, 0.1036, 0.0374, 0.0396]],
       dtype=torch.float64)
	q_value: tensor([[0.8221]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.7351300345436806, distance: 0.5889427104265398 entropy tensor([[ -0.8201,  -1.3505,  -1.8908, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
LOSS epoch 3 actor 0.14026596731676153 critic 39.07407184353012
epoch: 4, step: 0
	action: tensor([[-0.0738,  0.3872, -0.0290,  0.2410,  0.0361,  0.1162,  0.0744]],
       dtype=torch.float64)
	q_value: tensor([[0.9696]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.5285156126553091, distance: 0.7857607862420674 entropy tensor([[ -0.7796,  -0.8586,  -1.5498, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 1
	action: tensor([[0.1125, 0.2102, 0.0016, 0.2536, 0.0155, 0.1123, 0.0696]],
       dtype=torch.float64)
	q_value: tensor([[0.9743]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.6411902942154616, distance: 0.6854705239674381 entropy tensor([[ -0.8520,  -0.9393,  -1.3810, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 2
	action: tensor([[ 0.0133,  0.2873, -0.0613,  0.2386,  0.0303,  0.1292,  0.0625]],
       dtype=torch.float64)
	q_value: tensor([[0.9627]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.5708843486887476, distance: 0.7496246702081364 entropy tensor([[ -0.7901,  -0.9558,  -1.7553, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 3
	action: tensor([[-0.0939,  0.2324,  0.0275,  0.2479,  0.0223,  0.1203,  0.0654]],
       dtype=torch.float64)
	q_value: tensor([[0.9697]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.4636657759676217, distance: 0.8380587186627975 entropy tensor([[ -0.8387,  -0.9708,  -1.5854, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 4
	action: tensor([[ 0.1919,  0.3707, -0.0677,  0.2420,  0.0118,  0.1232,  0.0520]],
       dtype=torch.float64)
	q_value: tensor([[0.9655]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.7382881852153982, distance: 0.5854210816031927 entropy tensor([[ -0.9240,  -0.9827,  -1.5335, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 5
	action: tensor([[ 0.1460,  0.3086, -0.0776,  0.2468,  0.0371,  0.1163,  0.0819]],
       dtype=torch.float64)
	q_value: tensor([[0.9709]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.68831315521718, distance: 0.6388750523832349 entropy tensor([[ -0.7244,  -0.9176,  -1.6288, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 6
	action: tensor([[-0.1863,  0.3139, -0.0817,  0.2448,  0.0332,  0.1202,  0.0766]],
       dtype=torch.float64)
	q_value: tensor([[0.9706]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.38196548142242337, distance: 0.8996281975269123 entropy tensor([[ -0.7783,  -0.9181,  -1.6326, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 7
	action: tensor([[-0.2347,  0.3167,  0.0156,  0.2543,  0.0040,  0.1083,  0.0681]],
       dtype=torch.float64)
	q_value: tensor([[0.9744]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.3522768200166253, distance: 0.9209825740182396 entropy tensor([[ -0.9711,  -0.9672,  -1.3872, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 8
	action: tensor([[ 0.0734,  0.3130, -0.0432,  0.2505, -0.0019,  0.1130,  0.0606]],
       dtype=torch.float64)
	q_value: tensor([[0.9717]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.6346073503408038, distance: 0.6917299767391758 entropy tensor([[ -0.9603,  -0.9648,  -1.3419, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 9
	action: tensor([[ 0.0490,  0.2515, -0.0586,  0.2466,  0.0265,  0.1206,  0.0704]],
       dtype=torch.float64)
	q_value: tensor([[0.9687]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.5912126773558408, distance: 0.7316534128649466 entropy tensor([[ -0.7583,  -0.9673,  -1.5934, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 10
	action: tensor([[-0.0730,  0.3043, -0.0569,  0.2450,  0.0254,  0.1238,  0.0652]],
       dtype=torch.float64)
	q_value: tensor([[0.9677]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.49738961329675424, distance: 0.8112830882451212 entropy tensor([[ -0.8240,  -0.9704,  -1.6426, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 11
	action: tensor([[-0.0822,  0.2562, -0.0738,  0.2500,  0.0145,  0.1162,  0.0644]],
       dtype=torch.float64)
	q_value: tensor([[0.9713]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.46573563123439665, distance: 0.8364400105512568 entropy tensor([[ -0.8790,  -0.9751,  -1.4825, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 12
	action: tensor([[0.1405, 0.3353, 0.0240, 0.2476, 0.0120, 0.1177, 0.0628]],
       dtype=torch.float64)
	q_value: tensor([[0.9693]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.7061973560867765, distance: 0.6202753794741106 entropy tensor([[ -0.8940,  -0.9967,  -1.5208, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 13
	action: tensor([[-0.1940,  0.3973, -0.0044,  0.2429,  0.0345,  0.1224,  0.0697]],
       dtype=torch.float64)
	q_value: tensor([[0.9676]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.42253606221611006, distance: 0.8695991914704276 entropy tensor([[ -0.7381,  -0.9187,  -1.5798, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 14
	action: tensor([[-0.2161,  0.3316, -0.0079,  0.2583,  0.0032,  0.1077,  0.0695]],
       dtype=torch.float64)
	q_value: tensor([[0.9755]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.37450311175851037, distance: 0.9050431173448868 entropy tensor([[ -0.9104,  -0.9315,  -1.3119, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 15
	action: tensor([[-1.1428e-01,  4.0355e-01,  1.5096e-02,  2.5234e-01, -2.9429e-04,
          1.1177e-01,  6.4332e-02]], dtype=torch.float64)
	q_value: tensor([[0.9726]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.5049797402370839, distance: 0.8051340246283478 entropy tensor([[ -0.9343,  -0.9665,  -1.3466, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 16
	action: tensor([[ 0.1249,  0.3647, -0.1145,  0.2534,  0.0098,  0.1131,  0.0686]],
       dtype=torch.float64)
	q_value: tensor([[0.9733]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.6794489659094785, distance: 0.6478959786668195 entropy tensor([[ -0.8204,  -0.9391,  -1.3500, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 17
	action: tensor([[-0.0633,  0.2670, -0.0939,  0.2510,  0.0300,  0.1147,  0.0828]],
       dtype=torch.float64)
	q_value: tensor([[0.9730]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.4855600661711378, distance: 0.8207748305360865 entropy tensor([[ -0.7437,  -0.9485,  -1.5567, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 18
	action: tensor([[-0.1875,  0.3778, -0.0614,  0.2485,  0.0148,  0.1172,  0.0659]],
       dtype=torch.float64)
	q_value: tensor([[0.9714]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.41003662656133877, distance: 0.8789602163512944 entropy tensor([[ -0.8996,  -0.9752,  -1.5064, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 19
	action: tensor([[-0.2583,  0.3206, -0.1390,  0.2586,  0.0021,  0.1061,  0.0731]],
       dtype=torch.float64)
	q_value: tensor([[0.9757]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.29608154565493217, distance: 0.9601030930462693 entropy tensor([[ -0.9028,  -0.9492,  -1.3301, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 20
	action: tensor([[-0.1008,  0.2263,  0.0624,  0.2609, -0.0056,  0.1013,  0.0782]],
       dtype=torch.float64)
	q_value: tensor([[0.9763]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.46072174713228764, distance: 0.8403556935435499 entropy tensor([[ -0.9692,  -0.9725,  -1.3306, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 21
	action: tensor([[-0.0537,  0.4181, -0.0581,  0.2399,  0.0114,  0.1260,  0.0503]],
       dtype=torch.float64)
	q_value: tensor([[0.9642]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.5500629052426458, distance: 0.7675958059666953 entropy tensor([[ -0.9026,  -0.9736,  -1.4991, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 22
	action: tensor([[-0.1418,  0.2841, -0.1072,  0.2575,  0.0142,  0.1095,  0.0755]],
       dtype=torch.float64)
	q_value: tensor([[0.9748]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.4111955123169463, distance: 0.8780965057911942 entropy tensor([[ -0.7866,  -0.9461,  -1.3762, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 23
	action: tensor([[-0.0517,  0.3407, -0.0402,  0.2515,  0.0073,  0.1117,  0.0684]],
       dtype=torch.float64)
	q_value: tensor([[0.9728]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.5328928721561349, distance: 0.7821047805918869 entropy tensor([[ -0.9278,  -0.9839,  -1.4315, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 24
	action: tensor([[-0.0071,  0.3076, -0.0344,  0.2508,  0.0160,  0.1160,  0.0678]],
       dtype=torch.float64)
	q_value: tensor([[0.9718]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.5647370231988128, distance: 0.7549749764368854 entropy tensor([[ -0.8232,  -0.9623,  -1.4459, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 25
	action: tensor([[-0.0842,  0.3321, -0.1190,  0.2479,  0.0207,  0.1197,  0.0653]],
       dtype=torch.float64)
	q_value: tensor([[0.9696]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.48446060404207125, distance: 0.8216514432748213 entropy tensor([[ -0.8153,  -0.9685,  -1.5268, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 26
	action: tensor([[-0.0184,  0.1130,  0.0078,  0.2547,  0.0127,  0.1108,  0.0731]],
       dtype=torch.float64)
	q_value: tensor([[0.9740]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.4787925574009195, distance: 0.8261558787850382 entropy tensor([[ -0.8656,  -0.9746,  -1.4466, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 27
	action: tensor([[-0.0844,  0.2759,  0.0170,  0.2362,  0.0167,  0.1304,  0.0495]],
       dtype=torch.float64)
	q_value: tensor([[0.9592]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.4872717628054908, distance: 0.8194082102401592 entropy tensor([[ -0.8985,  -0.9777,  -1.7003, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 28
	action: tensor([[-0.0854,  0.4001,  0.0167,  0.2456,  0.0127,  0.1204,  0.0554]],
       dtype=torch.float64)
	q_value: tensor([[0.9672]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.5308849823561246, distance: 0.7837839416884455 entropy tensor([[ -0.8978,  -0.9872,  -1.5196, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 29
	action: tensor([[-0.0695,  0.1851, -0.0817,  0.2525,  0.0135,  0.1137,  0.0667]],
       dtype=torch.float64)
	q_value: tensor([[0.9726]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.4457892152716094, distance: 0.851910909141168 entropy tensor([[ -0.8204,  -0.9374,  -1.3783, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 30
	action: tensor([[-0.1086,  0.3390, -0.0976,  0.2434,  0.0125,  0.1216,  0.0595]],
       dtype=torch.float64)
	q_value: tensor([[0.9662]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.4663169806033792, distance: 0.8359848087331868 entropy tensor([[ -0.9128,  -1.0107,  -1.5967, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 31
	action: tensor([[-0.1074,  0.3961,  0.0473,  0.2548,  0.0099,  0.1106,  0.0710]],
       dtype=torch.float64)
	q_value: tensor([[0.9739]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.5160704423744511, distance: 0.7960636026856943 entropy tensor([[ -0.8721,  -0.9792,  -1.4323, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 32
	action: tensor([[-0.0102,  0.2481, -0.1894,  0.2510,  0.0123,  0.1154,  0.0656]],
       dtype=torch.float64)
	q_value: tensor([[0.9725]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.5102380196827326, distance: 0.8008463995561933 entropy tensor([[ -0.8340,  -0.9268,  -1.3477, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 33
	action: tensor([[ 0.0816,  0.4741, -0.0954,  0.2511,  0.0181,  0.1154,  0.0761]],
       dtype=torch.float64)
	q_value: tensor([[0.9717]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.669111246562879, distance: 0.6582603500149363 entropy tensor([[ -0.8619,  -1.0141,  -1.6026, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 34
	action: tensor([[-0.1189,  0.1788, -0.0467,  0.2578,  0.0269,  0.1062,  0.0905]],
       dtype=torch.float64)
	q_value: tensor([[0.9778]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.3994715200856085, distance: 0.8867955344996004 entropy tensor([[ -0.7371,  -0.8827,  -1.3507, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 35
	action: tensor([[-0.1741,  0.2716, -0.0757,  0.2414,  0.0084,  0.1217,  0.0552]],
       dtype=torch.float64)
	q_value: tensor([[0.9670]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.3757840001820356, distance: 0.9041159717723698 entropy tensor([[ -0.9755,  -0.9857,  -1.5217, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 36
	action: tensor([[ 0.0624,  0.2043, -0.0948,  0.2506,  0.0035,  0.1113,  0.0625]],
       dtype=torch.float64)
	q_value: tensor([[0.9710]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.5775498912129802, distance: 0.7437798463242637 entropy tensor([[ -0.9566,  -1.0059,  -1.4509, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 37
	action: tensor([[ 0.0837,  0.3638, -0.0158,  0.2442,  0.0245,  0.1250,  0.0672]],
       dtype=torch.float64)
	q_value: tensor([[0.9655]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.6627270480487846, distance: 0.664580279664547 entropy tensor([[ -0.8115,  -1.0046,  -1.7109, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 38
	action: tensor([[0.0551, 0.4761, 0.0373, 0.2478, 0.0293, 0.1183, 0.0715]],
       dtype=torch.float64)
	q_value: tensor([[0.9710]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.6712746546306125, distance: 0.6561049109016653 entropy tensor([[ -0.7641,  -0.9209,  -1.5152, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 39
	action: tensor([[-0.0815,  0.3241, -0.0554,  0.2521,  0.0279,  0.1123,  0.0771]],
       dtype=torch.float64)
	q_value: tensor([[0.9747]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.4968721548922438, distance: 0.8117006057371734 entropy tensor([[ -0.7280,  -0.8630,  -1.3360, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 40
	action: tensor([[ 0.0196,  0.3839, -0.0629,  0.2506,  0.0142,  0.1151,  0.0667]],
       dtype=torch.float64)
	q_value: tensor([[0.9726]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.6058287922122116, distance: 0.7184542899190959 entropy tensor([[ -0.8743,  -0.9621,  -1.4339, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 41
	action: tensor([[ 0.0949,  0.2465, -0.0447,  0.2529,  0.0218,  0.1136,  0.0762]],
       dtype=torch.float64)
	q_value: tensor([[0.9734]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.6322764030983657, distance: 0.6939328436844917 entropy tensor([[ -0.7713,  -0.9388,  -1.4421, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 42
	action: tensor([[-0.1084,  0.3462, -0.0072,  0.2428,  0.0291,  0.1256,  0.0669]],
       dtype=torch.float64)
	q_value: tensor([[0.9666]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.48768612868942907, distance: 0.8190770373010084 entropy tensor([[ -0.7979,  -0.9583,  -1.6894, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 43
	action: tensor([[-0.1602,  0.3210, -0.0357,  0.2507,  0.0126,  0.1149,  0.0626]],
       dtype=torch.float64)
	q_value: tensor([[0.9723]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.42115180203569014, distance: 0.8706408418047293 entropy tensor([[ -0.8897,  -0.9550,  -1.4110, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 44
	action: tensor([[-0.0790,  0.3669, -0.0837,  0.2514,  0.0055,  0.1127,  0.0639]],
       dtype=torch.float64)
	q_value: tensor([[0.9720]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.507456615146119, distance: 0.8031172212332546 entropy tensor([[ -0.9190,  -0.9805,  -1.4049, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 45
	action: tensor([[-0.0939,  0.1995,  0.0143,  0.2550,  0.0123,  0.1114,  0.0735]],
       dtype=torch.float64)
	q_value: tensor([[0.9741]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.44663343207477346, distance: 0.8512618135107147 entropy tensor([[ -0.8262,  -0.9690,  -1.4079, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 46
	action: tensor([[-0.0098,  0.2944, -0.0442,  0.2406,  0.0110,  0.1244,  0.0517]],
       dtype=torch.float64)
	q_value: tensor([[0.9645]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.5541363815060225, distance: 0.7641132163085942 entropy tensor([[ -0.9277,  -0.9845,  -1.5495, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 47
	action: tensor([[ 0.0556,  0.4297, -0.0376,  0.2482,  0.0196,  0.1198,  0.0641]],
       dtype=torch.float64)
	q_value: tensor([[0.9686]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.6509442820540665, distance: 0.6760893138373295 entropy tensor([[ -0.8200,  -0.9908,  -1.5732, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 48
	action: tensor([[0.0878, 0.3551, 0.0138, 0.2533, 0.0258, 0.1123, 0.0791]],
       dtype=torch.float64)
	q_value: tensor([[0.9742]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.6690703395292291, distance: 0.6583010383819694 entropy tensor([[ -0.7406,  -0.9063,  -1.4046, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 4, step: 49
	action: tensor([[-0.2475,  0.3010,  0.0044,  0.2453,  0.0306,  0.1203,  0.0697]],
       dtype=torch.float64)
	q_value: tensor([[0.9701]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.32936473532438004, distance: 0.9371300983364631 entropy tensor([[ -0.7636,  -0.9118,  -1.4996, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
LOSS epoch 4 actor 0.18115622930854952 critic 39.65956814093398
epoch: 5, step: 0
	action: tensor([[-0.0120,  0.2441,  0.0280,  0.3126, -0.0978, -0.0100,  0.0809]],
       dtype=torch.float64)
	q_value: tensor([[0.9948]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.5470466061879511, distance: 0.7701644218296683 entropy tensor([[ -0.7759,  -0.8568,  -1.1957, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 1
	action: tensor([[-0.0478,  0.5103,  0.0618,  0.3059, -0.0873,  0.0090,  0.0957]],
       dtype=torch.float64)
	q_value: tensor([[0.9911]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.5980269134797846, distance: 0.7255296771096934 entropy tensor([[ -0.5247,  -0.8797,  -1.3127, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 2
	action: tensor([[-0.2514,  0.3933,  0.0065,  0.3240, -0.0925, -0.0182,  0.1170]],
       dtype=torch.float64)
	q_value: tensor([[0.9953]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.36730869588181625, distance: 0.9102331035878173 entropy tensor([[ -0.4807,  -0.7922,  -1.0951, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 3
	action: tensor([[-0.0727,  0.2298,  0.1712,  0.3175, -0.1075, -0.0138,  0.1010]],
       dtype=torch.float64)
	q_value: tensor([[0.9953]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.5093346015949194, distance: 0.8015846824533635 entropy tensor([[ -0.5848,  -0.8122,  -1.0736, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 4
	action: tensor([[-0.2118,  0.5323, -0.0321,  0.2993, -0.0881,  0.0130,  0.0812]],
       dtype=torch.float64)
	q_value: tensor([[0.9898]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.4402859000342251, distance: 0.8561301999345895 entropy tensor([[ -0.5551,  -0.8405,  -1.2585, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 5
	action: tensor([[-0.1630,  0.3707, -0.0837,  0.3342, -0.1068, -0.0254,  0.1175]],
       dtype=torch.float64)
	q_value: tensor([[0.9965]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.43053150297969534, distance: 0.8635580664096565 entropy tensor([[ -0.5263,  -0.8114,  -1.0372, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 6
	action: tensor([[-0.0364,  0.3595, -0.0585,  0.3189, -0.1035, -0.0103,  0.1121]],
       dtype=torch.float64)
	q_value: tensor([[0.9953]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.5506184278064528, distance: 0.7671217969362234 entropy tensor([[ -0.5490,  -0.8333,  -1.1021, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 7
	action: tensor([[ 0.1586,  0.1920, -0.0174,  0.3163, -0.0938, -0.0053,  0.1129]],
       dtype=torch.float64)
	q_value: tensor([[0.9944]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.6697945880691556, distance: 0.6575802889801011 entropy tensor([[ -0.5090,  -0.8491,  -1.1992, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 8
	action: tensor([[-0.2348,  0.4977,  0.0761,  0.3048, -0.0747,  0.0165,  0.1131]],
       dtype=torch.float64)
	q_value: tensor([[0.9909]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.43331417247849446, distance: 0.8614456234747647 entropy tensor([[ -0.5042,  -0.8531,  -1.3894, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 9
	action: tensor([[-0.0728,  0.4065, -0.1240,  0.3239, -0.1039, -0.0213,  0.1069]],
       dtype=torch.float64)
	q_value: tensor([[0.9959]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.5183567223672223, distance: 0.7941809123378099 entropy tensor([[ -0.5619,  -0.7954,  -1.0554, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 10
	action: tensor([[-0.1878,  0.2473,  0.0936,  0.3238, -0.0983, -0.0120,  0.1224]],
       dtype=torch.float64)
	q_value: tensor([[0.9955]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.3896875105365042, distance: 0.8939903316564223 entropy tensor([[ -0.5183,  -0.8481,  -1.1141, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 11
	action: tensor([[ 0.0843,  0.2047,  0.0239,  0.3013, -0.0982,  0.0037,  0.0819]],
       dtype=torch.float64)
	q_value: tensor([[0.9922]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.6158152251860505, distance: 0.7092947873273798 entropy tensor([[ -0.6026,  -0.8473,  -1.1873, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 12
	action: tensor([[-0.1696,  0.4198, -0.0813,  0.3051, -0.0792,  0.0152,  0.1009]],
       dtype=torch.float64)
	q_value: tensor([[0.9901]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.43808122636392033, distance: 0.8578146605299208 entropy tensor([[ -0.5133,  -0.8710,  -1.4017, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 13
	action: tensor([[-0.1675,  0.2015,  0.1089,  0.3244, -0.1023, -0.0174,  0.1110]],
       dtype=torch.float64)
	q_value: tensor([[0.9959]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.3915024892018567, distance: 0.8926600446910459 entropy tensor([[ -0.5695,  -0.8402,  -1.1007, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 14
	action: tensor([[-0.2257,  0.5260, -0.0209,  0.2979, -0.0957,  0.0098,  0.0767]],
       dtype=torch.float64)
	q_value: tensor([[0.9909]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.42756048477100517, distance: 0.8658078037483306 entropy tensor([[ -0.5967,  -0.8642,  -1.2210, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 15
	action: tensor([[-0.0734,  0.1272,  0.0116,  0.3332, -0.1079, -0.0241,  0.1155]],
       dtype=torch.float64)
	q_value: tensor([[0.9963]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.4361389737657708, distance: 0.8592958848777957 entropy tensor([[ -0.5248,  -0.8134,  -1.0428, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 16
	action: tensor([[-0.2939,  0.3429, -0.0624,  0.3000, -0.0893,  0.0187,  0.0851]],
       dtype=torch.float64)
	q_value: tensor([[0.9902]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.2830898662634014, distance: 0.968922526456086 entropy tensor([[ -0.5801,  -0.8686,  -1.3192, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 17
	action: tensor([[-0.0559,  0.2548, -0.1687,  0.3179, -0.1106, -0.0146,  0.0957]],
       dtype=torch.float64)
	q_value: tensor([[0.9952]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.47259256682524386, distance: 0.8310550951014087 entropy tensor([[ -0.6410,  -0.8616,  -1.1132, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 18
	action: tensor([[-0.0519,  0.3329, -0.0692,  0.3163, -0.0957,  0.0024,  0.1126]],
       dtype=torch.float64)
	q_value: tensor([[0.9939]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.5261320550512592, distance: 0.7877444627157864 entropy tensor([[ -0.5516,  -0.8985,  -1.2140, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 19
	action: tensor([[-0.0587,  0.3940,  0.0924,  0.3151, -0.0936, -0.0039,  0.1097]],
       dtype=torch.float64)
	q_value: tensor([[0.9944]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.5684945240939094, distance: 0.7517091713566505 entropy tensor([[ -0.5309,  -0.8505,  -1.2164, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 20
	action: tensor([[ 0.0671,  0.4794, -0.1054,  0.3134, -0.0917, -0.0065,  0.1038]],
       dtype=torch.float64)
	q_value: tensor([[0.9937]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.656039242458528, distance: 0.6711369372339061 entropy tensor([[ -0.5186,  -0.8134,  -1.1700, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 21
	action: tensor([[ 0.0656,  0.4055,  0.0270,  0.3271, -0.0868, -0.0171,  0.1352]],
       dtype=torch.float64)
	q_value: tensor([[0.9959]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.6662633568475296, distance: 0.6610870348774891 entropy tensor([[ -0.4702,  -0.8304,  -1.1218, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 22
	action: tensor([[-0.1492,  0.1187, -0.0362,  0.3160, -0.0844, -0.0067,  0.1209]],
       dtype=torch.float64)
	q_value: tensor([[0.9944]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.33779808519970156, distance: 0.9312191734879742 entropy tensor([[ -0.4819,  -0.7953,  -1.1841, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 23
	action: tensor([[-0.2064,  0.3420,  0.0385,  0.3011, -0.0952,  0.0149,  0.0817]],
       dtype=torch.float64)
	q_value: tensor([[0.9916]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.396283786862597, distance: 0.8891460692381679 entropy tensor([[ -0.6560,  -0.8798,  -1.2791, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 24
	action: tensor([[-0.0320,  0.4020, -0.1152,  0.3118, -0.1019, -0.0069,  0.0899]],
       dtype=torch.float64)
	q_value: tensor([[0.9939]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.5548456162221965, distance: 0.7635052374522038 entropy tensor([[ -0.5897,  -0.8515,  -1.1526, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 25
	action: tensor([[-0.0472,  0.3270,  0.0387,  0.3229, -0.0947, -0.0106,  0.1212]],
       dtype=torch.float64)
	q_value: tensor([[0.9952]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.5496917433651823, distance: 0.7679123430700499 entropy tensor([[ -0.5067,  -0.8614,  -1.1561, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 26
	action: tensor([[ 7.7695e-02,  3.5220e-01,  1.7433e-03,  3.1031e-01, -9.0881e-02,
          2.0359e-04,  1.0275e-01]], dtype=torch.float64)
	q_value: tensor([[0.9933]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.6573155955473424, distance: 0.6698905685325552 entropy tensor([[ -0.5307,  -0.8365,  -1.2133, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 27
	action: tensor([[-0.0951,  0.3191, -0.0843,  0.3137, -0.0828, -0.0016,  0.1162]],
       dtype=torch.float64)
	q_value: tensor([[0.9933]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.4758381856671936, distance: 0.8284940289867643 entropy tensor([[ -0.4848,  -0.8446,  -1.2578, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 28
	action: tensor([[ 5.0011e-04,  2.2215e-01,  2.8037e-04,  3.1461e-01, -9.5939e-02,
         -4.3756e-03,  1.0641e-01]], dtype=torch.float64)
	q_value: tensor([[0.9946]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.5463002018494554, distance: 0.7707987226745772 entropy tensor([[ -0.5645,  -0.8572,  -1.1968, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 29
	action: tensor([[-0.2745,  0.2503, -0.0122,  0.3060, -0.0857,  0.0109,  0.0978]],
       dtype=torch.float64)
	q_value: tensor([[0.9917]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.2718823204652957, distance: 0.9764667991891296 entropy tensor([[ -0.5402,  -0.8645,  -1.3275, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 30
	action: tensor([[-0.0724,  0.1854,  0.0229,  0.3060, -0.1067, -0.0033,  0.0829]],
       dtype=torch.float64)
	q_value: tensor([[0.9937]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.46066503975132167, distance: 0.8403998758534323 entropy tensor([[ -0.6676,  -0.8783,  -1.1536, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 31
	action: tensor([[ 0.0176,  0.3522,  0.0758,  0.3032, -0.0909,  0.0125,  0.0850]],
       dtype=torch.float64)
	q_value: tensor([[0.9905]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.6188805914974889, distance: 0.7064594293206865 entropy tensor([[ -0.5719,  -0.8967,  -1.3266, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 32
	action: tensor([[ 0.1904,  0.5564,  0.0608,  0.3116, -0.0851, -0.0016,  0.1039]],
       dtype=torch.float64)
	q_value: tensor([[0.9926]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.7737321877613105, distance: 0.5443372841251118 entropy tensor([[ -0.5016,  -0.8378,  -1.2432, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 33
	action: tensor([[ 0.0144,  0.4223, -0.0520,  0.3243, -0.0770, -0.0209,  0.1393]],
       dtype=torch.float64)
	q_value: tensor([[0.9955]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.6131752358226558, distance: 0.7117276334090838 entropy tensor([[ -0.4258,  -0.7511,  -1.1076, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 34
	action: tensor([[-0.0736,  0.4233,  0.1404,  0.3204, -0.0894, -0.0107,  0.1240]],
       dtype=torch.float64)
	q_value: tensor([[0.9954]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.5726857759827269, distance: 0.7480495533855155 entropy tensor([[ -0.5034,  -0.8074,  -1.1508, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 35
	action: tensor([[-0.0251,  0.3232, -0.0248,  0.3128, -0.0918, -0.0086,  0.1048]],
       dtype=torch.float64)
	q_value: tensor([[0.9939]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.5545415736098201, distance: 0.7637659318439461 entropy tensor([[ -0.5142,  -0.7902,  -1.1269, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 36
	action: tensor([[ 2.1033e-01,  4.4937e-01,  1.4429e-02,  3.1240e-01, -9.1277e-02,
         -6.6569e-05,  1.0649e-01]], dtype=torch.float64)
	q_value: tensor([[0.9935]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.7749138442506274, distance: 0.5429140554691898 entropy tensor([[ -0.5192,  -0.8624,  -1.2397, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 37
	action: tensor([[ 0.2122,  0.0118, -0.0144,  0.3198, -0.0741, -0.0118,  0.1355]],
       dtype=torch.float64)
	q_value: tensor([[0.9944]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.6295373931337812, distance: 0.6965124479888999 entropy tensor([[ -0.4390,  -0.7957,  -1.1931, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 38
	action: tensor([[-0.0999,  0.2106,  0.0117,  0.2981, -0.0714,  0.0355,  0.1071]],
       dtype=torch.float64)
	q_value: tensor([[0.9874]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.44554246527631014, distance: 0.8521005352059305 entropy tensor([[ -0.5574,  -0.8060,  -1.4203, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 39
	action: tensor([[-0.0347,  0.4612, -0.0398,  0.3062, -0.0903,  0.0074,  0.0853]],
       dtype=torch.float64)
	q_value: tensor([[0.9923]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.5807042965853301, distance: 0.7409977671879937 entropy tensor([[ -0.6198,  -0.8692,  -1.2947, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 40
	action: tensor([[-0.0365,  0.3071,  0.0483,  0.3247, -0.0939, -0.0159,  0.1194]],
       dtype=torch.float64)
	q_value: tensor([[0.9953]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.5544457599770583, distance: 0.763848066617441 entropy tensor([[ -0.4901,  -0.8260,  -1.1306, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 41
	action: tensor([[-0.3546,  0.1867,  0.0514,  0.3085, -0.0894,  0.0029,  0.1007]],
       dtype=torch.float64)
	q_value: tensor([[0.9928]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.15889377054998044, distance: 1.0494992148441464 entropy tensor([[ -0.5308,  -0.8394,  -1.2321, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 42
	action: tensor([[-2.8511e-01,  3.3253e-01,  3.7755e-03,  2.9822e-01, -1.1148e-01,
          3.0516e-04,  7.0291e-02]], dtype=torch.float64)
	q_value: tensor([[0.9926]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.29904945600029864, distance: 0.9580769280843746 entropy tensor([[ -0.7168,  -0.8820,  -1.1362, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 43
	action: tensor([[-0.1909,  0.4115, -0.0526,  0.3129, -0.1097, -0.0094,  0.0874]],
       dtype=torch.float64)
	q_value: tensor([[0.9940]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.4202047235111285, distance: 0.8713527972411591 entropy tensor([[ -0.6095,  -0.8769,  -1.1395, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 44
	action: tensor([[ 0.0038,  0.3905, -0.0885,  0.3223, -0.1055, -0.0146,  0.1080]],
       dtype=torch.float64)
	q_value: tensor([[0.9953]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.589887328638119, distance: 0.7328385171431175 entropy tensor([[ -0.5473,  -0.8428,  -1.1031, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 45
	action: tensor([[-0.2253,  0.1596, -0.0309,  0.3198, -0.0915, -0.0082,  0.1226]],
       dtype=torch.float64)
	q_value: tensor([[0.9949]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.2778672641063097, distance: 0.9724453629632329 entropy tensor([[ -0.4931,  -0.8516,  -1.1752, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 46
	action: tensor([[ 0.0127,  0.3419,  0.0639,  0.3009, -0.1019,  0.0075,  0.0808]],
       dtype=torch.float64)
	q_value: tensor([[0.9926]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.6084100126557499, distance: 0.7160980360065492 entropy tensor([[ -0.6685,  -0.8805,  -1.2003, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 47
	action: tensor([[-0.4087,  0.2638,  0.0343,  0.3115, -0.0866, -0.0005,  0.1033]],
       dtype=torch.float64)
	q_value: tensor([[0.9924]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.13643152844015072, distance: 1.0634206326596016 entropy tensor([[ -0.4979,  -0.8522,  -1.2541, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 48
	action: tensor([[ 0.1398,  0.3028, -0.1640,  0.3062, -0.1174, -0.0104,  0.0781]],
       dtype=torch.float64)
	q_value: tensor([[0.9942]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.6659988063586535, distance: 0.6613490022953833 entropy tensor([[ -0.7016,  -0.8566,  -1.0903, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 5, step: 49
	action: tensor([[-0.4118,  0.2266,  0.0090,  0.3191, -0.0785,  0.0029,  0.1305]],
       dtype=torch.float64)
	q_value: tensor([[0.9937]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.10980520677850325, distance: 1.0796903459483258 entropy tensor([[ -0.4797,  -0.9090,  -1.2881, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
LOSS epoch 5 actor 0.22734916785194387 critic 36.85394187837288
epoch: 6, step: 0
	action: tensor([[ 0.0009,  0.3868,  0.0486,  0.2911, -0.0932, -0.1352,  0.2315]],
       dtype=torch.float64)
	q_value: tensor([[0.9979]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.593385586268103, distance: 0.7297062701407043 entropy tensor([[ -0.4622,  -0.7276,  -0.7484, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 1
	action: tensor([[-0.1200,  0.2472, -0.0809,  0.3089, -0.0851, -0.1276,  0.2681]],
       dtype=torch.float64)
	q_value: tensor([[0.9978]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.4080392678973903, distance: 0.8804468471062246 entropy tensor([[ -0.3255,  -0.6961,  -0.8194, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 2
	action: tensor([[ 0.0060,  0.1482,  0.1635,  0.3036, -0.0927, -0.1194,  0.2584]],
       dtype=torch.float64)
	q_value: tensor([[0.9980]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.5201426121156405, distance: 0.7927071694154113 entropy tensor([[ -0.4132,  -0.7049,  -0.7676, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 3
	action: tensor([[-0.0464,  0.5261, -0.1338,  0.2934, -0.0684, -0.1070,  0.2318]],
       dtype=torch.float64)
	q_value: tensor([[0.9958]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.5633050498844974, distance: 0.7562158541986266 entropy tensor([[ -0.4031,  -0.7477,  -0.8884, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 4
	action: tensor([[ 0.0213,  0.2250,  0.0194,  0.3277, -0.1000, -0.1449,  0.2961]],
       dtype=torch.float64)
	q_value: tensor([[0.9991]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.5574188118464245, distance: 0.7612953347931937 entropy tensor([[ -0.3305,  -0.6446,  -0.6786, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 5
	action: tensor([[ 0.1367,  0.2192, -0.0027,  0.3034, -0.0809, -0.1152,  0.2589]],
       dtype=torch.float64)
	q_value: tensor([[0.9974]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.648737857464738, distance: 0.6782227699916438 entropy tensor([[ -0.3575,  -0.7135,  -0.8258, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 6
	action: tensor([[-0.1281,  0.1598,  0.1261,  0.3083, -0.0776, -0.1138,  0.2584]],
       dtype=torch.float64)
	q_value: tensor([[0.9971]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.38900971083155145, distance: 0.8944866169603778 entropy tensor([[ -0.3461,  -0.7498,  -0.9022, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 7
	action: tensor([[-0.0762,  0.3462, -0.2666,  0.2897, -0.0736, -0.1114,  0.2308]],
       dtype=torch.float64)
	q_value: tensor([[0.9965]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.46190905928721837, distance: 0.8394300912999215 entropy tensor([[ -0.4410,  -0.7225,  -0.8147, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 8
	action: tensor([[ 0.0710,  0.2444, -0.2431,  0.3230, -0.1055, -0.1281,  0.2814]],
       dtype=torch.float64)
	q_value: tensor([[0.9988]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.571931840731909, distance: 0.7487091761515346 entropy tensor([[ -0.3892,  -0.7371,  -0.7297, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 9
	action: tensor([[-0.1214,  0.3167, -0.2054,  0.3229, -0.0975, -0.1197,  0.2822]],
       dtype=torch.float64)
	q_value: tensor([[0.9984]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.42005735241065467, distance: 0.8714635294832017 entropy tensor([[ -0.3496,  -0.7608,  -0.7679, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 10
	action: tensor([[-0.1647,  0.2724,  0.1104,  0.3144, -0.1032, -0.1272,  0.2824]],
       dtype=torch.float64)
	q_value: tensor([[0.9988]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.40613118627147116, distance: 0.8818646885019864 entropy tensor([[ -0.3864,  -0.6849,  -0.7147, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 11
	action: tensor([[-0.2390,  0.1915,  0.0261,  0.2948, -0.0797, -0.1218,  0.2503]],
       dtype=torch.float64)
	q_value: tensor([[0.9976]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.26196239834745494, distance: 0.9830960195957565 entropy tensor([[ -0.3938,  -0.6796,  -0.7612, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 12
	action: tensor([[-0.0099,  0.1505, -0.0716,  0.2906, -0.0853, -0.1148,  0.2366]],
       dtype=torch.float64)
	q_value: tensor([[0.9974]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.47032537266665453, distance: 0.8328394297460101 entropy tensor([[ -0.4644,  -0.7040,  -0.7449, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 13
	action: tensor([[ 0.0448,  0.3571,  0.0040,  0.3064, -0.0847, -0.1075,  0.2432]],
       dtype=torch.float64)
	q_value: tensor([[0.9970]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.6222253159066985, distance: 0.7033526349937058 entropy tensor([[ -0.4065,  -0.7760,  -0.8574, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 14
	action: tensor([[-0.3525,  0.1255, -0.0616,  0.3117, -0.0852, -0.1279,  0.2702]],
       dtype=torch.float64)
	q_value: tensor([[0.9980]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.07986930394102487, distance: 1.0976944108110789 entropy tensor([[ -0.3300,  -0.6987,  -0.8392, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 15
	action: tensor([[-0.0055,  0.3860, -0.1449,  0.2896, -0.0899, -0.1138,  0.2368]],
       dtype=torch.float64)
	q_value: tensor([[0.9979]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.562300129496613, distance: 0.7570854544091583 entropy tensor([[ -0.5155,  -0.7195,  -0.7124, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 16
	action: tensor([[-0.3132,  0.1441, -0.1513,  0.3206, -0.0975, -0.1302,  0.2796]],
       dtype=torch.float64)
	q_value: tensor([[0.9986]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.12570348931016084, distance: 1.0700056374758957 entropy tensor([[ -0.3467,  -0.7115,  -0.7665, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 17
	action: tensor([[-0.2246,  0.2444, -0.0162,  0.2971, -0.0965, -0.1150,  0.2480]],
       dtype=torch.float64)
	q_value: tensor([[0.9983]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.3024975660657283, distance: 0.9557175409238872 entropy tensor([[ -0.4933,  -0.7060,  -0.7077, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 18
	action: tensor([[-0.0831,  0.3601,  0.1084,  0.2959, -0.0900, -0.1200,  0.2486]],
       dtype=torch.float64)
	q_value: tensor([[0.9979]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.5177369197273596, distance: 0.7946917438934141 entropy tensor([[ -0.4295,  -0.6868,  -0.7415, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 19
	action: tensor([[ 0.1130,  0.3306,  0.0999,  0.3028, -0.0805, -0.1277,  0.2596]],
       dtype=torch.float64)
	q_value: tensor([[0.9977]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.6801091408716459, distance: 0.6472284638102846 entropy tensor([[ -0.3540,  -0.6887,  -0.7886, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 20
	action: tensor([[-0.0394,  0.2470, -0.0487,  0.3074, -0.0750, -0.1235,  0.2639]],
       dtype=torch.float64)
	q_value: tensor([[0.9974]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.49427056458494745, distance: 0.8137964842057419 entropy tensor([[ -0.3233,  -0.6948,  -0.8734, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 21
	action: tensor([[ 0.0873,  0.5098, -0.1362,  0.3066, -0.0879, -0.1182,  0.2567]],
       dtype=torch.float64)
	q_value: tensor([[0.9977]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.6712214971516148, distance: 0.6561579574246367 entropy tensor([[ -0.3922,  -0.7241,  -0.8111, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 22
	action: tensor([[-0.1275,  0.2595,  0.0676,  0.3294, -0.0960, -0.1435,  0.3009]],
       dtype=torch.float64)
	q_value: tensor([[0.9990]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.4351061218228215, distance: 0.8600825320513226 entropy tensor([[ -0.3032,  -0.6631,  -0.7213, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 23
	action: tensor([[-0.1866,  0.2289, -0.0299,  0.2970, -0.0811, -0.1206,  0.2559]],
       dtype=torch.float64)
	q_value: tensor([[0.9977]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.3328768659625718, distance: 0.934672992360543 entropy tensor([[ -0.3873,  -0.6734,  -0.7605, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 24
	action: tensor([[-0.1014,  0.3365, -0.0460,  0.2977, -0.0894, -0.1181,  0.2478]],
       dtype=torch.float64)
	q_value: tensor([[0.9978]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.46940578875585315, distance: 0.8335620749115763 entropy tensor([[ -0.4378,  -0.6999,  -0.7562, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 25
	action: tensor([[ 0.1117,  0.3379,  0.0283,  0.3073, -0.0927, -0.1268,  0.2656]],
       dtype=torch.float64)
	q_value: tensor([[0.9982]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.6742628188877102, distance: 0.6531160555222782 entropy tensor([[ -0.3736,  -0.6946,  -0.7735, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 26
	action: tensor([[ 0.0507,  0.1451, -0.0410,  0.3114, -0.0811, -0.1250,  0.2715]],
       dtype=torch.float64)
	q_value: tensor([[0.9978]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.5355429936523547, distance: 0.779882998101375 entropy tensor([[ -0.3177,  -0.7047,  -0.8610, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 27
	action: tensor([[-0.2868,  0.2858, -0.1936,  0.3056, -0.0802, -0.1069,  0.2484]],
       dtype=torch.float64)
	q_value: tensor([[0.9970]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.22502192557220435, distance: 1.0073987348516344 entropy tensor([[ -0.3854,  -0.7580,  -0.8734, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 28
	action: tensor([[ 0.0099,  0.3240, -0.0951,  0.3082, -0.1046, -0.1282,  0.2675]],
       dtype=torch.float64)
	q_value: tensor([[0.9988]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.5657787986032097, distance: 0.754070941880858 entropy tensor([[ -0.4584,  -0.6767,  -0.6741, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 29
	action: tensor([[-0.1383,  0.2446,  0.1774,  0.3150, -0.0938, -0.1247,  0.2742]],
       dtype=torch.float64)
	q_value: tensor([[0.9982]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.43036821214370957, distance: 0.863681866946738 entropy tensor([[ -0.3447,  -0.7131,  -0.7899, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 30
	action: tensor([[-0.2427,  0.3503,  0.0657,  0.2919, -0.0733, -0.1188,  0.2421]],
       dtype=torch.float64)
	q_value: tensor([[0.9970]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.34398876520111044, distance: 0.9268561411821272 entropy tensor([[ -0.4065,  -0.6921,  -0.7892, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 31
	action: tensor([[-0.0128,  0.0192, -0.0381,  0.2961, -0.0879, -0.1286,  0.2548]],
       dtype=torch.float64)
	q_value: tensor([[0.9981]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.3974529941841163, distance: 0.8882846546350742 entropy tensor([[ -0.4024,  -0.6806,  -0.7343, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 32
	action: tensor([[ 0.2450,  0.4235,  0.0077,  0.2994, -0.0771, -0.0939,  0.2306]],
       dtype=torch.float64)
	q_value: tensor([[0.9959]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.7889599110276101, distance: 0.5257014488121374 entropy tensor([[ -0.4430,  -0.7888,  -0.8945, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 33
	action: tensor([[-0.1723,  0.2226,  0.2116,  0.3205, -0.0785, -0.1368,  0.2849]],
       dtype=torch.float64)
	q_value: tensor([[0.9982]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.3886376128689579, distance: 0.8947589502478447 entropy tensor([[ -0.2921,  -0.6694,  -0.8750, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 34
	action: tensor([[-0.0119,  0.4556,  0.0684,  0.2875, -0.0710, -0.1165,  0.2374]],
       dtype=torch.float64)
	q_value: tensor([[0.9968]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.6042160255797477, distance: 0.7199225812172504 entropy tensor([[ -0.4321,  -0.6819,  -0.7745, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 35
	action: tensor([[-0.2178,  0.3921,  0.0072,  0.3113, -0.0845, -0.1352,  0.2740]],
       dtype=torch.float64)
	q_value: tensor([[0.9983]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.38407801350879833, distance: 0.8980893512340589 entropy tensor([[ -0.3225,  -0.6643,  -0.7927, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 36
	action: tensor([[ 0.0806,  0.4050,  0.0653,  0.3029, -0.0927, -0.1334,  0.2727]],
       dtype=torch.float64)
	q_value: tensor([[0.9985]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.6708889690113466, distance: 0.6564896942040289 entropy tensor([[ -0.3770,  -0.6493,  -0.7100, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 37
	action: tensor([[-0.0111,  0.2754,  0.0557,  0.3119, -0.0819, -0.1309,  0.2757]],
       dtype=torch.float64)
	q_value: tensor([[0.9981]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.549264609885172, distance: 0.7682764529029121 entropy tensor([[ -0.3039,  -0.6715,  -0.8328, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 38
	action: tensor([[ 0.0476,  0.0597, -0.0950,  0.3026, -0.0797, -0.1198,  0.2572]],
       dtype=torch.float64)
	q_value: tensor([[0.9975]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.4769283488217275, distance: 0.8276320205869686 entropy tensor([[ -0.3656,  -0.7070,  -0.8224, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 39
	action: tensor([[-0.2296,  0.4648, -0.0270,  0.3064, -0.0815, -0.0977,  0.2422]],
       dtype=torch.float64)
	q_value: tensor([[0.9966]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.3963093553259399, distance: 0.889127240574223 entropy tensor([[ -0.4127,  -0.7903,  -0.8964, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 40
	action: tensor([[-0.1398,  0.5514, -0.1371,  0.3117, -0.0976, -0.1424,  0.2810]],
       dtype=torch.float64)
	q_value: tensor([[0.9989]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.48837419755977385, distance: 0.8185268172097601 entropy tensor([[ -0.3654,  -0.6357,  -0.6821, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 41
	action: tensor([[-0.0568,  0.3290, -0.1611,  0.3281, -0.1068, -0.1478,  0.3077]],
       dtype=torch.float64)
	q_value: tensor([[0.9993]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.49727453317083115, distance: 0.8113759605960009 entropy tensor([[ -0.3292,  -0.6139,  -0.6058, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 42
	action: tensor([[-0.2894,  0.3245, -0.0118,  0.3150, -0.1009, -0.1263,  0.2852]],
       dtype=torch.float64)
	q_value: tensor([[0.9987]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.2770488661401681, distance: 0.9729962464162114 entropy tensor([[ -0.3603,  -0.6881,  -0.7318, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 43
	action: tensor([[-0.0940,  0.4279, -0.0203,  0.2980, -0.0935, -0.1286,  0.2654]],
       dtype=torch.float64)
	q_value: tensor([[0.9985]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.5125172924618944, distance: 0.7989807217922731 entropy tensor([[ -0.4077,  -0.6348,  -0.6986, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 44
	action: tensor([[ 0.1475,  0.2131,  0.1554,  0.3103, -0.0933, -0.1335,  0.2790]],
       dtype=torch.float64)
	q_value: tensor([[0.9986]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.6733420434996236, distance: 0.6540384998148359 entropy tensor([[ -0.3456,  -0.6658,  -0.7382, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 45
	action: tensor([[-0.3454,  0.2419,  0.2548,  0.3016, -0.0684, -0.1111,  0.2501]],
       dtype=torch.float64)
	q_value: tensor([[0.9964]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.20809675391831017, distance: 1.018339887071562 entropy tensor([[ -0.3547,  -0.7140,  -0.9009, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 46
	action: tensor([[ 0.0975,  0.4048, -0.1635,  0.2810, -0.0759, -0.1203,  0.2275]],
       dtype=torch.float64)
	q_value: tensor([[0.9970]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.652952997894915, distance: 0.6741411566882148 entropy tensor([[ -0.4755,  -0.7111,  -0.7600, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 47
	action: tensor([[ 0.0527,  0.1273, -0.0143,  0.3245, -0.0947, -0.1314,  0.2856]],
       dtype=torch.float64)
	q_value: tensor([[0.9986]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.5363889679291238, distance: 0.779172424628814 entropy tensor([[ -0.3309,  -0.7357,  -0.7800, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 48
	action: tensor([[-0.2856,  0.5227, -0.0045,  0.3030, -0.0775, -0.1057,  0.2482]],
       dtype=torch.float64)
	q_value: tensor([[0.9969]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.3636237369679287, distance: 0.9128799724840317 entropy tensor([[ -0.3785,  -0.7474,  -0.8732, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 6, step: 49
	action: tensor([[-0.4038,  0.0135, -0.1048,  0.3149, -0.0982, -0.1479,  0.2863]],
       dtype=torch.float64)
	q_value: tensor([[0.9990]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.06744524384736628, distance: 1.1823049191162736 entropy tensor([[ -0.3572,  -0.6320,  -0.6512, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
LOSS epoch 6 actor 0.22824330470736376 critic 31.75311123096719
epoch: 7, step: 0
	action: tensor([[-0.0568,  0.3007, -0.1159,  0.2882, -0.1664, -0.1380,  0.4046]],
       dtype=torch.float64)
	q_value: tensor([[0.9986]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.4883274520802, distance: 0.8185642093374591 entropy tensor([[ -0.3953,  -0.5873,  -0.4765, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 1
	action: tensor([[ 0.0117,  0.5429, -0.2391,  0.3099, -0.1878, -0.1542,  0.4655]],
       dtype=torch.float64)
	q_value: tensor([[0.9993]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.6067908287692723, distance: 0.7175770042040595 entropy tensor([[ -0.2662,  -0.4814,  -0.4301, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 2
	action: tensor([[ 0.0281,  0.4300,  0.0158,  0.3301, -0.2207, -0.1845,  0.5310]],
       dtype=torch.float64)
	q_value: tensor([[0.9998]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.6366378138191962, distance: 0.6898053497379769 entropy tensor([[ -0.2155,  -0.4152,  -0.2745, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 3
	action: tensor([[-0.2956,  0.1877, -0.1425,  0.3152, -0.1969, -0.1688,  0.5098]],
       dtype=torch.float64)
	q_value: tensor([[0.9996]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.17507530567355234, distance: 1.0393548465524975 entropy tensor([[ -0.1869,  -0.3915,  -0.3466, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 4
	action: tensor([[ 0.1057,  0.2838,  0.0950,  0.3010, -0.1893, -0.1426,  0.4697]],
       dtype=torch.float64)
	q_value: tensor([[0.9995]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.659478875344371, distance: 0.667772796308672 entropy tensor([[ -0.3111,  -0.4142,  -0.3661, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 5
	action: tensor([[ 0.0796,  0.1794, -0.2216,  0.3102, -0.1697, -0.1506,  0.4622]],
       dtype=torch.float64)
	q_value: tensor([[0.9991]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.554837977881971, distance: 0.7635117878625368 entropy tensor([[ -0.2228,  -0.4466,  -0.4655, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 6
	action: tensor([[-0.1032,  0.4288,  0.1094,  0.3192, -0.1852, -0.1443,  0.4660]],
       dtype=torch.float64)
	q_value: tensor([[0.9993]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.5313952885294195, distance: 0.7833575232190729 entropy tensor([[ -0.2688,  -0.4892,  -0.4081, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 7
	action: tensor([[ 0.0559,  0.4170,  0.1184,  0.3053, -0.1851, -0.1686,  0.4886]],
       dtype=torch.float64)
	q_value: tensor([[0.9994]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.66188813168451, distance: 0.6654062886992143 entropy tensor([[ -0.2066,  -0.4288,  -0.3803, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 8
	action: tensor([[ 0.0389,  0.3199, -0.2330,  0.3113, -0.1832, -0.1649,  0.4877]],
       dtype=torch.float64)
	q_value: tensor([[0.9994]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.5744023970585296, distance: 0.7465454964130238 entropy tensor([[ -0.2005,  -0.4168,  -0.4055, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 9
	action: tensor([[-0.6629,  0.2895, -0.1080,  0.3216, -0.2022, -0.1591,  0.4958]],
       dtype=torch.float64)
	q_value: tensor([[0.9996]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.20009196688199515, distance: 1.2536143580916572 entropy tensor([[ -0.2561,  -0.4647,  -0.3513, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 10
	action: tensor([[-0.2067,  0.1184, -0.0760,  0.3101, -0.2059, -0.1628,  0.4876]],
       dtype=torch.float64)
	q_value: tensor([[0.9997]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.24821360421995375, distance: 0.9922107419730352 entropy tensor([[ -0.3226,  -0.3545,  -0.2572, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 11
	action: tensor([[-0.0730,  0.5625, -0.0105,  0.2995, -0.1751, -0.1338,  0.4487]],
       dtype=torch.float64)
	q_value: tensor([[0.9991]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.5700438600295501, distance: 0.7503584383904064 entropy tensor([[ -0.3062,  -0.4544,  -0.4207, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 12
	action: tensor([[-0.3125,  0.4657,  0.0740,  0.3154, -0.2032, -0.1830,  0.5124]],
       dtype=torch.float64)
	q_value: tensor([[0.9997]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.33062160738494795, distance: 0.9362515246272535 entropy tensor([[ -0.1893,  -0.4138,  -0.2995, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 13
	action: tensor([[-0.1479,  0.0503,  0.0759,  0.2976, -0.1981, -0.1702,  0.5074]],
       dtype=torch.float64)
	q_value: tensor([[0.9996]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.28745603168281364, distance: 0.9659675278529358 entropy tensor([[ -0.2272,  -0.4047,  -0.3268, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 14
	action: tensor([[-0.0795,  0.3671, -0.2218,  0.2970, -0.1538, -0.1235,  0.4265]],
       dtype=torch.float64)
	q_value: tensor([[0.9987]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.4775128418981317, distance: 0.8271694831912895 entropy tensor([[ -0.3147,  -0.4821,  -0.4534, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 15
	action: tensor([[-0.1262,  0.4107, -0.1483,  0.3156, -0.2019, -0.1643,  0.4886]],
       dtype=torch.float64)
	q_value: tensor([[0.9996]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.4637742228817532, distance: 0.8379739865381928 entropy tensor([[ -0.2732,  -0.4631,  -0.3677, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 16
	action: tensor([[-0.3285,  0.3813, -0.0975,  0.3130, -0.2061, -0.1673,  0.5081]],
       dtype=torch.float64)
	q_value: tensor([[0.9996]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.2483764862813751, distance: 0.9921032499469359 entropy tensor([[ -0.2427,  -0.4141,  -0.3238, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 17
	action: tensor([[ 0.0124,  0.2471, -0.0146,  0.3028, -0.2056, -0.1627,  0.5028]],
       dtype=torch.float64)
	q_value: tensor([[0.9997]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.549725711211503, distance: 0.7678833797765809 entropy tensor([[ -0.2692,  -0.3838,  -0.3155, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 18
	action: tensor([[ 0.1711,  0.4927,  0.3026,  0.3098, -0.1782, -0.1460,  0.4677]],
       dtype=torch.float64)
	q_value: tensor([[0.9992]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.7824765702684685, distance: 0.5337153755767832 entropy tensor([[ -0.2412,  -0.4497,  -0.4245, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 19
	action: tensor([[-0.1299,  0.3116,  0.2010,  0.3222, -0.1718, -0.1784,  0.4897]],
       dtype=torch.float64)
	q_value: tensor([[0.9993]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.47055336715544915, distance: 0.8326601656959306 entropy tensor([[ -0.1498,  -0.3793,  -0.4318, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 20
	action: tensor([[ 0.0087,  0.3319, -0.0040,  0.2972, -0.1673, -0.1546,  0.4661]],
       dtype=torch.float64)
	q_value: tensor([[0.9991]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.5786034690214555, distance: 0.7428517850908419 entropy tensor([[ -0.2441,  -0.4525,  -0.4221, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 21
	action: tensor([[-0.2342,  0.3311, -0.0305,  0.3105, -0.1832, -0.1560,  0.4740]],
       dtype=torch.float64)
	q_value: tensor([[0.9993]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.3368694726707482, distance: 0.9318718738393997 entropy tensor([[ -0.2369,  -0.4502,  -0.4213, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 22
	action: tensor([[-0.4303,  0.6263, -0.0624,  0.2989, -0.1895, -0.1579,  0.4823]],
       dtype=torch.float64)
	q_value: tensor([[0.9995]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.2517277526878803, distance: 0.9898890319277099 entropy tensor([[ -0.2643,  -0.4288,  -0.3834, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 23
	action: tensor([[-0.0520,  0.4530, -0.2153,  0.3157, -0.2271, -0.1899,  0.5399]],
       dtype=torch.float64)
	q_value: tensor([[0.9998]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.5396671426016364, distance: 0.7764127894622979 entropy tensor([[ -0.2122,  -0.3529,  -0.2176, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 24
	action: tensor([[ 0.0059,  0.1002,  0.0041,  0.3240, -0.2184, -0.1724,  0.5303]],
       dtype=torch.float64)
	q_value: tensor([[0.9997]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.48049880641465614, distance: 0.8248024990306555 entropy tensor([[ -0.2308,  -0.3891,  -0.2728, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 25
	action: tensor([[-0.2569,  0.4017,  0.0512,  0.3063, -0.1657, -0.1304,  0.4515]],
       dtype=torch.float64)
	q_value: tensor([[0.9990]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.35734883837559783, distance: 0.9173695945658311 entropy tensor([[ -0.2580,  -0.4551,  -0.4182, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 26
	action: tensor([[ 0.2968,  0.2859, -0.0328,  0.2969, -0.1872, -0.1667,  0.4835]],
       dtype=torch.float64)
	q_value: tensor([[0.9995]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.7861929690535208, distance: 0.5291364564770246 entropy tensor([[ -0.2585,  -0.4367,  -0.3766, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 27
	action: tensor([[ 0.2249,  0.0533,  0.1726,  0.3194, -0.1766, -0.1546,  0.4760]],
       dtype=torch.float64)
	q_value: tensor([[0.9993]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.6607442609006247, distance: 0.6665309107788403 entropy tensor([[ -0.2162,  -0.4642,  -0.4505, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 28
	action: tensor([[ 0.2713,  0.3730,  0.0398,  0.3060, -0.1422, -0.1244,  0.4267]],
       dtype=torch.float64)
	q_value: tensor([[0.9983]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.8014100525778561, distance: 0.5099590738113856 entropy tensor([[ -0.2436,  -0.4529,  -0.4915, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 29
	action: tensor([[-0.2458,  0.3535,  0.0970,  0.3219, -0.1768, -0.1683,  0.4771]],
       dtype=torch.float64)
	q_value: tensor([[0.9992]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.35715084211760073, distance: 0.9175109012106749 entropy tensor([[ -0.2108,  -0.4409,  -0.4739, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 30
	action: tensor([[-0.0972,  0.4088,  0.1457,  0.2948, -0.1823, -0.1598,  0.4799]],
       dtype=torch.float64)
	q_value: tensor([[0.9994]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.5258213559224846, distance: 0.7880026690528074 entropy tensor([[ -0.2503,  -0.4396,  -0.3917, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 31
	action: tensor([[-0.1207,  0.1867,  0.0356,  0.3030, -0.1798, -0.1631,  0.4804]],
       dtype=torch.float64)
	q_value: tensor([[0.9994]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.3932677378837999, distance: 0.8913643027830535 entropy tensor([[ -0.2217,  -0.4436,  -0.3974, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 32
	action: tensor([[-0.0797,  0.2101, -0.1961,  0.3004, -0.1700, -0.1393,  0.4506]],
       dtype=torch.float64)
	q_value: tensor([[0.9990]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.41504624856150796, distance: 0.8752204538469424 entropy tensor([[ -0.2840,  -0.4598,  -0.4444, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 33
	action: tensor([[-0.1798,  0.1242,  0.0919,  0.3106, -0.1888, -0.1457,  0.4633]],
       dtype=torch.float64)
	q_value: tensor([[0.9993]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.30826450668412464, distance: 0.9517584106274259 entropy tensor([[ -0.2908,  -0.4795,  -0.4026, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 34
	action: tensor([[ 0.2338,  0.5741, -0.0664,  0.2955, -0.1583, -0.1346,  0.4350]],
       dtype=torch.float64)
	q_value: tensor([[0.9988]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.7882686243034831, distance: 0.526561743465531 entropy tensor([[ -0.3034,  -0.4722,  -0.4607, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 35
	action: tensor([[-0.1914,  0.6071, -0.2916,  0.3345, -0.2033, -0.1890,  0.5176]],
       dtype=torch.float64)
	q_value: tensor([[0.9997]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.43762319028363583, distance: 0.8581642039583538 entropy tensor([[ -0.1966,  -0.4055,  -0.3434, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 36
	action: tensor([[-0.0586,  0.3031,  0.0497,  0.3336, -0.2445, -0.1924,  0.5664]],
       dtype=torch.float64)
	q_value: tensor([[0.9999]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.5239159065724246, distance: 0.7895843448637715 entropy tensor([[ -0.2317,  -0.3218,  -0.1612, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 37
	action: tensor([[-0.1848,  0.4151,  0.2112,  0.3070, -0.1839, -0.1529,  0.4916]],
       dtype=torch.float64)
	q_value: tensor([[0.9994]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.4574925189259452, distance: 0.8428679865621841 entropy tensor([[ -0.2164,  -0.4092,  -0.3719, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 38
	action: tensor([[ 0.3590,  0.2647,  0.2469,  0.2974, -0.1755, -0.1648,  0.4818]],
       dtype=torch.float64)
	q_value: tensor([[0.9994]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.8364214430879973, distance: 0.4628282714037318 entropy tensor([[ -0.2249,  -0.4426,  -0.3908, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 39
	action: tensor([[-0.1979,  0.4025,  0.0968,  0.3195, -0.1552, -0.1500,  0.4506]],
       dtype=torch.float64)
	q_value: tensor([[0.9988]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.42719992816180696, distance: 0.8660804295689187 entropy tensor([[ -0.1824,  -0.4138,  -0.4738, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 40
	action: tensor([[-0.1432,  0.2657, -0.2066,  0.2983, -0.1835, -0.1665,  0.4828]],
       dtype=torch.float64)
	q_value: tensor([[0.9994]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.3729812227082704, distance: 0.9061434732659466 entropy tensor([[ -0.2405,  -0.4377,  -0.3837, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 41
	action: tensor([[-0.1801,  0.3615, -0.3059,  0.3097, -0.1982, -0.1492,  0.4799]],
       dtype=torch.float64)
	q_value: tensor([[0.9995]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.3630220493267373, distance: 0.913311430144013 entropy tensor([[ -0.2883,  -0.4468,  -0.3646, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 42
	action: tensor([[-0.3870,  0.4534,  0.1735,  0.3207, -0.2176, -0.1635,  0.5076]],
       dtype=torch.float64)
	q_value: tensor([[0.9997]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.26719525211308026, distance: 0.9796046331787628 entropy tensor([[ -0.2818,  -0.4128,  -0.2904, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 43
	action: tensor([[-0.1716,  0.3462,  0.0095,  0.2932, -0.1909, -0.1703,  0.5009]],
       dtype=torch.float64)
	q_value: tensor([[0.9996]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.4097449350852318, distance: 0.8791774785845392 entropy tensor([[ -0.2168,  -0.4140,  -0.3497, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 44
	action: tensor([[ 0.0340,  0.3114,  0.2538,  0.3020, -0.1879, -0.1561,  0.4819]],
       dtype=torch.float64)
	q_value: tensor([[0.9994]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.6246936321030166, distance: 0.7010510754699191 entropy tensor([[ -0.2502,  -0.4355,  -0.3873, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 45
	action: tensor([[-0.3515,  0.1180,  0.2428,  0.3062, -0.1618, -0.1538,  0.4566]],
       dtype=torch.float64)
	q_value: tensor([[0.9990]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.1205686058979708, distance: 1.0731431946032077 entropy tensor([[ -0.2163,  -0.4406,  -0.4579, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 46
	action: tensor([[ 0.0897,  0.6024, -0.2567,  0.2830, -0.1488, -0.1358,  0.4222]],
       dtype=torch.float64)
	q_value: tensor([[0.9986]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.6703421774897945, distance: 0.6570348203038524 entropy tensor([[ -0.3522,  -0.5118,  -0.4534, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 47
	action: tensor([[-0.0645,  0.3590, -0.2537,  0.3363, -0.2211, -0.1908,  0.5294]],
       dtype=torch.float64)
	q_value: tensor([[0.9998]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.49332069638277654, distance: 0.8145603677422047 entropy tensor([[ -0.2138,  -0.4156,  -0.2792, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 48
	action: tensor([[ 0.4170,  0.1419, -0.0414,  0.3226, -0.2153, -0.1637,  0.5197]],
       dtype=torch.float64)
	q_value: tensor([[0.9997]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.8112264346859137, distance: 0.49719560661809886 entropy tensor([[ -0.2458,  -0.3999,  -0.2900, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 7, step: 49
	action: tensor([[-0.3205,  0.5746, -0.0952,  0.3213, -0.1670, -0.1416,  0.4689]],
       dtype=torch.float64)
	q_value: tensor([[0.9991]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.33757871744764956, distance: 0.9313734032909003 entropy tensor([[ -0.2106,  -0.4409,  -0.4387, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
LOSS epoch 7 actor 0.2253455413014174 critic 34.11802010052946
epoch: 8, step: 0
	action: tensor([[-0.0907,  0.3761,  0.1945,  0.4556, -0.4661, -0.2451,  0.6835]],
       dtype=torch.float64)
	q_value: tensor([[0.9999]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.5960902679915254, distance: 0.7272753231555811 entropy tensor([[-1.0522e-01, -8.1456e-02,  1.5388e-02, -2.1607e+01, -2.1607e+01,
         -2.1607e+01, -2.1607e+01]], dtype=torch.float64,
       grad_fn=<AddBackward0>)
epoch: 8, step: 1
	action: tensor([[-0.1257,  0.0360,  0.4266,  0.4947, -0.4883, -0.2528,  0.7196]],
       dtype=torch.float64)
	q_value: tensor([[0.9998]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.46527128994130096, distance: 0.8368034161094589 entropy tensor([[ 9.4600e-03, -6.6230e-02,  3.0177e-02, -2.1607e+01, -2.1607e+01,
         -2.1607e+01, -2.1607e+01]], dtype=torch.float64,
       grad_fn=<AddBackward0>)
epoch: 8, step: 2
	action: tensor([[ 0.2119,  0.3103, -0.2205,  0.4889, -0.4454, -0.2288,  0.6603]],
       dtype=torch.float64)
	q_value: tensor([[0.9995]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.7476171377587872, distance: 0.5748924748726235 entropy tensor([[-3.1237e-02, -9.9085e-02,  1.3968e-02, -2.1607e+01, -2.1607e+01,
         -2.1607e+01, -2.1607e+01]], dtype=torch.float64,
       grad_fn=<AddBackward0>)
epoch: 8, step: 3
	action: tensor([[ 0.1408,  0.2488, -0.3161,  0.5085, -0.4925, -0.2801,  0.7492]],
       dtype=torch.float64)
	q_value: tensor([[0.9998]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.6696008326977325, distance: 0.6577731856194194 entropy tensor([[ -0.0388,  -0.0569,   0.0502, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 4
	action: tensor([[-0.3794,  0.4504, -0.0466,  0.5271, -0.5201, -0.2765,  0.7701]],
       dtype=torch.float64)
	q_value: tensor([[0.9999]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.3282719752160267, distance: 0.9378932863320445 entropy tensor([[-3.0211e-02, -1.8771e-02,  1.0532e-01, -2.1607e+01, -2.1607e+01,
         -2.1607e+01, -2.1607e+01]], dtype=torch.float64,
       grad_fn=<AddBackward0>)
epoch: 8, step: 5
	action: tensor([[-0.5814,  0.5733,  0.1648,  0.5236, -0.5660, -0.2638,  0.7846]],
       dtype=torch.float64)
	q_value: tensor([[0.9999]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.24819505240883588, distance: 0.9922229842753831 entropy tensor([[  0.0557,   0.0564,   0.1811, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 6
	action: tensor([[-3.0586e-01, -2.4049e-04, -7.7621e-01,  5.3931e-01, -5.9166e-01,
         -2.7070e-01,  8.0641e-01]], dtype=torch.float64)
	q_value: tensor([[0.9999]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.04696287333754179, distance: 1.1709068155425166 entropy tensor([[  0.1051,   0.0812,   0.2086, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 7
	action: tensor([[-0.2961,  0.4999, -0.0986,  0.5709, -0.5717, -0.2689,  0.8100]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.4213321804434085, distance: 0.8705051783910569 entropy tensor([[ -0.0334,   0.0949,   0.1931, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 8
	action: tensor([[ 0.1226, -0.0417,  0.2939,  0.5398, -0.5879, -0.2800,  0.8245]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.653160124243559, distance: 0.6739399544941947 entropy tensor([[  0.0795,   0.0924,   0.2289, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 9
	action: tensor([[-0.2894,  1.0631,  0.0830,  0.5107, -0.4713, -0.2485,  0.7067]],
       dtype=torch.float64)
	q_value: tensor([[0.9997]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.6739399544941947 entropy tensor([[ 1.1876e-02, -1.7317e-02,  1.0913e-01, -2.1607e+01, -2.1607e+01,
         -2.1607e+01, -2.1607e+01]], dtype=torch.float64,
       grad_fn=<AddBackward0>)
epoch: 8, step: 10
	action: tensor([[ 0.1094,  0.3628,  0.0898,  1.0045, -0.7740, -0.4103,  1.0879]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.8279498177675255, distance: 0.47466177806133164 entropy tensor([[ -0.6769,   0.7663,   0.5759, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 11
	action: tensor([[-0.4172,  1.2780, -0.3344,  0.6086, -0.6555, -0.3491,  1.0061]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.47466177806133164 entropy tensor([[  0.2394,   0.2756,   0.4064, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 12
	action: tensor([[ 0.0437, -0.9234, -0.3420,  1.0045, -0.7740, -0.4103,  1.0879]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.030771870403226043, distance: 1.1265998738998972 entropy tensor([[ -0.6769,   0.7663,   0.5759, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 13
	action: tensor([[-0.5289,  0.4123, -0.2433,  0.6327, -0.5350, -0.2634,  0.8306]],
       dtype=torch.float64)
	q_value: tensor([[0.9999]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.1265998738998972 entropy tensor([[  0.1611,   0.2278,   0.3652, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 14
	action: tensor([[ 0.0221,  0.5007, -0.5438,  1.0045, -0.7740, -0.4103,  1.0879]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.4847497285252975, distance: 0.8214210119371445 entropy tensor([[ -0.6769,   0.7663,   0.5759, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 15
	action: tensor([[-0.9240,  0.2794, -0.7925,  0.6576, -0.7307, -0.3659,  1.0946]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.6018459037859589, distance: 1.4483284467096618 entropy tensor([[  0.2268,   0.3442,   0.4933, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 16
	action: tensor([[ 0.2547, -0.0537,  0.8239,  0.6879, -0.7320, -0.3271,  0.9802]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.7902557730030466, distance: 0.5240849656511359 entropy tensor([[  0.0938,   0.3826,   0.4232, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 17
	action: tensor([[-0.1231,  0.6205, -0.5386,  0.5610, -0.4976, -0.2775,  0.7947]],
       dtype=torch.float64)
	q_value: tensor([[0.9998]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.4211623481000335, distance: 0.8706329106440283 entropy tensor([[  0.2059,   0.1247,   0.2776, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 18
	action: tensor([[-0.4122,  0.3137, -0.3051,  0.5660, -0.6124, -0.3114,  0.8894]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.1470343012501183, distance: 1.056872205579253 entropy tensor([[  0.0479,   0.1752,   0.3153, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 19
	action: tensor([[ 0.1552,  0.3857,  0.1952,  0.5609, -0.6065, -0.2657,  0.8255]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.7990863380288589, distance: 0.5129339298613778 entropy tensor([[  0.0545,   0.1426,   0.2400, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 20
	action: tensor([[-0.3110,  0.4865, -0.1953,  0.5325, -0.5274, -0.2903,  0.8017]],
       dtype=torch.float64)
	q_value: tensor([[0.9999]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.3389901176760596, distance: 0.9303806501681131 entropy tensor([[  0.0624,   0.0330,   0.1534, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 21
	action: tensor([[-0.0234,  0.6064, -0.0688,  0.5362, -0.5860, -0.2723,  0.8159]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.6336870069609876, distance: 0.6926005863797277 entropy tensor([[  0.0592,   0.0964,   0.2272, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 22
	action: tensor([[-0.0047,  0.2285, -0.2071,  0.5433, -0.5830, -0.2981,  0.8487]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.5511641397847565, distance: 0.7666558736007899 entropy tensor([[  0.0910,   0.0750,   0.2187, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 23
	action: tensor([[ 0.0742,  0.7060,  0.1869,  0.5383, -0.5477, -0.2701,  0.7851]],
       dtype=torch.float64)
	q_value: tensor([[0.9999]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.7666558736007899 entropy tensor([[ 2.9011e-03,  2.4234e-02,  1.3577e-01, -2.1607e+01, -2.1607e+01,
         -2.1607e+01, -2.1607e+01]], dtype=torch.float64,
       grad_fn=<AddBackward0>)
epoch: 8, step: 24
	action: tensor([[-0.0427,  1.6441,  0.3302,  1.0045, -0.7740, -0.4103,  1.0879]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.7666558736007899 entropy tensor([[ -0.6769,   0.7663,   0.5759, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 25
	action: tensor([[-0.2699, -0.0354, -0.1380,  1.0045, -0.7740, -0.4103,  1.0879]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.3607675213356807, distance: 0.9149262956270687 entropy tensor([[ -0.6769,   0.7663,   0.5759, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 26
	action: tensor([[-0.3010,  0.7485,  0.0786,  0.6078, -0.6445, -0.3039,  0.9310]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.9149262956270687 entropy tensor([[  0.2087,   0.3027,   0.3810, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 27
	action: tensor([[ 0.0674,  0.3491,  0.4084,  1.0045, -0.7740, -0.4103,  1.0879]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.8877637705925169, distance: 0.3833745041743131 entropy tensor([[ -0.6769,   0.7663,   0.5759, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 28
	action: tensor([[-0.1368,  0.4877, -0.2316,  0.5990, -0.6397, -0.3440,  0.9823]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.5158014967097954, distance: 0.7962847796247768 entropy tensor([[  0.2605,   0.2653,   0.4050, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 29
	action: tensor([[-0.2074,  0.2479,  0.5637,  0.5731, -0.6267, -0.2958,  0.8895]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.5420018071233594, distance: 0.7744414251256013 entropy tensor([[  0.0871,   0.1587,   0.2994, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 30
	action: tensor([[-0.2949,  0.3663, -0.1458,  0.5324, -0.5159, -0.2635,  0.7646]],
       dtype=torch.float64)
	q_value: tensor([[0.9998]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.32644815489306755, distance: 0.9391656683540339 entropy tensor([[  0.0802,   0.0316,   0.1536, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 31
	action: tensor([[-0.2605,  0.4306,  0.2829,  0.5204, -0.5555, -0.2607,  0.7712]],
       dtype=torch.float64)
	q_value: tensor([[0.9999]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.4898547934906443, distance: 0.8173415901101803 entropy tensor([[  0.0277,   0.0387,   0.1538, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 32
	action: tensor([[ 0.2012,  0.8220,  0.0126,  0.5188, -0.5300, -0.2646,  0.7664]],
       dtype=torch.float64)
	q_value: tensor([[0.9999]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.8173415901101803 entropy tensor([[ 8.0386e-02,  1.2268e-03,  1.1319e-01, -2.1607e+01, -2.1607e+01,
         -2.1607e+01, -2.1607e+01]], dtype=torch.float64,
       grad_fn=<AddBackward0>)
epoch: 8, step: 33
	action: tensor([[-0.0466,  0.1677,  0.1362,  1.0045, -0.7740, -0.4103,  1.0879]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.7412706468706652, distance: 0.5820758012061721 entropy tensor([[ -0.6769,   0.7663,   0.5759, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 34
	action: tensor([[ 0.0137,  1.1183, -0.2082,  0.6024, -0.6376, -0.3242,  0.9535]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.5820758012061721 entropy tensor([[  0.2241,   0.2769,   0.3844, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 35
	action: tensor([[-0.0104,  0.2772, -0.1048,  1.0045, -0.7740, -0.4103,  1.0879]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.6854167571587424, distance: 0.6418366105695462 entropy tensor([[ -0.6769,   0.7663,   0.5759, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 36
	action: tensor([[ 0.4046,  0.0538, -0.5308,  0.6152, -0.6660, -0.3363,  0.9969]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.6418366105695462 entropy tensor([[  0.2206,   0.2890,   0.3998, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 37
	action: tensor([[-0.2248,  0.7219, -0.7724,  1.0045, -0.7740, -0.4103,  1.0879]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.6418366105695462 entropy tensor([[ -0.6769,   0.7663,   0.5759, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 38
	action: tensor([[ 1.7912e-04,  7.2156e-01,  3.8155e-02,  1.0045e+00, -7.7401e-01,
         -4.1027e-01,  1.0879e+00]], dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.6418366105695462 entropy tensor([[ -0.6769,   0.7663,   0.5759, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 39
	action: tensor([[ 0.0194,  0.4831, -0.0986,  1.0045, -0.7740, -0.4103,  1.0879]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.6418366105695462 entropy tensor([[ -0.6769,   0.7663,   0.5759, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 40
	action: tensor([[-0.0674,  0.0794,  0.3540,  1.0045, -0.7740, -0.4103,  1.0879]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.7655729842634988, distance: 0.5540647523773025 entropy tensor([[ -0.6769,   0.7663,   0.5759, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 41
	action: tensor([[-0.0171,  0.7526, -0.0479,  0.5922, -0.6110, -0.3125,  0.9220]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.5540647523773025 entropy tensor([[  0.2294,   0.2600,   0.3735, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 42
	action: tensor([[ 0.0428,  0.6341, -0.2585,  1.0045, -0.7740, -0.4103,  1.0879]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.5540647523773025 entropy tensor([[ -0.6769,   0.7663,   0.5759, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 43
	action: tensor([[ 0.0348,  0.1205,  0.4308,  1.0045, -0.7740, -0.4103,  1.0879]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.8547334121195422, distance: 0.43615370003910897 entropy tensor([[ -0.6769,   0.7663,   0.5759, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 44
	action: tensor([[-0.3990,  0.3112, -0.4296,  0.5917, -0.6058, -0.3213,  0.9306]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.43615370003910897 entropy tensor([[  0.2405,   0.2518,   0.3820, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 45
	action: tensor([[ 0.0472,  0.5761, -0.1855,  1.0045, -0.7740, -0.4103,  1.0879]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.43615370003910897 entropy tensor([[ -0.6769,   0.7663,   0.5759, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 46
	action: tensor([[-0.0857,  1.1837, -0.7311,  1.0045, -0.7740, -0.4103,  1.0879]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.43615370003910897 entropy tensor([[ -0.6769,   0.7663,   0.5759, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 47
	action: tensor([[ 0.1218,  0.8025,  0.6717,  1.0045, -0.7740, -0.4103,  1.0879]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.30102084155677733, distance: 0.9567287096550778 entropy tensor([[ -0.6769,   0.7663,   0.5759, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 48
	action: tensor([[-0.2684,  0.2596,  0.4618,  0.6203, -0.6785, -0.3870,  1.0622]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.734815637472525, distance: 1.5072432838538887 entropy tensor([[  0.3711,   0.2826,   0.4679, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 8, step: 49
	action: tensor([[-0.1446,  0.1243, -0.3668,  0.5639, -0.5713, -0.2761,  0.8233]],
       dtype=torch.float64)
	q_value: tensor([[0.9999]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.9748578567844799, distance: 1.6081428136562868 entropy tensor([[  0.0945,   0.1283,   0.2573, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
LOSS epoch 8 actor 442.43456749814214 critic 1857.9745236920064
epoch: 9, step: 0
	action: tensor([[ 0.1202,  1.1141,  0.2572,  0.7189, -0.6684, -0.4398,  0.9760]],
       dtype=torch.float64)
	q_value: tensor([[0.9999]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.6081428136562868 entropy tensor([[  0.2310,   0.3273,   0.3085, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 9, step: 1
	action: tensor([[-0.6939,  0.0171, -0.5492,  0.9327, -0.6558, -0.7421,  0.7968]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.2703832911610562, distance: 1.2898050701166623 entropy tensor([[ 7.1472e-03,  6.5517e-01,  5.8500e-01, -2.1607e+01, -2.1607e+01,
         -2.1607e+01, -2.1607e+01]], dtype=torch.float64,
       grad_fn=<AddBackward0>)
epoch: 9, step: 2
	action: tensor([[-0.0731,  1.2359,  0.3937,  0.8063, -0.8139, -0.5403,  1.1571]],
       dtype=torch.float64)
	q_value: tensor([[0.9999]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2898050701166623 entropy tensor([[  0.4503,   0.5586,   0.5035, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 9, step: 3
	action: tensor([[ 0.1206,  0.4342, -0.0172,  0.9327, -0.6558, -0.7421,  0.7968]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.839180579635351, distance: 0.45890832844103313 entropy tensor([[ 7.1472e-03,  6.5517e-01,  5.8500e-01, -2.1607e+01, -2.1607e+01,
         -2.1607e+01, -2.1607e+01]], dtype=torch.float64,
       grad_fn=<AddBackward0>)
epoch: 9, step: 4
	action: tensor([[-0.4441,  0.6375,  0.4683,  0.8557, -0.7585, -0.5009,  1.2533]],
       dtype=torch.float64)
	q_value: tensor([[0.9999]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.6284802767088062, distance: 0.6975054905098669 entropy tensor([[  0.4818,   0.5074,   0.5182, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 9, step: 5
	action: tensor([[-1.0648,  0.9782,  0.0242,  1.0166, -0.8776, -0.5343,  1.3477]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.6975054905098669 entropy tensor([[  0.5173,   0.6874,   0.6680, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 9, step: 6
	action: tensor([[-0.4456,  0.3942, -0.7849,  0.9327, -0.6558, -0.7421,  0.7968]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.07934385554851742, distance: 1.0980077899025362 entropy tensor([[ 7.1472e-03,  6.5517e-01,  5.8500e-01, -2.1607e+01, -2.1607e+01,
         -2.1607e+01, -2.1607e+01]], dtype=torch.float64,
       grad_fn=<AddBackward0>)
epoch: 9, step: 7
	action: tensor([[ 0.1715,  0.4105, -0.4567,  0.8134, -0.8311, -0.5826,  1.2536]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.7187722826205187, distance: 0.6068561453581094 entropy tensor([[  0.4530,   0.6133,   0.5677, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 9, step: 8
	action: tensor([[-0.0240,  0.0286, -0.5202,  0.9078, -0.8331, -0.5886,  1.3638]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.47458125190647615, distance: 0.8294867935692446 entropy tensor([[  0.4468,   0.6165,   0.6228, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 9, step: 9
	action: tensor([[-0.2967,  0.4892, -0.0666,  0.9254, -0.8551, -0.5923,  1.3383]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.8294867935692446 entropy tensor([[  0.4413,   0.6724,   0.6220, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 9, step: 10
	action: tensor([[ 0.0729,  0.6597, -0.1068,  0.9327, -0.6558, -0.7421,  0.7968]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.8294867935692446 entropy tensor([[ 7.1472e-03,  6.5517e-01,  5.8500e-01, -2.1607e+01, -2.1607e+01,
         -2.1607e+01, -2.1607e+01]], dtype=torch.float64,
       grad_fn=<AddBackward0>)
epoch: 9, step: 11
	action: tensor([[-0.1971,  0.6914, -0.4897,  0.9327, -0.6558, -0.7421,  0.7968]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.8294867935692446 entropy tensor([[ 7.1472e-03,  6.5517e-01,  5.8500e-01, -2.1607e+01, -2.1607e+01,
         -2.1607e+01, -2.1607e+01]], dtype=torch.float64,
       grad_fn=<AddBackward0>)
epoch: 9, step: 12
	action: tensor([[-2.4161e-01,  3.1664e-04,  6.5554e-02,  9.3268e-01, -6.5580e-01,
         -7.4212e-01,  7.9675e-01]], dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.46778210567391587, distance: 0.8348365015524513 entropy tensor([[ 7.1472e-03,  6.5517e-01,  5.8500e-01, -2.1607e+01, -2.1607e+01,
         -2.1607e+01, -2.1607e+01]], dtype=torch.float64,
       grad_fn=<AddBackward0>)
epoch: 9, step: 13
	action: tensor([[-0.3549,  0.2410, -0.2738,  0.8472, -0.7604, -0.4770,  1.1448]],
       dtype=torch.float64)
	q_value: tensor([[0.9998]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.29487938307320627, distance: 0.9609225826023587 entropy tensor([[  0.4588,   0.4997,   0.4873, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 9, step: 14
	action: tensor([[-0.0660,  1.0111,  0.1183,  0.8606, -0.8245, -0.5262,  1.2366]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.9609225826023587 entropy tensor([[  0.4278,   0.6011,   0.5597, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 9, step: 15
	action: tensor([[-0.0759, -0.5415, -0.2790,  0.9327, -0.6558, -0.7421,  0.7968]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.2116217493452076, distance: 1.0160708933408127 entropy tensor([[ 7.1472e-03,  6.5517e-01,  5.8500e-01, -2.1607e+01, -2.1607e+01,
         -2.1607e+01, -2.1607e+01]], dtype=torch.float64,
       grad_fn=<AddBackward0>)
epoch: 9, step: 16
	action: tensor([[-0.7972,  0.4202, -0.5691,  0.8365, -0.7120, -0.4978,  1.0792]],
       dtype=torch.float64)
	q_value: tensor([[0.9996]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.18385598971044326, distance: 1.2451054416197995 entropy tensor([[  0.3912,   0.4458,   0.4213, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 9, step: 17
	action: tensor([[ 0.4849,  0.8473,  0.0966,  0.8474, -0.8726, -0.5774,  1.2479]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2451054416197995 entropy tensor([[  0.4331,   0.6722,   0.6062, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 9, step: 18
	action: tensor([[ 0.6118,  0.6829,  0.0895,  0.9327, -0.6558, -0.7421,  0.7968]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2451054416197995 entropy tensor([[ 7.1472e-03,  6.5517e-01,  5.8500e-01, -2.1607e+01, -2.1607e+01,
         -2.1607e+01, -2.1607e+01]], dtype=torch.float64,
       grad_fn=<AddBackward0>)
epoch: 9, step: 19
	action: tensor([[-0.4007,  0.5775, -0.2563,  0.9327, -0.6558, -0.7421,  0.7968]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.39900896649908124, distance: 0.887136993324964 entropy tensor([[ 7.1472e-03,  6.5517e-01,  5.8500e-01, -2.1607e+01, -2.1607e+01,
         -2.1607e+01, -2.1607e+01]], dtype=torch.float64,
       grad_fn=<AddBackward0>)
epoch: 9, step: 20
	action: tensor([[-0.4058,  0.7525, -0.8224,  0.8247, -0.8180, -0.5191,  1.2536]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.887136993324964 entropy tensor([[  0.4941,   0.5699,   0.5497, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 9, step: 21
	action: tensor([[ 0.2422,  0.6299, -0.0538,  0.9327, -0.6558, -0.7421,  0.7968]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.887136993324964 entropy tensor([[ 7.1472e-03,  6.5517e-01,  5.8500e-01, -2.1607e+01, -2.1607e+01,
         -2.1607e+01, -2.1607e+01]], dtype=torch.float64,
       grad_fn=<AddBackward0>)
epoch: 9, step: 22
	action: tensor([[ 0.5924,  0.8241, -0.5834,  0.9327, -0.6558, -0.7421,  0.7968]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.887136993324964 entropy tensor([[ 7.1472e-03,  6.5517e-01,  5.8500e-01, -2.1607e+01, -2.1607e+01,
         -2.1607e+01, -2.1607e+01]], dtype=torch.float64,
       grad_fn=<AddBackward0>)
epoch: 9, step: 23
	action: tensor([[-0.0156,  0.4002, -1.2458,  0.9327, -0.6558, -0.7421,  0.7968]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.2979418344480491, distance: 0.9588335918605838 entropy tensor([[ 7.1472e-03,  6.5517e-01,  5.8500e-01, -2.1607e+01, -2.1607e+01,
         -2.1607e+01, -2.1607e+01]], dtype=torch.float64,
       grad_fn=<AddBackward0>)
epoch: 9, step: 24
	action: tensor([[ 0.0270,  0.7303, -0.3580,  0.8321, -0.8260, -0.6557,  1.3354]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.9588335918605838 entropy tensor([[  0.4515,   0.5877,   0.6076, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 9, step: 25
	action: tensor([[ 0.4152, -0.0687, -0.5698,  0.9327, -0.6558, -0.7421,  0.7968]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.7041220937159914, distance: 0.6224621689618833 entropy tensor([[ 7.1472e-03,  6.5517e-01,  5.8500e-01, -2.1607e+01, -2.1607e+01,
         -2.1607e+01, -2.1607e+01]], dtype=torch.float64,
       grad_fn=<AddBackward0>)
epoch: 9, step: 26
	action: tensor([[-0.2286,  0.3967, -0.6146,  0.8217, -0.7197, -0.5405,  1.2018]],
       dtype=torch.float64)
	q_value: tensor([[0.9999]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.6224621689618833 entropy tensor([[  0.4118,   0.4554,   0.4521, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 9, step: 27
	action: tensor([[-0.0193,  0.2117, -0.6742,  0.9327, -0.6558, -0.7421,  0.7968]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.48751576326714563, distance: 0.8192132143734909 entropy tensor([[ 7.1472e-03,  6.5517e-01,  5.8500e-01, -2.1607e+01, -2.1607e+01,
         -2.1607e+01, -2.1607e+01]], dtype=torch.float64,
       grad_fn=<AddBackward0>)
epoch: 9, step: 28
	action: tensor([[-0.4771,  0.1304, -0.3180,  0.8072, -0.7729, -0.5511,  1.2283]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.11077776489720592, distance: 1.079100391603233 entropy tensor([[  0.4425,   0.5109,   0.4949, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 9, step: 29
	action: tensor([[ 0.1493,  1.2184,  0.0623,  0.8776, -0.8516, -0.5298,  1.2340]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.079100391603233 entropy tensor([[  0.4303,   0.6367,   0.5697, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 9, step: 30
	action: tensor([[-0.1423,  0.6203,  0.2924,  0.9327, -0.6558, -0.7421,  0.7968]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.5676601903010217, distance: 0.752435550885915 entropy tensor([[ 7.1472e-03,  6.5517e-01,  5.8500e-01, -2.1607e+01, -2.1607e+01,
         -2.1607e+01, -2.1607e+01]], dtype=torch.float64,
       grad_fn=<AddBackward0>)
epoch: 9, step: 31
	action: tensor([[ 0.6948,  0.2803, -0.1225,  0.8796, -0.7852, -0.4925,  1.2663]],
       dtype=torch.float64)
	q_value: tensor([[0.9999]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.8952891131437067, distance: 0.3702990622324226 entropy tensor([[  0.5365,   0.5420,   0.5530, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 9, step: 32
	action: tensor([[ 0.0504,  0.9553, -0.3605,  0.9771, -0.7930, -0.6120,  1.4054]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.3702990622324226 entropy tensor([[  0.4528,   0.5889,   0.6636, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 9, step: 33
	action: tensor([[ 0.3128,  0.3130, -1.0252,  0.9327, -0.6558, -0.7421,  0.7968]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.6639945491148458, distance: 0.6633303289782493 entropy tensor([[ 7.1472e-03,  6.5517e-01,  5.8500e-01, -2.1607e+01, -2.1607e+01,
         -2.1607e+01, -2.1607e+01]], dtype=torch.float64,
       grad_fn=<AddBackward0>)
epoch: 9, step: 34
	action: tensor([[-0.4215,  0.2467,  0.0547,  0.8211, -0.7763, -0.6103,  1.3098]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.4012981868411687, distance: 0.8854457953308714 entropy tensor([[  0.4598,   0.5057,   0.5326, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 9, step: 35
	action: tensor([[-0.3681,  0.8851,  1.1917,  0.9510, -0.8783, -0.5214,  1.2756]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.8362626550450059, distance: 0.4630528539243868 entropy tensor([[  0.4458,   0.6702,   0.6095, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 9, step: 36
	action: tensor([[ 0.0221, -0.0782, -0.4360,  1.1861, -0.9214, -0.5804,  1.4971]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.5681021520679035, distance: 0.7520508618418654 entropy tensor([[  0.6496,   0.7486,   0.7625, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 9, step: 37
	action: tensor([[-0.1949,  0.4982, -0.4974,  1.0104, -0.9124, -0.6452,  1.4656]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.4655878122055598, distance: 0.8365557146915875 entropy tensor([[  0.5413,   0.7702,   0.7233, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 9, step: 38
	action: tensor([[-0.0331,  1.0217, -0.3008,  0.9805, -0.9485, -0.6460,  1.5170]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.8365557146915875 entropy tensor([[  0.5493,   0.7918,   0.7628, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 9, step: 39
	action: tensor([[-0.4208, -0.1826,  0.2265,  0.9327, -0.6558, -0.7421,  0.7968]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.2119155914598183, distance: 1.0158815221570863 entropy tensor([[ 7.1472e-03,  6.5517e-01,  5.8500e-01, -2.1607e+01, -2.1607e+01,
         -2.1607e+01, -2.1607e+01]], dtype=torch.float64,
       grad_fn=<AddBackward0>)
epoch: 9, step: 40
	action: tensor([[-0.3617,  0.6254, -0.2057,  0.8587, -0.7557, -0.4709,  1.0960]],
       dtype=torch.float64)
	q_value: tensor([[0.9996]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.0158815221570863 entropy tensor([[  0.4509,   0.4721,   0.4854, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 9, step: 41
	action: tensor([[-0.1207,  0.0702, -0.4363,  0.9327, -0.6558, -0.7421,  0.7968]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.0158815221570863 entropy tensor([[ 7.1472e-03,  6.5517e-01,  5.8500e-01, -2.1607e+01, -2.1607e+01,
         -2.1607e+01, -2.1607e+01]], dtype=torch.float64,
       grad_fn=<AddBackward0>)
epoch: 9, step: 42
	action: tensor([[ 0.2072,  0.4906, -0.7653,  0.9327, -0.6558, -0.7421,  0.7968]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.6481309769405932, distance: 0.6788084046386469 entropy tensor([[ 7.1472e-03,  6.5517e-01,  5.8500e-01, -2.1607e+01, -2.1607e+01,
         -2.1607e+01, -2.1607e+01]], dtype=torch.float64,
       grad_fn=<AddBackward0>)
epoch: 9, step: 43
	action: tensor([[-0.3513,  0.3224,  0.0687,  0.8237, -0.7860, -0.5908,  1.3173]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.49270995449224975, distance: 0.815051147916688 entropy tensor([[  0.4626,   0.5315,   0.5322, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 9, step: 44
	action: tensor([[-0.1992,  1.2425, -0.4148,  0.9591, -0.8773, -0.5264,  1.2976]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.815051147916688 entropy tensor([[  0.4499,   0.6729,   0.6223, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 9, step: 45
	action: tensor([[-0.0502,  1.1952, -0.6279,  0.9327, -0.6558, -0.7421,  0.7968]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.815051147916688 entropy tensor([[ 7.1472e-03,  6.5517e-01,  5.8500e-01, -2.1607e+01, -2.1607e+01,
         -2.1607e+01, -2.1607e+01]], dtype=torch.float64,
       grad_fn=<AddBackward0>)
epoch: 9, step: 46
	action: tensor([[ 0.2857, -0.0727, -0.1544,  0.9327, -0.6558, -0.7421,  0.7968]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.6447172038877579, distance: 0.6820932979965532 entropy tensor([[ 7.1472e-03,  6.5517e-01,  5.8500e-01, -2.1607e+01, -2.1607e+01,
         -2.1607e+01, -2.1607e+01]], dtype=torch.float64,
       grad_fn=<AddBackward0>)
epoch: 9, step: 47
	action: tensor([[ 0.1606,  0.2428,  0.0776,  0.8464, -0.7244, -0.5050,  1.1795]],
       dtype=torch.float64)
	q_value: tensor([[0.9998]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.6820932979965532 entropy tensor([[  0.4167,   0.4763,   0.4567, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 9, step: 48
	action: tensor([[ 0.4942,  0.8229, -0.2733,  0.9327, -0.6558, -0.7421,  0.7968]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.6820932979965532 entropy tensor([[ 7.1472e-03,  6.5517e-01,  5.8500e-01, -2.1607e+01, -2.1607e+01,
         -2.1607e+01, -2.1607e+01]], dtype=torch.float64,
       grad_fn=<AddBackward0>)
epoch: 9, step: 49
	action: tensor([[ 0.4211,  1.4789, -0.1905,  0.9327, -0.6558, -0.7421,  0.7968]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.6820932979965532 entropy tensor([[ 7.1472e-03,  6.5517e-01,  5.8500e-01, -2.1607e+01, -2.1607e+01,
         -2.1607e+01, -2.1607e+01]], dtype=torch.float64,
       grad_fn=<AddBackward0>)
LOSS epoch 9 actor 624.4091459486807 critic 2315.8897547708825
epoch: 10, step: 0
	action: tensor([[-0.1214,  0.6966, -0.3701,  0.9612, -0.8731, -1.3856,  0.9082]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.743804607270754, distance: 0.5792184017876494 entropy tensor([[  0.6140,   0.8033,   0.7679, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 10, step: 1
	action: tensor([[ 0.6872,  0.3570, -0.6189,  1.0015, -1.1232, -1.1592,  1.8372]],
       dtype=torch.float64)
	q_value: tensor([[0.9999]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.9512378621666756, distance: 0.25269582238303245 entropy tensor([[  0.9595,   0.9449,   0.9323, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 10, step: 2
	action: tensor([[-0.0431,  0.7499, -0.1220,  1.2297, -1.2156, -1.4137,  2.2205]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.82312665558237, distance: 0.4812689986778613 entropy tensor([[  1.0153,   1.0945,   1.1376, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 10, step: 3
	action: tensor([[ 0.5373, -0.7569,  0.2449,  1.4428, -1.4306, -1.4752,  2.5581]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.6893446745022307, distance: 0.6378170063058674 entropy tensor([[  1.1618,   1.3229,   1.2899, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 10, step: 4
	action: tensor([[-0.9599,  1.1309, -0.1080,  1.7111, -1.4734, -1.6090,  2.6041]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.7650723919705232, distance: 0.5546560081117703 entropy tensor([[  1.2023,   1.3545,   1.3642, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 10, step: 5
	action: tensor([[-0.7407, -0.2633, -0.7162,  1.7023, -1.8070, -1.8207,  3.1017]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.3395553259716706, distance: 0.9299827958130178 entropy tensor([[  1.3394,   1.5494,   1.5100, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 10, step: 6
	action: tensor([[-2.2123,  4.9062, -1.2346,  1.7805, -1.8759, -2.0239,  3.0979]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.9299827958130178 entropy tensor([[  1.3838,   1.5861,   1.5406, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 10, step: 7
	action: tensor([[ 0.1607, -0.6625,  0.1657,  0.9612, -0.8731, -1.3856,  0.9082]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.4237065849914602, distance: 0.8687174030756076 entropy tensor([[  0.6140,   0.8033,   0.7679, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 10, step: 8
	action: tensor([[ 1.1076,  0.3249,  0.9709,  1.0820, -0.9725, -1.0435,  1.5986]],
       dtype=torch.float64)
	q_value: tensor([[0.9975]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.6510257874484662, distance: 0.6760103749451971 entropy tensor([[  0.8594,   0.8061,   0.8491, -21.6069,  -3.5270, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 10, step: 9
	action: tensor([[ 0.0100,  1.5548, -1.1312,  1.5095, -1.1229, -1.1601,  2.2306]],
       dtype=torch.float64)
	q_value: tensor([[0.9999]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.6760103749451971 entropy tensor([[  1.0892,   1.0956,   1.1578, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 10, step: 10
	action: tensor([[-0.5930, -0.3163, -0.4996,  0.9612, -0.8731, -1.3856,  0.9082]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.048715286892885734, distance: 1.1718863417125502 entropy tensor([[  0.6140,   0.8033,   0.7679, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 10, step: 11
	action: tensor([[-0.4208,  0.6940,  0.5103,  0.9855, -1.0713, -1.1666,  1.6159]],
       dtype=torch.float64)
	q_value: tensor([[0.9994]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.5172580375416689, distance: 0.7950862062810192 entropy tensor([[  0.9203,   0.8869,   0.8558, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 10, step: 12
	action: tensor([[-0.4535,  1.2897, -0.7509,  1.3165, -1.2420, -1.1191,  2.1183]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.34367001959886234, distance: 0.9270812862906253 entropy tensor([[  1.0422,   1.1427,   1.1123, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 10, step: 13
	action: tensor([[-0.6583,  1.3029,  0.6220,  1.3670, -1.5398, -1.6802,  2.6435]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.9270812862906253 entropy tensor([[  1.1820,   1.3662,   1.3720, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 10, step: 14
	action: tensor([[ 0.5753,  0.0965, -1.0832,  0.9612, -0.8731, -1.3856,  0.9082]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.9138891101563028, distance: 0.3358038970133606 entropy tensor([[  0.6140,   0.8033,   0.7679, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 10, step: 15
	action: tensor([[-0.6241,  0.7206,  0.2986,  0.9554, -1.1024, -1.3257,  1.8085]],
       dtype=torch.float64)
	q_value: tensor([[0.9999]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.6616733275009696, distance: 0.665617623132877 entropy tensor([[  0.9209,   0.8808,   0.8877, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 10, step: 16
	action: tensor([[ 0.2224,  1.5295,  0.6974,  1.3523, -1.3484, -1.2419,  2.2160]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.6798559434166744, distance: 0.6474845576941821 entropy tensor([[  1.0751,   1.2115,   1.1776, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 10, step: 17
	action: tensor([[ 0.2358,  0.4419, -0.8730,  1.6818, -1.4767, -1.4699,  2.8962]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.6146717383193578, distance: 0.7103495749026048 entropy tensor([[  1.2663,   1.3843,   1.4011, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 10, step: 18
	action: tensor([[ 0.4704,  0.7459, -0.8756,  1.6157, -1.6585, -1.9442,  3.0735]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.617274456008971, distance: 0.7079464654045781 entropy tensor([[  1.2979,   1.4899,   1.4909, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 10, step: 19
	action: tensor([[-0.3681,  0.1425,  2.0436,  1.7489, -1.7896, -2.0728,  3.3334]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.13406706000384827, distance: 1.0648754714006854 entropy tensor([[  1.3759,   1.5672,   1.5686, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 10, step: 20
	action: tensor([[-0.4215,  0.7877, -0.5526,  1.2538, -1.1094, -1.3714,  2.0440]],
       dtype=torch.float64)
	q_value: tensor([[0.9976]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.5799523844876602, distance: 0.7416618756169766 entropy tensor([[  1.4938,   0.7145,   0.9128, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 10, step: 21
	action: tensor([[ 0.3214,  0.6661,  0.3811,  1.3199, -1.4528, -1.5325,  2.4498]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.9102918745444202, distance: 0.3427461492057818 entropy tensor([[  1.1329,   1.2967,   1.2603, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 10, step: 22
	action: tensor([[-0.1021,  0.2448, -0.2670,  1.6702, -1.4952, -1.5187,  2.8022]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.8254626771095556, distance: 0.47808030012480707 entropy tensor([[  1.2350,   1.3938,   1.3882, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 10, step: 23
	action: tensor([[-0.4644,  0.6366, -0.9023,  1.6667, -1.6427, -1.7620,  2.9515]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.493446607754622, distance: 0.8144591510700592 entropy tensor([[  1.2778,   1.4967,   1.4470, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 10, step: 24
	action: tensor([[ 1.1345,  1.7484,  1.5904,  1.6803, -1.8387, -2.0382,  3.1720]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.8144591510700592 entropy tensor([[  1.3579,   1.5735,   1.5364, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 10, step: 25
	action: tensor([[-0.1184,  1.2531, -1.1139,  0.9612, -0.8731, -1.3856,  0.9082]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.8144591510700592 entropy tensor([[  0.6140,   0.8033,   0.7679, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 10, step: 26
	action: tensor([[-0.4247,  0.2807,  0.3894,  0.9612, -0.8731, -1.3856,  0.9082]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.5271884651545558, distance: 0.786865899881607 entropy tensor([[  0.6140,   0.8033,   0.7679, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 10, step: 27
	action: tensor([[-0.4325,  0.7216,  0.4844,  1.0722, -1.0499, -0.9910,  1.7331]],
       dtype=torch.float64)
	q_value: tensor([[0.9995]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.7859995546562304, distance: 0.5293757364331082 entropy tensor([[  0.9617,   0.8918,   0.9084, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 10, step: 28
	action: tensor([[-0.2422,  2.5524, -0.2351,  1.3370, -1.2545, -1.1520,  2.1754]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.5293757364331082 entropy tensor([[  1.0378,   1.1661,   1.1328, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 10, step: 29
	action: tensor([[-0.0270,  0.6741, -0.5019,  0.9612, -0.8731, -1.3856,  0.9082]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.7510031929070553, distance: 0.571022974981175 entropy tensor([[  0.6140,   0.8033,   0.7679, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 10, step: 30
	action: tensor([[-0.7401,  0.2644,  0.6302,  0.9856, -1.1205, -1.1891,  1.8376]],
       dtype=torch.float64)
	q_value: tensor([[0.9999]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.32077059552221543, distance: 0.9431156088073387 entropy tensor([[  0.9503,   0.9374,   0.9254, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 10, step: 31
	action: tensor([[-0.4597, -0.1765, -0.6786,  1.3956, -1.3168, -1.1842,  2.1201]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.24731086070720998, distance: 0.9928062855241869 entropy tensor([[  1.0425,   1.1886,   1.1502, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 10, step: 32
	action: tensor([[-0.3070,  0.9606, -0.4896,  1.3369, -1.4202, -1.5688,  2.3507]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.6248280574585121, distance: 0.7009255147438953 entropy tensor([[  1.1501,   1.2741,   1.2394, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 10, step: 33
	action: tensor([[ 0.3370, -1.0322,  1.5198,  1.4935, -1.5967, -1.6950,  2.7715]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.02369355668288531, distance: 1.1578216820280638 entropy tensor([[  1.2526,   1.4145,   1.3928, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 10, step: 34
	action: tensor([[ 1.5713e-03, -1.1252e+00, -1.0795e+00,  2.1134e+00, -1.6686e+00,
         -1.6278e+00,  2.8859e+00]], dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.4526811813867042, distance: 0.8465973090274502 entropy tensor([[  1.3166,   1.5100,   1.5005, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 10, step: 35
	action: tensor([[ 0.3569,  0.0531,  0.5349,  1.7411, -1.7440, -2.0855,  2.9998]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.7362566421351198, distance: 0.5876888604884679 entropy tensor([[  1.3966,   1.4971,   1.5047, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 10, step: 36
	action: tensor([[ 0.7884,  1.4123,  0.2435,  1.9974, -1.7778, -1.8195,  3.2656]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.4044993451457163, distance: 0.8830754574624179 entropy tensor([[  1.3923,   1.5888,   1.5735, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 10, step: 37
	action: tensor([[-1.2278,  0.8401, -0.6825,  1.1647, -1.0523, -1.6898,  2.3011]],
       dtype=torch.float64)
	q_value: tensor([[0.9998]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.29947577096559985, distance: 0.9577855347343237 entropy tensor([[  1.4201,   0.6249,   0.8858, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 10, step: 38
	action: tensor([[-0.6993,  0.4674, -0.2215,  1.4438, -1.6686, -1.7158,  2.5676]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.6669801687238244, distance: 0.6603766996883613 entropy tensor([[  1.1744,   1.4272,   1.3533, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 10, step: 39
	action: tensor([[-0.2140,  0.2532, -2.5493,  1.6339, -1.7080, -1.7412,  2.8577]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.14080245861452068, distance: 1.222255267252754 entropy tensor([[  1.3045,   1.4817,   1.4378, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 10, step: 40
	action: tensor([[ 1.9849,  3.9585, -0.0626,  1.5818, -1.9518, -2.4423,  3.1614]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.222255267252754 entropy tensor([[  1.3841,   1.5493,   1.5829, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 10, step: 41
	action: tensor([[-0.5078,  0.2252, -0.3082,  0.9612, -0.8731, -1.3856,  0.9082]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.38286925155122775, distance: 0.8989701804385577 entropy tensor([[  0.6140,   0.8033,   0.7679, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 10, step: 42
	action: tensor([[-0.5424, -0.2751,  0.0099,  0.9828, -1.0950, -1.1301,  1.7140]],
       dtype=torch.float64)
	q_value: tensor([[0.9998]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.1983520654303782, distance: 1.0245862710495968 entropy tensor([[  0.9334,   0.9165,   0.9068, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 10, step: 43
	action: tensor([[-0.2310,  0.7892, -0.8453,  1.2322, -1.2181, -1.2063,  1.9208]],
       dtype=torch.float64)
	q_value: tensor([[0.9999]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.4006491018781234, distance: 0.8859256451068883 entropy tensor([[  0.9777,   1.0774,   1.0378, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 10, step: 44
	action: tensor([[ 0.7621,  1.6380, -0.1165,  1.2500, -1.4145, -1.5762,  2.4076]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.8859256451068883 entropy tensor([[  1.1279,   1.2590,   1.2512, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 10, step: 45
	action: tensor([[-0.4009,  0.4458, -0.1795,  0.9612, -0.8731, -1.3856,  0.9082]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.5833783030901569, distance: 0.7386311771743327 entropy tensor([[  0.6140,   0.8033,   0.7679, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 10, step: 46
	action: tensor([[-0.2925,  0.5043, -0.5088,  1.0046, -1.1005, -1.1029,  1.7594]],
       dtype=torch.float64)
	q_value: tensor([[0.9998]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.5574598595887994, distance: 0.7612600303335699 entropy tensor([[  0.9512,   0.9264,   0.9212, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 10, step: 47
	action: tensor([[ 1.2976,  1.4920,  0.4174,  1.1698, -1.2699, -1.3446,  2.1115]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.3760729471000991, distance: 0.9039066918415549 entropy tensor([[  1.0264,   1.1390,   1.1083, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 10, step: 48
	action: tensor([[ 0.8719,  0.5484,  1.6446,  1.6538, -1.3702, -1.5261,  2.8750]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.7430933321326241, distance: 0.5800218863724133 entropy tensor([[  1.2691,   1.3165,   1.3810, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 10, step: 49
	action: tensor([[-0.4037,  1.8680, -0.6843,  2.1565, -1.6486, -1.6080,  3.2671]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.5800218863724133 entropy tensor([[  1.3624,   1.5621,   1.5520, -21.6069, -21.6069, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
LOSS epoch 10 actor 234.28221626950136 critic 1724.9101745141704
epoch: 11, step: 0
	action: tensor([[ 0.9299, -0.3120, -0.6408,  1.3555, -1.1113, -1.9590,  1.3407]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.8627652421084382, distance: 0.42392473124057223 entropy tensor([[  0.9872,   1.0709,   1.0667, -21.6069,  -4.2984, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 11, step: 1
	action: tensor([[-1.0449,  1.1280,  0.8309,  1.8630, -1.3200, -1.8687,  2.5488]],
       dtype=torch.float64)
	q_value: tensor([[0.9967]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.5995902863104261, distance: 0.7241174203685472 entropy tensor([[  1.2959,   1.2811,   1.3125, -21.6069,  -0.3719, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 11, step: 2
	action: tensor([[-4.1117,  0.8908, -2.0270,  3.0472, -2.2994, -2.4495,  3.8598]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.7241174203685472 entropy tensor([[  1.6453,   1.8139,   1.7803, -21.6069,  -0.2088, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 11, step: 3
	action: tensor([[ 1.0702, -0.6872,  0.2495,  1.3555, -1.1131, -1.9590,  1.3407]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.37930568203391124, distance: 0.901561958110829 entropy tensor([[  0.9872,   1.0709,   1.0667, -21.6069,  -4.2984, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 11, step: 4
	action: tensor([[ 0.5751,  2.8206,  1.1386,  2.1405, -1.3855, -1.8196,  2.5735]],
       dtype=torch.float64)
	q_value: tensor([[0.9855]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.901561958110829 entropy tensor([[  1.3175,   1.2960,   1.3222, -21.6069,  -0.1924, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 11, step: 5
	action: tensor([[ 0.7204,  0.7839,  0.6611,  1.3555, -1.1143, -1.9590,  1.3407]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.8815938944656654, distance: 0.39377101201996484 entropy tensor([[  0.9872,   1.0709,   1.0667, -21.6069,  -4.2984, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 11, step: 6
	action: tensor([[-0.3114,  0.4625, -1.5529,  2.2567, -1.7345, -1.8537,  2.9070]],
       dtype=torch.float64)
	q_value: tensor([[0.9989]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.3273504593049533, distance: 0.9385363943599836 entropy tensor([[  1.4205,   1.4127,   1.4159, -21.6069,  -0.5458, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 11, step: 7
	action: tensor([[ 0.1807,  3.5997, -1.7812,  2.7627, -2.7437, -3.0087,  4.1518]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.9385363943599836 entropy tensor([[  1.7020,   1.8227,   1.8676, -21.6069,  -0.5526, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 11, step: 8
	action: tensor([[-0.0544, -0.2968,  0.7126,  1.3555, -1.1162, -1.9590,  1.3407]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.5508028926959423, distance: 0.7669643343577566 entropy tensor([[  0.9872,   1.0709,   1.0667, -21.6069,  -4.2984, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 11, step: 9
	action: tensor([[ 0.0210,  1.1839, -1.1207,  2.2423, -1.5452, -1.7386,  2.6025]],
       dtype=torch.float64)
	q_value: tensor([[0.9895]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.7669643343577566 entropy tensor([[  1.3372,   1.3873,   1.3811, -21.6069,  -0.0480, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 11, step: 10
	action: tensor([[-0.6517,  0.7236,  0.3414,  1.3555, -1.1208, -1.9590,  1.3407]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.6498058385278317, distance: 0.6771909473297256 entropy tensor([[  0.9872,   1.0709,   1.0667, -21.6069,  -4.2984, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 11, step: 11
	action: tensor([[-1.0929,  1.0236,  2.1578,  2.1908, -1.9400, -1.8510,  2.8045]],
       dtype=torch.float64)
	q_value: tensor([[0.9993]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.15048458855064584, distance: 1.2274310235767978 entropy tensor([[  1.4123,   1.4780,   1.4526, -21.6069,  -0.3147, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 11, step: 12
	action: tensor([[-0.0800,  0.2567,  1.1507,  3.7209, -2.6610, -2.6469,  4.4882]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2274310235767978 entropy tensor([[  1.8191,   1.9571,   1.9168, -21.6069,  -0.0940, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 11, step: 13
	action: tensor([[-0.1432,  0.1722, -0.8842,  1.3555, -1.1189, -1.9590,  1.3407]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.6286523825031518, distance: 0.6973439127593167 entropy tensor([[  0.9872,   1.0709,   1.0667, -21.6069,  -4.2984, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 11, step: 14
	action: tensor([[ 0.5185, -0.9952,  0.5015,  1.8765, -1.7578, -1.9462,  2.6339]],
       dtype=torch.float64)
	q_value: tensor([[0.9991]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.44908332626711034, distance: 0.8493753472161921 entropy tensor([[  1.3274,   1.3757,   1.3868, -21.6069,  -0.5236, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 11, step: 15
	action: tensor([[-0.5906,  1.7736,  0.3347,  3.0285, -1.8608, -2.4461,  3.5389]],
       dtype=torch.float64)
	q_value: tensor([[0.9981]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.8493753472161921 entropy tensor([[  1.6044,   1.6915,   1.6927, -21.6069,  -0.0416, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 11, step: 16
	action: tensor([[-0.0377,  0.7469, -0.4810,  1.3555, -1.1161, -1.9590,  1.3407]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.7726404841380191, distance: 0.5456488712853836 entropy tensor([[  0.9872,   1.0709,   1.0667, -21.6069,  -4.2984, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 11, step: 17
	action: tensor([[ 0.3823,  1.1020,  0.4890,  1.9766, -1.8799, -1.9459,  2.8020]],
       dtype=torch.float64)
	q_value: tensor([[0.9997]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.6988582161794807, distance: 0.6279747801893918 entropy tensor([[  1.3665,   1.4246,   1.4276, -21.6069,  -0.6174, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 11, step: 18
	action: tensor([[-0.4021,  3.1530, -0.1222,  3.1907, -2.5553, -2.7227,  4.2657]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.6279747801893918 entropy tensor([[  1.7079,   1.8418,   1.8278, -21.6069,  -0.5301, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 11, step: 19
	action: tensor([[-1.0429,  1.1753, -1.0021,  1.3555, -1.1120, -1.9590,  1.3407]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.44008491575190245, distance: 0.8562838973935409 entropy tensor([[  0.9872,   1.0709,   1.0667, -21.6069,  -4.2984, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 11, step: 20
	action: tensor([[ 0.6771,  0.0153,  0.9855,  1.9829, -1.9939, -2.2109,  2.9274]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.8562838973935409 entropy tensor([[  1.4165,   1.5241,   1.5343, -21.6069,  -0.9612, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 11, step: 21
	action: tensor([[-0.1067,  1.5583, -1.1529,  1.3555, -1.1211, -1.9590,  1.3407]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.8562838973935409 entropy tensor([[  0.9872,   1.0709,   1.0667, -21.6069,  -4.2984, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 11, step: 22
	action: tensor([[ 0.3740, -0.1946, -0.1669,  1.3555, -1.1182, -1.9590,  1.3407]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.8562838973935409 entropy tensor([[  0.9872,   1.0709,   1.0667, -21.6069,  -4.2984, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 11, step: 23
	action: tensor([[ 0.3173,  0.6511,  0.0099,  1.3555, -1.1191, -1.9590,  1.3407]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.9168945637679469, distance: 0.3298917160225185 entropy tensor([[  0.9872,   1.0709,   1.0667, -21.6069,  -4.2984, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 11, step: 24
	action: tensor([[-2.3216, -0.3731,  0.1971,  2.0735, -1.7246, -1.8596,  2.7922]],
       dtype=torch.float64)
	q_value: tensor([[0.9993]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.3298917160225185 entropy tensor([[  1.3743,   1.4017,   1.3987, -21.6069,  -0.5045, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 11, step: 25
	action: tensor([[ 0.0359,  0.6697, -0.1693,  1.3555, -1.1144, -1.9590,  1.3407]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.8629360343290221, distance: 0.423660856445957 entropy tensor([[  0.9872,   1.0709,   1.0667, -21.6069,  -4.2984, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 11, step: 26
	action: tensor([[-0.6238, -0.1717,  0.5595,  2.0353, -1.7249, -1.8815,  2.7799]],
       dtype=torch.float64)
	q_value: tensor([[0.9995]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.5577932838168483, distance: 0.7609731971829161 entropy tensor([[  1.3694,   1.4186,   1.4111, -21.6069,  -0.5008, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 11, step: 27
	action: tensor([[-1.2160,  0.8425, -1.4637,  3.1112, -2.3611, -2.5076,  3.7706]],
       dtype=torch.float64)
	q_value: tensor([[0.9998]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.7609731971829161 entropy tensor([[  1.6430,   1.8117,   1.7555, -21.6069,   0.0363, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 11, step: 28
	action: tensor([[ 0.2101,  0.3825, -1.7651,  1.3555, -1.1180, -1.9590,  1.3407]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.5767526328111823, distance: 0.744481355459932 entropy tensor([[  0.9872,   1.0709,   1.0667, -21.6069,  -4.2984, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 11, step: 29
	action: tensor([[-2.2548,  0.7863, -0.4786,  1.7725, -1.9110, -2.1895,  2.7681]],
       dtype=torch.float64)
	q_value: tensor([[0.9999]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.744481355459932 entropy tensor([[  1.3797,   1.3942,   1.4555, -21.6069,  -1.0363, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 11, step: 30
	action: tensor([[ 0.4796,  0.0925,  0.2566,  1.3555, -1.1145, -1.9590,  1.3407]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.8949157547057109, distance: 0.37095864626682834 entropy tensor([[  0.9872,   1.0709,   1.0667, -21.6069,  -4.2984, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 11, step: 31
	action: tensor([[-2.5539, -1.2911, -1.6313,  2.1175, -1.5904, -1.7891,  2.6748]],
       dtype=torch.float64)
	q_value: tensor([[0.9968]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.37095864626682834 entropy tensor([[  1.3316,   1.3586,   1.3595, -21.6069,  -0.2806, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 11, step: 32
	action: tensor([[-0.3728,  1.1453,  0.5908,  1.3555, -1.1164, -1.9590,  1.3407]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.8241391196457906, distance: 0.4798895740994877 entropy tensor([[  0.9872,   1.0709,   1.0667, -21.6069,  -4.2984, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 11, step: 33
	action: tensor([[ 0.6097,  1.9748, -1.6665,  2.2621, -1.6534, -1.8779,  2.9441]],
       dtype=torch.float64)
	q_value: tensor([[0.9996]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.4798895740994877 entropy tensor([[  1.4510,   1.4954,   1.4810, -21.6069,  -0.5004, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 11, step: 34
	action: tensor([[ 1.2610, -0.0989,  0.1734,  1.3555, -1.1198, -1.9590,  1.3407]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.6052989896712117, distance: 0.7189369622747735 entropy tensor([[  0.9872,   1.0709,   1.0667, -21.6069,  -4.2984, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 11, step: 35
	action: tensor([[ 0.3165,  0.9650,  0.0644,  2.1043, -1.3755, -1.8554,  2.7039]],
       dtype=torch.float64)
	q_value: tensor([[0.9962]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.6989724542706158, distance: 0.6278556578210824 entropy tensor([[  1.3419,   1.3148,   1.3414, -21.6069,  -0.3668, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 11, step: 36
	action: tensor([[-2.3726,  0.4701,  1.1358,  2.8991, -2.1752, -2.6099,  4.0024]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.6278556578210824 entropy tensor([[  1.6223,   1.7657,   1.7623, -21.6069,  -0.3295, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 11, step: 37
	action: tensor([[-1.1593,  1.1080, -0.1973,  1.3555, -1.1112, -1.9590,  1.3407]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.5809100558446905, distance: 0.7408159314811068 entropy tensor([[  0.9872,   1.0709,   1.0667, -21.6069,  -4.2984, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 11, step: 38
	action: tensor([[ 1.4711,  2.3709, -0.8437,  2.1527, -1.8617, -2.0504,  2.9363]],
       dtype=torch.float64)
	q_value: tensor([[0.9999]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.7408159314811068 entropy tensor([[  1.4446,   1.5385,   1.5167, -21.6069,  -0.4653, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 11, step: 39
	action: tensor([[ 0.2032,  2.1711,  0.5340,  1.3555, -1.1146, -1.9590,  1.3407]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.7408159314811068 entropy tensor([[  0.9872,   1.0709,   1.0667, -21.6069,  -4.2984, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 11, step: 40
	action: tensor([[ 0.5547,  0.1105,  0.3782,  1.3555, -1.1146, -1.9590,  1.3407]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.7408159314811068 entropy tensor([[  0.9872,   1.0709,   1.0667, -21.6069,  -4.2984, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 11, step: 41
	action: tensor([[ 0.0930,  0.4652, -0.1667,  1.3555, -1.1172, -1.9590,  1.3407]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.8919161750238739, distance: 0.3762158132488324 entropy tensor([[  0.9872,   1.0709,   1.0667, -21.6069,  -4.2984, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 11, step: 42
	action: tensor([[-1.3823,  1.5639, -0.1100,  2.0241, -1.4795, -1.8568,  2.7246]],
       dtype=torch.float64)
	q_value: tensor([[0.9992]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.3762158132488324 entropy tensor([[  1.3519,   1.3984,   1.3907, -21.6069,  -0.4405, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 11, step: 43
	action: tensor([[ 0.8592,  1.2988, -0.6333,  1.3555, -1.1133, -1.9590,  1.3407]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.45804779237381044, distance: 0.8424365252003485 entropy tensor([[  0.9872,   1.0709,   1.0667, -21.6069,  -4.2984, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 11, step: 44
	action: tensor([[ 0.5225,  1.0691,  1.1792,  1.9970, -1.8612, -2.1158,  3.0542]],
       dtype=torch.float64)
	q_value: tensor([[0.9999]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.7675280082950892, distance: 0.5517495803200777 entropy tensor([[  1.4156,   1.4174,   1.4760, -21.6069,  -1.0683, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 11, step: 45
	action: tensor([[-0.4809,  0.5024, -1.7143,  3.5481, -2.3558, -2.8136,  4.5017]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.5517495803200777 entropy tensor([[  1.7579,   1.9088,   1.8868, -21.6069,  -0.4226, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 11, step: 46
	action: tensor([[-0.1485, -0.7608,  0.3809,  1.3555, -1.1192, -1.9590,  1.3407]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.3764909460642325, distance: 0.9036038556667854 entropy tensor([[  0.9872,   1.0709,   1.0667, -21.6069,  -4.2984, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 11, step: 47
	action: tensor([[ 0.5579,  1.5015,  0.5705,  2.1596, -1.3094, -1.7356,  2.4748]],
       dtype=torch.float64)
	q_value: tensor([[0.9725]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.6056838245770124, distance: 0.7185863937395194 entropy tensor([[  1.3113,   1.3409,   1.3445, -21.6069,   0.0393, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 11, step: 48
	action: tensor([[-2.6889,  1.8750,  1.4997,  2.9421, -1.9658, -2.5710,  4.1091]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.7185863937395194 entropy tensor([[  1.6552,   1.7621,   1.7605, -21.6069,  -0.6054, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 11, step: 49
	action: tensor([[-0.1626,  0.5542, -0.7137,  1.3555, -1.1201, -1.9590,  1.3407]],
       dtype=torch.float64)
	q_value: tensor([[1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.6930071161634763, distance: 0.6340461174987584 entropy tensor([[  0.9872,   1.0709,   1.0667, -21.6069,  -4.2984, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
LOSS epoch 11 actor 572.2747098884564 critic 2300.833318433063
epoch: 12, step: 0
	action: tensor([[ 0.0331,  0.2286, -0.6323,  1.8735, -2.0149, -1.6329,  3.0672]],
       dtype=torch.float64)
	q_value: tensor([[0.7380]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.6560623298835004, distance: 0.6711144127403911 entropy tensor([[  1.4007,   1.4908,   1.5342, -21.6069,   0.1276, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 12, step: 1
	action: tensor([[ 2.8433,  0.4898, -0.5953,  2.9878, -2.9658, -2.3742,  4.4792]],
       dtype=torch.float64)
	q_value: tensor([[0.7997]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.6711144127403911 entropy tensor([[  1.7418,   1.8811,   1.9377, -21.6069,   0.3936, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 12, step: 2
	action: tensor([[-0.4857, -0.5581, -1.0684,  1.1479, -1.0391, -1.9079,  2.1183]],
       dtype=torch.float64)
	q_value: tensor([[0.9999]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.004764298527792699, distance: 1.1470670136658359 entropy tensor([[  1.1284,   1.2598,   1.1943, -21.6069,  -0.6773, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 12, step: 3
	action: tensor([[-1.6720, -0.2972, -0.0271,  2.0998, -1.4797, -1.7679,  3.1065]],
       dtype=torch.float64)
	q_value: tensor([[0.1584]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.1470670136658359 entropy tensor([[  1.4249,   1.5489,   1.5658, -21.6069,   0.2427, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 12, step: 4
	action: tensor([[ 0.2953, -0.2759, -0.8314,  1.1479, -1.1682, -1.9079,  2.1183]],
       dtype=torch.float64)
	q_value: tensor([[0.9999]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.7979447231810205, distance: 0.5143891408110832 entropy tensor([[  1.1284,   1.2598,   1.1943, -21.6069,  -0.6773, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 12, step: 5
	action: tensor([[-0.4221, -0.0566, -1.9285,  2.1329, -1.5199, -1.7444,  3.1347]],
       dtype=torch.float64)
	q_value: tensor([[0.2731]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.024564331903230707, distance: 1.1583140110631107 entropy tensor([[  1.4030,   1.5284,   1.5616, -21.6069,   0.2857, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 12, step: 6
	action: tensor([[ 0.9275,  2.1056,  0.1791,  0.8892, -1.8792, -1.4212,  2.6456]],
       dtype=torch.float64)
	q_value: tensor([[-0.7087]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.1583140110631107 entropy tensor([[  1.4957,   1.0483,   1.2624,  -3.6540,  -1.5726, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 12, step: 7
	action: tensor([[ 0.2295, -1.8428,  1.0952,  1.1479, -1.1416, -1.9079,  2.1183]],
       dtype=torch.float64)
	q_value: tensor([[0.9999]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.1583140110631107 entropy tensor([[  1.1284,   1.2598,   1.1943, -21.6069,  -0.6773, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 12, step: 8
	action: tensor([[-0.3089, -0.1295, -0.7345,  1.1479, -1.1762, -1.9079,  2.1183]],
       dtype=torch.float64)
	q_value: tensor([[0.9999]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.48410379800345027, distance: 0.8219357275511535 entropy tensor([[  1.1284,   1.2598,   1.1943, -21.6069,  -0.6773, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 12, step: 9
	action: tensor([[-0.2216, -1.1767, -0.7079,  2.1724, -1.7186, -1.7538,  3.1823]],
       dtype=torch.float64)
	q_value: tensor([[0.4579]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.8219357275511535 entropy tensor([[  1.4271,   1.5738,   1.5881, -21.6069,   0.2427, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 12, step: 10
	action: tensor([[-0.5017,  1.4905, -1.7351,  1.1479, -1.0736, -1.9079,  2.1183]],
       dtype=torch.float64)
	q_value: tensor([[0.9999]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.2254099541999528, distance: 1.0071465028825506 entropy tensor([[  1.1284,   1.2598,   1.1943, -21.6069,  -0.6773, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 12, step: 11
	action: tensor([[ 0.1334,  2.8315, -5.1053,  2.1252, -2.6501, -2.2982,  3.9372]],
       dtype=torch.float64)
	q_value: tensor([[0.9984]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.0071465028825506 entropy tensor([[  1.5445,   1.7605,   1.8310, -21.6069,   0.1562, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 12, step: 12
	action: tensor([[-0.9968,  0.5458, -0.3178,  1.1479, -1.2583, -1.9079,  2.1183]],
       dtype=torch.float64)
	q_value: tensor([[0.9999]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.38639478588779375, distance: 0.8963986915966502 entropy tensor([[  1.1284,   1.2598,   1.1943, -21.6069,  -0.6773, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 12, step: 13
	action: tensor([[ 1.4516,  0.2755, -0.1252,  2.3807, -2.2066, -1.8795,  3.4847]],
       dtype=torch.float64)
	q_value: tensor([[0.8890]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.8963986915966502 entropy tensor([[  1.5185,   1.7130,   1.7036, -21.6069,   0.2630, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 12, step: 14
	action: tensor([[-0.4949, -0.2219, -0.4554,  1.1479, -1.2929, -1.9079,  2.1183]],
       dtype=torch.float64)
	q_value: tensor([[0.9999]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.3998291213293971, distance: 0.886531461753566 entropy tensor([[  1.1284,   1.2598,   1.1943, -21.6069,  -0.6773, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 12, step: 15
	action: tensor([[ 0.6776,  1.6373, -1.5684,  2.2941, -2.0627, -1.7451,  3.2066]],
       dtype=torch.float64)
	q_value: tensor([[0.1204]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.886531461753566 entropy tensor([[  1.4547,   1.6086,   1.6087, -21.6069,   0.2902, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 12, step: 16
	action: tensor([[-0.4982,  0.6198, -0.8361,  1.1479, -1.2024, -1.9079,  2.1183]],
       dtype=torch.float64)
	q_value: tensor([[0.9999]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.5342527460446876, distance: 0.7809654926058899 entropy tensor([[  1.1284,   1.2598,   1.1943, -21.6069,  -0.6773, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 12, step: 17
	action: tensor([[-0.8789,  2.1424,  0.0690,  2.1788, -2.0287, -1.8965,  3.4532]],
       dtype=torch.float64)
	q_value: tensor([[0.9409]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.7809654926058899 entropy tensor([[  1.4732,   1.6575,   1.6804, -21.6069,   0.1737, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 12, step: 18
	action: tensor([[ 0.1178, -0.5521, -1.6335,  1.1479, -1.1693, -1.9079,  2.1183]],
       dtype=torch.float64)
	q_value: tensor([[0.9999]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.3367637209545252, distance: 0.9319461752908492 entropy tensor([[  1.1284,   1.2598,   1.1943, -21.6069,  -0.6773, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 12, step: 19
	action: tensor([[-0.4395, -0.3781,  0.3550,  1.9608, -1.5887, -1.8487,  3.1685]],
       dtype=torch.float64)
	q_value: tensor([[0.3319]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.6368780600972206, distance: 0.6895772707354892 entropy tensor([[  1.4403,   1.5057,   1.5805, -21.6069,   0.1946, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 12, step: 20
	action: tensor([[ 0.9581, -0.2010,  0.4129,  0.8049, -1.5393, -0.8427,  2.2370]],
       dtype=torch.float64)
	q_value: tensor([[-0.9172]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.6895772707354892 entropy tensor([[  1.4386,   0.8955,   1.0711, -21.6069,  -0.5664, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 12, step: 21
	action: tensor([[ 1.0514,  1.4273, -2.1922,  1.1479, -1.3392, -1.9079,  2.1183]],
       dtype=torch.float64)
	q_value: tensor([[0.9999]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.4272082574607712, distance: 0.8660741325471439 entropy tensor([[  1.1284,   1.2598,   1.1943, -21.6069,  -0.6773, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 12, step: 22
	action: tensor([[ 0.1660,  2.0421, -2.0995,  2.0225, -2.4614, -2.3601,  4.0174]],
       dtype=torch.float64)
	q_value: tensor([[0.9938]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.8660741325471439 entropy tensor([[  1.6026,   1.6798,   1.8156, -21.6069,   0.1398, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 12, step: 23
	action: tensor([[ 0.2551,  0.4697,  0.4360,  1.1479, -1.3470, -1.9079,  2.1183]],
       dtype=torch.float64)
	q_value: tensor([[0.9999]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.9312127684289133, distance: 0.30013082968800964 entropy tensor([[  1.1284,   1.2598,   1.1943, -21.6069,  -0.6773, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 12, step: 24
	action: tensor([[-1.1653,  0.4616,  1.4768,  2.5503, -2.1228, -1.7728,  3.4026]],
       dtype=torch.float64)
	q_value: tensor([[0.3537]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.30013082968800964 entropy tensor([[  1.5056,   1.6421,   1.6432, -21.6069,   0.2381, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 12, step: 25
	action: tensor([[ 0.9641, -0.3902,  0.4348,  1.1479, -1.4008, -1.9079,  2.1183]],
       dtype=torch.float64)
	q_value: tensor([[0.9999]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.5104023367800977, distance: 0.8007120447058529 entropy tensor([[  1.1284,   1.2598,   1.1943, -21.6069,  -0.6773, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 12, step: 26
	action: tensor([[ 0.9203,  0.4684,  0.6564,  2.5900, -1.2907, -1.8250,  3.2426]],
       dtype=torch.float64)
	q_value: tensor([[-0.7363]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.8007120447058529 entropy tensor([[  1.5034,   1.5607,   1.5965, -21.6069,   0.3064, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 12, step: 27
	action: tensor([[-0.3187,  0.9672, -0.7389,  1.1479, -1.1606, -1.9079,  2.1183]],
       dtype=torch.float64)
	q_value: tensor([[0.9999]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.6289670217722056, distance: 0.6970484237555874 entropy tensor([[  1.1284,   1.2598,   1.1943, -21.6069,  -0.6773, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 12, step: 28
	action: tensor([[ 1.5898,  3.9856,  2.3508,  2.2083, -2.2535, -1.9254,  3.5484]],
       dtype=torch.float64)
	q_value: tensor([[0.9728]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.6970484237555874 entropy tensor([[  1.4942,   1.6725,   1.7126, -21.6069,   0.1593, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 12, step: 29
	action: tensor([[-0.7892,  1.9066,  1.2771,  1.1479, -1.1723, -1.9079,  2.1183]],
       dtype=torch.float64)
	q_value: tensor([[0.9999]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.6970484237555874 entropy tensor([[  1.1284,   1.2598,   1.1943, -21.6069,  -0.6773, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 12, step: 30
	action: tensor([[ 0.2209, -0.0473, -0.7761,  1.1479, -1.2607, -1.9079,  2.1183]],
       dtype=torch.float64)
	q_value: tensor([[0.9999]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.8337679912522448, distance: 0.4665670016904825 entropy tensor([[  1.1284,   1.2598,   1.1943, -21.6069,  -0.6773, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 12, step: 31
	action: tensor([[ 2.7995,  1.8697,  1.3217,  2.1381, -1.7438, -1.7528,  3.1896]],
       dtype=torch.float64)
	q_value: tensor([[0.4738]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.4665670016904825 entropy tensor([[  1.4285,   1.5479,   1.5834, -21.6069,   0.2603, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 12, step: 32
	action: tensor([[ 0.7329,  0.4377, -1.1958,  1.1479, -1.2667, -1.9079,  2.1183]],
       dtype=torch.float64)
	q_value: tensor([[0.9999]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.8748259470714923, distance: 0.40486837108529533 entropy tensor([[  1.1284,   1.2598,   1.1943, -21.6069,  -0.6773, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 12, step: 33
	action: tensor([[-1.2671,  0.5313, -0.1575,  2.0592, -2.1699, -1.9015,  3.4189]],
       dtype=torch.float64)
	q_value: tensor([[0.8563]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.5072992616306997, distance: 0.8032454974728589 entropy tensor([[  1.4726,   1.5687,   1.6512, -21.6069,   0.2334, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 12, step: 34
	action: tensor([[-1.0131, -0.3604,  1.0105,  1.0227, -2.0020, -1.2274,  2.7787]],
       dtype=torch.float64)
	q_value: tensor([[-0.7564]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.1108583544008862, distance: 1.079051491490297 entropy tensor([[  1.6142,   1.1623,   1.3653, -21.6069,  -0.3676, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 12, step: 35
	action: tensor([[-0.0861,  1.6577, -0.9955,  3.2140, -2.2241, -2.0375,  3.7800]],
       dtype=torch.float64)
	q_value: tensor([[-0.1013]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.079051491490297 entropy tensor([[  1.6521,   1.7938,   1.7965, -21.6069,   0.3086, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 12, step: 36
	action: tensor([[ 0.0606,  0.6795, -0.7530,  1.1479, -1.2603, -1.9079,  2.1183]],
       dtype=torch.float64)
	q_value: tensor([[0.9999]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.7525358455578215, distance: 0.5692628504271743 entropy tensor([[  1.1284,   1.2598,   1.1943, -21.6069,  -0.6773, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 12, step: 37
	action: tensor([[-1.3853,  0.9643,  1.6629,  2.1857, -2.2463, -1.8563,  3.4486]],
       dtype=torch.float64)
	q_value: tensor([[0.9141]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.07689694366953015, distance: 1.1875277459353455 entropy tensor([[  1.4806,   1.6274,   1.6719, -21.6069,   0.1955, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 12, step: 38
	action: tensor([[-0.5679,  0.4674, -1.0069,  1.1709, -1.9563, -0.9719,  2.9837]],
       dtype=torch.float64)
	q_value: tensor([[-0.7717]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.03677407404099975, distance: 1.1231060713597976 entropy tensor([[  1.7403,   1.2822,   1.4683, -21.6069,  -0.1249, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 12, step: 39
	action: tensor([[-1.8536,  1.6633,  0.9017,  2.6691, -2.3828, -2.3179,  4.1503]],
       dtype=torch.float64)
	q_value: tensor([[0.9902]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.1231060713597976 entropy tensor([[  1.6469,   1.8193,   1.8819, -21.6069,   0.1143, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 12, step: 40
	action: tensor([[ 0.9630, -1.3168,  0.3430,  1.1479, -1.1958, -1.9079,  2.1183]],
       dtype=torch.float64)
	q_value: tensor([[0.9999]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.1231060713597976 entropy tensor([[  1.1284,   1.2598,   1.1943, -21.6069,  -0.6773, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 12, step: 41
	action: tensor([[ 0.6592,  0.3808,  0.1598,  1.1479, -1.0068, -1.9079,  2.1183]],
       dtype=torch.float64)
	q_value: tensor([[0.9999]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.1231060713597976 entropy tensor([[  1.1284,   1.2598,   1.1943, -21.6069,  -0.6773, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 12, step: 42
	action: tensor([[ 0.5525,  0.6856, -0.5413,  1.1479, -1.3869, -1.9079,  2.1183]],
       dtype=torch.float64)
	q_value: tensor([[0.9999]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.8165854480363332, distance: 0.49008745736379195 entropy tensor([[  1.1284,   1.2598,   1.1943, -21.6069,  -0.6773, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 12, step: 43
	action: tensor([[-0.2448, -0.5563, -0.6081,  2.2784, -1.6035, -1.8688,  3.5167]],
       dtype=torch.float64)
	q_value: tensor([[0.8411]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.49008745736379195 entropy tensor([[  1.5114,   1.6213,   1.6730, -21.6069,   0.1964, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 12, step: 44
	action: tensor([[-0.0023, -0.4926,  0.7004,  1.1479, -1.1985, -1.9079,  2.1183]],
       dtype=torch.float64)
	q_value: tensor([[0.9999]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.4368826174973537, distance: 0.8587290600787645 entropy tensor([[  1.1284,   1.2598,   1.1943, -21.6069,  -0.6773, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 12, step: 45
	action: tensor([[-0.2825,  1.2971,  0.6347,  2.6464, -1.9975, -1.7025,  3.1117]],
       dtype=torch.float64)
	q_value: tensor([[-0.7218]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.8587290600787645 entropy tensor([[  1.4598,   1.5961,   1.5942, -21.6069,   0.4005, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 12, step: 46
	action: tensor([[-0.5386,  0.0246, -0.5663,  1.1479, -1.1524, -1.9079,  2.1183]],
       dtype=torch.float64)
	q_value: tensor([[0.9999]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.4007848861756411, distance: 0.8858252851960359 entropy tensor([[  1.1284,   1.2598,   1.1943, -21.6069,  -0.6773, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 12, step: 47
	action: tensor([[ 0.4844,  0.7901, -0.0372,  2.2325, -2.0638, -1.7724,  3.2438]],
       dtype=torch.float64)
	q_value: tensor([[0.6022]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.5531463383593778, distance: 0.7649611051373577 entropy tensor([[  1.4383,   1.6155,   1.6124, -21.6069,   0.2573, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 12, step: 48
	action: tensor([[-0.5478,  1.0301, -0.2082,  0.9638, -1.6697, -1.0836,  2.7184]],
       dtype=torch.float64)
	q_value: tensor([[-0.7310]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.6419698225308565, distance: 0.6847255131572798 entropy tensor([[  1.5543,   1.0314,   1.2509,  -2.6703,  -1.1066, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 12, step: 49
	action: tensor([[-1.2000,  4.3258, -0.1176,  2.6620, -2.4510, -2.1145,  3.9438]],
       dtype=torch.float64)
	q_value: tensor([[0.9941]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.6847255131572798 entropy tensor([[  1.5960,   1.7822,   1.8240, -21.6069,   0.0670, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
LOSS epoch 12 actor 586.0958893543299 critic 2318.1342720637285
epoch: 13, step: 0
	action: tensor([[-1.6591, -0.4823, -1.0695,  0.9052, -1.3886, -1.7318,  2.4297]],
       dtype=torch.float64)
	q_value: tensor([[-0.8414]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.6847255131572798 entropy tensor([[  1.1695,   1.3313,   1.2153,  -1.6424,  -0.1905, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 13, step: 1
	action: tensor([[-1.1757,  0.3131, -0.4381,  0.9061, -1.0968, -1.7318,  2.4297]],
       dtype=torch.float64)
	q_value: tensor([[-0.8414]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.03323204831519866, distance: 1.163203305064345 entropy tensor([[  1.1695,   1.3313,   1.2153,  -1.6424,  -0.1905, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 13, step: 2
	action: tensor([[-0.6725,  1.6429, -0.6295,  2.3578, -3.2719, -2.2187,  3.7143]],
       dtype=torch.float64)
	q_value: tensor([[-0.9996]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.163203305064345 entropy tensor([[  1.6118,   1.7667,   1.7887,  -0.7073,   0.7217, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 13, step: 3
	action: tensor([[ 0.7664,  0.5618, -0.2619,  0.8900, -1.3146, -1.7318,  2.4297]],
       dtype=torch.float64)
	q_value: tensor([[-0.8414]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.897246703768543, distance: 0.36682132439623 entropy tensor([[  1.1695,   1.3313,   1.2153,  -1.6424,  -0.1905, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 13, step: 4
	action: tensor([[-1.0097, -0.9508,  0.2787,  2.4901, -2.0363, -2.2144,  3.7473]],
       dtype=torch.float64)
	q_value: tensor([[-0.9998]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.36682132439623 entropy tensor([[  1.6295,   1.6802,   1.7618,  -2.1972,   0.7030, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 13, step: 5
	action: tensor([[-1.1659,  0.9708,  0.2600,  0.8544, -1.2710, -1.7318,  2.4297]],
       dtype=torch.float64)
	q_value: tensor([[-0.8414]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.40886293658592043, distance: 0.8798340962088744 entropy tensor([[  1.1695,   1.3313,   1.2153,  -1.6424,  -0.1905, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 13, step: 6
	action: tensor([[-0.9631, -0.4081, -1.0801,  2.7768, -2.1386, -2.2903,  3.9944]],
       dtype=torch.float64)
	q_value: tensor([[-0.9991]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.8798340962088744 entropy tensor([[  1.6888,   1.8296,   1.8729,  -0.6057,   0.7701, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 13, step: 7
	action: tensor([[-0.1467, -0.7533, -1.1621,  0.8839, -1.1785, -1.7318,  2.4297]],
       dtype=torch.float64)
	q_value: tensor([[-0.8414]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.15118711503412774, distance: 1.0542962821505657 entropy tensor([[  1.1695,   1.3313,   1.2153,  -1.6424,  -0.1905, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 13, step: 8
	action: tensor([[ 1.9568,  2.8730, -0.8474,  2.1808, -1.7282, -2.0975,  3.3643]],
       dtype=torch.float64)
	q_value: tensor([[-0.9999]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.0542962821505657 entropy tensor([[  1.5443,   1.5986,   1.6670,  -1.3735,   0.6924, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 13, step: 9
	action: tensor([[ 0.8285,  0.9730,  1.3906,  0.8693, -1.1336, -1.7318,  2.4297]],
       dtype=torch.float64)
	q_value: tensor([[-0.8414]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.928219676460096, distance: 0.30659099793234695 entropy tensor([[  1.1695,   1.3313,   1.2153,  -1.6424,  -0.1905, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 13, step: 10
	action: tensor([[ 0.7074,  2.8942, -2.4759,  2.9602, -1.7024, -2.3210,  3.9173]],
       dtype=torch.float64)
	q_value: tensor([[-0.9997]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.30659099793234695 entropy tensor([[  1.7299,   1.7513,   1.7966,  -1.1967,   0.6741, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 13, step: 11
	action: tensor([[-0.2019, -0.1996,  0.9534,  0.9363, -1.1669, -1.7318,  2.4297]],
       dtype=torch.float64)
	q_value: tensor([[-0.8414]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.35347971788663823, distance: 0.9201269903060108 entropy tensor([[  1.1695,   1.3313,   1.2153,  -1.6424,  -0.1905, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 13, step: 12
	action: tensor([[ 3.3936,  3.6599, -3.1557,  3.0318, -1.4747, -2.1255,  3.5179]],
       dtype=torch.float64)
	q_value: tensor([[-0.9995]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.9201269903060108 entropy tensor([[  1.6054,   1.7148,   1.7343,  -0.7922,   0.7870, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 13, step: 13
	action: tensor([[ 0.1339,  3.2178,  0.6561,  0.9333, -0.9749, -1.7318,  2.4297]],
       dtype=torch.float64)
	q_value: tensor([[-0.8414]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.9201269903060108 entropy tensor([[  1.1695,   1.3313,   1.2153,  -1.6424,  -0.1905, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 13, step: 14
	action: tensor([[-0.5743, -1.5913, -0.9492,  0.8763, -1.3506, -1.7318,  2.4297]],
       dtype=torch.float64)
	q_value: tensor([[-0.8414]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.9201269903060108 entropy tensor([[  1.1695,   1.3313,   1.2153,  -1.6424,  -0.1905, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 13, step: 15
	action: tensor([[-0.5561, -0.8519,  0.1736,  0.9346, -1.1582, -1.7318,  2.4297]],
       dtype=torch.float64)
	q_value: tensor([[-0.8414]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.01186043766037681, distance: 1.137537800077091 entropy tensor([[  1.1695,   1.3313,   1.2153,  -1.6424,  -0.1905, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 13, step: 16
	action: tensor([[-0.6957,  2.4578, -0.1591,  2.6018, -1.0137, -2.0463,  3.3169]],
       dtype=torch.float64)
	q_value: tensor([[-0.9996]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.137537800077091 entropy tensor([[  1.5496,   1.6459,   1.6670,  -0.8117,   0.8020, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 13, step: 17
	action: tensor([[-1.1791,  0.4616, -0.3444,  0.8849, -1.1594, -1.7318,  2.4297]],
       dtype=torch.float64)
	q_value: tensor([[-0.8414]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.09830531406150245, distance: 1.086641903305086 entropy tensor([[  1.1695,   1.3313,   1.2153,  -1.6424,  -0.1905, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 13, step: 18
	action: tensor([[-0.0976,  0.8134, -0.6435,  2.5630, -2.1401, -2.2382,  3.7705]],
       dtype=torch.float64)
	q_value: tensor([[-0.9996]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.086641903305086 entropy tensor([[  1.6250,   1.7834,   1.8066,  -0.6895,   0.7236, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 13, step: 19
	action: tensor([[ 0.7550,  1.6187, -0.0225,  0.9464, -0.9372, -1.7318,  2.4297]],
       dtype=torch.float64)
	q_value: tensor([[-0.8414]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.086641903305086 entropy tensor([[  1.1695,   1.3313,   1.2153,  -1.6424,  -0.1905, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 13, step: 20
	action: tensor([[ 0.1322,  0.6191,  1.4830,  0.8570, -1.3212, -1.7318,  2.4297]],
       dtype=torch.float64)
	q_value: tensor([[-0.8414]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.086641903305086 entropy tensor([[  1.1695,   1.3313,   1.2153,  -1.6424,  -0.1905, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 13, step: 21
	action: tensor([[ 1.1517,  0.6599,  0.6988,  0.9718, -0.6347, -1.7318,  2.4297]],
       dtype=torch.float64)
	q_value: tensor([[-0.8414]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.864086264655774, distance: 0.4218794464402748 entropy tensor([[  1.1695,   1.3313,   1.2153,  -1.6424,  -0.1905, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 13, step: 22
	action: tensor([[-0.2284, -0.8419,  0.0839,  2.6513, -1.4302, -2.1867,  3.5622]],
       dtype=torch.float64)
	q_value: tensor([[-0.9997]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.4218794464402748 entropy tensor([[  1.5894,   1.6276,   1.6995,  -2.8982,   0.6740, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 13, step: 23
	action: tensor([[ 1.8735,  1.0193, -0.1566,  0.8819, -1.1692, -1.7318,  2.4297]],
       dtype=torch.float64)
	q_value: tensor([[-0.8414]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.4218794464402748 entropy tensor([[  1.1695,   1.3313,   1.2153,  -1.6424,  -0.1905, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 13, step: 24
	action: tensor([[-0.8312, -1.0455, -0.5852,  0.8819, -1.3327, -1.7318,  2.4297]],
       dtype=torch.float64)
	q_value: tensor([[-0.8414]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.34914657131181204, distance: 1.3291875362755252 entropy tensor([[  1.1695,   1.3313,   1.2153,  -1.6424,  -0.1905, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 13, step: 25
	action: tensor([[ 0.7338,  2.8138, -1.5615,  2.3394, -1.8984, -2.1316,  3.4235]],
       dtype=torch.float64)
	q_value: tensor([[-0.9998]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.3291875362755252 entropy tensor([[  1.5969,   1.6577,   1.6922,  -0.9081,   0.7440, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 13, step: 26
	action: tensor([[-0.6268,  1.9104, -1.8638,  0.9769, -1.2111, -1.7318,  2.4297]],
       dtype=torch.float64)
	q_value: tensor([[-0.8414]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.3291875362755252 entropy tensor([[  1.1695,   1.3313,   1.2153,  -1.6424,  -0.1905, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 13, step: 27
	action: tensor([[-0.1275, -0.4087, -0.2472,  0.9561, -1.2349, -1.7318,  2.4297]],
       dtype=torch.float64)
	q_value: tensor([[-0.8414]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.5699053923200375, distance: 0.7504792554234286 entropy tensor([[  1.1695,   1.3313,   1.2153,  -1.6424,  -0.1905, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 13, step: 28
	action: tensor([[ 1.8639,  0.7881,  0.3545,  2.5171, -1.8086, -2.0507,  3.3847]],
       dtype=torch.float64)
	q_value: tensor([[-0.9997]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.7504792554234286 entropy tensor([[  1.5396,   1.6432,   1.6755,  -1.2408,   0.7246, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 13, step: 29
	action: tensor([[-0.3493, -0.0457, -0.1006,  0.9078, -0.9822, -1.7318,  2.4297]],
       dtype=torch.float64)
	q_value: tensor([[-0.8414]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.4759469335862465, distance: 0.828408080647215 entropy tensor([[  1.1695,   1.3313,   1.2153,  -1.6424,  -0.1905, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 13, step: 30
	action: tensor([[-0.4022,  0.8137, -0.1379,  2.4305, -2.5804, -2.0370,  3.4126]],
       dtype=torch.float64)
	q_value: tensor([[-0.9996]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.828408080647215 entropy tensor([[  1.5241,   1.6789,   1.6911,  -0.9550,   0.7303, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 13, step: 31
	action: tensor([[ 1.2488, -0.0442, -0.9436,  0.8987, -1.2515, -1.7318,  2.4297]],
       dtype=torch.float64)
	q_value: tensor([[-0.8414]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.8721990802692794, distance: 0.40909453971881615 entropy tensor([[  1.1695,   1.3313,   1.2153,  -1.6424,  -0.1905, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 13, step: 32
	action: tensor([[-0.0440,  3.4752,  0.4685,  2.2068, -2.5049, -2.1871,  3.5626]],
       dtype=torch.float64)
	q_value: tensor([[-0.9999]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.40909453971881615 entropy tensor([[  1.5845,   1.5718,   1.6976,  -2.3544,   0.7546, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 13, step: 33
	action: tensor([[-1.3468, -1.4492,  1.1870,  0.8437, -1.3047, -1.7318,  2.4297]],
       dtype=torch.float64)
	q_value: tensor([[-0.8414]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.40909453971881615 entropy tensor([[  1.1695,   1.3313,   1.2153,  -1.6424,  -0.1905, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 13, step: 34
	action: tensor([[ 0.3013, -1.6923,  0.6688,  0.9263, -1.4424, -1.7318,  2.4297]],
       dtype=torch.float64)
	q_value: tensor([[-0.8414]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.40909453971881615 entropy tensor([[  1.1695,   1.3313,   1.2153,  -1.6424,  -0.1905, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 13, step: 35
	action: tensor([[-0.2546,  1.0154, -1.2031,  0.9337, -1.6219, -1.7318,  2.4297]],
       dtype=torch.float64)
	q_value: tensor([[-0.8414]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.38892932512637557, distance: 0.8945454571529896 entropy tensor([[  1.1695,   1.3313,   1.2153,  -1.6424,  -0.1905, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 13, step: 36
	action: tensor([[ 0.4671,  0.5101, -1.7498,  2.3628, -2.1261, -2.4381,  4.1576]],
       dtype=torch.float64)
	q_value: tensor([[-0.9997]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.8945454571529896 entropy tensor([[  1.7034,   1.7883,   1.8963,  -1.4839,   0.6896, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 13, step: 37
	action: tensor([[-0.2380, -1.1513,  1.7375,  0.8908, -1.2418, -1.7318,  2.4297]],
       dtype=torch.float64)
	q_value: tensor([[-0.8414]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.41105982952328746, distance: 1.3593441028669768 entropy tensor([[  1.1695,   1.3313,   1.2153,  -1.6424,  -0.1905, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 13, step: 38
	action: tensor([[ 0.1464,  2.2701, -0.4975,  3.2582, -1.0924, -2.2213,  3.4929]],
       dtype=torch.float64)
	q_value: tensor([[-0.9996]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.3593441028669768 entropy tensor([[  1.6739,   1.6789,   1.7539,  -0.6987,   0.8978, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 13, step: 39
	action: tensor([[-0.0682,  1.5641,  0.2576,  0.8559, -1.3270, -1.7318,  2.4297]],
       dtype=torch.float64)
	q_value: tensor([[-0.8414]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.3593441028669768 entropy tensor([[  1.1695,   1.3313,   1.2153,  -1.6424,  -0.1905, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 13, step: 40
	action: tensor([[-0.8385,  1.4332, -0.7189,  0.8850, -1.2816, -1.7318,  2.4297]],
       dtype=torch.float64)
	q_value: tensor([[-0.8414]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.6222250981647356, distance: 0.7033528376930498 entropy tensor([[  1.1695,   1.3313,   1.2153,  -1.6424,  -0.1905, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 13, step: 41
	action: tensor([[-2.1797,  1.3937,  1.0628,  2.4339, -2.9371, -2.4437,  4.2085]],
       dtype=torch.float64)
	q_value: tensor([[-0.9990]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.7033528376930498 entropy tensor([[  1.6879,   1.8432,   1.9252,  -0.9079,   0.7388, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 13, step: 42
	action: tensor([[ 0.0956,  0.9328, -0.6336,  0.7394, -1.0003, -1.7318,  2.4297]],
       dtype=torch.float64)
	q_value: tensor([[-0.8414]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.8098176049142858, distance: 0.49904745998403033 entropy tensor([[  1.1695,   1.3313,   1.2153,  -1.6424,  -0.1905, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 13, step: 43
	action: tensor([[-1.0395,  0.0306, -0.1383,  2.2750, -2.7736, -2.1678,  3.6840]],
       dtype=torch.float64)
	q_value: tensor([[-0.9994]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.49904745998403033 entropy tensor([[  1.5709,   1.6945,   1.7801,  -1.2302,   0.6562, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 13, step: 44
	action: tensor([[-0.4332, -0.1244,  0.7334,  0.8772, -1.2136, -1.7318,  2.4297]],
       dtype=torch.float64)
	q_value: tensor([[-0.8414]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.30397232244063344, distance: 0.9547066510186532 entropy tensor([[  1.1695,   1.3313,   1.2153,  -1.6424,  -0.1905, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 13, step: 45
	action: tensor([[-0.2478,  2.2600,  1.3520,  2.6856, -2.4545, -2.0904,  3.5012]],
       dtype=torch.float64)
	q_value: tensor([[-0.9994]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.9547066510186532 entropy tensor([[  1.5958,   1.7139,   1.7280,  -0.7955,   0.7810, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 13, step: 46
	action: tensor([[ 0.6246,  0.9100,  0.8629,  0.8342, -0.9830, -1.7318,  2.4297]],
       dtype=torch.float64)
	q_value: tensor([[-0.8414]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.9739842075009864, distance: 0.18457599712884012 entropy tensor([[  1.1695,   1.3313,   1.2153,  -1.6424,  -0.1905, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 13, step: 47
	action: tensor([[ 1.8207,  2.5081, -1.5119,  2.7768, -2.4349, -2.2236,  3.7588]],
       dtype=torch.float64)
	q_value: tensor([[-0.9996]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.18457599712884012 entropy tensor([[  1.6506,   1.7118,   1.7669,  -1.2694,   0.6751, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 13, step: 48
	action: tensor([[ 0.5124,  0.1297, -0.2186,  0.9080, -1.2250, -1.7318,  2.4297]],
       dtype=torch.float64)
	q_value: tensor([[-0.8414]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.9601039339190375, distance: 0.22857131694254845 entropy tensor([[  1.1695,   1.3313,   1.2153,  -1.6424,  -0.1905, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 13, step: 49
	action: tensor([[-1.2448,  0.6138, -2.0384,  2.4203, -1.9345, -2.1173,  3.5376]],
       dtype=torch.float64)
	q_value: tensor([[-0.9997]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.22857131694254845 entropy tensor([[  1.5773,   1.6524,   1.7131,  -1.8920,   0.7169, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
LOSS epoch 13 actor 722.2999521908732 critic 2282.423472438794
epoch: 14, step: 0
	action: tensor([[-0.4100,  0.2321, -1.2241,  0.9389, -1.3241, -2.1550,  2.3223]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.4406580736905342, distance: 0.8558455170122216 entropy tensor([[ 1.3367e+00,  1.3860e+00,  1.2699e+00,  1.3814e-03,  6.0512e-02,
         -2.1607e+01, -2.1607e+01]], dtype=torch.float64,
       grad_fn=<AddBackward0>)
epoch: 14, step: 1
	action: tensor([[-1.1876, -0.2519, -0.6660,  2.0001, -2.0799, -2.9585,  3.6484]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.8558455170122216 entropy tensor([[  1.7830,   1.8010,   1.8655,   0.5186,   0.9143, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 14, step: 2
	action: tensor([[ 0.4506, -0.2693, -0.7892,  0.9325, -1.4138, -2.1550,  2.3223]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.844374642535988, distance: 0.4514367121453981 entropy tensor([[ 1.3367e+00,  1.3860e+00,  1.2699e+00,  1.3814e-03,  6.0512e-02,
         -2.1607e+01, -2.1607e+01]], dtype=torch.float64,
       grad_fn=<AddBackward0>)
epoch: 14, step: 3
	action: tensor([[ 0.7359,  1.5604, -0.3163,  2.0792, -2.5816, -2.8037,  3.4323]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.4514367121453981 entropy tensor([[  1.7555,   1.7111,   1.7877,   0.3779,   0.9636, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 14, step: 4
	action: tensor([[ 1.3015, -0.0709,  0.7374,  0.9872, -1.0409, -2.1550,  2.3223]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.5063013726848637, distance: 0.8040585106237433 entropy tensor([[ 1.3367e+00,  1.3860e+00,  1.2699e+00,  1.3814e-03,  6.0512e-02,
         -2.1607e+01, -2.1607e+01]], dtype=torch.float64,
       grad_fn=<AddBackward0>)
epoch: 14, step: 5
	action: tensor([[-0.4852,  0.3667,  0.8028,  2.4443, -1.1530, -2.8069,  3.5214]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.8040585106237433 entropy tensor([[  1.7953,   1.6931,   1.7904,   0.2388,   0.9860, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 14, step: 6
	action: tensor([[ 0.0844,  3.0281, -0.9478,  0.7860, -1.1927, -2.1550,  2.3223]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.8040585106237433 entropy tensor([[ 1.3367e+00,  1.3860e+00,  1.2699e+00,  1.3814e-03,  6.0512e-02,
         -2.1607e+01, -2.1607e+01]], dtype=torch.float64,
       grad_fn=<AddBackward0>)
epoch: 14, step: 7
	action: tensor([[ 0.3442, -0.2309,  0.7463,  0.8159, -1.1068, -2.1550,  2.3223]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.5291391134658135, distance: 0.7852410611161041 entropy tensor([[ 1.3367e+00,  1.3860e+00,  1.2699e+00,  1.3814e-03,  6.0512e-02,
         -2.1607e+01, -2.1607e+01]], dtype=torch.float64,
       grad_fn=<AddBackward0>)
epoch: 14, step: 8
	action: tensor([[ 2.9625,  3.5485, -1.4320,  3.1415, -0.0274, -2.6974,  3.3431]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.7852410611161041 entropy tensor([[  1.7442,   1.7231,   1.7661,   0.3960,   0.9741, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 14, step: 9
	action: tensor([[ 0.4708, -0.2334,  0.7374,  1.1032, -1.1709, -2.1550,  2.3223]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.5988612676517999, distance: 0.7247763142823144 entropy tensor([[ 1.3367e+00,  1.3860e+00,  1.2699e+00,  1.3814e-03,  6.0512e-02,
         -2.1607e+01, -2.1607e+01]], dtype=torch.float64,
       grad_fn=<AddBackward0>)
epoch: 14, step: 10
	action: tensor([[-1.7732,  1.4813,  1.1307,  3.1810, -1.6293, -2.7736,  3.4853]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.7247763142823144 entropy tensor([[  1.7794,   1.7433,   1.7994,   0.3900,   1.0118, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 14, step: 11
	action: tensor([[-1.2391,  0.8583, -1.0004,  1.1220, -0.9618, -2.1550,  2.3223]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.2510100976194566, distance: 0.9903636109544484 entropy tensor([[ 1.3367e+00,  1.3860e+00,  1.2699e+00,  1.3814e-03,  6.0512e-02,
         -2.1607e+01, -2.1607e+01]], dtype=torch.float64,
       grad_fn=<AddBackward0>)
epoch: 14, step: 12
	action: tensor([[-3.6193,  4.3907,  0.7605,  1.6471, -3.2036, -3.1545,  3.9614]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.9903636109544484 entropy tensor([[  1.8254,   1.9003,   1.9497,   0.6437,   0.9870, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 14, step: 13
	action: tensor([[-0.5767, -0.4838, -0.0371,  0.8025, -1.2959, -2.1550,  2.3223]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.2111256651735337, distance: 1.016390522540401 entropy tensor([[ 1.3367e+00,  1.3860e+00,  1.2699e+00,  1.3814e-03,  6.0512e-02,
         -2.1607e+01, -2.1607e+01]], dtype=torch.float64,
       grad_fn=<AddBackward0>)
epoch: 14, step: 14
	action: tensor([[ 0.9645,  0.9211,  0.7407,  3.3403, -1.7048, -2.7331,  3.3387]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.016390522540401 entropy tensor([[  1.7399,   1.7515,   1.7771,   0.5225,   0.9949, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 14, step: 15
	action: tensor([[-0.8385,  1.1707, -0.9570,  0.7328, -0.6911, -2.1550,  2.3223]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.5284606400313545, distance: 0.7858065927145248 entropy tensor([[ 1.3367e+00,  1.3860e+00,  1.2699e+00,  1.3814e-03,  6.0512e-02,
         -2.1607e+01, -2.1607e+01]], dtype=torch.float64,
       grad_fn=<AddBackward0>)
epoch: 14, step: 16
	action: tensor([[ 0.3710,  3.0468, -1.6075,  2.7162, -3.5151, -3.0104,  3.7675]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.7858065927145248 entropy tensor([[  1.7535,   1.8523,   1.9158,   0.6188,   0.9232, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 14, step: 17
	action: tensor([[ 2.9562,  1.4341,  0.5389,  0.8514, -1.1698, -2.1550,  2.3223]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.7858065927145248 entropy tensor([[ 1.3367e+00,  1.3860e+00,  1.2699e+00,  1.3814e-03,  6.0512e-02,
         -2.1607e+01, -2.1607e+01]], dtype=torch.float64,
       grad_fn=<AddBackward0>)
epoch: 14, step: 18
	action: tensor([[ 1.6468,  0.6685, -0.5094,  0.6747, -1.0612, -2.1550,  2.3223]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.7858065927145248 entropy tensor([[ 1.3367e+00,  1.3860e+00,  1.2699e+00,  1.3814e-03,  6.0512e-02,
         -2.1607e+01, -2.1607e+01]], dtype=torch.float64,
       grad_fn=<AddBackward0>)
epoch: 14, step: 19
	action: tensor([[-0.4370, -0.1622,  0.7828,  0.8281, -1.0639, -2.1550,  2.3223]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.17809818134673894, distance: 1.0374487792172424 entropy tensor([[ 1.3367e+00,  1.3860e+00,  1.2699e+00,  1.3814e-03,  6.0512e-02,
         -2.1607e+01, -2.1607e+01]], dtype=torch.float64,
       grad_fn=<AddBackward0>)
epoch: 14, step: 20
	action: tensor([[-2.8508, -0.7440, -1.8797,  1.8649, -0.7386, -2.7183,  3.3887]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.0374487792172424 entropy tensor([[  1.7476,   1.7736,   1.7997,   0.5458,   1.0186, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 14, step: 21
	action: tensor([[ 4.9456e-01,  4.5622e-01, -1.4912e-03,  9.0877e-01, -1.2271e+00,
         -2.1550e+00,  2.3223e+00]], dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.9283174548898497, distance: 0.30638210919541947 entropy tensor([[ 1.3367e+00,  1.3860e+00,  1.2699e+00,  1.3814e-03,  6.0512e-02,
         -2.1607e+01, -2.1607e+01]], dtype=torch.float64,
       grad_fn=<AddBackward0>)
epoch: 14, step: 22
	action: tensor([[-1.7367,  1.4672, -1.1580,  2.8609, -1.5677, -2.7963,  3.5902]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.30638210919541947 entropy tensor([[  1.7780,   1.7709,   1.8291,   0.4127,   0.9648, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 14, step: 23
	action: tensor([[-1.4704,  1.1444, -0.1534,  1.0864, -0.7232, -2.1550,  2.3223]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.27062716650505125, distance: 0.9773080700918726 entropy tensor([[ 1.3367e+00,  1.3860e+00,  1.2699e+00,  1.3814e-03,  6.0512e-02,
         -2.1607e+01, -2.1607e+01]], dtype=torch.float64,
       grad_fn=<AddBackward0>)
epoch: 14, step: 24
	action: tensor([[ 0.0533,  2.4281, -2.4084,  2.5931, -3.8168, -3.0997,  3.9740]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.9773080700918726 entropy tensor([[  1.8356,   1.9133,   1.9592,   0.6879,   1.0356, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 14, step: 25
	action: tensor([[-1.1316,  0.3795, -0.9122,  0.7601, -0.9657, -2.1550,  2.3223]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.03661412424934163, distance: 1.1231993169423171 entropy tensor([[ 1.3367e+00,  1.3860e+00,  1.2699e+00,  1.3814e-03,  6.0512e-02,
         -2.1607e+01, -2.1607e+01]], dtype=torch.float64,
       grad_fn=<AddBackward0>)
epoch: 14, step: 26
	action: tensor([[ 1.6912,  2.5547, -0.8819,  2.0610, -1.6299, -2.9469,  3.5987]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.1231993169423171 entropy tensor([[  1.7579,   1.8253,   1.8593,   0.6083,   0.9157, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 14, step: 27
	action: tensor([[ 0.4513,  1.6567, -0.4271,  0.8960, -0.8625, -2.1550,  2.3223]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.1231993169423171 entropy tensor([[ 1.3367e+00,  1.3860e+00,  1.2699e+00,  1.3814e-03,  6.0512e-02,
         -2.1607e+01, -2.1607e+01]], dtype=torch.float64,
       grad_fn=<AddBackward0>)
epoch: 14, step: 28
	action: tensor([[ 0.2801,  1.1393,  0.6264,  0.6447, -1.1068, -2.1550,  2.3223]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.1231993169423171 entropy tensor([[ 1.3367e+00,  1.3860e+00,  1.2699e+00,  1.3814e-03,  6.0512e-02,
         -2.1607e+01, -2.1607e+01]], dtype=torch.float64,
       grad_fn=<AddBackward0>)
epoch: 14, step: 29
	action: tensor([[ 0.6620,  0.5284,  0.0730,  0.8278, -1.2633, -2.1550,  2.3223]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.910670092187626, distance: 0.34202286151769407 entropy tensor([[ 1.3367e+00,  1.3860e+00,  1.2699e+00,  1.3814e-03,  6.0512e-02,
         -2.1607e+01, -2.1607e+01]], dtype=torch.float64,
       grad_fn=<AddBackward0>)
epoch: 14, step: 30
	action: tensor([[-0.2814,  0.2343, -0.5745,  2.5014, -1.9956, -2.8069,  3.6145]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.34202286151769407 entropy tensor([[  1.7894,   1.7639,   1.8292,   0.4056,   0.9519, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 14, step: 31
	action: tensor([[-1.4912,  0.8779, -0.1484,  1.2090, -1.2057, -2.1550,  2.3223]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.269239145825819, distance: 0.9782375529193893 entropy tensor([[ 1.3367e+00,  1.3860e+00,  1.2699e+00,  1.3814e-03,  6.0512e-02,
         -2.1607e+01, -2.1607e+01]], dtype=torch.float64,
       grad_fn=<AddBackward0>)
epoch: 14, step: 32
	action: tensor([[-0.6440,  3.7008, -0.7142,  3.2749, -1.9250, -3.1767,  4.0991]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.9782375529193893 entropy tensor([[  1.8966,   1.9417,   1.9854,   0.6813,   1.0763, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 14, step: 33
	action: tensor([[-1.1429,  0.5769, -0.2609,  0.7791, -1.5954, -2.1550,  2.3223]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.3371839510136506, distance: 0.9316508854647487 entropy tensor([[ 1.3367e+00,  1.3860e+00,  1.2699e+00,  1.3814e-03,  6.0512e-02,
         -2.1607e+01, -2.1607e+01]], dtype=torch.float64,
       grad_fn=<AddBackward0>)
epoch: 14, step: 34
	action: tensor([[ 0.9644,  0.6398, -1.5919,  2.7885, -1.1947, -3.0578,  3.8793]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.9316508854647487 entropy tensor([[  1.8627,   1.8944,   1.9369,   0.6439,   0.9918, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 14, step: 35
	action: tensor([[ 0.4190, -0.2234, -1.4304,  0.6659, -1.2724, -2.1550,  2.3223]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.7937655833667552, distance: 0.5196815093076215 entropy tensor([[ 1.3367e+00,  1.3860e+00,  1.2699e+00,  1.3814e-03,  6.0512e-02,
         -2.1607e+01, -2.1607e+01]], dtype=torch.float64,
       grad_fn=<AddBackward0>)
epoch: 14, step: 36
	action: tensor([[ 0.0412,  0.6738,  1.3344,  2.3121, -2.5206, -2.8367,  3.4151]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.5196815093076215 entropy tensor([[  1.7332,   1.6962,   1.7846,   0.4343,   0.9171, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 14, step: 37
	action: tensor([[ 1.4512, -1.6598, -2.3078,  0.7917, -1.3238, -2.1550,  2.3223]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.5196815093076215 entropy tensor([[ 1.3367e+00,  1.3860e+00,  1.2699e+00,  1.3814e-03,  6.0512e-02,
         -2.1607e+01, -2.1607e+01]], dtype=torch.float64,
       grad_fn=<AddBackward0>)
epoch: 14, step: 38
	action: tensor([[-1.1115, -3.5710,  0.0414,  0.3949, -1.0729, -2.1550,  2.3223]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.5196815093076215 entropy tensor([[ 1.3367e+00,  1.3860e+00,  1.2699e+00,  1.3814e-03,  6.0512e-02,
         -2.1607e+01, -2.1607e+01]], dtype=torch.float64,
       grad_fn=<AddBackward0>)
epoch: 14, step: 39
	action: tensor([[ 1.7589,  1.5587, -0.7592,  0.8916, -1.5143, -2.1550,  2.3223]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.5196815093076215 entropy tensor([[ 1.3367e+00,  1.3860e+00,  1.2699e+00,  1.3814e-03,  6.0512e-02,
         -2.1607e+01, -2.1607e+01]], dtype=torch.float64,
       grad_fn=<AddBackward0>)
epoch: 14, step: 40
	action: tensor([[-0.8322, -0.2774,  0.5964,  0.9635, -0.6834, -2.1550,  2.3223]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.22695013256701113, distance: 1.2675647380869133 entropy tensor([[ 1.3367e+00,  1.3860e+00,  1.2699e+00,  1.3814e-03,  6.0512e-02,
         -2.1607e+01, -2.1607e+01]], dtype=torch.float64,
       grad_fn=<AddBackward0>)
epoch: 14, step: 41
	action: tensor([[-0.0313,  0.2022,  0.9917,  3.5020, -2.2343, -2.6465,  3.2576]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.2675647380869133 entropy tensor([[  1.6907,   1.7541,   1.7739,   0.5838,   1.0212, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 14, step: 42
	action: tensor([[ 1.5507, -0.0815, -0.8984,  1.0011, -1.4196, -2.1550,  2.3223]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.6560302885667192, distance: 0.6711456726031096 entropy tensor([[ 1.3367e+00,  1.3860e+00,  1.2699e+00,  1.3814e-03,  6.0512e-02,
         -2.1607e+01, -2.1607e+01]], dtype=torch.float64,
       grad_fn=<AddBackward0>)
epoch: 14, step: 43
	action: tensor([[-2.2750,  1.2612,  1.0733,  2.3716, -1.7570, -2.9288,  3.6489]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.6711456726031096 entropy tensor([[  1.8034,   1.6829,   1.8061,   0.3605,   1.0042, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 14, step: 44
	action: tensor([[-1.2883, -0.1774, -0.0164,  0.6462, -1.2028, -2.1550,  2.3223]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: -0.3267753000097071, distance: 1.3181213127090299 entropy tensor([[ 1.3367e+00,  1.3860e+00,  1.2699e+00,  1.3814e-03,  6.0512e-02,
         -2.1607e+01, -2.1607e+01]], dtype=torch.float64,
       grad_fn=<AddBackward0>)
epoch: 14, step: 45
	action: tensor([[-1.5538,  3.4114, -4.1880,  2.2452, -3.4353, -2.8407,  3.4704]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 1.3181213127090299 entropy tensor([[  1.7761,   1.8043,   1.8338,   0.6382,   0.9833, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 14, step: 46
	action: tensor([[-1.3013,  1.2706,  0.1011,  0.7186, -1.3368, -2.1550,  2.3223]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	reward: 0.48421415722944894, distance: 0.8218478096309415 entropy tensor([[ 1.3367e+00,  1.3860e+00,  1.2699e+00,  1.3814e-03,  6.0512e-02,
         -2.1607e+01, -2.1607e+01]], dtype=torch.float64,
       grad_fn=<AddBackward0>)
epoch: 14, step: 47
	action: tensor([[ 0.4668,  0.2459,  0.3597,  2.9513, -2.6683, -3.1430,  4.0911]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.8218478096309415 entropy tensor([[  1.8905,   1.9291,   1.9928,   0.7060,   1.0364, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 14, step: 48
	action: tensor([[-2.1798, -0.0464, -0.6054,  0.8408, -1.7495, -2.1550,  2.3223]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.8218478096309415 entropy tensor([[ 1.3367e+00,  1.3860e+00,  1.2699e+00,  1.3814e-03,  6.0512e-02,
         -2.1607e+01, -2.1607e+01]], dtype=torch.float64,
       grad_fn=<AddBackward0>)
epoch: 14, step: 49
	action: tensor([[-0.4585,  1.7459, -0.0521,  1.1206, -1.7050, -2.1550,  2.3223]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.8218478096309415 entropy tensor([[ 1.3367e+00,  1.3860e+00,  1.2699e+00,  1.3814e-03,  6.0512e-02,
         -2.1607e+01, -2.1607e+01]], dtype=torch.float64,
       grad_fn=<AddBackward0>)
LOSS epoch 14 actor 720.7884625367996 critic 2274.705575755388
epoch: 15, step: 0
	action: tensor([[-0.3027, -0.1893, -1.6450,  0.4366, -1.3001, -2.2174,  2.3427]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.8218478096309415 entropy tensor([[  1.4509,   1.4400,   1.3343,   0.4436,   0.1970, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 15, step: 1
	action: tensor([[-0.3702, -1.4594, -0.2057,  0.9271, -1.4673, -2.2174,  2.3427]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.8218478096309415 entropy tensor([[  1.4509,   1.4400,   1.3343,   0.4436,   0.1970, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 15, step: 2
	action: tensor([[-0.7902, -0.4596, -0.6630,  1.5977, -1.2022, -2.2174,  2.3427]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.8218478096309415 entropy tensor([[  1.4509,   1.4400,   1.3343,   0.4436,   0.1970, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 15, step: 3
	action: tensor([[-0.7717,  0.4360, -1.3960,  1.0727, -1.3624, -2.2174,  2.3427]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.8218478096309415 entropy tensor([[  1.4509,   1.4400,   1.3343,   0.4436,   0.1970, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 15, step: 4
	action: tensor([[-0.7427,  0.4718,  0.1893,  0.7351, -0.9680, -2.2174,  2.3427]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.8218478096309415 entropy tensor([[  1.4509,   1.4400,   1.3343,   0.4436,   0.1970, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 15, step: 5
	action: tensor([[ 0.2176, -0.2907,  0.9952,  0.3574, -1.5224, -2.2174,  2.3427]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.8218478096309415 entropy tensor([[  1.4509,   1.4400,   1.3343,   0.4436,   0.1970, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 15, step: 6
	action: tensor([[-0.6237, -0.9316, -0.3881,  0.8467, -1.0525, -2.2174,  2.3427]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.8218478096309415 entropy tensor([[  1.4509,   1.4400,   1.3343,   0.4436,   0.1970, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 15, step: 7
	action: tensor([[-0.2140, -0.8009,  0.2496,  1.3016, -1.2832, -2.2174,  2.3427]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.8218478096309415 entropy tensor([[  1.4509,   1.4400,   1.3343,   0.4436,   0.1970, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 15, step: 8
	action: tensor([[-0.9214, -0.1025, -0.3229,  0.6266, -1.2233, -2.2174,  2.3427]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.8218478096309415 entropy tensor([[  1.4509,   1.4400,   1.3343,   0.4436,   0.1970, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 15, step: 9
	action: tensor([[ 0.0431,  0.8480,  0.4942,  0.6378, -0.9223, -2.2174,  2.3427]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.8218478096309415 entropy tensor([[  1.4509,   1.4400,   1.3343,   0.4436,   0.1970, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 15, step: 10
	action: tensor([[ 1.9969,  1.1017, -0.9401,  0.5051, -1.1995, -2.2174,  2.3427]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.8218478096309415 entropy tensor([[  1.4509,   1.4400,   1.3343,   0.4436,   0.1970, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 15, step: 11
	action: tensor([[-0.1571,  1.3655, -0.0906,  1.4698, -1.4371, -2.2174,  2.3427]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.8218478096309415 entropy tensor([[  1.4509,   1.4400,   1.3343,   0.4436,   0.1970, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 15, step: 12
	action: tensor([[-0.5680, -1.4415, -2.4341,  1.2251, -1.5493, -2.2174,  2.3427]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.8218478096309415 entropy tensor([[  1.4509,   1.4400,   1.3343,   0.4436,   0.1970, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 15, step: 13
	action: tensor([[ 1.2615, -0.1121, -0.2496,  0.4280, -1.4262, -2.2174,  2.3427]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.8218478096309415 entropy tensor([[  1.4509,   1.4400,   1.3343,   0.4436,   0.1970, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
epoch: 15, step: 14
	action: tensor([[-0.8944,  1.3718, -2.3858,  0.9475, -1.3813, -2.2174,  2.3427]],
       dtype=torch.float64)
	q_value: tensor([[-1.0000]], dtype=torch.float64, grad_fn=<TanhBackward0>)
	COLLISION
	reward: -50, distance: 0.8218478096309415 entropy tensor([[  1.4509,   1.4400,   1.3343,   0.4436,   0.1970, -21.6069, -21.6069]],
       dtype=torch.float64, grad_fn=<AddBackward0>)
