epoch: 0, step: 0
	action: tensor([[-1.9593,  0.0191,  0.4894, -2.0361, -1.0936,  1.1130, -0.0377]],
       dtype=torch.float64)
	q_value: tensor([[0.1350]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 1
	action: tensor([[-0.6968, -0.4104,  0.0968, -0.1268,  2.5812,  0.1399, -2.7931]],
       dtype=torch.float64)
	q_value: tensor([[0.1348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9157570252653799, distance: 1.583896892261317 entropy 1.8700141906738281
epoch: 0, step: 2
	action: tensor([[-2.2077, -1.2865,  2.3341, -0.1725, -1.3533, -3.1853,  1.1856]],
       dtype=torch.float64)
	q_value: tensor([[0.1174]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 3
	action: tensor([[-0.0554,  1.7286, -1.7369,  0.1032, -1.7056, -0.9975, -0.0645]],
       dtype=torch.float64)
	q_value: tensor([[0.1348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 4
	action: tensor([[-0.6306, -1.6934,  1.2000, -0.1464, -2.0359, -0.6648,  1.3151]],
       dtype=torch.float64)
	q_value: tensor([[0.1348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9591237642790562, distance: 1.601723802955105 entropy 1.8700141906738281
epoch: 0, step: 5
	action: tensor([[-2.1941,  1.4895, -0.7600,  2.4310, -2.8944, -2.9467,  1.2022]],
       dtype=torch.float64)
	q_value: tensor([[0.0852]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 6
	action: tensor([[-2.5578,  1.1132,  0.1369, -0.5089, -1.1655,  2.4437, -2.1840]],
       dtype=torch.float64)
	q_value: tensor([[0.1348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 7
	action: tensor([[-1.3758,  0.1885,  0.7697, -0.1590, -0.7816, -0.8752,  0.0137]],
       dtype=torch.float64)
	q_value: tensor([[0.1348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.3902865605544648, distance: 1.769219339746269 entropy 1.8700141906738281
epoch: 0, step: 8
	action: tensor([[-1.1914,  2.0543, -0.3765,  2.6112,  2.1019,  0.7651, -1.6637]],
       dtype=torch.float64)
	q_value: tensor([[0.1307]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 9
	action: tensor([[ 3.0459, -0.3413, -2.7828,  3.6249, -0.2391,  1.6839,  0.3934]],
       dtype=torch.float64)
	q_value: tensor([[0.1348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 10
	action: tensor([[ 0.6199,  3.0712, -2.3374,  1.6952,  0.1508, -0.4644,  1.0421]],
       dtype=torch.float64)
	q_value: tensor([[0.1348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 11
	action: tensor([[-4.2281, -0.2963, -3.2130, -0.4083, -0.7072, -1.4328,  0.7507]],
       dtype=torch.float64)
	q_value: tensor([[0.1348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 12
	action: tensor([[ 1.5790, -0.8908, -0.0242,  0.6604, -0.1080,  4.5334, -0.0600]],
       dtype=torch.float64)
	q_value: tensor([[0.1348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 13
	action: tensor([[-0.0828, -1.5968,  0.0843,  1.7751,  2.5164, -1.5034,  0.2958]],
       dtype=torch.float64)
	q_value: tensor([[0.1348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31087083442827457, distance: 0.9499636961812464 entropy 1.8700141906738281
epoch: 0, step: 14
	action: tensor([[ 0.6878,  1.8435,  1.1534,  1.4975,  0.2303, -0.8715,  1.8294]],
       dtype=torch.float64)
	q_value: tensor([[0.0918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31165258114578565, distance: 0.9494247248478076 entropy 1.8700141906738281
epoch: 0, step: 15
	action: tensor([[-0.2868,  1.2262,  1.1540, -1.1859,  1.3710,  1.2330, -1.4276]],
       dtype=torch.float64)
	q_value: tensor([[0.0725]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8776737917063229, distance: 0.40023627728873784 entropy 1.8700141906738281
epoch: 0, step: 16
	action: tensor([[ 1.6297,  0.9192,  0.5260,  0.8882, -0.7816, -0.9159, -1.9929]],
       dtype=torch.float64)
	q_value: tensor([[0.0917]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07809671690586828, distance: 1.098751229526393 entropy 1.8700141906738281
epoch: 0, step: 17
	action: tensor([[-0.3522, -2.4140,  0.3048,  1.4835, -1.1868, -0.3585, -1.2866]],
       dtype=torch.float64)
	q_value: tensor([[0.0671]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 18
	action: tensor([[ 0.5240, -2.6314, -0.9924, -0.2052, -1.1495, -1.0718,  1.2123]],
       dtype=torch.float64)
	q_value: tensor([[0.1348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 19
	action: tensor([[ 2.8051,  0.5051,  1.2697, -1.0645, -1.9151,  0.4190, -1.6224]],
       dtype=torch.float64)
	q_value: tensor([[0.1348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 20
	action: tensor([[-0.0077,  0.7173, -1.3515, -1.7835, -0.4010, -2.3254, -0.0332]],
       dtype=torch.float64)
	q_value: tensor([[0.1348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.570684463835688, distance: 0.7497992398978104 entropy 1.8700141906738281
epoch: 0, step: 21
	action: tensor([[-1.9372, -1.1028,  3.8312,  1.9495,  1.5735, -1.4802,  0.9288]],
       dtype=torch.float64)
	q_value: tensor([[0.1221]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 22
	action: tensor([[-1.5944,  1.9927,  2.9504, -2.0918,  1.1711, -0.1700, -0.7123]],
       dtype=torch.float64)
	q_value: tensor([[0.1348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 23
	action: tensor([[-1.2194, -3.1617, -2.5140, -0.4907,  1.2693, -0.6893,  0.3335]],
       dtype=torch.float64)
	q_value: tensor([[0.1348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 24
	action: tensor([[ 1.7764, -0.3723,  0.2691,  0.2795,  1.3593,  1.4197, -3.7490]],
       dtype=torch.float64)
	q_value: tensor([[0.1348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 25
	action: tensor([[ 0.5630, -1.3537,  1.0477, -2.0867,  3.7151,  1.5937,  0.0130]],
       dtype=torch.float64)
	q_value: tensor([[0.1348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 26
	action: tensor([[ 1.0281,  0.2987,  2.1092, -1.1399, -1.1510, -0.5958, -2.9102]],
       dtype=torch.float64)
	q_value: tensor([[0.1348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6465641754735225, distance: 0.6803180241436133 entropy 1.8700141906738281
epoch: 0, step: 27
	action: tensor([[-0.8198,  0.3289, -0.2742,  1.8035,  1.3763, -1.3015, -1.3162]],
       dtype=torch.float64)
	q_value: tensor([[0.0654]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.27814620649925814, distance: 1.2937398660748727 entropy 1.8700141906738281
epoch: 0, step: 28
	action: tensor([[-1.6447,  0.2587,  0.9408, -1.4672, -3.1416,  1.0579,  2.1386]],
       dtype=torch.float64)
	q_value: tensor([[0.0982]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7440146983110734, distance: 1.5112341663579822 entropy 1.8700141906738281
epoch: 0, step: 29
	action: tensor([[ 0.5112,  3.0169, -0.1258,  0.2339,  0.0999, -2.2426,  0.4859]],
       dtype=torch.float64)
	q_value: tensor([[0.0958]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 30
	action: tensor([[-1.3774, -2.1608,  0.3598, -0.0533, -1.5592, -1.8916, -2.2216]],
       dtype=torch.float64)
	q_value: tensor([[0.1348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 31
	action: tensor([[ 1.3077, -2.0733,  1.2732, -1.7927, -0.7643, -0.1623,  0.4039]],
       dtype=torch.float64)
	q_value: tensor([[0.1348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 32
	action: tensor([[-2.2259, -0.7816,  1.6631, -2.9089, -3.8466,  1.5054, -1.1199]],
       dtype=torch.float64)
	q_value: tensor([[0.1348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 33
	action: tensor([[-0.3931,  0.2614, -0.8851,  1.8492, -1.6676, -0.6893,  2.6150]],
       dtype=torch.float64)
	q_value: tensor([[0.1348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12886931739154106, distance: 1.0680666351406325 entropy 1.8700141906738281
epoch: 0, step: 34
	action: tensor([[ 2.0937,  1.7614, -0.0301, -0.5039,  1.6657,  0.4533, -1.2126]],
       dtype=torch.float64)
	q_value: tensor([[0.1152]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 35
	action: tensor([[-2.4831, -2.0516, -1.2569, -1.6638, -1.0805,  0.3564, -1.3060]],
       dtype=torch.float64)
	q_value: tensor([[0.1348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 36
	action: tensor([[ 1.4121, -0.5385,  0.0039, -0.6584, -1.1861, -0.0371, -2.0218]],
       dtype=torch.float64)
	q_value: tensor([[0.1348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.34484331960793746, distance: 1.3270660481635903 entropy 1.8700141906738281
epoch: 0, step: 37
	action: tensor([[ 2.0575,  0.2391,  0.2986, -2.2716,  0.7808, -1.9170,  0.9525]],
       dtype=torch.float64)
	q_value: tensor([[0.1218]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 38
	action: tensor([[ 0.6496, -0.1093, -0.2688, -1.8596, -1.2602, -0.9257, -5.1527]],
       dtype=torch.float64)
	q_value: tensor([[0.1348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0628042428538642 entropy 1.8700141906738281
epoch: 0, step: 39
	action: tensor([[-1.3950, -0.7915,  0.4791, -0.4327,  0.5239, -1.1229, -0.5106]],
       dtype=torch.float64)
	q_value: tensor([[0.1348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.356395313412511, distance: 1.7566319131646606 entropy 1.8700141906738281
epoch: 0, step: 40
	action: tensor([[-2.4995, -0.8969, -3.2721,  1.7028, -1.4051,  1.5673, -0.2436]],
       dtype=torch.float64)
	q_value: tensor([[0.1396]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 41
	action: tensor([[-1.4194,  1.1686,  0.1818, -1.4395,  1.0400, -0.3650, -0.2046]],
       dtype=torch.float64)
	q_value: tensor([[0.1348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9739194758400223, distance: 1.6077607026422542 entropy 1.8700141906738281
epoch: 0, step: 42
	action: tensor([[-2.3761, -0.1199, -0.9630, -0.1590, -2.9730,  0.1686, -1.2473]],
       dtype=torch.float64)
	q_value: tensor([[0.1080]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 43
	action: tensor([[-0.2113, -1.4599, -0.5193,  0.3212,  1.2594, -0.8681, -2.7335]],
       dtype=torch.float64)
	q_value: tensor([[0.1348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.046949672496977, distance: 1.637232193530352 entropy 1.8700141906738281
epoch: 0, step: 44
	action: tensor([[-0.3175,  1.5041,  0.0464, -2.1800, -2.6066, -1.4894, -0.5688]],
       dtype=torch.float64)
	q_value: tensor([[0.1354]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5923463629993333, distance: 0.7306381650702198 entropy 1.8700141906738281
epoch: 0, step: 45
	action: tensor([[-0.2087,  0.0076, -1.7403,  1.9707, -0.8329,  1.5452,  0.8354]],
       dtype=torch.float64)
	q_value: tensor([[0.1389]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12606364990594177, distance: 1.214334022399614 entropy 1.8700141906738281
epoch: 0, step: 46
	action: tensor([[ 0.7833,  0.5009, -1.3884, -3.8166,  0.9799, -0.0485,  0.2411]],
       dtype=torch.float64)
	q_value: tensor([[0.1354]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 47
	action: tensor([[ 0.9338,  1.2009, -0.1079,  1.3871, -1.4248, -0.9943, -1.0704]],
       dtype=torch.float64)
	q_value: tensor([[0.1348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3378032630048926, distance: 0.9312155328454541 entropy 1.8700141906738281
epoch: 0, step: 48
	action: tensor([[ 2.9774,  1.0271, -0.7001,  0.6043, -0.7497,  2.5748,  0.5929]],
       dtype=torch.float64)
	q_value: tensor([[0.0794]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 49
	action: tensor([[-1.7464,  0.1567, -1.4767, -0.9956, -2.2215, -3.6177, -1.1325]],
       dtype=torch.float64)
	q_value: tensor([[0.1348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 50
	action: tensor([[ 0.5457,  2.5765, -0.2245, -0.0854, -1.1768, -2.2895,  0.8335]],
       dtype=torch.float64)
	q_value: tensor([[0.1348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 51
	action: tensor([[-1.0738, -0.3487,  0.7559, -0.1942, -2.6864, -0.0225,  0.3119]],
       dtype=torch.float64)
	q_value: tensor([[0.1348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.458619360475541, distance: 1.7943300972823122 entropy 1.8700141906738281
epoch: 0, step: 52
	action: tensor([[ 0.3108,  2.1756, -1.4658,  2.2698, -1.4311,  2.5014,  2.7222]],
       dtype=torch.float64)
	q_value: tensor([[0.1132]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 53
	action: tensor([[ 1.6790,  1.4138, -0.3219, -1.8355, -0.8141, -2.3220, -2.1966]],
       dtype=torch.float64)
	q_value: tensor([[0.1348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31630878489052605, distance: 0.9462081687783573 entropy 1.8700141906738281
epoch: 0, step: 54
	action: tensor([[-0.5021, -1.2599,  1.3685,  1.8435, -0.7968,  0.6481,  0.9847]],
       dtype=torch.float64)
	q_value: tensor([[0.1178]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.711701513497469, distance: 0.6144377278532593 entropy 1.8700141906738281
epoch: 0, step: 55
	action: tensor([[-0.9994,  0.2374, -0.3372, -0.7988, -1.3342,  3.0011, -0.1398]],
       dtype=torch.float64)
	q_value: tensor([[0.0896]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 56
	action: tensor([[-0.7738,  0.3265,  0.5891,  1.0451,  1.8595, -0.8690, -0.6288]],
       dtype=torch.float64)
	q_value: tensor([[0.1348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3323483483466556, distance: 0.9350431589129972 entropy 1.8700141906738281
epoch: 0, step: 57
	action: tensor([[ 0.4771, -0.0800, -2.0245,  0.4872,  0.9518,  3.9062, -1.3086]],
       dtype=torch.float64)
	q_value: tensor([[0.1013]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 58
	action: tensor([[ 2.7657,  1.6117, -0.0067,  0.3964,  0.7627,  1.8054,  1.7571]],
       dtype=torch.float64)
	q_value: tensor([[0.1348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 59
	action: tensor([[ 0.3833,  2.6785, -0.1797,  0.8296, -2.5558,  0.0169,  1.2158]],
       dtype=torch.float64)
	q_value: tensor([[0.1348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 60
	action: tensor([[ 0.0817,  2.7860,  1.0348,  1.2804,  1.3654, -0.7029, -1.2765]],
       dtype=torch.float64)
	q_value: tensor([[0.1348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 61
	action: tensor([[-0.0588, -1.9642,  2.1337,  1.2828,  0.2291, -0.2535, -1.2508]],
       dtype=torch.float64)
	q_value: tensor([[0.1348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 62
	action: tensor([[-2.9723, -1.0058, -0.1644, -3.0057, -1.4195, -0.6628,  1.2649]],
       dtype=torch.float64)
	q_value: tensor([[0.1348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 63
	action: tensor([[ 2.7280, -0.3557,  0.5117, -1.6731, -0.0144, -1.7473,  1.6460]],
       dtype=torch.float64)
	q_value: tensor([[0.1348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 64
	action: tensor([[ 3.9615,  1.0804, -0.7166, -0.7238,  1.3542,  2.3151, -0.4391]],
       dtype=torch.float64)
	q_value: tensor([[0.1348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 65
	action: tensor([[ 0.5091,  1.7361,  0.3470,  0.7052, -1.7103,  0.6632, -1.6283]],
       dtype=torch.float64)
	q_value: tensor([[0.1348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 66
	action: tensor([[-2.6453, -0.1786,  1.9084, -0.2620,  1.6847, -2.7371,  0.1965]],
       dtype=torch.float64)
	q_value: tensor([[0.1348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 67
	action: tensor([[-1.5701, -1.1109, -0.2128,  0.8536,  0.9994, -1.4195,  0.9117]],
       dtype=torch.float64)
	q_value: tensor([[0.1348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.4131405414673854, distance: 1.7776571388810671 entropy 1.8700141906738281
epoch: 0, step: 68
	action: tensor([[ 1.9177,  3.8799,  0.2000, -0.7686,  2.3151, -2.1402, -2.4404]],
       dtype=torch.float64)
	q_value: tensor([[0.1151]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 69
	action: tensor([[-0.9468,  1.5084, -0.2656, -0.2835,  1.6504, -2.0857,  0.1972]],
       dtype=torch.float64)
	q_value: tensor([[0.1348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.004408932825278433, distance: 1.1418187988119874 entropy 1.8700141906738281
epoch: 0, step: 70
	action: tensor([[ 0.7743,  1.0925, -1.6635, -2.3262,  1.0242,  0.4755,  0.8392]],
       dtype=torch.float64)
	q_value: tensor([[0.0751]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24206750377427222, distance: 0.9962583104412873 entropy 1.8700141906738281
epoch: 0, step: 71
	action: tensor([[ 1.3186,  0.6255,  0.6263, -1.1278, -1.7141,  1.4150, -1.1968]],
       dtype=torch.float64)
	q_value: tensor([[0.0767]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8969589518871429, distance: 0.3673345912936029 entropy 1.8700141906738281
epoch: 0, step: 72
	action: tensor([[-0.1535, -0.5480,  0.4399,  1.5284, -0.8072, -0.0272,  0.8493]],
       dtype=torch.float64)
	q_value: tensor([[0.1022]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7331958640417948, distance: 0.5910891289942894 entropy 1.8700141906738281
epoch: 0, step: 73
	action: tensor([[-0.1859,  0.3183, -1.7385,  1.3584, -1.3279,  0.3857,  1.3750]],
       dtype=torch.float64)
	q_value: tensor([[0.0963]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5385405090525299, distance: 1.4194207868953093 entropy 1.8700141906738281
epoch: 0, step: 74
	action: tensor([[-0.8492, -2.6209,  1.2500,  0.0982, -0.6504,  1.7227, -0.0600]],
       dtype=torch.float64)
	q_value: tensor([[0.1330]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 75
	action: tensor([[-0.9119,  1.7046, -0.4734, -0.0767,  2.1952,  0.4731,  0.7449]],
       dtype=torch.float64)
	q_value: tensor([[0.1348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 76
	action: tensor([[ 0.9210,  0.8126, -1.4687, -0.2576, -1.6448,  0.9371,  2.3950]],
       dtype=torch.float64)
	q_value: tensor([[0.1348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9786741553042166, distance: 0.16711283786686693 entropy 1.8700141906738281
epoch: 0, step: 77
	action: tensor([[ 1.9292, -2.2030, -2.6043, -1.2840, -3.1324,  0.0819,  0.0647]],
       dtype=torch.float64)
	q_value: tensor([[0.0848]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 78
	action: tensor([[-0.3434, -1.5637,  1.7564,  1.4650, -1.8539, -2.1197,  2.4143]],
       dtype=torch.float64)
	q_value: tensor([[0.1348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04585904361398252, distance: 1.1177970584612327 entropy 1.8700141906738281
epoch: 0, step: 79
	action: tensor([[-1.5440, -0.5515, -1.7824,  2.4366,  1.4480, -2.3656,  1.0150]],
       dtype=torch.float64)
	q_value: tensor([[0.0923]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 80
	action: tensor([[ 1.0091, -0.6336,  1.3690,  1.2069,  0.8330,  1.2474, -2.8389]],
       dtype=torch.float64)
	q_value: tensor([[0.1348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05047677115371452, distance: 1.1150888932817165 entropy 1.8700141906738281
epoch: 0, step: 81
	action: tensor([[-2.0635,  0.1130,  1.1882, -0.2568,  3.2909,  0.0605,  2.9896]],
       dtype=torch.float64)
	q_value: tensor([[0.0834]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 82
	action: tensor([[ 0.4532,  0.5655, -1.2370, -0.1410, -1.6734,  2.3263, -0.6731]],
       dtype=torch.float64)
	q_value: tensor([[0.1348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7284943941220883, distance: 0.5962743035188093 entropy 1.8700141906738281
epoch: 0, step: 83
	action: tensor([[-0.0085, -0.5490, -0.3845,  1.5013, -2.7048, -1.5533,  0.2229]],
       dtype=torch.float64)
	q_value: tensor([[0.1428]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6218557920398955, distance: 0.7036965464178181 entropy 1.8700141906738281
epoch: 0, step: 84
	action: tensor([[-0.2037,  0.7388, -3.4542, -1.1721,  1.5442,  1.7104,  1.6942]],
       dtype=torch.float64)
	q_value: tensor([[0.1140]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1000353032939183 entropy 1.8700141906738281
epoch: 0, step: 85
	action: tensor([[ 0.6219, -1.7066,  3.0375,  2.7396, -1.9573,  1.8805,  1.6767]],
       dtype=torch.float64)
	q_value: tensor([[0.1348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.004171116646881301, distance: 1.141955163419014 entropy 1.8700141906738281
epoch: 0, step: 86
	action: tensor([[-2.8439,  0.1729,  0.6456,  0.7645, -0.8943,  0.3568,  1.1248]],
       dtype=torch.float64)
	q_value: tensor([[0.0487]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 87
	action: tensor([[-1.0513, -0.4924, -3.1981,  2.7264, -0.9368,  1.4578,  0.1648]],
       dtype=torch.float64)
	q_value: tensor([[0.1348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 88
	action: tensor([[-1.2205, -0.9339,  2.8184, -0.9485,  0.1644, -0.1052,  1.0440]],
       dtype=torch.float64)
	q_value: tensor([[0.1348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.3777847185577552, distance: 1.764586527447141 entropy 1.8700141906738281
epoch: 0, step: 89
	action: tensor([[ 1.5020,  3.5156, -3.2734, -1.5167,  0.2538,  0.9153,  0.2741]],
       dtype=torch.float64)
	q_value: tensor([[0.0847]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 90
	action: tensor([[ 2.7806, -2.6990, -2.3524,  0.8565,  1.0874,  2.5086,  1.2938]],
       dtype=torch.float64)
	q_value: tensor([[0.1348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 91
	action: tensor([[-0.9674, -0.5972, -3.1813,  3.6005,  0.8775, -0.3872,  0.2592]],
       dtype=torch.float64)
	q_value: tensor([[0.1348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 92
	action: tensor([[ 0.5221, -1.5654,  2.0430,  0.2446,  1.1830, -0.2598,  0.9844]],
       dtype=torch.float64)
	q_value: tensor([[0.1348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5488296259128946, distance: 1.4241591250238024 entropy 1.8700141906738281
epoch: 0, step: 93
	action: tensor([[-1.4747, -1.9068, -2.8112,  0.0504,  0.1003,  1.1831,  1.8208]],
       dtype=torch.float64)
	q_value: tensor([[0.0909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 94
	action: tensor([[-0.3563,  1.3215,  3.0326,  1.9024, -0.6974, -0.8698, -0.1003]],
       dtype=torch.float64)
	q_value: tensor([[0.1348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2093274435314063, distance: 1.0175482830298688 entropy 1.8700141906738281
epoch: 0, step: 95
	action: tensor([[ 1.1836, -2.5736, -1.8822, -2.5308, -0.2142, -1.1571, -1.1702]],
       dtype=torch.float64)
	q_value: tensor([[0.0677]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 96
	action: tensor([[-1.5670,  0.8187,  0.2428, -0.2879,  1.1284, -3.0677, -0.6410]],
       dtype=torch.float64)
	q_value: tensor([[0.1348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 97
	action: tensor([[-1.0924,  1.9941,  1.0459, -1.7623,  0.3973, -0.4035, -1.3536]],
       dtype=torch.float64)
	q_value: tensor([[0.1348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 98
	action: tensor([[ 1.1755, -0.3155,  0.2279,  1.7198, -0.8160, -3.1539, -1.0587]],
       dtype=torch.float64)
	q_value: tensor([[0.1348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 99
	action: tensor([[ 1.6688,  1.8507, -2.2622, -0.3547, -2.3097,  0.9459,  3.2449]],
       dtype=torch.float64)
	q_value: tensor([[0.1348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 100
	action: tensor([[-0.9270,  1.7025, -1.4602,  0.8919, -0.2607,  1.1153, -1.1303]],
       dtype=torch.float64)
	q_value: tensor([[0.1348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 101
	action: tensor([[ 3.3238,  2.4433,  0.6227, -1.5281,  1.6738, -0.1686,  0.8552]],
       dtype=torch.float64)
	q_value: tensor([[0.1348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 102
	action: tensor([[-1.9849,  1.6916, -0.4487, -1.1060,  2.1791,  0.4651,  0.5798]],
       dtype=torch.float64)
	q_value: tensor([[0.1348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 103
	action: tensor([[ 2.2062, -1.2376, -1.3809,  0.3189, -0.0503,  2.2779, -3.4708]],
       dtype=torch.float64)
	q_value: tensor([[0.1348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 104
	action: tensor([[ 0.3866,  2.1580, -2.0758, -0.9665,  0.4518,  0.6688,  1.5783]],
       dtype=torch.float64)
	q_value: tensor([[0.1348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 105
	action: tensor([[ 3.7031, -1.2712, -1.0027,  1.4719, -1.2097,  3.9280, -0.1458]],
       dtype=torch.float64)
	q_value: tensor([[0.1348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 106
	action: tensor([[ 1.8675,  1.0107,  0.4805, -1.2578,  1.2900, -1.6704,  1.0869]],
       dtype=torch.float64)
	q_value: tensor([[0.1348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9009797715427419, distance: 0.36009629689884676 entropy 1.8700141906738281
epoch: 0, step: 107
	action: tensor([[ 0.6463, -1.3426,  1.3300, -0.2506, -0.2057,  0.9433,  0.9104]],
       dtype=torch.float64)
	q_value: tensor([[0.0290]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4132747202082385, distance: 1.3604105417126127 entropy 1.8700141906738281
epoch: 0, step: 108
	action: tensor([[-1.3923, -1.4996,  1.8068,  0.8212,  1.5384, -1.4972, -2.6487]],
       dtype=torch.float64)
	q_value: tensor([[0.0882]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 109
	action: tensor([[-0.8013,  0.2790, -1.5725,  0.6310, -1.7552,  0.2403, -1.3040]],
       dtype=torch.float64)
	q_value: tensor([[0.1348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9641150988628047, distance: 1.6037628915281414 entropy 1.8700141906738281
epoch: 0, step: 110
	action: tensor([[ 1.1508, -3.5781, -0.8847,  2.5469,  2.1247,  1.2540, -1.1435]],
       dtype=torch.float64)
	q_value: tensor([[0.1835]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 111
	action: tensor([[ 0.0668, -2.4007,  2.3005, -1.7135,  1.1042, -0.9554, -0.5508]],
       dtype=torch.float64)
	q_value: tensor([[0.1348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 112
	action: tensor([[ 2.2421,  0.5327,  0.6598, -1.0974, -1.1962,  0.1049,  0.7624]],
       dtype=torch.float64)
	q_value: tensor([[0.1348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6942537217603311, distance: 0.6327574728537626 entropy 1.8700141906738281
epoch: 0, step: 113
	action: tensor([[-1.5650, -1.4370, -2.5331, -0.2617,  3.1994, -0.9540, -1.1397]],
       dtype=torch.float64)
	q_value: tensor([[0.0817]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 114
	action: tensor([[-0.1512, -1.4143, -0.0693, -1.5550, -1.1954, -2.1586, -1.3258]],
       dtype=torch.float64)
	q_value: tensor([[0.1348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26801259588379645, distance: 0.979058173066302 entropy 1.8700141906738281
epoch: 0, step: 115
	action: tensor([[-1.4774, -1.6650, -2.0807, -0.7101,  0.9710, -0.0307,  0.6777]],
       dtype=torch.float64)
	q_value: tensor([[0.1451]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 116
	action: tensor([[-0.8861,  0.6720, -0.2258, -3.0074, -1.0033,  2.2872, -0.2508]],
       dtype=torch.float64)
	q_value: tensor([[0.1348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 117
	action: tensor([[-1.5820, -4.3385, -0.0761,  1.0548,  0.6802,  1.4676,  0.2679]],
       dtype=torch.float64)
	q_value: tensor([[0.1348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 118
	action: tensor([[ 1.3660, -1.0372, -0.9634,  1.1775,  1.6717, -0.8106, -1.4789]],
       dtype=torch.float64)
	q_value: tensor([[0.1348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3211436100628604, distance: 0.9428566063659345 entropy 1.8700141906738281
epoch: 0, step: 119
	action: tensor([[ 1.5093, -1.4614, -0.1723, -0.9427,  1.0491, -0.3585,  2.0490]],
       dtype=torch.float64)
	q_value: tensor([[0.0955]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 120
	action: tensor([[-1.4982,  1.8184,  1.1850, -1.4266,  0.9364,  1.1527,  1.6469]],
       dtype=torch.float64)
	q_value: tensor([[0.1348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 121
	action: tensor([[ 0.9246, -0.1032, -2.1555,  1.5786, -0.9159,  2.5194, -0.3380]],
       dtype=torch.float64)
	q_value: tensor([[0.1348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33614009881723317, distance: 0.9323842135367595 entropy 1.8700141906738281
epoch: 0, step: 122
	action: tensor([[ 2.2538, -0.7552, -2.0149,  2.5505,  0.1502, -2.2700, -2.1838]],
       dtype=torch.float64)
	q_value: tensor([[0.1401]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 123
	action: tensor([[ 1.3194,  1.0413,  0.0728, -0.8402, -0.0736, -2.2829,  0.9827]],
       dtype=torch.float64)
	q_value: tensor([[0.1348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7349630504556222, distance: 0.5891283271048517 entropy 1.8700141906738281
epoch: 0, step: 124
	action: tensor([[ 1.5979,  0.6576, -1.1605,  2.2080,  0.7905,  1.6248,  0.1420]],
       dtype=torch.float64)
	q_value: tensor([[0.0635]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4135911479317206, distance: 0.8763083542320682 entropy 1.8700141906738281
epoch: 0, step: 125
	action: tensor([[ 0.3002,  2.1894, -0.1208,  1.4810,  1.2686,  0.0455, -0.5929]],
       dtype=torch.float64)
	q_value: tensor([[0.0911]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 126
	action: tensor([[ 0.0497, -1.3322, -0.7186, -2.8847, -1.2889, -0.9196, -0.3079]],
       dtype=torch.float64)
	q_value: tensor([[0.1348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 0, step: 127
	action: tensor([[-0.5280,  1.6251, -0.6264, -1.5663, -1.7155, -0.3389,  1.2214]],
       dtype=torch.float64)
	q_value: tensor([[0.1348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7189666073782647, distance: 0.6066464442071774 entropy 1.8700141906738281
LOSS epoch 0 actor 598.2239598134622 critic 2336.8939473664886 
epoch: 1, step: 0
	action: tensor([[ 1.4607, -2.3473,  1.3144, -0.1188, -0.7749, -1.5091, -0.6201]],
       dtype=torch.float64)
	q_value: tensor([[-0.1900]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 1
	action: tensor([[ 0.4669,  1.1111,  2.0697,  2.1573, -0.7956, -3.0568, -1.2549]],
       dtype=torch.float64)
	q_value: tensor([[-0.0928]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 2
	action: tensor([[ 2.4068, -0.6443, -2.0625,  1.3157,  1.0639, -3.8451, -2.0214]],
       dtype=torch.float64)
	q_value: tensor([[-0.0928]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 3
	action: tensor([[-1.4475,  0.5025, -0.9266,  0.1145, -0.7565, -3.5682, -0.8193]],
       dtype=torch.float64)
	q_value: tensor([[-0.0928]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 4
	action: tensor([[-2.9772, -0.2393, -2.2325, -0.3923, -2.3240, -1.1654, -1.2152]],
       dtype=torch.float64)
	q_value: tensor([[-0.0928]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 5
	action: tensor([[ 0.7616,  0.3432, -2.4067, -1.8504, -2.1052,  0.2161, -1.8603]],
       dtype=torch.float64)
	q_value: tensor([[-0.0928]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5275037645623886, distance: 0.7866034911737572 entropy 1.8700141906738281
epoch: 1, step: 6
	action: tensor([[-1.7246, -0.7642,  2.8304, -0.5078,  0.6866,  0.7068, -3.1002]],
       dtype=torch.float64)
	q_value: tensor([[-0.1995]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.8870357776127564, distance: 1.9443873020632871 entropy 1.8700141906738281
epoch: 1, step: 7
	action: tensor([[-3.0042,  1.2708, -1.5215, -0.3820, -0.7047, -1.2523,  1.9150]],
       dtype=torch.float64)
	q_value: tensor([[-0.1871]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 8
	action: tensor([[-0.0536, -3.3002, -1.3786, -0.6237,  2.2164,  2.5908, -0.0397]],
       dtype=torch.float64)
	q_value: tensor([[-0.0928]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 9
	action: tensor([[ 1.3169, -0.5039, -2.1001, -1.7167, -0.0423,  2.0351, -0.6568]],
       dtype=torch.float64)
	q_value: tensor([[-0.0928]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4295584963387036, distance: 0.8642954988819213 entropy 1.8700141906738281
epoch: 1, step: 10
	action: tensor([[ 0.4847,  2.8919,  1.8822, -2.3799, -1.2792, -0.3774,  1.7891]],
       dtype=torch.float64)
	q_value: tensor([[-0.1697]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 11
	action: tensor([[ 4.4445,  0.5116, -0.7409,  1.3652,  0.6173, -2.6625, -0.5265]],
       dtype=torch.float64)
	q_value: tensor([[-0.0928]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 12
	action: tensor([[-1.1904, -1.2908, -0.0408, -0.6599, -1.3675,  2.9623, -1.8159]],
       dtype=torch.float64)
	q_value: tensor([[-0.0928]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 13
	action: tensor([[ 0.1607,  0.2069, -0.7307,  0.8991,  1.6045, -0.6235, -3.1859]],
       dtype=torch.float64)
	q_value: tensor([[-0.0928]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9891096203300366 entropy 1.8700141906738281
epoch: 1, step: 14
	action: tensor([[ 0.7757, -0.1900,  1.9232,  1.4984, -1.7919,  1.1380, -0.5691]],
       dtype=torch.float64)
	q_value: tensor([[-0.0928]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6279520344894827, distance: 0.6980011855540148 entropy 1.8700141906738281
epoch: 1, step: 15
	action: tensor([[ 2.8758,  1.6949,  0.9396, -0.8164,  0.3913,  3.0887, -1.3494]],
       dtype=torch.float64)
	q_value: tensor([[-0.2508]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 16
	action: tensor([[ 1.8651,  2.4245, -1.5559, -2.4559, -1.9343,  0.1464,  0.3318]],
       dtype=torch.float64)
	q_value: tensor([[-0.0928]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 17
	action: tensor([[-1.2324,  0.2256, -0.0458,  1.6220,  1.4036,  2.4371,  2.3383]],
       dtype=torch.float64)
	q_value: tensor([[-0.0928]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3279123864299125, distance: 0.9381442888011218 entropy 1.8700141906738281
epoch: 1, step: 18
	action: tensor([[ 1.2613, -2.3607, -2.5256, -2.3305,  0.8903, -1.4396, -1.5343]],
       dtype=torch.float64)
	q_value: tensor([[-0.1783]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 19
	action: tensor([[-1.2928, -0.7009, -1.8933,  0.3439,  0.3680,  1.2764, -0.2091]],
       dtype=torch.float64)
	q_value: tensor([[-0.0928]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.4167720457086022, distance: 1.7789942227409254 entropy 1.8700141906738281
epoch: 1, step: 20
	action: tensor([[-3.0402, -1.9069, -1.8688,  0.7153,  1.1094, -1.2173,  1.2563]],
       dtype=torch.float64)
	q_value: tensor([[-0.0746]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 21
	action: tensor([[-0.9075,  0.5859,  2.4318,  0.0500,  0.4201, -1.1972, -0.5965]],
       dtype=torch.float64)
	q_value: tensor([[-0.0928]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13560050632948917, distance: 1.2194653985049337 entropy 1.8700141906738281
epoch: 1, step: 22
	action: tensor([[-2.0832e+00,  2.2031e-01, -8.4886e-01, -1.1589e-03, -7.5009e-01,
         -1.9213e+00,  2.1720e+00]], dtype=torch.float64)
	q_value: tensor([[-0.0932]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 23
	action: tensor([[-1.3004,  1.2179,  0.7188,  0.2272,  1.2560, -0.0375, -0.9589]],
       dtype=torch.float64)
	q_value: tensor([[-0.0928]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 24
	action: tensor([[ 0.4558, -0.0217, -0.9262, -0.0281, -1.2543, -1.8698, -1.9520]],
       dtype=torch.float64)
	q_value: tensor([[-0.0928]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9017244297088609, distance: 0.35873973221538574 entropy 1.8700141906738281
epoch: 1, step: 25
	action: tensor([[ 0.2335,  0.2581, -4.6559,  1.6557,  1.6715, -1.4365,  2.2176]],
       dtype=torch.float64)
	q_value: tensor([[-0.0862]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.37012512957416466 entropy 1.8700141906738281
epoch: 1, step: 26
	action: tensor([[-0.7956,  1.6720,  2.9431, -0.4808, -0.5230, -1.3801,  1.9102]],
       dtype=torch.float64)
	q_value: tensor([[-0.0928]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12946248917364933, distance: 1.0677029383453445 entropy 1.8700141906738281
epoch: 1, step: 27
	action: tensor([[-0.5746,  0.6973, -1.2605,  1.6124, -2.2986, -0.3343,  1.3309]],
       dtype=torch.float64)
	q_value: tensor([[-0.2298]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5015123097983263, distance: 1.4022360939318497 entropy 1.8700141906738281
epoch: 1, step: 28
	action: tensor([[-0.3319, -0.0986,  0.1884, -0.0248,  1.4847, -1.1892,  2.5686]],
       dtype=torch.float64)
	q_value: tensor([[-0.1606]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.351945085367972, distance: 1.3305653788040912 entropy 1.8700141906738281
epoch: 1, step: 29
	action: tensor([[ 0.0322, -0.0883,  2.5424,  1.0469,  0.3390, -2.6085,  1.0374]],
       dtype=torch.float64)
	q_value: tensor([[-0.1509]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 30
	action: tensor([[ 2.1025,  0.9473, -1.3757, -0.1571, -1.1780, -0.4832, -1.7894]],
       dtype=torch.float64)
	q_value: tensor([[-0.0928]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 31
	action: tensor([[-1.8751,  3.0868, -1.7341,  0.6795, -1.0645, -1.3215,  0.5918]],
       dtype=torch.float64)
	q_value: tensor([[-0.0928]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 32
	action: tensor([[-2.2117, -1.0676,  2.3497,  0.3478, -0.3066,  1.0949,  1.1265]],
       dtype=torch.float64)
	q_value: tensor([[-0.0928]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 33
	action: tensor([[-0.6724,  0.4919,  0.5269, -0.2558, -0.5252, -1.8490, -0.1200]],
       dtype=torch.float64)
	q_value: tensor([[-0.0928]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14793509443396835, distance: 1.2260702649062498 entropy 1.8700141906738281
epoch: 1, step: 34
	action: tensor([[ 1.8687, -0.5744,  0.2532,  0.0401,  1.6359,  1.6579,  0.1432]],
       dtype=torch.float64)
	q_value: tensor([[-0.0545]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17750497168722035, distance: 1.2417611492052594 entropy 1.8700141906738281
epoch: 1, step: 35
	action: tensor([[ 0.4932,  1.6292,  1.6031, -0.0748,  0.8056,  0.9211, -1.0854]],
       dtype=torch.float64)
	q_value: tensor([[-0.1198]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 36
	action: tensor([[-0.5125, -0.5239, -0.4518, -0.5073, -1.1061, -1.2608, -1.1735]],
       dtype=torch.float64)
	q_value: tensor([[-0.0928]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05036805806688216, distance: 1.1728094222992462 entropy 1.8700141906738281
epoch: 1, step: 37
	action: tensor([[ 0.4135, -1.1571,  2.1157, -0.1291, -2.1577, -2.0942, -3.7540]],
       dtype=torch.float64)
	q_value: tensor([[-0.0401]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.325703323897367 entropy 1.8700141906738281
epoch: 1, step: 38
	action: tensor([[-0.8501, -2.2043, -2.0115,  0.0939, -0.6060, -0.6155,  0.3794]],
       dtype=torch.float64)
	q_value: tensor([[-0.0928]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 39
	action: tensor([[-0.3027,  2.1858, -0.6265, -1.8805, -1.5203,  1.2076, -1.5191]],
       dtype=torch.float64)
	q_value: tensor([[-0.0928]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 40
	action: tensor([[ 0.2525, -1.3504, -0.1190, -0.4275, -0.6149,  0.1849,  0.5245]],
       dtype=torch.float64)
	q_value: tensor([[-0.0928]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9151589528039712, distance: 1.5836496377701939 entropy 1.8700141906738281
epoch: 1, step: 41
	action: tensor([[ 2.1880, -0.5726,  1.6256,  2.1461, -1.7910, -1.5045, -0.0375]],
       dtype=torch.float64)
	q_value: tensor([[-0.0515]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 42
	action: tensor([[-0.8248, -1.5118, -2.0247, -2.8173,  0.8545,  0.3500,  2.2374]],
       dtype=torch.float64)
	q_value: tensor([[-0.0928]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 43
	action: tensor([[-0.9520, -0.2842, -0.3389, -1.9513,  2.4090,  1.2787,  1.8359]],
       dtype=torch.float64)
	q_value: tensor([[-0.0928]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3701571454278102, distance: 0.9081818047474023 entropy 1.8700141906738281
epoch: 1, step: 44
	action: tensor([[ 1.1706,  2.7596,  0.9319, -3.0616, -2.1207,  1.4751,  0.1362]],
       dtype=torch.float64)
	q_value: tensor([[-0.1675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 45
	action: tensor([[ 0.2698,  0.9760,  2.4207, -1.7642,  1.5889,  2.0179,  2.0630]],
       dtype=torch.float64)
	q_value: tensor([[-0.0928]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3310089208071272, distance: 0.9359806201163723 entropy 1.8700141906738281
epoch: 1, step: 46
	action: tensor([[-1.2198, -0.9553,  3.6980, -0.6235,  2.2616, -3.0087,  1.8701]],
       dtype=torch.float64)
	q_value: tensor([[-0.2511]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 47
	action: tensor([[ 1.0918,  0.5177, -1.4624, -0.8733, -1.8390,  1.0347,  2.7377]],
       dtype=torch.float64)
	q_value: tensor([[-0.0928]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6134474971740167, distance: 0.711477119411709 entropy 1.8700141906738281
epoch: 1, step: 48
	action: tensor([[ 0.6289, -0.4421,  0.5101, -0.4147,  1.0145,  0.2421, -0.8042]],
       dtype=torch.float64)
	q_value: tensor([[-0.2948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03621653557531446, distance: 1.1234310648186965 entropy 1.8700141906738281
epoch: 1, step: 49
	action: tensor([[ 1.9723,  1.2984,  0.3296, -2.1972,  0.5619,  2.3894,  0.9273]],
       dtype=torch.float64)
	q_value: tensor([[-0.0300]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 50
	action: tensor([[-0.0637, -0.5278, -1.6619, -0.8065,  2.6208, -2.0699,  0.9024]],
       dtype=torch.float64)
	q_value: tensor([[-0.0928]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04894373120841822, distance: 1.1720139722520868 entropy 1.8700141906738281
epoch: 1, step: 51
	action: tensor([[ 1.3126,  1.2027, -0.4326, -1.2978,  0.3188, -2.6129,  1.3574]],
       dtype=torch.float64)
	q_value: tensor([[-0.1665]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 52
	action: tensor([[-0.0074, -1.2705, -3.3993,  1.6053,  0.6249,  0.5175, -0.8543]],
       dtype=torch.float64)
	q_value: tensor([[-0.0928]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 53
	action: tensor([[ 0.5054,  3.3519, -1.4075,  0.1660,  1.0584,  2.3328,  2.3355]],
       dtype=torch.float64)
	q_value: tensor([[-0.0928]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 54
	action: tensor([[ 1.4924,  0.4350, -2.5645,  3.5444, -1.2756,  0.1146,  1.1540]],
       dtype=torch.float64)
	q_value: tensor([[-0.0928]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 55
	action: tensor([[ 0.1781,  1.4802, -0.7175, -1.6367,  0.3513,  1.1484,  0.4677]],
       dtype=torch.float64)
	q_value: tensor([[-0.0928]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9507149577392265, distance: 0.2540471106466069 entropy 1.8700141906738281
epoch: 1, step: 56
	action: tensor([[ 0.3379,  0.3660,  0.6246, -0.8829,  2.3995, -1.9044,  1.4134]],
       dtype=torch.float64)
	q_value: tensor([[-0.1278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6893587169991603, distance: 0.6378025905791195 entropy 1.8700141906738281
epoch: 1, step: 57
	action: tensor([[-2.2389,  0.9891,  1.5022,  0.8585,  1.1689,  0.8688,  0.1038]],
       dtype=torch.float64)
	q_value: tensor([[-0.1735]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 58
	action: tensor([[ 0.1731, -0.2816,  1.4909, -1.0676, -1.0238,  0.3065, -1.5849]],
       dtype=torch.float64)
	q_value: tensor([[-0.0928]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2134858052621511, distance: 1.2605905396147403 entropy 1.8700141906738281
epoch: 1, step: 59
	action: tensor([[ 2.8413,  0.7096,  1.4318,  1.7277,  2.3708, -0.8563,  1.7213]],
       dtype=torch.float64)
	q_value: tensor([[-0.1447]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 60
	action: tensor([[ 1.0659, -2.6342,  0.1220,  0.5979, -0.5897, -1.7297,  3.1942]],
       dtype=torch.float64)
	q_value: tensor([[-0.0928]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 61
	action: tensor([[ 1.0209, -0.6822,  1.0870, -0.8247,  1.0675, -0.6623,  3.0933]],
       dtype=torch.float64)
	q_value: tensor([[-0.0928]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.081113767227297 entropy 1.8700141906738281
epoch: 1, step: 62
	action: tensor([[ 1.7039, -0.6389,  1.6214,  0.0346,  2.4823, -0.7791,  2.8821]],
       dtype=torch.float64)
	q_value: tensor([[-0.0928]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1733745456763307, distance: 1.2395813247331586 entropy 1.8700141906738281
epoch: 1, step: 63
	action: tensor([[ 1.0046, -1.5841,  1.9987, -1.2539,  1.2169, -0.3239, -0.4347]],
       dtype=torch.float64)
	q_value: tensor([[-0.2619]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 64
	action: tensor([[-2.6560, -3.9949, -0.3858,  0.8485,  1.8029, -1.0247, -2.3165]],
       dtype=torch.float64)
	q_value: tensor([[-0.0928]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 65
	action: tensor([[-1.8807,  2.0983, -2.1899,  0.0244,  0.7622, -2.2427,  1.1310]],
       dtype=torch.float64)
	q_value: tensor([[-0.0928]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 66
	action: tensor([[-0.6614,  1.8629, -2.1027, -0.5773, -2.3072,  0.2110, -3.0195]],
       dtype=torch.float64)
	q_value: tensor([[-0.0928]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 67
	action: tensor([[-1.2411, -0.3088, -0.0830, -0.0853, -1.5679,  0.9775,  0.5180]],
       dtype=torch.float64)
	q_value: tensor([[-0.0928]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.5515775894156452, distance: 1.8279364071493003 entropy 1.8700141906738281
epoch: 1, step: 68
	action: tensor([[ 0.9815, -1.8855,  0.7484, -0.2929,  0.4114,  4.1462,  1.5710]],
       dtype=torch.float64)
	q_value: tensor([[-0.1259]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 69
	action: tensor([[ 3.3194, -1.0486,  0.7846, -0.0989, -1.3500,  0.2589, -2.4247]],
       dtype=torch.float64)
	q_value: tensor([[-0.0928]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 70
	action: tensor([[ 1.4275,  2.1311,  2.6231,  0.5287,  0.8502, -1.1688, -0.2628]],
       dtype=torch.float64)
	q_value: tensor([[-0.0928]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 71
	action: tensor([[-0.7316,  1.2161,  1.6269, -2.1813, -0.7016,  2.3687,  0.2039]],
       dtype=torch.float64)
	q_value: tensor([[-0.0928]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15899997408289845, distance: 1.0494329544604024 entropy 1.8700141906738281
epoch: 1, step: 72
	action: tensor([[3.9155, 0.6280, 0.2466, 0.1345, 1.5860, 0.7352, 0.6904]],
       dtype=torch.float64)
	q_value: tensor([[-0.2536]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 73
	action: tensor([[ 2.7930,  0.6551, -0.0267, -2.8312,  2.2935, -0.7013,  2.5123]],
       dtype=torch.float64)
	q_value: tensor([[-0.0928]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 74
	action: tensor([[-1.6851,  0.0364,  1.7690, -2.0532, -0.0632, -2.3524,  2.6048]],
       dtype=torch.float64)
	q_value: tensor([[-0.0928]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 75
	action: tensor([[ 0.5940,  0.9117,  3.0193, -0.6688, -1.7216, -0.3150, -2.5504]],
       dtype=torch.float64)
	q_value: tensor([[-0.0928]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 76
	action: tensor([[-0.9466,  2.2106,  0.5178, -0.4972, -2.3858, -1.9287, -2.2913]],
       dtype=torch.float64)
	q_value: tensor([[-0.0928]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 77
	action: tensor([[-3.0841,  1.7058,  1.4518,  1.0535,  1.8164,  2.7334,  0.8055]],
       dtype=torch.float64)
	q_value: tensor([[-0.0928]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 78
	action: tensor([[ 0.7222,  0.1689, -1.3531, -1.0786, -1.0097,  0.5543, -2.2115]],
       dtype=torch.float64)
	q_value: tensor([[-0.0928]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6375915263535066, distance: 0.6888994928813058 entropy 1.8700141906738281
epoch: 1, step: 79
	action: tensor([[-0.2365,  4.5060,  0.5470,  0.4411, -0.8251, -0.7751, -0.7027]],
       dtype=torch.float64)
	q_value: tensor([[-0.1069]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 80
	action: tensor([[ 0.1081, -0.2522, -2.0329,  0.5243, -0.9093, -0.0102,  1.2024]],
       dtype=torch.float64)
	q_value: tensor([[-0.0928]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18860287430816136, distance: 1.247599182018523 entropy 1.8700141906738281
epoch: 1, step: 81
	action: tensor([[ 1.5021,  1.0021,  2.2698, -1.0818,  0.0131, -3.0030,  0.1735]],
       dtype=torch.float64)
	q_value: tensor([[-0.1121]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 82
	action: tensor([[ 0.6360, -2.6776, -2.0753,  1.4721,  1.3753,  0.2098, -1.7713]],
       dtype=torch.float64)
	q_value: tensor([[-0.0928]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 83
	action: tensor([[-2.5133,  1.2462, -0.4277, -3.9379,  0.0523, -0.9359,  3.6191]],
       dtype=torch.float64)
	q_value: tensor([[-0.0928]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 84
	action: tensor([[-0.6096,  0.3637,  0.4479,  2.8353,  0.2954,  1.0497, -0.9340]],
       dtype=torch.float64)
	q_value: tensor([[-0.0928]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5271886181344314, distance: 0.7868657725849483 entropy 1.8700141906738281
epoch: 1, step: 85
	action: tensor([[-1.4010,  1.6100,  0.3794,  0.3140,  4.6472,  1.3664,  0.8463]],
       dtype=torch.float64)
	q_value: tensor([[-0.1022]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 86
	action: tensor([[-1.1747, -0.9152, -0.5174, -0.0977, -1.8297, -0.4829, -0.5086]],
       dtype=torch.float64)
	q_value: tensor([[-0.0928]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.3870202220529526, distance: 1.7680101029740176 entropy 1.8700141906738281
epoch: 1, step: 87
	action: tensor([[-0.9009, -0.8821, -2.1003,  0.5946, -1.1262,  1.9962,  2.2455]],
       dtype=torch.float64)
	q_value: tensor([[-0.0708]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5093408501759049, distance: 1.4058868101120776 entropy 1.8700141906738281
epoch: 1, step: 88
	action: tensor([[-1.4002,  2.1423,  0.2253,  1.6900, -1.4726, -0.9239, -0.7907]],
       dtype=torch.float64)
	q_value: tensor([[-0.2070]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 89
	action: tensor([[-1.4905,  0.7514,  1.0205,  0.0611,  0.7964, -0.4524, -0.0199]],
       dtype=torch.float64)
	q_value: tensor([[-0.0928]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8370804077719618, distance: 1.551032064282813 entropy 1.8700141906738281
epoch: 1, step: 90
	action: tensor([[-0.6064,  2.1291,  1.1354, -1.3817, -1.0253, -0.4659, -1.2948]],
       dtype=torch.float64)
	q_value: tensor([[-0.0321]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 91
	action: tensor([[-1.5010,  2.0040,  0.5404,  1.0557, -1.4641,  1.7344, -0.4585]],
       dtype=torch.float64)
	q_value: tensor([[-0.0928]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 92
	action: tensor([[-0.0104,  2.9052, -0.9694,  1.1549,  2.7152, -0.1666, -0.0712]],
       dtype=torch.float64)
	q_value: tensor([[-0.0928]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 93
	action: tensor([[ 0.6766, -0.5171, -2.1930,  0.8605,  0.4465, -0.6285,  0.7082]],
       dtype=torch.float64)
	q_value: tensor([[-0.0928]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06465379488251033, distance: 1.1807579993333415 entropy 1.8700141906738281
epoch: 1, step: 94
	action: tensor([[ 2.2893,  1.5364,  0.5028, -0.9621, -3.0071, -2.8845,  0.6888]],
       dtype=torch.float64)
	q_value: tensor([[-0.0980]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 95
	action: tensor([[-1.2779, -0.8309, -0.5830, -0.0031, -0.1615,  0.7284, -0.2451]],
       dtype=torch.float64)
	q_value: tensor([[-0.0928]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.624491427092695, distance: 1.853869982577838 entropy 1.8700141906738281
epoch: 1, step: 96
	action: tensor([[-0.0655, -0.3905, -0.3223,  2.8099, -0.9679, -1.9640, -0.2624]],
       dtype=torch.float64)
	q_value: tensor([[-0.0373]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6820006014536105, distance: 0.6453121506456972 entropy 1.8700141906738281
epoch: 1, step: 97
	action: tensor([[ 0.5121, -0.2520,  1.1478,  0.5445,  0.6506,  2.1194,  1.0550]],
       dtype=torch.float64)
	q_value: tensor([[-0.1303]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6416130949048695, distance: 0.6850665454353607 entropy 1.8700141906738281
epoch: 1, step: 98
	action: tensor([[ 0.7869,  2.3015, -1.7624, -1.6012,  0.0858,  1.0160,  1.2133]],
       dtype=torch.float64)
	q_value: tensor([[-0.1487]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 99
	action: tensor([[-2.0058, -0.9172,  2.4229, -1.5277,  1.9951,  0.6725, -0.6776]],
       dtype=torch.float64)
	q_value: tensor([[-0.0928]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 100
	action: tensor([[-0.3252,  0.4602,  1.0272, -0.0623,  0.0808, -0.6061, -1.6112]],
       dtype=torch.float64)
	q_value: tensor([[-0.0928]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.053498523884408344, distance: 1.1745558141009393 entropy 1.8700141906738281
epoch: 1, step: 101
	action: tensor([[ 0.5756, -0.6588, -1.5870, -3.8149, -1.3590, -1.9757,  1.0293]],
       dtype=torch.float64)
	q_value: tensor([[-0.0434]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 102
	action: tensor([[ 2.4443, -1.2618,  0.8611,  1.3743, -0.0648, -1.1550, -1.6814]],
       dtype=torch.float64)
	q_value: tensor([[-0.0928]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 103
	action: tensor([[-0.0963, -0.1109,  1.3522, -0.5298,  1.4949, -0.7224, -0.1035]],
       dtype=torch.float64)
	q_value: tensor([[-0.0928]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14193542401777415, distance: 1.2228620459623125 entropy 1.8700141906738281
epoch: 1, step: 104
	action: tensor([[-1.6949, -0.7313,  0.2075, -1.0692,  2.3142, -2.3681, -1.3651]],
       dtype=torch.float64)
	q_value: tensor([[-0.0568]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.00748887788607, distance: 1.6213742340806212 entropy 1.8700141906738281
epoch: 1, step: 105
	action: tensor([[ 0.7450,  0.6782, -1.1838, -0.6461,  0.1421, -0.0832,  2.5422]],
       dtype=torch.float64)
	q_value: tensor([[-0.1095]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9403362265546912, distance: 0.279519461994575 entropy 1.8700141906738281
epoch: 1, step: 106
	action: tensor([[ 0.7660, -0.3276,  2.6553,  0.0808, -2.8837, -0.9442,  1.2511]],
       dtype=torch.float64)
	q_value: tensor([[-0.1815]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20399328177075826, distance: 1.0209748870195081 entropy 1.8700141906738281
epoch: 1, step: 107
	action: tensor([[-0.7529,  0.0102, -2.9776,  1.0429, -0.6388, -1.3267,  0.8379]],
       dtype=torch.float64)
	q_value: tensor([[-0.3145]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8377054098106738, distance: 1.5512958839178408 entropy 1.8700141906738281
epoch: 1, step: 108
	action: tensor([[-0.3077,  1.4779, -0.9563, -0.6641,  0.5315, -1.3862,  0.9384]],
       dtype=torch.float64)
	q_value: tensor([[-0.1258]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7658788394587561, distance: 0.5537031923133959 entropy 1.8700141906738281
epoch: 1, step: 109
	action: tensor([[ 0.9380,  2.4698, -0.9183, -0.3678, -1.2379, -0.8404,  2.8601]],
       dtype=torch.float64)
	q_value: tensor([[-0.0869]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 110
	action: tensor([[ 1.9317, -1.1259,  0.1596,  0.0802,  0.5272,  2.1323, -1.0091]],
       dtype=torch.float64)
	q_value: tensor([[-0.0928]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03946087974821022, distance: 1.1215385912137876 entropy 1.8700141906738281
epoch: 1, step: 111
	action: tensor([[-0.1397, -0.9665,  0.6852,  0.5207,  1.5118,  2.2864,  2.9215]],
       dtype=torch.float64)
	q_value: tensor([[-0.1372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 112
	action: tensor([[-0.3556, -0.0327, -1.0270, -0.1062, -3.9327,  2.8741, -0.5360]],
       dtype=torch.float64)
	q_value: tensor([[-0.0928]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 113
	action: tensor([[ 1.8162,  0.7410,  2.9533, -3.8071, -0.3391, -0.1440,  1.3024]],
       dtype=torch.float64)
	q_value: tensor([[-0.0928]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 114
	action: tensor([[-1.3961, -1.1509, -1.2284, -0.3412, -0.7393,  1.0039, -1.6391]],
       dtype=torch.float64)
	q_value: tensor([[-0.0928]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.3684448962674645, distance: 1.761117512699172 entropy 1.8700141906738281
epoch: 1, step: 115
	action: tensor([[ 0.7673, -1.2965,  1.4544,  1.5727,  0.6881, -2.0834,  1.9594]],
       dtype=torch.float64)
	q_value: tensor([[-0.0746]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 116
	action: tensor([[-0.0718,  1.2583,  3.0910, -0.3455, -0.3944,  1.7344, -1.4432]],
       dtype=torch.float64)
	q_value: tensor([[-0.0928]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7468764052733975, distance: 0.575735498699907 entropy 1.8700141906738281
epoch: 1, step: 117
	action: tensor([[ 0.8050,  0.8030, -1.7836,  2.6250,  1.4211, -0.2452, -2.7760]],
       dtype=torch.float64)
	q_value: tensor([[-0.2695]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20024461740466037, distance: 1.0233761209955448 entropy 1.8700141906738281
epoch: 1, step: 118
	action: tensor([[ 1.4167, -1.5109, -1.9020,  2.3974,  0.0610,  1.5084,  0.0081]],
       dtype=torch.float64)
	q_value: tensor([[-0.1462]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 119
	action: tensor([[-1.2116, -0.3924,  0.9934, -2.3276, -0.0676, -1.8770,  0.2691]],
       dtype=torch.float64)
	q_value: tensor([[-0.0928]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2360354790788165, distance: 1.000214827351247 entropy 1.8700141906738281
epoch: 1, step: 120
	action: tensor([[-2.7171,  3.0091,  0.6989,  0.8946,  0.2606,  0.2276,  0.6458]],
       dtype=torch.float64)
	q_value: tensor([[-0.1283]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 121
	action: tensor([[ 1.3311,  0.5593,  1.5592,  0.3225, -0.5685,  1.2390,  3.4875]],
       dtype=torch.float64)
	q_value: tensor([[-0.0928]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7286069325885594 entropy 1.8700141906738281
epoch: 1, step: 122
	action: tensor([[ 0.3039, -0.5838,  0.7883,  0.1815,  0.1385,  0.0688,  0.6489]],
       dtype=torch.float64)
	q_value: tensor([[-0.0928]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1868115105229078, distance: 1.0319349096783335 entropy 1.8700141906738281
epoch: 1, step: 123
	action: tensor([[ 0.4400,  1.5076, -0.2848, -0.3429, -1.7199, -1.6767, -0.5623]],
       dtype=torch.float64)
	q_value: tensor([[-0.0494]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7029425459562186, distance: 0.623701689608074 entropy 1.8700141906738281
epoch: 1, step: 124
	action: tensor([[ 2.4569, -0.0697,  1.3479, -2.3956, -1.6377,  0.7412, -3.3273]],
       dtype=torch.float64)
	q_value: tensor([[-0.1229]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 125
	action: tensor([[-2.0162, -3.4882, -1.3045, -1.6669,  0.1603,  0.3539,  1.1865]],
       dtype=torch.float64)
	q_value: tensor([[-0.0928]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
epoch: 1, step: 126
	action: tensor([[-1.5583,  0.5040, -2.7695, -1.4843,  1.4608,  0.6943,  1.3971]],
       dtype=torch.float64)
	q_value: tensor([[-0.0928]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26023308860961714, distance: 0.9842471010652186 entropy 1.8700141906738281
epoch: 1, step: 127
	action: tensor([[ 1.3550,  1.0209,  0.2115,  0.6038, -3.3018,  0.1687, -1.2134]],
       dtype=torch.float64)
	q_value: tensor([[-0.2014]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.8700141906738281
LOSS epoch 1 actor 561.6810939111186 critic 2324.5837552806333 
epoch: 2, step: 0
	action: tensor([[-0.1149,  1.9280, -0.2204, -2.3205,  0.4590,  1.3100,  1.5280]],
       dtype=torch.float64)
	q_value: tensor([[-0.3424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 2, step: 1
	action: tensor([[-0.8377,  1.5414, -0.2202,  2.3514,  1.3848,  0.9946, -0.2836]],
       dtype=torch.float64)
	q_value: tensor([[-0.3424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.47532459114979586, distance: 0.8288998252480511 entropy 1.7646539211273193
epoch: 2, step: 2
	action: tensor([[-1.9481, -0.0150, -1.3011,  1.1749, -3.5976,  0.9489,  0.4172]],
       dtype=torch.float64)
	q_value: tensor([[-0.3698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 2, step: 3
	action: tensor([[ 1.8246,  0.0596,  0.5555,  0.0607, -0.3691, -1.1492,  2.5936]],
       dtype=torch.float64)
	q_value: tensor([[-0.3424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2608068578672613, distance: 0.9838653319804253 entropy 1.7646539211273193
epoch: 2, step: 4
	action: tensor([[-1.6166,  0.3393,  0.3931, -0.6690,  2.0036,  0.7319,  0.5353]],
       dtype=torch.float64)
	q_value: tensor([[-0.5588]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.3602871947463857, distance: 1.7580819632405469 entropy 1.7646539211273193
epoch: 2, step: 5
	action: tensor([[ 1.5483,  3.1629, -1.5198,  1.1547,  0.2999,  0.0845, -0.4004]],
       dtype=torch.float64)
	q_value: tensor([[-0.3432]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 2, step: 6
	action: tensor([[-2.4886,  4.1597, -0.0750, -0.9685,  0.5838,  1.6689, -1.2819]],
       dtype=torch.float64)
	q_value: tensor([[-0.3424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 2, step: 7
	action: tensor([[ 0.3494,  0.3677,  0.6696, -0.5978,  0.7849,  0.2866, -1.7778]],
       dtype=torch.float64)
	q_value: tensor([[-0.3424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.546279361737295, distance: 0.7708164252914165 entropy 1.7646539211273193
epoch: 2, step: 8
	action: tensor([[-0.3758,  1.6043, -4.5429, -0.4710, -2.6121,  1.8486,  2.7717]],
       dtype=torch.float64)
	q_value: tensor([[-0.2469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 2, step: 9
	action: tensor([[ 1.3746,  1.3644, -0.3369,  0.2125, -1.7513, -2.6666,  0.1715]],
       dtype=torch.float64)
	q_value: tensor([[-0.3424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3978618950070847, distance: 0.8879831993411944 entropy 1.7646539211273193
epoch: 2, step: 10
	action: tensor([[-0.8776, -0.0487,  1.2430, -0.0032,  0.4823,  0.7279, -0.7660]],
       dtype=torch.float64)
	q_value: tensor([[-0.5279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5846627804385294, distance: 1.4405393368643387 entropy 1.7646539211273193
epoch: 2, step: 11
	action: tensor([[-0.7897, -0.9248, -1.2645, -0.8970, -1.9371, -1.5277, -0.0524]],
       dtype=torch.float64)
	q_value: tensor([[-0.2639]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21275445558760464, distance: 1.2602106128667405 entropy 1.7646539211273193
epoch: 2, step: 12
	action: tensor([[-0.4115, -1.0573, -1.5734,  1.6988,  2.5221, -0.3925,  2.9702]],
       dtype=torch.float64)
	q_value: tensor([[-0.4555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.842970419954485, distance: 1.5535165191917986 entropy 1.7646539211273193
epoch: 2, step: 13
	action: tensor([[ 0.8623,  1.1621,  0.6628, -1.8160,  0.5220, -2.1411,  1.1059]],
       dtype=torch.float64)
	q_value: tensor([[-0.6687]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4984555889436769, distance: 0.8104223151066868 entropy 1.7646539211273193
epoch: 2, step: 14
	action: tensor([[-2.4277,  0.5705,  1.5848, -0.6560, -0.6700,  0.5276,  1.3586]],
       dtype=torch.float64)
	q_value: tensor([[-0.4807]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 2, step: 15
	action: tensor([[-0.6277, -0.7400, -1.4206, -1.9258, -0.9313, -1.2095, -0.9138]],
       dtype=torch.float64)
	q_value: tensor([[-0.3424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7023671569295301, distance: 0.6243054406525453 entropy 1.7646539211273193
epoch: 2, step: 16
	action: tensor([[-1.3199,  2.9342, -2.3609, -1.7269,  0.7924,  0.6621, -1.2578]],
       dtype=torch.float64)
	q_value: tensor([[-0.3856]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 2, step: 17
	action: tensor([[ 1.3633,  2.8626,  0.9529,  0.3044, -1.0127, -1.0011, -2.1224]],
       dtype=torch.float64)
	q_value: tensor([[-0.3424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 2, step: 18
	action: tensor([[-1.0488,  0.1578, -0.4108, -0.0991, -1.2548, -1.5434,  0.4791]],
       dtype=torch.float64)
	q_value: tensor([[-0.3424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12758541516987076, distance: 1.2151542724054554 entropy 1.7646539211273193
epoch: 2, step: 19
	action: tensor([[-1.5414,  0.2992,  1.1511,  0.0485,  0.3912,  0.6458,  1.0111]],
       dtype=torch.float64)
	q_value: tensor([[-0.3518]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1207930201460465, distance: 1.666501987594922 entropy 1.7646539211273193
epoch: 2, step: 20
	action: tensor([[ 0.5813, -1.1694,  0.8243, -0.9931,  1.0499,  0.6507, -0.7407]],
       dtype=torch.float64)
	q_value: tensor([[-0.3870]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9461009786653312, distance: 1.5963913973767594 entropy 1.7646539211273193
epoch: 2, step: 21
	action: tensor([[-0.9894,  1.3441, -2.8643, -0.0504,  1.1349, -0.4424,  0.1091]],
       dtype=torch.float64)
	q_value: tensor([[-0.2697]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 2, step: 22
	action: tensor([[-0.5678,  0.1519, -2.8375, -0.5038, -0.6712, -1.7498, -0.6767]],
       dtype=torch.float64)
	q_value: tensor([[-0.3424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26052712350231944, distance: 0.9840514774510501 entropy 1.7646539211273193
epoch: 2, step: 23
	action: tensor([[ 0.7259,  0.9899, -0.8837, -0.7141,  2.3920,  1.6051, -1.0261]],
       dtype=torch.float64)
	q_value: tensor([[-0.4468]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8173380619422921, distance: 0.48908092384039004 entropy 1.7646539211273193
epoch: 2, step: 24
	action: tensor([[ 0.0515,  2.5456, -0.0241,  0.3068,  3.4571,  0.3296, -0.4833]],
       dtype=torch.float64)
	q_value: tensor([[-0.3601]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 2, step: 25
	action: tensor([[-0.4553, -0.8107, -0.7244, -0.4041, -0.9869,  0.3953,  0.2981]],
       dtype=torch.float64)
	q_value: tensor([[-0.3424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8671467300815705, distance: 1.5636729287879587 entropy 1.7646539211273193
epoch: 2, step: 26
	action: tensor([[ 1.4452, -0.0771, -2.9543,  2.4437, -2.1893, -1.5182, -0.3638]],
       dtype=torch.float64)
	q_value: tensor([[-0.2613]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44725182182936407, distance: 0.8507860360030374 entropy 1.7646539211273193
epoch: 2, step: 27
	action: tensor([[ 1.4467, -4.5126,  1.8560,  0.8263,  1.6651,  1.1831,  0.0961]],
       dtype=torch.float64)
	q_value: tensor([[-0.6908]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 2, step: 28
	action: tensor([[-1.1222, -0.0293, -1.5888, -0.4753,  0.6277, -0.5558,  1.6048]],
       dtype=torch.float64)
	q_value: tensor([[-0.3424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6069990780796708, distance: 1.4506562286722375 entropy 1.7646539211273193
epoch: 2, step: 29
	action: tensor([[-0.0067,  0.6473,  0.1023, -0.1388,  1.1722, -1.1637, -1.7087]],
       dtype=torch.float64)
	q_value: tensor([[-0.3744]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2768080083417751, distance: 0.9731583142287912 entropy 1.7646539211273193
epoch: 2, step: 30
	action: tensor([[ 1.5082,  0.7616, -0.5667,  2.0538,  0.5302, -2.3814, -1.5612]],
       dtype=torch.float64)
	q_value: tensor([[-0.2342]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.883751277842876, distance: 0.39016722708369517 entropy 1.7646539211273193
epoch: 2, step: 31
	action: tensor([[-1.7987, -0.8563, -0.6442,  1.3055, -0.0649, -0.8736,  1.7908]],
       dtype=torch.float64)
	q_value: tensor([[-0.5058]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.66416509407675, distance: 1.867829628828017 entropy 1.7646539211273193
epoch: 2, step: 32
	action: tensor([[ 0.2530,  0.8320,  0.8202,  0.1122, -1.1655, -1.3351,  2.8712]],
       dtype=torch.float64)
	q_value: tensor([[-0.4993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.85321432239638, distance: 0.4384282542920504 entropy 1.7646539211273193
epoch: 2, step: 33
	action: tensor([[ 0.0736, -0.5799, -1.0628,  0.9344, -3.9060, -1.4418, -0.4109]],
       dtype=torch.float64)
	q_value: tensor([[-0.5980]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2947736495197326 entropy 1.7646539211273193
epoch: 2, step: 34
	action: tensor([[-1.1680,  2.4712, -1.2936,  2.4282, -0.9351, -0.8608,  1.2348]],
       dtype=torch.float64)
	q_value: tensor([[-0.3424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 2, step: 35
	action: tensor([[ 0.5818, -0.4762,  1.9562, -1.4291, -0.2519, -0.9942, -1.1610]],
       dtype=torch.float64)
	q_value: tensor([[-0.3424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27580024572559547, distance: 0.9738361225867025 entropy 1.7646539211273193
epoch: 2, step: 36
	action: tensor([[ 1.2136,  0.1274,  0.0323,  1.6213, -0.3906, -0.9628,  0.0153]],
       dtype=torch.float64)
	q_value: tensor([[-0.4243]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8756879833992186, distance: 0.40347185879818925 entropy 1.7646539211273193
epoch: 2, step: 37
	action: tensor([[-0.1711,  0.8207, -0.5277, -1.5441,  0.3023, -0.6432, -1.1525]],
       dtype=torch.float64)
	q_value: tensor([[-0.3692]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42763256307603215, distance: 0.8657532932525602 entropy 1.7646539211273193
epoch: 2, step: 38
	action: tensor([[ 0.5792,  1.6647,  1.9733,  1.5737, -0.5497, -1.4206,  0.0771]],
       dtype=torch.float64)
	q_value: tensor([[-0.2495]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9446189625896175, distance: 0.26930054769391687 entropy 1.7646539211273193
epoch: 2, step: 39
	action: tensor([[-1.7615,  1.2040,  0.9719, -2.1108, -0.4043,  1.1010, -0.4120]],
       dtype=torch.float64)
	q_value: tensor([[-0.4930]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.47652122949119424, distance: 1.3905177630593317 entropy 1.7646539211273193
epoch: 2, step: 40
	action: tensor([[-0.3575,  0.3369,  0.3271, -0.1192,  0.7612,  1.4165,  0.1194]],
       dtype=torch.float64)
	q_value: tensor([[-0.5226]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5149933540393774, distance: 0.7969490148685089 entropy 1.7646539211273193
epoch: 2, step: 41
	action: tensor([[ 2.4554,  0.5983, -0.9975,  3.2853, -0.9776,  0.6657, -0.4245]],
       dtype=torch.float64)
	q_value: tensor([[-0.2586]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 2, step: 42
	action: tensor([[ 0.6036,  0.6101,  0.4889, -0.2217,  2.9328,  1.5317,  1.6482]],
       dtype=torch.float64)
	q_value: tensor([[-0.3424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7736323675028335, distance: 0.5444573407440402 entropy 1.7646539211273193
epoch: 2, step: 43
	action: tensor([[-0.8356, -0.8881, -2.5090, -2.8149, -1.9101, -1.4916,  0.3798]],
       dtype=torch.float64)
	q_value: tensor([[-0.4847]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 2, step: 44
	action: tensor([[ 1.9704,  0.8775,  1.2444,  2.2155, -3.6685,  0.0637, -1.3530]],
       dtype=torch.float64)
	q_value: tensor([[-0.3424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 2, step: 45
	action: tensor([[-3.0347,  1.3707,  0.2555, -2.5492, -0.8273, -2.7404, -2.2326]],
       dtype=torch.float64)
	q_value: tensor([[-0.3424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 2, step: 46
	action: tensor([[-1.8899,  0.4328, -0.7065, -0.1903, -0.1345, -1.2895, -0.9020]],
       dtype=torch.float64)
	q_value: tensor([[-0.3424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 2, step: 47
	action: tensor([[ 1.3171, -0.0047,  1.6643, -1.7927,  0.2717, -2.6790,  0.7098]],
       dtype=torch.float64)
	q_value: tensor([[-0.3424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5788168953509224, distance: 0.7426636437585421 entropy 1.7646539211273193
epoch: 2, step: 48
	action: tensor([[-0.6819, -0.1677, -0.4868, -0.9551, -0.6315, -0.7034,  2.0789]],
       dtype=torch.float64)
	q_value: tensor([[-0.5560]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16108044869905624, distance: 1.2330703339162545 entropy 1.7646539211273193
epoch: 2, step: 49
	action: tensor([[ 0.3321, -0.9120, -0.8265,  0.5528, -0.8199,  0.1216,  0.8159]],
       dtype=torch.float64)
	q_value: tensor([[-0.4399]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.19989557932770707, distance: 1.2535117808153058 entropy 1.7646539211273193
epoch: 2, step: 50
	action: tensor([[-1.1886,  0.7972,  0.5249,  0.8143,  2.0918, -0.7776,  1.5353]],
       dtype=torch.float64)
	q_value: tensor([[-0.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 2, step: 51
	action: tensor([[-0.2886, -2.2400,  0.0606, -0.4246, -1.7799,  0.9186,  2.0129]],
       dtype=torch.float64)
	q_value: tensor([[-0.3424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 2, step: 52
	action: tensor([[ 0.8682,  0.5090, -1.9659,  0.7634, -1.0227, -0.6763,  0.2743]],
       dtype=torch.float64)
	q_value: tensor([[-0.3424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9047682557303263, distance: 0.35314052859740125 entropy 1.7646539211273193
epoch: 2, step: 53
	action: tensor([[-1.7965, -1.4805, -1.5567, -0.7738,  2.0128, -3.1620,  1.0626]],
       dtype=torch.float64)
	q_value: tensor([[-0.3950]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 2, step: 54
	action: tensor([[-3.5922, -0.5881, -0.1988, -1.5544, -1.8749,  0.5223,  0.9720]],
       dtype=torch.float64)
	q_value: tensor([[-0.3424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 2, step: 55
	action: tensor([[ 0.4795, -0.4422,  0.5767,  2.4412, -3.3375,  0.6054, -0.8022]],
       dtype=torch.float64)
	q_value: tensor([[-0.3424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 2, step: 56
	action: tensor([[ 1.6247,  1.5157,  1.7540,  0.0372,  0.6999,  1.1709, -0.0493]],
       dtype=torch.float64)
	q_value: tensor([[-0.3424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 2, step: 57
	action: tensor([[ 2.2227,  0.1203,  0.8766, -3.4057, -0.2535, -0.2218, -0.9528]],
       dtype=torch.float64)
	q_value: tensor([[-0.3424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 2, step: 58
	action: tensor([[ 4.1601,  1.1588,  0.8427, -0.9045, -2.0635, -1.4040, -1.1916]],
       dtype=torch.float64)
	q_value: tensor([[-0.3424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 2, step: 59
	action: tensor([[ 2.2542,  0.6631,  0.5951,  0.3907,  0.4049, -0.5188, -0.8410]],
       dtype=torch.float64)
	q_value: tensor([[-0.3424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7067313173675467, distance: 0.6197114743099693 entropy 1.7646539211273193
epoch: 2, step: 60
	action: tensor([[-0.5781,  0.8364,  1.2740, -1.3140, -0.1118,  0.1065,  2.6556]],
       dtype=torch.float64)
	q_value: tensor([[-0.3087]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6557378229003101, distance: 1.472490420589959 entropy 1.7646539211273193
epoch: 2, step: 61
	action: tensor([[-0.9665, -1.2284,  1.6717,  0.1195,  0.1283, -1.6647, -0.0791]],
       dtype=torch.float64)
	q_value: tensor([[-0.5971]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.062603705190917, distance: 1.6434806306942926 entropy 1.7646539211273193
epoch: 2, step: 62
	action: tensor([[-1.3495, -0.8791, -0.1579,  1.1133,  1.5773,  0.6128, -1.0785]],
       dtype=torch.float64)
	q_value: tensor([[-0.4486]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0265649892316397, distance: 1.6290595533062724 entropy 1.7646539211273193
epoch: 2, step: 63
	action: tensor([[-1.1549,  1.2374, -1.4655,  0.3086, -0.8577, -3.0235,  1.5900]],
       dtype=torch.float64)
	q_value: tensor([[-0.3128]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 2, step: 64
	action: tensor([[-0.2473, -0.6375,  1.3470,  3.0859,  0.2006,  1.3481, -1.3573]],
       dtype=torch.float64)
	q_value: tensor([[-0.3424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 2, step: 65
	action: tensor([[ 1.6846, -1.0074,  0.4966, -0.4853,  1.0135, -1.4675,  2.8294]],
       dtype=torch.float64)
	q_value: tensor([[-0.3424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.006050155077230501, distance: 1.1408772720003395 entropy 1.7646539211273193
epoch: 2, step: 66
	action: tensor([[ 1.2387,  1.6665, -0.2513,  0.3511,  1.0619, -0.4769, -1.4091]],
       dtype=torch.float64)
	q_value: tensor([[-0.6624]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 2, step: 67
	action: tensor([[ 1.1297, -1.0766,  0.3664,  1.0958, -1.1652, -0.9437, -0.5173]],
       dtype=torch.float64)
	q_value: tensor([[-0.3424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09339884445407531, distance: 1.0895943119364964 entropy 1.7646539211273193
epoch: 2, step: 68
	action: tensor([[-0.8023, -1.7400, -0.3349, -1.2597,  0.1960, -1.9439,  1.2599]],
       dtype=torch.float64)
	q_value: tensor([[-0.4103]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 2, step: 69
	action: tensor([[ 0.9523, -0.5706, -0.7644,  1.5389,  0.7187, -0.8640,  1.4848]],
       dtype=torch.float64)
	q_value: tensor([[-0.3424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6226395511428151, distance: 0.7029669112154812 entropy 1.7646539211273193
epoch: 2, step: 70
	action: tensor([[ 0.9152, -1.9115, -0.3585, -0.8594,  2.9266, -0.5540,  2.3590]],
       dtype=torch.float64)
	q_value: tensor([[-0.4510]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 2, step: 71
	action: tensor([[ 0.5487,  2.7516,  1.9938, -0.7452, -0.5526, -1.9839,  1.2819]],
       dtype=torch.float64)
	q_value: tensor([[-0.3424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 2, step: 72
	action: tensor([[-1.2648,  0.5217,  0.6790,  0.3824, -0.6920, -0.9466, -0.0676]],
       dtype=torch.float64)
	q_value: tensor([[-0.3424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6750610569892832, distance: 1.4810578223398674 entropy 1.7646539211273193
epoch: 2, step: 73
	action: tensor([[-1.4751,  1.3580,  1.4098, -0.4491,  0.8841, -0.9520,  0.7357]],
       dtype=torch.float64)
	q_value: tensor([[-0.2519]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17367917922778275, distance: 1.2397422254261103 entropy 1.7646539211273193
epoch: 2, step: 74
	action: tensor([[ 0.0728, -0.6942, -0.9073,  0.3632, -1.0561,  0.6533,  0.3849]],
       dtype=torch.float64)
	q_value: tensor([[-0.3320]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.41771457918398647, distance: 1.3625457580846145 entropy 1.7646539211273193
epoch: 2, step: 75
	action: tensor([[-1.0205,  1.9214,  1.2227,  1.2218,  0.0228, -1.1681, -0.8860]],
       dtype=torch.float64)
	q_value: tensor([[-0.2757]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4430710329542039, distance: 0.8539974950045739 entropy 1.7646539211273193
epoch: 2, step: 76
	action: tensor([[ 0.1451, -0.9721, -1.7820,  2.8740,  0.9479,  1.0255,  0.1694]],
       dtype=torch.float64)
	q_value: tensor([[-0.3472]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 2, step: 77
	action: tensor([[-0.2652, -2.4155, -1.8343, -2.6277, -2.3012,  0.5571,  1.6351]],
       dtype=torch.float64)
	q_value: tensor([[-0.3424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 2, step: 78
	action: tensor([[ 1.2235,  3.9010,  1.4412, -0.6431, -1.8797, -2.6092, -1.4838]],
       dtype=torch.float64)
	q_value: tensor([[-0.3424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 2, step: 79
	action: tensor([[-0.9054, -1.4306, -2.6957, -0.4091,  0.4591, -0.9441, -0.9924]],
       dtype=torch.float64)
	q_value: tensor([[-0.3424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6467689055957622, distance: 1.4684968601644373 entropy 1.7646539211273193
epoch: 2, step: 80
	action: tensor([[ 1.6944, -0.3433,  2.1226, -0.1573,  0.0729, -1.1336,  0.5856]],
       dtype=torch.float64)
	q_value: tensor([[-0.3576]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6449929055780763, distance: 0.681828592282881 entropy 1.7646539211273193
epoch: 2, step: 81
	action: tensor([[-0.9784, -0.7246, -0.2889,  1.1400, -2.4302,  2.4132,  0.0653]],
       dtype=torch.float64)
	q_value: tensor([[-0.5117]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7907694385208666, distance: 1.5313572898936254 entropy 1.7646539211273193
epoch: 2, step: 82
	action: tensor([[ 0.3013,  0.0675, -0.0081,  1.0263, -0.7563,  0.1776, -1.0847]],
       dtype=torch.float64)
	q_value: tensor([[-0.6546]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8008861486671126, distance: 0.510631297109521 entropy 1.7646539211273193
epoch: 2, step: 83
	action: tensor([[ 1.1094,  0.9210,  2.9464, -2.0732,  0.9058, -0.2983,  1.4415]],
       dtype=torch.float64)
	q_value: tensor([[-0.2718]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05279081344677483, distance: 1.1137292967902244 entropy 1.7646539211273193
epoch: 2, step: 84
	action: tensor([[ 1.4309,  1.9399, -0.7362,  0.7333,  2.7877,  0.1550,  1.6568]],
       dtype=torch.float64)
	q_value: tensor([[-0.6408]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 2, step: 85
	action: tensor([[ 0.3498,  0.1602, -0.3075, -0.1396, -0.2215,  1.9538,  0.8639]],
       dtype=torch.float64)
	q_value: tensor([[-0.3424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7579945240373763, distance: 0.5629493082379887 entropy 1.7646539211273193
epoch: 2, step: 86
	action: tensor([[-0.1814,  1.6285, -0.2220, -2.0836, -1.5787, -2.1080,  0.1893]],
       dtype=torch.float64)
	q_value: tensor([[-0.4417]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7874966866840036, distance: 0.5275207487335534 entropy 1.7646539211273193
epoch: 2, step: 87
	action: tensor([[ 1.2423,  0.7848,  1.2763,  0.8748, -0.7230, -1.5192,  0.5359]],
       dtype=torch.float64)
	q_value: tensor([[-0.5539]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8265459373816197, distance: 0.4765943959912449 entropy 1.7646539211273193
epoch: 2, step: 88
	action: tensor([[ 0.6045, -1.3441, -1.2940,  0.9246,  1.5282, -0.5974, -0.1796]],
       dtype=torch.float64)
	q_value: tensor([[-0.4372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6027718186214728, distance: 1.4487469748151547 entropy 1.7646539211273193
epoch: 2, step: 89
	action: tensor([[ 1.1074,  0.0841,  4.1371, -0.3652, -0.8315,  1.4482, -0.1230]],
       dtype=torch.float64)
	q_value: tensor([[-0.3621]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.6970175844528776 entropy 1.7646539211273193
epoch: 2, step: 90
	action: tensor([[ 1.0172, -0.8425,  1.8493, -0.8191, -1.3960,  2.1659,  1.4685]],
       dtype=torch.float64)
	q_value: tensor([[-0.3424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6128942011615899, distance: 0.7119861274824266 entropy 1.7646539211273193
epoch: 2, step: 91
	action: tensor([[ 0.9459,  0.5734,  2.4709, -1.4102,  1.6321, -1.4556, -0.8065]],
       dtype=torch.float64)
	q_value: tensor([[-0.8494]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6935918768409097, distance: 0.6334419634879084 entropy 1.7646539211273193
epoch: 2, step: 92
	action: tensor([[-0.7871,  2.5566,  1.4524,  1.1775,  0.3460, -0.8906,  1.6848]],
       dtype=torch.float64)
	q_value: tensor([[-0.4597]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 2, step: 93
	action: tensor([[-1.6690,  2.1984, -3.3137, -1.8201,  1.8536,  0.1558,  0.7872]],
       dtype=torch.float64)
	q_value: tensor([[-0.3424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 2, step: 94
	action: tensor([[-0.7742, -1.8846,  0.1930, -1.8586, -0.5618,  0.5466, -2.9692]],
       dtype=torch.float64)
	q_value: tensor([[-0.3424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 2, step: 95
	action: tensor([[-3.0835,  0.8839,  0.4786,  0.4957, -0.1613,  2.2792, -1.0090]],
       dtype=torch.float64)
	q_value: tensor([[-0.3424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 2, step: 96
	action: tensor([[-1.3779, -0.0236, -0.6881, -0.3809,  0.1424, -3.0755,  0.8903]],
       dtype=torch.float64)
	q_value: tensor([[-0.3424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 2, step: 97
	action: tensor([[ 0.7408, -0.4270, -1.2010, -0.1186,  1.8065, -0.3647,  0.6536]],
       dtype=torch.float64)
	q_value: tensor([[-0.3424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2435914343030411, distance: 0.9952562473401223 entropy 1.7646539211273193
epoch: 2, step: 98
	action: tensor([[-0.0044,  2.6741,  0.0687,  0.9492, -0.3192,  1.6671, -0.5308]],
       dtype=torch.float64)
	q_value: tensor([[-0.3418]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 2, step: 99
	action: tensor([[-1.6035, -0.3242, -3.9400, -1.1174, -2.2271, -1.5737,  1.6601]],
       dtype=torch.float64)
	q_value: tensor([[-0.3424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.784243440609673 entropy 1.7646539211273193
epoch: 2, step: 100
	action: tensor([[-0.8280, -0.8247,  1.0017, -0.2522, -0.3762, -0.5435, -0.1297]],
       dtype=torch.float64)
	q_value: tensor([[-0.3424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.3896550770534941, distance: 1.7689856216137796 entropy 1.7646539211273193
epoch: 2, step: 101
	action: tensor([[ 1.0148, -1.6712, -1.7769,  0.1435,  2.4245, -1.4429,  0.5604]],
       dtype=torch.float64)
	q_value: tensor([[-0.2997]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 2, step: 102
	action: tensor([[-2.1192,  1.4001,  1.4295,  1.2909,  0.1124,  2.2062,  1.0617]],
       dtype=torch.float64)
	q_value: tensor([[-0.3424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 2, step: 103
	action: tensor([[-1.8119,  1.1630, -3.5419, -4.6422, -0.2983, -0.2334, -0.5730]],
       dtype=torch.float64)
	q_value: tensor([[-0.3424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 2, step: 104
	action: tensor([[-0.4622, -2.4309,  2.1087, -0.5040,  0.1184,  0.2887,  1.0661]],
       dtype=torch.float64)
	q_value: tensor([[-0.3424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 2, step: 105
	action: tensor([[-0.6703, -0.8713, -1.7614, -1.8564,  0.4515, -1.8526,  0.8204]],
       dtype=torch.float64)
	q_value: tensor([[-0.3424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.43157355837994316, distance: 0.8627676035544974 entropy 1.7646539211273193
epoch: 2, step: 106
	action: tensor([[-0.3669,  0.6818,  0.3114, -1.1809,  0.8560,  0.6246, -1.0963]],
       dtype=torch.float64)
	q_value: tensor([[-0.4551]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.00778917894748643, distance: 1.1398787903410377 entropy 1.7646539211273193
epoch: 2, step: 107
	action: tensor([[ 1.3153, -2.6595,  4.8078,  0.4025,  0.3435,  2.8085,  2.8266]],
       dtype=torch.float64)
	q_value: tensor([[-0.2387]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 2, step: 108
	action: tensor([[-3.5179, -2.4031, -0.4620,  0.3343, -0.3203, -0.6011, -0.2179]],
       dtype=torch.float64)
	q_value: tensor([[-0.3424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 2, step: 109
	action: tensor([[-0.5595,  1.9762, -0.9619, -1.2557, -1.1024,  0.8714, -0.6645]],
       dtype=torch.float64)
	q_value: tensor([[-0.3424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 2, step: 110
	action: tensor([[ 1.5319e+00, -8.8319e-04, -3.3652e-01, -2.1819e+00,  4.4830e-01,
         -1.2567e+00, -2.2039e+00]], dtype=torch.float64)
	q_value: tensor([[-0.3424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12014575219946333, distance: 1.0734011613061116 entropy 1.7646539211273193
epoch: 2, step: 111
	action: tensor([[ 2.0562,  1.4944,  0.0174, -0.1986,  0.8063,  0.9170, -0.4499]],
       dtype=torch.float64)
	q_value: tensor([[-0.4357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 2, step: 112
	action: tensor([[ 0.6233, -0.4044, -1.5766, -0.9564,  2.1797, -2.9628,  2.5920]],
       dtype=torch.float64)
	q_value: tensor([[-0.3424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 2, step: 113
	action: tensor([[-0.8695,  2.2747,  0.5566, -0.0709, -0.8814,  0.0270,  0.4678]],
       dtype=torch.float64)
	q_value: tensor([[-0.3424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 2, step: 114
	action: tensor([[-0.1181,  0.2519, -1.0282,  0.1288,  0.8946,  1.0207,  2.7102]],
       dtype=torch.float64)
	q_value: tensor([[-0.3424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3985470213148181, distance: 0.8874778719076071 entropy 1.7646539211273193
epoch: 2, step: 115
	action: tensor([[-2.6445, -0.3454, -1.7903,  0.2958,  0.7764, -2.9189,  2.6169]],
       dtype=torch.float64)
	q_value: tensor([[-0.5070]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 2, step: 116
	action: tensor([[-2.0720,  2.3301, -1.6593, -2.0850, -0.5248,  0.1162,  0.8168]],
       dtype=torch.float64)
	q_value: tensor([[-0.3424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 2, step: 117
	action: tensor([[-2.4867e+00,  8.4041e-01,  2.0545e+00,  7.6969e-01,  2.9119e-04,
         -1.0807e+00, -1.8561e+00]], dtype=torch.float64)
	q_value: tensor([[-0.3424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 2, step: 118
	action: tensor([[-2.3199,  1.3816,  0.1953, -2.1134,  1.3131, -2.1673,  0.6619]],
       dtype=torch.float64)
	q_value: tensor([[-0.3424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 2, step: 119
	action: tensor([[ 0.2993, -0.9135, -0.0857, -0.9952, -1.1198,  0.4139,  0.4982]],
       dtype=torch.float64)
	q_value: tensor([[-0.3424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.806326555905363, distance: 1.5379946567995273 entropy 1.7646539211273193
epoch: 2, step: 120
	action: tensor([[-2.4385, -0.0518,  0.4265,  1.3026, -1.6995, -0.0223,  1.3017]],
       dtype=torch.float64)
	q_value: tensor([[-0.3628]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 2, step: 121
	action: tensor([[-0.0035,  0.3083,  0.4104, -0.6698,  0.1183,  1.0668,  0.5502]],
       dtype=torch.float64)
	q_value: tensor([[-0.3424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4540665653035847, distance: 0.8455251684488674 entropy 1.7646539211273193
epoch: 2, step: 122
	action: tensor([[-1.0558, -0.3622,  0.6426, -1.0756, -0.9138, -0.9307,  3.1128]],
       dtype=torch.float64)
	q_value: tensor([[-0.2777]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9945471440070661, distance: 1.6161395047394367 entropy 1.7646539211273193
epoch: 2, step: 123
	action: tensor([[ 1.3957, -1.7510,  0.3855, -1.5125, -1.9053,  0.4630, -1.0836]],
       dtype=torch.float64)
	q_value: tensor([[-0.7102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 2, step: 124
	action: tensor([[-2.0966,  2.1912,  0.0935,  0.7487,  0.6680, -1.8256,  0.5613]],
       dtype=torch.float64)
	q_value: tensor([[-0.3424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 2, step: 125
	action: tensor([[ 1.4746, -1.8259, -0.6333,  0.6044,  1.2767, -0.5721,  1.1477]],
       dtype=torch.float64)
	q_value: tensor([[-0.3424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 2, step: 126
	action: tensor([[-1.7510,  0.7722,  0.0570, -2.1414, -3.6498,  0.1082, -0.8372]],
       dtype=torch.float64)
	q_value: tensor([[-0.3424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 2, step: 127
	action: tensor([[-0.5990,  0.2728, -0.1143, -0.6430,  0.9244,  0.1665,  0.2720]],
       dtype=torch.float64)
	q_value: tensor([[-0.3424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3982307552402413, distance: 1.3531505504773476 entropy 1.7646539211273193
LOSS epoch 2 actor 429.73185819687995 critic 2168.0210898211485 
epoch: 3, step: 0
	action: tensor([[-0.7849, -0.1565, -2.4108, -3.0603,  1.3028,  1.4273,  1.3161]],
       dtype=torch.float64)
	q_value: tensor([[-0.4172]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 1
	action: tensor([[ 0.5200, -2.3102, -1.0354, -1.2343,  0.7272, -2.5095,  0.2732]],
       dtype=torch.float64)
	q_value: tensor([[-0.7973]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 2
	action: tensor([[ 2.1163,  0.9854,  0.4972, -2.3626, -0.2747, -0.7111,  0.3890]],
       dtype=torch.float64)
	q_value: tensor([[-0.7973]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28302011536552574, distance: 0.9689696603779231 entropy 1.7646539211273193
epoch: 3, step: 3
	action: tensor([[-1.0514, -0.1444, -0.1205, -1.2919, -1.6074, -2.1388,  0.7723]],
       dtype=torch.float64)
	q_value: tensor([[-1.1149]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.011782362717671524, distance: 1.137582738792072 entropy 1.7646539211273193
epoch: 3, step: 4
	action: tensor([[ 0.1870, -0.1714, -1.3258, -2.4950,  1.4511,  0.2645,  0.1060]],
       dtype=torch.float64)
	q_value: tensor([[-1.1769]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 5
	action: tensor([[ 2.1342,  1.2198,  0.4564, -1.8374, -1.0804, -3.0559,  0.2479]],
       dtype=torch.float64)
	q_value: tensor([[-0.7973]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 6
	action: tensor([[ 1.5007,  0.0164,  0.2923, -3.2298, -0.6950,  2.1162, -0.2643]],
       dtype=torch.float64)
	q_value: tensor([[-0.7973]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 7
	action: tensor([[-1.1906,  1.5735, -1.7536, -0.8822, -0.9373,  0.2722,  1.3340]],
       dtype=torch.float64)
	q_value: tensor([[-0.7973]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 8
	action: tensor([[ 0.4947, -0.4637, -0.8413,  0.7060, -0.0874, -2.0453, -1.7707]],
       dtype=torch.float64)
	q_value: tensor([[-0.7973]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41855696472508797, distance: 0.8725900965288792 entropy 1.7646539211273193
epoch: 3, step: 9
	action: tensor([[ 1.1757, -0.3657, -0.1814,  1.0198,  2.2959, -1.1587,  1.3284]],
       dtype=torch.float64)
	q_value: tensor([[-0.8326]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9316477881093651, distance: 0.2991802906551837 entropy 1.7646539211273193
epoch: 3, step: 10
	action: tensor([[-0.0162,  1.3269, -2.7792, -0.7660, -1.2946, -0.5729, -0.6126]],
       dtype=torch.float64)
	q_value: tensor([[-1.0943]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 11
	action: tensor([[ 1.7089, -0.9186, -1.0261, -1.1058, -0.1229, -0.2232,  0.4045]],
       dtype=torch.float64)
	q_value: tensor([[-0.7973]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.925599513641586, distance: 1.5879604333282409 entropy 1.7646539211273193
epoch: 3, step: 12
	action: tensor([[-0.9881,  1.0417,  0.8904,  1.0595,  0.0032,  1.4908,  0.9043]],
       dtype=torch.float64)
	q_value: tensor([[-0.8229]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8136582079793381, distance: 0.4939827996047082 entropy 1.7646539211273193
epoch: 3, step: 13
	action: tensor([[ 0.8167,  0.8292,  0.6453, -0.9905, -0.1245, -0.7591,  2.3864]],
       dtype=torch.float64)
	q_value: tensor([[-0.9000]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7706810025148865, distance: 0.547995144595864 entropy 1.7646539211273193
epoch: 3, step: 14
	action: tensor([[-2.9401,  0.5406, -3.4271,  0.6587, -1.6466,  0.1088, -1.6840]],
       dtype=torch.float64)
	q_value: tensor([[-1.0696]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 15
	action: tensor([[ 0.7672,  1.3486, -0.3042, -1.6896, -1.4325, -1.5425,  0.4995]],
       dtype=torch.float64)
	q_value: tensor([[-0.7973]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7254789377543135, distance: 0.5995763978283787 entropy 1.7646539211273193
epoch: 3, step: 16
	action: tensor([[-0.4126,  0.7180,  1.6299,  2.9471, -0.5023,  1.0521,  0.7649]],
       dtype=torch.float64)
	q_value: tensor([[-1.1511]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5216982718996748, distance: 0.7914211792558882 entropy 1.7646539211273193
epoch: 3, step: 17
	action: tensor([[ 2.1110, -0.0606,  0.5171,  0.4271, -2.3347,  0.0536, -0.5133]],
       dtype=torch.float64)
	q_value: tensor([[-1.1672]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 18
	action: tensor([[ 0.4544,  0.7484,  1.5968, -0.3226,  1.5473, -2.1156,  0.2237]],
       dtype=torch.float64)
	q_value: tensor([[-0.7973]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.749533278332505, distance: 0.5727059682193288 entropy 1.7646539211273193
epoch: 3, step: 19
	action: tensor([[-0.7271, -0.4096,  0.4999,  1.0107, -0.1405, -0.8167,  0.1598]],
       dtype=torch.float64)
	q_value: tensor([[-0.8903]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3518547237375631, distance: 1.330520911736831 entropy 1.7646539211273193
epoch: 3, step: 20
	action: tensor([[-0.1494,  0.7652, -1.5601, -0.8143, -2.8943,  1.8716,  1.7616]],
       dtype=torch.float64)
	q_value: tensor([[-0.6405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8365305143343532, distance: 0.462673942880576 entropy 1.7646539211273193
epoch: 3, step: 21
	action: tensor([[-0.8066,  0.1637, -1.5486, -1.0501,  1.2844, -3.1025,  0.7924]],
       dtype=torch.float64)
	q_value: tensor([[-1.7925]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.014491375485810698, distance: 1.136022434188716 entropy 1.7646539211273193
epoch: 3, step: 22
	action: tensor([[-1.2232,  1.1485,  2.0843,  0.6049,  0.1861, -0.9845, -1.4535]],
       dtype=torch.float64)
	q_value: tensor([[-0.9265]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 23
	action: tensor([[ 0.3809, -0.4843,  2.6634, -0.1063,  3.3189,  1.5553,  0.7270]],
       dtype=torch.float64)
	q_value: tensor([[-0.7973]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7132565897987705 entropy 1.7646539211273193
epoch: 3, step: 24
	action: tensor([[ 0.7883, -0.2516,  0.0642,  0.2501,  1.2273,  1.6347, -1.5983]],
       dtype=torch.float64)
	q_value: tensor([[-0.7973]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7695118428932027, distance: 0.5493903173683579 entropy 1.7646539211273193
epoch: 3, step: 25
	action: tensor([[ 0.2123, -0.3118, -0.8408, -1.7533, -0.2785, -1.5384, -0.2064]],
       dtype=torch.float64)
	q_value: tensor([[-0.7267]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44891239206652, distance: 0.8495071058311933 entropy 1.7646539211273193
epoch: 3, step: 26
	action: tensor([[ 1.0269,  1.5537, -1.1397, -1.1207,  1.4454,  0.1920, -1.7309]],
       dtype=torch.float64)
	q_value: tensor([[-0.7466]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.501640753833479, distance: 0.8078448363906404 entropy 1.7646539211273193
epoch: 3, step: 27
	action: tensor([[-1.2097, -0.3856, -0.3873, -2.6978, -1.1406, -1.9336,  2.9902]],
       dtype=torch.float64)
	q_value: tensor([[-0.8276]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 28
	action: tensor([[ 0.4039,  0.2983,  1.3015, -1.5267,  1.9282,  0.0945, -2.1060]],
       dtype=torch.float64)
	q_value: tensor([[-0.7973]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0033384357910168916, distance: 1.1462528223264372 entropy 1.7646539211273193
epoch: 3, step: 29
	action: tensor([[ 2.6289, -1.6621,  1.0576, -0.1732, -0.9246, -2.0347,  0.8713]],
       dtype=torch.float64)
	q_value: tensor([[-0.8190]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 30
	action: tensor([[ 0.8320,  0.8134, -2.1434, -4.0621, -0.0481, -0.3719,  0.8078]],
       dtype=torch.float64)
	q_value: tensor([[-0.7973]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 31
	action: tensor([[-0.4206, -1.3699,  1.5126, -1.5429,  1.6194, -1.5660, -0.2838]],
       dtype=torch.float64)
	q_value: tensor([[-0.7973]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.45956437176011966, distance: 1.3825101233235078 entropy 1.7646539211273193
epoch: 3, step: 32
	action: tensor([[ 1.4645, -0.1127,  1.6156, -0.6404, -1.9444, -2.3829, -3.3313]],
       dtype=torch.float64)
	q_value: tensor([[-1.0110]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 33
	action: tensor([[-2.1283,  0.3193,  0.4403, -0.3100, -0.2127, -0.9869,  2.5974]],
       dtype=torch.float64)
	q_value: tensor([[-0.7973]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 34
	action: tensor([[ 2.4082,  1.7513,  0.0464, -0.4645, -1.9108, -1.7329, -0.6385]],
       dtype=torch.float64)
	q_value: tensor([[-0.7973]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 35
	action: tensor([[ 1.9179, -0.5021, -1.7091, -1.8227, -1.8445, -0.7249, -1.3547]],
       dtype=torch.float64)
	q_value: tensor([[-0.7973]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03572724183397802, distance: 1.164606991280903 entropy 1.7646539211273193
epoch: 3, step: 36
	action: tensor([[ 0.0267, -0.3982, -1.0891,  1.5584,  1.0513,  0.1219, -0.2632]],
       dtype=torch.float64)
	q_value: tensor([[-1.2268]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05423212205701944, distance: 1.174964690824453 entropy 1.7646539211273193
epoch: 3, step: 37
	action: tensor([[ 1.3960,  1.5361,  1.4809, -0.3947, -0.8223, -3.3896,  1.4258]],
       dtype=torch.float64)
	q_value: tensor([[-0.6964]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 38
	action: tensor([[-0.3299, -0.9568,  0.5581,  1.3004, -1.1779,  2.1562,  1.1127]],
       dtype=torch.float64)
	q_value: tensor([[-0.7973]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13741507711024037, distance: 1.0628148764493643 entropy 1.7646539211273193
epoch: 3, step: 39
	action: tensor([[-1.0635,  2.7523,  2.0627, -0.1146,  0.4315,  0.9800,  0.2709]],
       dtype=torch.float64)
	q_value: tensor([[-1.3164]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 40
	action: tensor([[-2.3928, -1.2969, -0.2535, -1.4876, -1.4589,  0.7376,  1.3327]],
       dtype=torch.float64)
	q_value: tensor([[-0.7973]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 41
	action: tensor([[-2.1362,  0.2259, -1.9518, -1.9944,  1.6846,  1.0724,  2.5049]],
       dtype=torch.float64)
	q_value: tensor([[-0.7973]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 42
	action: tensor([[-0.8624, -0.5801, -0.6553, -2.7610,  0.1570,  1.2869,  0.9121]],
       dtype=torch.float64)
	q_value: tensor([[-0.7973]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 43
	action: tensor([[ 0.5962,  1.4653,  0.5300, -0.3105,  2.7502, -1.4376, -2.2561]],
       dtype=torch.float64)
	q_value: tensor([[-0.7973]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 44
	action: tensor([[-2.9240,  0.5290,  2.0366, -3.2958, -0.6563, -0.6992, -0.1893]],
       dtype=torch.float64)
	q_value: tensor([[-0.7973]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 45
	action: tensor([[-0.1960, -0.4417, -0.5245, -1.1698,  2.8574,  0.6752,  1.2635]],
       dtype=torch.float64)
	q_value: tensor([[-0.7973]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.015129038068081968, distance: 1.1529681724381473 entropy 1.7646539211273193
epoch: 3, step: 46
	action: tensor([[ 1.7774, -1.1256,  0.3405, -0.9744,  0.8069,  0.0064, -0.2044]],
       dtype=torch.float64)
	q_value: tensor([[-1.0186]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7812906797033012, distance: 1.5272990831244837 entropy 1.7646539211273193
epoch: 3, step: 47
	action: tensor([[ 0.6825, -0.5313, -0.9667, -0.4230, -0.6700, -0.1291,  2.9832]],
       dtype=torch.float64)
	q_value: tensor([[-0.7421]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13357082501883022, distance: 1.0651805485926054 entropy 1.7646539211273193
epoch: 3, step: 48
	action: tensor([[ 1.0451, -1.1163, -0.7875,  1.2522,  0.4503, -0.6212, -0.1375]],
       dtype=torch.float64)
	q_value: tensor([[-1.2694]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2108936545331781, distance: 1.0165399735174088 entropy 1.7646539211273193
epoch: 3, step: 49
	action: tensor([[-0.2205,  0.2289, -1.9785, -0.3098, -2.0773,  0.4369,  2.1571]],
       dtype=torch.float64)
	q_value: tensor([[-0.7708]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.48207154118480233, distance: 0.8235530516239906 entropy 1.7646539211273193
epoch: 3, step: 50
	action: tensor([[-2.6386,  0.1184,  0.6445, -0.3539,  0.2274,  1.3212,  1.1036]],
       dtype=torch.float64)
	q_value: tensor([[-1.4038]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 51
	action: tensor([[ 2.6911,  0.9963,  1.1271, -3.3466, -1.5789,  0.2940,  0.9123]],
       dtype=torch.float64)
	q_value: tensor([[-0.7973]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 52
	action: tensor([[ 0.9724,  0.9881, -2.1821,  1.6659,  0.6064, -0.5709,  1.1286]],
       dtype=torch.float64)
	q_value: tensor([[-0.7973]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8142689716773712, distance: 0.49317258310236806 entropy 1.7646539211273193
epoch: 3, step: 53
	action: tensor([[-0.1733,  0.7367,  0.1377, -1.0775,  0.3863, -1.9166, -1.0308]],
       dtype=torch.float64)
	q_value: tensor([[-1.0023]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17344465926753816, distance: 1.040381596955664 entropy 1.7646539211273193
epoch: 3, step: 54
	action: tensor([[ 0.2093, -1.8111, -0.6392,  0.1884, -0.7655, -0.7475,  4.0653]],
       dtype=torch.float64)
	q_value: tensor([[-0.6635]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 55
	action: tensor([[ 0.0502, -0.1505,  1.8784, -1.3236,  1.5593, -1.0627, -0.0303]],
       dtype=torch.float64)
	q_value: tensor([[-0.7973]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27190565723303384, distance: 0.9764511507835695 entropy 1.7646539211273193
epoch: 3, step: 56
	action: tensor([[ 1.5995,  2.0363, -0.5101, -0.4737, -0.0223, -0.3485,  1.6655]],
       dtype=torch.float64)
	q_value: tensor([[-0.8303]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 57
	action: tensor([[ 1.2294,  2.2779, -0.7233,  0.5142,  2.1382, -0.4557,  1.8370]],
       dtype=torch.float64)
	q_value: tensor([[-0.7973]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 58
	action: tensor([[ 0.2736,  2.0158, -1.7764,  2.9719,  1.5879, -1.5381, -1.0815]],
       dtype=torch.float64)
	q_value: tensor([[-0.7973]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 59
	action: tensor([[ 3.5529e-01, -9.6520e-02,  1.5528e+00, -2.2903e+00,  3.1594e+00,
          8.8912e-04,  1.2887e+00]], dtype=torch.float64)
	q_value: tensor([[-0.7973]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 60
	action: tensor([[-2.8533, -1.6073, -0.9966,  0.5539, -0.8948, -2.9854,  0.7097]],
       dtype=torch.float64)
	q_value: tensor([[-0.7973]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 61
	action: tensor([[ 0.2562, -1.2879, -1.1174, -1.2683, -0.1381, -2.6278, -0.6649]],
       dtype=torch.float64)
	q_value: tensor([[-0.7973]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3841027672220503, distance: 0.8980713040867481 entropy 1.7646539211273193
epoch: 3, step: 62
	action: tensor([[ 0.7242,  1.1536,  2.1710,  2.2637,  2.3350, -0.8509,  1.3602]],
       dtype=torch.float64)
	q_value: tensor([[-0.8586]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3057660626900712, distance: 0.9534756651338435 entropy 1.7646539211273193
epoch: 3, step: 63
	action: tensor([[-0.2172, -1.1086, -0.8660,  1.5565, -2.3307, -0.3101,  0.5048]],
       dtype=torch.float64)
	q_value: tensor([[-1.1851]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.30086453479732533, distance: 1.3051869720580989 entropy 1.7646539211273193
epoch: 3, step: 64
	action: tensor([[-1.9934,  1.4066, -3.5420,  0.5978,  1.0092,  0.1525, -0.8265]],
       dtype=torch.float64)
	q_value: tensor([[-1.1031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 65
	action: tensor([[-3.2159, -1.7093, -1.1315, -1.4446, -1.0932, -1.0070, -0.0675]],
       dtype=torch.float64)
	q_value: tensor([[-0.7973]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 66
	action: tensor([[-0.1546, -0.1966, -0.0047, -1.9054,  0.5229,  1.2018,  2.1621]],
       dtype=torch.float64)
	q_value: tensor([[-0.7973]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.060824536292226394, distance: 1.1786326602626684 entropy 1.7646539211273193
epoch: 3, step: 67
	action: tensor([[-0.6336,  0.4658,  1.4722, -0.9138, -0.1902, -0.3250, -0.7681]],
       dtype=torch.float64)
	q_value: tensor([[-1.2457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8255872458445668, distance: 1.546172660370733 entropy 1.7646539211273193
epoch: 3, step: 68
	action: tensor([[-2.0638, -2.3111,  0.8190, -0.4740, -1.3878,  2.7853,  0.8840]],
       dtype=torch.float64)
	q_value: tensor([[-0.6677]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 69
	action: tensor([[-1.3224,  0.6882,  2.8941,  1.0977,  3.2697, -0.8458, -1.0159]],
       dtype=torch.float64)
	q_value: tensor([[-0.7973]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0774638047801457 entropy 1.7646539211273193
epoch: 3, step: 70
	action: tensor([[ 2.2871,  0.1203, -1.1676, -1.1068, -1.7460, -0.0062,  1.7678]],
       dtype=torch.float64)
	q_value: tensor([[-0.7973]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2717496965999744, distance: 1.290498531305883 entropy 1.7646539211273193
epoch: 3, step: 71
	action: tensor([[ 0.3595, -0.6434, -1.5329, -1.6306,  1.2466,  1.5239, -0.0279]],
       dtype=torch.float64)
	q_value: tensor([[-1.3782]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7760542410739265, distance: 0.541536974616474 entropy 1.7646539211273193
epoch: 3, step: 72
	action: tensor([[ 0.8889,  0.0331, -0.9173, -2.8551, -0.4775, -2.2484,  1.5350]],
       dtype=torch.float64)
	q_value: tensor([[-0.8894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 73
	action: tensor([[-0.5774,  0.5391,  0.3540, -2.9029,  0.3897,  0.0423,  1.6705]],
       dtype=torch.float64)
	q_value: tensor([[-0.7973]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 74
	action: tensor([[-0.8215,  1.5727, -1.1535, -2.8179, -1.7485, -1.5341,  1.9962]],
       dtype=torch.float64)
	q_value: tensor([[-0.7973]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 75
	action: tensor([[ 0.1154,  0.0413,  1.6568, -2.3179,  0.1588, -0.0419,  0.1208]],
       dtype=torch.float64)
	q_value: tensor([[-0.7973]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.19260814193268905, distance: 1.2496994487233635 entropy 1.7646539211273193
epoch: 3, step: 76
	action: tensor([[ 1.3703,  0.9693,  1.6089,  1.0647,  0.8835,  1.3790, -0.7384]],
       dtype=torch.float64)
	q_value: tensor([[-0.9726]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35082628485484557, distance: 0.9220132384739633 entropy 1.7646539211273193
epoch: 3, step: 77
	action: tensor([[-0.4904,  0.2883, -0.1868,  2.9529, -2.3311, -1.1612, -0.1990]],
       dtype=torch.float64)
	q_value: tensor([[-0.9658]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 78
	action: tensor([[-0.8360,  0.2104, -1.6637, -1.4941,  2.4814, -1.5382, -2.7290]],
       dtype=torch.float64)
	q_value: tensor([[-0.7973]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.041009607285724115, distance: 1.1206340687555434 entropy 1.7646539211273193
epoch: 3, step: 79
	action: tensor([[-2.0984,  0.7966,  0.7345, -2.2435, -0.7006, -1.8831, -0.1751]],
       dtype=torch.float64)
	q_value: tensor([[-0.9300]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 80
	action: tensor([[-1.0106,  3.2793, -1.0752, -1.2988,  1.3543, -0.4931,  0.7731]],
       dtype=torch.float64)
	q_value: tensor([[-0.7973]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 81
	action: tensor([[-0.4824, -0.7344,  0.9795,  0.5583, -3.6686, -0.2637,  2.6867]],
       dtype=torch.float64)
	q_value: tensor([[-0.7973]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2452417300796474 entropy 1.7646539211273193
epoch: 3, step: 82
	action: tensor([[-0.0078, -0.9965,  0.6097, -1.6456,  2.1912,  0.2038,  2.2768]],
       dtype=torch.float64)
	q_value: tensor([[-0.7973]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8223639051485949, distance: 1.5448070605782351 entropy 1.7646539211273193
epoch: 3, step: 83
	action: tensor([[-0.3823,  0.9827,  1.0327,  1.6634, -0.0177,  1.1492,  0.6630]],
       dtype=torch.float64)
	q_value: tensor([[-1.2893]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.71901170559058, distance: 0.6065977671308439 entropy 1.7646539211273193
epoch: 3, step: 84
	action: tensor([[ 0.5930, -0.6781, -1.1777,  0.6458, -1.0435,  1.3311,  2.1584]],
       dtype=torch.float64)
	q_value: tensor([[-0.8670]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04602410846203875, distance: 1.170381747876924 entropy 1.7646539211273193
epoch: 3, step: 85
	action: tensor([[ 0.7598,  0.0536, -0.3628, -2.1389,  0.6392, -0.8096,  0.8154]],
       dtype=torch.float64)
	q_value: tensor([[-1.2097]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05558969467472674, distance: 1.1757209695752002 entropy 1.7646539211273193
epoch: 3, step: 86
	action: tensor([[ 0.6003, -0.8869,  0.7331,  1.5914,  0.4331,  0.0751, -1.3312]],
       dtype=torch.float64)
	q_value: tensor([[-0.8438]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8031507921594352, distance: 0.5077191320998393 entropy 1.7646539211273193
epoch: 3, step: 87
	action: tensor([[-0.7939, -0.2135, -1.5851,  1.1105,  2.0228,  0.0809, -1.4507]],
       dtype=torch.float64)
	q_value: tensor([[-0.7875]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.245015324772634, distance: 1.7146139319208515 entropy 1.7646539211273193
epoch: 3, step: 88
	action: tensor([[ 0.0083,  1.2490, -0.6006,  1.2072, -0.6403, -0.5999, -0.2220]],
       dtype=torch.float64)
	q_value: tensor([[-0.7822]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 89
	action: tensor([[ 2.3705,  1.5914, -0.1192,  0.4314, -0.1143, -2.3193,  0.0955]],
       dtype=torch.float64)
	q_value: tensor([[-0.7973]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6659553878373967, distance: 0.661391986965986 entropy 1.7646539211273193
epoch: 3, step: 90
	action: tensor([[-0.5123,  0.5319,  2.1465, -0.6870, -2.2811, -4.1005,  1.0037]],
       dtype=torch.float64)
	q_value: tensor([[-0.9495]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 91
	action: tensor([[ 1.0585, -1.5573, -0.2342, -0.2354, -1.9898, -1.8607,  0.6822]],
       dtype=torch.float64)
	q_value: tensor([[-0.7973]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.014031101447983585, distance: 1.1362876883844466 entropy 1.7646539211273193
epoch: 3, step: 92
	action: tensor([[-0.2875,  1.3053,  1.3266, -0.3189,  1.8911,  0.2438, -0.4553]],
       dtype=torch.float64)
	q_value: tensor([[-1.1625]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 93
	action: tensor([[ 0.0792, -0.4285, -0.0139,  0.7687,  1.6890,  2.8944, -1.6384]],
       dtype=torch.float64)
	q_value: tensor([[-0.7973]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 94
	action: tensor([[ 1.6645, -0.6811, -0.5617, -0.2928, -1.6446, -5.1364,  1.4906]],
       dtype=torch.float64)
	q_value: tensor([[-0.7973]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 95
	action: tensor([[-1.4385,  1.4098, -0.6189, -2.7880,  2.5210, -2.0093, -0.8050]],
       dtype=torch.float64)
	q_value: tensor([[-0.7973]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 96
	action: tensor([[-0.4855,  1.1260, -0.8977,  0.2962, -0.1499,  0.2810, -1.6222]],
       dtype=torch.float64)
	q_value: tensor([[-0.7973]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 97
	action: tensor([[ 2.7563,  0.1575, -0.4473, -0.6886, -1.9320, -1.0219,  0.1816]],
       dtype=torch.float64)
	q_value: tensor([[-0.7973]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 98
	action: tensor([[ 0.7120,  0.9347,  0.3239, -1.7669, -2.6891, -0.1182,  3.4124]],
       dtype=torch.float64)
	q_value: tensor([[-0.7973]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7137476525269 entropy 1.7646539211273193
epoch: 3, step: 99
	action: tensor([[-0.5251,  0.5761,  1.2464, -0.6809,  1.8956, -1.9120, -4.1969]],
       dtype=torch.float64)
	q_value: tensor([[-0.7973]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0343886864457263 entropy 1.7646539211273193
epoch: 3, step: 100
	action: tensor([[ 0.3695,  0.6504, -2.1407,  0.4409,  0.2628, -0.6984, -0.8565]],
       dtype=torch.float64)
	q_value: tensor([[-0.7973]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8306457922636502, distance: 0.47092819076996534 entropy 1.7646539211273193
epoch: 3, step: 101
	action: tensor([[ 0.3667, -1.2588,  0.3740,  0.9246, -1.7350, -1.3406, -2.3451]],
       dtype=torch.float64)
	q_value: tensor([[-0.7018]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28042598440933264, distance: 0.970721009810156 entropy 1.7646539211273193
epoch: 3, step: 102
	action: tensor([[ 0.2172,  2.2053,  1.4096, -0.5796,  1.3249,  1.6122,  4.3462]],
       dtype=torch.float64)
	q_value: tensor([[-1.1200]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 103
	action: tensor([[ 1.1836,  2.5520, -0.8182,  0.5484, -2.3743,  0.5642,  2.4693]],
       dtype=torch.float64)
	q_value: tensor([[-0.7973]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 104
	action: tensor([[-1.5010,  0.4797, -0.6236,  0.7334,  2.3800, -1.3923,  1.6156]],
       dtype=torch.float64)
	q_value: tensor([[-0.7973]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5837281630714533, distance: 1.4401144667678167 entropy 1.7646539211273193
epoch: 3, step: 105
	action: tensor([[ 1.2180,  0.6916,  2.5252,  0.5062,  2.4351, -0.1585,  1.5357]],
       dtype=torch.float64)
	q_value: tensor([[-1.0421]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7842193237193371, distance: 0.5315730665252021 entropy 1.7646539211273193
epoch: 3, step: 106
	action: tensor([[-1.2442, -1.0300,  1.9531, -0.4662,  0.7805, -1.8511, -1.1737]],
       dtype=torch.float64)
	q_value: tensor([[-1.1998]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8175806791936486, distance: 1.5427783723915534 entropy 1.7646539211273193
epoch: 3, step: 107
	action: tensor([[-1.5451, -0.2171,  1.2668, -1.4592,  2.7995, -1.1447, -1.2530]],
       dtype=torch.float64)
	q_value: tensor([[-1.0313]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.490899324865656, distance: 1.8060708398981575 entropy 1.7646539211273193
epoch: 3, step: 108
	action: tensor([[-3.0410,  0.3019, -1.1871, -1.6091, -1.5356, -2.1429, -0.0783]],
       dtype=torch.float64)
	q_value: tensor([[-0.9277]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 109
	action: tensor([[-0.0457,  0.1588,  2.5912,  0.5483, -1.1871, -1.9310,  1.6953]],
       dtype=torch.float64)
	q_value: tensor([[-0.7973]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14072742367446978, distance: 1.0607722965067639 entropy 1.7646539211273193
epoch: 3, step: 110
	action: tensor([[-0.0615,  1.0761, -0.0824, -1.1909, -1.6727,  2.0414,  1.5137]],
       dtype=torch.float64)
	q_value: tensor([[-1.4389]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2976431806203039, distance: 0.9590375129092705 entropy 1.7646539211273193
epoch: 3, step: 111
	action: tensor([[ 2.6197,  1.0870, -1.7341, -1.5369, -1.0209, -1.1608, -0.8702]],
       dtype=torch.float64)
	q_value: tensor([[-1.5009]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 112
	action: tensor([[ 0.9454, -1.1637,  1.8405, -0.6609,  0.3371,  0.4075, -2.0564]],
       dtype=torch.float64)
	q_value: tensor([[-0.7973]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.40867497585603174, distance: 1.3581948930216228 entropy 1.7646539211273193
epoch: 3, step: 113
	action: tensor([[-1.3766,  0.4587, -0.8762,  0.7685, -0.3202,  1.6698, -1.6635]],
       dtype=torch.float64)
	q_value: tensor([[-0.9782]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.42403128567777015, distance: 1.3655778338075801 entropy 1.7646539211273193
epoch: 3, step: 114
	action: tensor([[-0.0302,  0.9281,  2.8264,  0.4059, -1.8151, -0.8822, -0.6244]],
       dtype=torch.float64)
	q_value: tensor([[-0.9119]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.43024445884119134, distance: 0.8637756798984747 entropy 1.7646539211273193
epoch: 3, step: 115
	action: tensor([[-0.5213, -1.0339, -0.7526, -0.9258, -1.9934, -1.6422,  0.6522]],
       dtype=torch.float64)
	q_value: tensor([[-1.3653]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07615376446848776, distance: 1.1871179118082213 entropy 1.7646539211273193
epoch: 3, step: 116
	action: tensor([[ 0.3595,  0.5712, -0.4827, -0.4963, -0.6777, -1.0807, -0.2715]],
       dtype=torch.float64)
	q_value: tensor([[-1.1209]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8940266535106981, distance: 0.3725246520572848 entropy 1.7646539211273193
epoch: 3, step: 117
	action: tensor([[-3.6985,  1.1542, -0.4086, -1.2708,  0.4986,  0.4983,  1.2802]],
       dtype=torch.float64)
	q_value: tensor([[-0.5697]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 118
	action: tensor([[ 0.8900, -0.3453, -0.7938, -0.2645,  0.0428,  0.5901,  0.3418]],
       dtype=torch.float64)
	q_value: tensor([[-0.7973]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.47299460483568667, distance: 0.8307382817483643 entropy 1.7646539211273193
epoch: 3, step: 119
	action: tensor([[-0.0278,  0.5908, -0.6221,  1.3373,  0.4163, -0.6151,  3.1494]],
       dtype=torch.float64)
	q_value: tensor([[-0.5518]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17872742955440135, distance: 1.0370515676730527 entropy 1.7646539211273193
epoch: 3, step: 120
	action: tensor([[-3.1488,  0.6314, -1.2326, -0.8770,  0.8223, -1.5425, -2.1613]],
       dtype=torch.float64)
	q_value: tensor([[-1.1684]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 121
	action: tensor([[-0.1744,  0.9770,  0.0421, -3.1469,  2.1191, -0.1382, -1.4452]],
       dtype=torch.float64)
	q_value: tensor([[-0.7973]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 122
	action: tensor([[ 1.9722,  2.1274, -0.6950, -2.6912, -1.5215,  1.2417,  1.3507]],
       dtype=torch.float64)
	q_value: tensor([[-0.7973]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 123
	action: tensor([[ 0.4369,  0.9886, -1.4941, -0.1647, -1.5932, -0.8489, -3.0235]],
       dtype=torch.float64)
	q_value: tensor([[-0.7973]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 124
	action: tensor([[ 0.5812,  3.5384,  0.2751, -0.9463,  0.7692,  2.5040, -0.3049]],
       dtype=torch.float64)
	q_value: tensor([[-0.7973]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 125
	action: tensor([[ 2.1778,  0.6543, -2.0191, -0.4134,  1.4159, -1.5679, -2.1754]],
       dtype=torch.float64)
	q_value: tensor([[-0.7973]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36471174428419106, distance: 0.9120992669211273 entropy 1.7646539211273193
epoch: 3, step: 126
	action: tensor([[-2.3924,  1.3975, -3.1307, -0.1372,  1.9747, -0.7912,  0.8688]],
       dtype=torch.float64)
	q_value: tensor([[-0.9982]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
epoch: 3, step: 127
	action: tensor([[-2.0442,  0.6848,  0.2532,  1.2688, -0.7022, -1.4080,  2.0815]],
       dtype=torch.float64)
	q_value: tensor([[-0.7973]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.7646539211273193
LOSS epoch 3 actor 436.77353699103503 critic 2146.5277059019527 
epoch: 4, step: 0
	action: tensor([[ 0.6542, -0.0295, -0.3304, -1.3040,  1.0622, -1.0512, -0.6207]],
       dtype=torch.float64)
	q_value: tensor([[-1.6500]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.015445172758521974, distance: 1.1354725671622776 entropy 1.6592931747436523
epoch: 4, step: 1
	action: tensor([[ 0.1770,  0.3290,  2.7353, -0.6086,  0.3520,  0.6907, -0.3666]],
       dtype=torch.float64)
	q_value: tensor([[-1.1569]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5726371605887612, distance: 0.7480921048469472 entropy 1.6592931747436523
epoch: 4, step: 2
	action: tensor([[-1.0512, -0.9985,  0.4104, -0.9268, -1.4749,  1.5894, -0.2103]],
       dtype=torch.float64)
	q_value: tensor([[-1.9703]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.114051552646382, distance: 1.6638511840639845 entropy 1.6592931747436523
epoch: 4, step: 3
	action: tensor([[-0.4031,  1.0372, -0.2738, -0.1847,  1.4449, -1.0416, -0.0694]],
       dtype=torch.float64)
	q_value: tensor([[-2.2327]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06199591957130057, distance: 1.1083043991783585 entropy 1.6592931747436523
epoch: 4, step: 4
	action: tensor([[-2.1376,  0.7933,  0.0546,  1.4908, -0.5315,  0.4962, -0.5147]],
       dtype=torch.float64)
	q_value: tensor([[-1.0020]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 4, step: 5
	action: tensor([[-1.1353, -1.2523, -0.6556, -2.0991,  1.3444, -1.0689, -1.9353]],
       dtype=torch.float64)
	q_value: tensor([[-1.6500]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2008085975774334, distance: 1.0230152196232933 entropy 1.6592931747436523
epoch: 4, step: 6
	action: tensor([[-0.3908,  1.0611, -2.5366,  1.0165, -0.2859, -0.4056,  0.1043]],
       dtype=torch.float64)
	q_value: tensor([[-1.6920]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07859110196461061, distance: 1.188461479361721 entropy 1.6592931747436523
epoch: 4, step: 7
	action: tensor([[ 1.7090, -0.7108,  2.6365, -0.4483,  0.4893,  0.6730,  0.3026]],
       dtype=torch.float64)
	q_value: tensor([[-1.6755]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04115407992449027, distance: 1.1676540659051582 entropy 1.6592931747436523
epoch: 4, step: 8
	action: tensor([[-0.0962, -0.7478,  0.0083,  2.2684,  0.5488, -0.5199,  0.2964]],
       dtype=torch.float64)
	q_value: tensor([[-2.3613]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5594705239707588, distance: 0.7595286834468177 entropy 1.6592931747436523
epoch: 4, step: 9
	action: tensor([[-0.1253, -0.9771,  1.5935, -0.2206, -1.1884,  0.2653, -0.1298]],
       dtype=torch.float64)
	q_value: tensor([[-1.7567]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7147242040418023, distance: 1.4984899422546492 entropy 1.6592931747436523
epoch: 4, step: 10
	action: tensor([[-1.6225,  3.2597,  0.4308, -2.0098,  0.1417, -0.1247, -0.5372]],
       dtype=torch.float64)
	q_value: tensor([[-1.9333]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 4, step: 11
	action: tensor([[ 0.0104,  1.5915, -0.8796,  1.6573,  1.5503,  0.2059,  1.4559]],
       dtype=torch.float64)
	q_value: tensor([[-1.6500]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3957377923826111, distance: 1.35194371855237 entropy 1.6592931747436523
epoch: 4, step: 12
	action: tensor([[ 2.1798, -0.0514,  0.9028,  0.9443, -0.3791, -3.5089, -1.1305]],
       dtype=torch.float64)
	q_value: tensor([[-1.7404]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 4, step: 13
	action: tensor([[-1.2551, -0.6619, -1.3485, -0.2454, -0.8121, -2.1096,  0.5410]],
       dtype=torch.float64)
	q_value: tensor([[-1.6500]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.27440996762292347, distance: 1.2918475712893758 entropy 1.6592931747436523
epoch: 4, step: 14
	action: tensor([[-0.3401, -0.2630, -1.2450, -1.1216,  0.4138, -0.3470, -1.7926]],
       dtype=torch.float64)
	q_value: tensor([[-2.0200]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3317122238197969, distance: 0.9354884976415561 entropy 1.6592931747436523
epoch: 4, step: 15
	action: tensor([[ 0.0323, -1.0209,  0.6409,  0.5519, -0.3596, -0.9388, -0.8318]],
       dtype=torch.float64)
	q_value: tensor([[-1.1273]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3942539403929537, distance: 1.351224880868882 entropy 1.6592931747436523
epoch: 4, step: 16
	action: tensor([[ 3.4075,  0.8449, -0.8819,  0.1465, -1.1728,  0.9343, -3.0115]],
       dtype=torch.float64)
	q_value: tensor([[-1.3441]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 4, step: 17
	action: tensor([[ 1.6398,  1.5233,  3.2032, -0.9981,  2.0420, -0.4440, -0.5144]],
       dtype=torch.float64)
	q_value: tensor([[-1.6500]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 4, step: 18
	action: tensor([[ 0.7103,  0.5283,  0.7831, -1.8744, -1.3381, -0.6594,  0.3777]],
       dtype=torch.float64)
	q_value: tensor([[-1.6500]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27444213384993776, distance: 0.9747488258215316 entropy 1.6592931747436523
epoch: 4, step: 19
	action: tensor([[-1.5961,  0.9325, -2.7145,  0.9344, -1.6036, -1.3598, -0.1265]],
       dtype=torch.float64)
	q_value: tensor([[-2.1758]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.833906727885692, distance: 1.5496917290799115 entropy 1.6592931747436523
epoch: 4, step: 20
	action: tensor([[ 0.8807, -1.9670, -0.6225, -0.4402, -0.1145, -1.6716,  0.3523]],
       dtype=torch.float64)
	q_value: tensor([[-2.2461]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 4, step: 21
	action: tensor([[-0.3114,  0.0245,  0.9373, -2.1081,  2.4863,  0.3243, -0.4718]],
       dtype=torch.float64)
	q_value: tensor([[-1.6500]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5433736697752336, distance: 1.42164851813982 entropy 1.6592931747436523
epoch: 4, step: 22
	action: tensor([[-1.9443, -0.7861,  0.9209, -0.9809,  0.1472,  0.0647, -1.1080]],
       dtype=torch.float64)
	q_value: tensor([[-1.5950]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 4, step: 23
	action: tensor([[ 2.0641, -0.1586, -1.0204, -0.3043, -0.9232, -0.9644,  0.4689]],
       dtype=torch.float64)
	q_value: tensor([[-1.6500]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04692221723469303, distance: 1.1708840807487195 entropy 1.6592931747436523
epoch: 4, step: 24
	action: tensor([[ 0.6084,  2.3121, -0.1074, -1.6502, -0.2470, -2.2004, -1.9887]],
       dtype=torch.float64)
	q_value: tensor([[-1.6156]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 4, step: 25
	action: tensor([[-2.0938,  0.1668, -1.3164, -0.5161,  0.3920, -1.0226,  1.5079]],
       dtype=torch.float64)
	q_value: tensor([[-1.6500]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 4, step: 26
	action: tensor([[-0.4205,  1.1150,  2.9345, -1.7163,  0.8933,  0.3324,  1.0220]],
       dtype=torch.float64)
	q_value: tensor([[-1.6500]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.059444638579711384, distance: 1.1098106164318216 entropy 1.6592931747436523
epoch: 4, step: 27
	action: tensor([[ 0.4523,  0.2961,  1.2781,  1.6961, -1.1534, -1.0466,  1.0218]],
       dtype=torch.float64)
	q_value: tensor([[-2.3487]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7890752024812973, distance: 0.5255578335292012 entropy 1.6592931747436523
epoch: 4, step: 28
	action: tensor([[-3.7383,  0.0323, -1.7211, -1.2977, -0.6594,  1.2556,  0.0742]],
       dtype=torch.float64)
	q_value: tensor([[-2.0054]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 4, step: 29
	action: tensor([[-0.8526,  1.5490, -0.8396,  0.2904,  1.4815, -0.6402,  1.2494]],
       dtype=torch.float64)
	q_value: tensor([[-1.6500]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 4, step: 30
	action: tensor([[ 0.7622, -1.5827,  2.4498, -0.7043,  1.1589, -1.9903, -1.4435]],
       dtype=torch.float64)
	q_value: tensor([[-1.6500]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4795010531134871, distance: 0.8255941763804722 entropy 1.6592931747436523
epoch: 4, step: 31
	action: tensor([[-0.7475,  1.2324,  0.6215,  0.0763, -0.7655, -1.2525, -0.5802]],
       dtype=torch.float64)
	q_value: tensor([[-2.3311]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19136943900184633, distance: 1.0290388438344402 entropy 1.6592931747436523
epoch: 4, step: 32
	action: tensor([[ 0.3911, -1.9071, -1.2642, -1.3593, -0.8826,  1.2281, -2.2639]],
       dtype=torch.float64)
	q_value: tensor([[-1.3111]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 4, step: 33
	action: tensor([[ 1.4727,  0.7951, -1.7034,  0.2626, -0.9857,  0.8257,  0.4281]],
       dtype=torch.float64)
	q_value: tensor([[-1.6500]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9421142642658761, distance: 0.27532298666117033 entropy 1.6592931747436523
epoch: 4, step: 34
	action: tensor([[ 1.4018, -1.1001,  1.0177,  1.0727,  1.0192, -0.6294, -0.2639]],
       dtype=torch.float64)
	q_value: tensor([[-1.9615]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.062040178618323116, distance: 1.179307788646254 entropy 1.6592931747436523
epoch: 4, step: 35
	action: tensor([[-0.5753,  0.1120, -0.3122, -0.9976, -0.1630,  1.7179,  0.3404]],
       dtype=torch.float64)
	q_value: tensor([[-1.7574]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16245500997278328, distance: 1.2338000117717198 entropy 1.6592931747436523
epoch: 4, step: 36
	action: tensor([[ 2.2867,  0.6883, -1.9726,  0.0484,  1.7902, -0.2263, -0.2097]],
       dtype=torch.float64)
	q_value: tensor([[-1.7442]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 4, step: 37
	action: tensor([[ 1.1464,  0.1394, -0.1737,  0.5228,  0.5819, -0.8141,  2.2303]],
       dtype=torch.float64)
	q_value: tensor([[-1.6500]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8991924108573798, distance: 0.36333171392858016 entropy 1.6592931747436523
epoch: 4, step: 38
	action: tensor([[-1.1566, -0.0074, -1.2232, -0.9334,  0.1083,  0.6939,  1.4427]],
       dtype=torch.float64)
	q_value: tensor([[-1.8144]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7189014323565328, distance: 1.500314062726916 entropy 1.6592931747436523
epoch: 4, step: 39
	action: tensor([[ 0.7053,  0.9268,  0.9493,  1.8477, -1.3587, -0.1525,  1.0142]],
       dtype=torch.float64)
	q_value: tensor([[-1.8327]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3489979027270922, distance: 0.9233107395569607 entropy 1.6592931747436523
epoch: 4, step: 40
	action: tensor([[-2.1586,  1.4825, -0.9198,  0.2410, -1.1377,  1.2926,  1.4392]],
       dtype=torch.float64)
	q_value: tensor([[-2.0261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0847159452599473, distance: 1.1918310768811171 entropy 1.6592931747436523
epoch: 4, step: 41
	action: tensor([[-0.1348,  0.9925,  1.4556, -0.5805,  0.1079, -0.3612,  0.4342]],
       dtype=torch.float64)
	q_value: tensor([[-2.3155]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 4, step: 42
	action: tensor([[-0.2443, -0.0872,  0.9552, -0.2395,  2.2222,  0.9320, -0.5196]],
       dtype=torch.float64)
	q_value: tensor([[-1.6500]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3317713493946597, distance: 1.3206007119685244 entropy 1.6592931747436523
epoch: 4, step: 43
	action: tensor([[-1.3499, -1.7628,  1.2648,  0.4027,  1.6903, -0.9250,  1.0248]],
       dtype=torch.float64)
	q_value: tensor([[-1.3041]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 4, step: 44
	action: tensor([[-0.9119,  0.0870,  1.2108,  0.5998, -0.1431,  0.5134,  1.0977]],
       dtype=torch.float64)
	q_value: tensor([[-1.6500]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.035220325807569064, distance: 1.1240115291989203 entropy 1.6592931747436523
epoch: 4, step: 45
	action: tensor([[-0.3095, -1.8178,  1.1736,  0.6935,  0.3579,  2.1121,  1.5764]],
       dtype=torch.float64)
	q_value: tensor([[-1.6788]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 4, step: 46
	action: tensor([[-1.1510,  0.3281, -0.6269, -0.5213, -2.5920, -0.4571, -0.2293]],
       dtype=torch.float64)
	q_value: tensor([[-1.6500]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.789143682100875, distance: 1.5306620079663038 entropy 1.6592931747436523
epoch: 4, step: 47
	action: tensor([[-1.4284,  1.3475, -2.6536,  0.5877, -0.5389, -0.1714,  2.0235]],
       dtype=torch.float64)
	q_value: tensor([[-2.2607]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 4, step: 48
	action: tensor([[ 1.3209,  0.2721, -0.6736, -1.5951, -1.7178, -1.6311, -0.5437]],
       dtype=torch.float64)
	q_value: tensor([[-1.6500]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34137041247931055, distance: 0.9287039902890577 entropy 1.6592931747436523
epoch: 4, step: 49
	action: tensor([[-0.7584, -0.4393,  0.4269,  0.4865, -0.6430, -0.7723, -1.0604]],
       dtype=torch.float64)
	q_value: tensor([[-2.1294]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6736858298637849, distance: 1.4804497223823019 entropy 1.6592931747436523
epoch: 4, step: 50
	action: tensor([[ 0.2488,  1.1939, -0.4971,  0.5688,  0.2730,  0.2018,  0.5121]],
       dtype=torch.float64)
	q_value: tensor([[-1.2233]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 4, step: 51
	action: tensor([[ 1.9952, -0.6794, -0.4264, -0.5581, -1.2637, -3.2463, -0.5845]],
       dtype=torch.float64)
	q_value: tensor([[-1.6500]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 4, step: 52
	action: tensor([[ 0.8443,  1.7887,  0.7668,  1.1089, -1.0469, -0.2640,  0.0187]],
       dtype=torch.float64)
	q_value: tensor([[-1.6500]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 4, step: 53
	action: tensor([[-0.4503, -1.0625,  0.6142, -0.0858, -0.1707, -1.7028,  2.0036]],
       dtype=torch.float64)
	q_value: tensor([[-1.6500]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7534262198718928, distance: 1.5153063442020942 entropy 1.6592931747436523
epoch: 4, step: 54
	action: tensor([[ 0.8206, -1.8184,  0.2753,  2.4010,  0.9868, -0.4762,  1.0878]],
       dtype=torch.float64)
	q_value: tensor([[-2.3581]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 4, step: 55
	action: tensor([[ 1.4231,  0.9503,  0.5658, -0.9114, -1.3312,  0.8765,  0.3020]],
       dtype=torch.float64)
	q_value: tensor([[-1.6500]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.04822520723838482 entropy 1.6592931747436523
epoch: 4, step: 56
	action: tensor([[ 0.3956, -1.2186,  1.5653, -1.4382, -0.8666, -2.9309,  0.7132]],
       dtype=torch.float64)
	q_value: tensor([[-1.6500]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 4, step: 57
	action: tensor([[ 1.6491, -0.9680, -0.1355, -1.4150,  0.9650, -1.3148,  0.2391]],
       dtype=torch.float64)
	q_value: tensor([[-1.6500]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18803924415152595, distance: 1.2473033439724066 entropy 1.6592931747436523
epoch: 4, step: 58
	action: tensor([[-3.0182,  0.5966, -0.0972, -3.2500, -0.4895,  0.9728,  1.4068]],
       dtype=torch.float64)
	q_value: tensor([[-1.7397]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 4, step: 59
	action: tensor([[ 1.9201,  1.8602, -1.9350, -1.1745,  1.4150, -1.6464, -2.3198]],
       dtype=torch.float64)
	q_value: tensor([[-1.6500]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 4, step: 60
	action: tensor([[-0.2408, -0.0914,  0.7046, -0.5913,  1.1789, -0.4644,  0.7114]],
       dtype=torch.float64)
	q_value: tensor([[-1.6500]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5904450441776132, distance: 1.4431651301475872 entropy 1.6592931747436523
epoch: 4, step: 61
	action: tensor([[-0.1480,  0.5194, -1.2386,  1.1418,  1.2346, -1.0959, -0.9386]],
       dtype=torch.float64)
	q_value: tensor([[-1.1699]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1392127910138803, distance: 1.221403385788258 entropy 1.6592931747436523
epoch: 4, step: 62
	action: tensor([[-0.5925,  0.0244,  1.1033, -0.7157,  1.4366, -1.4083,  1.3607]],
       dtype=torch.float64)
	q_value: tensor([[-1.3085]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4688519564131539, distance: 1.386901782382754 entropy 1.6592931747436523
epoch: 4, step: 63
	action: tensor([[-1.2520,  0.9609, -1.8891,  1.1461,  0.3079,  0.7629,  0.1618]],
       dtype=torch.float64)
	q_value: tensor([[-1.8293]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6913181115391889, distance: 1.48822756072599 entropy 1.6592931747436523
epoch: 4, step: 64
	action: tensor([[-0.9871,  0.2915,  1.2157, -3.8040,  1.2895, -0.3848, -1.3081]],
       dtype=torch.float64)
	q_value: tensor([[-1.6199]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 4, step: 65
	action: tensor([[ 0.1713, -0.8085, -0.3194,  1.7758,  0.0319,  1.0077,  0.5446]],
       dtype=torch.float64)
	q_value: tensor([[-1.6500]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.633994196373295, distance: 0.6923101186720607 entropy 1.6592931747436523
epoch: 4, step: 66
	action: tensor([[ 0.1797,  0.1532, -0.0758, -1.7802,  0.9902, -1.2407, -0.9050]],
       dtype=torch.float64)
	q_value: tensor([[-1.5308]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0851854255495208, distance: 1.1920889695904104 entropy 1.6592931747436523
epoch: 4, step: 67
	action: tensor([[-1.1415,  1.8746,  2.3183, -0.3560, -1.1570, -3.5447,  1.9579]],
       dtype=torch.float64)
	q_value: tensor([[-1.2921]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 4, step: 68
	action: tensor([[-0.7566, -0.8342, -0.9225, -2.7490, -1.0478, -0.4470,  0.6356]],
       dtype=torch.float64)
	q_value: tensor([[-1.6500]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 4, step: 69
	action: tensor([[ 2.3652, -1.2616,  2.2042,  0.3582,  0.4824,  0.8589,  0.1172]],
       dtype=torch.float64)
	q_value: tensor([[-1.6500]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0215225573893258, distance: 1.6270316050568692 entropy 1.6592931747436523
epoch: 4, step: 70
	action: tensor([[-1.5356, -1.2512, -0.3496,  0.0925, -0.0694, -0.5207,  1.7862]],
       dtype=torch.float64)
	q_value: tensor([[-2.3070]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.4893562118698198, distance: 1.8055113224735058 entropy 1.6592931747436523
epoch: 4, step: 71
	action: tensor([[ 0.2596,  1.1562, -1.9889, -1.0136, -0.5639, -0.8134,  0.8401]],
       dtype=torch.float64)
	q_value: tensor([[-2.2778]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6666686696873481, distance: 0.6606854780335405 entropy 1.6592931747436523
epoch: 4, step: 72
	action: tensor([[ 2.0061, -0.4103, -1.5812, -1.5118, -1.3633, -1.1442,  2.8071]],
       dtype=torch.float64)
	q_value: tensor([[-1.9329]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 4, step: 73
	action: tensor([[ 0.4723, -1.2628,  0.8255, -1.1308, -0.4773, -0.5196,  1.6660]],
       dtype=torch.float64)
	q_value: tensor([[-1.6500]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8830617217990562, distance: 1.5703229248164776 entropy 1.6592931747436523
epoch: 4, step: 74
	action: tensor([[ 1.0975, -0.4319,  4.4793, -2.1016, -0.8816, -1.0029, -0.8542]],
       dtype=torch.float64)
	q_value: tensor([[-2.1190]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.811688427916983 entropy 1.6592931747436523
epoch: 4, step: 75
	action: tensor([[ 0.8055,  1.2545, -0.0382, -0.6735,  1.5093, -1.8157,  1.6194]],
       dtype=torch.float64)
	q_value: tensor([[-1.6500]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8126221765743978, distance: 0.4953541299497399 entropy 1.6592931747436523
epoch: 4, step: 76
	action: tensor([[-2.4909,  1.6408,  0.6334,  0.5710,  1.6122,  0.7758,  0.4437]],
       dtype=torch.float64)
	q_value: tensor([[-1.7229]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 4, step: 77
	action: tensor([[-3.5754, -0.1980, -0.4351, -1.0045, -1.1833,  0.7223,  0.4631]],
       dtype=torch.float64)
	q_value: tensor([[-1.6500]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 4, step: 78
	action: tensor([[-1.5982,  0.3549,  0.2466,  1.9779,  1.2931,  0.4668, -1.1483]],
       dtype=torch.float64)
	q_value: tensor([[-1.6500]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6104689577444125, distance: 0.7142129651821542 entropy 1.6592931747436523
epoch: 4, step: 79
	action: tensor([[-1.4597,  1.8184,  1.3831,  0.6488, -0.2781, -1.3667,  0.8911]],
       dtype=torch.float64)
	q_value: tensor([[-1.6432]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 4, step: 80
	action: tensor([[-0.4887, -1.2316,  1.4187, -0.7594, -0.8655, -0.1710, -1.5232]],
       dtype=torch.float64)
	q_value: tensor([[-1.6500]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.4213397496503295, distance: 1.7806745804439392 entropy 1.6592931747436523
epoch: 4, step: 81
	action: tensor([[-0.1806, -1.1396, -0.1882, -0.3566,  0.7845,  0.9736, -0.7352]],
       dtype=torch.float64)
	q_value: tensor([[-1.7957]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5273055223246597, distance: 1.4142287255165042 entropy 1.6592931747436523
epoch: 4, step: 82
	action: tensor([[ 0.6568, -2.6923, -0.7201,  0.0942, -0.4752, -0.2294,  1.9584]],
       dtype=torch.float64)
	q_value: tensor([[-1.0792]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 4, step: 83
	action: tensor([[ 2.0842, -0.5767,  0.2246,  0.3908,  0.2324, -3.0358, -0.2359]],
       dtype=torch.float64)
	q_value: tensor([[-1.6500]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 4, step: 84
	action: tensor([[-2.9046, -0.7220,  2.2691, -2.5227,  0.0378, -1.8698, -0.6054]],
       dtype=torch.float64)
	q_value: tensor([[-1.6500]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 4, step: 85
	action: tensor([[ 0.2448,  2.7051, -0.3309, -0.8062, -0.7863, -1.3936,  0.4106]],
       dtype=torch.float64)
	q_value: tensor([[-1.6500]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 4, step: 86
	action: tensor([[-0.0034, -1.4805, -0.6278, -1.2712,  0.1931, -0.3515,  0.4355]],
       dtype=torch.float64)
	q_value: tensor([[-1.6500]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3202567448028224, distance: 1.314879307015463 entropy 1.6592931747436523
epoch: 4, step: 87
	action: tensor([[-1.0064, -0.5950,  1.6656, -0.3235, -1.4524, -1.2760, -1.0160]],
       dtype=torch.float64)
	q_value: tensor([[-1.3321]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1845515729808986, distance: 1.6913669664628916 entropy 1.6592931747436523
epoch: 4, step: 88
	action: tensor([[ 0.6768, -0.9864, -1.9573, -1.8382, -0.6708, -0.8116,  1.3692]],
       dtype=torch.float64)
	q_value: tensor([[-2.0196]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7438593131686104, distance: 0.5791565576730919 entropy 1.6592931747436523
epoch: 4, step: 89
	action: tensor([[-1.6854,  1.3891, -0.3460,  0.1532,  0.3362, -1.0026,  0.9963]],
       dtype=torch.float64)
	q_value: tensor([[-2.2130]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 4, step: 90
	action: tensor([[ 0.3621,  0.9310,  0.2449, -0.5448, -0.2676, -0.4906,  1.5721]],
       dtype=torch.float64)
	q_value: tensor([[-1.6500]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7887676533355003, distance: 0.525940851523518 entropy 1.6592931747436523
epoch: 4, step: 91
	action: tensor([[-1.2376,  1.2011,  0.5102,  1.2583,  0.8756,  0.4755,  1.4575]],
       dtype=torch.float64)
	q_value: tensor([[-1.4579]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 4, step: 92
	action: tensor([[ 0.1943,  1.2317,  1.5782,  0.2707, -1.0295, -2.5206, -2.7503]],
       dtype=torch.float64)
	q_value: tensor([[-1.6500]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6731163973808084, distance: 0.6542643565078363 entropy 1.6592931747436523
epoch: 4, step: 93
	action: tensor([[-0.4507, -1.1950, -1.6121,  3.0637,  0.4299,  0.1978, -1.5058]],
       dtype=torch.float64)
	q_value: tensor([[-2.2577]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 4, step: 94
	action: tensor([[ 0.5994, -0.2191, -1.4657, -0.6600,  1.2470, -1.5009, -0.8519]],
       dtype=torch.float64)
	q_value: tensor([[-1.6500]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31816735362802395, distance: 0.9449211918037765 entropy 1.6592931747436523
epoch: 4, step: 95
	action: tensor([[ 0.7578, -0.4528,  0.7252, -1.8355,  1.8918,  0.3221,  2.8952]],
       dtype=torch.float64)
	q_value: tensor([[-1.4090]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.37825512022234786, distance: 1.3434499779847249 entropy 1.6592931747436523
epoch: 4, step: 96
	action: tensor([[-0.6593,  0.9853, -0.4228, -0.5574, -1.8292,  1.3303,  0.8886]],
       dtype=torch.float64)
	q_value: tensor([[-2.7390]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1646951403714192, distance: 1.2349882481451127 entropy 1.6592931747436523
epoch: 4, step: 97
	action: tensor([[-0.8384,  0.4760, -2.4037, -0.7997,  1.6597, -1.0759, -0.0129]],
       dtype=torch.float64)
	q_value: tensor([[-2.3403]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3056403034877886, distance: 0.9535620214056973 entropy 1.6592931747436523
epoch: 4, step: 98
	action: tensor([[ 3.3965,  1.3710, -0.3291, -1.5952,  0.2659,  0.0374,  0.1697]],
       dtype=torch.float64)
	q_value: tensor([[-1.7955]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 4, step: 99
	action: tensor([[ 2.1236,  2.2669, -4.2547,  0.2334,  0.6998,  1.1675,  0.5183]],
       dtype=torch.float64)
	q_value: tensor([[-1.6500]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 4, step: 100
	action: tensor([[-1.0395,  0.8979,  2.4777, -0.4199, -0.4513, -0.3939, -1.7737]],
       dtype=torch.float64)
	q_value: tensor([[-1.6500]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 4, step: 101
	action: tensor([[-0.2894, -0.0051,  3.1606, -0.9927, -0.6596, -2.0279,  1.5504]],
       dtype=torch.float64)
	q_value: tensor([[-1.6500]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9711155659419218 entropy 1.6592931747436523
epoch: 4, step: 102
	action: tensor([[ 1.2667, -2.0407, -0.2347, -0.3381,  1.7496, -0.1264, -0.6097]],
       dtype=torch.float64)
	q_value: tensor([[-1.6500]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 4, step: 103
	action: tensor([[ 1.0732,  0.4562, -2.6991, -1.2532,  1.3722,  1.7694,  1.9294]],
       dtype=torch.float64)
	q_value: tensor([[-1.6500]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8812548185245268, distance: 0.3943344239507213 entropy 1.6592931747436523
epoch: 4, step: 104
	action: tensor([[-0.1402, -0.7090, -0.8937,  0.1652, -3.5978, -2.3858, -1.8503]],
       dtype=torch.float64)
	q_value: tensor([[-2.7220]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 4, step: 105
	action: tensor([[ 2.9428, -0.6805, -0.5400, -3.1330, -1.3206, -2.1865, -1.5755]],
       dtype=torch.float64)
	q_value: tensor([[-1.6500]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 4, step: 106
	action: tensor([[-1.5004,  1.8068, -1.9168,  1.4811,  0.3338, -2.9874,  0.0559]],
       dtype=torch.float64)
	q_value: tensor([[-1.6500]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 4, step: 107
	action: tensor([[ 1.3694,  1.0531,  0.6338, -0.6085,  0.6504,  0.2887,  1.5327]],
       dtype=torch.float64)
	q_value: tensor([[-1.6500]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9320454272873914, distance: 0.2983087802267148 entropy 1.6592931747436523
epoch: 4, step: 108
	action: tensor([[0.5821, 0.2023, 0.2466, 2.0694, 0.9327, 1.2582, 0.3156]],
       dtype=torch.float64)
	q_value: tensor([[-1.6547]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5649057472194223, distance: 0.7548286341481999 entropy 1.6592931747436523
epoch: 4, step: 109
	action: tensor([[-0.4197, -0.4243, -0.9542, -1.6512, -0.4285, -1.2261, -1.7379]],
       dtype=torch.float64)
	q_value: tensor([[-1.5972]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.545543574861125, distance: 0.7714411785121792 entropy 1.6592931747436523
epoch: 4, step: 110
	action: tensor([[-2.5888, -0.4322, -0.5296, -1.6790, -1.1145,  0.8071,  0.8078]],
       dtype=torch.float64)
	q_value: tensor([[-1.4729]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 4, step: 111
	action: tensor([[-0.3031, -0.0147,  0.3904, -2.1740,  3.1511, -1.2543, -0.4737]],
       dtype=torch.float64)
	q_value: tensor([[-1.6500]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.354688354816829 entropy 1.6592931747436523
epoch: 4, step: 112
	action: tensor([[-1.9899, -0.6034,  2.2851,  2.0173,  1.2808,  1.1540,  1.3591]],
       dtype=torch.float64)
	q_value: tensor([[-1.6500]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 4, step: 113
	action: tensor([[ 1.1688, -0.5663,  1.0074,  0.9658, -1.2169,  1.8514,  2.8650]],
       dtype=torch.float64)
	q_value: tensor([[-1.6500]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7915603400719008, distance: 0.5224525716590543 entropy 1.6592931747436523
epoch: 4, step: 114
	action: tensor([[-0.8933,  0.9964, -1.6883,  0.2781, -0.5676,  0.1486,  2.6587]],
       dtype=torch.float64)
	q_value: tensor([[-3.2246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24024664969501508, distance: 1.2744145599061507 entropy 1.6592931747436523
epoch: 4, step: 115
	action: tensor([[ 1.9002,  0.5869,  1.8999,  0.0894, -2.2778, -0.1686,  1.6264]],
       dtype=torch.float64)
	q_value: tensor([[-2.4286]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.47110682453102026, distance: 0.8322248409816076 entropy 1.6592931747436523
epoch: 4, step: 116
	action: tensor([[ 1.5912, -0.3801, -0.8802,  0.4510, -0.8817,  1.3096, -1.0766]],
       dtype=torch.float64)
	q_value: tensor([[-3.0575]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.588194391620875, distance: 0.7343495308615188 entropy 1.6592931747436523
epoch: 4, step: 117
	action: tensor([[ 0.0947, -0.6431, -0.3600,  0.7576,  0.6016, -0.0624, -0.8153]],
       dtype=torch.float64)
	q_value: tensor([[-1.7948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29757416562100203, distance: 0.9590846302331125 entropy 1.6592931747436523
epoch: 4, step: 118
	action: tensor([[-0.4306,  0.2161,  0.4224, -0.4405, -1.4394,  0.1153,  0.4393]],
       dtype=torch.float64)
	q_value: tensor([[-0.9591]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15290256144634973, distance: 1.2287201921613178 entropy 1.6592931747436523
epoch: 4, step: 119
	action: tensor([[ 1.5735, -0.0370, -0.6714, -0.2517, -1.2974,  0.2948,  0.6401]],
       dtype=torch.float64)
	q_value: tensor([[-1.5960]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10824839382625018, distance: 1.0806340392117135 entropy 1.6592931747436523
epoch: 4, step: 120
	action: tensor([[ 1.5314, -0.4006,  0.3123, -1.1546, -1.8160, -0.5315, -1.0743]],
       dtype=torch.float64)
	q_value: tensor([[-1.7761]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.40911753968177655, distance: 1.3584082285087604 entropy 1.6592931747436523
epoch: 4, step: 121
	action: tensor([[ 2.9698, -1.4776,  1.0871,  0.9905, -0.2095, -2.0457, -0.8318]],
       dtype=torch.float64)
	q_value: tensor([[-2.0999]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 4, step: 122
	action: tensor([[ 0.5258,  1.8806,  0.2781, -1.1891,  0.3571,  1.2292, -0.5059]],
       dtype=torch.float64)
	q_value: tensor([[-1.6500]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 4, step: 123
	action: tensor([[ 1.6429, -1.4026,  1.8092, -1.0700,  1.9252,  0.0194,  0.8211]],
       dtype=torch.float64)
	q_value: tensor([[-1.6500]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14621572742162514, distance: 1.0573792133445916 entropy 1.6592931747436523
epoch: 4, step: 124
	action: tensor([[-1.1514,  1.4366, -0.6339, -0.8925,  1.2511, -0.3705,  0.7107]],
       dtype=torch.float64)
	q_value: tensor([[-2.2444]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04067079756935921, distance: 1.1673830339234756 entropy 1.6592931747436523
epoch: 4, step: 125
	action: tensor([[ 1.3988,  0.2680,  2.0279, -0.7586, -2.2653, -1.5351, -0.6177]],
       dtype=torch.float64)
	q_value: tensor([[-1.3561]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7757380279390252, distance: 0.5419191669371262 entropy 1.6592931747436523
epoch: 4, step: 126
	action: tensor([[-2.3887,  1.2032,  0.8051,  1.7340, -0.1440,  1.0831,  0.4329]],
       dtype=torch.float64)
	q_value: tensor([[-2.8025]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 4, step: 127
	action: tensor([[-0.7589,  1.6107,  0.5581,  0.3041, -0.2200, -2.2028,  0.2762]],
       dtype=torch.float64)
	q_value: tensor([[-1.6500]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31453313083651124, distance: 0.9474360980334632 entropy 1.6592931747436523
LOSS epoch 4 actor 357.7084765039211 critic 2077.3136720101925 
epoch: 5, step: 0
	action: tensor([[ 0.5479, -0.5676, -2.3763,  0.2895, -0.4495,  0.1381, -1.5300]],
       dtype=torch.float64)
	q_value: tensor([[-2.7767]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08277091782263812, distance: 1.190762048502517 entropy 1.6592931747436523
epoch: 5, step: 1
	action: tensor([[ 0.0722, -0.7264,  1.6387,  0.6798,  0.8282, -1.3405, -0.0901]],
       dtype=torch.float64)
	q_value: tensor([[-2.7407]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19330510650209143, distance: 1.0278064698738354 entropy 1.6592931747436523
epoch: 5, step: 2
	action: tensor([[-1.1532, -0.4737,  1.1280,  0.9786, -0.8749, -0.5036,  0.1303]],
       dtype=torch.float64)
	q_value: tensor([[-3.1404]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6358313494795655, distance: 1.4636119836127182 entropy 1.6592931747436523
epoch: 5, step: 3
	action: tensor([[ 0.4505, -0.2604,  2.5436, -0.6812, -0.0281, -2.4397, -1.9669]],
       dtype=torch.float64)
	q_value: tensor([[-3.0355]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6678054804162092, distance: 0.6595578975428033 entropy 1.6592931747436523
epoch: 5, step: 4
	action: tensor([[-0.5366,  0.0343, -0.8762,  2.0889,  0.6664,  1.0009, -0.7644]],
       dtype=torch.float64)
	q_value: tensor([[-4.1302]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24047566552466737, distance: 0.9973039512364463 entropy 1.6592931747436523
epoch: 5, step: 5
	action: tensor([[ 0.3482, -0.3335,  0.3043,  0.2270, -0.1241, -0.5386, -0.4173]],
       dtype=torch.float64)
	q_value: tensor([[-2.7735]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3043464114889569, distance: 0.9544500568378389 entropy 1.6592931747436523
epoch: 5, step: 6
	action: tensor([[ 1.5139,  0.3994,  0.7627,  0.7767, -0.0260, -0.6536, -0.8440]],
       dtype=torch.float64)
	q_value: tensor([[-1.5411]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6369327944799881, distance: 0.6895252978242676 entropy 1.6592931747436523
epoch: 5, step: 7
	action: tensor([[ 0.4929,  0.4933, -1.4308, -1.6357, -0.9959, -0.3241,  1.0327]],
       dtype=torch.float64)
	q_value: tensor([[-2.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6776116570751618, distance: 0.6497501048611378 entropy 1.6592931747436523
epoch: 5, step: 8
	action: tensor([[-2.1344,  0.9344,  0.8112,  0.7886,  0.6414,  2.4425,  3.5265]],
       dtype=torch.float64)
	q_value: tensor([[-3.7416]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 5, step: 9
	action: tensor([[2.2069, 1.2327, 0.5708, 0.5094, 1.2203, 1.7264, 1.1479]],
       dtype=torch.float64)
	q_value: tensor([[-3.1277]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8040054727448296, distance: 0.5066157247425916 entropy 1.6592931747436523
epoch: 5, step: 10
	action: tensor([[ 1.1132,  0.1869, -0.3436, -3.0264,  0.6753, -0.6154,  0.2319]],
       dtype=torch.float64)
	q_value: tensor([[-3.4007]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 5, step: 11
	action: tensor([[-1.4507,  2.0317, -1.3544, -2.3223,  0.9040, -0.4789, -0.8285]],
       dtype=torch.float64)
	q_value: tensor([[-3.1277]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 5, step: 12
	action: tensor([[-1.4530, -1.2649,  1.2424,  0.4716,  1.0317, -0.3671,  3.2103]],
       dtype=torch.float64)
	q_value: tensor([[-3.1277]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 5, step: 13
	action: tensor([[ 0.4370, -0.1462,  1.8053, -0.3437, -0.2433,  0.2520, -1.0661]],
       dtype=torch.float64)
	q_value: tensor([[-3.1277]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.47592727395596435, distance: 0.8284236191917752 entropy 1.6592931747436523
epoch: 5, step: 14
	action: tensor([[-1.0347, -0.4285,  1.0497, -0.7699, -3.0586,  0.1177,  0.5627]],
       dtype=torch.float64)
	q_value: tensor([[-2.7899]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.6370800473851963, distance: 1.8583107947349982 entropy 1.6592931747436523
epoch: 5, step: 15
	action: tensor([[ 0.5378, -2.8498,  0.2798, -0.6675, -0.3528, -0.9050, -1.3374]],
       dtype=torch.float64)
	q_value: tensor([[-5.4031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 5, step: 16
	action: tensor([[ 3.8616,  1.5659,  0.2741, -1.1508, -2.1235, -2.0944,  0.4033]],
       dtype=torch.float64)
	q_value: tensor([[-3.1277]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 5, step: 17
	action: tensor([[-0.1096,  0.7230, -0.1011, -0.3616, -0.3215,  0.1484,  1.8296]],
       dtype=torch.float64)
	q_value: tensor([[-3.1277]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.45502902017551417, distance: 0.8447795291947724 entropy 1.6592931747436523
epoch: 5, step: 18
	action: tensor([[ 3.0370,  0.1841,  1.2971, -0.5971, -2.1922,  0.6890,  0.6030]],
       dtype=torch.float64)
	q_value: tensor([[-2.8907]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 5, step: 19
	action: tensor([[ 0.3248,  0.0716,  1.4426,  0.0216, -0.1620,  0.5408, -1.1205]],
       dtype=torch.float64)
	q_value: tensor([[-3.1277]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7327364847070627, distance: 0.5915977743148381 entropy 1.6592931747436523
epoch: 5, step: 20
	action: tensor([[ 0.0791, -0.9540,  1.9239,  1.0054, -1.6539, -2.1913,  0.9833]],
       dtype=torch.float64)
	q_value: tensor([[-2.4620]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1562732362573358, distance: 1.2305150504632103 entropy 1.6592931747436523
epoch: 5, step: 21
	action: tensor([[-0.2124,  0.8695, -0.2585, -0.8096,  3.0083, -1.5288, -0.0513]],
       dtype=torch.float64)
	q_value: tensor([[-4.9897]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5247690192327228, distance: 0.7888765849899363 entropy 1.6592931747436523
epoch: 5, step: 22
	action: tensor([[-0.1884,  1.4937,  2.1012,  0.6560, -1.5755,  0.2392,  0.5755]],
       dtype=torch.float64)
	q_value: tensor([[-3.1160]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 5, step: 23
	action: tensor([[-0.3616, -0.0381, -1.1365,  1.5572, -0.2250,  0.7203,  1.2233]],
       dtype=torch.float64)
	q_value: tensor([[-3.1277]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3628985601679964, distance: 1.3359446331734772 entropy 1.6592931747436523
epoch: 5, step: 24
	action: tensor([[ 1.4339,  0.5490, -1.7948,  2.5334,  0.8064, -0.9966, -1.2667]],
       dtype=torch.float64)
	q_value: tensor([[-2.8635]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 5, step: 25
	action: tensor([[ 0.7774, -1.7362,  0.0204, -0.1012,  0.5093, -0.2250,  1.5369]],
       dtype=torch.float64)
	q_value: tensor([[-3.1277]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7777798063037655, distance: 1.52579320916594 entropy 1.6592931747436523
epoch: 5, step: 26
	action: tensor([[-0.6569,  0.1039,  1.1946,  0.1186,  0.4227,  0.7098,  1.9890]],
       dtype=torch.float64)
	q_value: tensor([[-2.9831]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08219983571425615, distance: 1.1904479873381335 entropy 1.6592931747436523
epoch: 5, step: 27
	action: tensor([[ 0.9201,  0.5434,  0.6846, -0.9122, -0.0083, -2.5648,  0.4082]],
       dtype=torch.float64)
	q_value: tensor([[-3.5360]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6371391361141852, distance: 0.6893293313537298 entropy 1.6592931747436523
epoch: 5, step: 28
	action: tensor([[ 0.5555, -2.0172,  0.6590,  0.1267,  0.9971, -2.9082,  0.3612]],
       dtype=torch.float64)
	q_value: tensor([[-3.1106]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 5, step: 29
	action: tensor([[-1.9860, -0.8172,  1.2684, -1.2438,  1.1386, -1.3473,  0.4936]],
       dtype=torch.float64)
	q_value: tensor([[-3.1277]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 5, step: 30
	action: tensor([[ 0.3809,  0.1489,  1.0551, -0.1996, -0.3346, -0.0515,  1.5797]],
       dtype=torch.float64)
	q_value: tensor([[-3.1277]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5578263972191484, distance: 0.7609447049840854 entropy 1.6592931747436523
epoch: 5, step: 31
	action: tensor([[ 1.5244, -1.4668,  1.9069,  2.1556,  0.4051,  0.2381, -0.3262]],
       dtype=torch.float64)
	q_value: tensor([[-2.9487]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.28186732102211076, distance: 1.2956217538294827 entropy 1.6592931747436523
epoch: 5, step: 32
	action: tensor([[ 0.3970, -1.3647, -0.0605, -0.4348, -0.5656, -1.5248,  1.5025]],
       dtype=torch.float64)
	q_value: tensor([[-4.4261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 5, step: 33
	action: tensor([[-0.0728,  0.5713, -1.3294, -0.1518,  1.0900,  0.5470, -0.9754]],
       dtype=torch.float64)
	q_value: tensor([[-3.1277]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5816141660228243, distance: 0.7401933499566745 entropy 1.6592931747436523
epoch: 5, step: 34
	action: tensor([[-1.2253,  0.0966,  0.2759, -2.5856, -1.4310,  0.3666,  0.2547]],
       dtype=torch.float64)
	q_value: tensor([[-1.9391]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.597413217635552, distance: 1.446323124914396 entropy 1.6592931747436523
epoch: 5, step: 35
	action: tensor([[ 0.2203,  0.5230, -0.8893, -1.4178, -1.2592, -1.2405, -2.4330]],
       dtype=torch.float64)
	q_value: tensor([[-4.4356]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.838559349316211, distance: 0.4597938339986027 entropy 1.6592931747436523
epoch: 5, step: 36
	action: tensor([[ 2.6283, -2.2340,  1.3931, -0.4009,  2.8010,  0.3826,  0.1011]],
       dtype=torch.float64)
	q_value: tensor([[-3.3559]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 5, step: 37
	action: tensor([[-1.2902, -0.8445,  0.0223, -1.1442, -1.0272,  0.6288, -0.6860]],
       dtype=torch.float64)
	q_value: tensor([[-3.1277]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2352216196382444, distance: 1.7108699090962303 entropy 1.6592931747436523
epoch: 5, step: 38
	action: tensor([[-0.6563,  0.4438,  2.8236,  0.2680,  0.0999,  0.8287, -0.4943]],
       dtype=torch.float64)
	q_value: tensor([[-3.2008]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23151675573014696, distance: 1.269921441204021 entropy 1.6592931747436523
epoch: 5, step: 39
	action: tensor([[-0.8111, -0.6618, -1.3114, -2.3303, -1.8414, -2.5287, -0.0156]],
       dtype=torch.float64)
	q_value: tensor([[-3.6214]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2657033878552675, distance: 0.9806012793459932 entropy 1.6592931747436523
epoch: 5, step: 40
	action: tensor([[ 0.6456, -0.0122,  2.7063,  0.3758,  0.2897, -1.3654,  0.7465]],
       dtype=torch.float64)
	q_value: tensor([[-4.7294]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8689281714148279, distance: 0.4142966030847908 entropy 1.6592931747436523
epoch: 5, step: 41
	action: tensor([[ 0.9858,  0.6015, -0.9610, -2.2018,  1.1254, -1.8735, -0.8025]],
       dtype=torch.float64)
	q_value: tensor([[-4.0528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.48287210268016645, distance: 0.8229163229213102 entropy 1.6592931747436523
epoch: 5, step: 42
	action: tensor([[-1.8032, -1.5202, -0.6605, -1.0736,  0.6376, -1.2833,  0.2413]],
       dtype=torch.float64)
	q_value: tensor([[-3.2688]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.018173733834040307, distance: 1.1338980708984683 entropy 1.6592931747436523
epoch: 5, step: 43
	action: tensor([[-0.4655,  0.7656, -0.4260,  0.4432,  0.5635,  0.8942,  0.5789]],
       dtype=torch.float64)
	q_value: tensor([[-3.3505]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 5, step: 44
	action: tensor([[ 0.7918, -2.4056,  1.7105,  1.2808, -0.2277,  0.3368,  1.1176]],
       dtype=torch.float64)
	q_value: tensor([[-3.1277]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 5, step: 45
	action: tensor([[-0.8240, -0.5961,  2.1587,  0.7342,  0.5299,  0.0255, -0.4959]],
       dtype=torch.float64)
	q_value: tensor([[-3.1277]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6424335303643307, distance: 1.4665625630664971 entropy 1.6592931747436523
epoch: 5, step: 46
	action: tensor([[-2.3350, -0.0247,  2.8992, -0.3185, -1.0684, -0.2937,  0.2825]],
       dtype=torch.float64)
	q_value: tensor([[-3.2098]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 5, step: 47
	action: tensor([[-0.2461, -1.5916,  1.6094,  0.0419,  1.3469, -0.0308, -0.5681]],
       dtype=torch.float64)
	q_value: tensor([[-3.1277]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9484453287340924, distance: 1.5973526460248735 entropy 1.6592931747436523
epoch: 5, step: 48
	action: tensor([[ 0.5312, -1.2360, -0.7797,  1.7284, -2.5995,  1.0081,  3.0111]],
       dtype=torch.float64)
	q_value: tensor([[-2.8827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.014556317226981585, distance: 1.1526428827262214 entropy 1.6592931747436523
epoch: 5, step: 49
	action: tensor([[-0.6257,  0.2438, -0.7392, -1.3123,  0.9259,  1.3058,  0.3741]],
       dtype=torch.float64)
	q_value: tensor([[-5.9301]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37669207197408683, distance: 0.9034581057336467 entropy 1.6592931747436523
epoch: 5, step: 50
	action: tensor([[ 1.4326,  0.7479, -0.5829,  0.1419,  0.8749, -0.1028, -2.1277]],
       dtype=torch.float64)
	q_value: tensor([[-2.7847]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8549934477208151, distance: 0.43576315500103907 entropy 1.6592931747436523
epoch: 5, step: 51
	action: tensor([[-0.0870, -0.4889, -2.1830, -1.4254, -2.0758, -1.6109, -1.7971]],
       dtype=torch.float64)
	q_value: tensor([[-2.4559]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5447398978682878, distance: 0.7721229992890504 entropy 1.6592931747436523
epoch: 5, step: 52
	action: tensor([[ 0.6994,  0.9437, -1.0548,  1.2406,  0.4764, -0.3867,  0.6483]],
       dtype=torch.float64)
	q_value: tensor([[-4.3417]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.49740798227876437, distance: 0.8112682630634194 entropy 1.6592931747436523
epoch: 5, step: 53
	action: tensor([[ 0.6368,  1.0166, -0.8628, -1.0295,  0.1367, -0.4328, -0.7097]],
       dtype=torch.float64)
	q_value: tensor([[-2.2323]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7715745580898414, distance: 0.5469264541361448 entropy 1.6592931747436523
epoch: 5, step: 54
	action: tensor([[ 2.2271,  3.4696,  0.8085, -0.5250,  0.6539,  1.3586,  1.1448]],
       dtype=torch.float64)
	q_value: tensor([[-2.1943]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 5, step: 55
	action: tensor([[ 1.2326,  2.8473, -0.5210, -0.2055,  0.4687,  0.2696, -0.9621]],
       dtype=torch.float64)
	q_value: tensor([[-3.1277]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 5, step: 56
	action: tensor([[ 0.8439, -1.4489, -1.4521,  0.2731,  0.2979, -1.3626,  0.4938]],
       dtype=torch.float64)
	q_value: tensor([[-3.1277]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.45015623160547524, distance: 1.378047190016488 entropy 1.6592931747436523
epoch: 5, step: 57
	action: tensor([[ 0.8793, -0.1987,  1.4532,  0.2094, -3.1134, -1.2276, -1.0623]],
       dtype=torch.float64)
	q_value: tensor([[-3.0801]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6317146485820173, distance: 0.6944626861373061 entropy 1.6592931747436523
epoch: 5, step: 58
	action: tensor([[-1.6900,  1.4013,  1.1149,  0.3513,  0.2805,  0.4660,  0.0032]],
       dtype=torch.float64)
	q_value: tensor([[-4.9623]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 5, step: 59
	action: tensor([[ 0.1343,  1.9020,  0.9098,  3.6513, -1.2081, -0.3442, -1.3408]],
       dtype=torch.float64)
	q_value: tensor([[-3.1277]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 5, step: 60
	action: tensor([[-1.2482,  0.8018,  1.8600, -2.0231,  0.2610,  0.5060,  2.4861]],
       dtype=torch.float64)
	q_value: tensor([[-3.1277]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5574546183174582, distance: 1.4281189887305434 entropy 1.6592931747436523
epoch: 5, step: 61
	action: tensor([[ 0.0748,  1.8581,  1.6032, -0.8277,  1.1296,  2.1059,  2.4202]],
       dtype=torch.float64)
	q_value: tensor([[-5.2557]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 5, step: 62
	action: tensor([[ 0.2182, -0.7262, -2.6005, -1.0100, -0.6343, -0.0966, -0.2837]],
       dtype=torch.float64)
	q_value: tensor([[-3.1277]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6371884061600316, distance: 0.6892825304389436 entropy 1.6592931747436523
epoch: 5, step: 63
	action: tensor([[ 1.1747, -0.6028, -1.2730, -1.3452, -0.8669,  3.4004,  0.5049]],
       dtype=torch.float64)
	q_value: tensor([[-3.4311]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 5, step: 64
	action: tensor([[-1.2783, -1.8329,  1.5438, -1.2180,  0.7936, -0.8884,  1.5916]],
       dtype=torch.float64)
	q_value: tensor([[-3.1277]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 5, step: 65
	action: tensor([[ 0.9118,  0.9074,  1.2987,  0.5355,  1.9665, -0.8299, -0.4130]],
       dtype=torch.float64)
	q_value: tensor([[-3.1277]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 5, step: 66
	action: tensor([[ 1.4733, -0.6387,  1.8210,  1.7461,  2.0027, -0.4816,  0.2124]],
       dtype=torch.float64)
	q_value: tensor([[-3.1277]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2662837287536637, distance: 1.287722269929643 entropy 1.6592931747436523
epoch: 5, step: 67
	action: tensor([[-2.3302, -2.2706, -0.2982, -0.4820,  0.3641, -1.5369, -1.5640]],
       dtype=torch.float64)
	q_value: tensor([[-4.0389]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 5, step: 68
	action: tensor([[ 1.3257,  0.4350, -3.2434, -1.3041, -2.4766, -1.9610, -0.1635]],
       dtype=torch.float64)
	q_value: tensor([[-3.1277]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.23934151705287302 entropy 1.6592931747436523
epoch: 5, step: 69
	action: tensor([[ 1.0700,  1.3325, -1.5051, -1.0186,  1.6879,  0.0245,  0.0730]],
       dtype=torch.float64)
	q_value: tensor([[-3.1277]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44483100224898753, distance: 0.852647054674542 entropy 1.6592931747436523
epoch: 5, step: 70
	action: tensor([[-0.6474,  0.1336,  0.1289, -1.0872,  0.7138, -0.4653,  0.0203]],
       dtype=torch.float64)
	q_value: tensor([[-2.8052]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8535783898765237, distance: 1.5579810538590628 entropy 1.6592931747436523
epoch: 5, step: 71
	action: tensor([[-2.4270,  2.3653,  0.2771,  0.0914, -1.1423,  0.7590, -2.1947]],
       dtype=torch.float64)
	q_value: tensor([[-1.7766]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 5, step: 72
	action: tensor([[ 0.6863, -1.6627, -0.2885,  0.6532, -2.2334, -0.5126,  0.2924]],
       dtype=torch.float64)
	q_value: tensor([[-3.1277]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1510576434284333, distance: 1.0543766863421808 entropy 1.6592931747436523
epoch: 5, step: 73
	action: tensor([[ 0.4403,  2.4738, -1.7243, -1.6988, -1.6554, -2.5423,  0.4498]],
       dtype=torch.float64)
	q_value: tensor([[-3.5067]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 5, step: 74
	action: tensor([[ 1.9909, -0.0624, -0.0740, -1.1184,  1.0086,  2.6865,  0.4781]],
       dtype=torch.float64)
	q_value: tensor([[-3.1277]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28122286812608777, distance: 0.9701833541741 entropy 1.6592931747436523
epoch: 5, step: 75
	action: tensor([[-1.1856,  1.1185,  2.0590, -1.2481, -0.1557,  1.1004, -1.5191]],
       dtype=torch.float64)
	q_value: tensor([[-3.4647]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.142295069203763, distance: 1.2230545970542566 entropy 1.6592931747436523
epoch: 5, step: 76
	action: tensor([[-0.1230, -0.8395,  2.2403, -0.1999,  0.7698,  0.4690,  0.6640]],
       dtype=torch.float64)
	q_value: tensor([[-3.7823]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6319236744422974, distance: 1.4618627997255826 entropy 1.6592931747436523
epoch: 5, step: 77
	action: tensor([[-0.1943,  0.6305, -0.6810, -1.0433,  0.8354, -0.4921,  0.2276]],
       dtype=torch.float64)
	q_value: tensor([[-3.5952]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.48061824276062604, distance: 0.8247076801345833 entropy 1.6592931747436523
epoch: 5, step: 78
	action: tensor([[-0.6015,  1.8436,  0.7788,  1.6251,  0.8899, -0.9015,  0.4792]],
       dtype=torch.float64)
	q_value: tensor([[-1.8751]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 5, step: 79
	action: tensor([[ 0.0385,  0.1676,  0.3621, -0.2433,  0.4740, -1.5584,  2.4287]],
       dtype=torch.float64)
	q_value: tensor([[-3.1277]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.059023964941440465, distance: 1.110058776134943 entropy 1.6592931747436523
epoch: 5, step: 80
	action: tensor([[ 0.6737, -1.5781, -0.8394, -0.3596, -0.6937,  2.4671,  2.0036]],
       dtype=torch.float64)
	q_value: tensor([[-3.7764]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 5, step: 81
	action: tensor([[ 0.7057,  1.2840,  0.0780,  2.0743, -0.0328,  0.4438,  2.0733]],
       dtype=torch.float64)
	q_value: tensor([[-3.1277]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13195080506188317, distance: 1.2175042043382842 entropy 1.6592931747436523
epoch: 5, step: 82
	action: tensor([[ 2.7590,  0.2054, -0.6595,  1.0548, -0.8291, -1.7233, -0.1394]],
       dtype=torch.float64)
	q_value: tensor([[-3.5093]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 5, step: 83
	action: tensor([[-0.3103, -0.8045,  0.0877,  0.9778, -1.0538,  0.8010,  1.7174]],
       dtype=torch.float64)
	q_value: tensor([[-3.1277]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10214784768244778, distance: 1.2013695544782106 entropy 1.6592931747436523
epoch: 5, step: 84
	action: tensor([[ 0.6341, -0.0042,  0.6981,  1.0100, -0.8537,  0.4589,  0.8076]],
       dtype=torch.float64)
	q_value: tensor([[-3.6380]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9914730972129018, distance: 0.10567015613863283 entropy 1.6592931747436523
epoch: 5, step: 85
	action: tensor([[-0.5019, -0.5482, -0.1223,  0.2818,  1.1530,  1.4752,  1.5942]],
       dtype=torch.float64)
	q_value: tensor([[-2.7355]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17551915568039667, distance: 1.0390751969502257 entropy 1.6592931747436523
epoch: 5, step: 86
	action: tensor([[-0.0613, -2.0750,  0.0064, -1.6044, -0.1226,  0.7204, -0.2275]],
       dtype=torch.float64)
	q_value: tensor([[-3.4053]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 5, step: 87
	action: tensor([[-0.4984, -0.8590,  1.1010, -1.1706,  0.3335, -0.6623,  0.5748]],
       dtype=torch.float64)
	q_value: tensor([[-3.1277]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0655896587099347, distance: 1.6446698029031674 entropy 1.6592931747436523
epoch: 5, step: 88
	action: tensor([[ 1.6141, -2.2351,  0.4606, -3.5199, -0.5403, -1.8451,  0.5391]],
       dtype=torch.float64)
	q_value: tensor([[-2.9178]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 5, step: 89
	action: tensor([[-0.4952,  0.8345,  1.2952,  0.8590,  0.3388, -1.0162, -0.2533]],
       dtype=torch.float64)
	q_value: tensor([[-3.1277]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1796804710950708, distance: 1.0364496724203485 entropy 1.6592931747436523
epoch: 5, step: 90
	action: tensor([[-0.6780,  1.5630, -0.6167, -1.1613, -0.3549,  0.7347,  0.9070]],
       dtype=torch.float64)
	q_value: tensor([[-2.1957]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4484984438247285, distance: 0.849826098634629 entropy 1.6592931747436523
epoch: 5, step: 91
	action: tensor([[-2.2535, -0.7727,  0.6370,  0.2677,  0.1367,  0.3533, -2.0092]],
       dtype=torch.float64)
	q_value: tensor([[-3.3996]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 5, step: 92
	action: tensor([[ 1.3017,  1.2823,  2.6248, -2.5408,  1.7779, -0.7141,  0.3349]],
       dtype=torch.float64)
	q_value: tensor([[-3.1277]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.059568878386880186, distance: 1.1779349022597672 entropy 1.6592931747436523
epoch: 5, step: 93
	action: tensor([[-2.8968, -1.0115,  0.2808, -2.1447,  0.1748, -2.8357, -0.6916]],
       dtype=torch.float64)
	q_value: tensor([[-3.9321]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 5, step: 94
	action: tensor([[-3.8262,  1.1691,  0.0446, -2.3641,  0.9361, -0.1106, -1.2013]],
       dtype=torch.float64)
	q_value: tensor([[-3.1277]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 5, step: 95
	action: tensor([[-0.7403,  0.5235,  0.2874, -1.5430, -0.0758,  1.9322,  0.5009]],
       dtype=torch.float64)
	q_value: tensor([[-3.1277]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20591786245842814, distance: 1.2566535352125192 entropy 1.6592931747436523
epoch: 5, step: 96
	action: tensor([[ 0.0033,  0.7414, -1.0391,  0.5193, -0.3692, -2.1611,  0.0365]],
       dtype=torch.float64)
	q_value: tensor([[-3.8254]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8176250026072724, distance: 0.4886966282293351 entropy 1.6592931747436523
epoch: 5, step: 97
	action: tensor([[-2.3023, -0.2139,  0.9711, -0.6810, -1.3053,  0.9823,  0.6721]],
       dtype=torch.float64)
	q_value: tensor([[-2.7278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 5, step: 98
	action: tensor([[ 1.9970, -4.2876, -0.8357, -2.1665,  1.2656, -1.7398,  2.6187]],
       dtype=torch.float64)
	q_value: tensor([[-3.1277]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 5, step: 99
	action: tensor([[ 1.0908, -2.0357,  0.7598, -0.2786,  0.2311, -0.6849,  0.8025]],
       dtype=torch.float64)
	q_value: tensor([[-3.1277]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 5, step: 100
	action: tensor([[-1.7511,  0.9553, -1.8026,  2.6692,  1.3791,  2.4604,  1.0273]],
       dtype=torch.float64)
	q_value: tensor([[-3.1277]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 5, step: 101
	action: tensor([[ 1.3088,  0.5172,  0.8818, -0.6880, -2.3773, -2.3046, -0.1614]],
       dtype=torch.float64)
	q_value: tensor([[-3.1277]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7545180176735387, distance: 0.5669783870408223 entropy 1.6592931747436523
epoch: 5, step: 102
	action: tensor([[-0.2132,  0.1065, -0.8575, -0.0611, -0.8390, -0.0610, -0.7193]],
       dtype=torch.float64)
	q_value: tensor([[-4.5827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1841865665316309, distance: 1.0335990926702145 entropy 1.6592931747436523
epoch: 5, step: 103
	action: tensor([[-0.2108,  2.3364, -0.6298,  1.0972, -0.4233,  1.4385,  0.1608]],
       dtype=torch.float64)
	q_value: tensor([[-1.8910]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 5, step: 104
	action: tensor([[ 2.7655, -0.2093,  0.2775, -1.9486,  0.3954, -0.3453,  0.9038]],
       dtype=torch.float64)
	q_value: tensor([[-3.1277]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 5, step: 105
	action: tensor([[ 1.9918, -1.3489, -1.2164,  1.5258, -0.3388, -1.6880, -0.5843]],
       dtype=torch.float64)
	q_value: tensor([[-3.1277]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10493653474493692, distance: 1.0826388524796255 entropy 1.6592931747436523
epoch: 5, step: 106
	action: tensor([[-0.8894, -0.7313, -0.3150,  0.6331, -0.5414, -1.7599,  0.0180]],
       dtype=torch.float64)
	q_value: tensor([[-3.6361]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5344028290746066, distance: 1.41751083945763 entropy 1.6592931747436523
epoch: 5, step: 107
	action: tensor([[-1.2303, -1.6429, -0.0103,  2.7523,  0.1137,  2.7830, -0.1597]],
       dtype=torch.float64)
	q_value: tensor([[-2.9423]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 5, step: 108
	action: tensor([[-1.0795,  1.4592,  0.2790, -2.2457, -2.5159,  2.2520, -1.1259]],
       dtype=torch.float64)
	q_value: tensor([[-3.1277]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15626575868411263, distance: 1.0511375009812207 entropy 1.6592931747436523
epoch: 5, step: 109
	action: tensor([[ 0.3786,  0.3036,  1.7395, -2.2807, -0.7578,  1.1111, -1.0219]],
       dtype=torch.float64)
	q_value: tensor([[-6.6222]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24775508826878334, distance: 0.9925132714600975 entropy 1.6592931747436523
epoch: 5, step: 110
	action: tensor([[-0.3436,  0.8829, -0.3295, -2.8726, -0.1321, -1.6975,  2.3044]],
       dtype=torch.float64)
	q_value: tensor([[-4.4367]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 5, step: 111
	action: tensor([[-0.3556, -0.5304, -0.6574, -0.7799,  1.0945, -0.9441, -0.3339]],
       dtype=torch.float64)
	q_value: tensor([[-3.1277]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.49452393107416937, distance: 1.3989691258425203 entropy 1.6592931747436523
epoch: 5, step: 112
	action: tensor([[ 2.6311, -1.7635, -1.2477, -0.8084,  1.1865, -0.6121, -0.0785]],
       dtype=torch.float64)
	q_value: tensor([[-1.9925]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 5, step: 113
	action: tensor([[ 1.1642, -0.1293,  1.4789,  0.0654,  0.9922, -1.4952, -0.4816]],
       dtype=torch.float64)
	q_value: tensor([[-3.1277]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8963326787292673, distance: 0.36844921162709915 entropy 1.6592931747436523
epoch: 5, step: 114
	action: tensor([[ 0.5180,  1.0214,  1.5545,  1.3347, -0.8558, -0.7362,  1.5617]],
       dtype=torch.float64)
	q_value: tensor([[-2.8723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9350082129693387, distance: 0.2917332515835282 entropy 1.6592931747436523
epoch: 5, step: 115
	action: tensor([[ 2.1264,  1.0334, -0.9567,  0.9328,  2.0676,  2.0317, -0.3582]],
       dtype=torch.float64)
	q_value: tensor([[-3.5915]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 5, step: 116
	action: tensor([[ 1.8325,  1.8074,  1.3632,  0.4762,  0.4570,  2.1134, -1.3892]],
       dtype=torch.float64)
	q_value: tensor([[-3.1277]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 5, step: 117
	action: tensor([[ 1.1334, -0.7781,  1.2017, -0.6146,  0.7152, -1.7758,  0.7340]],
       dtype=torch.float64)
	q_value: tensor([[-3.1277]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.45151893423469114, distance: 0.8474957191199426 entropy 1.6592931747436523
epoch: 5, step: 118
	action: tensor([[-0.2528,  1.6685,  0.1306, -1.1026, -2.8159, -0.7892,  0.1377]],
       dtype=torch.float64)
	q_value: tensor([[-3.5373]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44816591576246334, distance: 0.8500822615034591 entropy 1.6592931747436523
epoch: 5, step: 119
	action: tensor([[ 0.1552,  0.7428,  0.9648, -1.4148, -1.7889,  2.5878,  1.0983]],
       dtype=torch.float64)
	q_value: tensor([[-4.9857]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33035419004087496, distance: 0.9364385226675989 entropy 1.6592931747436523
epoch: 5, step: 120
	action: tensor([[-1.7593, -1.1912, -0.5309, -1.3976,  0.4490, -0.2819,  1.0927]],
       dtype=torch.float64)
	q_value: tensor([[-5.5547]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.028718104825122603, distance: 1.1606596454639142 entropy 1.6592931747436523
epoch: 5, step: 121
	action: tensor([[-2.1784,  0.3334,  0.8000,  1.0074,  0.4126,  0.3620,  0.3446]],
       dtype=torch.float64)
	q_value: tensor([[-3.6748]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 5, step: 122
	action: tensor([[-1.8172,  1.1555,  0.2673, -0.9251, -2.3431,  1.0788,  1.6169]],
       dtype=torch.float64)
	q_value: tensor([[-3.1277]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
epoch: 5, step: 123
	action: tensor([[-0.2935, -0.5579, -0.4057, -0.4571, -1.3188, -0.0937,  0.2614]],
       dtype=torch.float64)
	q_value: tensor([[-3.1277]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.51364919088427, distance: 1.4078918983434558 entropy 1.6592931747436523
epoch: 5, step: 124
	action: tensor([[-0.3301, -0.3289, -0.8241,  1.0887,  0.7381,  0.9018,  0.2913]],
       dtype=torch.float64)
	q_value: tensor([[-2.5661]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.009922779128646697, distance: 1.138652561808427 entropy 1.6592931747436523
epoch: 5, step: 125
	action: tensor([[ 0.9115, -0.2711, -1.4452, -0.6029,  3.5694, -0.2755, -0.5638]],
       dtype=torch.float64)
	q_value: tensor([[-2.1964]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7952756240338519 entropy 1.6592931747436523
epoch: 5, step: 126
	action: tensor([[ 1.9049, -0.9172,  2.5709, -0.9030,  0.7436, -0.8361, -0.0931]],
       dtype=torch.float64)
	q_value: tensor([[-3.1277]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5373430631844509, distance: 0.778370256937316 entropy 1.6592931747436523
epoch: 5, step: 127
	action: tensor([[ 3.1967,  0.9000, -0.4876, -0.2116,  0.1544,  0.7728,  0.4894]],
       dtype=torch.float64)
	q_value: tensor([[-4.1228]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.6592931747436523
LOSS epoch 5 actor 355.80309624609134 critic 1854.6568849063447 
epoch: 6, step: 0
	action: tensor([[-0.2874,  1.6476,  0.3912,  0.6806,  0.9415,  1.3329,  1.2136]],
       dtype=torch.float64)
	q_value: tensor([[-5.5910]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 6, step: 1
	action: tensor([[ 0.0558,  0.5566,  0.7420, -0.8884, -1.5284, -0.4695,  1.7506]],
       dtype=torch.float64)
	q_value: tensor([[-5.5910]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20361223793444427, distance: 1.0212192251843852 entropy 1.553932785987854
epoch: 6, step: 2
	action: tensor([[-1.0140, -0.9173, -1.8325, -1.5296,  1.6685,  0.7999,  0.1294]],
       dtype=torch.float64)
	q_value: tensor([[-6.8515]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1423939533803844, distance: 1.0597431315041188 entropy 1.553932785987854
epoch: 6, step: 3
	action: tensor([[-0.2067, -1.3956, -0.2111,  0.1256, -0.0736, -0.8127,  0.7715]],
       dtype=torch.float64)
	q_value: tensor([[-6.2527]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 6, step: 4
	action: tensor([[ 2.6170,  1.8685, -0.5271,  1.1988,  0.1889, -0.0907,  0.5115]],
       dtype=torch.float64)
	q_value: tensor([[-5.5910]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 6, step: 5
	action: tensor([[ 2.0859,  0.1268,  0.6811, -0.5845, -0.0966, -2.4165, -0.5267]],
       dtype=torch.float64)
	q_value: tensor([[-5.5910]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4834550278924201, distance: 0.8224523816658216 entropy 1.553932785987854
epoch: 6, step: 6
	action: tensor([[0.0647, 0.1535, 0.1292, 0.6722, 0.3586, 0.3610, 0.0140]],
       dtype=torch.float64)
	q_value: tensor([[-5.6187]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8094752048180573, distance: 0.49949649486452163 entropy 1.553932785987854
epoch: 6, step: 7
	action: tensor([[ 0.2223, -1.8886, -2.0861, -0.7605, -0.3987,  0.9618, -0.8470]],
       dtype=torch.float64)
	q_value: tensor([[-2.4406]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 6, step: 8
	action: tensor([[ 1.0922,  1.1247, -1.0127,  0.7583, -1.5841, -1.3910, -1.8849]],
       dtype=torch.float64)
	q_value: tensor([[-5.5910]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 6, step: 9
	action: tensor([[ 0.0500, -0.0103,  0.6557,  1.1930,  2.0767, -0.6368, -0.3472]],
       dtype=torch.float64)
	q_value: tensor([[-5.5910]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8763968265968967, distance: 0.4023198899188618 entropy 1.553932785987854
epoch: 6, step: 10
	action: tensor([[-0.6178, -0.2181,  0.2734, -0.5804,  0.4021,  1.5083,  0.9624]],
       dtype=torch.float64)
	q_value: tensor([[-4.4385]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21288490119542947, distance: 1.2602783860744107 entropy 1.553932785987854
epoch: 6, step: 11
	action: tensor([[-0.5866,  3.5520,  1.1183,  0.3748,  1.4891,  1.6998, -0.7529]],
       dtype=torch.float64)
	q_value: tensor([[-5.0994]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 6, step: 12
	action: tensor([[-1.0362, -3.8124,  0.6971, -0.8737, -0.0302, -0.1671,  0.9989]],
       dtype=torch.float64)
	q_value: tensor([[-5.5910]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 6, step: 13
	action: tensor([[ 0.8478, -0.2491,  0.7143, -0.6420, -1.3118,  1.1740,  0.2200]],
       dtype=torch.float64)
	q_value: tensor([[-5.5910]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5592462505440658, distance: 0.7597219967606985 entropy 1.553932785987854
epoch: 6, step: 14
	action: tensor([[-0.8785,  0.7024, -0.1525, -0.3116, -2.1112,  0.2838,  0.7200]],
       dtype=torch.float64)
	q_value: tensor([[-5.8863]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5170336618699201, distance: 1.4094650200098446 entropy 1.553932785987854
epoch: 6, step: 15
	action: tensor([[ 1.3999, -0.3968, -0.5860, -0.5222,  0.5042, -0.6961, -0.1233]],
       dtype=torch.float64)
	q_value: tensor([[-6.6556]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.288871348970263, distance: 1.2991565225528756 entropy 1.553932785987854
epoch: 6, step: 16
	action: tensor([[ 0.0072,  0.7212,  0.5243,  0.3812, -2.7053,  0.1905,  0.5510]],
       dtype=torch.float64)
	q_value: tensor([[-3.6404]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.685957409278996, distance: 0.6412848336454959 entropy 1.553932785987854
epoch: 6, step: 17
	action: tensor([[-1.6235,  0.3898,  0.6841,  0.4997,  0.4672,  1.0619,  0.4593]],
       dtype=torch.float64)
	q_value: tensor([[-7.4050]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2875357372449452, distance: 1.2984832131327941 entropy 1.553932785987854
epoch: 6, step: 18
	action: tensor([[ 8.8093e-01,  3.3681e-01,  3.3349e-01, -4.5104e-02,  1.2447e-01,
          1.0379e+00, -3.8098e-04]], dtype=torch.float64)
	q_value: tensor([[-4.7684]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9860173338331181, distance: 0.13531679029875499 entropy 1.553932785987854
epoch: 6, step: 19
	action: tensor([[ 0.3139,  1.0470,  1.7718,  0.5387, -0.7241,  0.3049, -0.3622]],
       dtype=torch.float64)
	q_value: tensor([[-3.4253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9350127931523982, distance: 0.291722971705869 entropy 1.553932785987854
epoch: 6, step: 20
	action: tensor([[ 0.2826, -1.2181, -0.1923,  0.8505, -0.5624, -0.7132, -0.3261]],
       dtype=torch.float64)
	q_value: tensor([[-4.9845]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1597845211151916, distance: 1.2323820026302392 entropy 1.553932785987854
epoch: 6, step: 21
	action: tensor([[-0.1034,  1.0471,  2.3844, -0.3481,  0.7120, -1.2008,  2.1941]],
       dtype=torch.float64)
	q_value: tensor([[-4.0650]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37461886221111296, distance: 0.9049593727256969 entropy 1.553932785987854
epoch: 6, step: 22
	action: tensor([[-0.5646, -0.9009,  1.1794,  0.4639, -0.7975, -1.2197, -0.0024]],
       dtype=torch.float64)
	q_value: tensor([[-7.0057]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5609543228616931, distance: 1.4297226277384092 entropy 1.553932785987854
epoch: 6, step: 23
	action: tensor([[ 1.9938, -1.7633,  0.2086, -0.9382, -0.4641, -0.8040,  0.4687]],
       dtype=torch.float64)
	q_value: tensor([[-5.2836]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 6, step: 24
	action: tensor([[ 1.5704,  1.1468,  0.1737, -0.9222,  0.3573,  0.0380,  2.0180]],
       dtype=torch.float64)
	q_value: tensor([[-5.5910]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.802202584677296, distance: 0.5089404850653403 entropy 1.553932785987854
epoch: 6, step: 25
	action: tensor([[-0.4428, -0.1065,  0.7248,  1.4539, -1.2115,  0.7640, -0.7642]],
       dtype=torch.float64)
	q_value: tensor([[-6.1048]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6681238461128143, distance: 0.6592417711001759 entropy 1.553932785987854
epoch: 6, step: 26
	action: tensor([[-1.4928, -0.8839,  1.5483, -0.8720, -1.6301, -0.5639,  0.3734]],
       dtype=torch.float64)
	q_value: tensor([[-4.7451]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.6528991280808687, distance: 1.8638761954715384 entropy 1.553932785987854
epoch: 6, step: 27
	action: tensor([[ 0.3602,  0.1985,  1.3068, -0.9298, -0.9221,  0.6308, -2.3972]],
       dtype=torch.float64)
	q_value: tensor([[-7.3766]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38596852733915665, distance: 0.8967099921370906 entropy 1.553932785987854
epoch: 6, step: 28
	action: tensor([[-1.0355, -0.6764, -1.2469, -1.1602, -1.0544, -1.2749,  0.4995]],
       dtype=torch.float64)
	q_value: tensor([[-5.9790]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19003949400741404, distance: 1.0298847201047963 entropy 1.553932785987854
epoch: 6, step: 29
	action: tensor([[-0.5502, -0.3430,  1.1730,  1.8335, -0.0923,  2.6830, -1.6024]],
       dtype=torch.float64)
	q_value: tensor([[-6.0955]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4265909611969916, distance: 0.8665406898691289 entropy 1.553932785987854
epoch: 6, step: 30
	action: tensor([[-0.2906, -2.4632, -1.2101, -1.9142, -0.6972,  0.9630, -0.5742]],
       dtype=torch.float64)
	q_value: tensor([[-6.1533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 6, step: 31
	action: tensor([[-0.7312, -2.6530,  0.2033, -0.7588,  0.4971, -0.4092,  1.2779]],
       dtype=torch.float64)
	q_value: tensor([[-5.5910]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 6, step: 32
	action: tensor([[ 2.5413, -0.2554,  0.5477, -0.1462,  1.3915, -0.4063, -0.2620]],
       dtype=torch.float64)
	q_value: tensor([[-5.5910]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 6, step: 33
	action: tensor([[ 1.8326,  0.3778, -1.7864,  1.4424,  0.8057,  0.1964,  0.2944]],
       dtype=torch.float64)
	q_value: tensor([[-5.5910]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8645630953642135, distance: 0.42113874932258644 entropy 1.553932785987854
epoch: 6, step: 34
	action: tensor([[ 2.1189,  0.4681, -2.0552, -2.7721,  0.7819,  0.4218,  1.4330]],
       dtype=torch.float64)
	q_value: tensor([[-4.9954]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 6, step: 35
	action: tensor([[-1.3156,  1.2996,  1.5832, -1.1499,  1.3474,  0.1665,  0.9937]],
       dtype=torch.float64)
	q_value: tensor([[-5.5910]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6415404658941446, distance: 1.4661637910666179 entropy 1.553932785987854
epoch: 6, step: 36
	action: tensor([[-1.4372, -1.1665, -2.3922,  0.0099,  1.4063, -0.1856, -0.7331]],
       dtype=torch.float64)
	q_value: tensor([[-5.5618]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.5995355521816164, distance: 1.8450348508536234 entropy 1.553932785987854
epoch: 6, step: 37
	action: tensor([[-2.0078,  0.0611,  1.5444, -0.5948, -1.0370,  0.7540,  1.4640]],
       dtype=torch.float64)
	q_value: tensor([[-6.1569]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 6, step: 38
	action: tensor([[-0.5516,  1.6017,  0.2443, -1.0864,  0.9713,  2.6418, -1.2352]],
       dtype=torch.float64)
	q_value: tensor([[-5.5910]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6785399777877179, distance: 0.6488139492383317 entropy 1.553932785987854
epoch: 6, step: 39
	action: tensor([[ 0.9317,  0.5744,  0.2640, -0.2641,  2.1795, -2.0829, -1.7725]],
       dtype=torch.float64)
	q_value: tensor([[-6.0237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9110265133191602, distance: 0.34133985389386456 entropy 1.553932785987854
epoch: 6, step: 40
	action: tensor([[-1.6822,  0.7638,  0.3061, -1.2220, -0.6034, -1.6433,  1.0409]],
       dtype=torch.float64)
	q_value: tensor([[-5.2415]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6009575884211085, distance: 1.4479268004500148 entropy 1.553932785987854
epoch: 6, step: 41
	action: tensor([[-0.4552,  0.4677,  0.1702,  0.3917,  1.0803,  1.4305, -1.1108]],
       dtype=torch.float64)
	q_value: tensor([[-6.4581]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6863378342230673, distance: 0.6408962961011215 entropy 1.553932785987854
epoch: 6, step: 42
	action: tensor([[ 1.9998,  1.9287, -0.3463,  0.1782,  0.9902,  2.1912, -0.4350]],
       dtype=torch.float64)
	q_value: tensor([[-3.6084]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 6, step: 43
	action: tensor([[ 0.0243, -1.4762,  0.7059,  0.3334, -1.2158, -0.9762, -1.1048]],
       dtype=torch.float64)
	q_value: tensor([[-5.5910]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4876495838645487, distance: 1.3957480051839313 entropy 1.553932785987854
epoch: 6, step: 44
	action: tensor([[-1.7741, -1.7613,  0.2125, -1.1199, -1.2557,  2.0358,  0.8782]],
       dtype=torch.float64)
	q_value: tensor([[-5.0094]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 6, step: 45
	action: tensor([[ 1.2522,  0.7182,  0.2111, -0.9843, -1.2519, -0.5013, -0.6684]],
       dtype=torch.float64)
	q_value: tensor([[-5.5910]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7316943749584687, distance: 0.5927500266239855 entropy 1.553932785987854
epoch: 6, step: 46
	action: tensor([[-1.0841, -0.0443, -2.0313, -1.4709,  1.3500, -0.4968, -0.5085]],
       dtype=torch.float64)
	q_value: tensor([[-5.1372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3744187312970866, distance: 0.9051041611181826 entropy 1.553932785987854
epoch: 6, step: 47
	action: tensor([[ 2.0955, -0.2060,  1.2205, -1.2627,  0.4721,  1.9424,  1.3565]],
       dtype=torch.float64)
	q_value: tensor([[-5.4874]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2969809302589921, distance: 0.9594895433292273 entropy 1.553932785987854
epoch: 6, step: 48
	action: tensor([[-2.1169, -0.4345,  0.7967, -0.0572,  0.0365, -0.7308,  0.9494]],
       dtype=torch.float64)
	q_value: tensor([[-7.3124]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.8787780351474517, distance: 1.9416045607606 entropy 1.553932785987854
epoch: 6, step: 49
	action: tensor([[ 0.1999,  1.7583,  3.3028,  0.9573, -2.3177, -0.3098, -1.3153]],
       dtype=torch.float64)
	q_value: tensor([[-5.6143]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 6, step: 50
	action: tensor([[ 1.3355,  1.9755,  1.1300, -0.4639, -1.2230, -1.2922,  0.5343]],
       dtype=torch.float64)
	q_value: tensor([[-5.5910]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 6, step: 51
	action: tensor([[-0.2345,  1.1113,  0.3440, -1.1379, -0.0737, -1.2590,  0.3513]],
       dtype=torch.float64)
	q_value: tensor([[-5.5910]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19436826532728801, distance: 1.0271289635483303 entropy 1.553932785987854
epoch: 6, step: 52
	action: tensor([[ 1.5941, -1.7642,  0.0438, -0.5046, -0.9937,  0.8170, -0.1303]],
       dtype=torch.float64)
	q_value: tensor([[-4.2647]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.605052061026986, distance: 1.4497771651778888 entropy 1.553932785987854
epoch: 6, step: 53
	action: tensor([[-1.0107, -1.7185,  2.3805, -0.2411, -1.2062, -0.3201,  0.0844]],
       dtype=torch.float64)
	q_value: tensor([[-5.1865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 6, step: 54
	action: tensor([[ 0.7328, -1.2077,  0.6114, -0.5624,  2.0974,  1.4006,  0.3920]],
       dtype=torch.float64)
	q_value: tensor([[-5.5910]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.664191628162049, distance: 1.4762447282481486 entropy 1.553932785987854
epoch: 6, step: 55
	action: tensor([[-0.4359, -0.1545, -0.9985,  0.1580, -0.2544, -0.3565,  0.3579]],
       dtype=torch.float64)
	q_value: tensor([[-5.2613]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3026066859874148, distance: 1.3060606495976523 entropy 1.553932785987854
epoch: 6, step: 56
	action: tensor([[ 0.4142,  0.5870,  0.4418, -0.1936, -0.5636, -0.7645, -0.6761]],
       dtype=torch.float64)
	q_value: tensor([[-3.3186]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.792090428188231, distance: 0.5217878176459746 entropy 1.553932785987854
epoch: 6, step: 57
	action: tensor([[ 0.7819, -0.4476, -0.7819,  0.5484, -0.7298,  0.1463,  0.6459]],
       dtype=torch.float64)
	q_value: tensor([[-3.2832]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5710392776185073, distance: 0.7494893348595554 entropy 1.553932785987854
epoch: 6, step: 58
	action: tensor([[ 0.1179, -0.6888, -1.0518,  0.2193,  2.3265, -0.1721,  0.6526]],
       dtype=torch.float64)
	q_value: tensor([[-3.6427]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.329934245777024, distance: 1.3196895504600585 entropy 1.553932785987854
epoch: 6, step: 59
	action: tensor([[-1.1579,  0.9417,  0.8797,  0.3729,  0.1082, -0.1336,  2.0859]],
       dtype=torch.float64)
	q_value: tensor([[-5.3015]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 6, step: 60
	action: tensor([[ 0.1359,  0.7780, -0.6093,  0.1616,  1.2548, -1.3102, -0.5428]],
       dtype=torch.float64)
	q_value: tensor([[-5.5910]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5315227358771735, distance: 0.783250990329665 entropy 1.553932785987854
epoch: 6, step: 61
	action: tensor([[ 0.9939,  0.6782,  0.2174,  0.5382,  0.6813,  0.4348, -1.6352]],
       dtype=torch.float64)
	q_value: tensor([[-3.4941]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 6, step: 62
	action: tensor([[ 0.1181,  0.5010, -0.3183,  0.6693, -0.2403, -1.8108, -0.8612]],
       dtype=torch.float64)
	q_value: tensor([[-5.5910]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7887216459600965, distance: 0.5259981245717956 entropy 1.553932785987854
epoch: 6, step: 63
	action: tensor([[ 0.7876, -0.5978,  2.0378,  0.9517,  1.6782, -0.0862, -0.1401]],
       dtype=torch.float64)
	q_value: tensor([[-4.1496]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.029296304530757933, distance: 1.1609857789257012 entropy 1.553932785987854
epoch: 6, step: 64
	action: tensor([[ 1.1622,  0.9051,  1.6425,  0.5028, -0.3761,  0.1696,  0.3139]],
       dtype=torch.float64)
	q_value: tensor([[-5.6679]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7148373532522667, distance: 0.6110869534742522 entropy 1.553932785987854
epoch: 6, step: 65
	action: tensor([[-0.8695,  0.8116,  0.0676,  0.4565, -0.9879, -1.2639, -0.4826]],
       dtype=torch.float64)
	q_value: tensor([[-5.0586]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29585584134536014, distance: 0.9602570043669375 entropy 1.553932785987854
epoch: 6, step: 66
	action: tensor([[-0.7719,  0.5972,  1.2582,  0.8946, -1.4454,  0.9244, -0.5520]],
       dtype=torch.float64)
	q_value: tensor([[-4.1905]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5621912927094938, distance: 0.7571795755551557 entropy 1.553932785987854
epoch: 6, step: 67
	action: tensor([[ 1.6190,  0.7193, -0.4722, -0.5162, -1.5784, -1.6146, -1.0796]],
       dtype=torch.float64)
	q_value: tensor([[-5.7694]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3914613402853506, distance: 0.8926902267148233 entropy 1.553932785987854
epoch: 6, step: 68
	action: tensor([[ 1.2913,  1.1947, -0.4116,  0.3899, -1.0269, -0.7419, -2.0699]],
       dtype=torch.float64)
	q_value: tensor([[-5.9194]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 6, step: 69
	action: tensor([[-1.1143, -1.8156,  0.6478, -0.9026, -0.2847, -0.5216, -1.5787]],
       dtype=torch.float64)
	q_value: tensor([[-5.5910]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 6, step: 70
	action: tensor([[ 0.0196,  0.9385, -1.8718, -0.9144,  0.8278, -1.5182, -0.4821]],
       dtype=torch.float64)
	q_value: tensor([[-5.5910]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9208532560544441, distance: 0.3219387249872489 entropy 1.553932785987854
epoch: 6, step: 71
	action: tensor([[-0.2533,  0.0553,  1.3205,  0.4805,  1.2608, -0.5875,  0.4811]],
       dtype=torch.float64)
	q_value: tensor([[-5.0084]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4245413798639214, distance: 0.8680879809538188 entropy 1.553932785987854
epoch: 6, step: 72
	action: tensor([[ 0.2523, -2.0688, -1.5469, -0.7251, -0.1237,  0.6133, -0.8506]],
       dtype=torch.float64)
	q_value: tensor([[-4.2016]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 6, step: 73
	action: tensor([[-0.4059, -0.0116,  0.0289, -0.2612,  1.2703,  1.4162, -0.9292]],
       dtype=torch.float64)
	q_value: tensor([[-5.5910]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.291994208754205, distance: 0.9628865011574885 entropy 1.553932785987854
epoch: 6, step: 74
	action: tensor([[ 2.4012,  0.2707,  1.4868,  0.2630,  0.8609, -0.5287, -1.7790]],
       dtype=torch.float64)
	q_value: tensor([[-3.3935]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 6, step: 75
	action: tensor([[-1.8576,  0.7645, -1.9053,  0.3523, -0.3855, -1.2664, -1.1506]],
       dtype=torch.float64)
	q_value: tensor([[-5.5910]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 6, step: 76
	action: tensor([[-0.6923,  0.5993,  0.1163,  1.7743, -1.4646, -1.8679, -0.2432]],
       dtype=torch.float64)
	q_value: tensor([[-5.5910]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7170152285884761, distance: 0.6087489508672345 entropy 1.553932785987854
epoch: 6, step: 77
	action: tensor([[ 0.8930, -1.0751, -1.3546,  1.4644,  0.4322, -0.3566,  0.1521]],
       dtype=torch.float64)
	q_value: tensor([[-5.7210]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.020732155398148766, distance: 1.1324197637497913 entropy 1.553932785987854
epoch: 6, step: 78
	action: tensor([[ 1.8932,  0.8065,  0.4258,  0.1713,  1.6617, -2.4612, -1.6793]],
       dtype=torch.float64)
	q_value: tensor([[-4.8734]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 6, step: 79
	action: tensor([[-0.5514,  0.2837, -0.0678, -0.1830, -0.4507, -0.3557,  1.3177]],
       dtype=torch.float64)
	q_value: tensor([[-5.5910]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.30948517162079003, distance: 1.3095044714995774 entropy 1.553932785987854
epoch: 6, step: 80
	action: tensor([[ 1.8980, -2.6849,  0.2114, -0.7787,  1.2607,  0.7580,  0.7020]],
       dtype=torch.float64)
	q_value: tensor([[-4.2426]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 6, step: 81
	action: tensor([[-0.7746,  0.6885, -0.6058,  1.5801,  0.0831, -0.0854,  1.7038]],
       dtype=torch.float64)
	q_value: tensor([[-5.5910]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10317392255856395, distance: 1.2019286484101102 entropy 1.553932785987854
epoch: 6, step: 82
	action: tensor([[ 0.5576,  0.9305,  1.0160,  0.8287,  0.0337, -0.8832,  2.7273]],
       dtype=torch.float64)
	q_value: tensor([[-5.1711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 6, step: 83
	action: tensor([[ 2.7625,  1.5541,  0.9298,  2.3573, -0.4181, -0.5101, -1.4217]],
       dtype=torch.float64)
	q_value: tensor([[-5.5910]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 6, step: 84
	action: tensor([[-0.2974,  0.1681, -0.5350,  1.8018,  0.7795, -1.8764,  1.1334]],
       dtype=torch.float64)
	q_value: tensor([[-5.5910]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.021912673710067776, distance: 1.1317369855840145 entropy 1.553932785987854
epoch: 6, step: 85
	action: tensor([[ 1.6983, -0.9605, -1.8524,  0.3981,  0.0022,  1.0049, -0.5396]],
       dtype=torch.float64)
	q_value: tensor([[-5.8128]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.01059249137701923, distance: 1.1503890172120719 entropy 1.553932785987854
epoch: 6, step: 86
	action: tensor([[-0.7738, -0.3949, -1.2941, -2.1982, -1.3587,  0.0840,  0.2217]],
       dtype=torch.float64)
	q_value: tensor([[-4.6735]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5924217157719442, distance: 0.7305706345112586 entropy 1.553932785987854
epoch: 6, step: 87
	action: tensor([[ 1.1569,  1.6945,  1.0007, -1.4385, -0.2475, -0.5101, -1.3044]],
       dtype=torch.float64)
	q_value: tensor([[-7.1409]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.838780212496502, distance: 0.4594792097562832 entropy 1.553932785987854
epoch: 6, step: 88
	action: tensor([[-0.9257, -2.0300, -0.3715,  0.1773, -0.9539, -0.4412,  1.0614]],
       dtype=torch.float64)
	q_value: tensor([[-5.5464]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 6, step: 89
	action: tensor([[-0.4348, -0.5921,  1.0681, -1.4728, -0.9140, -0.6087, -1.3301]],
       dtype=torch.float64)
	q_value: tensor([[-5.5910]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9328906616498653, distance: 1.5909639434806713 entropy 1.553932785987854
epoch: 6, step: 90
	action: tensor([[ 1.2776,  0.1590,  0.8558, -0.1659, -0.5301,  2.0378,  2.1604]],
       dtype=torch.float64)
	q_value: tensor([[-5.2700]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.862181884210077, distance: 0.42482478599145157 entropy 1.553932785987854
epoch: 6, step: 91
	action: tensor([[-0.6224,  1.3965, -0.6270,  0.2263, -0.0156, -0.6774,  0.0071]],
       dtype=torch.float64)
	q_value: tensor([[-8.0966]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 6, step: 92
	action: tensor([[ 1.0219,  0.2346,  0.4007, -0.2225,  0.2581, -1.8575,  0.7422]],
       dtype=torch.float64)
	q_value: tensor([[-5.5910]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6616253118380468, distance: 0.6656648540197013 entropy 1.553932785987854
epoch: 6, step: 93
	action: tensor([[-1.0483, -1.1202,  2.2055,  0.5561, -0.3808, -0.1902,  0.8234]],
       dtype=torch.float64)
	q_value: tensor([[-4.8965]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0295079594745729, distance: 1.6302419813361602 entropy 1.553932785987854
epoch: 6, step: 94
	action: tensor([[ 0.6097, -0.2574,  0.8836, -0.3494,  0.1432,  0.9840,  0.3300]],
       dtype=torch.float64)
	q_value: tensor([[-7.1110]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5505679002454982, distance: 0.7671649225406844 entropy 1.553932785987854
epoch: 6, step: 95
	action: tensor([[ 0.5690,  0.2546, -0.8615,  0.1463, -0.2597,  0.5593,  0.0130]],
       dtype=torch.float64)
	q_value: tensor([[-3.9883]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8145795614658953, distance: 0.4927600552120212 entropy 1.553932785987854
epoch: 6, step: 96
	action: tensor([[ 0.8229, -0.1054, -0.6704,  1.2439,  0.0386,  0.0420, -0.8850]],
       dtype=torch.float64)
	q_value: tensor([[-2.9549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8565444662690863, distance: 0.4334263854418746 entropy 1.553932785987854
epoch: 6, step: 97
	action: tensor([[-0.8493,  1.4576,  0.5790,  0.5324, -1.1995, -0.9163, -0.9599]],
       dtype=torch.float64)
	q_value: tensor([[-3.4361]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 6, step: 98
	action: tensor([[-0.4715,  1.4446, -0.8116,  2.4614, -0.3991, -0.3655, -0.3348]],
       dtype=torch.float64)
	q_value: tensor([[-5.5910]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.25969106004896236, distance: 1.2843657532359107 entropy 1.553932785987854
epoch: 6, step: 99
	action: tensor([[-0.3048,  0.0732,  0.4649,  1.9565,  1.8599, -2.6324, -0.7865]],
       dtype=torch.float64)
	q_value: tensor([[-4.3897]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 6, step: 100
	action: tensor([[ 1.2329, -0.8821, -0.4113, -0.5249,  0.0430, -0.4670, -1.7480]],
       dtype=torch.float64)
	q_value: tensor([[-5.5910]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7051303207943218, distance: 1.4942920360034073 entropy 1.553932785987854
epoch: 6, step: 101
	action: tensor([[-0.9785,  0.2012,  0.5645, -0.3680,  1.1651, -0.0501, -1.2917]],
       dtype=torch.float64)
	q_value: tensor([[-3.7400]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1180638378462207, distance: 1.6654293577210062 entropy 1.553932785987854
epoch: 6, step: 102
	action: tensor([[ 0.4969, -0.9945, -0.2974, -1.7436,  1.0064, -0.0451, -2.2622]],
       dtype=torch.float64)
	q_value: tensor([[-3.1644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6053557717117153, distance: 1.449914323342106 entropy 1.553932785987854
epoch: 6, step: 103
	action: tensor([[-0.1435, -0.8933,  0.6937,  1.7226, -2.4120,  0.4854, -1.8347]],
       dtype=torch.float64)
	q_value: tensor([[-4.1307]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6199171174458029, distance: 0.7054981005692696 entropy 1.553932785987854
epoch: 6, step: 104
	action: tensor([[-0.0642,  1.1127, -0.1382,  0.0349,  0.4962, -0.7472,  0.0572]],
       dtype=torch.float64)
	q_value: tensor([[-6.7888]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5340721717005763, distance: 0.7811168715610529 entropy 1.553932785987854
epoch: 6, step: 105
	action: tensor([[ 0.2303,  1.4126, -1.1017,  1.4014,  0.8611, -0.1538, -0.0877]],
       dtype=torch.float64)
	q_value: tensor([[-2.6121]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.01750406833879825, distance: 1.1342846988619522 entropy 1.553932785987854
epoch: 6, step: 106
	action: tensor([[ 1.4229,  0.7718,  1.2707,  0.8419,  2.8315, -0.5638, -0.8857]],
       dtype=torch.float64)
	q_value: tensor([[-3.8910]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 6, step: 107
	action: tensor([[-1.3174, -0.8433, -0.5318,  0.6950,  1.2018,  1.6827,  0.0833]],
       dtype=torch.float64)
	q_value: tensor([[-5.5910]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6470700803848661, distance: 1.4686311394828069 entropy 1.553932785987854
epoch: 6, step: 108
	action: tensor([[-0.2798,  1.0020, -1.5854,  1.2960,  0.3213,  1.1697, -0.1258]],
       dtype=torch.float64)
	q_value: tensor([[-5.3313]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.26347016361641584, distance: 1.2862908745856843 entropy 1.553932785987854
epoch: 6, step: 109
	action: tensor([[-0.1608, -0.0308,  0.8028, -2.3350, -2.0845, -1.5287,  1.9083]],
       dtype=torch.float64)
	q_value: tensor([[-4.6778]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1353826480399023, distance: 1.2193484192558453 entropy 1.553932785987854
epoch: 6, step: 110
	action: tensor([[-1.7177, -1.8151,  0.4555, -1.1171,  0.2932, -0.5334,  0.1140]],
       dtype=torch.float64)
	q_value: tensor([[-9.8432]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 6, step: 111
	action: tensor([[ 0.3436,  0.7443, -0.1133,  2.1404,  1.3863,  1.4588, -1.8291]],
       dtype=torch.float64)
	q_value: tensor([[-5.5910]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.543045327748743, distance: 0.7735586635061443 entropy 1.553932785987854
epoch: 6, step: 112
	action: tensor([[ 1.2882, -2.0462, -0.5568,  0.4756, -0.2276, -0.2748, -0.5193]],
       dtype=torch.float64)
	q_value: tensor([[-5.3124]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 6, step: 113
	action: tensor([[ 0.8673, -1.3532,  0.2733,  1.7200,  0.4994,  0.1978,  1.2522]],
       dtype=torch.float64)
	q_value: tensor([[-5.5910]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7851334193092803, distance: 0.5304459398943536 entropy 1.553932785987854
epoch: 6, step: 114
	action: tensor([[ 0.1545, -1.2245,  0.8867,  0.4733, -1.4011,  0.1893, -1.1627]],
       dtype=torch.float64)
	q_value: tensor([[-5.7455]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.28516523501195445, distance: 1.2972873335397923 entropy 1.553932785987854
epoch: 6, step: 115
	action: tensor([[-2.3044,  1.1485,  0.1814, -0.1376, -0.4150, -1.9948, -1.2607]],
       dtype=torch.float64)
	q_value: tensor([[-5.0532]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 6, step: 116
	action: tensor([[ 3.0606,  2.4643, -1.1918, -1.6963, -0.4605, -0.3316, -0.2897]],
       dtype=torch.float64)
	q_value: tensor([[-5.5910]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 6, step: 117
	action: tensor([[-0.0568, -0.1699,  0.3695, -2.1958,  1.5625, -1.4357, -0.3273]],
       dtype=torch.float64)
	q_value: tensor([[-5.5910]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24071363748194918, distance: 1.274654463809253 entropy 1.553932785987854
epoch: 6, step: 118
	action: tensor([[-0.2316, -1.7165,  0.2707,  1.0916, -1.2901, -1.1952,  1.1374]],
       dtype=torch.float64)
	q_value: tensor([[-4.9296]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 6, step: 119
	action: tensor([[-1.4807, -1.1070, -0.6738,  0.2993,  0.0989, -1.3854,  1.2567]],
       dtype=torch.float64)
	q_value: tensor([[-5.5910]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.049482268742476, distance: 1.638244716310635 entropy 1.553932785987854
epoch: 6, step: 120
	action: tensor([[ 0.7303,  2.3005, -0.1925,  0.6693,  0.7215, -0.2712, -3.8198]],
       dtype=torch.float64)
	q_value: tensor([[-6.6690]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 6, step: 121
	action: tensor([[ 1.0628,  0.7418, -0.5884,  1.6754, -0.4118,  0.3137,  1.0536]],
       dtype=torch.float64)
	q_value: tensor([[-5.5910]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3935005277593473, distance: 0.8911932879011641 entropy 1.553932785987854
epoch: 6, step: 122
	action: tensor([[ 0.2842,  0.7293,  1.9700, -1.0343,  0.8451, -0.7826, -0.4424]],
       dtype=torch.float64)
	q_value: tensor([[-4.7313]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19832594155291117, distance: 1.0246029653782331 entropy 1.553932785987854
epoch: 6, step: 123
	action: tensor([[ 0.7935, -0.5771,  0.8111,  1.5516,  1.9410, -0.9859, -0.7879]],
       dtype=torch.float64)
	q_value: tensor([[-4.3100]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9287967489657294, distance: 0.305356103343379 entropy 1.553932785987854
epoch: 6, step: 124
	action: tensor([[ 1.3927, -0.0606, -0.9747,  0.4154, -0.7942, -0.7577,  0.4735]],
       dtype=torch.float64)
	q_value: tensor([[-5.3918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6840584545200137, distance: 0.6432207741218646 entropy 1.553932785987854
epoch: 6, step: 125
	action: tensor([[ 2.2353, -1.8755, -1.3308,  0.8579, -1.3787, -0.7756, -0.7194]],
       dtype=torch.float64)
	q_value: tensor([[-4.2231]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 6, step: 126
	action: tensor([[ 0.3051, -0.1559,  0.8588, -0.9315, -0.3137,  0.7104,  0.1785]],
       dtype=torch.float64)
	q_value: tensor([[-5.5910]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07037345854410881, distance: 1.1033440328406994 entropy 1.553932785987854
epoch: 6, step: 127
	action: tensor([[ 1.9463, -0.9324,  2.8053, -0.2356, -0.0105, -0.5258,  0.6000]],
       dtype=torch.float64)
	q_value: tensor([[-4.2154]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
LOSS epoch 6 actor 261.92094523452107 critic 1621.567499583477 
epoch: 7, step: 0
	action: tensor([[-0.3634,  0.7059,  0.1161, -1.4282, -1.0307, -0.8263,  1.6641]],
       dtype=torch.float64)
	q_value: tensor([[-9.5339]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09238539082192143, distance: 1.0902031490631232 entropy 1.553932785987854
epoch: 7, step: 1
	action: tensor([[-1.8688,  0.3626, -0.0119,  0.1411, -0.3048, -1.3439,  2.3946]],
       dtype=torch.float64)
	q_value: tensor([[-10.8334]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1717924227477332, distance: 1.6864204116131118 entropy 1.553932785987854
epoch: 7, step: 2
	action: tensor([[ 0.4654, -0.6650,  1.3988, -1.9526, -0.4434,  1.0230,  0.2350]],
       dtype=torch.float64)
	q_value: tensor([[-12.3674]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10222549878229525, distance: 1.2014118745827014 entropy 1.553932785987854
epoch: 7, step: 3
	action: tensor([[-0.0514, -0.6075, -1.1188, -1.5426, -1.4086, -0.1845, -1.3592]],
       dtype=torch.float64)
	q_value: tensor([[-10.5076]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33908479361332833, distance: 0.9303140189295134 entropy 1.553932785987854
epoch: 7, step: 4
	action: tensor([[-0.6335,  0.9869,  1.3072,  0.9148, -0.1905, -1.2985,  0.9090]],
       dtype=torch.float64)
	q_value: tensor([[-9.1392]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3861050612053816, distance: 0.8966102919635948 entropy 1.553932785987854
epoch: 7, step: 5
	action: tensor([[-0.7437,  1.4744, -0.2914, -0.8737, -0.3910,  0.8880, -0.9953]],
       dtype=torch.float64)
	q_value: tensor([[-7.9747]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 7, step: 6
	action: tensor([[-1.0639,  0.9617,  1.5146, -0.4684,  1.0004, -2.2356,  0.8394]],
       dtype=torch.float64)
	q_value: tensor([[-9.5339]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0930942943431019, distance: 1.0897773076249795 entropy 1.553932785987854
epoch: 7, step: 7
	action: tensor([[ 0.3970, -0.1430,  0.2953,  2.3985, -0.2005,  0.2422,  0.2518]],
       dtype=torch.float64)
	q_value: tensor([[-9.8522]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6382242531665825, distance: 0.6882978573912816 entropy 1.553932785987854
epoch: 7, step: 8
	action: tensor([[ 0.1674,  0.0323, -2.0957,  0.8872,  0.8933, -0.5351,  2.0537]],
       dtype=torch.float64)
	q_value: tensor([[-8.0351]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.048260780655424496, distance: 1.171632370315066 entropy 1.553932785987854
epoch: 7, step: 9
	action: tensor([[ 1.0634,  2.0069,  1.8208, -0.9669,  0.4395,  0.0321,  0.1582]],
       dtype=torch.float64)
	q_value: tensor([[-10.4550]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 7, step: 10
	action: tensor([[ 1.5708,  1.0992,  0.0912,  1.1057, -0.1256,  2.5594, -0.7182]],
       dtype=torch.float64)
	q_value: tensor([[-9.5339]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.45759598951765956, distance: 0.8427876040687073 entropy 1.553932785987854
epoch: 7, step: 11
	action: tensor([[ 0.9705,  1.1073, -2.2196,  0.9385, -1.4142,  0.1363, -0.5481]],
       dtype=torch.float64)
	q_value: tensor([[-9.7640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8701048353271756, distance: 0.41243278963137686 entropy 1.553932785987854
epoch: 7, step: 12
	action: tensor([[ 1.0026,  1.8144, -1.7824,  0.9950, -0.3050,  0.2738, -0.3528]],
       dtype=torch.float64)
	q_value: tensor([[-9.5455]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 7, step: 13
	action: tensor([[-0.9092,  1.7454, -1.3321, -0.5727, -0.2820,  0.5368,  0.4320]],
       dtype=torch.float64)
	q_value: tensor([[-9.5339]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 7, step: 14
	action: tensor([[ 0.1404,  1.0868,  0.1259,  0.7571, -0.4481, -1.4963, -2.1945]],
       dtype=torch.float64)
	q_value: tensor([[-9.5339]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7994471093705373, distance: 0.5124731971088579 entropy 1.553932785987854
epoch: 7, step: 15
	action: tensor([[-0.1970, -0.3073,  1.9684,  1.1328,  1.4557, -0.4174,  1.2958]],
       dtype=torch.float64)
	q_value: tensor([[-8.0376]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.475156000718134, distance: 0.8290329869638395 entropy 1.553932785987854
epoch: 7, step: 16
	action: tensor([[-0.3073, -1.0059, -1.2460,  0.0280,  0.0140,  0.2471, -0.3271]],
       dtype=torch.float64)
	q_value: tensor([[-10.4777]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9209999085911189, distance: 1.5860627495979367 entropy 1.553932785987854
epoch: 7, step: 17
	action: tensor([[ 0.3350,  1.6130, -1.4538,  0.9189, -0.1101, -0.1586,  1.3169]],
       dtype=torch.float64)
	q_value: tensor([[-5.6254]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 7, step: 18
	action: tensor([[ 0.9492,  1.1151, -0.2096,  0.3043,  0.3808,  0.6258, -0.4428]],
       dtype=torch.float64)
	q_value: tensor([[-9.5339]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 7, step: 19
	action: tensor([[ 0.6865, -1.0497, -1.1220,  0.0708,  0.1828, -0.1968,  1.4234]],
       dtype=torch.float64)
	q_value: tensor([[-9.5339]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4438100484442238, distance: 1.375028574322495 entropy 1.553932785987854
epoch: 7, step: 20
	action: tensor([[ 0.2842, -0.6422, -0.2776,  1.8840,  0.7379, -0.9784, -0.3340]],
       dtype=torch.float64)
	q_value: tensor([[-7.7164]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4767806470145596, distance: 0.8277488631997464 entropy 1.553932785987854
epoch: 7, step: 21
	action: tensor([[ 0.1779, -0.5492, -0.8149, -2.6533,  1.5697, -0.2360, -0.6940]],
       dtype=torch.float64)
	q_value: tensor([[-7.7884]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03848952522133964, distance: 1.122105531322904 entropy 1.553932785987854
epoch: 7, step: 22
	action: tensor([[ 0.7142, -0.4169,  1.3512, -0.3715,  0.3711,  1.3649, -1.2439]],
       dtype=torch.float64)
	q_value: tensor([[-7.5288]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38261313017938015, distance: 0.8991567062212228 entropy 1.553932785987854
epoch: 7, step: 23
	action: tensor([[ 1.5916,  1.1073,  2.1310,  1.4109, -0.4084,  2.2840,  0.1772]],
       dtype=torch.float64)
	q_value: tensor([[-7.2542]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3171385537243129, distance: 0.9456338068460028 entropy 1.553932785987854
epoch: 7, step: 24
	action: tensor([[-0.7934, -0.7702, -0.2100, -0.9202,  1.4679,  0.1963,  0.6725]],
       dtype=torch.float64)
	q_value: tensor([[-12.5240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8241358945157653, distance: 1.5455579305259994 entropy 1.553932785987854
epoch: 7, step: 25
	action: tensor([[ 1.2352, -0.8954,  0.8837,  0.9155, -0.4844,  0.9887,  0.3658]],
       dtype=torch.float64)
	q_value: tensor([[-7.5687]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4164960104924962, distance: 0.8741351984219414 entropy 1.553932785987854
epoch: 7, step: 26
	action: tensor([[-0.5249, -0.4804, -1.1760,  0.7712, -1.3977, -2.1520,  0.6261]],
       dtype=torch.float64)
	q_value: tensor([[-8.7009]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11276828508997028, distance: 1.077891934123199 entropy 1.553932785987854
epoch: 7, step: 27
	action: tensor([[ 0.0839, -0.1142,  1.5327, -0.5677,  0.3684, -0.1706,  0.7315]],
       dtype=torch.float64)
	q_value: tensor([[-10.6746]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10370554061559933, distance: 1.2022182174431093 entropy 1.553932785987854
epoch: 7, step: 28
	action: tensor([[-0.3117,  2.1302, -1.5956, -0.8769,  0.4019, -2.2032, -0.6154]],
       dtype=torch.float64)
	q_value: tensor([[-7.2095]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 7, step: 29
	action: tensor([[-0.7168, -0.2222, -1.0977, -0.2413, -1.0414,  0.3014, -0.7372]],
       dtype=torch.float64)
	q_value: tensor([[-9.5339]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8205498608009401, distance: 1.5440379918305969 entropy 1.553932785987854
epoch: 7, step: 30
	action: tensor([[-1.0271, -1.1443,  0.6450, -0.3594, -0.0697, -2.7201,  0.0971]],
       dtype=torch.float64)
	q_value: tensor([[-6.9776]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.442505583848134, distance: 1.3744072732656363 entropy 1.553932785987854
epoch: 7, step: 31
	action: tensor([[ 0.3205, -0.7397, -0.2897, -0.2682,  0.3040, -1.8243, -0.1735]],
       dtype=torch.float64)
	q_value: tensor([[-10.0154]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1956457697037013, distance: 1.25128995756982 entropy 1.553932785987854
epoch: 7, step: 32
	action: tensor([[-0.3263,  0.5421, -0.7168, -2.2802, -1.1181, -1.4398, -1.0192]],
       dtype=torch.float64)
	q_value: tensor([[-7.5853]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6685579319709786, distance: 0.6588104940064995 entropy 1.553932785987854
epoch: 7, step: 33
	action: tensor([[-0.3873, -0.5105,  0.2385,  0.8323,  0.4887,  0.1150, -0.0340]],
       dtype=torch.float64)
	q_value: tensor([[-10.5076]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06051796034841661, distance: 1.1091772013407133 entropy 1.553932785987854
epoch: 7, step: 34
	action: tensor([[ 1.2774, -0.6915,  1.2786,  0.4631, -0.7438, -0.9292, -0.1459]],
       dtype=torch.float64)
	q_value: tensor([[-5.1304]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15294372602975792, distance: 1.2287421277979647 entropy 1.553932785987854
epoch: 7, step: 35
	action: tensor([[-0.4662,  0.9947,  0.8326,  0.8063, -0.4657, -0.4040,  0.9916]],
       dtype=torch.float64)
	q_value: tensor([[-8.8490]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 7, step: 36
	action: tensor([[-0.4687, -0.1326, -0.8490, -0.8091,  1.1385, -1.5832, -0.0667]],
       dtype=torch.float64)
	q_value: tensor([[-9.5339]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21585391298433643, distance: 1.2618199562228527 entropy 1.553932785987854
epoch: 7, step: 37
	action: tensor([[-0.7066,  0.5166, -0.3986, -0.4868,  0.2937,  1.3524,  1.9806]],
       dtype=torch.float64)
	q_value: tensor([[-7.2405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.022417162796354195, distance: 1.131445077804378 entropy 1.553932785987854
epoch: 7, step: 38
	action: tensor([[-1.5510,  1.4311,  1.8230,  1.3229, -0.9097,  0.5702,  0.7021]],
       dtype=torch.float64)
	q_value: tensor([[-10.3560]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2494524488548534, distance: 0.991392888812444 entropy 1.553932785987854
epoch: 7, step: 39
	action: tensor([[ 0.2337, -0.1410, -2.0961, -0.8669,  0.4624,  0.5759,  0.4017]],
       dtype=torch.float64)
	q_value: tensor([[-10.5502]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5426141086311498, distance: 0.7739235734687864 entropy 1.553932785987854
epoch: 7, step: 40
	action: tensor([[ 2.2306,  0.4641, -0.9302,  1.1958,  0.1167,  0.1173,  1.0873]],
       dtype=torch.float64)
	q_value: tensor([[-8.0025]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 7, step: 41
	action: tensor([[-0.2510,  0.5323,  0.6011, -0.6631, -0.2855,  0.0710, -0.7435]],
       dtype=torch.float64)
	q_value: tensor([[-9.5339]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07886770148386657, distance: 1.1886138572187126 entropy 1.553932785987854
epoch: 7, step: 42
	action: tensor([[0.7363, 0.4413, 0.6352, 2.3940, 0.2087, 0.4338, 0.6768]],
       dtype=torch.float64)
	q_value: tensor([[-5.5123]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26627712224688604, distance: 0.9802181136005157 entropy 1.553932785987854
epoch: 7, step: 43
	action: tensor([[ 0.1100,  0.8760,  0.5842, -0.8829, -0.0609,  1.0668, -0.8709]],
       dtype=torch.float64)
	q_value: tensor([[-8.4216]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6386493851586272, distance: 0.6878933202464987 entropy 1.553932785987854
epoch: 7, step: 44
	action: tensor([[ 0.2874, -2.6077, -0.4240,  0.4236, -1.6045, -1.0620, -1.8443]],
       dtype=torch.float64)
	q_value: tensor([[-7.1591]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 7, step: 45
	action: tensor([[-0.9938,  0.5314,  1.3069,  0.7395, -0.6969, -1.0798,  0.4463]],
       dtype=torch.float64)
	q_value: tensor([[-9.5339]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.28709955965165723, distance: 1.2982632513710723 entropy 1.553932785987854
epoch: 7, step: 46
	action: tensor([[-0.3089, -0.5706,  0.3668,  0.8676,  0.0479,  1.9199, -0.5331]],
       dtype=torch.float64)
	q_value: tensor([[-8.0771]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5511654127965572, distance: 0.7666547863852256 entropy 1.553932785987854
epoch: 7, step: 47
	action: tensor([[-0.3888,  0.9674, -2.0721, -2.0761,  2.7108,  0.1000,  0.8771]],
       dtype=torch.float64)
	q_value: tensor([[-7.3719]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7400544361008625, distance: 0.5834422810049132 entropy 1.553932785987854
epoch: 7, step: 48
	action: tensor([[ 0.7947, -2.3512,  1.1990,  3.3252,  0.6554, -0.3020, -1.2685]],
       dtype=torch.float64)
	q_value: tensor([[-11.8878]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 7, step: 49
	action: tensor([[ 0.4667, -0.3283,  0.3804,  0.1612,  0.1814,  0.5208,  3.4149]],
       dtype=torch.float64)
	q_value: tensor([[-9.5339]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.6768459963463066 entropy 1.553932785987854
epoch: 7, step: 50
	action: tensor([[ 0.2823,  0.2287,  0.2731,  0.8127,  2.2290, -1.1940, -0.3041]],
       dtype=torch.float64)
	q_value: tensor([[-9.5339]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7263817845061687, distance: 0.5985896403623531 entropy 1.553932785987854
epoch: 7, step: 51
	action: tensor([[ 0.3303,  0.2892,  0.9823, -0.9213, -0.1987, -0.5383, -1.4606]],
       dtype=torch.float64)
	q_value: tensor([[-7.4569]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08718120171254296, distance: 1.0933242499725826 entropy 1.553932785987854
epoch: 7, step: 52
	action: tensor([[-0.3248,  0.7894, -1.3749,  2.0524,  0.2550,  0.1938, -1.7942]],
       dtype=torch.float64)
	q_value: tensor([[-6.4727]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3654014745847398, distance: 1.337170777736404 entropy 1.553932785987854
epoch: 7, step: 53
	action: tensor([[-1.3318, -0.3513,  0.0082, -0.5214, -2.0299, -1.9656, -2.4917]],
       dtype=torch.float64)
	q_value: tensor([[-8.1906]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.39665271036855665, distance: 1.3523867512604992 entropy 1.553932785987854
epoch: 7, step: 54
	action: tensor([[-1.7928,  0.3250, -1.0317,  0.2841, -1.4834,  0.0607,  1.5030]],
       dtype=torch.float64)
	q_value: tensor([[-11.6957]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 7, step: 55
	action: tensor([[-0.2078,  0.3945,  1.9773,  0.3522,  0.9941, -1.8322,  0.6106]],
       dtype=torch.float64)
	q_value: tensor([[-9.5339]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6198325518956205, distance: 0.7055765801846565 entropy 1.553932785987854
epoch: 7, step: 56
	action: tensor([[ 0.5083,  0.3033,  0.7531,  0.6547, -0.4698, -0.1063,  1.1954]],
       dtype=torch.float64)
	q_value: tensor([[-9.3927]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9890482727000114, distance: 0.1197561991719165 entropy 1.553932785987854
epoch: 7, step: 57
	action: tensor([[ 2.8013, -1.1619,  1.0520, -0.3339, -0.7200, -0.7163, -1.0560]],
       dtype=torch.float64)
	q_value: tensor([[-6.7473]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 7, step: 58
	action: tensor([[-0.7966,  2.0351, -2.0783,  0.5543,  0.6762,  1.7457, -1.4651]],
       dtype=torch.float64)
	q_value: tensor([[-9.5339]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 7, step: 59
	action: tensor([[ 1.9205, -1.0702,  0.9586, -2.5415, -0.3148,  1.4390, -0.8053]],
       dtype=torch.float64)
	q_value: tensor([[-9.5339]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18481923746784412, distance: 1.0331982320629858 entropy 1.553932785987854
epoch: 7, step: 60
	action: tensor([[-1.9408,  0.0584, -1.3927,  0.5945,  0.7162, -0.5237,  0.1958]],
       dtype=torch.float64)
	q_value: tensor([[-10.9797]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.7932558851926217, distance: 1.9125467267486043 entropy 1.553932785987854
epoch: 7, step: 61
	action: tensor([[ 1.7849,  0.9495, -2.5500, -0.2227,  0.6054, -1.3718,  0.6456]],
       dtype=torch.float64)
	q_value: tensor([[-8.0303]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4609971954165185, distance: 0.8401410510116719 entropy 1.553932785987854
epoch: 7, step: 62
	action: tensor([[-1.1296,  0.3629, -1.6170,  0.8700,  0.9046,  1.9532,  0.4841]],
       dtype=torch.float64)
	q_value: tensor([[-9.8244]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2681361964215334, distance: 1.2886638409298465 entropy 1.553932785987854
epoch: 7, step: 63
	action: tensor([[ 1.0373, -0.1315, -0.8680, -1.1220, -0.8223, -0.4586, -0.8936]],
       dtype=torch.float64)
	q_value: tensor([[-9.7611]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20308581376160573, distance: 1.0215566899856836 entropy 1.553932785987854
epoch: 7, step: 64
	action: tensor([[ 0.0263, -0.0814, -0.4713,  0.1625, -0.3096,  1.6611,  1.0740]],
       dtype=torch.float64)
	q_value: tensor([[-6.7384]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22449322234591085, distance: 1.0077423085410575 entropy 1.553932785987854
epoch: 7, step: 65
	action: tensor([[-1.0219, -1.4383,  0.0599,  1.3545,  0.0504,  0.0172,  1.6858]],
       dtype=torch.float64)
	q_value: tensor([[-8.4134]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9078070184403582, distance: 1.5806070486976864 entropy 1.553932785987854
epoch: 7, step: 66
	action: tensor([[ 0.2253, -0.4088, -1.7705, -0.5383,  0.0779,  0.8407,  0.6244]],
       dtype=torch.float64)
	q_value: tensor([[-10.7358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11925658191753963, distance: 1.0739434075939682 entropy 1.553932785987854
epoch: 7, step: 67
	action: tensor([[-1.4476, -1.8123, -2.0337,  0.9263,  0.0559,  1.2050,  0.8042]],
       dtype=torch.float64)
	q_value: tensor([[-7.5422]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 7, step: 68
	action: tensor([[ 0.0302, -0.8461,  0.5718,  0.1588, -0.3016,  0.1203, -0.3897]],
       dtype=torch.float64)
	q_value: tensor([[-9.5339]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.30262512808793995, distance: 1.3060698950655276 entropy 1.553932785987854
epoch: 7, step: 69
	action: tensor([[-1.9623,  0.6105, -0.0177, -0.3191,  1.7117,  0.3270, -0.7501]],
       dtype=torch.float64)
	q_value: tensor([[-5.1362]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2258256113785628, distance: 1.7072702040614864 entropy 1.553932785987854
epoch: 7, step: 70
	action: tensor([[-1.3865, -1.7565,  0.2803,  0.0844, -0.5315, -0.0483,  0.3117]],
       dtype=torch.float64)
	q_value: tensor([[-6.7714]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 7, step: 71
	action: tensor([[-0.8537,  1.1439,  0.6988, -1.8209, -1.6876,  0.8402,  1.0537]],
       dtype=torch.float64)
	q_value: tensor([[-9.5339]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6962417358222179, distance: 1.4903921885013331 entropy 1.553932785987854
epoch: 7, step: 72
	action: tensor([[-0.5126,  1.7856, -0.5192, -0.2263, -0.3516, -0.2392, -2.5242]],
       dtype=torch.float64)
	q_value: tensor([[-13.7832]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 7, step: 73
	action: tensor([[-0.0588,  2.1571, -0.9810,  1.9956, -0.0503,  0.0419, -2.9939]],
       dtype=torch.float64)
	q_value: tensor([[-9.5339]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 7, step: 74
	action: tensor([[-0.3138,  0.3555, -0.3387, -1.1110,  0.1258, -2.2336,  1.3264]],
       dtype=torch.float64)
	q_value: tensor([[-9.5339]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27128644854163886, distance: 0.9768662745573515 entropy 1.553932785987854
epoch: 7, step: 75
	action: tensor([[-0.5028, -0.1820, -1.3049, -1.3310, -1.1067, -0.1064,  0.7558]],
       dtype=torch.float64)
	q_value: tensor([[-9.5911]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.46407530121450213, distance: 0.8377387020311547 entropy 1.553932785987854
epoch: 7, step: 76
	action: tensor([[ 0.0254,  0.1476, -1.7167, -1.2248, -1.5789, -0.2430, -0.1517]],
       dtype=torch.float64)
	q_value: tensor([[-9.8569]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.933519680572729, distance: 0.2950551789712369 entropy 1.553932785987854
epoch: 7, step: 77
	action: tensor([[-2.4230,  1.1172,  2.2794,  2.3791, -0.5620,  1.2118,  0.2564]],
       dtype=torch.float64)
	q_value: tensor([[-10.1268]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 7, step: 78
	action: tensor([[ 1.8471, -0.7009,  2.1306,  1.3990, -0.3729,  0.0175,  0.3008]],
       dtype=torch.float64)
	q_value: tensor([[-9.5339]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7705741128043695, distance: 1.5226978985367132 entropy 1.553932785987854
epoch: 7, step: 79
	action: tensor([[ 0.2754, -1.5728, -1.5142, -0.6411,  0.8002, -1.7188,  0.0316]],
       dtype=torch.float64)
	q_value: tensor([[-11.4387]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 7, step: 80
	action: tensor([[ 0.2151, -0.2257,  1.2847,  0.8667, -2.2054,  1.1311,  0.3432]],
       dtype=torch.float64)
	q_value: tensor([[-9.5339]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.735828205391359, distance: 0.5881660010939449 entropy 1.553932785987854
epoch: 7, step: 81
	action: tensor([[ 0.7664, -0.1131,  1.1631, -0.3090,  1.5080, -0.0206, -0.2528]],
       dtype=torch.float64)
	q_value: tensor([[-11.6356]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5457544062121391, distance: 0.7712622143203277 entropy 1.553932785987854
epoch: 7, step: 82
	action: tensor([[-0.4268,  1.6026, -0.0301, -1.5229,  1.3472, -0.8164,  2.2155]],
       dtype=torch.float64)
	q_value: tensor([[-5.7656]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06519043509254985, distance: 1.1064155400318998 entropy 1.553932785987854
epoch: 7, step: 83
	action: tensor([[-0.3045,  2.8580, -0.9734,  1.9961, -0.1060, -0.1882, -0.0281]],
       dtype=torch.float64)
	q_value: tensor([[-10.6795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 7, step: 84
	action: tensor([[-0.8536,  1.6334,  1.0606,  1.5076, -1.5327,  2.7490,  0.8895]],
       dtype=torch.float64)
	q_value: tensor([[-9.5339]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 7, step: 85
	action: tensor([[ 0.4435,  0.4659,  2.7989,  0.9465,  2.8694, -0.1057, -0.9472]],
       dtype=torch.float64)
	q_value: tensor([[-9.5339]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6679014924025733, distance: 0.6594625768249285 entropy 1.553932785987854
epoch: 7, step: 86
	action: tensor([[ 0.8361,  0.0269,  0.6131, -3.1197, -0.3351,  2.5250, -1.0614]],
       dtype=torch.float64)
	q_value: tensor([[-10.0247]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 7, step: 87
	action: tensor([[ 1.9526,  0.9425, -0.9069, -0.5432, -0.3143, -0.6624,  0.9179]],
       dtype=torch.float64)
	q_value: tensor([[-9.5339]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29748248673170397, distance: 0.9591472168725099 entropy 1.553932785987854
epoch: 7, step: 88
	action: tensor([[-0.6364,  0.8952,  1.5138, -1.2918,  0.9977,  1.4670,  0.7156]],
       dtype=torch.float64)
	q_value: tensor([[-8.1237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20476198560114534, distance: 1.2560511372314407 entropy 1.553932785987854
epoch: 7, step: 89
	action: tensor([[ 0.0426,  0.4794,  0.4149,  1.1137, -2.7220,  0.9998, -1.3692]],
       dtype=torch.float64)
	q_value: tensor([[-9.3327]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7372599434728665, distance: 0.5865699871203038 entropy 1.553932785987854
epoch: 7, step: 90
	action: tensor([[ 1.2517,  1.2959,  0.2519, -0.0248, -2.2398,  0.2894,  0.3825]],
       dtype=torch.float64)
	q_value: tensor([[-12.1033]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 7, step: 91
	action: tensor([[ 1.4631, -0.4747,  0.7072, -0.1905,  1.9157,  0.6727,  1.4936]],
       dtype=torch.float64)
	q_value: tensor([[-9.5339]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13603661964088198, distance: 1.2196995363199923 entropy 1.553932785987854
epoch: 7, step: 92
	action: tensor([[-1.6284, -0.3912, -1.3000,  0.6151,  0.8881,  1.2666, -0.7305]],
       dtype=torch.float64)
	q_value: tensor([[-9.3436]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1646024764755087, distance: 1.6836265616652883 entropy 1.553932785987854
epoch: 7, step: 93
	action: tensor([[-0.1069, -1.3364,  1.1864, -1.7644,  0.5364, -0.7297, -0.1801]],
       dtype=torch.float64)
	q_value: tensor([[-8.5999]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.48465075866829066, distance: 1.3943405111712712 entropy 1.553932785987854
epoch: 7, step: 94
	action: tensor([[ 0.5269, -1.3902, -1.0866,  0.2116, -0.2913, -1.6972, -0.2298]],
       dtype=torch.float64)
	q_value: tensor([[-8.4022]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 7, step: 95
	action: tensor([[-0.3811,  1.1006,  0.2375, -0.0269,  0.7917,  0.6202, -0.3555]],
       dtype=torch.float64)
	q_value: tensor([[-9.5339]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 7, step: 96
	action: tensor([[ 0.7916, -0.5667,  0.8499, -0.0093,  0.2529,  1.6887,  0.8446]],
       dtype=torch.float64)
	q_value: tensor([[-9.5339]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5886615045799442, distance: 0.7339329246958332 entropy 1.553932785987854
epoch: 7, step: 97
	action: tensor([[ 1.9414,  0.2431,  0.5692,  0.6143, -0.5456, -0.1038, -1.0762]],
       dtype=torch.float64)
	q_value: tensor([[-8.7457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 7, step: 98
	action: tensor([[-2.0141,  1.5201, -2.1240,  0.5576,  0.4795,  0.3537,  0.0497]],
       dtype=torch.float64)
	q_value: tensor([[-9.5339]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 7, step: 99
	action: tensor([[ 0.2143,  1.3921, -1.4032,  0.1478,  0.3889,  0.4237,  1.1529]],
       dtype=torch.float64)
	q_value: tensor([[-9.5339]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 7, step: 100
	action: tensor([[ 1.3049,  0.4530, -1.4694, -0.0849, -1.1820, -0.3211, -0.4065]],
       dtype=torch.float64)
	q_value: tensor([[-9.5339]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8380014491872272, distance: 0.46058761735879605 entropy 1.553932785987854
epoch: 7, step: 101
	action: tensor([[0.7898, 0.5794, 2.6101, 0.1113, 0.4652, 1.9248, 1.4025]],
       dtype=torch.float64)
	q_value: tensor([[-7.5704]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8402804016139936, distance: 0.4573364307453686 entropy 1.553932785987854
epoch: 7, step: 102
	action: tensor([[-1.3868,  0.6874,  0.0985,  1.7916,  0.3624, -1.2973,  0.7092]],
       dtype=torch.float64)
	q_value: tensor([[-12.1892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16742470512240226, distance: 1.0441633686073235 entropy 1.553932785987854
epoch: 7, step: 103
	action: tensor([[-0.5984,  0.4086,  1.3227, -0.2905,  0.8030, -1.4395,  0.5232]],
       dtype=torch.float64)
	q_value: tensor([[-8.2135]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.28780248381879736, distance: 1.298617713481198 entropy 1.553932785987854
epoch: 7, step: 104
	action: tensor([[-0.6139,  1.0294,  0.0536, -0.4002,  0.7929, -0.1942, -0.2747]],
       dtype=torch.float64)
	q_value: tensor([[-7.5620]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05830126742273101, distance: 1.1772300825543636 entropy 1.553932785987854
epoch: 7, step: 105
	action: tensor([[ 0.4556,  1.4182, -1.0576, -0.6656, -1.7726,  0.9328,  0.4229]],
       dtype=torch.float64)
	q_value: tensor([[-4.6043]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8599841861814261, distance: 0.42819859481803246 entropy 1.553932785987854
epoch: 7, step: 106
	action: tensor([[ 0.6591, -1.2924,  0.2057, -0.0041, -0.4404,  2.1615,  0.0135]],
       dtype=torch.float64)
	q_value: tensor([[-11.8364]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1765967215536549, distance: 1.0383959585777411 entropy 1.553932785987854
epoch: 7, step: 107
	action: tensor([[ 0.6307, -1.2065,  0.2610, -0.3232, -1.7187, -1.0149,  0.3079]],
       dtype=torch.float64)
	q_value: tensor([[-9.4481]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5402750094216273, distance: 1.4202206658599885 entropy 1.553932785987854
epoch: 7, step: 108
	action: tensor([[ 0.1858, -0.1583, -0.8946,  0.0792,  0.8724,  0.2803, -1.6933]],
       dtype=torch.float64)
	q_value: tensor([[-9.4460]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.016683442429908935, distance: 1.1538505692324073 entropy 1.553932785987854
epoch: 7, step: 109
	action: tensor([[ 1.4808,  0.4737, -2.0462,  0.0982,  2.1798,  1.7964,  0.3855]],
       dtype=torch.float64)
	q_value: tensor([[-5.1258]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9273928757637628, distance: 0.30835167452022694 entropy 1.553932785987854
epoch: 7, step: 110
	action: tensor([[ 1.5674, -0.7438,  0.4553, -0.8665, -0.9376, -2.9969,  1.3612]],
       dtype=torch.float64)
	q_value: tensor([[-10.0822]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.007145062099704669, distance: 1.1484251827317211 entropy 1.553932785987854
epoch: 7, step: 111
	action: tensor([[-0.1511,  0.8151,  0.4660,  0.6853,  0.1250,  0.1459, -0.6588]],
       dtype=torch.float64)
	q_value: tensor([[-12.0660]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 7, step: 112
	action: tensor([[-1.5067, -1.1983,  0.4229,  1.5659,  0.3256,  1.8264, -0.6212]],
       dtype=torch.float64)
	q_value: tensor([[-9.5339]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04736859735437848, distance: 1.1169124707858415 entropy 1.553932785987854
epoch: 7, step: 113
	action: tensor([[ 1.1333, -1.1091, -1.3454, -1.8618, -0.2618, -0.8132, -0.4992]],
       dtype=torch.float64)
	q_value: tensor([[-9.4939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07457330242511151, distance: 1.1008488812755965 entropy 1.553932785987854
epoch: 7, step: 114
	action: tensor([[ 1.1434, -0.3499, -0.3513, -0.4391, -1.1331,  0.8858,  0.7164]],
       dtype=torch.float64)
	q_value: tensor([[-8.5943]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27252529633186184, distance: 0.9760355617172921 entropy 1.553932785987854
epoch: 7, step: 115
	action: tensor([[-0.1023, -0.0737,  1.2049, -0.8114,  0.4142,  2.4088,  0.1238]],
       dtype=torch.float64)
	q_value: tensor([[-8.3769]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39361709013780866, distance: 0.8911076451218508 entropy 1.553932785987854
epoch: 7, step: 116
	action: tensor([[4.4403e-01, 1.2072e+00, 1.8238e+00, 3.6368e-01, 4.8968e-04, 7.3310e-01,
         1.8134e+00]], dtype=torch.float64)
	q_value: tensor([[-9.2033]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9247005620237345, distance: 0.3140165664782185 entropy 1.553932785987854
epoch: 7, step: 117
	action: tensor([[ 0.6305, -0.5516, -2.3208, -0.3426,  1.9966, -0.0828,  0.0458]],
       dtype=torch.float64)
	q_value: tensor([[-9.9042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36321668569181953, distance: 0.9131718827716923 entropy 1.553932785987854
epoch: 7, step: 118
	action: tensor([[-0.2423, -0.0314, -0.0832, -0.5877,  0.5488, -0.5912, -2.4676]],
       dtype=torch.float64)
	q_value: tensor([[-9.0307]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.43130065413397944, distance: 1.3690588851415357 entropy 1.553932785987854
epoch: 7, step: 119
	action: tensor([[-2.9042, -0.6765, -0.1936,  1.6123, -0.4120, -0.7795,  0.8009]],
       dtype=torch.float64)
	q_value: tensor([[-6.3643]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 7, step: 120
	action: tensor([[ 0.4768,  0.2149, -1.4753,  0.9861,  1.0293,  1.1185, -1.2465]],
       dtype=torch.float64)
	q_value: tensor([[-9.5339]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4965030458698009, distance: 0.8119982945753755 entropy 1.553932785987854
epoch: 7, step: 121
	action: tensor([[ 1.6179, -0.8147,  0.4489,  0.3972,  0.7643, -0.6960, -1.2206]],
       dtype=torch.float64)
	q_value: tensor([[-7.0382]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11107737581487509, distance: 1.2062264446555298 entropy 1.553932785987854
epoch: 7, step: 122
	action: tensor([[ 0.2193, -0.9928, -1.9199, -0.2598, -0.2511,  1.8358,  1.2364]],
       dtype=torch.float64)
	q_value: tensor([[-6.8865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.40657597545450597, distance: 1.3571826245968115 entropy 1.553932785987854
epoch: 7, step: 123
	action: tensor([[ 1.1197, -1.5644,  0.1399,  0.3982,  0.6115, -1.4048, -1.4400]],
       dtype=torch.float64)
	q_value: tensor([[-10.8416]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 7, step: 124
	action: tensor([[-1.6625, -0.3943, -0.6190, -1.4314, -0.8227,  0.1690, -0.3730]],
       dtype=torch.float64)
	q_value: tensor([[-9.5339]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 7, step: 125
	action: tensor([[-2.1060, -1.6296, -0.6809, -0.1358, -1.2856,  0.1005, -0.1786]],
       dtype=torch.float64)
	q_value: tensor([[-9.5339]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 7, step: 126
	action: tensor([[ 1.8523,  1.2640, -0.7379,  0.7775, -1.0714, -3.0522,  0.4634]],
       dtype=torch.float64)
	q_value: tensor([[-9.5339]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
epoch: 7, step: 127
	action: tensor([[-2.0248, -2.2648,  1.4278,  1.0626, -0.2090,  0.0848, -0.8363]],
       dtype=torch.float64)
	q_value: tensor([[-9.5339]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.553932785987854
LOSS epoch 7 actor 190.91355208981395 critic 1289.5861678221809 
epoch: 8, step: 0
	action: tensor([[ 1.4682, -0.0849, -0.5765, -0.2959,  1.3943, -0.2529, -1.1998]],
       dtype=torch.float64)
	q_value: tensor([[-15.5307]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25569541836250853, distance: 0.9872611321285988 entropy 1.448572039604187
epoch: 8, step: 1
	action: tensor([[-0.5370,  1.4521, -2.5477, -0.0880, -1.3909,  2.0585,  0.3755]],
       dtype=torch.float64)
	q_value: tensor([[-9.1669]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6655037346985853, distance: 0.6618389614888465 entropy 1.448572039604187
epoch: 8, step: 2
	action: tensor([[ 1.5030, -0.3370,  0.3080, -2.0622, -0.8540,  2.4792,  1.0278]],
       dtype=torch.float64)
	q_value: tensor([[-22.7917]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 8, step: 3
	action: tensor([[-0.4126, -0.6521,  0.2072, -0.3847,  1.1093,  1.3823,  0.7788]],
       dtype=torch.float64)
	q_value: tensor([[-15.5307]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11333477825697491, distance: 1.207451182427869 entropy 1.448572039604187
epoch: 8, step: 4
	action: tensor([[-0.2020,  0.9031, -1.2017, -0.3986,  0.4927,  0.1304,  0.0937]],
       dtype=torch.float64)
	q_value: tensor([[-12.0713]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6711668111799811, distance: 0.6562125247749409 entropy 1.448572039604187
epoch: 8, step: 5
	action: tensor([[ 1.3493,  1.3682,  1.4294, -0.8434,  0.6396, -0.4912, -0.5943]],
       dtype=torch.float64)
	q_value: tensor([[-9.4502]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7953304386406063, distance: 0.5177061482500739 entropy 1.448572039604187
epoch: 8, step: 6
	action: tensor([[ 0.5690, -0.7104, -1.5527, -0.7970, -0.5329,  0.8619, -0.5560]],
       dtype=torch.float64)
	q_value: tensor([[-11.9576]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0022435808377110344, distance: 1.1456272491973323 entropy 1.448572039604187
epoch: 8, step: 7
	action: tensor([[ 0.0835, -0.6831, -0.2455,  0.9087,  0.2116, -0.5972, -0.2635]],
       dtype=torch.float64)
	q_value: tensor([[-11.3310]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04466573179736011, distance: 1.1184958355288845 entropy 1.448572039604187
epoch: 8, step: 8
	action: tensor([[ 0.4658,  0.2414,  1.3254, -0.7498,  0.7244,  1.4077,  0.4069]],
       dtype=torch.float64)
	q_value: tensor([[-8.3368]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6956163512935154, distance: 0.6313458827668122 entropy 1.448572039604187
epoch: 8, step: 9
	action: tensor([[-0.5864,  1.0203,  0.1582,  0.5258, -1.8232, -2.9441, -1.0746]],
       dtype=torch.float64)
	q_value: tensor([[-11.8199]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7345958935484432, distance: 0.589536246973197 entropy 1.448572039604187
epoch: 8, step: 10
	action: tensor([[ 0.2758, -0.4145, -1.7938, -0.3249, -0.5109, -0.9175,  0.1492]],
       dtype=torch.float64)
	q_value: tensor([[-15.7127]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6476853038403174, distance: 0.6792381539101486 entropy 1.448572039604187
epoch: 8, step: 11
	action: tensor([[ 0.0467, -1.1644, -0.1517, -0.3045,  0.3309, -0.8932,  0.5255]],
       dtype=torch.float64)
	q_value: tensor([[-11.6098]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8483007301293346, distance: 1.5557614673484181 entropy 1.448572039604187
epoch: 8, step: 12
	action: tensor([[-0.7050,  0.2265, -0.1713, -0.5182, -1.7723,  0.3482, -0.1094]],
       dtype=torch.float64)
	q_value: tensor([[-10.4857]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7087010447497653, distance: 1.4958558208480213 entropy 1.448572039604187
epoch: 8, step: 13
	action: tensor([[ 0.1768,  1.0322,  0.2836, -1.3037,  0.6050, -1.4438, -0.6982]],
       dtype=torch.float64)
	q_value: tensor([[-14.6443]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.305624129407959, distance: 0.9535731272479008 entropy 1.448572039604187
epoch: 8, step: 14
	action: tensor([[-1.1707,  0.5478,  1.0229, -0.0386,  0.2923,  1.5461,  1.8209]],
       dtype=torch.float64)
	q_value: tensor([[-10.5411]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.024894093652640947, distance: 1.158500400981782 entropy 1.448572039604187
epoch: 8, step: 15
	action: tensor([[ 5.3920e-01, -1.5423e-01, -1.1834e+00,  2.6461e-04,  1.2508e+00,
         -1.7221e+00,  1.3955e+00]], dtype=torch.float64)
	q_value: tensor([[-16.9911]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3171058354838111, distance: 0.9456564608586818 entropy 1.448572039604187
epoch: 8, step: 16
	action: tensor([[ 2.0081, -1.1251,  1.9386, -1.5604,  0.3891, -1.3438,  1.2249]],
       dtype=torch.float64)
	q_value: tensor([[-14.6464]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 8, step: 17
	action: tensor([[ 2.0787, -1.8990,  1.5599, -0.2827, -0.2496,  0.1468,  0.4349]],
       dtype=torch.float64)
	q_value: tensor([[-15.5307]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 8, step: 18
	action: tensor([[-0.1976,  3.3673,  1.2284,  0.8786,  0.1966,  1.3570,  1.1867]],
       dtype=torch.float64)
	q_value: tensor([[-15.5307]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 8, step: 19
	action: tensor([[ 1.6898, -0.7177, -1.1743, -0.9671,  1.3034,  0.2362, -0.1045]],
       dtype=torch.float64)
	q_value: tensor([[-15.5307]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.651124158992284, distance: 1.4704374642150448 entropy 1.448572039604187
epoch: 8, step: 20
	action: tensor([[-1.3911, -0.4178,  0.4286,  0.0738, -0.9985,  1.1605, -1.8114]],
       dtype=torch.float64)
	q_value: tensor([[-10.6752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.346512691733583, distance: 1.752944423058994 entropy 1.448572039604187
epoch: 8, step: 21
	action: tensor([[ 0.1487, -1.9417,  1.0949,  0.0923,  0.6114,  0.3860,  1.1669]],
       dtype=torch.float64)
	q_value: tensor([[-14.0275]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 8, step: 22
	action: tensor([[ 0.8038,  0.0688, -0.9477,  1.1853,  0.8074,  1.7167, -1.2840]],
       dtype=torch.float64)
	q_value: tensor([[-15.5307]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7877122512663927, distance: 0.5272531208057869 entropy 1.448572039604187
epoch: 8, step: 23
	action: tensor([[ 0.6270, -1.4769, -0.0161, -0.7685, -0.9942,  1.2590,  0.9717]],
       dtype=torch.float64)
	q_value: tensor([[-11.9021]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6581659865915703, distance: 1.4735697393759104 entropy 1.448572039604187
epoch: 8, step: 24
	action: tensor([[-0.4112, -0.3488, -0.2167,  1.4641,  0.2672, -0.4345, -0.8436]],
       dtype=torch.float64)
	q_value: tensor([[-15.3497]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.035663754771812495, distance: 1.1237531922100932 entropy 1.448572039604187
epoch: 8, step: 25
	action: tensor([[-0.4576, -0.2235,  1.1000,  0.5210, -1.3881, -0.2932,  2.1198]],
       dtype=torch.float64)
	q_value: tensor([[-9.5017]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.024614116032152467, distance: 1.1301729963152747 entropy 1.448572039604187
epoch: 8, step: 26
	action: tensor([[-0.3998, -0.4680,  1.0432,  1.0497, -0.6465, -0.4543, -0.5709]],
       dtype=torch.float64)
	q_value: tensor([[-18.0474]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23279715903391474, distance: 1.002332458784178 entropy 1.448572039604187
epoch: 8, step: 27
	action: tensor([[ 0.6777,  1.6957,  0.2281,  0.2800,  1.5767, -0.2318, -1.8119]],
       dtype=torch.float64)
	q_value: tensor([[-10.6472]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 8, step: 28
	action: tensor([[-0.7072,  0.6406,  1.3672,  0.2654,  1.4727,  0.1615,  1.4386]],
       dtype=torch.float64)
	q_value: tensor([[-15.5307]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05929074343769902, distance: 1.1099014071941653 entropy 1.448572039604187
epoch: 8, step: 29
	action: tensor([[-0.6684,  0.6222,  0.7396, -2.1700, -1.1217,  0.0512, -0.3239]],
       dtype=torch.float64)
	q_value: tensor([[-13.4382]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6426877432614373, distance: 1.4666760546282869 entropy 1.448572039604187
epoch: 8, step: 30
	action: tensor([[ 0.8619,  0.5300,  0.1211, -0.0944, -0.3483, -2.1723, -1.8966]],
       dtype=torch.float64)
	q_value: tensor([[-17.3083]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8074024383914415, distance: 0.5022062178933083 entropy 1.448572039604187
epoch: 8, step: 31
	action: tensor([[ 0.4261,  0.0213, -0.1240, -0.5423,  0.0013, -1.1362, -0.8038]],
       dtype=torch.float64)
	q_value: tensor([[-13.5270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24825605467094292, distance: 0.9921827284386646 entropy 1.448572039604187
epoch: 8, step: 32
	action: tensor([[ 0.1338, -1.3194, -0.2862, -0.7539, -1.7754,  1.1629,  0.2034]],
       dtype=torch.float64)
	q_value: tensor([[-8.3056]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8966038158571359, distance: 1.5759593212679464 entropy 1.448572039604187
epoch: 8, step: 33
	action: tensor([[ 1.7329, -0.5262,  1.7958,  0.4656,  1.4366,  2.2513,  1.1559]],
       dtype=torch.float64)
	q_value: tensor([[-16.0419]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.023217431332977156, distance: 1.1309818713911084 entropy 1.448572039604187
epoch: 8, step: 34
	action: tensor([[ 0.3290, -0.6287,  0.1321,  0.7630,  0.8044,  0.2167,  0.1423]],
       dtype=torch.float64)
	q_value: tensor([[-18.3893]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6021377365847539, distance: 0.7218102879495678 entropy 1.448572039604187
epoch: 8, step: 35
	action: tensor([[-0.4146, -0.5136, -0.3048, -0.8433,  0.6988, -1.5428, -0.2756]],
       dtype=torch.float64)
	q_value: tensor([[-8.1692]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4728322311065176, distance: 1.3887796147356022 entropy 1.448572039604187
epoch: 8, step: 36
	action: tensor([[ 0.2078,  0.2925, -2.2956, -1.4196,  1.4815, -0.4473,  0.4936]],
       dtype=torch.float64)
	q_value: tensor([[-10.5861]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9704619994004468, distance: 0.19667413137600367 entropy 1.448572039604187
epoch: 8, step: 37
	action: tensor([[ 0.0696, -2.0377, -0.1999,  0.5399, -1.2195,  0.8957,  0.6075]],
       dtype=torch.float64)
	q_value: tensor([[-14.9477]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 8, step: 38
	action: tensor([[-1.0161, -0.1801,  0.0310, -2.1444,  1.1199, -0.6334, -0.2041]],
       dtype=torch.float64)
	q_value: tensor([[-15.5307]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.35308003605452853, distance: 1.3311237629070356 entropy 1.448572039604187
epoch: 8, step: 39
	action: tensor([[ 0.3866, -0.5500, -1.6289,  1.2288, -1.1422, -0.5414, -0.6119]],
       dtype=torch.float64)
	q_value: tensor([[-12.5552]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03045648744102003, distance: 1.1616399037430585 entropy 1.448572039604187
epoch: 8, step: 40
	action: tensor([[-0.7107,  0.9873,  0.2327,  1.1392, -0.2390,  1.8639,  0.8537]],
       dtype=torch.float64)
	q_value: tensor([[-11.5592]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4562436204003142, distance: 0.843837605869221 entropy 1.448572039604187
epoch: 8, step: 41
	action: tensor([[-1.1289,  2.5515, -0.4331,  0.1775, -1.4536,  0.1297,  0.8713]],
       dtype=torch.float64)
	q_value: tensor([[-14.3246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 8, step: 42
	action: tensor([[-0.2664,  0.1920, -0.7145, -0.4603, -0.7827,  0.5428,  0.6783]],
       dtype=torch.float64)
	q_value: tensor([[-15.5307]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.027913118865564135, distance: 1.1282601113959096 entropy 1.448572039604187
epoch: 8, step: 43
	action: tensor([[-0.0740, -1.7433,  0.6518,  0.0510, -1.3467, -0.6418,  2.3363]],
       dtype=torch.float64)
	q_value: tensor([[-11.1942]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 8, step: 44
	action: tensor([[ 0.0201, -0.7246,  0.5687,  1.0950, -2.7512,  0.0447,  0.9874]],
       dtype=torch.float64)
	q_value: tensor([[-15.5307]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5688560341718791, distance: 0.7513942189429629 entropy 1.448572039604187
epoch: 8, step: 45
	action: tensor([[-0.1827, -0.5790,  1.3581,  1.4711,  0.3640, -1.1468,  0.1818]],
       dtype=torch.float64)
	q_value: tensor([[-18.6720]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4203069431338725, distance: 0.8712759828172006 entropy 1.448572039604187
epoch: 8, step: 46
	action: tensor([[-0.0296, -0.1303, -1.0942,  2.4326, -0.0463,  1.6543, -1.5420]],
       dtype=torch.float64)
	q_value: tensor([[-13.7421]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2760682058209919, distance: 0.973655942076011 entropy 1.448572039604187
epoch: 8, step: 47
	action: tensor([[-0.6119,  0.9482, -0.0214, -1.2861,  0.1206,  1.8955,  0.4821]],
       dtype=torch.float64)
	q_value: tensor([[-14.2218]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2730333497634446, distance: 0.97569468056911 entropy 1.448572039604187
epoch: 8, step: 48
	action: tensor([[ 0.8415,  0.0882,  0.2490,  0.1095, -0.9227, -1.2499,  0.3322]],
       dtype=torch.float64)
	q_value: tensor([[-16.1166]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6864322875113171, distance: 0.6407997920800705 entropy 1.448572039604187
epoch: 8, step: 49
	action: tensor([[ 1.2488,  0.3776,  0.7252, -0.8446, -0.6311, -2.3402, -1.2786]],
       dtype=torch.float64)
	q_value: tensor([[-10.8561]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.572949411870613, distance: 0.7478187593648736 entropy 1.448572039604187
epoch: 8, step: 50
	action: tensor([[ 1.8543,  0.9448,  0.2944,  2.0724,  0.3858, -1.1090,  0.0941]],
       dtype=torch.float64)
	q_value: tensor([[-14.8708]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 8, step: 51
	action: tensor([[-0.8374,  0.4589,  1.2027, -1.2324, -0.5074,  1.9800, -1.9738]],
       dtype=torch.float64)
	q_value: tensor([[-15.5307]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23030378452170241, distance: 1.269295888356177 entropy 1.448572039604187
epoch: 8, step: 52
	action: tensor([[ 1.8698, -3.3330, -0.1536,  0.8387, -0.9380,  1.6374, -0.0662]],
       dtype=torch.float64)
	q_value: tensor([[-18.2824]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 8, step: 53
	action: tensor([[ 1.5555,  1.0242, -0.5428, -0.8232,  0.2954, -1.8724,  1.3773]],
       dtype=torch.float64)
	q_value: tensor([[-15.5307]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5567847313818997, distance: 0.7618404886978134 entropy 1.448572039604187
epoch: 8, step: 54
	action: tensor([[-0.5479, -0.9640, -0.5300,  2.4207,  1.3797, -0.0840, -0.6718]],
       dtype=torch.float64)
	q_value: tensor([[-14.7572]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.00692420094077284, distance: 1.1482992542863244 entropy 1.448572039604187
epoch: 8, step: 55
	action: tensor([[ 0.1134, -0.5300, -0.5737, -2.3694,  2.0581,  0.8177, -0.3524]],
       dtype=torch.float64)
	q_value: tensor([[-14.3902]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3704119899528968, distance: 0.9079980536931617 entropy 1.448572039604187
epoch: 8, step: 56
	action: tensor([[-0.2866, -0.8936, -0.7021,  0.0577, -1.4157,  0.5991, -1.3429]],
       dtype=torch.float64)
	q_value: tensor([[-12.9588]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9424556384925122, distance: 1.5948955558300422 entropy 1.448572039604187
epoch: 8, step: 57
	action: tensor([[ 1.0052,  1.3009,  0.3611,  3.3067,  1.9526, -0.9579, -1.2167]],
       dtype=torch.float64)
	q_value: tensor([[-12.0855]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 8, step: 58
	action: tensor([[-0.0216,  1.2550, -0.1544,  1.4381,  1.1156, -0.9580, -0.0145]],
       dtype=torch.float64)
	q_value: tensor([[-15.5307]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03453360070907707, distance: 1.1244114907866996 entropy 1.448572039604187
epoch: 8, step: 59
	action: tensor([[ 0.7806, -1.1503, -0.2960,  1.0594,  0.4283, -0.3059,  0.6376]],
       dtype=torch.float64)
	q_value: tensor([[-9.6564]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2611372783828769, distance: 0.983645412713905 entropy 1.448572039604187
epoch: 8, step: 60
	action: tensor([[-0.3642,  1.3090, -2.1423, -1.0090, -2.1459,  0.8870,  0.7077]],
       dtype=torch.float64)
	q_value: tensor([[-11.1022]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9451236681854264, distance: 0.2680706269117164 entropy 1.448572039604187
epoch: 8, step: 61
	action: tensor([[ 0.5577,  0.0475, -0.1802, -0.2420, -0.3571, -1.1214, -0.1802]],
       dtype=torch.float64)
	q_value: tensor([[-23.7252]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5238197253834111, distance: 0.7896640989733237 entropy 1.448572039604187
epoch: 8, step: 62
	action: tensor([[ 0.9755,  1.6513,  1.3650, -1.1925, -2.1115, -1.6882, -0.1227]],
       dtype=torch.float64)
	q_value: tensor([[-8.4266]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8955295303083208, distance: 0.3698737129048693 entropy 1.448572039604187
epoch: 8, step: 63
	action: tensor([[ 1.7391, -1.1253,  0.3098, -0.1763, -0.6312, -0.0639,  0.0177]],
       dtype=torch.float64)
	q_value: tensor([[-20.7622]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.789209783269667, distance: 1.5306902833961233 entropy 1.448572039604187
epoch: 8, step: 64
	action: tensor([[ 0.0941,  0.5472,  0.1726,  0.1945, -0.5139, -0.7646,  0.4959]],
       dtype=torch.float64)
	q_value: tensor([[-11.4408]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7586854922638343, distance: 0.5621450740935293 entropy 1.448572039604187
epoch: 8, step: 65
	action: tensor([[ 1.0824, -0.9524, -0.5637, -0.7824, -0.6592, -0.8905,  1.8303]],
       dtype=torch.float64)
	q_value: tensor([[-8.3069]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6156398860570724, distance: 1.4545510775625379 entropy 1.448572039604187
epoch: 8, step: 66
	action: tensor([[ 0.7105,  0.3740, -0.2345,  1.1135, -0.0303, -1.7263,  2.4886]],
       dtype=torch.float64)
	q_value: tensor([[-15.8628]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9714141987002677, distance: 0.19347812882604914 entropy 1.448572039604187
epoch: 8, step: 67
	action: tensor([[ 0.4428,  0.1023, -0.6281, -1.5872,  0.2537, -0.5319, -1.1073]],
       dtype=torch.float64)
	q_value: tensor([[-17.5949]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1804336612822296, distance: 1.0359737462916616 entropy 1.448572039604187
epoch: 8, step: 68
	action: tensor([[ 0.9117,  1.0905, -1.5289,  0.6280, -1.7177,  0.6193, -0.8666]],
       dtype=torch.float64)
	q_value: tensor([[-9.6480]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9019128124989093, distance: 0.35839573620263127 entropy 1.448572039604187
epoch: 8, step: 69
	action: tensor([[ 1.0245,  0.9154, -0.4612, -0.7654, -0.2488, -2.3574, -0.3438]],
       dtype=torch.float64)
	q_value: tensor([[-15.2038]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7912441195261031, distance: 0.5228487237767437 entropy 1.448572039604187
epoch: 8, step: 70
	action: tensor([[-0.7878,  1.2742,  0.8019, -2.2229, -0.4898,  0.0428,  2.4250]],
       dtype=torch.float64)
	q_value: tensor([[-13.1393]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.76620012373413, distance: 1.5208159150744218 entropy 1.448572039604187
epoch: 8, step: 71
	action: tensor([[-1.8478,  0.5119,  0.3735,  0.0294, -0.3490,  0.6562,  1.3306]],
       dtype=torch.float64)
	q_value: tensor([[-22.6608]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 8, step: 72
	action: tensor([[ 1.3842, -0.9806, -1.2201, -0.5600, -0.7278,  1.4018, -1.5720]],
       dtype=torch.float64)
	q_value: tensor([[-15.5307]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.349106415241363, distance: 1.329167755123999 entropy 1.448572039604187
epoch: 8, step: 73
	action: tensor([[ 0.3118, -0.8976, -1.0684,  1.5494, -2.5385, -0.6623, -0.0195]],
       dtype=torch.float64)
	q_value: tensor([[-14.0562]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2604393386862155, distance: 0.9841098854178195 entropy 1.448572039604187
epoch: 8, step: 74
	action: tensor([[ 0.0638,  0.1353, -1.1758, -1.3940, -0.5981,  0.6394,  0.9279]],
       dtype=torch.float64)
	q_value: tensor([[-16.0369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5915660971842451, distance: 0.7313370665365263 entropy 1.448572039604187
epoch: 8, step: 75
	action: tensor([[-0.1117,  0.8486, -1.4136, -1.1448, -0.2074, -1.2034, -0.3046]],
       dtype=torch.float64)
	q_value: tensor([[-14.9380]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8930284053869099, distance: 0.374275094066543 entropy 1.448572039604187
epoch: 8, step: 76
	action: tensor([[-1.4369,  0.7699, -0.2894,  0.3870,  1.1895, -0.5323,  1.1590]],
       dtype=torch.float64)
	q_value: tensor([[-12.7189]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7345905360824125, distance: 1.5071454943373144 entropy 1.448572039604187
epoch: 8, step: 77
	action: tensor([[-2.2657,  1.2915, -0.5712, -0.6103, -0.8104,  1.5148, -0.3256]],
       dtype=torch.float64)
	q_value: tensor([[-12.4761]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 8, step: 78
	action: tensor([[ 1.3989, -0.3460,  0.5825, -0.5881,  3.2578,  0.7188,  0.9747]],
       dtype=torch.float64)
	q_value: tensor([[-15.5307]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0948032414342759 entropy 1.448572039604187
epoch: 8, step: 79
	action: tensor([[ 0.1252,  1.0235, -0.4728,  0.7986, -0.3144, -0.1133, -0.6610]],
       dtype=torch.float64)
	q_value: tensor([[-15.5307]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 8, step: 80
	action: tensor([[-1.1107,  0.6943, -0.2821,  1.0313,  1.4270, -0.0285, -1.3848]],
       dtype=torch.float64)
	q_value: tensor([[-15.5307]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 8, step: 81
	action: tensor([[ 0.5724, -0.9779, -1.3289, -0.4936,  1.5876, -1.8866,  2.5882]],
       dtype=torch.float64)
	q_value: tensor([[-15.5307]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3027410229342109, distance: 1.3061279944372541 entropy 1.448572039604187
epoch: 8, step: 82
	action: tensor([[ 0.3167, -0.3321,  0.7458, -0.6451, -0.5213,  0.4936, -0.4486]],
       dtype=torch.float64)
	q_value: tensor([[-20.8923]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.003455696039179168, distance: 1.142365289892804 entropy 1.448572039604187
epoch: 8, step: 83
	action: tensor([[ 0.0632,  0.0517, -0.2272,  1.2373, -0.8469,  0.3992,  0.7948]],
       dtype=torch.float64)
	q_value: tensor([[-9.7644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5067800585395283, distance: 0.8036686120538927 entropy 1.448572039604187
epoch: 8, step: 84
	action: tensor([[ 0.4864, -0.4125, -0.7634,  0.6617, -1.2824, -1.0317, -0.5643]],
       dtype=torch.float64)
	q_value: tensor([[-10.0931]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7223446656010908, distance: 0.6029894366606767 entropy 1.448572039604187
epoch: 8, step: 85
	action: tensor([[-0.6490,  2.6547, -1.0998, -0.1298, -0.5755,  0.0482, -0.8252]],
       dtype=torch.float64)
	q_value: tensor([[-10.6822]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 8, step: 86
	action: tensor([[-0.4244,  0.4379, -0.7395, -1.2380,  0.0275, -1.1825, -0.2527]],
       dtype=torch.float64)
	q_value: tensor([[-15.5307]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3961472624936718, distance: 0.8892465992896479 entropy 1.448572039604187
epoch: 8, step: 87
	action: tensor([[ 1.0124,  0.6645, -0.2122, -0.8366, -0.1436, -0.9154, -0.3699]],
       dtype=torch.float64)
	q_value: tensor([[-10.5294]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6870563277728712, distance: 0.6401618373293444 entropy 1.448572039604187
epoch: 8, step: 88
	action: tensor([[-0.9241,  1.5407, -1.8765, -0.1906, -0.6193, -0.1814, -0.9926]],
       dtype=torch.float64)
	q_value: tensor([[-9.3854]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 8, step: 89
	action: tensor([[ 0.7807, -0.0271,  1.2203,  0.7337, -0.0085, -0.1511, -1.3972]],
       dtype=torch.float64)
	q_value: tensor([[-15.5307]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.812766545804654, distance: 0.4951632651149988 entropy 1.448572039604187
epoch: 8, step: 90
	action: tensor([[ 1.4550,  0.2787, -0.6867,  0.7324,  0.7794,  1.8332, -1.6795]],
       dtype=torch.float64)
	q_value: tensor([[-10.1080]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8121043146318001, distance: 0.49603817048946436 entropy 1.448572039604187
epoch: 8, step: 91
	action: tensor([[-1.8031,  0.5525,  0.9200, -0.8392, -1.1387, -0.1845,  0.0447]],
       dtype=torch.float64)
	q_value: tensor([[-13.2569]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.6134232839566107, distance: 1.8499567334827818 entropy 1.448572039604187
epoch: 8, step: 92
	action: tensor([[-1.7252, -0.1937, -0.2089, -0.1229,  0.4649,  0.2016, -1.5764]],
       dtype=torch.float64)
	q_value: tensor([[-14.9086]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 8, step: 93
	action: tensor([[ 0.2686,  0.1976, -0.1007, -0.3616, -0.6547, -0.0899, -2.0067]],
       dtype=torch.float64)
	q_value: tensor([[-15.5307]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4987998442887739, distance: 0.8101441342532547 entropy 1.448572039604187
epoch: 8, step: 94
	action: tensor([[ 1.4912,  1.3144,  0.2560, -1.8544,  0.2178, -0.7427,  0.4165]],
       dtype=torch.float64)
	q_value: tensor([[-10.1695]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6176943920419142, distance: 0.7075579705304141 entropy 1.448572039604187
epoch: 8, step: 95
	action: tensor([[-1.0151, -0.9375,  2.1559, -1.0338, -0.5645,  1.0555,  0.6634]],
       dtype=torch.float64)
	q_value: tensor([[-14.6573]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2692511574784495, distance: 1.7238440575038656 entropy 1.448572039604187
epoch: 8, step: 96
	action: tensor([[-2.0331,  0.7458,  0.9491,  1.9677, -0.7302, -1.1027, -0.6407]],
       dtype=torch.float64)
	q_value: tensor([[-18.9149]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 8, step: 97
	action: tensor([[ 0.3538,  1.0438,  0.4159, -0.2703, -0.6741,  1.7364, -0.2629]],
       dtype=torch.float64)
	q_value: tensor([[-15.5307]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5975832040277361, distance: 0.7259299969295008 entropy 1.448572039604187
epoch: 8, step: 98
	action: tensor([[ 1.3141, -0.3537, -1.4048,  0.1700,  1.2691, -0.1236,  0.0915]],
       dtype=torch.float64)
	q_value: tensor([[-14.3366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3480887818085362, distance: 0.9239552138688255 entropy 1.448572039604187
epoch: 8, step: 99
	action: tensor([[ 0.2872,  0.2618,  0.6839, -0.2276,  0.0943,  0.7449, -0.8575]],
       dtype=torch.float64)
	q_value: tensor([[-10.3232]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7858211696890621, distance: 0.5295963271012268 entropy 1.448572039604187
epoch: 8, step: 100
	action: tensor([[ 0.7953, -1.4771, -1.2965, -1.3088,  0.1378,  0.9676, -0.4427]],
       dtype=torch.float64)
	q_value: tensor([[-7.9870]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.36785385175204377, distance: 1.3383710752365532 entropy 1.448572039604187
epoch: 8, step: 101
	action: tensor([[-0.8165,  0.5807, -0.7524,  0.3833,  1.2838,  1.6778,  0.7209]],
       dtype=torch.float64)
	q_value: tensor([[-11.3492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37843540835243095, distance: 0.9021937754087993 entropy 1.448572039604187
epoch: 8, step: 102
	action: tensor([[-0.4326, -0.9599,  0.6416,  0.9051,  1.2347, -1.2757, -0.2550]],
       dtype=torch.float64)
	q_value: tensor([[-13.0907]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.33819369574145863, distance: 1.3237811207532604 entropy 1.448572039604187
epoch: 8, step: 103
	action: tensor([[ 0.0823,  1.6217, -0.6140,  1.1070,  0.2698, -1.0337,  1.6427]],
       dtype=torch.float64)
	q_value: tensor([[-12.4860]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 8, step: 104
	action: tensor([[ 1.0758,  1.0176, -0.0562, -0.9529, -0.1788, -0.6898,  0.1489]],
       dtype=torch.float64)
	q_value: tensor([[-15.5307]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8706932581132895, distance: 0.4114975726986922 entropy 1.448572039604187
epoch: 8, step: 105
	action: tensor([[-0.7083,  1.4154, -0.2161,  0.8494,  0.7654,  0.0122,  0.5537]],
       dtype=torch.float64)
	q_value: tensor([[-10.6079]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 8, step: 106
	action: tensor([[-0.1978, -1.7236, -1.0833, -0.1828,  0.4358, -0.5730, -2.0668]],
       dtype=torch.float64)
	q_value: tensor([[-15.5307]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7305807517422223, distance: 1.5054024821251448 entropy 1.448572039604187
epoch: 8, step: 107
	action: tensor([[-0.3826,  0.2712, -0.4849,  1.4517,  0.6067, -0.0572, -1.7567]],
       dtype=torch.float64)
	q_value: tensor([[-10.2949]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.059460510468802186, distance: 1.1098012523549945 entropy 1.448572039604187
epoch: 8, step: 108
	action: tensor([[-0.5286, -0.0650, -0.3850,  2.7861, -1.8607,  1.2358, -1.0690]],
       dtype=torch.float64)
	q_value: tensor([[-10.4508]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 8, step: 109
	action: tensor([[-0.3455, -0.2734,  0.3870, -0.2262,  0.1186, -0.0719,  1.9119]],
       dtype=torch.float64)
	q_value: tensor([[-15.5307]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5450524604638816, distance: 1.42242150062616 entropy 1.448572039604187
epoch: 8, step: 110
	action: tensor([[-0.3070,  0.6771, -0.7150,  0.4026, -2.0977,  0.1395, -0.0908]],
       dtype=torch.float64)
	q_value: tensor([[-13.9834]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15328407960065504, distance: 1.0529931745307295 entropy 1.448572039604187
epoch: 8, step: 111
	action: tensor([[ 0.7048, -1.3964, -0.3860,  0.6905, -0.0212,  0.0660, -1.1936]],
       dtype=torch.float64)
	q_value: tensor([[-14.6176]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.01571925934822227, distance: 1.1533033059227076 entropy 1.448572039604187
epoch: 8, step: 112
	action: tensor([[ 2.8011,  0.9788, -0.6526, -0.7077,  0.7333,  0.1203,  0.9688]],
       dtype=torch.float64)
	q_value: tensor([[-9.3930]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 8, step: 113
	action: tensor([[ 1.2123,  0.4428, -0.6641,  0.8734,  0.2155,  0.6157, -0.8891]],
       dtype=torch.float64)
	q_value: tensor([[-15.5307]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9057655338986703, distance: 0.3512865973384766 entropy 1.448572039604187
epoch: 8, step: 114
	action: tensor([[0.5121, 1.8787, 0.5889, 0.4867, 0.4544, 1.8356, 0.0739]],
       dtype=torch.float64)
	q_value: tensor([[-9.0509]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 8, step: 115
	action: tensor([[ 1.1918, -2.8283,  0.3759, -1.2198,  1.4587, -0.2760,  1.0906]],
       dtype=torch.float64)
	q_value: tensor([[-15.5307]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 8, step: 116
	action: tensor([[-2.0957,  0.4084, -1.6005,  0.3087,  0.7073, -0.8658, -0.2289]],
       dtype=torch.float64)
	q_value: tensor([[-15.5307]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 8, step: 117
	action: tensor([[-0.1100,  0.1158, -0.5470, -2.8818,  1.8117,  0.4373, -0.7222]],
       dtype=torch.float64)
	q_value: tensor([[-15.5307]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 8, step: 118
	action: tensor([[-1.0820e-01, -4.0578e-01,  5.1226e-01,  4.1846e-01, -1.4875e+00,
          1.1157e+00, -9.4655e-04]], dtype=torch.float64)
	q_value: tensor([[-15.5307]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11561243885073913, distance: 1.0761628755387633 entropy 1.448572039604187
epoch: 8, step: 119
	action: tensor([[-0.2628, -0.3198,  0.8246,  0.4883,  0.7534, -1.7454,  0.4221]],
       dtype=torch.float64)
	q_value: tensor([[-13.2154]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12568520629711055, distance: 1.2141299506458227 entropy 1.448572039604187
epoch: 8, step: 120
	action: tensor([[-1.2885, -1.6561, -0.7633,  1.4460,  1.7720,  1.9527,  0.0779]],
       dtype=torch.float64)
	q_value: tensor([[-12.8085]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 8, step: 121
	action: tensor([[ 0.1964,  0.7337, -0.8638, -0.0259,  1.5240,  1.7908, -1.1291]],
       dtype=torch.float64)
	q_value: tensor([[-15.5307]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8681949710473497, distance: 0.41545375029983955 entropy 1.448572039604187
epoch: 8, step: 122
	action: tensor([[ 1.7452,  0.1557, -0.8138, -1.1106,  1.0262,  0.1062, -0.1217]],
       dtype=torch.float64)
	q_value: tensor([[-11.4070]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1782946544919224, distance: 1.2421774672245498 entropy 1.448572039604187
epoch: 8, step: 123
	action: tensor([[ 0.9809, -0.5665,  0.0246,  0.2666,  2.4661,  1.3060,  0.6471]],
       dtype=torch.float64)
	q_value: tensor([[-10.5418]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14167419192824637, distance: 1.0601877425728226 entropy 1.448572039604187
epoch: 8, step: 124
	action: tensor([[-1.4673, -0.2555, -2.7005,  0.1685, -1.5632, -1.1353,  1.0416]],
       dtype=torch.float64)
	q_value: tensor([[-13.5795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.5887010544157372, distance: 1.841185913612747 entropy 1.448572039604187
epoch: 8, step: 125
	action: tensor([[ 0.2886,  0.5059,  1.1823,  0.3818, -0.2244, -1.2000, -0.8342]],
       dtype=torch.float64)
	q_value: tensor([[-20.4982]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7295816554087661, distance: 0.5950791968603236 entropy 1.448572039604187
epoch: 8, step: 126
	action: tensor([[ 0.4347,  0.5671, -0.1450,  0.4559,  2.1516, -0.1924, -1.5532]],
       dtype=torch.float64)
	q_value: tensor([[-10.3136]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7973270703904641, distance: 0.5151747462718178 entropy 1.448572039604187
epoch: 8, step: 127
	action: tensor([[-1.1518, -0.5516, -0.2268,  0.5998,  0.8502, -1.0678,  0.1901]],
       dtype=torch.float64)
	q_value: tensor([[-9.6119]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.4401852667688457, distance: 1.78759072891285 entropy 1.448572039604187
LOSS epoch 8 actor 155.08517285964558 critic 895.379752174182 
epoch: 9, step: 0
	action: tensor([[-0.8001,  0.4496, -0.5806,  0.5726,  1.0362,  0.1189, -0.0964]],
       dtype=torch.float64)
	q_value: tensor([[-17.5026]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3005477287174707, distance: 1.305028033008338 entropy 1.448572039604187
epoch: 9, step: 1
	action: tensor([[ 0.9386,  1.2913, -2.1373, -1.6523,  1.0365, -0.8325, -0.3651]],
       dtype=torch.float64)
	q_value: tensor([[-12.7879]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2488136605526995, distance: 0.9918146845326569 entropy 1.448572039604187
epoch: 9, step: 2
	action: tensor([[ 1.1614,  0.4799,  1.6577, -1.4797,  2.4250, -1.1714, -0.5254]],
       dtype=torch.float64)
	q_value: tensor([[-23.7713]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9253566895272931, distance: 0.31264546927729137 entropy 1.448572039604187
epoch: 9, step: 3
	action: tensor([[ 0.6818,  0.4409,  2.1084,  1.4513, -0.8362, -0.4775, -0.4615]],
       dtype=torch.float64)
	q_value: tensor([[-19.9129]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6341903470025546, distance: 0.6921245816353301 entropy 1.448572039604187
epoch: 9, step: 4
	action: tensor([[-0.2635, -1.0752,  0.2432,  0.1369,  0.1806,  0.5882, -0.0215]],
       dtype=torch.float64)
	q_value: tensor([[-21.9862]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.48845166011773244, distance: 1.3961242179316113 entropy 1.448572039604187
epoch: 9, step: 5
	action: tensor([[ 1.6088,  1.0599,  1.1147, -0.3816, -0.3396,  0.9720,  0.9413]],
       dtype=torch.float64)
	q_value: tensor([[-12.9089]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7788285024654709, distance: 0.5381722155924904 entropy 1.448572039604187
epoch: 9, step: 6
	action: tensor([[-0.6264,  0.1457,  1.2551, -0.7796, -0.7591, -1.5589, -1.2569]],
       dtype=torch.float64)
	q_value: tensor([[-21.3408]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7174283939245603, distance: 1.4996710665919055 entropy 1.448572039604187
epoch: 9, step: 7
	action: tensor([[ 0.4654, -0.6633,  2.3438,  0.0901,  0.3971,  0.1088,  1.7518]],
       dtype=torch.float64)
	q_value: tensor([[-19.3463]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08817852913590274, distance: 1.1937318173344225 entropy 1.448572039604187
epoch: 9, step: 8
	action: tensor([[-1.1052, -0.3837,  0.2464, -0.3029, -0.1282,  0.6612, -0.5351]],
       dtype=torch.float64)
	q_value: tensor([[-27.4265]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.3981697512488709, distance: 1.7721343904295461 entropy 1.448572039604187
epoch: 9, step: 9
	action: tensor([[ 1.2801, -1.8389,  1.3639, -0.7633, -1.2894, -0.1067,  1.0429]],
       dtype=torch.float64)
	q_value: tensor([[-14.3553]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 9, step: 10
	action: tensor([[-0.6766, -0.4709,  0.8420, -1.2084, -0.9441,  0.7279, -1.2896]],
       dtype=torch.float64)
	q_value: tensor([[-24.0762]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2428089543735026, distance: 1.7137711752303242 entropy 1.448572039604187
epoch: 9, step: 11
	action: tensor([[-0.0581,  1.3370,  0.7500, -0.7417, -0.4273, -0.3750, -1.0002]],
       dtype=torch.float64)
	q_value: tensor([[-21.4450]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25236975421506536, distance: 0.9894642890319417 entropy 1.448572039604187
epoch: 9, step: 12
	action: tensor([[-0.4281, -0.2040,  0.7662,  0.7149, -0.2094,  0.6853,  1.9820]],
       dtype=torch.float64)
	q_value: tensor([[-16.9264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5078668021992403, distance: 0.8027827360777845 entropy 1.448572039604187
epoch: 9, step: 13
	action: tensor([[-0.4912, -0.1554,  0.2021,  0.2129, -0.6434,  0.5413, -0.3900]],
       dtype=torch.float64)
	q_value: tensor([[-22.1919]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2284335527001653, distance: 1.2683307688332226 entropy 1.448572039604187
epoch: 9, step: 14
	action: tensor([[-0.1333, -1.1043,  1.0838,  0.9730,  2.6865, -1.2075,  0.9413]],
       dtype=torch.float64)
	q_value: tensor([[-12.5605]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5472392888194282, distance: 0.770000593614006 entropy 1.448572039604187
epoch: 9, step: 15
	action: tensor([[ 1.7066, -0.8970, -0.4887, -0.7638, -0.9305,  0.7928,  0.9387]],
       dtype=torch.float64)
	q_value: tensor([[-27.7010]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.657495155541945, distance: 1.4732716340166652 entropy 1.448572039604187
epoch: 9, step: 16
	action: tensor([[-1.4814, -0.8516,  0.1930, -1.6115,  0.9524,  0.9087, -0.0845]],
       dtype=torch.float64)
	q_value: tensor([[-21.4090]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5901090968129081, distance: 1.443012703275267 entropy 1.448572039604187
epoch: 9, step: 17
	action: tensor([[-0.3620,  2.4701,  0.0798,  1.3099, -0.5759,  0.0513, -0.7764]],
       dtype=torch.float64)
	q_value: tensor([[-20.7563]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 9, step: 18
	action: tensor([[-1.1006, -0.2496,  0.3526,  0.3347,  1.3757,  0.5603, -0.0477]],
       dtype=torch.float64)
	q_value: tensor([[-24.0762]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8111537514489495, distance: 1.5400483406889174 entropy 1.448572039604187
epoch: 9, step: 19
	action: tensor([[ 0.6512, -0.4723,  0.1742, -0.8353, -0.8558, -0.3433,  1.4190]],
       dtype=torch.float64)
	q_value: tensor([[-14.5632]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3162739097107097, distance: 1.312894502534659 entropy 1.448572039604187
epoch: 9, step: 20
	action: tensor([[-0.3438, -0.5897, -0.0316, -0.2232, -0.0255, -1.6016,  1.5128]],
       dtype=torch.float64)
	q_value: tensor([[-20.5391]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.44902866288683474, distance: 1.3775113356597453 entropy 1.448572039604187
epoch: 9, step: 21
	action: tensor([[-1.6089, -0.6353, -0.1288, -0.5336, -0.5757,  0.1742,  0.5963]],
       dtype=torch.float64)
	q_value: tensor([[-22.2586]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.7494701378874997, distance: 1.8974974321565556 entropy 1.448572039604187
epoch: 9, step: 22
	action: tensor([[ 1.2079,  0.1911, -0.0977,  1.3491,  1.7107, -0.6021, -0.2547]],
       dtype=torch.float64)
	q_value: tensor([[-20.8616]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9163442907869156, distance: 0.33098208388045125 entropy 1.448572039604187
epoch: 9, step: 23
	action: tensor([[ 0.4035, -0.3567,  0.7040, -0.2941,  2.0899, -0.2216,  0.7129]],
       dtype=torch.float64)
	q_value: tensor([[-16.8715]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13376108877756498, distance: 1.065063587859943 entropy 1.448572039604187
epoch: 9, step: 24
	action: tensor([[-0.6384,  1.0035, -1.1145,  1.6768, -0.2849,  2.8849, -0.2724]],
       dtype=torch.float64)
	q_value: tensor([[-17.0144]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 9, step: 25
	action: tensor([[ 0.3390,  0.7567, -1.3064, -1.4638,  0.1038, -1.6427, -1.0572]],
       dtype=torch.float64)
	q_value: tensor([[-24.0762]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6303794480583902, distance: 0.6957204174542162 entropy 1.448572039604187
epoch: 9, step: 26
	action: tensor([[ 1.0983, -0.7936, -0.3616,  0.2059,  0.0475,  0.5407, -1.5377]],
       dtype=torch.float64)
	q_value: tensor([[-20.1113]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2592662797941835, distance: 0.9848900522808371 entropy 1.448572039604187
epoch: 9, step: 27
	action: tensor([[-0.8727,  0.6811,  0.2894, -1.5383, -0.3061, -0.5215,  0.3281]],
       dtype=torch.float64)
	q_value: tensor([[-14.3305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6243115136212831, distance: 1.4584493613672984 entropy 1.448572039604187
epoch: 9, step: 28
	action: tensor([[-0.0049,  1.3531,  0.6607,  1.0855, -1.1880,  1.3843,  1.0383]],
       dtype=torch.float64)
	q_value: tensor([[-19.4419]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 9, step: 29
	action: tensor([[ 1.2575, -0.0384, -0.0725, -0.7765, -0.7128, -0.0561, -0.1595]],
       dtype=torch.float64)
	q_value: tensor([[-24.0762]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1833509456394986, distance: 1.0341283043177167 entropy 1.448572039604187
epoch: 9, step: 30
	action: tensor([[ 2.3820,  0.8448,  0.7658, -0.9336,  0.1162,  0.4561,  1.3906]],
       dtype=torch.float64)
	q_value: tensor([[-14.9524]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 9, step: 31
	action: tensor([[ 0.2434, -0.3163, -1.9908, -1.7235, -1.6869, -0.4577, -0.9404]],
       dtype=torch.float64)
	q_value: tensor([[-24.0762]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9307197010913449, distance: 0.30120457886424845 entropy 1.448572039604187
epoch: 9, step: 32
	action: tensor([[ 0.5696, -0.1753,  0.6438, -2.0082,  1.5112, -1.3666, -1.1745]],
       dtype=torch.float64)
	q_value: tensor([[-27.0209]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11809133978074471, distance: 1.074653597673418 entropy 1.448572039604187
epoch: 9, step: 33
	action: tensor([[1.2447, 1.0509, 0.9911, 1.7469, 1.0092, 1.6127, 0.8764]],
       dtype=torch.float64)
	q_value: tensor([[-18.6822]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30127539703532813, distance: 0.956554482205745 entropy 1.448572039604187
epoch: 9, step: 34
	action: tensor([[-0.4483,  0.6426, -1.2363, -0.2003,  0.7324, -0.7372,  0.0955]],
       dtype=torch.float64)
	q_value: tensor([[-21.6284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.976007080164386, distance: 0.17725489803200592 entropy 1.448572039604187
epoch: 9, step: 35
	action: tensor([[ 0.3529, -0.2768,  0.0705, -0.1634, -1.9647, -1.8791,  0.0422]],
       dtype=torch.float64)
	q_value: tensor([[-14.7621]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10241368917737503, distance: 1.0841635536861387 entropy 1.448572039604187
epoch: 9, step: 36
	action: tensor([[ 1.2125, -0.1919,  0.7841,  1.5267,  1.8070, -0.5804, -0.5224]],
       dtype=torch.float64)
	q_value: tensor([[-23.1283]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4076363396884648, distance: 1.3576940924517864 entropy 1.448572039604187
epoch: 9, step: 37
	action: tensor([[ 1.2918, -1.4938, -2.9377,  1.2941, -1.3079, -1.0707,  2.1690]],
       dtype=torch.float64)
	q_value: tensor([[-18.5917]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 9, step: 38
	action: tensor([[-1.9812,  0.6059,  1.5628,  0.8783, -0.5977,  0.0360, -0.2767]],
       dtype=torch.float64)
	q_value: tensor([[-24.0762]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 9, step: 39
	action: tensor([[-1.4446, -0.5368,  0.1925, -0.0510, -0.5428, -0.7949,  0.5976]],
       dtype=torch.float64)
	q_value: tensor([[-24.0762]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.490096646404119, distance: 1.805779818306776 entropy 1.448572039604187
epoch: 9, step: 40
	action: tensor([[ 0.5833,  0.8849,  2.5320, -0.6322,  1.5220, -1.5617, -1.3245]],
       dtype=torch.float64)
	q_value: tensor([[-19.8639]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7204406843037416, distance: 0.6050533608650069 entropy 1.448572039604187
epoch: 9, step: 41
	action: tensor([[-0.2319, -1.1173,  0.4305,  0.2607,  0.2290,  1.5690, -1.8446]],
       dtype=torch.float64)
	q_value: tensor([[-21.3778]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10689129904132089, distance: 1.0814559975256912 entropy 1.448572039604187
epoch: 9, step: 42
	action: tensor([[-0.0089, -1.5072,  0.4471, -0.8946, -0.3225, -0.2590,  0.4381]],
       dtype=torch.float64)
	q_value: tensor([[-17.3732]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 9, step: 43
	action: tensor([[ 0.0076, -0.1175,  0.8770, -0.0545,  1.5049,  1.0818, -0.0349]],
       dtype=torch.float64)
	q_value: tensor([[-24.0762]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27432356181744233, distance: 0.974828470211894 entropy 1.448572039604187
epoch: 9, step: 44
	action: tensor([[ 0.3701,  0.7781, -1.0399, -1.3506, -0.1410, -2.0554, -1.0948]],
       dtype=torch.float64)
	q_value: tensor([[-13.5812]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6575290751543008, distance: 0.6696818776021888 entropy 1.448572039604187
epoch: 9, step: 45
	action: tensor([[ 0.8309,  1.3070, -0.0496, -1.7327, -1.1085, -0.7056,  0.4412]],
       dtype=torch.float64)
	q_value: tensor([[-20.9134]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7919814727114622, distance: 0.5219245217755442 entropy 1.448572039604187
epoch: 9, step: 46
	action: tensor([[ 2.1419,  0.2912,  1.4048,  0.5149,  0.2956, -0.4995,  0.6280]],
       dtype=torch.float64)
	q_value: tensor([[-25.1252]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 9, step: 47
	action: tensor([[-0.7673, -1.6430, -0.0188, -0.4432, -1.1610, -1.0760,  1.0697]],
       dtype=torch.float64)
	q_value: tensor([[-24.0762]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.467865859560876, distance: 1.386436163949585 entropy 1.448572039604187
epoch: 9, step: 48
	action: tensor([[-1.0669, -1.1248, -0.0020, -0.0422,  0.6738, -0.2607, -1.0991]],
       dtype=torch.float64)
	q_value: tensor([[-24.3597]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.4413366531995595, distance: 1.7880124110297904 entropy 1.448572039604187
epoch: 9, step: 49
	action: tensor([[-1.6145,  0.8154,  0.5411,  1.8852, -0.2181,  0.9894,  1.7933]],
       dtype=torch.float64)
	q_value: tensor([[-14.2667]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6934822817310675, distance: 0.633555237141177 entropy 1.448572039604187
epoch: 9, step: 50
	action: tensor([[ 0.3601,  0.7975,  1.8041, -0.5446,  2.1597, -0.7900,  0.0765]],
       dtype=torch.float64)
	q_value: tensor([[-25.8372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.644511493204434, distance: 0.6822907372973139 entropy 1.448572039604187
epoch: 9, step: 51
	action: tensor([[ 1.2405, -0.9704,  0.8393,  0.4271, -0.8888, -0.6250, -0.7753]],
       dtype=torch.float64)
	q_value: tensor([[-16.9700]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3636491162382487, distance: 1.3363124387361578 entropy 1.448572039604187
epoch: 9, step: 52
	action: tensor([[-1.2166,  0.6416, -0.3912, -1.0825, -0.2270,  0.1135, -0.4580]],
       dtype=torch.float64)
	q_value: tensor([[-18.5064]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9251308246292582, distance: 1.5877671675698721 entropy 1.448572039604187
epoch: 9, step: 53
	action: tensor([[-1.8006, -1.6862, -0.8357, -1.4876, -0.7544, -1.2308,  0.9704]],
       dtype=torch.float64)
	q_value: tensor([[-17.8361]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 9, step: 54
	action: tensor([[-0.5010, -1.2958, -0.4823, -1.5046,  0.3657, -1.1645,  0.0362]],
       dtype=torch.float64)
	q_value: tensor([[-24.0762]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09846452481965606, distance: 1.086545965776871 entropy 1.448572039604187
epoch: 9, step: 55
	action: tensor([[ 0.3082,  0.2777,  1.6299, -0.7515,  1.2694,  0.8512, -1.1812]],
       dtype=torch.float64)
	q_value: tensor([[-18.4451]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3412544789787696, distance: 0.9287857230027325 entropy 1.448572039604187
epoch: 9, step: 56
	action: tensor([[-1.5807, -0.8292,  0.5339, -0.6795,  1.4725,  0.4228,  0.5438]],
       dtype=torch.float64)
	q_value: tensor([[-14.8496]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.5128828246912933, distance: 1.814023096417588 entropy 1.448572039604187
epoch: 9, step: 57
	action: tensor([[ 0.6531,  1.8091, -0.0722, -1.5500, -0.3329,  1.1288,  1.1211]],
       dtype=torch.float64)
	q_value: tensor([[-20.4022]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9224554896284576, distance: 0.31866342697819694 entropy 1.448572039604187
epoch: 9, step: 58
	action: tensor([[-0.4969,  0.4750, -0.8196,  0.4848, -0.6600,  1.0793, -0.2730]],
       dtype=torch.float64)
	q_value: tensor([[-26.0160]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07126051548283996, distance: 1.1028174974387608 entropy 1.448572039604187
epoch: 9, step: 59
	action: tensor([[ 0.7970, -0.0146, -0.7865, -0.2981, -1.3087, -0.2637,  0.2262]],
       dtype=torch.float64)
	q_value: tensor([[-15.7503]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5424863444343282, distance: 0.774031658153335 entropy 1.448572039604187
epoch: 9, step: 60
	action: tensor([[ 0.2066,  1.1309, -0.8116,  1.2162, -1.0197, -1.4281,  1.5900]],
       dtype=torch.float64)
	q_value: tensor([[-16.7428]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 9, step: 61
	action: tensor([[-1.0179,  0.9074,  0.4953,  0.8842,  1.5836,  0.1990,  0.6048]],
       dtype=torch.float64)
	q_value: tensor([[-24.0762]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 9, step: 62
	action: tensor([[-0.4731,  0.2323,  1.4200,  0.0854, -1.9473, -0.9904, -1.3377]],
       dtype=torch.float64)
	q_value: tensor([[-24.0762]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.018251840451173784, distance: 1.1338529678558007 entropy 1.448572039604187
epoch: 9, step: 63
	action: tensor([[-0.9979,  0.4568, -0.5060, -0.7201,  0.3620, -0.7869,  1.0072]],
       dtype=torch.float64)
	q_value: tensor([[-23.0224]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5679339394139, distance: 1.4329154776397066 entropy 1.448572039604187
epoch: 9, step: 64
	action: tensor([[-0.0779,  0.2560,  0.6614, -2.1283,  0.7885, -2.0561,  0.7394]],
       dtype=torch.float64)
	q_value: tensor([[-17.4301]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22441633079944057, distance: 1.0077922661598484 entropy 1.448572039604187
epoch: 9, step: 65
	action: tensor([[ 0.2824,  1.3782,  0.7260, -1.5194,  0.6633, -2.0739, -1.7024]],
       dtype=torch.float64)
	q_value: tensor([[-22.7734]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34663035294614297, distance: 0.9249881537079111 entropy 1.448572039604187
epoch: 9, step: 66
	action: tensor([[ 1.1555, -0.3005,  1.0035, -0.3829,  1.9914,  0.9748,  0.5616]],
       dtype=torch.float64)
	q_value: tensor([[-20.9085]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14032933539793357, distance: 1.061017988112161 entropy 1.448572039604187
epoch: 9, step: 67
	action: tensor([[ 0.9006, -0.2900,  0.5086, -1.4715,  0.1727,  0.3492,  1.1818]],
       dtype=torch.float64)
	q_value: tensor([[-18.2417]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1368441911014664, distance: 1.220132981655752 entropy 1.448572039604187
epoch: 9, step: 68
	action: tensor([[ 1.4785, -0.3895,  0.7024, -1.3132, -0.5373,  0.7709,  2.2995]],
       dtype=torch.float64)
	q_value: tensor([[-19.6123]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19687911542271697, distance: 1.025527127809314 entropy 1.448572039604187
epoch: 9, step: 69
	action: tensor([[ 0.2957,  0.9168,  1.1986, -0.8567, -0.9754,  0.7702,  1.9974]],
       dtype=torch.float64)
	q_value: tensor([[-29.5848]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.43125608249832825, distance: 0.8630085051475883 entropy 1.448572039604187
epoch: 9, step: 70
	action: tensor([[ 0.4161,  0.5191, -0.9377,  1.1719,  0.1785, -1.1951, -0.0575]],
       dtype=torch.float64)
	q_value: tensor([[-27.0858]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6963680635415563, distance: 0.6305658083248297 entropy 1.448572039604187
epoch: 9, step: 71
	action: tensor([[-0.2268, -1.3788, -0.7446,  0.2681,  1.4693, -0.1340, -0.0888]],
       dtype=torch.float64)
	q_value: tensor([[-14.4407]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0181746698718048, distance: 1.6256837655475573 entropy 1.448572039604187
epoch: 9, step: 72
	action: tensor([[-2.3186, -0.1676,  0.1632, -0.6345,  1.3945, -0.1749, -1.4498]],
       dtype=torch.float64)
	q_value: tensor([[-15.7567]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 9, step: 73
	action: tensor([[ 0.7534,  0.7974,  0.4415,  0.3008, -0.1946,  0.1537, -0.6297]],
       dtype=torch.float64)
	q_value: tensor([[-24.0762]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 9, step: 74
	action: tensor([[-0.6685, -0.5606, -1.7847,  0.1541, -0.4478,  1.1721,  0.4767]],
       dtype=torch.float64)
	q_value: tensor([[-24.0762]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.104183470615744, distance: 1.6599633353515022 entropy 1.448572039604187
epoch: 9, step: 75
	action: tensor([[ 1.1120,  1.8972,  0.4256, -0.4082,  0.9047,  0.1301,  0.7389]],
       dtype=torch.float64)
	q_value: tensor([[-20.0881]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 9, step: 76
	action: tensor([[-0.2444,  0.6307, -0.4793, -0.0873,  1.7317,  0.3809, -0.5648]],
       dtype=torch.float64)
	q_value: tensor([[-24.0762]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.47918415811031556, distance: 0.8258454610961572 entropy 1.448572039604187
epoch: 9, step: 77
	action: tensor([[-1.4058,  0.2844,  0.4699, -0.7841,  0.6881, -1.6633, -0.2660]],
       dtype=torch.float64)
	q_value: tensor([[-12.5782]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.940651983094974, distance: 1.5941549185657755 entropy 1.448572039604187
epoch: 9, step: 78
	action: tensor([[ 1.4788,  0.4264, -0.5450, -0.5609,  0.6632, -1.3739,  1.5185]],
       dtype=torch.float64)
	q_value: tensor([[-18.0038]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4470031321253348, distance: 0.8509774051859166 entropy 1.448572039604187
epoch: 9, step: 79
	action: tensor([[ 0.3100,  0.4095,  0.7982, -0.7541,  1.2335,  1.1145,  1.3986]],
       dtype=torch.float64)
	q_value: tensor([[-20.1174]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5753079488009625, distance: 0.7457508542947715 entropy 1.448572039604187
epoch: 9, step: 80
	action: tensor([[ 0.0790, -0.5312, -0.9402, -0.3502,  0.8070, -0.0315,  0.4951]],
       dtype=torch.float64)
	q_value: tensor([[-19.2252]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10807041274121931, distance: 1.2045931039819444 entropy 1.448572039604187
epoch: 9, step: 81
	action: tensor([[ 1.4240,  0.2774, -0.6606, -0.1212,  0.0237, -0.5164, -1.1489]],
       dtype=torch.float64)
	q_value: tensor([[-12.9298]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5245353419067809, distance: 0.789070511651908 entropy 1.448572039604187
epoch: 9, step: 82
	action: tensor([[-0.4997, -0.6646,  0.5405,  0.1575,  0.3073, -0.9210, -0.6323]],
       dtype=torch.float64)
	q_value: tensor([[-13.1068]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9039964927065978, distance: 1.579027760478554 entropy 1.448572039604187
epoch: 9, step: 83
	action: tensor([[ 0.9194, -0.9997, -1.6955, -0.4185,  0.4798, -0.7805,  0.2992]],
       dtype=torch.float64)
	q_value: tensor([[-13.3667]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3048797598856863, distance: 1.3071997034623188 entropy 1.448572039604187
epoch: 9, step: 84
	action: tensor([[-0.2207, -1.4363,  1.3686, -0.2579, -0.6316,  0.0606,  0.1070]],
       dtype=torch.float64)
	q_value: tensor([[-16.8324]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 9, step: 85
	action: tensor([[ 1.2909,  0.4379, -0.8856, -3.0168,  0.7843,  1.1634, -0.4551]],
       dtype=torch.float64)
	q_value: tensor([[-24.0762]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 9, step: 86
	action: tensor([[ 2.6105,  1.5225, -0.5400,  0.1531,  1.6529, -0.5451,  2.1816]],
       dtype=torch.float64)
	q_value: tensor([[-24.0762]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 9, step: 87
	action: tensor([[-0.2976,  0.4319,  1.0765,  0.4942,  0.2248,  1.0991,  0.5879]],
       dtype=torch.float64)
	q_value: tensor([[-24.0762]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8459866210934368, distance: 0.44909261943641093 entropy 1.448572039604187
epoch: 9, step: 88
	action: tensor([[-0.0924,  0.2855, -0.5529, -1.2416, -0.8412,  0.3676, -0.2026]],
       dtype=torch.float64)
	q_value: tensor([[-15.1394]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24071615412563074, distance: 0.9971460502915571 entropy 1.448572039604187
epoch: 9, step: 89
	action: tensor([[-1.2651,  0.1726, -0.8940,  1.8892, -0.7725, -1.0838,  0.1057]],
       dtype=torch.float64)
	q_value: tensor([[-18.6970]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.33430147899344487, distance: 1.3218545704073388 entropy 1.448572039604187
epoch: 9, step: 90
	action: tensor([[ 1.1217, -0.5149, -0.9134, -0.6539,  0.4592,  0.2296, -0.3195]],
       dtype=torch.float64)
	q_value: tensor([[-19.2828]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06080537565005173, distance: 1.1786220159667165 entropy 1.448572039604187
epoch: 9, step: 91
	action: tensor([[-0.7009, -1.8345,  0.0560,  0.6615,  0.7306,  0.5403,  1.3022]],
       dtype=torch.float64)
	q_value: tensor([[-11.8959]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 9, step: 92
	action: tensor([[ 0.3100, -0.9126, -0.0549, -0.5627, -0.3624,  2.3308,  1.3133]],
       dtype=torch.float64)
	q_value: tensor([[-24.0762]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15731598826737836, distance: 1.0504831009416837 entropy 1.448572039604187
epoch: 9, step: 93
	action: tensor([[-0.3404,  1.2913, -0.6308,  0.1964, -0.6655,  0.3944, -1.6906]],
       dtype=torch.float64)
	q_value: tensor([[-26.1674]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 9, step: 94
	action: tensor([[ 2.3764,  0.2249, -2.6629, -1.3345,  0.5781,  1.3160,  1.2376]],
       dtype=torch.float64)
	q_value: tensor([[-24.0762]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.723091205950231, distance: 0.602178253003756 entropy 1.448572039604187
epoch: 9, step: 95
	action: tensor([[-0.8992,  2.6438,  0.8332, -0.0202,  0.3920,  0.1166,  0.1331]],
       dtype=torch.float64)
	q_value: tensor([[-28.7514]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 9, step: 96
	action: tensor([[ 0.6038,  0.4167, -0.6027, -0.2028,  0.7825,  0.1164, -0.1215]],
       dtype=torch.float64)
	q_value: tensor([[-24.0762]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.914031270878432, distance: 0.33552659269323903 entropy 1.448572039604187
epoch: 9, step: 97
	action: tensor([[-1.6308, -0.9856,  1.4888,  0.1424,  0.7128,  0.4333,  1.3404]],
       dtype=torch.float64)
	q_value: tensor([[-9.8289]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.4831350515511996, distance: 1.8032538307890735 entropy 1.448572039604187
epoch: 9, step: 98
	action: tensor([[ 1.0752, -0.3812,  0.8463, -1.4940,  1.8609,  0.3634,  1.4863]],
       dtype=torch.float64)
	q_value: tensor([[-26.6491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16360242431413385, distance: 1.2344087797013765 entropy 1.448572039604187
epoch: 9, step: 99
	action: tensor([[-1.1144, -0.4565,  1.1369, -1.1019, -0.0226,  1.4943, -0.0452]],
       dtype=torch.float64)
	q_value: tensor([[-22.6556]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2307918625554217, distance: 1.7091737692956486 entropy 1.448572039604187
epoch: 9, step: 100
	action: tensor([[-0.0821, -0.7209, -1.0244, -0.6850,  1.3272,  1.8820,  0.1535]],
       dtype=torch.float64)
	q_value: tensor([[-21.4812]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3709475023507375, distance: 0.9076118109057293 entropy 1.448572039604187
epoch: 9, step: 101
	action: tensor([[-0.2803,  0.4936,  1.2768, -0.3648, -1.3754, -1.0295, -0.5376]],
       dtype=torch.float64)
	q_value: tensor([[-20.0584]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.031428205131632136, distance: 1.1621874863305766 entropy 1.448572039604187
epoch: 9, step: 102
	action: tensor([[ 0.7957, -0.6774,  0.1204, -1.0880, -0.0740,  1.7043,  1.2401]],
       dtype=torch.float64)
	q_value: tensor([[-20.3139]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16231005151805633, distance: 1.047365695838901 entropy 1.448572039604187
epoch: 9, step: 103
	action: tensor([[-1.7966, -0.8066, -0.5822, -1.2451, -0.5902,  0.0464,  0.2802]],
       dtype=torch.float64)
	q_value: tensor([[-23.2918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7100003467858744, distance: 1.496424439678717 entropy 1.448572039604187
epoch: 9, step: 104
	action: tensor([[ 0.6691, -0.8637, -1.8802,  0.0294,  0.2058,  1.3102, -0.7316]],
       dtype=torch.float64)
	q_value: tensor([[-22.5719]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15044779234955907, distance: 1.227411394821946 entropy 1.448572039604187
epoch: 9, step: 105
	action: tensor([[-0.8555, -0.1756,  0.4375,  0.7693,  1.4010,  0.5792, -2.1495]],
       dtype=torch.float64)
	q_value: tensor([[-17.2228]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11644967086940428, distance: 1.2091391085679979 entropy 1.448572039604187
epoch: 9, step: 106
	action: tensor([[-0.5784,  1.3817, -0.8018, -1.2980, -0.2816,  1.0723,  0.3739]],
       dtype=torch.float64)
	q_value: tensor([[-16.2062]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5489381809967291, distance: 0.7685546008412506 entropy 1.448572039604187
epoch: 9, step: 107
	action: tensor([[-0.8153,  1.1648, -1.1972,  0.2283,  1.9744, -0.5744, -1.6654]],
       dtype=torch.float64)
	q_value: tensor([[-24.4335]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 9, step: 108
	action: tensor([[ 0.5710,  1.5160, -0.0446,  0.3703, -0.4292,  2.1357,  0.9151]],
       dtype=torch.float64)
	q_value: tensor([[-24.0762]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2704744719922938, distance: 0.9774103646682692 entropy 1.448572039604187
epoch: 9, step: 109
	action: tensor([[ 0.3794,  0.8720, -0.8443,  0.6228, -0.6613, -0.4918,  0.4928]],
       dtype=torch.float64)
	q_value: tensor([[-23.9084]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 9, step: 110
	action: tensor([[-0.1819,  1.8212, -1.7370,  0.1583,  0.0788,  1.8573,  1.1000]],
       dtype=torch.float64)
	q_value: tensor([[-24.0762]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 9, step: 111
	action: tensor([[-0.1148,  0.4378, -2.4216, -0.7740,  0.0502, -0.6473,  0.4074]],
       dtype=torch.float64)
	q_value: tensor([[-24.0762]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9061614666985613, distance: 0.3505478427493267 entropy 1.448572039604187
epoch: 9, step: 112
	action: tensor([[ 0.2652, -2.4309, -1.5477, -1.5770, -1.7170, -1.1378,  0.7275]],
       dtype=torch.float64)
	q_value: tensor([[-22.4402]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 9, step: 113
	action: tensor([[ 1.0675,  0.1226, -0.0416, -0.2795, -0.0599, -0.5066, -0.1885]],
       dtype=torch.float64)
	q_value: tensor([[-24.0762]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5716089414004768, distance: 0.7489915051613565 entropy 1.448572039604187
epoch: 9, step: 114
	action: tensor([[-0.6809,  0.3142, -0.1960,  1.2224,  0.4536, -0.3259, -0.1914]],
       dtype=torch.float64)
	q_value: tensor([[-11.2390]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06367895983775163, distance: 1.1073096495563104 entropy 1.448572039604187
epoch: 9, step: 115
	action: tensor([[ 2.5406, -1.0007, -0.5270,  0.4052,  1.4412, -0.1664, -1.1887]],
       dtype=torch.float64)
	q_value: tensor([[-12.1370]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 9, step: 116
	action: tensor([[-1.6952, -0.4261, -2.0017,  1.0922,  0.5986, -0.7919, -0.2798]],
       dtype=torch.float64)
	q_value: tensor([[-24.0762]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 9, step: 117
	action: tensor([[ 0.1902,  1.6291,  0.3457, -0.9602,  1.1704, -0.8756, -0.2142]],
       dtype=torch.float64)
	q_value: tensor([[-24.0762]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3953544137769699, distance: 0.8898301908735493 entropy 1.448572039604187
epoch: 9, step: 118
	action: tensor([[ 2.3006,  0.1484, -0.4942,  1.2037, -0.6793, -0.6575,  0.6418]],
       dtype=torch.float64)
	q_value: tensor([[-14.6133]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 9, step: 119
	action: tensor([[-0.8425, -1.0925, -1.5086, -1.4309,  0.0654, -0.1974, -1.0933]],
       dtype=torch.float64)
	q_value: tensor([[-24.0762]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.281835598015154, distance: 0.9697697439914108 entropy 1.448572039604187
epoch: 9, step: 120
	action: tensor([[-0.1037, -1.6812, -0.1359,  1.3882,  0.6406,  0.6608, -1.2868]],
       dtype=torch.float64)
	q_value: tensor([[-19.9113]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.448572039604187
epoch: 9, step: 121
	action: tensor([[ 0.7169, -0.9731,  1.2832,  1.3160, -0.8827, -1.1875, -0.7627]],
       dtype=torch.float64)
	q_value: tensor([[-24.0762]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08227797354222788, distance: 1.1904909633846903 entropy 1.448572039604187
epoch: 9, step: 122
	action: tensor([[ 0.2579,  1.0153,  1.4612, -1.1071, -0.8233,  1.2994,  0.6649]],
       dtype=torch.float64)
	q_value: tensor([[-21.8236]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42968792229105246, distance: 0.8641974444687268 entropy 1.448572039604187
epoch: 9, step: 123
	action: tensor([[-0.0726, -0.1643, -1.4683,  1.5461,  0.9439,  0.4861,  0.0775]],
       dtype=torch.float64)
	q_value: tensor([[-25.8460]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2995570060400603, distance: 1.304530870481959 entropy 1.448572039604187
epoch: 9, step: 124
	action: tensor([[ 1.7197, -0.4089, -0.0054,  1.7611, -0.3778,  0.4566,  0.7293]],
       dtype=torch.float64)
	q_value: tensor([[-17.8108]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8010925179423639, distance: 0.5103666095230659 entropy 1.448572039604187
epoch: 9, step: 125
	action: tensor([[-0.5141,  0.4066, -0.7545,  0.3024,  0.5132, -0.4292, -0.4402]],
       dtype=torch.float64)
	q_value: tensor([[-19.9981]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13239155713537665, distance: 1.2177412134353702 entropy 1.448572039604187
epoch: 9, step: 126
	action: tensor([[ 1.2450,  0.3424, -1.0473,  1.1026, -0.9468,  0.2825, -1.7911]],
       dtype=torch.float64)
	q_value: tensor([[-11.2135]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9407564826187108, distance: 0.2785332912377217 entropy 1.448572039604187
epoch: 9, step: 127
	action: tensor([[ 0.4991, -0.3789,  0.0729, -0.1220,  1.5344,  1.1188, -1.0552]],
       dtype=torch.float64)
	q_value: tensor([[-19.3053]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.43965647422189114, distance: 0.8566114447658224 entropy 1.448572039604187
LOSS epoch 9 actor 198.42645892835122 critic 612.5666559500893 
epoch: 10, step: 0
	action: tensor([[ 0.4212,  0.4704, -1.1126,  1.3921,  1.1738,  0.1622, -0.6158]],
       dtype=torch.float64)
	q_value: tensor([[-18.6611]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31567190560760183, distance: 0.9466487771204065 entropy 1.3432114124298096
epoch: 10, step: 1
	action: tensor([[ 1.3209, -0.5511,  0.4629,  0.3342,  0.8615,  0.9536, -1.1494]],
       dtype=torch.float64)
	q_value: tensor([[-22.0703]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2430538521591542, distance: 0.995609850526894 entropy 1.3432114124298096
epoch: 10, step: 2
	action: tensor([[ 1.3661, -0.2865,  0.6258, -1.6131,  0.0597, -1.5941, -1.1499]],
       dtype=torch.float64)
	q_value: tensor([[-21.4979]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21986856752028083, distance: 1.0107426259209402 entropy 1.3432114124298096
epoch: 10, step: 3
	action: tensor([[ 0.7275, -0.0955,  1.5742,  0.9813, -0.7541,  0.6001, -0.0127]],
       dtype=torch.float64)
	q_value: tensor([[-29.8397]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7203003508517503, distance: 0.6052052044434041 entropy 1.3432114124298096
epoch: 10, step: 4
	action: tensor([[-0.4472, -1.0244, -1.8141, -2.1427,  0.2434, -0.7033,  0.5429]],
       dtype=torch.float64)
	q_value: tensor([[-27.3395]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7397786708823422, distance: 0.58375167355072 entropy 1.3432114124298096
epoch: 10, step: 5
	action: tensor([[ 0.3836,  1.4680,  0.1237, -0.2162, -0.5344, -0.3649,  0.5000]],
       dtype=torch.float64)
	q_value: tensor([[-35.6579]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 10, step: 6
	action: tensor([[-0.6840, -2.2784, -0.7151,  0.2790, -0.7344, -0.6700, -0.6741]],
       dtype=torch.float64)
	q_value: tensor([[-35.3292]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 10, step: 7
	action: tensor([[ 0.6662,  0.7959,  0.9526,  2.3694, -0.9821, -0.1503,  0.6333]],
       dtype=torch.float64)
	q_value: tensor([[-35.3292]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4509632686685161, distance: 0.8479249089362979 entropy 1.3432114124298096
epoch: 10, step: 8
	action: tensor([[-2.3958, -0.9418,  1.7776,  0.3928,  0.0038, -0.2514, -0.3077]],
       dtype=torch.float64)
	q_value: tensor([[-29.6733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 10, step: 9
	action: tensor([[-1.6557,  1.2474, -2.0266, -0.3652, -0.5357,  0.4893, -1.2675]],
       dtype=torch.float64)
	q_value: tensor([[-35.3292]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 10, step: 10
	action: tensor([[ 1.3182,  1.1227, -1.7231, -0.7041,  1.1402,  0.9759, -0.6789]],
       dtype=torch.float64)
	q_value: tensor([[-35.3292]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 10, step: 11
	action: tensor([[ 0.0787,  0.4335,  1.8070, -1.1910,  1.0327, -0.4743,  0.3341]],
       dtype=torch.float64)
	q_value: tensor([[-35.3292]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 10, step: 12
	action: tensor([[ 0.5703, -1.8324, -1.1076, -0.9237, -2.1976, -0.4593, -1.2410]],
       dtype=torch.float64)
	q_value: tensor([[-35.3292]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 10, step: 13
	action: tensor([[ 0.7219,  0.9055,  0.1435,  1.1418, -0.4876,  0.3825,  1.0168]],
       dtype=torch.float64)
	q_value: tensor([[-35.3292]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37143170245013024, distance: 0.907262435988166 entropy 1.3432114124298096
epoch: 10, step: 14
	action: tensor([[-0.2140, -0.7239, -0.6428, -0.2430,  0.6098,  0.7456, -0.0575]],
       dtype=torch.float64)
	q_value: tensor([[-21.6513]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5443836340518358, distance: 1.422113596502715 entropy 1.3432114124298096
epoch: 10, step: 15
	action: tensor([[-1.2065e-03, -1.3951e+00,  8.8611e-01, -6.6066e-01, -8.7213e-01,
          1.4653e-01, -1.1826e+00]], dtype=torch.float64)
	q_value: tensor([[-17.4238]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 10, step: 16
	action: tensor([[ 1.1648, -0.0722, -0.4927,  0.2215, -0.3323,  0.2506, -0.7753]],
       dtype=torch.float64)
	q_value: tensor([[-35.3292]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8294836906541658, distance: 0.4725411733855371 entropy 1.3432114124298096
epoch: 10, step: 17
	action: tensor([[-0.8453,  0.4709, -1.2652,  0.5480, -0.6057,  0.0728,  0.7502]],
       dtype=torch.float64)
	q_value: tensor([[-17.2240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7054823570375346, distance: 1.4944462816348343 entropy 1.3432114124298096
epoch: 10, step: 18
	action: tensor([[ 2.1382, -0.7863,  2.5370, -0.0513,  0.4690, -1.3101, -0.6581]],
       dtype=torch.float64)
	q_value: tensor([[-24.9044]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23424350458383458, distance: 1.00138720485768 entropy 1.3432114124298096
epoch: 10, step: 19
	action: tensor([[-0.0123,  0.4529,  1.1954,  0.0117,  0.0976, -0.3637, -0.9489]],
       dtype=torch.float64)
	q_value: tensor([[-36.5660]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4859139842019916, distance: 0.8204924486958538 entropy 1.3432114124298096
epoch: 10, step: 20
	action: tensor([[ 1.4503, -0.9723, -0.1199,  1.0741, -0.4567,  1.4969,  1.9781]],
       dtype=torch.float64)
	q_value: tensor([[-17.4237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7438595224158573, distance: 0.5791563211098552 entropy 1.3432114124298096
epoch: 10, step: 21
	action: tensor([[-1.7339, -0.2543, -0.3166, -0.5642, -1.9139, -0.1258, -0.9663]],
       dtype=torch.float64)
	q_value: tensor([[-36.0142]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.6923048860351813, distance: 1.8776680347779848 entropy 1.3432114124298096
epoch: 10, step: 22
	action: tensor([[ 0.5396, -0.4756, -0.2685,  0.5705, -0.7391, -0.0710,  0.4001]],
       dtype=torch.float64)
	q_value: tensor([[-35.7592]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5579691448710223, distance: 0.760821866536339 entropy 1.3432114124298096
epoch: 10, step: 23
	action: tensor([[ 1.3984,  0.1136, -0.7948,  1.8391,  0.1665, -0.4337,  2.1501]],
       dtype=torch.float64)
	q_value: tensor([[-17.0866]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9214905414109359, distance: 0.32063998865200344 entropy 1.3432114124298096
epoch: 10, step: 24
	action: tensor([[ 0.0498,  0.7852,  0.1839,  0.0378, -0.8369, -0.9897, -0.3570]],
       dtype=torch.float64)
	q_value: tensor([[-31.9767]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.803415950958978, distance: 0.5073770642716787 entropy 1.3432114124298096
epoch: 10, step: 25
	action: tensor([[ 1.2294,  1.1951,  1.0125, -0.0666,  2.1493, -2.3080, -0.4373]],
       dtype=torch.float64)
	q_value: tensor([[-19.9168]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5708076782311893, distance: 0.7496916352550654 entropy 1.3432114124298096
epoch: 10, step: 26
	action: tensor([[ 8.2048e-01,  1.3293e+00,  3.5625e-01, -1.1597e+00,  8.3418e-01,
          7.2277e-04,  6.3247e-01]], dtype=torch.float64)
	q_value: tensor([[-30.3584]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9572133027809809, distance: 0.2367069857846295 entropy 1.3432114124298096
epoch: 10, step: 27
	action: tensor([[ 0.3803,  1.0438,  1.3362,  0.8588, -0.3849, -2.3210, -0.4124]],
       dtype=torch.float64)
	q_value: tensor([[-23.4963]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8429764320432451, distance: 0.45346013601052326 entropy 1.3432114124298096
epoch: 10, step: 28
	action: tensor([[-1.1675,  0.5340, -0.3638, -0.3799, -0.0336, -0.1282,  0.1099]],
       dtype=torch.float64)
	q_value: tensor([[-28.8056]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8152320569289777, distance: 1.5417812846292067 entropy 1.3432114124298096
epoch: 10, step: 29
	action: tensor([[-0.3950,  0.6199,  0.5146, -0.5667,  0.0802,  1.1663,  0.1080]],
       dtype=torch.float64)
	q_value: tensor([[-20.9346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.287903303914424, distance: 0.9656643056707719 entropy 1.3432114124298096
epoch: 10, step: 30
	action: tensor([[-0.8263, -0.5355,  1.5014, -1.5319, -0.8241,  0.2302, -1.5668]],
       dtype=torch.float64)
	q_value: tensor([[-22.7633]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.204972285719363, distance: 1.6992538433109992 entropy 1.3432114124298096
epoch: 10, step: 31
	action: tensor([[-0.5345,  1.2820, -1.5339, -0.4742, -0.0801,  0.1208, -0.8904]],
       dtype=torch.float64)
	q_value: tensor([[-32.4347]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 10, step: 32
	action: tensor([[ 1.7550,  0.3360,  1.2789, -1.8630, -0.2738, -0.4930,  0.3464]],
       dtype=torch.float64)
	q_value: tensor([[-35.3292]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5540923438497973, distance: 0.7641509508566613 entropy 1.3432114124298096
epoch: 10, step: 33
	action: tensor([[ 1.6519, -0.7249, -0.2002, -0.9002, -0.7853,  0.0354,  0.1662]],
       dtype=torch.float64)
	q_value: tensor([[-32.8731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7521501482426043, distance: 1.51475485496704 entropy 1.3432114124298096
epoch: 10, step: 34
	action: tensor([[ 0.5699, -0.4316,  0.5870, -0.8319, -2.3779,  0.5316,  0.7758]],
       dtype=torch.float64)
	q_value: tensor([[-25.1094]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21807224460232777, distance: 1.2629705301033087 entropy 1.3432114124298096
epoch: 10, step: 35
	action: tensor([[-0.3763, -0.4442, -0.8404,  0.8920, -0.1592,  0.3950,  1.9425]],
       dtype=torch.float64)
	q_value: tensor([[-40.6835]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.48651214156551537, distance: 1.3952143155248387 entropy 1.3432114124298096
epoch: 10, step: 36
	action: tensor([[ 3.8672e-01,  6.3295e-01, -1.6524e+00,  7.4277e-04, -5.0972e-01,
         -4.2758e-01, -7.5485e-01]], dtype=torch.float64)
	q_value: tensor([[-30.4163]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9384733579084341, distance: 0.2838496087886957 entropy 1.3432114124298096
epoch: 10, step: 37
	action: tensor([[ 0.8742,  0.2717, -0.5167, -0.6011, -0.3302,  0.8359,  0.5896]],
       dtype=torch.float64)
	q_value: tensor([[-23.1354]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8515246682699154, distance: 0.4409444141908006 entropy 1.3432114124298096
epoch: 10, step: 38
	action: tensor([[ 0.5331, -1.4825, -1.4324,  0.1252,  0.6735, -0.8428, -1.2162]],
       dtype=torch.float64)
	q_value: tensor([[-22.8567]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.690612657294227, distance: 1.487917156094664 entropy 1.3432114124298096
epoch: 10, step: 39
	action: tensor([[ 0.0133, -1.3985, -1.3530, -0.7338,  0.4266,  0.4216, -0.2631]],
       dtype=torch.float64)
	q_value: tensor([[-22.9869]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 10, step: 40
	action: tensor([[-0.3496,  0.5121, -0.3689, -0.4915,  1.3427,  0.3241,  0.5901]],
       dtype=torch.float64)
	q_value: tensor([[-35.3292]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2969162211301747, distance: 0.959533700242237 entropy 1.3432114124298096
epoch: 10, step: 41
	action: tensor([[ 1.3508,  1.1902, -0.4879,  0.0186,  0.9881, -0.5024, -0.2958]],
       dtype=torch.float64)
	q_value: tensor([[-19.9824]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 10, step: 42
	action: tensor([[ 0.7405,  1.2145,  0.5487,  0.1689, -0.3253,  0.3227, -1.3181]],
       dtype=torch.float64)
	q_value: tensor([[-35.3292]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 10, step: 43
	action: tensor([[ 0.5681, -1.0803, -0.1671, -0.6584,  0.7697, -0.9723,  0.2743]],
       dtype=torch.float64)
	q_value: tensor([[-35.3292]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7905061108564144, distance: 1.5312446948394671 entropy 1.3432114124298096
epoch: 10, step: 44
	action: tensor([[-1.6267, -0.3425,  1.3344, -2.0155, -0.4177, -1.4504,  0.9851]],
       dtype=torch.float64)
	q_value: tensor([[-22.3446]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.007248981732869253, distance: 1.140189044747964 entropy 1.3432114124298096
epoch: 10, step: 45
	action: tensor([[ 0.4962, -0.7209, -0.6131,  0.0089, -0.5388, -0.1011, -0.0429]],
       dtype=torch.float64)
	q_value: tensor([[-42.3486]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10023860468581325, distance: 1.2003285414157245 entropy 1.3432114124298096
epoch: 10, step: 46
	action: tensor([[ 0.3659, -0.6832, -0.3448, -1.3659,  0.2497,  1.0988,  1.0332]],
       dtype=torch.float64)
	q_value: tensor([[-16.2778]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2024053727946975, distance: 1.2548220666093448 entropy 1.3432114124298096
epoch: 10, step: 47
	action: tensor([[ 0.6388,  0.7611, -0.5088, -0.2793,  1.3151,  0.6318,  1.0565]],
       dtype=torch.float64)
	q_value: tensor([[-28.3469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9904243692531611, distance: 0.11197998076286192 entropy 1.3432114124298096
epoch: 10, step: 48
	action: tensor([[ 0.3887,  1.4011,  0.1172, -1.3114,  0.0258,  1.4906,  0.6040]],
       dtype=torch.float64)
	q_value: tensor([[-22.2026]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8199071845302768, distance: 0.4856293061207154 entropy 1.3432114124298096
epoch: 10, step: 49
	action: tensor([[-1.0703,  1.8717,  0.2825, -1.0805,  1.0400,  0.3937,  0.2013]],
       dtype=torch.float64)
	q_value: tensor([[-32.9190]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 10, step: 50
	action: tensor([[ 1.6660, -0.1610,  1.2599,  0.7050,  1.5451,  1.7987,  0.7193]],
       dtype=torch.float64)
	q_value: tensor([[-35.3292]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.024231282111282004, distance: 1.1581257322093348 entropy 1.3432114124298096
epoch: 10, step: 51
	action: tensor([[ 0.4037,  2.2039,  0.2570,  0.0476, -0.5420,  1.0298,  1.3289]],
       dtype=torch.float64)
	q_value: tensor([[-31.6208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 10, step: 52
	action: tensor([[-1.0491,  1.0261, -0.4771, -0.1150, -0.4734,  0.0068, -0.0489]],
       dtype=torch.float64)
	q_value: tensor([[-35.3292]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.22770898171667975, distance: 1.2679566618153564 entropy 1.3432114124298096
epoch: 10, step: 53
	action: tensor([[-1.1717,  0.0193, -1.9054, -0.2937, -0.4466, -0.5209,  1.1398]],
       dtype=torch.float64)
	q_value: tensor([[-23.8516]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6515945305476258, distance: 1.4706468981031462 entropy 1.3432114124298096
epoch: 10, step: 54
	action: tensor([[ 0.0368,  0.9604,  0.2573, -0.5025,  0.1699, -0.5352, -0.7982]],
       dtype=torch.float64)
	q_value: tensor([[-34.3534]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4974770057148836, distance: 0.8112125534183612 entropy 1.3432114124298096
epoch: 10, step: 55
	action: tensor([[ 0.1126, -1.7249,  0.0598, -1.9200,  1.5697, -1.1363, -1.3103]],
       dtype=torch.float64)
	q_value: tensor([[-16.7843]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5814270173306875, distance: 1.4390678484158974 entropy 1.3432114124298096
epoch: 10, step: 56
	action: tensor([[-0.6292,  1.9214, -0.3745, -0.0134,  0.0308, -1.0050,  0.2723]],
       dtype=torch.float64)
	q_value: tensor([[-27.2089]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 10, step: 57
	action: tensor([[ 1.5404,  0.5252, -1.2911, -0.5255,  1.3941, -0.1306,  1.6685]],
       dtype=torch.float64)
	q_value: tensor([[-35.3292]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.414897751709409, distance: 0.8753315388981985 entropy 1.3432114124298096
epoch: 10, step: 58
	action: tensor([[ 2.1427,  0.0777, -0.0385, -1.5862,  0.3148, -0.3359, -0.3556]],
       dtype=torch.float64)
	q_value: tensor([[-30.2571]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 10, step: 59
	action: tensor([[ 1.0463, -0.1377,  0.1118,  0.3475,  2.6250, -1.0973,  2.0129]],
       dtype=torch.float64)
	q_value: tensor([[-35.3292]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9669350523625523, distance: 0.20808492242357238 entropy 1.3432114124298096
epoch: 10, step: 60
	action: tensor([[ 0.5995,  0.0487, -1.1974,  1.0424, -0.9658, -0.8140,  0.5160]],
       dtype=torch.float64)
	q_value: tensor([[-38.2463]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7819657447099456, distance: 0.5343416887527356 entropy 1.3432114124298096
epoch: 10, step: 61
	action: tensor([[ 1.4563, -0.3900,  0.6810, -0.8060,  0.1676, -0.1566,  1.3348]],
       dtype=torch.float64)
	q_value: tensor([[-21.7301]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.02698863508642224, distance: 1.1287964885010997 entropy 1.3432114124298096
epoch: 10, step: 62
	action: tensor([[-1.3019, -0.0564,  0.3044,  0.8322,  0.1400, -1.3559,  1.1507]],
       dtype=torch.float64)
	q_value: tensor([[-27.9270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0746508214188575, distance: 1.6482732081022802 entropy 1.3432114124298096
epoch: 10, step: 63
	action: tensor([[ 0.4392,  0.1767,  0.9715, -0.8133,  0.0817, -0.0485, -0.3948]],
       dtype=torch.float64)
	q_value: tensor([[-29.6423]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2709470290222392, distance: 0.9770937500039487 entropy 1.3432114124298096
epoch: 10, step: 64
	action: tensor([[-1.4680, -0.2051,  0.2043,  0.9307,  0.0302, -0.0696, -0.9993]],
       dtype=torch.float64)
	q_value: tensor([[-17.6931]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9215689863711731, distance: 1.5862976601328873 entropy 1.3432114124298096
epoch: 10, step: 65
	action: tensor([[ 0.6267,  0.4702,  0.8551,  0.7356,  1.4497, -1.0272,  0.9283]],
       dtype=torch.float64)
	q_value: tensor([[-21.3727]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9214168879189435, distance: 0.32079035727619204 entropy 1.3432114124298096
epoch: 10, step: 66
	action: tensor([[ 0.1834,  0.0549,  0.3087, -1.4882, -0.8741,  0.9034,  0.6843]],
       dtype=torch.float64)
	q_value: tensor([[-25.4986]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15072719927167633, distance: 1.2275604352312903 entropy 1.3432114124298096
epoch: 10, step: 67
	action: tensor([[-0.1853,  0.2292, -0.1947, -0.7146,  0.5994,  1.1412, -1.6631]],
       dtype=torch.float64)
	q_value: tensor([[-31.6801]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39997566063030143, distance: 0.886423226219224 entropy 1.3432114124298096
epoch: 10, step: 68
	action: tensor([[ 0.9351,  0.1470,  1.0510,  1.5862, -0.2870, -0.0814, -0.0730]],
       dtype=torch.float64)
	q_value: tensor([[-21.8988]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6186271644585791, distance: 0.7066942719507102 entropy 1.3432114124298096
epoch: 10, step: 69
	action: tensor([[-0.5545, -0.7302,  0.6979, -1.6081,  0.1300, -0.8766, -0.0589]],
       dtype=torch.float64)
	q_value: tensor([[-23.8008]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5966996972728118, distance: 1.44600007253996 entropy 1.3432114124298096
epoch: 10, step: 70
	action: tensor([[-0.1349, -0.4112,  0.4552,  0.3133,  0.0060, -0.6813, -0.3497]],
       dtype=torch.float64)
	q_value: tensor([[-25.6593]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21381543614026177, distance: 1.26076174118544 entropy 1.3432114124298096
epoch: 10, step: 71
	action: tensor([[-0.0358,  0.5029,  0.2695,  0.9236, -1.0736, -0.8155, -0.5743]],
       dtype=torch.float64)
	q_value: tensor([[-16.0804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8637041612613451, distance: 0.4224720592027246 entropy 1.3432114124298096
epoch: 10, step: 72
	action: tensor([[-0.0541, -1.9327,  1.0210, -0.1034, -0.4669,  1.8247,  1.4880]],
       dtype=torch.float64)
	q_value: tensor([[-20.5723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 10, step: 73
	action: tensor([[ 0.5910, -1.3714, -0.6943, -0.4835,  1.2326,  0.1522, -0.0294]],
       dtype=torch.float64)
	q_value: tensor([[-35.3292]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7429909632326901, distance: 1.5107905546920386 entropy 1.3432114124298096
epoch: 10, step: 74
	action: tensor([[ 0.6889, -0.1962, -0.3401, -0.4160,  1.0582,  2.1525,  1.5565]],
       dtype=torch.float64)
	q_value: tensor([[-19.7233]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8108331403221994, distance: 0.49771327042674296 entropy 1.3432114124298096
epoch: 10, step: 75
	action: tensor([[ 0.7404, -0.1316,  1.4406, -1.2090,  1.5201, -0.1973, -0.9291]],
       dtype=torch.float64)
	q_value: tensor([[-34.4998]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3421451008558539, distance: 0.9281576529523979 entropy 1.3432114124298096
epoch: 10, step: 76
	action: tensor([[ 1.0773, -0.1834,  0.5794, -0.2846, -0.9882,  1.4156,  0.6622]],
       dtype=torch.float64)
	q_value: tensor([[-21.9638]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8353664756219583, distance: 0.46431833238164893 entropy 1.3432114124298096
epoch: 10, step: 77
	action: tensor([[ 0.5450,  1.3667,  1.5071, -0.5711, -2.0410, -1.7187,  1.1959]],
       dtype=torch.float64)
	q_value: tensor([[-30.7778]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8704081708624644, distance: 0.411950944735983 entropy 1.3432114124298096
epoch: 10, step: 78
	action: tensor([[-0.5580,  0.6596,  0.2311, -0.1514, -0.4015,  0.5598,  2.3259]],
       dtype=torch.float64)
	q_value: tensor([[-43.7235]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.012214144142376648, distance: 1.151311636251356 entropy 1.3432114124298096
epoch: 10, step: 79
	action: tensor([[ 1.3358e+00, -9.0090e-02, -5.4001e-01, -1.2821e+00, -4.9131e-04,
          3.0336e-01,  9.9490e-02]], dtype=torch.float64)
	q_value: tensor([[-35.3106]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12957031900019467, distance: 1.2162233283799098 entropy 1.3432114124298096
epoch: 10, step: 80
	action: tensor([[-0.9599,  0.8344, -0.1092,  0.3773, -0.2912,  0.8137,  1.6037]],
       dtype=torch.float64)
	q_value: tensor([[-22.9542]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 10, step: 81
	action: tensor([[ 0.3344,  1.8793,  0.7317,  0.9233,  0.3251,  0.8343, -0.0335]],
       dtype=torch.float64)
	q_value: tensor([[-35.3292]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 10, step: 82
	action: tensor([[-0.2317,  0.4951, -0.2041,  0.0152,  1.5803,  1.0339, -1.0210]],
       dtype=torch.float64)
	q_value: tensor([[-35.3292]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6493133041339332, distance: 0.6776670012937791 entropy 1.3432114124298096
epoch: 10, step: 83
	action: tensor([[ 0.7133,  0.5489, -0.1805, -1.1992,  1.0112, -1.3163,  0.2656]],
       dtype=torch.float64)
	q_value: tensor([[-18.5183]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5379781648351802, distance: 0.777835828549708 entropy 1.3432114124298096
epoch: 10, step: 84
	action: tensor([[ 0.3923,  1.8769,  0.5426,  0.0177, -0.1687,  0.7328, -1.8492]],
       dtype=torch.float64)
	q_value: tensor([[-22.5057]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 10, step: 85
	action: tensor([[ 0.0521, -1.7928,  0.2487,  0.3467, -0.0284,  0.9057,  0.5600]],
       dtype=torch.float64)
	q_value: tensor([[-35.3292]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 10, step: 86
	action: tensor([[-0.8518, -0.3433, -0.2009, -2.2102, -0.5941, -0.5212,  0.9001]],
       dtype=torch.float64)
	q_value: tensor([[-35.3292]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20901814877522695, distance: 1.017747285495128 entropy 1.3432114124298096
epoch: 10, step: 87
	action: tensor([[ 1.3933,  1.6837, -0.7959, -1.3811, -0.2198,  0.9723, -0.1414]],
       dtype=torch.float64)
	q_value: tensor([[-36.7338]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6034087110830499, distance: 0.7206564510716199 entropy 1.3432114124298096
epoch: 10, step: 88
	action: tensor([[ 0.5422, -0.5206,  0.7601, -1.7599,  1.5992, -0.1198, -1.1474]],
       dtype=torch.float64)
	q_value: tensor([[-34.7583]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.47292907024745157, distance: 1.3888252703138175 entropy 1.3432114124298096
epoch: 10, step: 89
	action: tensor([[ 1.7608,  0.6678,  0.3605,  0.6534, -0.4794, -1.2466,  0.4062]],
       dtype=torch.float64)
	q_value: tensor([[-22.2086]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6269319943560439, distance: 0.6989573819894875 entropy 1.3432114124298096
epoch: 10, step: 90
	action: tensor([[ 0.8939,  0.6840, -0.1117,  0.3893,  0.9540, -0.5691, -0.7599]],
       dtype=torch.float64)
	q_value: tensor([[-24.4684]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9652881550593166, distance: 0.21320409496090492 entropy 1.3432114124298096
epoch: 10, step: 91
	action: tensor([[ 0.7385,  0.0451, -0.0163,  0.0163,  0.3729,  0.8231, -0.6306]],
       dtype=torch.float64)
	q_value: tensor([[-16.6587]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9319591876292233, distance: 0.2984980087752372 entropy 1.3432114124298096
epoch: 10, step: 92
	action: tensor([[ 0.2067,  1.9106, -1.3054, -0.3692, -0.6080,  0.1876,  0.6324]],
       dtype=torch.float64)
	q_value: tensor([[-14.9225]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 10, step: 93
	action: tensor([[ 1.3875,  1.5415, -0.2717, -0.0613, -1.9752,  0.4829, -0.2863]],
       dtype=torch.float64)
	q_value: tensor([[-35.3292]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 10, step: 94
	action: tensor([[-0.6856, -0.1057,  1.3928, -0.8993,  0.2673,  0.6499,  1.4548]],
       dtype=torch.float64)
	q_value: tensor([[-35.3292]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0461127639294254, distance: 1.6368974628449673 entropy 1.3432114124298096
epoch: 10, step: 95
	action: tensor([[ 0.0325,  0.5274, -2.4721, -0.8881, -1.3619, -0.5862, -0.2647]],
       dtype=torch.float64)
	q_value: tensor([[-31.9634]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.783917764800944, distance: 0.5319443802007872 entropy 1.3432114124298096
epoch: 10, step: 96
	action: tensor([[-0.1302, -0.4340,  2.3714,  0.2357, -0.0695, -0.3911, -0.6649]],
       dtype=torch.float64)
	q_value: tensor([[-38.6920]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20481477119210156, distance: 1.2560786533201471 entropy 1.3432114124298096
epoch: 10, step: 97
	action: tensor([[ 1.2196,  0.3821, -0.3897,  1.0353,  1.2067, -0.5942,  0.8451]],
       dtype=torch.float64)
	q_value: tensor([[-28.3319]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9727608307430716, distance: 0.18886593039056737 entropy 1.3432114124298096
epoch: 10, step: 98
	action: tensor([[-0.3089,  0.3649, -0.5998,  0.2751,  3.1948, -0.5562,  1.0315]],
       dtype=torch.float64)
	q_value: tensor([[-23.7354]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0774436177133633 entropy 1.3432114124298096
epoch: 10, step: 99
	action: tensor([[-1.5808,  0.1201, -0.4334,  0.8284, -0.5683, -1.5187,  0.6203]],
       dtype=torch.float64)
	q_value: tensor([[-35.3292]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8517266145938083, distance: 1.5572026266453902 entropy 1.3432114124298096
epoch: 10, step: 100
	action: tensor([[-0.3962, -0.0555,  1.4563, -0.0816, -0.4738, -1.1035,  1.6564]],
       dtype=torch.float64)
	q_value: tensor([[-29.7695]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5386432789930891, distance: 1.419468192651978 entropy 1.3432114124298096
epoch: 10, step: 101
	action: tensor([[-0.5306, -0.5087,  1.6111, -0.1337, -0.3519,  2.1297, -0.9920]],
       dtype=torch.float64)
	q_value: tensor([[-33.7829]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17400434218369953, distance: 1.0400293020875473 entropy 1.3432114124298096
epoch: 10, step: 102
	action: tensor([[ 2.9878,  0.8778, -0.4609, -0.5270,  0.2610,  0.6452, -1.3815]],
       dtype=torch.float64)
	q_value: tensor([[-33.1254]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 10, step: 103
	action: tensor([[-0.5822, -0.0315,  1.2806,  0.6492,  0.0825,  0.6317, -1.2249]],
       dtype=torch.float64)
	q_value: tensor([[-35.3292]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36194863161955604, distance: 0.914080649853272 entropy 1.3432114124298096
epoch: 10, step: 104
	action: tensor([[ 0.8629, -0.1572, -0.4533, -0.4489, -0.8865, -0.1261, -0.5207]],
       dtype=torch.float64)
	q_value: tensor([[-20.7908]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3944690254115626, distance: 0.890481446017665 entropy 1.3432114124298096
epoch: 10, step: 105
	action: tensor([[-0.9291, -0.6084,  0.5502, -1.1719,  0.7848,  0.7670, -0.3937]],
       dtype=torch.float64)
	q_value: tensor([[-18.8525]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.121607724031625, distance: 1.6668220507189293 entropy 1.3432114124298096
epoch: 10, step: 106
	action: tensor([[ 0.6078,  0.1095, -1.3564,  1.1055,  1.2137, -0.4630, -1.0093]],
       dtype=torch.float64)
	q_value: tensor([[-22.1944]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44426705334631855, distance: 0.853080010514383 entropy 1.3432114124298096
epoch: 10, step: 107
	action: tensor([[ 0.1311, -0.8690, -0.7059, -0.0810,  0.0618, -0.5426,  1.3651]],
       dtype=torch.float64)
	q_value: tensor([[-23.0546]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4544229109701543, distance: 1.3800729603779827 entropy 1.3432114124298096
epoch: 10, step: 108
	action: tensor([[ 0.2107, -0.4863,  0.4225,  2.3132, -1.7195, -0.4513, -0.5382]],
       dtype=torch.float64)
	q_value: tensor([[-24.6460]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8377478466649779, distance: 0.46094799250641255 entropy 1.3432114124298096
epoch: 10, step: 109
	action: tensor([[-0.4885,  0.2498,  0.9810,  0.9954,  0.6497, -0.4880,  1.1764]],
       dtype=torch.float64)
	q_value: tensor([[-29.9996]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5242885370615169, distance: 0.7892752809827751 entropy 1.3432114124298096
epoch: 10, step: 110
	action: tensor([[-0.1299, -0.4248, -0.3876, -0.0010, -0.2925, -0.4448,  0.2555]],
       dtype=torch.float64)
	q_value: tensor([[-24.6068]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2294737664490305, distance: 1.2688676541707662 entropy 1.3432114124298096
epoch: 10, step: 111
	action: tensor([[-0.2412,  0.3121, -0.4160, -0.1643,  1.0806, -1.1291,  2.1600]],
       dtype=torch.float64)
	q_value: tensor([[-16.3297]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03881919839611214, distance: 1.1663440464800343 entropy 1.3432114124298096
epoch: 10, step: 112
	action: tensor([[ 1.1385, -1.7373, -0.5178,  1.3933, -1.1205,  0.1265, -1.0623]],
       dtype=torch.float64)
	q_value: tensor([[-32.2062]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 10, step: 113
	action: tensor([[-0.4080, -0.7821, -0.0092, -0.7469, -0.7805,  0.2796, -1.7960]],
       dtype=torch.float64)
	q_value: tensor([[-35.3292]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0027865899145372, distance: 1.619474189068969 entropy 1.3432114124298096
epoch: 10, step: 114
	action: tensor([[-0.1784, -1.2935,  0.4005, -0.0364,  0.4108, -0.2840,  0.5332]],
       dtype=torch.float64)
	q_value: tensor([[-24.7017]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0333913896276816, distance: 1.6318009565544254 entropy 1.3432114124298096
epoch: 10, step: 115
	action: tensor([[ 0.9981, -0.1189,  0.0725,  1.1530, -0.5187, -2.2482, -0.3391]],
       dtype=torch.float64)
	q_value: tensor([[-21.9245]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7399162033255415, distance: 0.5835973906466014 entropy 1.3432114124298096
epoch: 10, step: 116
	action: tensor([[ 1.2488,  1.4334, -1.5614,  0.6295, -0.5799, -0.5715, -0.3374]],
       dtype=torch.float64)
	q_value: tensor([[-29.6928]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 10, step: 117
	action: tensor([[ 0.4892, -0.0077,  0.6363,  0.0711,  0.7085,  1.2885,  0.2940]],
       dtype=torch.float64)
	q_value: tensor([[-35.3292]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8792085800735421, distance: 0.39771753268695886 entropy 1.3432114124298096
epoch: 10, step: 118
	action: tensor([[ 1.1829,  0.4888, -1.6052, -0.8654, -1.6895,  1.2533,  0.5347]],
       dtype=torch.float64)
	q_value: tensor([[-19.4625]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5791475545827468, distance: 0.7423720641540339 entropy 1.3432114124298096
epoch: 10, step: 119
	action: tensor([[ 1.7339, -0.0558, -0.2297,  0.7707,  1.2424, -1.8577,  0.3564]],
       dtype=torch.float64)
	q_value: tensor([[-40.6811]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 10, step: 120
	action: tensor([[ 1.4275, -0.3015, -0.1208,  1.1471,  0.1083,  0.7915, -0.2403]],
       dtype=torch.float64)
	q_value: tensor([[-35.3292]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8943990639808471, distance: 0.37186951478566355 entropy 1.3432114124298096
epoch: 10, step: 121
	action: tensor([[ 0.1069, -0.1511,  0.0384,  0.1882,  0.8512, -1.2255,  1.6190]],
       dtype=torch.float64)
	q_value: tensor([[-21.3634]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06831049755807783, distance: 1.1045675856444452 entropy 1.3432114124298096
epoch: 10, step: 122
	action: tensor([[ 1.7996, -0.0994,  0.1012, -0.0426, -0.4423, -0.0314,  0.7806]],
       dtype=torch.float64)
	q_value: tensor([[-27.9243]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2698128613084205, distance: 0.9778534736242834 entropy 1.3432114124298096
epoch: 10, step: 123
	action: tensor([[ 0.3447, -0.9296,  0.8115,  1.5414,  0.4242,  0.4487,  0.8013]],
       dtype=torch.float64)
	q_value: tensor([[-22.8413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7991403319250994, distance: 0.512865001840041 entropy 1.3432114124298096
epoch: 10, step: 124
	action: tensor([[-0.8132,  0.9616, -0.2514, -0.1851,  1.2098,  0.7011, -0.1508]],
       dtype=torch.float64)
	q_value: tensor([[-27.4667]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2552997991385083, distance: 0.987523476070416 entropy 1.3432114124298096
epoch: 10, step: 125
	action: tensor([[-0.3991, -0.1507,  1.0862, -0.6462,  0.3437, -0.8874, -0.3228]],
       dtype=torch.float64)
	q_value: tensor([[-20.1079]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7731504911973275, distance: 1.5238053415572803 entropy 1.3432114124298096
epoch: 10, step: 126
	action: tensor([[-1.3042, -1.5505,  0.3117, -0.8228, -1.3811,  1.0450, -1.1559]],
       dtype=torch.float64)
	q_value: tensor([[-19.0888]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 10, step: 127
	action: tensor([[-0.4145, -0.0337, -0.5525,  1.3457, -0.4403, -0.1828,  0.0682]],
       dtype=torch.float64)
	q_value: tensor([[-35.3292]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04726182348611441, distance: 1.1710739741835334 entropy 1.3432114124298096
LOSS epoch 10 actor 309.2327830319941 critic 339.4683984882934 
epoch: 11, step: 0
	action: tensor([[-0.6328,  0.1672, -0.1419, -0.4884, -1.0055, -0.8876,  0.7285]],
       dtype=torch.float64)
	q_value: tensor([[-23.8998]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13797256159401727, distance: 1.220738350715416 entropy 1.3432114124298096
epoch: 11, step: 1
	action: tensor([[ 0.6673, -0.4953,  0.1586, -0.3007, -0.1383, -1.5074, -0.3889]],
       dtype=torch.float64)
	q_value: tensor([[-33.2970]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08326562563791673, distance: 1.1910340414102552 entropy 1.3432114124298096
epoch: 11, step: 2
	action: tensor([[ 1.6980, -0.0255, -0.7144, -0.0745, -0.3459,  1.9579,  0.3257]],
       dtype=torch.float64)
	q_value: tensor([[-29.5390]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8128278838400023, distance: 0.4950821502578648 entropy 1.3432114124298096
epoch: 11, step: 3
	action: tensor([[ 0.8151, -0.6082, -0.6964,  0.5145, -0.6201, -0.1316,  0.1128]],
       dtype=torch.float64)
	q_value: tensor([[-40.4450]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4467710792106817, distance: 0.8511559333899856 entropy 1.3432114124298096
epoch: 11, step: 4
	action: tensor([[ 0.0018, -0.3976, -1.4491, -0.5058,  1.5145, -0.3751,  1.6800]],
       dtype=torch.float64)
	q_value: tensor([[-22.6830]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08340962484839287, distance: 1.0955806155360583 entropy 1.3432114124298096
epoch: 11, step: 5
	action: tensor([[ 0.2350, -0.6271, -0.4419, -0.6678, -0.8912, -0.6669,  1.3088]],
       dtype=torch.float64)
	q_value: tensor([[-42.4251]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15327638012641676, distance: 1.2289193769620468 entropy 1.3432114124298096
epoch: 11, step: 6
	action: tensor([[-1.5479,  1.1051, -1.1257, -0.3526, -0.7241,  0.0300,  0.6725]],
       dtype=torch.float64)
	q_value: tensor([[-35.8141]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 11, step: 7
	action: tensor([[-0.4454, -0.2600,  0.5192, -1.3209,  1.1587, -0.6906, -1.3148]],
       dtype=torch.float64)
	q_value: tensor([[-47.2428]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9470726755211185, distance: 1.5967898902955349 entropy 1.3432114124298096
epoch: 11, step: 8
	action: tensor([[ 1.4084,  0.1224, -1.0391, -1.4924, -0.6707, -0.9490,  0.5472]],
       dtype=torch.float64)
	q_value: tensor([[-27.5453]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07759372165696665, distance: 1.1879118631886803 entropy 1.3432114124298096
epoch: 11, step: 9
	action: tensor([[ 0.1952, -0.4230, -2.0152, -0.1365,  0.0088,  0.7855, -0.1465]],
       dtype=torch.float64)
	q_value: tensor([[-40.6804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04220033615740215, distance: 1.1682406066530544 entropy 1.3432114124298096
epoch: 11, step: 10
	action: tensor([[ 0.0349,  0.4141,  1.3914, -0.9883, -1.1979,  2.1233,  0.3226]],
       dtype=torch.float64)
	q_value: tensor([[-33.4347]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5884293611789864, distance: 0.7341399970404613 entropy 1.3432114124298096
epoch: 11, step: 11
	action: tensor([[ 1.1428,  0.0302, -0.7908, -0.3333, -0.4202, -0.0875, -0.6756]],
       dtype=torch.float64)
	q_value: tensor([[-55.7365]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.562914579600336, distance: 0.7565538634232171 entropy 1.3432114124298096
epoch: 11, step: 12
	action: tensor([[-1.0486,  0.2234, -0.2330, -0.1122,  0.1122, -1.2247,  0.5427]],
       dtype=torch.float64)
	q_value: tensor([[-23.2379]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7877083188871925, distance: 1.5300478882643345 entropy 1.3432114124298096
epoch: 11, step: 13
	action: tensor([[-0.5174, -0.5350,  0.8745, -0.3268, -1.1190,  0.3447,  0.8525]],
       dtype=torch.float64)
	q_value: tensor([[-30.5102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.957052861922939, distance: 1.6008770237244447 entropy 1.3432114124298096
epoch: 11, step: 14
	action: tensor([[-0.4479,  0.8058,  0.6461, -1.6480,  1.2325,  0.8413,  0.7556]],
       dtype=torch.float64)
	q_value: tensor([[-36.7140]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20218370800575824, distance: 1.2547063973453274 entropy 1.3432114124298096
epoch: 11, step: 15
	action: tensor([[ 0.0988, -0.3669, -0.0661, -0.2390,  0.1412, -0.7754, -0.3078]],
       dtype=torch.float64)
	q_value: tensor([[-38.1543]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23747957095682048, distance: 1.2729921111755012 entropy 1.3432114124298096
epoch: 11, step: 16
	action: tensor([[ 1.0035,  0.4517, -0.8950, -1.1687, -1.1931,  0.1894,  0.1448]],
       dtype=torch.float64)
	q_value: tensor([[-19.9357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5315217515120132, distance: 0.783251813213336 entropy 1.3432114124298096
epoch: 11, step: 17
	action: tensor([[ 1.1642, -1.1676,  2.9228,  0.0219, -0.9512, -0.8059,  1.3321]],
       dtype=torch.float64)
	q_value: tensor([[-40.8495]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3339989688115901, distance: 1.3217047177915577 entropy 1.3432114124298096
epoch: 11, step: 18
	action: tensor([[-0.8690, -1.5841,  0.9321,  0.3890,  1.0623,  0.4861,  0.4311]],
       dtype=torch.float64)
	q_value: tensor([[-61.2903]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 11, step: 19
	action: tensor([[ 0.4516,  0.2868,  1.4773,  0.5832, -0.1818, -0.4561,  0.5734]],
       dtype=torch.float64)
	q_value: tensor([[-47.2428]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9087588305779662, distance: 0.3456623790817676 entropy 1.3432114124298096
epoch: 11, step: 20
	action: tensor([[ 1.9370e+00, -6.5880e-01, -5.0447e-01, -3.7457e-01,  3.7528e-01,
         -1.0943e-01, -1.0206e-05]], dtype=torch.float64)
	q_value: tensor([[-30.5767]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 11, step: 21
	action: tensor([[-0.0663,  0.6895, -0.6800,  0.7143, -1.1816,  1.2678, -1.4036]],
       dtype=torch.float64)
	q_value: tensor([[-47.2428]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07342723336928791, distance: 1.1015303282607796 entropy 1.3432114124298096
epoch: 11, step: 22
	action: tensor([[ 0.6035,  0.5203, -0.8956,  0.6011,  0.2322,  0.0683, -1.1450]],
       dtype=torch.float64)
	q_value: tensor([[-37.5556]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7804727545411645, distance: 0.5361680207323103 entropy 1.3432114124298096
epoch: 11, step: 23
	action: tensor([[-0.3436,  0.9958, -0.3358,  1.3587,  1.0150,  1.4462, -1.6649]],
       dtype=torch.float64)
	q_value: tensor([[-23.0993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5148210398985849, distance: 0.7970905731241891 entropy 1.3432114124298096
epoch: 11, step: 24
	action: tensor([[-2.0622, -1.0911,  0.3764, -0.4321,  1.0093, -0.4230, -0.4635]],
       dtype=torch.float64)
	q_value: tensor([[-34.6083]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 11, step: 25
	action: tensor([[-0.9240,  0.7675,  0.0199, -0.6648, -0.9062, -1.8175,  0.6124]],
       dtype=torch.float64)
	q_value: tensor([[-47.2428]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07422550518838877, distance: 1.101055724364094 entropy 1.3432114124298096
epoch: 11, step: 26
	action: tensor([[-0.5062,  1.6789, -0.5491, -0.9046, -0.3001, -1.4714,  0.7879]],
       dtype=torch.float64)
	q_value: tensor([[-40.6412]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7239849408082972, distance: 0.6012056897172998 entropy 1.3432114124298096
epoch: 11, step: 27
	action: tensor([[ 0.4183,  0.2992,  1.2698, -0.0311,  1.7479, -1.2586, -1.5782]],
       dtype=torch.float64)
	q_value: tensor([[-39.5241]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8746622744982886, distance: 0.40513297944087434 entropy 1.3432114124298096
epoch: 11, step: 28
	action: tensor([[ 0.2004, -0.6091,  2.1097, -0.1207,  2.2276,  0.2503,  0.9101]],
       dtype=torch.float64)
	q_value: tensor([[-31.5037]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15881836209887013, distance: 1.231868577490245 entropy 1.3432114124298096
epoch: 11, step: 29
	action: tensor([[ 1.5089, -1.1589,  0.2934, -0.8479,  1.9628, -1.1626,  0.5015]],
       dtype=torch.float64)
	q_value: tensor([[-44.5631]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2882095993376763, distance: 1.2988229645401714 entropy 1.3432114124298096
epoch: 11, step: 30
	action: tensor([[ 0.2938, -0.0382,  1.0452, -0.3745, -0.8216, -0.9586,  0.4290]],
       dtype=torch.float64)
	q_value: tensor([[-42.4015]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09110787910769125, distance: 1.0909701362482442 entropy 1.3432114124298096
epoch: 11, step: 31
	action: tensor([[-0.2811, -0.4732,  0.6488,  0.7172,  0.2736,  2.0177,  0.4930]],
       dtype=torch.float64)
	q_value: tensor([[-31.8177]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.643771093360842, distance: 0.6830008941410189 entropy 1.3432114124298096
epoch: 11, step: 32
	action: tensor([[ 0.8842, -0.1847,  0.8155,  0.3603,  0.2325,  1.4268, -0.1143]],
       dtype=torch.float64)
	q_value: tensor([[-36.9467]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7850845473932877, distance: 0.5305062620612155 entropy 1.3432114124298096
epoch: 11, step: 33
	action: tensor([[ 1.0620,  0.1033, -0.1778,  1.4479, -0.3540, -0.8536,  0.5221]],
       dtype=torch.float64)
	q_value: tensor([[-29.5893]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9667724241595416, distance: 0.20859602210447217 entropy 1.3432114124298096
epoch: 11, step: 34
	action: tensor([[ 8.7128e-01, -1.2869e+00, -5.0247e-04, -2.4016e+00,  1.0022e+00,
          7.9911e-02, -5.7165e-01]], dtype=torch.float64)
	q_value: tensor([[-29.7365]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3829104667530925, distance: 1.3457169577197485 entropy 1.3432114124298096
epoch: 11, step: 35
	action: tensor([[ 0.1778, -0.4820,  0.0791,  0.7764, -0.5164,  0.3211,  0.2921]],
       dtype=torch.float64)
	q_value: tensor([[-34.9494]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5890323087250069, distance: 0.733602045464698 entropy 1.3432114124298096
epoch: 11, step: 36
	action: tensor([[ 0.3341,  0.4073,  0.2274,  0.2618,  0.0846,  0.3958, -0.9291]],
       dtype=torch.float64)
	q_value: tensor([[-22.0522]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8792458769366965, distance: 0.39765612616643176 entropy 1.3432114124298096
epoch: 11, step: 37
	action: tensor([[ 1.0155, -0.6699, -0.6255, -0.9650, -0.3527, -0.1164, -1.6757]],
       dtype=torch.float64)
	q_value: tensor([[-18.5486]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.48234016034958205, distance: 1.3932550655455191 entropy 1.3432114124298096
epoch: 11, step: 38
	action: tensor([[ 1.3384, -0.1695,  1.0800, -0.4218,  0.1080,  0.6417, -0.8294]],
       dtype=torch.float64)
	q_value: tensor([[-29.1809]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4877490275433579, distance: 0.819026755050998 entropy 1.3432114124298096
epoch: 11, step: 39
	action: tensor([[ 0.0791,  0.4663, -0.5220,  1.0642, -0.6448,  0.7304,  1.2644]],
       dtype=torch.float64)
	q_value: tensor([[-30.0783]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.253335945007215, distance: 0.9888247213026201 entropy 1.3432114124298096
epoch: 11, step: 40
	action: tensor([[ 1.2295, -0.4287, -0.9976, -1.0815, -1.4707, -0.1070,  1.0349]],
       dtype=torch.float64)
	q_value: tensor([[-31.4594]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2725186148637382, distance: 1.2908885993948143 entropy 1.3432114124298096
epoch: 11, step: 41
	action: tensor([[ 1.4084, -0.2350, -0.4031,  0.1536,  1.2222, -0.0447,  0.4434]],
       dtype=torch.float64)
	q_value: tensor([[-44.0040]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4414607166188078, distance: 0.8552312371488692 entropy 1.3432114124298096
epoch: 11, step: 42
	action: tensor([[ 0.5851, -0.8247,  0.2255,  1.6754,  0.1553, -0.7205,  0.6428]],
       dtype=torch.float64)
	q_value: tensor([[-28.1377]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8027180313009212, distance: 0.5082769203218687 entropy 1.3432114124298096
epoch: 11, step: 43
	action: tensor([[-8.3551e-02,  1.1501e+00, -8.3296e-01, -9.4821e-04, -6.4911e-01,
         -1.0434e+00, -1.7478e+00]], dtype=torch.float64)
	q_value: tensor([[-34.6264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8098964494112154, distance: 0.4989440034447936 entropy 1.3432114124298096
epoch: 11, step: 44
	action: tensor([[-0.1709,  0.1627,  0.2172, -0.9010,  0.4016, -0.4705,  0.9925]],
       dtype=torch.float64)
	q_value: tensor([[-33.6639]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.33690958759459555, distance: 1.3231458292716851 entropy 1.3432114124298096
epoch: 11, step: 45
	action: tensor([[ 0.2225, -2.0636,  0.5919, -0.3500, -1.2857,  1.3698, -1.0641]],
       dtype=torch.float64)
	q_value: tensor([[-28.0163]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 11, step: 46
	action: tensor([[-0.4257, -0.2428, -0.6756, -1.5420,  0.0452,  0.9036, -0.3999]],
       dtype=torch.float64)
	q_value: tensor([[-47.2428]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06889371819407875, distance: 1.1831068140097079 entropy 1.3432114124298096
epoch: 11, step: 47
	action: tensor([[ 0.7815,  0.1076, -0.5098,  0.4070, -0.3603,  0.8273,  0.3014]],
       dtype=torch.float64)
	q_value: tensor([[-35.8349]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8688081438251704, distance: 0.41448625346265544 entropy 1.3432114124298096
epoch: 11, step: 48
	action: tensor([[-6.2946e-01, -3.6018e-01, -2.5450e-01, -7.2763e-04,  8.0220e-01,
          2.0705e-01, -1.7656e-01]], dtype=torch.float64)
	q_value: tensor([[-23.7118]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7519200876738839, distance: 1.5146554066395947 entropy 1.3432114124298096
epoch: 11, step: 49
	action: tensor([[-0.1227,  1.0549, -1.8818, -0.0377, -0.9917, -0.9860, -0.8327]],
       dtype=torch.float64)
	q_value: tensor([[-21.1486]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 11, step: 50
	action: tensor([[-0.2265, -0.1506, -1.6608, -0.9792, -0.1999,  0.2082, -0.9054]],
       dtype=torch.float64)
	q_value: tensor([[-47.2428]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.49871079012879893, distance: 0.8102161050018744 entropy 1.3432114124298096
epoch: 11, step: 51
	action: tensor([[-0.1413, -1.7112, -0.0774, -0.1163, -0.2873, -1.9427, -0.0928]],
       dtype=torch.float64)
	q_value: tensor([[-35.0721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 11, step: 52
	action: tensor([[ 0.7859, -0.6537,  0.6341,  0.6145,  1.1885,  1.2831,  1.6014]],
       dtype=torch.float64)
	q_value: tensor([[-47.2428]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41744743524278294, distance: 0.8734222528609619 entropy 1.3432114124298096
epoch: 11, step: 53
	action: tensor([[-0.0538,  0.7579,  0.6506, -0.0591,  1.4685, -0.3510,  0.2456]],
       dtype=torch.float64)
	q_value: tensor([[-39.9568]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4706346705329988, distance: 0.8325962303728962 entropy 1.3432114124298096
epoch: 11, step: 54
	action: tensor([[ 1.3837, -0.0900,  1.0084,  0.1792, -0.1551,  0.0867,  0.5055]],
       dtype=torch.float64)
	q_value: tensor([[-23.3598]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.52565947249841, distance: 0.78813716866191 entropy 1.3432114124298096
epoch: 11, step: 55
	action: tensor([[-1.5337,  0.8262,  0.5841,  1.1248,  0.9558, -0.0994, -0.8685]],
       dtype=torch.float64)
	q_value: tensor([[-30.3613]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 11, step: 56
	action: tensor([[-0.2357, -0.2080,  1.0629, -0.1583,  0.5584, -0.6324, -0.9890]],
       dtype=torch.float64)
	q_value: tensor([[-47.2428]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4359569623232997, distance: 1.371283988547627 entropy 1.3432114124298096
epoch: 11, step: 57
	action: tensor([[-1.4561,  0.2481,  0.2129, -0.9081,  0.4026, -1.4325,  0.0380]],
       dtype=torch.float64)
	q_value: tensor([[-23.8697]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.015229934077666, distance: 1.6244973081052472 entropy 1.3432114124298096
epoch: 11, step: 58
	action: tensor([[-0.0549,  0.5445,  0.8898, -2.1424, -1.4108, -0.0151,  1.1281]],
       dtype=torch.float64)
	q_value: tensor([[-34.9129]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.35327465783147005, distance: 1.331219491300805 entropy 1.3432114124298096
epoch: 11, step: 59
	action: tensor([[-0.4380,  1.2469, -1.0947, -0.0185,  0.2092, -0.7888, -1.3602]],
       dtype=torch.float64)
	q_value: tensor([[-55.8334]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 11, step: 60
	action: tensor([[ 0.8888, -0.2591,  0.7782, -1.0224,  0.5662,  0.1916, -1.9137]],
       dtype=torch.float64)
	q_value: tensor([[-47.2428]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08817480653020848, distance: 1.0927290453034268 entropy 1.3432114124298096
epoch: 11, step: 61
	action: tensor([[ 0.3278, -0.3847, -1.0272, -0.1612,  1.4996,  1.0666, -1.1089]],
       dtype=torch.float64)
	q_value: tensor([[-29.6274]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5949488530610465, distance: 0.7283022076417508 entropy 1.3432114124298096
epoch: 11, step: 62
	action: tensor([[-0.1029,  0.1106, -1.4888,  1.9650, -1.6471, -0.7107, -0.1296]],
       dtype=torch.float64)
	q_value: tensor([[-29.4279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13864986222746456, distance: 1.2211015773421372 entropy 1.3432114124298096
epoch: 11, step: 63
	action: tensor([[-0.1711,  0.1131, -0.6451, -1.5082, -1.1170, -1.2059, -0.9996]],
       dtype=torch.float64)
	q_value: tensor([[-35.5067]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.610960102057935, distance: 0.7137625616971025 entropy 1.3432114124298096
epoch: 11, step: 64
	action: tensor([[-1.7531, -0.6960, -1.1266,  1.5415,  0.0187, -1.0936,  0.5990]],
       dtype=torch.float64)
	q_value: tensor([[-40.0947]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.5256279597029367, distance: 1.8186175660669435 entropy 1.3432114124298096
epoch: 11, step: 65
	action: tensor([[ 0.1384,  0.8883, -0.0558, -0.5430,  0.2334,  1.1446, -0.1900]],
       dtype=torch.float64)
	q_value: tensor([[-42.9817]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7639941794045944, distance: 0.5559273624498741 entropy 1.3432114124298096
epoch: 11, step: 66
	action: tensor([[-0.6708,  0.2725,  1.0923,  2.0110, -1.1942, -0.3653,  0.2762]],
       dtype=torch.float64)
	q_value: tensor([[-28.6866]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8486743986671635, distance: 0.4451566827607103 entropy 1.3432114124298096
epoch: 11, step: 67
	action: tensor([[ 0.1114, -1.3848,  0.7623, -0.6427, -0.0556,  1.6741,  0.3596]],
       dtype=torch.float64)
	q_value: tensor([[-36.8714]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3241242162871074, distance: 1.3168037582959595 entropy 1.3432114124298096
epoch: 11, step: 68
	action: tensor([[-0.8201,  1.7967,  0.7815, -1.3804, -0.9566, -1.5301, -0.1588]],
       dtype=torch.float64)
	q_value: tensor([[-38.2829]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.006447857903450149, distance: 1.148027610671315 entropy 1.3432114124298096
epoch: 11, step: 69
	action: tensor([[ 0.2831,  0.3958, -0.4340, -0.9731, -1.7322,  0.3681, -0.4662]],
       dtype=torch.float64)
	q_value: tensor([[-45.0760]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.468649814011346, distance: 0.8341556806913211 entropy 1.3432114124298096
epoch: 11, step: 70
	action: tensor([[ 1.2465, -0.5389,  0.6550, -1.2922, -0.2880, -0.7488,  0.0317]],
       dtype=torch.float64)
	q_value: tensor([[-42.6179]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2257925581953879, distance: 1.266966650738778 entropy 1.3432114124298096
epoch: 11, step: 71
	action: tensor([[-1.1966, -1.6178, -1.0146, -0.0599, -0.3002,  0.9480,  0.5810]],
       dtype=torch.float64)
	q_value: tensor([[-33.4737]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 11, step: 72
	action: tensor([[-0.1333,  1.0253, -1.5357, -0.2978, -0.2940, -0.1238, -1.5242]],
       dtype=torch.float64)
	q_value: tensor([[-47.2428]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 11, step: 73
	action: tensor([[ 1.0481, -0.1083, -1.0757, -0.6365, -1.1716,  0.2419,  0.6399]],
       dtype=torch.float64)
	q_value: tensor([[-47.2428]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37517750059454535, distance: 0.904555092872302 entropy 1.3432114124298096
epoch: 11, step: 74
	action: tensor([[ 1.6233,  0.2539,  0.3439,  0.3914,  0.2421, -0.8594, -0.5936]],
       dtype=torch.float64)
	q_value: tensor([[-36.2154]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6257112653313186, distance: 0.7000999896545026 entropy 1.3432114124298096
epoch: 11, step: 75
	action: tensor([[-0.1466,  1.1523, -0.5511, -1.0349, -0.2786,  0.7274,  1.3098]],
       dtype=torch.float64)
	q_value: tensor([[-27.5682]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.687954312061141, distance: 0.6392427130603937 entropy 1.3432114124298096
epoch: 11, step: 76
	action: tensor([[ 1.2169, -0.0034,  0.5236, -1.9398,  0.4444, -0.6891, -0.1387]],
       dtype=torch.float64)
	q_value: tensor([[-43.3234]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10060015348574369, distance: 1.0852582546289649 entropy 1.3432114124298096
epoch: 11, step: 77
	action: tensor([[ 1.2466, -1.0415, -0.3971, -1.3842,  0.1193, -1.8035,  0.6509]],
       dtype=torch.float64)
	q_value: tensor([[-34.9495]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10413305112203597, distance: 1.2024510291579036 entropy 1.3432114124298096
epoch: 11, step: 78
	action: tensor([[ 1.0784, -0.4152,  0.6514, -0.5452,  0.4004, -1.0751, -0.3949]],
       dtype=torch.float64)
	q_value: tensor([[-42.3544]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10853926733779717, distance: 1.0804577830369428 entropy 1.3432114124298096
epoch: 11, step: 79
	action: tensor([[-0.9537,  0.0778, -0.9530, -0.0928,  0.0850, -0.6913, -0.4403]],
       dtype=torch.float64)
	q_value: tensor([[-28.1183]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7013788929361615, distance: 1.4926473478036497 entropy 1.3432114124298096
epoch: 11, step: 80
	action: tensor([[ 1.3441, -0.7097, -0.2108,  0.2378, -0.5323, -1.3114, -0.1188]],
       dtype=torch.float64)
	q_value: tensor([[-27.7477]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16218373093677663, distance: 1.2336560390568725 entropy 1.3432114124298096
epoch: 11, step: 81
	action: tensor([[ 0.4723, -0.1629, -1.8956,  0.3316,  0.2316, -0.7816, -0.7974]],
       dtype=torch.float64)
	q_value: tensor([[-32.2628]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5218082963359154, distance: 0.7913301481430237 entropy 1.3432114124298096
epoch: 11, step: 82
	action: tensor([[ 0.5054, -0.4483,  0.0346,  0.2278,  0.3431,  0.1274, -0.1927]],
       dtype=torch.float64)
	q_value: tensor([[-30.7233]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44446949646246103, distance: 0.8529246158167916 entropy 1.3432114124298096
epoch: 11, step: 83
	action: tensor([[ 0.5653, -0.9152,  0.3151,  0.0402, -1.4277,  0.6472, -0.7422]],
       dtype=torch.float64)
	q_value: tensor([[-17.5695]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11480007988643859, distance: 1.2082455070319518 entropy 1.3432114124298096
epoch: 11, step: 84
	action: tensor([[ 1.3425,  0.7785, -0.1806, -1.2460, -0.6103, -0.7157, -0.1360]],
       dtype=torch.float64)
	q_value: tensor([[-33.3608]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.43921856752310773, distance: 0.8569460989675003 entropy 1.3432114124298096
epoch: 11, step: 85
	action: tensor([[ 0.6451,  0.7134,  0.1379, -1.3713,  0.6689, -0.8090, -1.7883]],
       dtype=torch.float64)
	q_value: tensor([[-34.9108]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4909734154640538, distance: 0.8164449845875432 entropy 1.3432114124298096
epoch: 11, step: 86
	action: tensor([[ 1.4527, -0.7497,  1.5521, -0.5802, -0.1169,  2.0944,  1.2121]],
       dtype=torch.float64)
	q_value: tensor([[-31.5959]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11307110637683504, distance: 1.0777079706368213 entropy 1.3432114124298096
epoch: 11, step: 87
	action: tensor([[-1.4597, -0.5383, -0.5188,  1.0341,  0.9026,  0.0508,  0.1707]],
       dtype=torch.float64)
	q_value: tensor([[-53.5821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.4490843919374479, distance: 1.7908473497343267 entropy 1.3432114124298096
epoch: 11, step: 88
	action: tensor([[ 0.0792,  0.1508, -0.1205, -0.7294,  0.8212,  0.9976,  1.7943]],
       dtype=torch.float64)
	q_value: tensor([[-33.8988]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5485829240515154, distance: 0.7688571986325287 entropy 1.3432114124298096
epoch: 11, step: 89
	action: tensor([[ 1.3573,  0.2712,  0.2621,  0.4815,  1.3704, -0.9161, -0.4390]],
       dtype=torch.float64)
	q_value: tensor([[-39.3924]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9626352120635435, distance: 0.22120144218772578 entropy 1.3432114124298096
epoch: 11, step: 90
	action: tensor([[-0.1505, -0.5201, -0.7304, -1.7058,  2.3077, -0.4667, -0.3789]],
       dtype=torch.float64)
	q_value: tensor([[-28.2814]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11726415201183471, distance: 1.2095800784926132 entropy 1.3432114124298096
epoch: 11, step: 91
	action: tensor([[ 1.0570,  0.1012,  0.2788, -1.4840, -0.2930, -1.5237,  1.3262]],
       dtype=torch.float64)
	q_value: tensor([[-37.0822]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16651797433887838, distance: 1.0447317961356954 entropy 1.3432114124298096
epoch: 11, step: 92
	action: tensor([[-0.5907,  0.3581, -0.6928, -0.3044,  0.5388, -1.1963,  0.5303]],
       dtype=torch.float64)
	q_value: tensor([[-42.5352]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13584573481668571, distance: 1.2195970607886317 entropy 1.3432114124298096
epoch: 11, step: 93
	action: tensor([[ 3.0656,  1.6362,  0.0688,  0.4435, -0.4961, -0.6102, -1.2721]],
       dtype=torch.float64)
	q_value: tensor([[-28.7657]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 11, step: 94
	action: tensor([[ 0.2739, -0.8547,  0.2264, -0.0114, -0.1468, -1.0309,  0.7611]],
       dtype=torch.float64)
	q_value: tensor([[-47.2428]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.48410187668520943, distance: 1.394082740402692 entropy 1.3432114124298096
epoch: 11, step: 95
	action: tensor([[-0.7414,  0.5114, -0.2869, -0.5805, -0.5525, -0.5044, -0.8801]],
       dtype=torch.float64)
	q_value: tensor([[-29.8534]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.25594689037751883, distance: 1.2824555777473754 entropy 1.3432114124298096
epoch: 11, step: 96
	action: tensor([[ 0.3850,  0.2962,  0.0398, -0.7500,  1.6886, -0.4137,  0.0642]],
       dtype=torch.float64)
	q_value: tensor([[-28.3751]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4018030591411702, distance: 0.8850723779390098 entropy 1.3432114124298096
epoch: 11, step: 97
	action: tensor([[ 0.4009, -1.6295, -0.9229,  0.7204,  0.3965, -0.4493,  1.2129]],
       dtype=torch.float64)
	q_value: tensor([[-25.3705]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 11, step: 98
	action: tensor([[ 1.1101,  0.1255, -0.7340,  0.1139, -0.0139,  0.0616, -1.0985]],
       dtype=torch.float64)
	q_value: tensor([[-47.2428]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8731579875107652, distance: 0.40755690473871836 entropy 1.3432114124298096
epoch: 11, step: 99
	action: tensor([[ 1.1658, -1.0194, -0.2137, -1.2262, -0.1138,  0.4048,  1.5108]],
       dtype=torch.float64)
	q_value: tensor([[-23.2154]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8870082992267183, distance: 1.5719676285351454 entropy 1.3432114124298096
epoch: 11, step: 100
	action: tensor([[ 0.8602,  2.1736, -1.0129,  0.8545,  0.6571, -1.5067, -0.4389]],
       dtype=torch.float64)
	q_value: tensor([[-40.4388]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 11, step: 101
	action: tensor([[-0.1795,  1.0778, -0.0280,  0.1852, -1.2649, -1.1462,  2.2863]],
       dtype=torch.float64)
	q_value: tensor([[-47.2428]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7932086439575086, distance: 0.5203827404055901 entropy 1.3432114124298096
epoch: 11, step: 102
	action: tensor([[ 1.3045,  0.5196, -0.5283,  0.1224,  1.7574,  0.1177,  0.4561]],
       dtype=torch.float64)
	q_value: tensor([[-47.5481]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8883352544357714, distance: 0.3823972264191221 entropy 1.3432114124298096
epoch: 11, step: 103
	action: tensor([[-0.6201,  0.2182,  0.4882, -0.7927,  0.5833, -0.3651, -1.0173]],
       dtype=torch.float64)
	q_value: tensor([[-30.7041]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8869569910354169, distance: 1.5719462573085057 entropy 1.3432114124298096
epoch: 11, step: 104
	action: tensor([[-0.0676,  0.5396, -0.3463,  0.5158,  0.2840,  0.4394, -2.1249]],
       dtype=torch.float64)
	q_value: tensor([[-21.9776]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.493423580408067, distance: 0.8144776630574184 entropy 1.3432114124298096
epoch: 11, step: 105
	action: tensor([[ 1.3538,  0.5696,  0.5559,  0.6470, -1.2225,  0.3445, -0.8052]],
       dtype=torch.float64)
	q_value: tensor([[-29.1955]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 11, step: 106
	action: tensor([[ 0.6016,  0.6359,  0.6878,  0.2598,  1.9669, -1.1243, -0.7969]],
       dtype=torch.float64)
	q_value: tensor([[-47.2428]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.852989753682614, distance: 0.4387635037587139 entropy 1.3432114124298096
epoch: 11, step: 107
	action: tensor([[ 0.4040, -1.0209,  0.6060, -0.8566,  0.7076,  1.7400,  0.8823]],
       dtype=torch.float64)
	q_value: tensor([[-29.3951]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20561479214672618, distance: 1.2564956147096031 entropy 1.3432114124298096
epoch: 11, step: 108
	action: tensor([[ 0.3163, -0.4393,  0.8972, -0.4889, -1.6528,  0.7448, -0.0269]],
       dtype=torch.float64)
	q_value: tensor([[-40.4716]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.02243270874721781, distance: 1.1571084378873426 entropy 1.3432114124298096
epoch: 11, step: 109
	action: tensor([[-0.7226, -0.4442,  0.2579,  1.4992, -1.0742, -0.4222, -1.6630]],
       dtype=torch.float64)
	q_value: tensor([[-41.3320]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2546408346650345, distance: 0.9879602954739086 entropy 1.3432114124298096
epoch: 11, step: 110
	action: tensor([[ 0.6877,  0.2621,  0.4938, -2.4438,  0.0168,  0.1282, -0.1135]],
       dtype=torch.float64)
	q_value: tensor([[-34.7758]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04501010969256902, distance: 1.169814335811622 entropy 1.3432114124298096
epoch: 11, step: 111
	action: tensor([[ 1.0563, -1.1855,  0.2818,  0.4787,  2.1503,  0.6428,  1.1840]],
       dtype=torch.float64)
	q_value: tensor([[-38.3706]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.380217124440585, distance: 1.3444058667348344 entropy 1.3432114124298096
epoch: 11, step: 112
	action: tensor([[-0.3976, -0.8040,  1.2054, -1.0351, -0.9519,  0.3779, -0.6948]],
       dtype=torch.float64)
	q_value: tensor([[-41.3971]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1946447894804355, distance: 1.6952697490902644 entropy 1.3432114124298096
epoch: 11, step: 113
	action: tensor([[ 1.0104, -1.6271,  0.6596, -1.2245, -1.1136, -0.2577,  1.8883]],
       dtype=torch.float64)
	q_value: tensor([[-37.1912]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 11, step: 114
	action: tensor([[-0.8104,  0.8571, -1.1183, -1.0132, -0.2929, -1.2339,  0.7930]],
       dtype=torch.float64)
	q_value: tensor([[-47.2428]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5843371687765461, distance: 0.7377806980965157 entropy 1.3432114124298096
epoch: 11, step: 115
	action: tensor([[ 0.4579,  0.0945,  0.1984, -0.0100, -1.1750,  0.6058,  0.3552]],
       dtype=torch.float64)
	q_value: tensor([[-40.6980]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7088812529234368, distance: 0.6174357615764205 entropy 1.3432114124298096
epoch: 11, step: 116
	action: tensor([[-0.5653, -2.3057, -0.2109,  1.2331, -0.2939,  0.1455, -0.7522]],
       dtype=torch.float64)
	q_value: tensor([[-30.2852]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 11, step: 117
	action: tensor([[ 0.0995, -0.2960,  0.7325, -1.6703,  0.2516, -0.2674, -0.0697]],
       dtype=torch.float64)
	q_value: tensor([[-47.2428]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6032074749770189, distance: 1.4489438565347956 entropy 1.3432114124298096
epoch: 11, step: 118
	action: tensor([[ 0.3262, -0.9822,  1.7495,  0.6432,  0.5805,  0.1989, -1.0269]],
       dtype=torch.float64)
	q_value: tensor([[-29.7998]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18457599676876368, distance: 1.2454840131891984 entropy 1.3432114124298096
epoch: 11, step: 119
	action: tensor([[ 0.8016, -1.0002,  0.4616, -0.6254, -0.3995,  0.1787,  1.1325]],
       dtype=torch.float64)
	q_value: tensor([[-33.2455]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6546706429121345, distance: 1.472015808735095 entropy 1.3432114124298096
epoch: 11, step: 120
	action: tensor([[-1.4310, -0.7470,  1.1741,  1.1622, -0.9978,  0.2458, -0.1983]],
       dtype=torch.float64)
	q_value: tensor([[-34.6200]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5370255990584019, distance: 1.4187218048727024 entropy 1.3432114124298096
epoch: 11, step: 121
	action: tensor([[ 1.5143, -0.5747, -0.3131, -1.8610, -0.7939,  0.7629,  0.1550]],
       dtype=torch.float64)
	q_value: tensor([[-37.3671]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5134125260736564, distance: 1.4077818294131108 entropy 1.3432114124298096
epoch: 11, step: 122
	action: tensor([[ 0.6601, -0.5850, -1.4002, -1.2934, -0.9995, -0.0481,  1.0803]],
       dtype=torch.float64)
	q_value: tensor([[-42.8536]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20348426337411596, distance: 1.021301273677417 entropy 1.3432114124298096
epoch: 11, step: 123
	action: tensor([[ 1.8718,  2.2413, -1.5844,  1.2313, -0.3099,  0.9566,  0.3462]],
       dtype=torch.float64)
	q_value: tensor([[-42.3943]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 11, step: 124
	action: tensor([[-0.6501, -1.0029, -0.1868, -0.5770, -1.3078,  0.6444,  0.0633]],
       dtype=torch.float64)
	q_value: tensor([[-47.2428]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1952549223431315, distance: 1.6955053835562246 entropy 1.3432114124298096
epoch: 11, step: 125
	action: tensor([[ 2.2080,  0.5109, -0.4388, -0.0787, -0.2544, -0.3991,  2.1026]],
       dtype=torch.float64)
	q_value: tensor([[-36.7476]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
epoch: 11, step: 126
	action: tensor([[ 0.8276, -1.6156,  0.0068, -0.0047, -0.6963,  0.0909, -1.6960]],
       dtype=torch.float64)
	q_value: tensor([[-47.2428]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5868357923886502, distance: 1.4415266878758572 entropy 1.3432114124298096
epoch: 11, step: 127
	action: tensor([[-0.7571, -1.4016,  0.8325, -0.2394,  0.0969,  0.0456, -0.2166]],
       dtype=torch.float64)
	q_value: tensor([[-30.9355]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.3432114124298096
LOSS epoch 11 actor 476.21812219618766 critic 172.7420464426246 
epoch: 12, step: 0
	action: tensor([[ 1.3355, -0.3429,  0.2224, -1.8020,  0.7204, -1.1899,  2.4436]],
       dtype=torch.float64)
	q_value: tensor([[-51.7499]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08965162942710447, distance: 1.0918437762609252 entropy 1.2378510236740112
epoch: 12, step: 1
	action: tensor([[ 0.6931, -0.3861,  1.5092,  0.4361, -0.5337,  0.0150,  0.3927]],
       dtype=torch.float64)
	q_value: tensor([[-58.6384]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4302662865862611, distance: 0.8637591338058437 entropy 1.2378510236740112
epoch: 12, step: 2
	action: tensor([[ 0.1823,  0.5059, -0.0183,  0.3629, -0.3884, -0.0628,  0.5449]],
       dtype=torch.float64)
	q_value: tensor([[-37.1896]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7627165283798107, distance: 0.5574301271992007 entropy 1.2378510236740112
epoch: 12, step: 3
	action: tensor([[-0.6029,  0.4581, -0.2409,  1.2587,  0.9928, -0.4647, -1.7614]],
       dtype=torch.float64)
	q_value: tensor([[-21.9554]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2814196408647385, distance: 0.9700505461766561 entropy 1.2378510236740112
epoch: 12, step: 4
	action: tensor([[ 0.8992,  0.4496, -0.1759, -0.0115,  0.0356, -0.1202, -0.7051]],
       dtype=torch.float64)
	q_value: tensor([[-35.1949]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8670973106268811, distance: 0.4171800942662753 entropy 1.2378510236740112
epoch: 12, step: 5
	action: tensor([[ 0.7464,  0.1991, -1.5415, -1.0945, -1.1775,  0.6649, -0.8405]],
       dtype=torch.float64)
	q_value: tensor([[-22.1330]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4929546507843555, distance: 0.8148545502781221 entropy 1.2378510236740112
epoch: 12, step: 6
	action: tensor([[ 1.4854,  1.0170,  0.2509, -1.1825, -1.5156,  0.4394,  1.0079]],
       dtype=torch.float64)
	q_value: tensor([[-46.9415]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7172252621393993, distance: 0.6085229997741892 entropy 1.2378510236740112
epoch: 12, step: 7
	action: tensor([[ 0.5339,  0.3078,  0.2819, -0.1222,  0.7457, -1.4703,  1.5453]],
       dtype=torch.float64)
	q_value: tensor([[-55.8910]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7686615124017747, distance: 0.5504028059615972 entropy 1.2378510236740112
epoch: 12, step: 8
	action: tensor([[-0.3441,  0.5222,  0.5341, -1.7450,  0.7834, -0.2484,  0.1953]],
       dtype=torch.float64)
	q_value: tensor([[-40.6556]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21465123551030008, distance: 1.2611957291318219 entropy 1.2378510236740112
epoch: 12, step: 9
	action: tensor([[-0.0957, -0.1423,  0.5609, -0.6980, -0.1764,  0.4898,  0.5977]],
       dtype=torch.float64)
	q_value: tensor([[-33.9481]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07066139110406366, distance: 1.10317315063107 entropy 1.2378510236740112
epoch: 12, step: 10
	action: tensor([[-0.2671, -0.6473,  0.8772,  0.5357,  0.1846, -0.5231,  0.3798]],
       dtype=torch.float64)
	q_value: tensor([[-29.1882]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04706744001312446, distance: 1.1170890027631264 entropy 1.2378510236740112
epoch: 12, step: 11
	action: tensor([[ 0.4513,  0.7126, -0.0259, -2.0717,  0.0777, -1.1170,  1.0904]],
       dtype=torch.float64)
	q_value: tensor([[-30.8829]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29556888785627466, distance: 0.9604526468551605 entropy 1.2378510236740112
epoch: 12, step: 12
	action: tensor([[ 1.8003,  0.9609, -0.0080, -0.5641,  0.3903, -0.1357,  1.1425]],
       dtype=torch.float64)
	q_value: tensor([[-47.5934]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4394534201116772, distance: 0.8567666377071698 entropy 1.2378510236740112
epoch: 12, step: 13
	action: tensor([[ 0.4128,  1.7675,  0.0688, -1.1856,  1.0383, -0.3349, -0.2935]],
       dtype=torch.float64)
	q_value: tensor([[-36.8712]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 12, step: 14
	action: tensor([[ 0.4657, -0.6993,  2.4866, -0.7895,  0.1853, -0.2292, -0.3777]],
       dtype=torch.float64)
	q_value: tensor([[-51.7499]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37443252127061943, distance: 0.9050941852510693 entropy 1.2378510236740112
epoch: 12, step: 15
	action: tensor([[ 1.1564,  0.7336,  0.8929,  0.5240,  0.8897, -0.1746,  1.7446]],
       dtype=torch.float64)
	q_value: tensor([[-45.4516]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8401915200404948, distance: 0.4574636634971929 entropy 1.2378510236740112
epoch: 12, step: 16
	action: tensor([[ 0.7900,  0.7001, -0.2324, -0.8875,  1.3004,  1.6967, -0.9545]],
       dtype=torch.float64)
	q_value: tensor([[-39.2147]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9267587438597212, distance: 0.3096952792377376 entropy 1.2378510236740112
epoch: 12, step: 17
	action: tensor([[ 1.3296, -1.4366, -1.3285,  2.3144,  0.8562, -0.0288, -1.7894]],
       dtype=torch.float64)
	q_value: tensor([[-38.4802]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6268714299308205, distance: 0.699014114587213 entropy 1.2378510236740112
epoch: 12, step: 18
	action: tensor([[ 2.0044,  0.3718,  2.3235,  1.2400, -1.0219,  0.4040,  0.3529]],
       dtype=torch.float64)
	q_value: tensor([[-51.6969]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 12, step: 19
	action: tensor([[ 1.1064,  1.9498, -0.0218,  1.7233,  0.2789, -1.5511,  2.0100]],
       dtype=torch.float64)
	q_value: tensor([[-51.7499]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 12, step: 20
	action: tensor([[ 0.1977,  0.7276,  0.5653,  0.8028,  0.4870, -0.1468,  1.2951]],
       dtype=torch.float64)
	q_value: tensor([[-51.7499]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 12, step: 21
	action: tensor([[-0.8790, -0.2385,  0.9300, -0.4944, -0.0794, -0.0536, -0.3943]],
       dtype=torch.float64)
	q_value: tensor([[-51.7499]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.3632887092468273, distance: 1.7591994611359607 entropy 1.2378510236740112
epoch: 12, step: 22
	action: tensor([[-0.2582,  0.0023,  0.7499, -0.7114, -0.3881, -0.4708, -1.1286]],
       dtype=torch.float64)
	q_value: tensor([[-28.1125]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6018602480892437, distance: 1.4483349314832399 entropy 1.2378510236740112
epoch: 12, step: 23
	action: tensor([[-0.6198,  0.7034, -1.3059,  0.1186,  0.7805,  0.3356,  0.2140]],
       dtype=torch.float64)
	q_value: tensor([[-28.6339]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09895273786687975, distance: 1.1996269145741165 entropy 1.2378510236740112
epoch: 12, step: 24
	action: tensor([[-1.4115, -0.7430,  0.2726, -1.8556, -0.4779,  0.0249,  0.2796]],
       dtype=torch.float64)
	q_value: tensor([[-33.9662]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.41757574345529314, distance: 1.3624790399075124 entropy 1.2378510236740112
epoch: 12, step: 25
	action: tensor([[ 0.9538,  0.5424,  0.4707,  0.5896, -0.5049, -0.9809, -1.4646]],
       dtype=torch.float64)
	q_value: tensor([[-49.4473]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.964406456420668, distance: 0.21589486199271077 entropy 1.2378510236740112
epoch: 12, step: 26
	action: tensor([[ 0.4942,  1.4628,  0.7221, -0.1424,  2.4878, -0.7684,  0.3088]],
       dtype=torch.float64)
	q_value: tensor([[-34.2779]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 12, step: 27
	action: tensor([[ 0.0301,  0.2053, -0.7324,  0.4795,  0.2110, -1.1220, -0.7435]],
       dtype=torch.float64)
	q_value: tensor([[-51.7499]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44451544103427754, distance: 0.85288934496642 entropy 1.2378510236740112
epoch: 12, step: 28
	action: tensor([[ 0.6714, -0.9095,  0.2024,  0.4497, -0.4917, -0.5127, -0.4013]],
       dtype=torch.float64)
	q_value: tensor([[-27.8111]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.012228118939929589, distance: 1.1513195838243067 entropy 1.2378510236740112
epoch: 12, step: 29
	action: tensor([[-1.0993,  0.0840,  1.1212, -0.7471, -0.6390,  1.0590, -1.0749]],
       dtype=torch.float64)
	q_value: tensor([[-28.6351]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0140389874820803, distance: 1.624017220094508 entropy 1.2378510236740112
epoch: 12, step: 30
	action: tensor([[-0.9513,  0.1744,  1.8369,  0.1222, -0.4774,  1.5814,  0.0976]],
       dtype=torch.float64)
	q_value: tensor([[-44.1132]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09009015591895686, distance: 1.0915807670045001 entropy 1.2378510236740112
epoch: 12, step: 31
	action: tensor([[-0.7396,  0.5792, -1.1097, -0.3446,  0.2871,  0.6043, -1.9443]],
       dtype=torch.float64)
	q_value: tensor([[-46.2718]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12901706147793135, distance: 1.2159254420474352 entropy 1.2378510236740112
epoch: 12, step: 32
	action: tensor([[ 0.1684, -1.3291,  0.0778,  0.2307,  0.3687,  0.2545,  0.1586]],
       dtype=torch.float64)
	q_value: tensor([[-40.8087]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4819408102638645, distance: 1.3930673778528395 entropy 1.2378510236740112
epoch: 12, step: 33
	action: tensor([[ 0.8367,  0.3003,  0.0893, -1.6329, -0.3713, -1.0677,  0.6971]],
       dtype=torch.float64)
	q_value: tensor([[-27.6561]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1618129268858155, distance: 1.0476764277884196 entropy 1.2378510236740112
epoch: 12, step: 34
	action: tensor([[ 0.4744, -0.0939,  0.6177,  0.6023, -0.6606, -1.4728,  0.2106]],
       dtype=torch.float64)
	q_value: tensor([[-41.5622]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6250770820202791, distance: 0.7006928525649686 entropy 1.2378510236740112
epoch: 12, step: 35
	action: tensor([[ 0.7380,  1.5826,  0.1890,  0.0579, -1.5062,  0.9095, -0.1980]],
       dtype=torch.float64)
	q_value: tensor([[-35.1291]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 12, step: 36
	action: tensor([[ 1.3875,  0.8164,  0.3748,  0.2950,  0.7771,  1.9908, -0.3781]],
       dtype=torch.float64)
	q_value: tensor([[-51.7499]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7611600069225919, distance: 0.5592554412418788 entropy 1.2378510236740112
epoch: 12, step: 37
	action: tensor([[ 0.0554,  0.4498,  0.5714, -0.2017,  0.7862, -1.0144,  0.5525]],
       dtype=torch.float64)
	q_value: tensor([[-41.4640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23756702769989024, distance: 1.2730370936461755 entropy 1.2378510236740112
epoch: 12, step: 38
	action: tensor([[ 1.3989, -0.3924, -1.0891, -0.5487, -2.0260,  0.0843, -0.6844]],
       dtype=torch.float64)
	q_value: tensor([[-28.2096]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15845674909185925, distance: 1.0497718285163764 entropy 1.2378510236740112
epoch: 12, step: 39
	action: tensor([[ 1.7418, -0.1601,  0.5560,  0.1807,  2.1063, -0.7271, -0.4100]],
       dtype=torch.float64)
	q_value: tensor([[-47.3127]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 12, step: 40
	action: tensor([[-0.2337, -1.1162,  1.5443, -0.6863, -0.6714, -0.6280, -0.0534]],
       dtype=torch.float64)
	q_value: tensor([[-51.7499]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.179312515386853, distance: 1.6893376053297848 entropy 1.2378510236740112
epoch: 12, step: 41
	action: tensor([[ 1.2779,  0.6343,  1.0372, -0.7242, -0.3151,  0.1695, -0.1814]],
       dtype=torch.float64)
	q_value: tensor([[-41.5696]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9733426114616607, distance: 0.18683812239852782 entropy 1.2378510236740112
epoch: 12, step: 42
	action: tensor([[ 0.4899,  0.4613, -0.4649,  0.3635,  0.2969,  1.0726,  1.7602]],
       dtype=torch.float64)
	q_value: tensor([[-35.6921]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8123871516771219, distance: 0.4956646898339433 entropy 1.2378510236740112
epoch: 12, step: 43
	action: tensor([[-1.6056, -0.3677,  0.7301,  1.3554, -1.2361, -0.3955, -0.6312]],
       dtype=torch.float64)
	q_value: tensor([[-38.9080]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5261851428301119, distance: 1.413709915256536 entropy 1.2378510236740112
epoch: 12, step: 44
	action: tensor([[ 0.7155, -1.3028,  0.4409,  0.1232,  2.2767, -0.6990,  0.2174]],
       dtype=torch.float64)
	q_value: tensor([[-41.0288]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2418511340785352, distance: 1.2752386368606736 entropy 1.2378510236740112
epoch: 12, step: 45
	action: tensor([[ 0.7441,  0.4624, -0.8746, -0.9306, -0.5861,  0.2696, -0.4447]],
       dtype=torch.float64)
	q_value: tensor([[-44.0083]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7844174047283872, distance: 0.5313290254305151 entropy 1.2378510236740112
epoch: 12, step: 46
	action: tensor([[-0.7296, -0.3566, -0.5018, -0.1520, -1.0387,  1.5840, -0.4323]],
       dtype=torch.float64)
	q_value: tensor([[-34.6760]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8495554105188576, distance: 1.5562894258723288 entropy 1.2378510236740112
epoch: 12, step: 47
	action: tensor([[ 0.7103, -0.8695,  0.2468,  0.5039, -0.8146,  0.5641,  1.0186]],
       dtype=torch.float64)
	q_value: tensor([[-42.6013]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.423237298822826, distance: 0.8690710372575805 entropy 1.2378510236740112
epoch: 12, step: 48
	action: tensor([[-0.3681, -0.4734, -0.1351, -0.5954, -0.1958,  1.0715, -0.4372]],
       dtype=torch.float64)
	q_value: tensor([[-35.9700]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4320267007602083, distance: 1.3694060779408594 entropy 1.2378510236740112
epoch: 12, step: 49
	action: tensor([[-0.6681,  0.2413,  1.0556, -0.6412, -0.8951,  0.3632, -0.7174]],
       dtype=torch.float64)
	q_value: tensor([[-31.0139]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7373273658474986, distance: 1.508334009673468 entropy 1.2378510236740112
epoch: 12, step: 50
	action: tensor([[0.4216, 1.1177, 0.2158, 1.2495, 1.1265, 0.1524, 1.1682]],
       dtype=torch.float64)
	q_value: tensor([[-37.7514]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5060659613133781, distance: 0.8042501882458919 entropy 1.2378510236740112
epoch: 12, step: 51
	action: tensor([[ 2.3207,  0.5682, -0.6506,  0.8503, -0.2323, -0.1958,  0.1500]],
       dtype=torch.float64)
	q_value: tensor([[-32.4299]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 12, step: 52
	action: tensor([[ 0.6935,  1.1911, -0.5967, -0.1061,  1.0407, -0.4298, -2.1045]],
       dtype=torch.float64)
	q_value: tensor([[-51.7499]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 12, step: 53
	action: tensor([[ 0.3159, -0.6852,  0.2558,  0.2473, -0.9785,  0.4199, -1.4784]],
       dtype=torch.float64)
	q_value: tensor([[-51.7499]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17007244410650035, distance: 1.0425017329484678 entropy 1.2378510236740112
epoch: 12, step: 54
	action: tensor([[ 0.0229, -0.1817, -0.2494, -1.2190,  0.5224, -0.3571,  0.0998]],
       dtype=torch.float64)
	q_value: tensor([[-33.0124]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23567664609800998, distance: 1.2720644410382382 entropy 1.2378510236740112
epoch: 12, step: 55
	action: tensor([[ 1.0211, -0.9356,  1.0494,  1.0004, -1.5517, -0.0785, -0.3554]],
       dtype=torch.float64)
	q_value: tensor([[-26.2132]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07577475631321595, distance: 1.100134049549737 entropy 1.2378510236740112
epoch: 12, step: 56
	action: tensor([[-0.5230, -0.1136,  0.4513, -0.3677,  1.0491, -1.4850, -0.0081]],
       dtype=torch.float64)
	q_value: tensor([[-42.4547]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6671979648261046, distance: 1.477577533332059 entropy 1.2378510236740112
epoch: 12, step: 57
	action: tensor([[ 0.4740, -0.2858, -1.3523, -0.9933,  1.0214, -1.6261, -0.0906]],
       dtype=torch.float64)
	q_value: tensor([[-34.1364]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2403077252252045, distance: 0.9974142033108386 entropy 1.2378510236740112
epoch: 12, step: 58
	action: tensor([[ 0.7510,  1.4103, -0.0255, -0.6432, -0.1877, -0.7997,  0.9019]],
       dtype=torch.float64)
	q_value: tensor([[-41.0271]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9870064779083043, distance: 0.13044280760816784 entropy 1.2378510236740112
epoch: 12, step: 59
	action: tensor([[ 1.5287,  0.6287, -0.7147, -0.0171,  0.1155,  0.1755,  0.4944]],
       dtype=torch.float64)
	q_value: tensor([[-35.8046]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7726379777401148, distance: 0.5456518788775955 entropy 1.2378510236740112
epoch: 12, step: 60
	action: tensor([[-0.0800, -0.1444,  0.9242, -0.4639,  0.8090,  0.3144,  0.9755]],
       dtype=torch.float64)
	q_value: tensor([[-30.5086]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1797764088690963, distance: 1.2429582666610621 entropy 1.2378510236740112
epoch: 12, step: 61
	action: tensor([[ 0.2739,  0.2573, -0.0912, -0.5171,  1.2587,  0.7547, -1.1042]],
       dtype=torch.float64)
	q_value: tensor([[-31.7033]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6958704497133044, distance: 0.6310823050645216 entropy 1.2378510236740112
epoch: 12, step: 62
	action: tensor([[ 0.3762, -0.3755,  0.9056,  0.4703, -1.3859,  0.8512, -0.3233]],
       dtype=torch.float64)
	q_value: tensor([[-25.8131]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7271771688692896, distance: 0.5977189831501218 entropy 1.2378510236740112
epoch: 12, step: 63
	action: tensor([[-0.7203,  0.5107,  1.2199,  0.4192, -0.6128,  0.1719,  0.1715]],
       dtype=torch.float64)
	q_value: tensor([[-38.4237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1974444242449196, distance: 1.0251661350882675 entropy 1.2378510236740112
epoch: 12, step: 64
	action: tensor([[-1.0370,  1.0669, -2.5761, -0.8449, -1.2124, -0.0364, -0.3362]],
       dtype=torch.float64)
	q_value: tensor([[-31.6668]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 12, step: 65
	action: tensor([[ 0.1918, -0.9632, -0.6234, -1.4876, -0.1059,  0.6746,  0.6019]],
       dtype=torch.float64)
	q_value: tensor([[-51.7499]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4154370700856842, distance: 1.361450877031548 entropy 1.2378510236740112
epoch: 12, step: 66
	action: tensor([[ 0.2985, -0.8745, -0.2447,  1.2256, -0.1484,  0.2114, -1.5660]],
       dtype=torch.float64)
	q_value: tensor([[-38.9464]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4794348717144794, distance: 0.8256466618285286 entropy 1.2378510236740112
epoch: 12, step: 67
	action: tensor([[ 1.1413,  0.2627, -0.2038, -0.4831,  0.3338,  1.4094,  1.0497]],
       dtype=torch.float64)
	q_value: tensor([[-34.2755]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.911234671248683, distance: 0.3409403292356515 entropy 1.2378510236740112
epoch: 12, step: 68
	action: tensor([[ 0.3796, -0.4737,  0.1674,  0.3267,  0.6563,  0.5991,  0.6246]],
       dtype=torch.float64)
	q_value: tensor([[-39.1497]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6206624695376197, distance: 0.7048060113423797 entropy 1.2378510236740112
epoch: 12, step: 69
	action: tensor([[ 1.0978,  0.1387,  0.6137, -2.1036, -0.3442,  1.4380,  0.9031]],
       dtype=torch.float64)
	q_value: tensor([[-26.1173]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5371668499507862, distance: 0.7785184726793762 entropy 1.2378510236740112
epoch: 12, step: 70
	action: tensor([[ 4.9822e-04, -4.1915e-01,  2.3595e-01,  2.1746e-01,  4.0797e-01,
          8.9585e-01, -2.7740e-01]], dtype=torch.float64)
	q_value: tensor([[-53.6772]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44628346793472196, distance: 0.8515309515820184 entropy 1.2378510236740112
epoch: 12, step: 71
	action: tensor([[ 0.1087,  1.4154, -0.9919,  0.0966, -0.5356, -0.2934, -1.0208]],
       dtype=torch.float64)
	q_value: tensor([[-22.8129]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 12, step: 72
	action: tensor([[ 1.5035,  1.9831, -0.6511,  1.0439, -0.1086, -0.4115,  0.5547]],
       dtype=torch.float64)
	q_value: tensor([[-51.7499]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 12, step: 73
	action: tensor([[ 0.8020, -1.3429, -0.3456, -1.7034,  1.0687,  0.1055, -0.2873]],
       dtype=torch.float64)
	q_value: tensor([[-51.7499]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.705240901030497, distance: 1.4943404887478475 entropy 1.2378510236740112
epoch: 12, step: 74
	action: tensor([[ 0.8969, -0.0521,  0.6094,  0.6531,  0.2349,  0.4937, -0.9408]],
       dtype=torch.float64)
	q_value: tensor([[-34.3372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9095610236447814, distance: 0.34413949122704607 entropy 1.2378510236740112
epoch: 12, step: 75
	action: tensor([[ 0.3396,  0.5987,  0.1660, -0.1111,  0.9688,  0.1945, -0.4898]],
       dtype=torch.float64)
	q_value: tensor([[-27.2742]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8572115339494903, distance: 0.4324174957288516 entropy 1.2378510236740112
epoch: 12, step: 76
	action: tensor([[ 0.2842, -0.3982, -0.7632,  0.3580,  0.9754,  0.9092, -0.0481]],
       dtype=torch.float64)
	q_value: tensor([[-20.2212]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5693937445084586, distance: 0.750925514144303 entropy 1.2378510236740112
epoch: 12, step: 77
	action: tensor([[ 1.6816,  1.5103,  0.9772, -0.3267, -0.4686, -1.9807,  0.4859]],
       dtype=torch.float64)
	q_value: tensor([[-27.7414]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8747015590095513, distance: 0.4050694841990485 entropy 1.2378510236740112
epoch: 12, step: 78
	action: tensor([[ 0.9498, -0.0841,  1.0731,  0.7748,  0.3970, -1.3294,  0.2632]],
       dtype=torch.float64)
	q_value: tensor([[-47.6198]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8338389153472129, distance: 0.46646745897018616 entropy 1.2378510236740112
epoch: 12, step: 79
	action: tensor([[-0.1709,  0.6163,  1.9232,  0.9725,  0.5260, -1.1595, -1.7115]],
       dtype=torch.float64)
	q_value: tensor([[-36.7629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7745621361668685, distance: 0.5433380547342241 entropy 1.2378510236740112
epoch: 12, step: 80
	action: tensor([[ 0.7310,  0.5808, -0.0549, -0.5563,  0.9200, -1.7365,  1.8363]],
       dtype=torch.float64)
	q_value: tensor([[-39.6041]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7175742668984202, distance: 0.6081473597951542 entropy 1.2378510236740112
epoch: 12, step: 81
	action: tensor([[-0.5422, -0.4390,  0.3404, -0.9149,  0.8421,  0.5320, -0.8569]],
       dtype=torch.float64)
	q_value: tensor([[-45.9514]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8969400138737917, distance: 1.5760989948560093 entropy 1.2378510236740112
epoch: 12, step: 82
	action: tensor([[ 0.9606, -1.3012, -1.0885,  0.7115,  0.2837, -0.9559, -0.1961]],
       dtype=torch.float64)
	q_value: tensor([[-26.9215]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.48050915791073856, distance: 1.3923943178742904 entropy 1.2378510236740112
epoch: 12, step: 83
	action: tensor([[ 1.6321,  0.0808, -0.0267, -0.4011, -1.4119,  0.0887,  0.3919]],
       dtype=torch.float64)
	q_value: tensor([[-36.1586]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3120414399613055, distance: 0.9491565141163977 entropy 1.2378510236740112
epoch: 12, step: 84
	action: tensor([[-0.3130,  0.0672, -0.0358, -0.5546,  0.1949,  0.7314,  0.1713]],
       dtype=torch.float64)
	q_value: tensor([[-40.0796]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.014458437118822776, distance: 1.1525872803267467 entropy 1.2378510236740112
epoch: 12, step: 85
	action: tensor([[-1.5667,  1.0582,  0.2492,  0.4286,  1.2867,  0.1582,  0.6774]],
       dtype=torch.float64)
	q_value: tensor([[-25.8528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 12, step: 86
	action: tensor([[ 0.8357, -0.6671,  0.2705, -0.3966,  0.5494, -0.4378, -0.1360]],
       dtype=torch.float64)
	q_value: tensor([[-51.7499]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3048035721462936, distance: 1.3071615413085447 entropy 1.2378510236740112
epoch: 12, step: 87
	action: tensor([[ 0.1933,  1.7557,  0.2245, -1.2934,  1.0206, -1.2336, -2.9038]],
       dtype=torch.float64)
	q_value: tensor([[-25.8029]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38860220637859444, distance: 0.8947848594435577 entropy 1.2378510236740112
epoch: 12, step: 88
	action: tensor([[ 1.0624,  0.6986, -0.0682, -0.3755, -0.3932, -0.2578,  0.6080]],
       dtype=torch.float64)
	q_value: tensor([[-48.8493]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9267007845969849, distance: 0.3098177932502933 entropy 1.2378510236740112
epoch: 12, step: 89
	action: tensor([[ 1.4997,  0.2482, -0.2725, -0.4198, -0.5199, -0.5616,  1.5575]],
       dtype=torch.float64)
	q_value: tensor([[-29.3569]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32144820695627263, distance: 0.9426450568985065 entropy 1.2378510236740112
epoch: 12, step: 90
	action: tensor([[ 1.1588,  1.4292,  0.3395, -0.4273,  0.1798, -1.4112, -0.4561]],
       dtype=torch.float64)
	q_value: tensor([[-40.4051]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9659984617070654, distance: 0.21101142781304866 entropy 1.2378510236740112
epoch: 12, step: 91
	action: tensor([[ 0.8918, -0.4141,  1.5228,  0.3040,  1.5756, -0.8285, -0.4898]],
       dtype=torch.float64)
	q_value: tensor([[-35.7806]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6485733590235063, distance: 0.6783815595624136 entropy 1.2378510236740112
epoch: 12, step: 92
	action: tensor([[-0.2025, -0.4700,  0.8808,  0.5791, -0.3711, -1.1452, -1.3280]],
       dtype=torch.float64)
	q_value: tensor([[-37.8309]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1793099233337876, distance: 1.24271250849239 entropy 1.2378510236740112
epoch: 12, step: 93
	action: tensor([[ 1.4027,  0.6977, -1.2578, -0.8263,  0.1730,  0.6713,  0.0685]],
       dtype=torch.float64)
	q_value: tensor([[-34.6731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6343163051092154, distance: 0.6920054128176851 entropy 1.2378510236740112
epoch: 12, step: 94
	action: tensor([[ 1.5817, -1.2691,  1.9826, -1.3589,  0.5988, -0.4422, -0.0404]],
       dtype=torch.float64)
	q_value: tensor([[-38.0584]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5737675435721792, distance: 0.747102090712151 entropy 1.2378510236740112
epoch: 12, step: 95
	action: tensor([[ 2.0826, -0.4634,  0.2098,  0.2860,  1.1363, -0.9029,  0.3685]],
       dtype=torch.float64)
	q_value: tensor([[-49.8176]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 12, step: 96
	action: tensor([[ 0.1712,  0.0463,  0.3682,  0.8180,  0.1602, -0.1745, -1.3024]],
       dtype=torch.float64)
	q_value: tensor([[-51.7499]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8002874157587755, distance: 0.5113984518593073 entropy 1.2378510236740112
epoch: 12, step: 97
	action: tensor([[ 0.6636, -0.6979,  0.1471, -0.7165, -0.4023, -0.8841, -0.9767]],
       dtype=torch.float64)
	q_value: tensor([[-25.7420]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4977515520996365, distance: 1.400478940048494 entropy 1.2378510236740112
epoch: 12, step: 98
	action: tensor([[ 0.9155, -0.6295, -1.2535, -0.4200,  0.5778,  0.0571, -0.0194]],
       dtype=torch.float64)
	q_value: tensor([[-31.3373]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.009615602764423525, distance: 1.1388291844099963 entropy 1.2378510236740112
epoch: 12, step: 99
	action: tensor([[ 0.2679, -0.3185, -0.3071, -1.6094, -0.0154, -0.0555, -0.9977]],
       dtype=torch.float64)
	q_value: tensor([[-27.9020]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.22907382752288363, distance: 1.268661260648334 entropy 1.2378510236740112
epoch: 12, step: 100
	action: tensor([[ 1.0280,  0.4875,  0.7552,  0.6519, -0.3225, -0.6232, -1.2430]],
       dtype=torch.float64)
	q_value: tensor([[-31.8196]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9316854271902898, distance: 0.29909790545121806 entropy 1.2378510236740112
epoch: 12, step: 101
	action: tensor([[ 0.5028,  0.4220, -0.3797,  0.1773,  0.5536, -1.1953, -0.7214]],
       dtype=torch.float64)
	q_value: tensor([[-32.2750]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7855682125358457, distance: 0.5299089762255955 entropy 1.2378510236740112
epoch: 12, step: 102
	action: tensor([[ 1.1600,  0.9293,  0.2409, -0.2898, -1.1685,  0.8224,  1.3731]],
       dtype=torch.float64)
	q_value: tensor([[-27.9131]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9557165308678898, distance: 0.24081165985100245 entropy 1.2378510236740112
epoch: 12, step: 103
	action: tensor([[ 0.4148,  1.8152, -0.0268,  0.5550, -0.9851, -0.7378, -0.4892]],
       dtype=torch.float64)
	q_value: tensor([[-46.2886]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 12, step: 104
	action: tensor([[ 0.7196,  0.9977,  0.7151,  0.8271, -0.5158,  0.3478, -0.2612]],
       dtype=torch.float64)
	q_value: tensor([[-51.7499]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 12, step: 105
	action: tensor([[-0.0370,  0.3073, -0.2814, -1.2200, -0.4424,  0.4378,  1.8545]],
       dtype=torch.float64)
	q_value: tensor([[-51.7499]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17058688836136293, distance: 1.0421785769337328 entropy 1.2378510236740112
epoch: 12, step: 106
	action: tensor([[ 1.0075,  0.9864, -0.7363, -0.0427,  0.2733, -0.5292, -0.8186]],
       dtype=torch.float64)
	q_value: tensor([[-48.0498]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9475452859962595, distance: 0.2620890973002741 entropy 1.2378510236740112
epoch: 12, step: 107
	action: tensor([[-0.3768,  0.9818, -0.3613, -0.3812, -1.5384,  0.1561,  1.5488]],
       dtype=torch.float64)
	q_value: tensor([[-28.8950]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33897962542018334, distance: 0.9303880341372245 entropy 1.2378510236740112
epoch: 12, step: 108
	action: tensor([[ 1.2930, -0.0037, -0.0629, -0.0993,  0.8627,  0.2936,  0.2891]],
       dtype=torch.float64)
	q_value: tensor([[-49.2054]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6276344966595909, distance: 0.6982989892384606 entropy 1.2378510236740112
epoch: 12, step: 109
	action: tensor([[ 0.7675,  0.3737,  0.0277,  0.0873,  0.0363, -0.3041, -1.0979]],
       dtype=torch.float64)
	q_value: tensor([[-27.3206]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9427237339161944, distance: 0.2738697354379052 entropy 1.2378510236740112
epoch: 12, step: 110
	action: tensor([[ 1.0350,  1.2669, -0.0313, -0.6507, -0.8796,  0.5324, -0.6764]],
       dtype=torch.float64)
	q_value: tensor([[-23.6345]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9353069435664907, distance: 0.2910620127416228 entropy 1.2378510236740112
epoch: 12, step: 111
	action: tensor([[ 0.8419, -0.2917, -0.6285,  1.1565, -0.5974,  0.7609,  0.3157]],
       dtype=torch.float64)
	q_value: tensor([[-42.2384]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7832121763807461, distance: 0.5328121699402696 entropy 1.2378510236740112
epoch: 12, step: 112
	action: tensor([[ 0.7598,  0.5529,  0.0261, -0.0840,  0.3008,  0.3863,  0.3792]],
       dtype=torch.float64)
	q_value: tensor([[-29.8307]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9910830658100719, distance: 0.10805987827155737 entropy 1.2378510236740112
epoch: 12, step: 113
	action: tensor([[ 0.1458, -0.0897,  0.1137, -0.3457, -0.5913,  1.0642,  0.6382]],
       dtype=torch.float64)
	q_value: tensor([[-22.9214]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3615042401463209, distance: 0.9143989149350003 entropy 1.2378510236740112
epoch: 12, step: 114
	action: tensor([[-0.0229, -0.5449,  0.4812,  0.7616, -1.4906,  0.1058, -0.9740]],
       dtype=torch.float64)
	q_value: tensor([[-33.5516]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4225290347901799, distance: 0.8696044827318513 entropy 1.2378510236740112
epoch: 12, step: 115
	action: tensor([[ 0.4694,  0.5975, -0.2995, -0.1423, -0.2961,  1.3193, -0.0880]],
       dtype=torch.float64)
	q_value: tensor([[-33.9684]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8071660743168361, distance: 0.502514288020912 entropy 1.2378510236740112
epoch: 12, step: 116
	action: tensor([[ 2.3562,  0.4685,  0.5029, -0.0187, -0.9907, -0.7424, -0.5064]],
       dtype=torch.float64)
	q_value: tensor([[-33.7494]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 12, step: 117
	action: tensor([[ 0.3529, -1.2560,  0.0472, -0.7338, -1.1966, -0.2700,  1.2674]],
       dtype=torch.float64)
	q_value: tensor([[-51.7499]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9551557283500682, distance: 1.6001009041860899 entropy 1.2378510236740112
epoch: 12, step: 118
	action: tensor([[-0.1116, -1.3322,  0.4130,  0.4442, -0.0829,  0.4157,  0.7417]],
       dtype=torch.float64)
	q_value: tensor([[-43.9476]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 12, step: 119
	action: tensor([[ 0.3549,  1.8654, -0.4940, -0.1304, -1.2222,  0.0720,  0.1568]],
       dtype=torch.float64)
	q_value: tensor([[-51.7499]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 12, step: 120
	action: tensor([[ 1.2863,  0.5808, -1.5956,  0.6833, -1.7115,  0.0198,  0.1085]],
       dtype=torch.float64)
	q_value: tensor([[-51.7499]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.989877927917606, distance: 0.11513076917305616 entropy 1.2378510236740112
epoch: 12, step: 121
	action: tensor([[ 0.1282, -1.1371, -1.1148, -0.4316, -0.0267, -1.6731, -1.4085]],
       dtype=torch.float64)
	q_value: tensor([[-41.2681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.025839392263429795, distance: 1.1294629136224592 entropy 1.2378510236740112
epoch: 12, step: 122
	action: tensor([[ 0.0741, -1.1832, -0.5164, -0.7524, -0.2089, -0.7746, -0.1419]],
       dtype=torch.float64)
	q_value: tensor([[-39.8656]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.49954889111472034, distance: 1.401318992836871 entropy 1.2378510236740112
epoch: 12, step: 123
	action: tensor([[-0.8069,  0.4297,  1.1758,  0.4830, -0.4227,  0.3244,  0.3013]],
       dtype=torch.float64)
	q_value: tensor([[-30.8050]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17565466737082147, distance: 1.0389898022296469 entropy 1.2378510236740112
epoch: 12, step: 124
	action: tensor([[ 9.3632e-01, -2.2730e+00, -1.9975e-03, -1.6252e+00,  6.5759e-01,
          3.4539e-02, -1.2586e+00]], dtype=torch.float64)
	q_value: tensor([[-31.4552]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 12, step: 125
	action: tensor([[-0.7505,  0.8478,  0.0940, -0.2833, -0.4604,  1.1442,  0.0666]],
       dtype=torch.float64)
	q_value: tensor([[-51.7499]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.002550970285583931, distance: 1.142883727871317 entropy 1.2378510236740112
epoch: 12, step: 126
	action: tensor([[1.5138, 0.8566, 0.2295, 0.1407, 0.7587, 0.1244, 0.2469]],
       dtype=torch.float64)
	q_value: tensor([[-38.1501]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27594584841255954, distance: 0.9737382212252458 entropy 1.2378510236740112
epoch: 12, step: 127
	action: tensor([[ 2.5066,  1.1254,  0.1959, -0.5803, -1.2229, -0.2248, -0.8259]],
       dtype=torch.float64)
	q_value: tensor([[-29.1632]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
LOSS epoch 12 actor 542.898434770372 critic 143.47740891227895 
epoch: 13, step: 0
	action: tensor([[ 0.7396, -0.7274,  0.4290,  0.2724,  1.7065, -0.5633, -1.8592]],
       dtype=torch.float64)
	q_value: tensor([[-46.8840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31406872011312614, distance: 0.9477569924505764 entropy 1.2378510236740112
epoch: 13, step: 1
	action: tensor([[ 2.6595,  0.1588, -0.5731, -1.2052,  0.4215, -1.2410,  0.1618]],
       dtype=torch.float64)
	q_value: tensor([[-36.3258]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 13, step: 2
	action: tensor([[ 0.2205,  0.2315,  1.5560, -1.4382,  1.2852, -1.7544, -0.2850]],
       dtype=torch.float64)
	q_value: tensor([[-46.8840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4347124940692829, distance: 0.8603821400053928 entropy 1.2378510236740112
epoch: 13, step: 3
	action: tensor([[ 2.3700,  0.3099,  0.2211, -0.1509,  0.1834, -0.3866, -1.1175]],
       dtype=torch.float64)
	q_value: tensor([[-39.4298]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 13, step: 4
	action: tensor([[ 1.0161, -1.0796, -0.8383, -1.4917,  0.2233, -0.5228, -1.6789]],
       dtype=torch.float64)
	q_value: tensor([[-46.8840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5844649392516605, distance: 1.4404494101898648 entropy 1.2378510236740112
epoch: 13, step: 5
	action: tensor([[ 1.4392, -0.8576,  1.1643,  0.8067, -0.1321, -0.9740,  1.2016]],
       dtype=torch.float64)
	q_value: tensor([[-35.2875]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2661516136668638, distance: 1.2876550922611196 entropy 1.2378510236740112
epoch: 13, step: 6
	action: tensor([[ 1.9654,  0.3248,  1.2490, -1.1772,  0.0889,  1.4149, -0.1802]],
       dtype=torch.float64)
	q_value: tensor([[-45.0210]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 13, step: 7
	action: tensor([[ 1.3878, -2.4822,  1.2050,  1.2753, -0.2893, -1.2050,  0.1782]],
       dtype=torch.float64)
	q_value: tensor([[-46.8840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 13, step: 8
	action: tensor([[ 3.2452,  0.6340, -0.9417, -0.2242, -1.6898,  0.2064, -0.5657]],
       dtype=torch.float64)
	q_value: tensor([[-46.8840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 13, step: 9
	action: tensor([[ 2.1879, -0.2318, -0.0064, -0.4140,  1.2878,  0.5971,  0.2534]],
       dtype=torch.float64)
	q_value: tensor([[-46.8840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12252967817458393, distance: 1.2124270278934848 entropy 1.2378510236740112
epoch: 13, step: 10
	action: tensor([[ 1.0624,  0.2578,  0.4257, -1.1624,  0.0240, -0.9724,  1.9063]],
       dtype=torch.float64)
	q_value: tensor([[-31.8237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3347933468653129, distance: 0.9333294838095142 entropy 1.2378510236740112
epoch: 13, step: 11
	action: tensor([[ 1.8914, -1.2855,  0.0205,  0.8294, -1.0443,  0.0047,  1.1259]],
       dtype=torch.float64)
	q_value: tensor([[-41.5929]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 13, step: 12
	action: tensor([[ 1.2735,  0.8970,  0.8232, -1.0020, -2.1560, -0.8772, -0.8612]],
       dtype=torch.float64)
	q_value: tensor([[-46.8840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9508258214547943, distance: 0.25376121799230555 entropy 1.2378510236740112
epoch: 13, step: 13
	action: tensor([[-0.5830,  0.7432, -0.5286, -0.1422, -0.4895, -1.1585, -2.2750]],
       dtype=torch.float64)
	q_value: tensor([[-52.3913]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39791052834501317, distance: 0.8879473384160295 entropy 1.2378510236740112
epoch: 13, step: 14
	action: tensor([[ 1.1348, -0.2149,  0.1902, -0.2617,  0.8353,  0.0646, -0.1260]],
       dtype=torch.float64)
	q_value: tensor([[-39.0840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4091532334804715, distance: 0.8796180342440874 entropy 1.2378510236740112
epoch: 13, step: 15
	action: tensor([[ 1.8965, -0.1480,  0.0099,  0.8973, -0.5613, -0.1390, -0.8441]],
       dtype=torch.float64)
	q_value: tensor([[-24.0111]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 13, step: 16
	action: tensor([[ 1.1209,  0.4143, -0.1666, -2.0825, -1.4750, -0.3312,  0.4980]],
       dtype=torch.float64)
	q_value: tensor([[-46.8840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08626354595848407, distance: 1.0938736707692776 entropy 1.2378510236740112
epoch: 13, step: 17
	action: tensor([[ 1.8958, -0.2521,  0.8477, -0.7171,  0.9172, -1.4261,  0.3155]],
       dtype=torch.float64)
	q_value: tensor([[-50.6022]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 13, step: 18
	action: tensor([[ 1.1176, -0.0213,  0.6065, -0.3581, -0.4300, -0.2010,  0.4640]],
       dtype=torch.float64)
	q_value: tensor([[-46.8840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5450663911685131, distance: 0.7718460826441922 entropy 1.2378510236740112
epoch: 13, step: 19
	action: tensor([[ 0.8139, -0.0677,  1.1932,  0.6913, -1.0475, -1.0319, -0.5009]],
       dtype=torch.float64)
	q_value: tensor([[-27.8859]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6012100774258202, distance: 0.7226512876280389 entropy 1.2378510236740112
epoch: 13, step: 20
	action: tensor([[ 1.1828, -0.2261,  0.2383, -0.2090,  0.2872, -0.8631,  0.4568]],
       dtype=torch.float64)
	q_value: tensor([[-36.4684]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27332565982529133, distance: 0.9754984995970896 entropy 1.2378510236740112
epoch: 13, step: 21
	action: tensor([[ 0.8189, -0.3089, -0.4756,  0.6653,  0.4651, -0.5316,  0.0123]],
       dtype=torch.float64)
	q_value: tensor([[-28.8646]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6729963669406838, distance: 0.6543844671989696 entropy 1.2378510236740112
epoch: 13, step: 22
	action: tensor([[ 0.7351, -0.0307, -0.6399, -0.7595,  0.0875,  0.5741,  0.2617]],
       dtype=torch.float64)
	q_value: tensor([[-25.2382]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5577682611235719, distance: 0.7609947270792786 entropy 1.2378510236740112
epoch: 13, step: 23
	action: tensor([[ 1.1112, -0.5488, -0.6759, -1.6290,  0.3638, -0.2364,  0.5335]],
       dtype=torch.float64)
	q_value: tensor([[-26.1256]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4937813998482228, distance: 1.3986215545203806 entropy 1.2378510236740112
epoch: 13, step: 24
	action: tensor([[ 1.3830,  0.5863, -0.2629, -0.3256,  0.1801, -1.4398,  0.1343]],
       dtype=torch.float64)
	q_value: tensor([[-34.4519]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6309405277438465, distance: 0.6951921691710622 entropy 1.2378510236740112
epoch: 13, step: 25
	action: tensor([[ 1.9388, -0.6227,  0.6361, -0.5115,  0.1483,  0.1017,  0.3384]],
       dtype=torch.float64)
	q_value: tensor([[-31.9741]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 13, step: 26
	action: tensor([[ 1.0576, -0.1522, -0.6343, -0.2895, -0.5328, -0.0070,  0.5439]],
       dtype=torch.float64)
	q_value: tensor([[-46.8840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44388590641270187, distance: 0.8533725009099409 entropy 1.2378510236740112
epoch: 13, step: 27
	action: tensor([[ 0.9944, -0.8168,  1.0724, -0.0728, -0.9276, -0.3071,  2.3329]],
       dtype=torch.float64)
	q_value: tensor([[-26.2744]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2409449575621896, distance: 1.2747732822948525 entropy 1.2378510236740112
epoch: 13, step: 28
	action: tensor([[ 1.0001, -0.1166,  0.5180,  1.7637, -0.1754, -0.4636, -0.3163]],
       dtype=torch.float64)
	q_value: tensor([[-49.4681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7885142273349912, distance: 0.5262562557094284 entropy 1.2378510236740112
epoch: 13, step: 29
	action: tensor([[ 1.2850,  0.7358,  0.9651,  1.4965,  0.8817,  0.1718, -0.7950]],
       dtype=torch.float64)
	q_value: tensor([[-35.1369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1712568877160654, distance: 1.0417575563141546 entropy 1.2378510236740112
epoch: 13, step: 30
	action: tensor([[ 0.6993, -1.0475, -0.1796,  0.7134,  0.3282, -0.1201,  0.3627]],
       dtype=torch.float64)
	q_value: tensor([[-34.5530]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14295089460095656, distance: 1.0593989697004609 entropy 1.2378510236740112
epoch: 13, step: 31
	action: tensor([[ 2.8105,  0.8925, -1.1361,  0.3720, -0.3342, -0.0841,  0.0064]],
       dtype=torch.float64)
	q_value: tensor([[-28.1461]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 13, step: 32
	action: tensor([[ 2.7593, -2.6738, -0.0546, -0.3263, -0.7557, -0.5861, -0.5550]],
       dtype=torch.float64)
	q_value: tensor([[-46.8840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 13, step: 33
	action: tensor([[ 1.6230, -0.4723, -0.2163, -0.5053, -0.2994, -2.3729,  0.5351]],
       dtype=torch.float64)
	q_value: tensor([[-46.8840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.023300936083497703, distance: 1.157599629131831 entropy 1.2378510236740112
epoch: 13, step: 34
	action: tensor([[ 0.4214, -0.0683,  0.7135, -0.1493,  0.4124, -0.9230, -0.7920]],
       dtype=torch.float64)
	q_value: tensor([[-44.1007]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.311085828771826, distance: 0.9498154999044192 entropy 1.2378510236740112
epoch: 13, step: 35
	action: tensor([[ 0.1514, -0.6363, -0.0886, -0.0443, -0.9614, -0.7834, -0.6100]],
       dtype=torch.float64)
	q_value: tensor([[-25.6934]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07728316109597877, distance: 1.1877406738287812 entropy 1.2378510236740112
epoch: 13, step: 36
	action: tensor([[ 0.1178, -0.5980, -0.1119,  0.4611, -0.5164, -1.1672,  0.2821]],
       dtype=torch.float64)
	q_value: tensor([[-27.5849]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11696788739041886, distance: 1.0753378736954649 entropy 1.2378510236740112
epoch: 13, step: 37
	action: tensor([[-0.1354,  0.4595,  0.0031,  0.6250,  0.9999, -0.7463,  0.2102]],
       dtype=torch.float64)
	q_value: tensor([[-30.0669]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36054085908870626, distance: 0.915088490819561 entropy 1.2378510236740112
epoch: 13, step: 38
	action: tensor([[-0.1524,  0.0346, -1.1238, -0.8501, -0.7106,  0.6022,  1.5825]],
       dtype=torch.float64)
	q_value: tensor([[-25.1401]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2516905458225184, distance: 0.989913642094824 entropy 1.2378510236740112
epoch: 13, step: 39
	action: tensor([[ 2.2046,  0.9137, -0.8115,  0.3435,  1.2508, -0.2957,  0.4617]],
       dtype=torch.float64)
	q_value: tensor([[-42.8864]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 13, step: 40
	action: tensor([[ 0.5845,  0.3588,  0.8553,  0.3998, -0.0672, -1.4334, -1.6764]],
       dtype=torch.float64)
	q_value: tensor([[-46.8840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8431139456696966, distance: 0.45326153332035185 entropy 1.2378510236740112
epoch: 13, step: 41
	action: tensor([[ 0.1244, -0.3198, -0.6863,  1.2945, -0.1268,  0.2331,  0.0825]],
       dtype=torch.float64)
	q_value: tensor([[-35.8547]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28339037946501056, distance: 0.9687194295010496 entropy 1.2378510236740112
epoch: 13, step: 42
	action: tensor([[ 0.5609, -0.3914,  0.7598, -0.0321, -0.0624, -0.9111,  0.2348]],
       dtype=torch.float64)
	q_value: tensor([[-25.9477]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10756294833160662, distance: 1.0810492744886508 entropy 1.2378510236740112
epoch: 13, step: 43
	action: tensor([[ 1.9106, -0.4263, -1.0320,  0.2517, -0.4365,  0.1843, -1.0582]],
       dtype=torch.float64)
	q_value: tensor([[-27.9982]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 13, step: 44
	action: tensor([[ 1.0862,  0.3747, -0.5750,  0.4417, -0.0063, -0.9229,  1.1286]],
       dtype=torch.float64)
	q_value: tensor([[-46.8840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9432698728196922, distance: 0.2725609108292454 entropy 1.2378510236740112
epoch: 13, step: 45
	action: tensor([[ 0.6289,  1.2706, -0.7851,  0.3586,  0.0806, -0.4055, -1.0676]],
       dtype=torch.float64)
	q_value: tensor([[-30.1699]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 13, step: 46
	action: tensor([[ 2.6154, -2.0405, -0.0681, -0.5321, -0.0565, -1.0552,  1.5361]],
       dtype=torch.float64)
	q_value: tensor([[-46.8840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 13, step: 47
	action: tensor([[ 1.2180, -0.2986,  0.7042, -0.6424,  1.3020,  0.0363, -1.2905]],
       dtype=torch.float64)
	q_value: tensor([[-46.8840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1944546580286457, distance: 1.0270738894859226 entropy 1.2378510236740112
epoch: 13, step: 48
	action: tensor([[ 1.2383,  0.0630,  0.4837, -0.4924, -0.4268, -0.3513, -1.5519]],
       dtype=torch.float64)
	q_value: tensor([[-30.8300]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.47087427714740304, distance: 0.8324077800415773 entropy 1.2378510236740112
epoch: 13, step: 49
	action: tensor([[ 0.5921,  0.8011, -0.4851,  0.7598, -0.1907,  0.7135, -0.0089]],
       dtype=torch.float64)
	q_value: tensor([[-33.6868]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7711143227542527, distance: 0.5474771550909217 entropy 1.2378510236740112
epoch: 13, step: 50
	action: tensor([[ 2.4545, -2.0002,  0.2646,  0.3964,  1.0181,  0.1596, -0.5560]],
       dtype=torch.float64)
	q_value: tensor([[-24.6251]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 13, step: 51
	action: tensor([[ 1.4575, -1.0355,  0.7345, -0.4456, -0.0560, -1.2207, -0.9838]],
       dtype=torch.float64)
	q_value: tensor([[-46.8840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3781489590513898, distance: 1.343398236849431 entropy 1.2378510236740112
epoch: 13, step: 52
	action: tensor([[ 2.5848, -1.3288, -1.2306, -0.0295, -0.3461,  1.0302, -0.9175]],
       dtype=torch.float64)
	q_value: tensor([[-39.4084]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 13, step: 53
	action: tensor([[ 1.1879, -0.7319, -0.1163, -0.3083,  0.1904, -1.2399,  0.5963]],
       dtype=torch.float64)
	q_value: tensor([[-46.8840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3654480338670798, distance: 1.3371935758592304 entropy 1.2378510236740112
epoch: 13, step: 54
	action: tensor([[ 1.2026, -1.0820, -1.6467, -1.0172,  0.6271,  1.3719,  0.3403]],
       dtype=torch.float64)
	q_value: tensor([[-34.3452]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.031911297264612726, distance: 1.1624596225312196 entropy 1.2378510236740112
epoch: 13, step: 55
	action: tensor([[ 0.3989,  0.7227,  0.1428,  0.8566, -1.5434, -0.8369,  0.1317]],
       dtype=torch.float64)
	q_value: tensor([[-41.7788]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8791137190771878, distance: 0.39787367157972614 entropy 1.2378510236740112
epoch: 13, step: 56
	action: tensor([[ 0.8485,  1.3052,  0.3059, -0.3108,  1.2428, -0.7168,  0.3903]],
       dtype=torch.float64)
	q_value: tensor([[-32.0101]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 13, step: 57
	action: tensor([[ 1.8100,  1.0885, -0.3442, -1.1265,  0.4352,  0.4523,  0.3714]],
       dtype=torch.float64)
	q_value: tensor([[-46.8840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5097365671697281, distance: 0.8012562758994513 entropy 1.2378510236740112
epoch: 13, step: 58
	action: tensor([[ 1.5735, -0.3527, -0.1499,  0.9417,  1.1849, -0.2417,  1.2404]],
       dtype=torch.float64)
	q_value: tensor([[-36.5124]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7002994085801278, distance: 0.6264703096282598 entropy 1.2378510236740112
epoch: 13, step: 59
	action: tensor([[-0.0565,  0.3589, -0.8168, -1.5572,  1.2208,  0.0482,  0.2121]],
       dtype=torch.float64)
	q_value: tensor([[-38.8312]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5326126631542953, distance: 0.7823393305808077 entropy 1.2378510236740112
epoch: 13, step: 60
	action: tensor([[ 1.7516, -1.3263,  0.5809, -0.3265, -0.5060, -1.4056, -1.0723]],
       dtype=torch.float64)
	q_value: tensor([[-33.5694]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.610039372207922, distance: 1.4520278341751411 entropy 1.2378510236740112
epoch: 13, step: 61
	action: tensor([[ 0.6047, -1.1748,  0.7356,  0.1954, -0.0473, -1.6637, -0.0982]],
       dtype=torch.float64)
	q_value: tensor([[-43.5720]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4595914611882814, distance: 1.3825229529169198 entropy 1.2378510236740112
epoch: 13, step: 62
	action: tensor([[-1.2244, -1.7281, -0.1871, -0.2234,  1.4777,  0.0322, -1.0757]],
       dtype=torch.float64)
	q_value: tensor([[-40.2351]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 13, step: 63
	action: tensor([[ 2.5718, -0.1827, -0.4167,  0.1478, -0.7637,  0.0301, -0.2321]],
       dtype=torch.float64)
	q_value: tensor([[-46.8840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 13, step: 64
	action: tensor([[-0.4236, -1.0872, -0.3404, -1.0106, -0.2649, -1.1178, -0.6307]],
       dtype=torch.float64)
	q_value: tensor([[-46.8840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15702334980761035, distance: 1.2309141241134607 entropy 1.2378510236740112
epoch: 13, step: 65
	action: tensor([[ 1.2381, -0.9043, -1.4760,  0.7640,  0.4670, -0.9787,  0.0613]],
       dtype=torch.float64)
	q_value: tensor([[-32.1472]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18257214565877078, distance: 1.2444301251763648 entropy 1.2378510236740112
epoch: 13, step: 66
	action: tensor([[ 1.7539,  0.7425, -1.3094, -2.1114,  0.5268,  0.0331,  0.1001]],
       dtype=torch.float64)
	q_value: tensor([[-38.2598]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 13, step: 67
	action: tensor([[ 1.1939,  0.6902, -1.0554,  0.8345, -0.9579,  0.0670,  0.6053]],
       dtype=torch.float64)
	q_value: tensor([[-46.8840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9254647171333601, distance: 0.3124191491273817 entropy 1.2378510236740112
epoch: 13, step: 68
	action: tensor([[ 0.2127, -0.2341,  0.4596,  0.0605, -0.1090, -0.6432, -0.7568]],
       dtype=torch.float64)
	q_value: tensor([[-30.9847]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14790296086200105, distance: 1.0563339095734412 entropy 1.2378510236740112
epoch: 13, step: 69
	action: tensor([[ 0.6956,  0.5636, -0.9695,  0.8370,  0.1525, -0.5855,  0.4320]],
       dtype=torch.float64)
	q_value: tensor([[-23.2880]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8434100247950801, distance: 0.452833628317215 entropy 1.2378510236740112
epoch: 13, step: 70
	action: tensor([[ 0.1980, -0.0099,  0.8300, -0.5098,  0.3607,  0.2700, -2.0776]],
       dtype=torch.float64)
	q_value: tensor([[-26.0602]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2086448057344219, distance: 1.0179874452662079 entropy 1.2378510236740112
epoch: 13, step: 71
	action: tensor([[1.3677, 0.7356, 0.4585, 1.6234, 0.2189, 0.8646, 0.2429]],
       dtype=torch.float64)
	q_value: tensor([[-31.4908]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1350983072519456, distance: 1.0642411976638504 entropy 1.2378510236740112
epoch: 13, step: 72
	action: tensor([[ 1.9158,  0.5462,  0.4407, -0.2807, -0.3969, -0.4055,  0.3183]],
       dtype=torch.float64)
	q_value: tensor([[-35.2348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 13, step: 73
	action: tensor([[ 2.2884,  0.7595, -0.0979, -0.7310,  1.0513, -2.8038,  0.1945]],
       dtype=torch.float64)
	q_value: tensor([[-46.8840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 13, step: 74
	action: tensor([[-0.6625,  0.7048,  0.7488, -0.7709,  0.8078, -1.1684,  1.2732]],
       dtype=torch.float64)
	q_value: tensor([[-46.8840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5867791722986888, distance: 1.441500969997794 entropy 1.2378510236740112
epoch: 13, step: 75
	action: tensor([[ 2.1851,  0.7718,  1.2744, -0.5664,  0.0467,  2.6411, -0.7526]],
       dtype=torch.float64)
	q_value: tensor([[-37.1259]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 13, step: 76
	action: tensor([[-0.9155,  0.1513, -0.0093,  1.5222,  0.2416, -0.6397, -0.1498]],
       dtype=torch.float64)
	q_value: tensor([[-46.8840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.008177358455196293, distance: 1.149013584318873 entropy 1.2378510236740112
epoch: 13, step: 77
	action: tensor([[ 0.9957,  1.0125, -0.4050, -0.6929, -0.2258, -0.4436,  0.2952]],
       dtype=torch.float64)
	q_value: tensor([[-29.9076]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9232950361424206, distance: 0.31693370533289356 entropy 1.2378510236740112
epoch: 13, step: 78
	action: tensor([[ 2.9183, -0.9050, -0.6194, -1.4514, -0.4423, -1.2094,  0.7233]],
       dtype=torch.float64)
	q_value: tensor([[-28.9246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 13, step: 79
	action: tensor([[ 2.5261,  0.5779,  0.6147, -1.1428,  0.6874,  0.0950,  0.6151]],
       dtype=torch.float64)
	q_value: tensor([[-46.8840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 13, step: 80
	action: tensor([[ 0.0546, -1.3125,  1.3610, -0.1188, -0.2063, -0.9914,  0.5667]],
       dtype=torch.float64)
	q_value: tensor([[-46.8840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9420307183681822, distance: 1.5947211013222815 entropy 1.2378510236740112
epoch: 13, step: 81
	action: tensor([[ 1.2529, -0.3321,  1.0146,  0.8065, -0.5648, -0.4196, -0.3959]],
       dtype=torch.float64)
	q_value: tensor([[-39.4133]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3766107570923164, distance: 0.9035170350283925 entropy 1.2378510236740112
epoch: 13, step: 82
	action: tensor([[ 0.4250, -0.1113, -0.1946, -0.3094,  0.5976, -0.4032, -0.5338]],
       dtype=torch.float64)
	q_value: tensor([[-34.0967]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26197461288362756, distance: 0.9830878844333004 entropy 1.2378510236740112
epoch: 13, step: 83
	action: tensor([[ 0.6682,  0.5555, -0.6514, -1.5347, -0.5843, -1.1671, -1.2391]],
       dtype=torch.float64)
	q_value: tensor([[-19.4590]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5065264803713381, distance: 0.8038751797635315 entropy 1.2378510236740112
epoch: 13, step: 84
	action: tensor([[ 0.1384, -0.7012,  0.4602, -1.0519, -0.0317, -0.1512, -0.6583]],
       dtype=torch.float64)
	q_value: tensor([[-38.4824]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.849387580283611, distance: 1.5562188147556126 entropy 1.2378510236740112
epoch: 13, step: 85
	action: tensor([[ 2.6964,  0.2747,  0.0429, -0.9497,  1.1683, -0.4574,  1.1356]],
       dtype=torch.float64)
	q_value: tensor([[-25.3755]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 13, step: 86
	action: tensor([[ 1.0535,  0.0020, -0.5422, -1.2478,  0.3182, -0.0348,  0.3619]],
       dtype=torch.float64)
	q_value: tensor([[-46.8840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05923831229605059, distance: 1.1099323373602341 entropy 1.2378510236740112
epoch: 13, step: 87
	action: tensor([[ 0.4445, -0.0159, -0.2195, -0.5263, -1.1005,  0.4514, -1.7786]],
       dtype=torch.float64)
	q_value: tensor([[-29.6683]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36649615565498717, distance: 0.9108174040989637 entropy 1.2378510236740112
epoch: 13, step: 88
	action: tensor([[ 0.6520, -0.6764,  1.3008,  0.0218,  1.6471, -0.3645, -0.3163]],
       dtype=torch.float64)
	q_value: tensor([[-37.4269]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1593962007327836, distance: 1.0491857117262406 entropy 1.2378510236740112
epoch: 13, step: 89
	action: tensor([[ 1.2836, -0.4146,  1.4957, -1.8126,  1.2985, -0.3851,  0.4489]],
       dtype=torch.float64)
	q_value: tensor([[-34.2441]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5974662646488194, distance: 0.7260354642430354 entropy 1.2378510236740112
epoch: 13, step: 90
	action: tensor([[ 1.4451,  0.2866, -0.3238,  0.8824, -0.0673, -1.1004, -0.1309]],
       dtype=torch.float64)
	q_value: tensor([[-40.5448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8532821373910688, distance: 0.43832696564773926 entropy 1.2378510236740112
epoch: 13, step: 91
	action: tensor([[ 0.3679, -0.3441,  0.2480, -0.4378, -1.4272, -1.2004, -0.7333]],
       dtype=torch.float64)
	q_value: tensor([[-31.0212]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20499501051385272, distance: 1.0203322656481355 entropy 1.2378510236740112
epoch: 13, step: 92
	action: tensor([[ 1.2652, -0.1177, -1.0936,  0.1774, -0.9821, -0.9729, -0.9697]],
       dtype=torch.float64)
	q_value: tensor([[-35.3504]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6646524965229776, distance: 0.6626805622401639 entropy 1.2378510236740112
epoch: 13, step: 93
	action: tensor([[ 2.6944, -0.0444,  0.8240, -0.2413, -0.7446, -0.6204, -0.7012]],
       dtype=torch.float64)
	q_value: tensor([[-33.3801]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 13, step: 94
	action: tensor([[ 0.8380,  0.7734,  1.6905,  0.1829,  0.1263, -1.2493, -0.1523]],
       dtype=torch.float64)
	q_value: tensor([[-46.8840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6628949847367833, distance: 0.6644148034428031 entropy 1.2378510236740112
epoch: 13, step: 95
	action: tensor([[ 0.8917,  0.2618, -0.1814,  0.8155, -0.8700, -1.5271,  0.7638]],
       dtype=torch.float64)
	q_value: tensor([[-35.7601]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9446979005763307, distance: 0.26910855396689903 entropy 1.2378510236740112
epoch: 13, step: 96
	action: tensor([[-0.2827, -0.2022, -0.1848, -0.5621,  0.7000, -0.7588, -0.0064]],
       dtype=torch.float64)
	q_value: tensor([[-34.7256]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.003222014163197, distance: 1.6196502238029584 entropy 1.2378510236740112
epoch: 13, step: 97
	action: tensor([[ 0.4630,  0.2142, -0.0783,  0.1692,  0.3494,  2.1847,  1.0053]],
       dtype=torch.float64)
	q_value: tensor([[-23.5110]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7098775115221366, distance: 0.6163783702854388 entropy 1.2378510236740112
epoch: 13, step: 98
	action: tensor([[ 1.0198,  0.3039,  1.7685,  0.6393, -0.4763, -1.5409, -0.2667]],
       dtype=torch.float64)
	q_value: tensor([[-42.1711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7823560366087488, distance: 0.5338632257608266 entropy 1.2378510236740112
epoch: 13, step: 99
	action: tensor([[ 0.7850, -0.1914,  1.7058,  0.6934,  1.5308, -2.6923, -0.2977]],
       dtype=torch.float64)
	q_value: tensor([[-41.1066]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 13, step: 100
	action: tensor([[ 0.4792, -0.7611, -0.1516,  0.6137, -1.1620, -0.4642, -0.8682]],
       dtype=torch.float64)
	q_value: tensor([[-46.8840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36672620702451053, distance: 0.9106520114030828 entropy 1.2378510236740112
epoch: 13, step: 101
	action: tensor([[ 1.3901, -1.1359,  0.4863,  0.2405,  1.7056, -0.0576,  0.2466]],
       dtype=torch.float64)
	q_value: tensor([[-30.0877]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4708666900816256, distance: 1.3878526202766968 entropy 1.2378510236740112
epoch: 13, step: 102
	action: tensor([[-0.1153, -0.2303,  1.1386, -0.4922, -1.1824, -0.7271,  1.6057]],
       dtype=torch.float64)
	q_value: tensor([[-37.9406]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.47447624390318377, distance: 1.3895544941251206 entropy 1.2378510236740112
epoch: 13, step: 103
	action: tensor([[ 1.7006, -0.3876,  0.4599,  0.3518, -0.6491, -0.2171, -0.0240]],
       dtype=torch.float64)
	q_value: tensor([[-42.4844]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12218673600088437, distance: 1.07215546274212 entropy 1.2378510236740112
epoch: 13, step: 104
	action: tensor([[ 1.2299,  0.0768,  0.4185,  0.6708, -1.5140, -1.4398, -0.7955]],
       dtype=torch.float64)
	q_value: tensor([[-32.0510]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5665797495727728, distance: 0.7533751529314628 entropy 1.2378510236740112
epoch: 13, step: 105
	action: tensor([[ 0.1028, -1.9389, -0.7303, -1.2176, -1.2336,  0.2149,  2.4334]],
       dtype=torch.float64)
	q_value: tensor([[-40.4943]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 13, step: 106
	action: tensor([[ 2.2898, -1.0226,  0.7472, -1.6983, -1.3511, -0.8560, -1.2886]],
       dtype=torch.float64)
	q_value: tensor([[-46.8840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3401854328865399, distance: 1.3247658973573957 entropy 1.2378510236740112
epoch: 13, step: 107
	action: tensor([[ 2.0467, -0.0581,  1.0626, -0.7839, -0.1227, -1.7988, -1.2061]],
       dtype=torch.float64)
	q_value: tensor([[-50.0695]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 13, step: 108
	action: tensor([[ 0.6481,  0.6889,  0.3172, -1.1340, -0.4558, -1.3607, -0.7470]],
       dtype=torch.float64)
	q_value: tensor([[-46.8840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5874671627148564, distance: 0.7349976578305567 entropy 1.2378510236740112
epoch: 13, step: 109
	action: tensor([[ 1.7111, -0.3012,  0.5529, -1.0869,  2.4907, -1.6053,  0.6393]],
       dtype=torch.float64)
	q_value: tensor([[-34.3637]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5094930561556906, distance: 0.8014552408773689 entropy 1.2378510236740112
epoch: 13, step: 110
	action: tensor([[ 3.0030,  0.4368,  0.7327, -0.8675,  1.5046, -1.2156,  0.7399]],
       dtype=torch.float64)
	q_value: tensor([[-48.9556]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 13, step: 111
	action: tensor([[ 0.4168, -1.9500,  0.7180,  1.4051, -1.6128, -0.7250,  0.1063]],
       dtype=torch.float64)
	q_value: tensor([[-46.8840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 13, step: 112
	action: tensor([[ 2.0982, -0.9606, -1.1889, -0.3893,  0.9509, -1.1712, -0.7277]],
       dtype=torch.float64)
	q_value: tensor([[-46.8840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7842484238446081, distance: 1.5285665590926771 entropy 1.2378510236740112
epoch: 13, step: 113
	action: tensor([[ 0.3357, -0.3142,  0.6748, -1.7462, -0.7558, -0.6556, -0.5502]],
       dtype=torch.float64)
	q_value: tensor([[-38.8988]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.37298829658298094, distance: 1.3408806108637155 entropy 1.2378510236740112
epoch: 13, step: 114
	action: tensor([[ 1.2785, -1.3052, -0.0876, -0.8160, -0.3923, -0.8128, -0.1967]],
       dtype=torch.float64)
	q_value: tensor([[-37.2815]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9343944483242586, distance: 1.5915827071984654 entropy 1.2378510236740112
epoch: 13, step: 115
	action: tensor([[ 1.9244, -0.8910,  0.0616, -0.5497, -0.1637, -1.0686,  0.7810]],
       dtype=torch.float64)
	q_value: tensor([[-34.0312]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 13, step: 116
	action: tensor([[ 0.5763,  0.7272, -0.5823, -1.2780,  0.1708, -1.4380,  0.6427]],
       dtype=torch.float64)
	q_value: tensor([[-46.8840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5897381280363612, distance: 0.7329718097875743 entropy 1.2378510236740112
epoch: 13, step: 117
	action: tensor([[ 1.2183, -1.0646,  0.7666,  0.1279,  0.3957,  0.6423, -0.0369]],
       dtype=torch.float64)
	q_value: tensor([[-34.2180]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.299106339381078, distance: 1.3043046551083777 entropy 1.2378510236740112
epoch: 13, step: 118
	action: tensor([[ 0.2117, -0.1792, -0.3401, -1.4007,  0.5079, -1.2970, -0.2474]],
       dtype=torch.float64)
	q_value: tensor([[-32.4595]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09248625539616118, distance: 1.1960922713805018 entropy 1.2378510236740112
epoch: 13, step: 119
	action: tensor([[ 1.6742,  0.9694, -0.8856, -0.2869,  1.6837, -0.2680,  0.2656]],
       dtype=torch.float64)
	q_value: tensor([[-31.5070]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6449125997170841, distance: 0.6819057058629354 entropy 1.2378510236740112
epoch: 13, step: 120
	action: tensor([[ 0.6576, -0.9053,  1.8624, -1.6279, -0.6871,  0.7775, -0.6098]],
       dtype=torch.float64)
	q_value: tensor([[-36.7336]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25943843157282753, distance: 0.9847755979081132 entropy 1.2378510236740112
epoch: 13, step: 121
	action: tensor([[ 2.4554, -0.1008,  0.5051,  0.9564,  0.1687, -0.7001,  0.5332]],
       dtype=torch.float64)
	q_value: tensor([[-47.1289]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 13, step: 122
	action: tensor([[ 0.7954,  0.7432,  0.3458, -1.6239,  2.0181, -0.1598,  0.4299]],
       dtype=torch.float64)
	q_value: tensor([[-46.8840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5738761839174944, distance: 0.7470068720110379 entropy 1.2378510236740112
epoch: 13, step: 123
	action: tensor([[ 0.3057, -1.1046,  0.6476,  0.6185, -0.9804,  0.3724,  0.2269]],
       dtype=torch.float64)
	q_value: tensor([[-36.4636]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08287166448732808, distance: 1.095902074557483 entropy 1.2378510236740112
epoch: 13, step: 124
	action: tensor([[ 1.0728, -0.4926, -0.3280,  0.9300,  1.2863, -1.4566, -1.0403]],
       dtype=torch.float64)
	q_value: tensor([[-32.9398]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6167524206847476, distance: 0.7084291182225316 entropy 1.2378510236740112
epoch: 13, step: 125
	action: tensor([[ 2.5711, -0.1140,  1.7665, -0.9240, -0.4860,  0.5375,  0.1276]],
       dtype=torch.float64)
	q_value: tensor([[-39.1841]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 13, step: 126
	action: tensor([[ 2.4264,  2.0043,  0.5307, -0.4678, -0.1681, -0.9288,  0.1711]],
       dtype=torch.float64)
	q_value: tensor([[-46.8840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.2378510236740112
epoch: 13, step: 127
	action: tensor([[ 0.1879, -0.9046,  1.9557, -0.2434, -0.1368, -0.4521,  1.7378]],
       dtype=torch.float64)
	q_value: tensor([[-46.8840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4701022519506093, distance: 1.3874919263564607 entropy 1.2378510236740112
LOSS epoch 13 actor 442.4029114020701 critic 150.39305500379498 
epoch: 14, step: 0
	action: tensor([[ 1.9180,  0.0788,  0.7470, -1.0108,  1.8648, -0.0893,  0.2286]],
       dtype=torch.float64)
	q_value: tensor([[-43.9741]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44526433985533265, distance: 0.8523142225426648 entropy 1.1324905157089233
epoch: 14, step: 1
	action: tensor([[ 2.8852, -0.0272,  2.1089, -0.9571,  0.3136, -1.2666,  1.3024]],
       dtype=torch.float64)
	q_value: tensor([[-33.0316]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 14, step: 2
	action: tensor([[ 3.7359, -0.9811,  0.8754,  0.6452, -0.1352, -1.1146,  0.6652]],
       dtype=torch.float64)
	q_value: tensor([[-39.2272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 14, step: 3
	action: tensor([[ 2.6202,  0.5962,  0.7957, -0.3055,  0.7051,  1.4596,  0.0180]],
       dtype=torch.float64)
	q_value: tensor([[-39.2272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 14, step: 4
	action: tensor([[ 1.9877,  0.4379,  0.6098, -1.4371,  0.0440, -0.7789,  0.1809]],
       dtype=torch.float64)
	q_value: tensor([[-39.2272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4241882942799253, distance: 0.8683542575712702 entropy 1.1324905157089233
epoch: 14, step: 5
	action: tensor([[ 3.7379, -0.1239,  0.7664, -0.9675,  0.4933, -0.7076,  0.4587]],
       dtype=torch.float64)
	q_value: tensor([[-32.6000]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 14, step: 6
	action: tensor([[ 0.9809, -0.1879,  2.0753, -0.1475, -0.7213, -0.4703, -0.3407]],
       dtype=torch.float64)
	q_value: tensor([[-39.2272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6472568068574875, distance: 0.6796510845370353 entropy 1.1324905157089233
epoch: 14, step: 7
	action: tensor([[ 3.2327,  0.1623,  0.9130, -0.8043, -0.4463,  0.2815,  0.3131]],
       dtype=torch.float64)
	q_value: tensor([[-37.7839]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 14, step: 8
	action: tensor([[ 2.2571,  0.3376,  1.2712, -0.5060,  0.2629, -0.4792, -0.7881]],
       dtype=torch.float64)
	q_value: tensor([[-39.2272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8151380817107335, distance: 0.4920173538351911 entropy 1.1324905157089233
epoch: 14, step: 9
	action: tensor([[ 2.2906, -0.8113,  1.4409, -1.6632, -1.0762, -2.2659, -0.3389]],
       dtype=torch.float64)
	q_value: tensor([[-32.4316]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 14, step: 10
	action: tensor([[ 1.0720, -1.0239, -0.4119,  0.2825,  0.3468, -0.1094, -1.6259]],
       dtype=torch.float64)
	q_value: tensor([[-39.2272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17080477607129518, distance: 1.238223197263612 entropy 1.1324905157089233
epoch: 14, step: 11
	action: tensor([[ 2.2189,  1.1373,  1.8874, -0.5489, -0.7826, -1.3867, -0.6636]],
       dtype=torch.float64)
	q_value: tensor([[-30.7310]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 14, step: 12
	action: tensor([[ 1.4839,  0.0456,  1.8398,  0.4266, -0.2760, -0.7935,  1.6134]],
       dtype=torch.float64)
	q_value: tensor([[-39.2272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5216300155475757, distance: 0.7914776473725721 entropy 1.1324905157089233
epoch: 14, step: 13
	action: tensor([[ 3.3407,  0.1724,  0.6457, -0.8910, -0.8839, -0.6023,  0.9284]],
       dtype=torch.float64)
	q_value: tensor([[-41.2564]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 14, step: 14
	action: tensor([[ 2.6406, -2.1365, -0.4227, -1.3876,  0.2263, -0.7614, -0.3060]],
       dtype=torch.float64)
	q_value: tensor([[-39.2272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 14, step: 15
	action: tensor([[ 2.5370,  0.5676, -0.2342, -0.2762,  0.7792, -2.1299, -0.2550]],
       dtype=torch.float64)
	q_value: tensor([[-39.2272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 14, step: 16
	action: tensor([[ 2.2871, -2.0080,  1.2289, -0.9654,  0.9838, -1.2562,  0.8610]],
       dtype=torch.float64)
	q_value: tensor([[-39.2272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 14, step: 17
	action: tensor([[ 2.0674, -0.5731,  0.2622,  0.1560,  0.5045, -1.8236,  0.7000]],
       dtype=torch.float64)
	q_value: tensor([[-39.2272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1314587089119006, distance: 1.0664780669805545 entropy 1.1324905157089233
epoch: 14, step: 18
	action: tensor([[ 3.6377, -0.6991,  0.1230, -0.2513,  0.7009, -1.1553, -0.4035]],
       dtype=torch.float64)
	q_value: tensor([[-39.9373]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 14, step: 19
	action: tensor([[ 1.6754,  0.3812, -0.6764,  0.2302,  1.5542, -1.6117, -0.1154]],
       dtype=torch.float64)
	q_value: tensor([[-39.2272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8095245153011308, distance: 0.4994318523404031 entropy 1.1324905157089233
epoch: 14, step: 20
	action: tensor([[ 4.1169e+00, -8.1446e-01, -8.2187e-03, -8.4251e-02,  1.5675e-03,
         -1.0088e+00, -2.4471e-01]], dtype=torch.float64)
	q_value: tensor([[-37.9350]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 14, step: 21
	action: tensor([[ 1.7628, -1.3816,  0.1674, -1.4883, -0.6468, -1.2685,  0.1804]],
       dtype=torch.float64)
	q_value: tensor([[-39.2272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3986772222045212, distance: 1.3533665694704209 entropy 1.1324905157089233
epoch: 14, step: 22
	action: tensor([[ 2.2969, -0.6740,  1.9829, -1.2597, -0.2467, -2.4206, -0.0092]],
       dtype=torch.float64)
	q_value: tensor([[-38.5266]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 14, step: 23
	action: tensor([[ 1.8802,  1.1529,  0.9936, -0.6894, -0.8117, -0.5569,  1.2634]],
       dtype=torch.float64)
	q_value: tensor([[-39.2272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9459781636222727, distance: 0.2659753351826312 entropy 1.1324905157089233
epoch: 14, step: 24
	action: tensor([[ 4.0828, -0.9667, -0.4018, -0.3906, -0.8252, -1.4162,  0.6908]],
       dtype=torch.float64)
	q_value: tensor([[-37.7751]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 14, step: 25
	action: tensor([[ 1.7908, -0.8029,  0.3913, -1.1819, -0.4519, -2.8779,  0.3701]],
       dtype=torch.float64)
	q_value: tensor([[-39.2272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 14, step: 26
	action: tensor([[ 2.0790,  0.8443,  0.8781, -0.8400, -1.5343, -0.8351,  0.3517]],
       dtype=torch.float64)
	q_value: tensor([[-39.2272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7461701205402176, distance: 0.5765381696912116 entropy 1.1324905157089233
epoch: 14, step: 27
	action: tensor([[ 3.5673,  0.8690,  1.3276,  0.4333,  0.4673, -1.5275, -0.2291]],
       dtype=torch.float64)
	q_value: tensor([[-40.5045]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 14, step: 28
	action: tensor([[ 2.3319,  0.5417, -0.1068, -2.0281, -0.1486, -0.6904, -0.0224]],
       dtype=torch.float64)
	q_value: tensor([[-39.2272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.003313061031343212, distance: 1.146238327678991 entropy 1.1324905157089233
epoch: 14, step: 29
	action: tensor([[ 5.9004,  0.2154,  0.4823, -1.2291, -0.3611, -1.9862,  0.0289]],
       dtype=torch.float64)
	q_value: tensor([[-36.5000]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1198203897449526 entropy 1.1324905157089233
epoch: 14, step: 30
	action: tensor([[ 1.4630, -0.2931,  0.2811, -2.4899, -1.2817, -2.1227,  1.1375]],
       dtype=torch.float64)
	q_value: tensor([[-39.2272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1351549927526845, distance: 1.06420632200446 entropy 1.1324905157089233
epoch: 14, step: 31
	action: tensor([[ 3.7424,  0.3226,  0.6260, -0.7601, -0.0408, -0.9782,  0.3011]],
       dtype=torch.float64)
	q_value: tensor([[-49.7175]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 14, step: 32
	action: tensor([[ 1.8775, -0.3654,  0.1709, -1.9186,  0.9487, -0.4816, -0.1947]],
       dtype=torch.float64)
	q_value: tensor([[-39.2272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13217528289830205, distance: 1.217624920344685 entropy 1.1324905157089233
epoch: 14, step: 33
	action: tensor([[ 3.6513,  0.1067,  1.9869, -0.2709, -0.2102, -3.3347,  0.9311]],
       dtype=torch.float64)
	q_value: tensor([[-32.8041]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 14, step: 34
	action: tensor([[ 1.9745, -1.3530,  1.5861, -0.9292,  0.1993, -0.1532, -0.3497]],
       dtype=torch.float64)
	q_value: tensor([[-39.2272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0021068746539580907, distance: 1.1455491146633952 entropy 1.1324905157089233
epoch: 14, step: 35
	action: tensor([[ 4.1362, -0.4901,  1.5016, -0.1169, -0.5842, -1.8850, -0.6680]],
       dtype=torch.float64)
	q_value: tensor([[-37.5776]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 14, step: 36
	action: tensor([[ 2.0513, -0.6709,  0.5396, -1.5891, -0.2430, -0.1637, -0.8754]],
       dtype=torch.float64)
	q_value: tensor([[-39.2272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2525875133441753, distance: 1.2807392883847255 entropy 1.1324905157089233
epoch: 14, step: 37
	action: tensor([[ 2.2400,  1.0422,  1.2785, -0.5444,  0.7499, -1.3000, -0.9733]],
       dtype=torch.float64)
	q_value: tensor([[-33.5617]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 14, step: 38
	action: tensor([[ 0.8800,  0.6574,  1.1374,  1.2954, -0.6510, -0.7102,  0.6405]],
       dtype=torch.float64)
	q_value: tensor([[-39.2272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.0978134953661463 entropy 1.1324905157089233
epoch: 14, step: 39
	action: tensor([[ 1.3504, -1.3010, -0.4943,  0.8188,  0.9966, -0.6777, -1.0724]],
       dtype=torch.float64)
	q_value: tensor([[-39.2272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0836765356033966, distance: 1.191259914613874 entropy 1.1324905157089233
epoch: 14, step: 40
	action: tensor([[ 2.9183, -0.9282,  2.3121, -1.5583,  0.4525, -1.2115,  0.5611]],
       dtype=torch.float64)
	q_value: tensor([[-36.0853]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 14, step: 41
	action: tensor([[ 2.8112,  0.3492,  0.5511,  0.5899,  0.0655, -0.9039,  1.0992]],
       dtype=torch.float64)
	q_value: tensor([[-39.2272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 14, step: 42
	action: tensor([[ 1.5085,  0.7971,  0.1592, -1.3836,  0.7881, -1.3916, -0.9158]],
       dtype=torch.float64)
	q_value: tensor([[-39.2272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6583970291297645, distance: 0.6688327226035028 entropy 1.1324905157089233
epoch: 14, step: 43
	action: tensor([[ 3.4426,  0.0401,  0.8835,  0.0948, -1.2020, -1.4383, -0.1449]],
       dtype=torch.float64)
	q_value: tensor([[-34.6826]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 14, step: 44
	action: tensor([[ 1.1652,  1.1192,  1.2668,  0.4912,  1.1323, -0.1710, -0.9936]],
       dtype=torch.float64)
	q_value: tensor([[-39.2272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 14, step: 45
	action: tensor([[ 1.2180,  0.5540,  0.3618, -0.7816, -0.4123, -0.9154, -0.4193]],
       dtype=torch.float64)
	q_value: tensor([[-39.2272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6534147413326901, distance: 0.6736925375367822 entropy 1.1324905157089233
epoch: 14, step: 46
	action: tensor([[ 2.3189,  1.6088,  0.6795, -1.6727,  0.3786, -1.0794, -0.2858]],
       dtype=torch.float64)
	q_value: tensor([[-27.9211]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 14, step: 47
	action: tensor([[ 0.8819,  1.1827,  0.5004, -0.9992, -1.5496, -1.1789,  0.1735]],
       dtype=torch.float64)
	q_value: tensor([[-39.2272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9773652135383464, distance: 0.17216501078339658 entropy 1.1324905157089233
epoch: 14, step: 48
	action: tensor([[ 3.2486, -0.3870, -1.1750, -1.0893,  1.2547, -1.4393,  0.5762]],
       dtype=torch.float64)
	q_value: tensor([[-38.2320]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 14, step: 49
	action: tensor([[ 1.3629, -0.1456,  0.3991, -2.4076,  1.0115, -0.3446, -1.3218]],
       dtype=torch.float64)
	q_value: tensor([[-39.2272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0019801008763140615, distance: 1.1432107340709292 entropy 1.1324905157089233
epoch: 14, step: 50
	action: tensor([[ 4.4371, -0.7637,  1.8185,  0.9431, -0.5698, -2.5101,  0.2106]],
       dtype=torch.float64)
	q_value: tensor([[-34.7255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 14, step: 51
	action: tensor([[ 1.2574, -0.0932,  0.5226,  0.2331,  1.2396,  0.3118, -0.2017]],
       dtype=torch.float64)
	q_value: tensor([[-39.2272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6037350692218437, distance: 0.7203598730706174 entropy 1.1324905157089233
epoch: 14, step: 52
	action: tensor([[ 2.1423, -1.8142,  0.8275, -1.1135, -0.5677, -0.4591,  0.1871]],
       dtype=torch.float64)
	q_value: tensor([[-26.2654]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 14, step: 53
	action: tensor([[ 1.5203,  0.8028,  1.0998,  0.3748,  0.3857, -0.0237, -1.1518]],
       dtype=torch.float64)
	q_value: tensor([[-39.2272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.940129331401906, distance: 0.2800036852517313 entropy 1.1324905157089233
epoch: 14, step: 54
	action: tensor([[ 1.8532,  0.7235,  1.0348, -1.9543,  0.5288, -1.1269, -2.0377]],
       dtype=torch.float64)
	q_value: tensor([[-31.4954]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 14, step: 55
	action: tensor([[ 2.4418, -0.5921, -0.1343,  0.1143, -0.7693, -0.9274,  0.7210]],
       dtype=torch.float64)
	q_value: tensor([[-39.2272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 14, step: 56
	action: tensor([[ 1.9355, -0.5603,  0.4252, -0.2862, -0.5032, -1.1221,  0.1885]],
       dtype=torch.float64)
	q_value: tensor([[-39.2272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.35148079690112755, distance: 1.33033688608857 entropy 1.1324905157089233
epoch: 14, step: 57
	action: tensor([[ 1.8363, -0.0794,  1.8436, -1.0586, -0.2759, -2.6727, -0.0501]],
       dtype=torch.float64)
	q_value: tensor([[-32.7706]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 14, step: 58
	action: tensor([[ 1.3121,  0.3884, -1.2666, -1.2515, -1.1167, -0.2189, -0.6908]],
       dtype=torch.float64)
	q_value: tensor([[-39.2272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20828139626793163, distance: 1.0182211606791705 entropy 1.1324905157089233
epoch: 14, step: 59
	action: tensor([[ 3.3988,  0.8105,  1.2748, -0.6964,  0.0579, -2.5262,  0.0355]],
       dtype=torch.float64)
	q_value: tensor([[-34.7184]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 14, step: 60
	action: tensor([[ 2.7127, -0.3384,  1.1482,  0.2150,  0.5433, -0.7885,  0.0378]],
       dtype=torch.float64)
	q_value: tensor([[-39.2272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 14, step: 61
	action: tensor([[ 0.7806, -0.5319,  0.9636, -0.7851, -0.3048, -0.1828, -0.1489]],
       dtype=torch.float64)
	q_value: tensor([[-39.2272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17076952982050098, distance: 1.2382045592059676 entropy 1.1324905157089233
epoch: 14, step: 62
	action: tensor([[ 1.8527,  0.4852,  0.0660, -0.3862, -1.4246, -0.8250,  0.4234]],
       dtype=torch.float64)
	q_value: tensor([[-25.8638]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 14, step: 63
	action: tensor([[ 1.8803, -0.2424,  1.3347, -0.8936,  0.8496,  0.6469,  1.1097]],
       dtype=torch.float64)
	q_value: tensor([[-39.2272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2650452569468804, distance: 0.981040624604505 entropy 1.1324905157089233
epoch: 14, step: 64
	action: tensor([[ 2.7618,  1.4135,  0.1550, -0.2014,  0.2109, -0.8132,  1.6097]],
       dtype=torch.float64)
	q_value: tensor([[-36.5694]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 14, step: 65
	action: tensor([[ 0.5264,  1.0052,  0.0981, -0.8631, -0.2152, -1.0624, -0.1532]],
       dtype=torch.float64)
	q_value: tensor([[-39.2272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8367749183524139, distance: 0.46232794048069176 entropy 1.1324905157089233
epoch: 14, step: 66
	action: tensor([[ 3.0448,  0.4092,  0.1880, -0.6994,  0.0822, -1.1382,  0.3156]],
       dtype=torch.float64)
	q_value: tensor([[-25.5456]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 14, step: 67
	action: tensor([[ 2.3563, -1.7184,  0.9155,  0.1616,  0.8905, -0.5212,  1.2655]],
       dtype=torch.float64)
	q_value: tensor([[-39.2272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5484230381141375, distance: 1.423972182663981 entropy 1.1324905157089233
epoch: 14, step: 68
	action: tensor([[ 2.0383, -0.0667,  0.6960, -1.1198,  0.3345, -1.7162, -0.7213]],
       dtype=torch.float64)
	q_value: tensor([[-41.5719]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 14, step: 69
	action: tensor([[ 0.8371, -1.0911,  0.6043, -0.1649, -1.4050, -1.2297,  0.5978]],
       dtype=torch.float64)
	q_value: tensor([[-39.2272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5147396773937591, distance: 1.4083989546389166 entropy 1.1324905157089233
epoch: 14, step: 70
	action: tensor([[ 2.3245,  0.5127,  2.0145, -0.8169,  0.5672, -0.4418, -0.3044]],
       dtype=torch.float64)
	q_value: tensor([[-36.9709]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 14, step: 71
	action: tensor([[ 2.5633, -0.1544, -0.6046, -1.5567, -0.7087, -0.3219, -0.6343]],
       dtype=torch.float64)
	q_value: tensor([[-39.2272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 14, step: 72
	action: tensor([[ 2.1330, -1.6097,  0.7940, -0.7756, -0.7811, -0.8886, -0.1461]],
       dtype=torch.float64)
	q_value: tensor([[-39.2272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.618844898775869, distance: 1.4559930898126152 entropy 1.1324905157089233
epoch: 14, step: 73
	action: tensor([[ 3.5058, -1.6800,  1.4730,  0.7365,  0.8895, -2.3656, -0.0583]],
       dtype=torch.float64)
	q_value: tensor([[-37.5484]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 14, step: 74
	action: tensor([[ 1.5213,  0.6596,  1.8480,  0.0859, -0.2784, -1.2064,  0.0466]],
       dtype=torch.float64)
	q_value: tensor([[-39.2272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9804415674052566, distance: 0.16003822174493626 entropy 1.1324905157089233
epoch: 14, step: 75
	action: tensor([[ 3.7455,  1.1580,  1.0689, -0.4492, -0.2816, -0.9437,  0.3878]],
       dtype=torch.float64)
	q_value: tensor([[-38.5169]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 14, step: 76
	action: tensor([[ 2.3124, -0.8834,  1.1164,  2.1891,  0.3478, -1.8692,  0.5699]],
       dtype=torch.float64)
	q_value: tensor([[-39.2272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1389267709823767, distance: 1.0618831680279324 entropy 1.1324905157089233
epoch: 14, step: 77
	action: tensor([[ 3.3943,  0.0907,  1.4363, -2.2504, -0.1594, -0.8874, -1.6804]],
       dtype=torch.float64)
	q_value: tensor([[-53.6414]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 14, step: 78
	action: tensor([[ 2.0568,  0.2698,  1.2849, -0.8651,  0.0289,  0.2674, -0.2370]],
       dtype=torch.float64)
	q_value: tensor([[-39.2272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7734287944182681, distance: 0.5447021017800289 entropy 1.1324905157089233
epoch: 14, step: 79
	action: tensor([[ 1.0117, -0.2457,  0.9496, -0.4093,  0.5943, -3.2299,  0.1863]],
       dtype=torch.float64)
	q_value: tensor([[-32.0524]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 14, step: 80
	action: tensor([[ 1.1116,  0.6926, -0.4477,  0.0282, -1.7971, -2.0051,  0.0316]],
       dtype=torch.float64)
	q_value: tensor([[-39.2272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7612877542710181, distance: 0.5591058579277428 entropy 1.1324905157089233
epoch: 14, step: 81
	action: tensor([[ 4.7489, -0.1713,  0.6857, -0.4546, -0.6965, -2.2112,  0.7817]],
       dtype=torch.float64)
	q_value: tensor([[-37.5841]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 14, step: 82
	action: tensor([[ 2.4111,  1.7571, -0.7662,  0.1152,  0.0688, -1.7755, -1.1426]],
       dtype=torch.float64)
	q_value: tensor([[-39.2272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 14, step: 83
	action: tensor([[ 1.5005,  0.0170, -0.1135,  0.6391,  0.5760, -0.6893,  0.8361]],
       dtype=torch.float64)
	q_value: tensor([[-39.2272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7651131646098446, distance: 0.5546078746231967 entropy 1.1324905157089233
epoch: 14, step: 84
	action: tensor([[ 1.6005,  0.5529,  2.6443,  0.2403, -0.8317, -0.1254, -0.4478]],
       dtype=torch.float64)
	q_value: tensor([[-29.6333]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6324001600877183, distance: 0.6938160626740915 entropy 1.1324905157089233
epoch: 14, step: 85
	action: tensor([[ 3.2441,  0.3068, -0.2001, -0.7070, -2.1061, -1.3392, -0.8540]],
       dtype=torch.float64)
	q_value: tensor([[-46.1443]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 14, step: 86
	action: tensor([[ 2.5560, -0.4974,  0.7496,  0.2242, -0.7247, -0.1027, -0.5800]],
       dtype=torch.float64)
	q_value: tensor([[-39.2272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 14, step: 87
	action: tensor([[ 2.3151, -0.3156,  1.1297, -1.4798,  0.9526, -0.5272, -1.0206]],
       dtype=torch.float64)
	q_value: tensor([[-39.2272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5567736254136756, distance: 0.7618500336343891 entropy 1.1324905157089233
epoch: 14, step: 88
	action: tensor([[ 3.4516, -1.0557,  1.9268,  0.6753, -1.1924, -1.3601, -0.4216]],
       dtype=torch.float64)
	q_value: tensor([[-33.7590]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 14, step: 89
	action: tensor([[ 1.5020,  0.6065, -0.3556, -1.1826,  2.6585, -0.2235, -0.0918]],
       dtype=torch.float64)
	q_value: tensor([[-39.2272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3761489301317642, distance: 0.9038516504215364 entropy 1.1324905157089233
epoch: 14, step: 90
	action: tensor([[ 4.7917, -0.0268,  0.8513, -1.6014,  0.0805, -2.6642, -0.1927]],
       dtype=torch.float64)
	q_value: tensor([[-38.9994]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 14, step: 91
	action: tensor([[ 1.6992,  1.1592, -0.1348, -0.5229, -0.4934, -1.0803,  0.6481]],
       dtype=torch.float64)
	q_value: tensor([[-39.2272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5806345530990001, distance: 0.7410593915003438 entropy 1.1324905157089233
epoch: 14, step: 92
	action: tensor([[ 1.6234, -0.2026, -0.1482,  0.4104, -0.0568, -1.6414,  0.3921]],
       dtype=torch.float64)
	q_value: tensor([[-31.5371]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22428541923423995, distance: 1.0078773157176515 entropy 1.1324905157089233
epoch: 14, step: 93
	action: tensor([[ 4.0954, -1.0001, -0.1518, -0.8666,  0.9915, -1.4909,  0.7920]],
       dtype=torch.float64)
	q_value: tensor([[-34.6445]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 14, step: 94
	action: tensor([[ 1.2906,  0.4371, -0.2120, -0.3399, -0.1159, -0.6588, -0.2863]],
       dtype=torch.float64)
	q_value: tensor([[-39.2272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6470969353409647, distance: 0.6798050840231092 entropy 1.1324905157089233
epoch: 14, step: 95
	action: tensor([[ 3.3260, -0.9559,  1.7375,  0.3078,  0.7910, -0.1300, -1.0570]],
       dtype=torch.float64)
	q_value: tensor([[-23.3456]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 14, step: 96
	action: tensor([[ 1.7560,  0.4428,  0.7746,  0.0953,  0.5141, -0.6129,  0.4600]],
       dtype=torch.float64)
	q_value: tensor([[-39.2272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7150898875157166, distance: 0.6108163104283448 entropy 1.1324905157089233
epoch: 14, step: 97
	action: tensor([[ 2.3248, -0.6504,  0.3128,  0.8652,  0.1081,  0.0199,  0.7142]],
       dtype=torch.float64)
	q_value: tensor([[-28.7340]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 14, step: 98
	action: tensor([[ 2.1594,  0.3350,  1.4696, -0.7786,  0.4981, -1.3002, -0.1009]],
       dtype=torch.float64)
	q_value: tensor([[-39.2272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9366375502360136, distance: 0.2880531794595074 entropy 1.1324905157089233
epoch: 14, step: 99
	action: tensor([[ 3.1671, -0.1409,  1.1133, -1.6481, -0.0450, -1.8486, -0.9444]],
       dtype=torch.float64)
	q_value: tensor([[-35.8180]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 14, step: 100
	action: tensor([[ 0.4275,  0.4366,  1.1962, -1.2835,  0.0674, -1.6307,  1.0942]],
       dtype=torch.float64)
	q_value: tensor([[-39.2272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2544272316173113, distance: 0.9881018488504036 entropy 1.1324905157089233
epoch: 14, step: 101
	action: tensor([[ 3.7817,  0.0161, -0.4540, -0.9576,  0.5064, -3.3620,  0.6763]],
       dtype=torch.float64)
	q_value: tensor([[-35.8405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 14, step: 102
	action: tensor([[ 1.1062, -1.1843, -0.3086, -0.4404, -0.5886,  0.3826, -1.1277]],
       dtype=torch.float64)
	q_value: tensor([[-39.2272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7540130038935231, distance: 1.5155598716000211 entropy 1.1324905157089233
epoch: 14, step: 103
	action: tensor([[ 2.3251, -0.1215,  0.7886, -0.2450,  1.5596, -1.4446, -0.2740]],
       dtype=torch.float64)
	q_value: tensor([[-29.2407]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 14, step: 104
	action: tensor([[ 2.1427,  0.2023, -0.0971, -0.5614,  0.2151, -2.0036, -0.5317]],
       dtype=torch.float64)
	q_value: tensor([[-39.2272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31480397339654775, distance: 0.9472489034494198 entropy 1.1324905157089233
epoch: 14, step: 105
	action: tensor([[ 2.7181, -0.5742,  1.0904,  0.3560, -0.3869, -1.0728,  0.7302]],
       dtype=torch.float64)
	q_value: tensor([[-35.8048]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 14, step: 106
	action: tensor([[ 1.5029,  0.1467, -0.4316, -0.7704,  0.0051, -0.8082, -0.4206]],
       dtype=torch.float64)
	q_value: tensor([[-39.2272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.003071770349470704, distance: 1.1425853208268937 entropy 1.1324905157089233
epoch: 14, step: 107
	action: tensor([[ 3.1539, -0.3530,  0.7423,  0.0038,  0.4688, -0.9514,  1.0876]],
       dtype=torch.float64)
	q_value: tensor([[-27.0359]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 14, step: 108
	action: tensor([[ 2.3198, -0.4008,  0.4655, -0.9438,  0.1195, -0.7762, -1.0216]],
       dtype=torch.float64)
	q_value: tensor([[-39.2272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10900228369673282, distance: 1.2050995200933499 entropy 1.1324905157089233
epoch: 14, step: 109
	action: tensor([[ 3.4725,  0.3829, -0.6278, -0.7448, -0.8355, -0.6297, -0.5055]],
       dtype=torch.float64)
	q_value: tensor([[-31.3916]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 14, step: 110
	action: tensor([[ 1.1323,  1.3171, -0.9229, -0.4742,  1.8226, -1.3925,  0.3459]],
       dtype=torch.float64)
	q_value: tensor([[-39.2272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9369571046133158, distance: 0.28732589535806713 entropy 1.1324905157089233
epoch: 14, step: 111
	action: tensor([[ 2.8205, -0.0511,  0.4712, -1.6503,  0.7174, -0.0460, -1.3365]],
       dtype=torch.float64)
	q_value: tensor([[-36.6709]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 14, step: 112
	action: tensor([[ 2.3162, -0.1057,  0.3220, -0.8741, -0.4490, -0.5618,  0.0856]],
       dtype=torch.float64)
	q_value: tensor([[-39.2272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09995913472423412, distance: 1.2001760849054803 entropy 1.1324905157089233
epoch: 14, step: 113
	action: tensor([[ 2.4461,  1.3017,  0.7970, -0.2023,  0.3332, -2.8437,  0.7593]],
       dtype=torch.float64)
	q_value: tensor([[-29.2255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 14, step: 114
	action: tensor([[ 1.9353,  0.5742,  0.9941, -1.0364,  1.0309, -1.7746, -0.8925]],
       dtype=torch.float64)
	q_value: tensor([[-39.2272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8904839783057195, distance: 0.37870018984230924 entropy 1.1324905157089233
epoch: 14, step: 115
	action: tensor([[ 3.8347,  0.4467,  1.1452, -1.2800, -0.0438, -1.2927, -0.0160]],
       dtype=torch.float64)
	q_value: tensor([[-38.3033]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 14, step: 116
	action: tensor([[ 2.1091,  0.2281,  0.7673, -2.1472, -0.8197, -1.6406,  0.8911]],
       dtype=torch.float64)
	q_value: tensor([[-39.2272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15479918120745417, distance: 1.0520506470877318 entropy 1.1324905157089233
epoch: 14, step: 117
	action: tensor([[ 6.1800, -0.5387, -0.0400, -0.8054,  0.8268, -2.4280,  0.5007]],
       dtype=torch.float64)
	q_value: tensor([[-45.3756]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 14, step: 118
	action: tensor([[ 2.5947,  0.1439, -0.3677, -1.3919,  0.7572, -1.8108,  0.4971]],
       dtype=torch.float64)
	q_value: tensor([[-39.2272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 14, step: 119
	action: tensor([[ 1.1502, -0.5541, -0.2297,  0.4284,  0.6068, -2.0090, -1.4992]],
       dtype=torch.float64)
	q_value: tensor([[-39.2272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23972157772733682, distance: 0.9977989111280656 entropy 1.1324905157089233
epoch: 14, step: 120
	action: tensor([[ 4.4190,  0.0628,  0.2713, -0.9036, -1.3299, -0.9408,  0.7433]],
       dtype=torch.float64)
	q_value: tensor([[-38.2051]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 14, step: 121
	action: tensor([[ 1.9371,  0.3951,  0.7783, -0.7039,  1.3321, -1.1256,  0.0119]],
       dtype=torch.float64)
	q_value: tensor([[-39.2272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8907795828443829, distance: 0.3781887526304953 entropy 1.1324905157089233
epoch: 14, step: 122
	action: tensor([[ 3.1147,  0.7312,  2.6808, -0.0295,  0.6781, -1.2944,  0.8890]],
       dtype=torch.float64)
	q_value: tensor([[-32.6624]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 14, step: 123
	action: tensor([[ 0.1789, -0.3911, -0.6138,  0.0135, -0.1392, -1.1760,  0.1546]],
       dtype=torch.float64)
	q_value: tensor([[-39.2272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1922457058883429, distance: 1.0284811373097384 entropy 1.1324905157089233
epoch: 14, step: 124
	action: tensor([[ 2.4278, -1.1156,  0.5217, -0.4518, -0.3688, -0.6534, -0.1477]],
       dtype=torch.float64)
	q_value: tensor([[-25.2055]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 14, step: 125
	action: tensor([[ 0.4595, -0.2210,  0.5415, -0.3715, -0.5267, -0.8515,  0.3564]],
       dtype=torch.float64)
	q_value: tensor([[-39.2272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.052298627086234784, distance: 1.1140186157636969 entropy 1.1324905157089233
epoch: 14, step: 126
	action: tensor([[ 2.1321, -0.3225, -0.4953,  1.1303,  0.0920, -0.4808, -0.5422]],
       dtype=torch.float64)
	q_value: tensor([[-23.5547]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 14, step: 127
	action: tensor([[ 2.6113e+00, -8.3408e-01,  9.4741e-01,  1.9431e-01, -2.8381e-01,
          8.9712e-04, -8.1052e-02]], dtype=torch.float64)
	q_value: tensor([[-39.2272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
LOSS epoch 14 actor 427.1417507796822 critic 317.8135697763135 
epoch: 15, step: 0
	action: tensor([[ 0.4667, -1.0331, -0.7125, -0.8734, -0.7925,  0.6599,  0.9089]],
       dtype=torch.float64)
	q_value: tensor([[-33.6853]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6714035416466775, distance: 1.4794399852064788 entropy 1.1324905157089233
epoch: 15, step: 1
	action: tensor([[ 2.0222,  0.1127,  0.2439, -0.6749,  0.7833, -0.9793,  1.4085]],
       dtype=torch.float64)
	q_value: tensor([[-28.9329]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 2
	action: tensor([[ 1.7924,  1.2123,  0.7535, -0.4049, -0.9806,  0.9140, -0.0642]],
       dtype=torch.float64)
	q_value: tensor([[-33.6853]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 3
	action: tensor([[-0.1880,  0.3774, -0.4756,  0.5312, -0.3393, -1.1361,  0.3764]],
       dtype=torch.float64)
	q_value: tensor([[-33.6853]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.47397542033130435, distance: 0.829964873687494 entropy 1.1324905157089233
epoch: 15, step: 4
	action: tensor([[ 0.9562, -0.0816, -0.1082,  0.0794, -0.9228, -0.4679,  0.6473]],
       dtype=torch.float64)
	q_value: tensor([[-22.4827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6542732840617851, distance: 0.672857602335004 entropy 1.1324905157089233
epoch: 15, step: 5
	action: tensor([[ 0.6892, -0.0573,  1.7689,  0.3108, -0.9662, -2.2026,  0.5342]],
       dtype=torch.float64)
	q_value: tensor([[-23.6462]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.531378548613733, distance: 0.7833715149923742 entropy 1.1324905157089233
epoch: 15, step: 6
	action: tensor([[ 4.0952,  1.0477,  0.9546, -0.4312,  1.2404, -1.0372,  0.3999]],
       dtype=torch.float64)
	q_value: tensor([[-43.4981]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 7
	action: tensor([[ 0.1184,  1.0754,  0.6398, -0.5457, -0.0634,  0.3479,  0.3648]],
       dtype=torch.float64)
	q_value: tensor([[-33.6853]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5755826498535437, distance: 0.7455096304696042 entropy 1.1324905157089233
epoch: 15, step: 8
	action: tensor([[ 1.1030,  0.2337,  1.4507, -0.1818,  0.2389, -0.2097,  0.7036]],
       dtype=torch.float64)
	q_value: tensor([[-22.2049]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9018479474946661, distance: 0.35851422007115935 entropy 1.1324905157089233
epoch: 15, step: 9
	action: tensor([[ 1.9534,  1.6280, -0.6918, -0.5905, -0.3397, -1.3484,  1.6281]],
       dtype=torch.float64)
	q_value: tensor([[-29.4505]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 10
	action: tensor([[ 0.4624, -0.5949, -0.8194, -1.4117,  0.3050, -1.9007, -0.8901]],
       dtype=torch.float64)
	q_value: tensor([[-33.6853]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17479692748762754, distance: 1.0395302015353602 entropy 1.1324905157089233
epoch: 15, step: 11
	action: tensor([[ 4.2750,  0.5351,  1.2577, -0.7705, -0.8201, -2.7498,  0.8499]],
       dtype=torch.float64)
	q_value: tensor([[-34.7485]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 12
	action: tensor([[ 1.4884,  0.3156,  0.9329, -0.5397, -0.2575,  0.8417, -0.8424]],
       dtype=torch.float64)
	q_value: tensor([[-33.6853]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7885191682701136, distance: 0.5262501082197611 entropy 1.1324905157089233
epoch: 15, step: 13
	action: tensor([[ 4.2760,  0.8580,  0.7785, -1.2906, -0.1352, -3.0302,  0.4460]],
       dtype=torch.float64)
	q_value: tensor([[-32.1237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 14
	action: tensor([[ 2.6359,  0.5737,  0.4794, -0.3564,  0.0235, -1.2889, -0.3960]],
       dtype=torch.float64)
	q_value: tensor([[-33.6853]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 15
	action: tensor([[ 0.8161,  1.4819,  0.7867, -1.3713,  1.1598,  1.4200, -0.5427]],
       dtype=torch.float64)
	q_value: tensor([[-33.6853]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9532107423591141, distance: 0.2475310993374292 entropy 1.1324905157089233
epoch: 15, step: 16
	action: tensor([[ 4.2224, -1.0434,  1.3389, -1.2526,  0.3766, -2.7471,  0.4790]],
       dtype=torch.float64)
	q_value: tensor([[-34.3187]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 17
	action: tensor([[-0.0374, -0.0505, -0.4104, -0.5585,  0.6569, -0.1054,  0.4797]],
       dtype=torch.float64)
	q_value: tensor([[-33.6853]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.030748723072156592, distance: 1.1266133266793672 entropy 1.1324905157089233
epoch: 15, step: 18
	action: tensor([[ 0.5073,  0.7257,  1.1132,  1.0551, -0.8113, -0.5732,  0.9463]],
       dtype=torch.float64)
	q_value: tensor([[-18.2300]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9632484639776296, distance: 0.21937869100351626 entropy 1.1324905157089233
epoch: 15, step: 19
	action: tensor([[ 2.8337, -0.0823, -0.1943, -0.5831,  0.0090, -1.2490, -0.3111]],
       dtype=torch.float64)
	q_value: tensor([[-29.9322]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 20
	action: tensor([[ 1.2656,  0.0553, -0.6510,  0.5493,  1.0034, -0.3205, -0.4179]],
       dtype=torch.float64)
	q_value: tensor([[-33.6853]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8947528288511902, distance: 0.3712461077501438 entropy 1.1324905157089233
epoch: 15, step: 21
	action: tensor([[ 2.9022,  0.6821,  0.6688,  0.7200,  0.1520, -2.1833,  0.4239]],
       dtype=torch.float64)
	q_value: tensor([[-27.3733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 22
	action: tensor([[ 1.5895,  0.3636, -0.7257,  1.4572, -0.1941, -0.2675,  1.1349]],
       dtype=torch.float64)
	q_value: tensor([[-33.6853]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9415076778645737, distance: 0.27676178619184383 entropy 1.1324905157089233
epoch: 15, step: 23
	action: tensor([[ 2.5396,  0.7257,  1.1889, -0.9144, -0.0035, -2.5444,  0.5503]],
       dtype=torch.float64)
	q_value: tensor([[-32.6247]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 24
	action: tensor([[ 1.1730, -0.5804,  1.2474, -0.0877,  0.4348, -0.1267,  1.9114]],
       dtype=torch.float64)
	q_value: tensor([[-33.6853]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15895455474037667, distance: 1.0494612921000064 entropy 1.1324905157089233
epoch: 15, step: 25
	action: tensor([[ 2.1331,  0.3146,  0.9108, -0.5998,  0.7642, -2.2320, -0.1743]],
       dtype=torch.float64)
	q_value: tensor([[-38.6012]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 26
	action: tensor([[ 0.4421, -1.3116, -0.5320, -1.0622,  0.8179, -1.7816, -0.6526]],
       dtype=torch.float64)
	q_value: tensor([[-33.6853]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3529512903516736, distance: 1.3310604331199234 entropy 1.1324905157089233
epoch: 15, step: 27
	action: tensor([[ 3.5034, -0.5794,  1.1511, -0.5230, -0.2797, -1.6316,  0.8066]],
       dtype=torch.float64)
	q_value: tensor([[-35.9377]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 28
	action: tensor([[-0.6307, -0.2103, -0.7486, -0.0567,  0.0983, -0.6329,  0.5531]],
       dtype=torch.float64)
	q_value: tensor([[-33.6853]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5137634963605349, distance: 1.407945056868179 entropy 1.1324905157089233
epoch: 15, step: 29
	action: tensor([[ 2.2593, -1.0105,  0.9194, -0.3620,  0.4627, -0.5651,  1.0671]],
       dtype=torch.float64)
	q_value: tensor([[-22.1659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 30
	action: tensor([[ 0.8580,  0.1765,  0.9698, -0.0079, -0.8868, -0.0794,  0.2706]],
       dtype=torch.float64)
	q_value: tensor([[-33.6853]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8584906049581285, distance: 0.4304763834648251 entropy 1.1324905157089233
epoch: 15, step: 31
	action: tensor([[-0.1538,  0.1482,  0.2408, -0.5122,  0.7798, -0.5244, -0.3013]],
       dtype=torch.float64)
	q_value: tensor([[-26.8465]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23469537434626786, distance: 1.2715592567664937 entropy 1.1324905157089233
epoch: 15, step: 32
	action: tensor([[ 1.4672,  0.2122,  0.1224, -0.5044,  0.0526,  0.0572,  0.3909]],
       dtype=torch.float64)
	q_value: tensor([[-17.9850]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5156679302864711, distance: 0.7963945998489181 entropy 1.1324905157089233
epoch: 15, step: 33
	action: tensor([[ 2.8173,  0.5817,  1.4601, -2.0952, -0.6999, -2.6563,  1.3316]],
       dtype=torch.float64)
	q_value: tensor([[-24.7217]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 34
	action: tensor([[ 0.0510, -0.9762,  1.1934, -0.4218, -0.1819, -1.0240,  0.3702]],
       dtype=torch.float64)
	q_value: tensor([[-33.6853]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8409932779002403, distance: 1.5526829878896122 entropy 1.1324905157089233
epoch: 15, step: 35
	action: tensor([[ 2.6157,  0.6453,  1.2434,  0.6783, -0.0354, -2.0802,  0.7019]],
       dtype=torch.float64)
	q_value: tensor([[-31.8777]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 36
	action: tensor([[ 0.3998, -0.4697, -0.3845, -0.3881,  1.3359,  0.4242, -0.3215]],
       dtype=torch.float64)
	q_value: tensor([[-33.6853]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1398075390956579, distance: 1.0613399435427717 entropy 1.1324905157089233
epoch: 15, step: 37
	action: tensor([[ 2.1042, -0.5859,  1.2907, -1.0458, -0.7653, -2.8079,  0.2409]],
       dtype=torch.float64)
	q_value: tensor([[-22.7532]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 38
	action: tensor([[ 1.1198, -1.2150,  0.0312, -0.5886, -0.5280, -0.7018, -0.6029]],
       dtype=torch.float64)
	q_value: tensor([[-33.6853]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9767039185861803, distance: 1.608894269654673 entropy 1.1324905157089233
epoch: 15, step: 39
	action: tensor([[ 3.5105,  0.1806,  1.3695,  0.1408,  1.0436, -0.8307,  0.7783]],
       dtype=torch.float64)
	q_value: tensor([[-29.8919]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 40
	action: tensor([[-0.2023,  0.7545,  0.0557, -0.8822,  0.3181, -0.3510,  0.9159]],
       dtype=torch.float64)
	q_value: tensor([[-33.6853]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13563357330603654, distance: 1.063911830549407 entropy 1.1324905157089233
epoch: 15, step: 41
	action: tensor([[ 1.8783, -0.6739, -0.1783,  0.2284,  0.4665,  0.3471, -0.2536]],
       dtype=torch.float64)
	q_value: tensor([[-22.0553]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06254813067031428, distance: 1.1079781169666518 entropy 1.1324905157089233
epoch: 15, step: 42
	action: tensor([[ 4.2869,  1.2152,  1.6428, -0.7470, -1.1373, -2.7710,  0.5838]],
       dtype=torch.float64)
	q_value: tensor([[-27.6936]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 43
	action: tensor([[ 1.2377, -0.0212,  0.0947, -0.4835, -1.9012, -1.6083, -0.5764]],
       dtype=torch.float64)
	q_value: tensor([[-33.6853]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4937371573013388, distance: 0.8142255383021892 entropy 1.1324905157089233
epoch: 15, step: 44
	action: tensor([[ 2.2690,  1.4727,  2.6493, -0.4697, -0.1351, -2.3360,  1.6580]],
       dtype=torch.float64)
	q_value: tensor([[-38.5095]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 45
	action: tensor([[-0.0805, -1.0484, -0.1975, -0.6383,  1.4252, -0.2118,  1.3033]],
       dtype=torch.float64)
	q_value: tensor([[-33.6853]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9075465540710119, distance: 1.580499148405174 entropy 1.1324905157089233
epoch: 15, step: 46
	action: tensor([[ 0.6308, -0.0212,  1.8287, -1.9272, -1.8321, -1.5955,  1.0540]],
       dtype=torch.float64)
	q_value: tensor([[-33.6693]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2476369059781215, distance: 0.9925912333601564 entropy 1.1324905157089233
epoch: 15, step: 47
	action: tensor([[ 3.7151,  1.5918,  1.0114, -0.6110, -1.2545, -1.5458,  1.2688]],
       dtype=torch.float64)
	q_value: tensor([[-51.4477]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 48
	action: tensor([[ 0.4197, -0.4777,  1.0001,  0.6374, -0.9768,  0.0945, -1.1428]],
       dtype=torch.float64)
	q_value: tensor([[-33.6853]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6302262472041666, distance: 0.6958645841132552 entropy 1.1324905157089233
epoch: 15, step: 49
	action: tensor([[ 1.6420, -0.3163, -0.2625,  0.5346,  0.4842, -1.1691, -0.7182]],
       dtype=torch.float64)
	q_value: tensor([[-30.0730]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30697209611221943, distance: 0.9526471091472971 entropy 1.1324905157089233
epoch: 15, step: 50
	action: tensor([[ 3.2394,  0.4907,  0.3990, -0.4727, -1.6335, -2.1193, -0.4081]],
       dtype=torch.float64)
	q_value: tensor([[-32.9608]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 51
	action: tensor([[ 1.7139,  1.3819,  0.4780, -0.2234, -0.0482, -1.2282, -0.9983]],
       dtype=torch.float64)
	q_value: tensor([[-33.6853]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8717074950996324, distance: 0.40988057393977556 entropy 1.1324905157089233
epoch: 15, step: 52
	action: tensor([[ 2.5995, -0.2046,  1.3920, -0.4560, -0.2822, -1.7963,  0.4106]],
       dtype=torch.float64)
	q_value: tensor([[-34.2519]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 53
	action: tensor([[-0.5385,  0.7924, -0.6153,  1.2038, -0.1885, -1.1910, -0.0602]],
       dtype=torch.float64)
	q_value: tensor([[-33.6853]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.182198180947872, distance: 1.0348579238513154 entropy 1.1324905157089233
epoch: 15, step: 54
	action: tensor([[ 2.6536,  0.7246,  1.0843, -0.3023, -2.2607, -0.3133,  0.3542]],
       dtype=torch.float64)
	q_value: tensor([[-24.9966]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 55
	action: tensor([[ 2.0540,  1.2302,  0.7919, -1.1276, -0.7756, -1.0759,  0.9220]],
       dtype=torch.float64)
	q_value: tensor([[-33.6853]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8522159988017584, distance: 0.43991665419775083 entropy 1.1324905157089233
epoch: 15, step: 56
	action: tensor([[ 3.3882, -0.1365,  1.1517, -0.5494, -0.2110, -1.6032,  0.3529]],
       dtype=torch.float64)
	q_value: tensor([[-37.9086]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 57
	action: tensor([[-0.2633, -0.8647, -0.3426, -0.1416, -0.9019,  0.1050,  0.3668]],
       dtype=torch.float64)
	q_value: tensor([[-33.6853]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7624375883805956, distance: 1.519195154697538 entropy 1.1324905157089233
epoch: 15, step: 58
	action: tensor([[ 0.6566,  1.1274,  1.6565, -0.1362,  0.0940,  0.1056,  0.8102]],
       dtype=torch.float64)
	q_value: tensor([[-22.7126]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.854883691349419, distance: 0.43592803974267846 entropy 1.1324905157089233
epoch: 15, step: 59
	action: tensor([[ 3.1133,  0.9340,  0.3228,  0.2163,  0.4057, -0.7156,  0.8261]],
       dtype=torch.float64)
	q_value: tensor([[-27.9078]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 60
	action: tensor([[ 0.9271,  0.6289, -0.2649, -0.9500, -0.6733, -0.2606, -0.4506]],
       dtype=torch.float64)
	q_value: tensor([[-33.6853]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7574803584526816, distance: 0.5635470128581631 entropy 1.1324905157089233
epoch: 15, step: 61
	action: tensor([[ 2.6414,  0.5677,  0.0275,  0.4507, -1.4641, -1.2817,  0.3938]],
       dtype=torch.float64)
	q_value: tensor([[-25.3278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 62
	action: tensor([[ 1.7120,  0.7539,  1.8692,  0.4295, -0.9758, -1.2440, -0.3327]],
       dtype=torch.float64)
	q_value: tensor([[-33.6853]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9902670837646673, distance: 0.11289590416738385 entropy 1.1324905157089233
epoch: 15, step: 63
	action: tensor([[ 2.5540,  1.2969,  1.6513,  0.1656,  0.7180, -2.1116,  1.9327]],
       dtype=torch.float64)
	q_value: tensor([[-42.6234]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 64
	action: tensor([[ 1.5224,  0.4509,  0.6052,  0.6171,  0.3206, -0.6848, -1.0366]],
       dtype=torch.float64)
	q_value: tensor([[-33.6853]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7563521145623231, distance: 0.5648563515585325 entropy 1.1324905157089233
epoch: 15, step: 65
	action: tensor([[ 3.1313,  0.7696,  1.9434, -0.5469, -0.7640,  0.3305, -0.3889]],
       dtype=torch.float64)
	q_value: tensor([[-31.0658]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 66
	action: tensor([[-0.2102,  0.3001,  0.2685,  0.1612, -1.0286, -1.0775, -1.3282]],
       dtype=torch.float64)
	q_value: tensor([[-33.6853]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4380076870281, distance: 0.8578707905789031 entropy 1.1324905157089233
epoch: 15, step: 67
	action: tensor([[ 1.5822,  0.0302, -0.1014, -0.8415, -0.9828, -1.3826,  0.4564]],
       dtype=torch.float64)
	q_value: tensor([[-27.7102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.051965358565374986, distance: 1.17370083241308 entropy 1.1324905157089233
epoch: 15, step: 68
	action: tensor([[ 2.7144,  0.2783,  0.6271, -1.9651, -0.2413, -2.0627,  1.0618]],
       dtype=torch.float64)
	q_value: tensor([[-33.4444]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 69
	action: tensor([[ 0.4401, -0.5723,  0.4355,  0.4215, -0.5690, -1.2103,  1.2906]],
       dtype=torch.float64)
	q_value: tensor([[-33.6853]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1371218252274855, distance: 1.062995522997924 entropy 1.1324905157089233
epoch: 15, step: 70
	action: tensor([[ 2.4248, -0.3178,  0.5247, -0.5276, -0.2135, -0.2623, -0.0667]],
       dtype=torch.float64)
	q_value: tensor([[-32.2280]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 71
	action: tensor([[-0.4764,  0.6822,  0.5219,  0.1859, -0.1016, -0.9006, -0.3660]],
       dtype=torch.float64)
	q_value: tensor([[-33.6853]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07183056875323779, distance: 1.1024789949692426 entropy 1.1324905157089233
epoch: 15, step: 72
	action: tensor([[ 1.2066,  1.2317,  0.3804,  0.6473, -0.0113, -0.0931,  0.6381]],
       dtype=torch.float64)
	q_value: tensor([[-20.3382]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 73
	action: tensor([[ 1.5074, -0.0767, -0.3301, -0.2013,  0.1525, -1.1376,  0.6525]],
       dtype=torch.float64)
	q_value: tensor([[-33.6853]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12051339151400486, distance: 1.0731768822852639 entropy 1.1324905157089233
epoch: 15, step: 74
	action: tensor([[ 3.2143,  0.9149,  0.4195, -0.8372, -0.6900, -3.3220, -0.6834]],
       dtype=torch.float64)
	q_value: tensor([[-29.5688]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 75
	action: tensor([[ 0.3813,  0.9622, -0.1716,  0.7544,  0.4795, -0.4271,  1.1180]],
       dtype=torch.float64)
	q_value: tensor([[-33.6853]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 76
	action: tensor([[ 0.2066, -0.0563,  0.4132,  0.2453, -0.2901,  0.9594,  1.3594]],
       dtype=torch.float64)
	q_value: tensor([[-33.6853]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7718810069511103, distance: 0.5465594607774216 entropy 1.1324905157089233
epoch: 15, step: 77
	action: tensor([[ 2.2107, -0.1691,  1.3631, -0.3835,  0.1647, -1.4512,  0.3266]],
       dtype=torch.float64)
	q_value: tensor([[-27.1597]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 78
	action: tensor([[ 0.3757,  0.5364, -0.5032,  0.7832,  0.3370, -0.6216,  0.6389]],
       dtype=torch.float64)
	q_value: tensor([[-33.6853]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7151013117061854, distance: 0.6108040641943251 entropy 1.1324905157089233
epoch: 15, step: 79
	action: tensor([[ 1.1839, -0.2418,  1.6259,  0.0508, -0.7409, -0.4071,  0.9295]],
       dtype=torch.float64)
	q_value: tensor([[-21.8476]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.45859253433172, distance: 0.8420130322318846 entropy 1.1324905157089233
epoch: 15, step: 80
	action: tensor([[ 1.4487,  0.4032, -0.6999, -0.4396, -0.8979, -1.4203,  0.7849]],
       dtype=torch.float64)
	q_value: tensor([[-35.9593]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.361717591897347, distance: 0.9142461301228522 entropy 1.1324905157089233
epoch: 15, step: 81
	action: tensor([[ 3.8356,  1.4974,  0.1230, -0.3437, -1.7883, -3.1934,  1.0761]],
       dtype=torch.float64)
	q_value: tensor([[-31.5450]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 82
	action: tensor([[-0.0448, -1.2074, -1.6766, -0.1026, -0.2756, -0.4313, -0.0858]],
       dtype=torch.float64)
	q_value: tensor([[-33.6853]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5612391300608481, distance: 1.4298530533117226 entropy 1.1324905157089233
epoch: 15, step: 83
	action: tensor([[ 1.9265e+00, -8.9625e-04,  2.1084e+00, -5.6317e-01, -8.8115e-01,
         -2.1777e+00,  3.2914e-01]], dtype=torch.float64)
	q_value: tensor([[-27.6700]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7255263630902349, distance: 0.5995246051898838 entropy 1.1324905157089233
epoch: 15, step: 84
	action: tensor([[ 5.3742,  0.7684,  0.0668,  0.1019, -0.6858, -2.2223, -0.2376]],
       dtype=torch.float64)
	q_value: tensor([[-49.9138]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 85
	action: tensor([[ 0.4021,  0.9168,  0.8237, -0.4981,  0.3661, -1.2879,  0.3551]],
       dtype=torch.float64)
	q_value: tensor([[-33.6853]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5353998686381318, distance: 0.7800031514910908 entropy 1.1324905157089233
epoch: 15, step: 86
	action: tensor([[ 2.0661,  0.5388,  0.3055,  0.0671,  0.4869, -1.1411, -0.4257]],
       dtype=torch.float64)
	q_value: tensor([[-25.9865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 87
	action: tensor([[ 0.3506, -0.0958,  0.7195, -0.9344,  0.5496, -1.1614,  0.1585]],
       dtype=torch.float64)
	q_value: tensor([[-33.6853]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05624427565604595, distance: 1.1760854508301533 entropy 1.1324905157089233
epoch: 15, step: 88
	action: tensor([[ 2.8329,  0.2884,  1.2666,  0.1965,  0.6474, -0.8997,  1.0146]],
       dtype=torch.float64)
	q_value: tensor([[-25.7502]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 89
	action: tensor([[ 1.4169, -0.1635,  1.0009,  0.2795,  0.3639, -0.0359,  0.4871]],
       dtype=torch.float64)
	q_value: tensor([[-33.6853]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4170819487457653, distance: 0.8736961972269218 entropy 1.1324905157089233
epoch: 15, step: 90
	action: tensor([[ 2.3463,  0.7464,  1.8893,  0.5354, -0.2938, -2.0979,  0.8671]],
       dtype=torch.float64)
	q_value: tensor([[-29.6771]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 91
	action: tensor([[ 0.7076,  0.0989,  1.1635, -0.5404, -0.7900, -0.3620, -0.7491]],
       dtype=torch.float64)
	q_value: tensor([[-33.6853]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5409743910061833, distance: 0.7753095812508731 entropy 1.1324905157089233
epoch: 15, step: 92
	action: tensor([[ 2.6215,  0.1865,  1.0556,  0.7592, -1.3080, -1.4219,  0.9732]],
       dtype=torch.float64)
	q_value: tensor([[-30.0684]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 93
	action: tensor([[-0.8053,  0.1423,  0.4172, -0.1574, -0.3600, -0.2519,  0.5549]],
       dtype=torch.float64)
	q_value: tensor([[-33.6853]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8106075412119216, distance: 1.5398160982302314 entropy 1.1324905157089233
epoch: 15, step: 94
	action: tensor([[ 0.7812, -0.9707, -0.1099, -1.1997,  0.2994, -0.0637,  0.8263]],
       dtype=torch.float64)
	q_value: tensor([[-21.0022]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8572338016218566, distance: 1.5595165317365265 entropy 1.1324905157089233
epoch: 15, step: 95
	action: tensor([[ 2.4193,  0.0273,  0.7181, -0.9455, -0.4565, -1.1104,  0.7037]],
       dtype=torch.float64)
	q_value: tensor([[-27.3380]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 96
	action: tensor([[ 1.3476, -0.2400, -0.0699, -0.3421, -0.9890,  0.8310, -0.5187]],
       dtype=torch.float64)
	q_value: tensor([[-33.6853]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.48769962671805833, distance: 0.8190662470425923 entropy 1.1324905157089233
epoch: 15, step: 97
	action: tensor([[ 1.9882,  0.9197,  0.9708, -0.9477,  0.0306, -2.0089,  0.2163]],
       dtype=torch.float64)
	q_value: tensor([[-29.5209]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 98
	action: tensor([[ 0.3280,  0.9789,  0.6027, -0.8003,  0.1757,  0.3403,  0.9165]],
       dtype=torch.float64)
	q_value: tensor([[-33.6853]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.684814343021673, distance: 0.6424508621679026 entropy 1.1324905157089233
epoch: 15, step: 99
	action: tensor([[ 1.1847,  0.7896, -0.1389, -0.8965, -0.4193, -1.1024,  0.5963]],
       dtype=torch.float64)
	q_value: tensor([[-23.9771]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.653397662054522, distance: 0.6737091366854931 entropy 1.1324905157089233
epoch: 15, step: 100
	action: tensor([[ 2.0100,  1.1486,  0.5692, -0.6179, -0.0478, -2.8360, -0.2443]],
       dtype=torch.float64)
	q_value: tensor([[-28.0424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 101
	action: tensor([[ 0.6686,  1.3956, -1.1168, -0.6285, -1.6817,  0.1082, -1.4343]],
       dtype=torch.float64)
	q_value: tensor([[-33.6853]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 102
	action: tensor([[ 0.7871,  0.0953, -0.1107,  0.2563,  0.8126, -0.0702, -1.0391]],
       dtype=torch.float64)
	q_value: tensor([[-33.6853]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8875786026093014, distance: 0.3836906205873058 entropy 1.1324905157089233
epoch: 15, step: 103
	action: tensor([[ 3.0980,  0.3238,  1.1679, -1.5407, -0.8056, -1.0775, -0.7089]],
       dtype=torch.float64)
	q_value: tensor([[-22.3471]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 104
	action: tensor([[ 1.5810,  0.2326, -0.0126,  0.0419, -1.0039,  0.0157, -1.2529]],
       dtype=torch.float64)
	q_value: tensor([[-33.6853]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6043150371007933, distance: 0.7198325256655405 entropy 1.1324905157089233
epoch: 15, step: 105
	action: tensor([[ 3.1731,  1.8661,  1.9012, -0.8231,  0.7348, -3.2421, -0.5472]],
       dtype=torch.float64)
	q_value: tensor([[-31.7239]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 106
	action: tensor([[ 0.3149, -0.2385,  0.5068, -0.3209,  0.8267,  0.0838,  1.1980]],
       dtype=torch.float64)
	q_value: tensor([[-33.6853]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14564518380222502, distance: 1.057732452582222 entropy 1.1324905157089233
epoch: 15, step: 107
	action: tensor([[ 2.7233,  0.4857,  0.3769, -0.9611, -0.9019, -1.3924,  0.2759]],
       dtype=torch.float64)
	q_value: tensor([[-25.6056]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 108
	action: tensor([[ 0.9062,  0.6848, -0.7722,  1.0466, -0.6882, -0.7897,  0.2648]],
       dtype=torch.float64)
	q_value: tensor([[-33.6853]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.843260056285141, distance: 0.4530504191187773 entropy 1.1324905157089233
epoch: 15, step: 109
	action: tensor([[ 0.7242,  0.4867,  1.6157,  0.1639, -0.6346, -0.7563, -0.1132]],
       dtype=torch.float64)
	q_value: tensor([[-24.7911]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9159191400543403, distance: 0.33182206804354253 entropy 1.1324905157089233
epoch: 15, step: 110
	action: tensor([[ 2.2066,  0.2502, -0.3507, -0.5972,  1.6445, -1.2514, -0.0919]],
       dtype=torch.float64)
	q_value: tensor([[-31.9664]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 111
	action: tensor([[-0.0547, -0.6003,  0.1140, -1.0072,  0.8769,  0.4157,  0.9026]],
       dtype=torch.float64)
	q_value: tensor([[-33.6853]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5750161136732268, distance: 1.4361479873630973 entropy 1.1324905157089233
epoch: 15, step: 112
	action: tensor([[ 1.6629,  0.2786, -0.5783, -1.9667, -0.1046, -0.9772,  0.0053]],
       dtype=torch.float64)
	q_value: tensor([[-26.4479]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0953270325149913, distance: 1.1976463529870875 entropy 1.1324905157089233
epoch: 15, step: 113
	action: tensor([[ 3.9324,  1.2442,  1.0848, -1.1113,  0.0323, -1.3642,  0.2516]],
       dtype=torch.float64)
	q_value: tensor([[-34.4952]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 114
	action: tensor([[-0.1834, -0.7246, -0.6236, -0.5988, -0.3500, -0.9974,  0.3729]],
       dtype=torch.float64)
	q_value: tensor([[-33.6853]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08493544070918269, distance: 1.1919516560364973 entropy 1.1324905157089233
epoch: 15, step: 115
	action: tensor([[ 1.1994,  0.0075,  1.8568, -0.6216,  0.0203, -1.1482,  0.2336]],
       dtype=torch.float64)
	q_value: tensor([[-25.1109]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8514179611292472, distance: 0.4411028360182911 entropy 1.1324905157089233
epoch: 15, step: 116
	action: tensor([[ 2.0045,  0.7691,  2.2845, -1.7579, -2.3217, -1.3562,  0.5616]],
       dtype=torch.float64)
	q_value: tensor([[-37.3855]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 117
	action: tensor([[-8.9359e-01,  1.6129e+00,  1.1583e-03,  4.8144e-01, -2.1378e-01,
         -6.9521e-02, -2.7321e-01]], dtype=torch.float64)
	q_value: tensor([[-33.6853]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 118
	action: tensor([[ 0.8767,  1.1110, -1.3412, -0.5475,  0.0831, -0.4267, -0.7656]],
       dtype=torch.float64)
	q_value: tensor([[-33.6853]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8068427999714342, distance: 0.5029353289491966 entropy 1.1324905157089233
epoch: 15, step: 119
	action: tensor([[ 1.9886,  0.5436,  0.4753, -0.5627, -1.0789, -1.6400, -0.1307]],
       dtype=torch.float64)
	q_value: tensor([[-27.4594]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 120
	action: tensor([[ 0.8989,  1.1710, -0.1649, -0.3211, -0.2167, -0.1083, -0.5459]],
       dtype=torch.float64)
	q_value: tensor([[-33.6853]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 121
	action: tensor([[ 0.0847, -0.4167, -0.6385, -0.8177, -0.8327, -0.5875,  0.2159]],
       dtype=torch.float64)
	q_value: tensor([[-33.6853]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14175360075224075, distance: 1.0601386993025756 entropy 1.1324905157089233
epoch: 15, step: 122
	action: tensor([[ 1.5758,  0.6058,  0.3831, -1.5635, -0.2530, -1.3868, -0.1223]],
       dtype=torch.float64)
	q_value: tensor([[-23.7834]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3226489379985882, distance: 0.9418106590243619 entropy 1.1324905157089233
epoch: 15, step: 123
	action: tensor([[ 2.5812,  0.9498,  3.0951, -0.1440,  0.7481, -2.4602, -0.4422]],
       dtype=torch.float64)
	q_value: tensor([[-35.5704]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 124
	action: tensor([[ 1.3821,  0.7277, -1.0931, -1.1215, -0.0688,  0.2419, -1.0363]],
       dtype=torch.float64)
	q_value: tensor([[-33.6853]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3961013309629292, distance: 0.8892804185292622 entropy 1.1324905157089233
epoch: 15, step: 125
	action: tensor([[ 2.8044, -0.6061,  2.1379, -0.9421, -0.7038, -1.2818,  0.9509]],
       dtype=torch.float64)
	q_value: tensor([[-30.6764]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
epoch: 15, step: 126
	action: tensor([[-0.5933, -0.8919,  0.8558, -1.7115,  0.4312,  1.6733, -0.9633]],
       dtype=torch.float64)
	q_value: tensor([[-33.6853]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7101358466253846, distance: 1.4964837265833055 entropy 1.1324905157089233
epoch: 15, step: 127
	action: tensor([[ 3.5360, -1.1220,  0.3965, -1.2339,  1.9026, -1.7169, -0.3902]],
       dtype=torch.float64)
	q_value: tensor([[-34.8346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.1324905157089233
LOSS epoch 15 actor 306.74353241322245 critic 327.80119700566644 
epoch: 16, step: 0
	action: tensor([[-0.0956, -1.3223,  0.3610,  0.2766,  0.0360,  1.3633, -0.3268]],
       dtype=torch.float64)
	q_value: tensor([[-32.2576]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0021504135297523153, distance: 1.1431131851337821 entropy 1.0271300077438354
epoch: 16, step: 1
	action: tensor([[ 0.9345, -0.9783, -0.2578,  0.7944,  0.3185, -1.7084,  0.1743]],
       dtype=torch.float64)
	q_value: tensor([[-30.0348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05285834960839941, distance: 1.1741988915877408 entropy 1.0271300077438354
epoch: 16, step: 2
	action: tensor([[ 0.9233,  1.0963,  1.7042, -0.6560, -1.1198, -2.7246,  0.5133]],
       dtype=torch.float64)
	q_value: tensor([[-43.4166]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 16, step: 3
	action: tensor([[-0.1591, -0.1260,  0.4452,  0.5348, -0.8382, -0.6700, -0.6003]],
       dtype=torch.float64)
	q_value: tensor([[-32.2576]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35889392920927643, distance: 0.9162661402986563 entropy 1.0271300077438354
epoch: 16, step: 4
	action: tensor([[-0.4639,  1.3728,  1.1845, -0.6530, -0.8778, -1.0982,  0.3142]],
       dtype=torch.float64)
	q_value: tensor([[-26.0133]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.029543216239264147, distance: 1.1611250215284776 entropy 1.0271300077438354
epoch: 16, step: 5
	action: tensor([[ 0.3839,  1.8563,  1.1986, -0.4132, -0.8037, -0.5895,  1.5474]],
       dtype=torch.float64)
	q_value: tensor([[-33.6321]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 16, step: 6
	action: tensor([[ 1.5005,  1.2622, -0.7469,  0.5093, -0.4457,  0.0861,  0.7156]],
       dtype=torch.float64)
	q_value: tensor([[-32.2576]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 16, step: 7
	action: tensor([[ 0.0682, -0.1264, -0.4178,  0.1230,  0.3495,  0.1221,  0.8665]],
       dtype=torch.float64)
	q_value: tensor([[-32.2576]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2912957162412454, distance: 0.9633613582977881 entropy 1.0271300077438354
epoch: 16, step: 8
	action: tensor([[ 0.0400,  0.5087,  0.6504,  0.4295, -1.0149, -0.3400,  0.3877]],
       dtype=torch.float64)
	q_value: tensor([[-20.3422]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7658391194496872, distance: 0.5537501597960328 entropy 1.0271300077438354
epoch: 16, step: 9
	action: tensor([[ 0.0928,  1.0685,  0.2864, -0.9554,  0.8530, -0.7558,  1.0973]],
       dtype=torch.float64)
	q_value: tensor([[-25.0376]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34443884932345237, distance: 0.9265381322911479 entropy 1.0271300077438354
epoch: 16, step: 10
	action: tensor([[ 1.4089,  1.1983,  1.2407, -0.0380,  0.2736, -0.2777,  0.1410]],
       dtype=torch.float64)
	q_value: tensor([[-27.1857]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 16, step: 11
	action: tensor([[ 1.6469, -0.3384,  0.7205, -0.0161, -1.4470,  0.0416,  0.2933]],
       dtype=torch.float64)
	q_value: tensor([[-32.2576]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09199534721293268, distance: 1.0904373790388453 entropy 1.0271300077438354
epoch: 16, step: 12
	action: tensor([[ 2.1477, -0.0668,  1.0762, -0.6101,  0.0113, -1.2080,  0.6076]],
       dtype=torch.float64)
	q_value: tensor([[-38.9646]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 16, step: 13
	action: tensor([[-1.4203, -1.8956, -0.2890, -0.4649,  0.0192,  0.2920, -0.6130]],
       dtype=torch.float64)
	q_value: tensor([[-32.2576]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 16, step: 14
	action: tensor([[-0.4637, -0.3080,  1.1491,  1.1883,  1.1182, -0.7559, -1.4250]],
       dtype=torch.float64)
	q_value: tensor([[-32.2576]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42119541870065424, distance: 0.870608039452219 entropy 1.0271300077438354
epoch: 16, step: 15
	action: tensor([[ 2.0264,  1.4413,  0.5534,  0.5088, -1.1363, -0.8367, -0.2564]],
       dtype=torch.float64)
	q_value: tensor([[-38.1353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 16, step: 16
	action: tensor([[-0.9729, -0.7744,  0.0733,  0.3623,  0.7000,  0.2396,  0.6044]],
       dtype=torch.float64)
	q_value: tensor([[-32.2576]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1385667577084613, distance: 1.6734706470407854 entropy 1.0271300077438354
epoch: 16, step: 17
	action: tensor([[ 0.6591,  0.3650, -0.6553,  1.7029, -1.2256,  0.3705,  0.3365]],
       dtype=torch.float64)
	q_value: tensor([[-28.3791]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.40551298907889133, distance: 0.8823235646552597 entropy 1.0271300077438354
epoch: 16, step: 18
	action: tensor([[ 1.4330,  0.8915,  1.0201, -0.5928, -1.0254, -1.1099, -0.5813]],
       dtype=torch.float64)
	q_value: tensor([[-30.4847]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 16, step: 19
	action: tensor([[ 1.1892, -0.3349,  0.5141, -0.1568,  0.5604, -1.5821, -0.3820]],
       dtype=torch.float64)
	q_value: tensor([[-32.2576]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37076114845227004, distance: 0.9077462388550902 entropy 1.0271300077438354
epoch: 16, step: 20
	action: tensor([[ 2.0663, -0.5520,  1.5521,  0.7857, -0.5990, -1.5559,  1.6495]],
       dtype=torch.float64)
	q_value: tensor([[-37.9118]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 16, step: 21
	action: tensor([[-0.8396,  0.1381,  0.3160,  1.5789, -0.1464,  0.3473,  1.3010]],
       dtype=torch.float64)
	q_value: tensor([[-32.2576]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5524643861453404, distance: 0.7655445937425119 entropy 1.0271300077438354
epoch: 16, step: 22
	action: tensor([[ 0.6586,  1.0632, -0.1584,  0.9270, -0.5804, -0.8967,  0.4427]],
       dtype=torch.float64)
	q_value: tensor([[-31.9322]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 16, step: 23
	action: tensor([[-0.0288,  0.0390,  0.5538,  0.1455, -0.4503, -0.1429, -0.1569]],
       dtype=torch.float64)
	q_value: tensor([[-32.2576]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3117628059548814, distance: 0.9493487062953199 entropy 1.0271300077438354
epoch: 16, step: 24
	action: tensor([[ 1.1258, -0.6380,  0.1514, -0.5971, -0.1777,  0.0448,  0.2934]],
       dtype=torch.float64)
	q_value: tensor([[-20.1433]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2937882836881731, distance: 1.3016322495192327 entropy 1.0271300077438354
epoch: 16, step: 25
	action: tensor([[ 2.9002, -0.1667,  0.1300, -0.2151, -0.6791, -1.0140,  0.5151]],
       dtype=torch.float64)
	q_value: tensor([[-26.7636]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 16, step: 26
	action: tensor([[-0.4251, -0.3657, -0.3278, -0.0985,  0.0332,  0.3281,  0.4704]],
       dtype=torch.float64)
	q_value: tensor([[-32.2576]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.48779496726666527, distance: 1.3958162045899882 entropy 1.0271300077438354
epoch: 16, step: 27
	action: tensor([[-0.4197, -0.0779, -0.1593,  0.7668, -0.1902, -0.5613,  0.5533]],
       dtype=torch.float64)
	q_value: tensor([[-18.6822]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03939611340639715, distance: 1.166667869909692 entropy 1.0271300077438354
epoch: 16, step: 28
	action: tensor([[ 0.6381, -0.3006,  0.2505,  0.0472, -0.6736, -0.4305, -0.0957]],
       dtype=torch.float64)
	q_value: tensor([[-22.6575]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.369594712469558, distance: 0.9085872053925516 entropy 1.0271300077438354
epoch: 16, step: 29
	action: tensor([[ 0.9722,  0.7416,  1.6601, -0.3109, -0.3299,  0.1319, -0.1320]],
       dtype=torch.float64)
	q_value: tensor([[-23.8819]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9777728428831897, distance: 0.1706077097347814 entropy 1.0271300077438354
epoch: 16, step: 30
	action: tensor([[ 0.7525,  1.1757, -0.3901,  0.2113, -1.7042, -1.3263, -0.3801]],
       dtype=torch.float64)
	q_value: tensor([[-35.4624]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6525239728039761, distance: 0.6745577200626708 entropy 1.0271300077438354
epoch: 16, step: 31
	action: tensor([[ 2.0548,  1.2431,  0.8336, -0.7092, -1.3210, -0.5190, -0.7351]],
       dtype=torch.float64)
	q_value: tensor([[-35.5000]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 16, step: 32
	action: tensor([[-0.8298,  0.0197,  1.0330, -0.2809,  0.7328, -0.9235,  0.0616]],
       dtype=torch.float64)
	q_value: tensor([[-32.2576]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0405353650384455, distance: 1.6346649710067525 entropy 1.0271300077438354
epoch: 16, step: 33
	action: tensor([[ 2.5158,  1.7151,  1.0417,  0.4240, -0.2141, -0.7290,  0.6642]],
       dtype=torch.float64)
	q_value: tensor([[-28.1200]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 16, step: 34
	action: tensor([[ 0.3874,  0.6821,  0.0573, -0.7359,  0.4029, -1.3588, -0.3911]],
       dtype=torch.float64)
	q_value: tensor([[-32.2576]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.55213548963736, distance: 0.7658258436511559 entropy 1.0271300077438354
epoch: 16, step: 35
	action: tensor([[ 1.1570,  0.2537,  0.9371,  0.9177, -0.2915, -2.0510, -0.0978]],
       dtype=torch.float64)
	q_value: tensor([[-27.0077]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8355615941971922, distance: 0.46404310292591744 entropy 1.0271300077438354
epoch: 16, step: 36
	action: tensor([[ 2.6239,  0.2941,  0.8901,  0.2345, -1.2242, -1.0799,  0.1479]],
       dtype=torch.float64)
	q_value: tensor([[-45.3408]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 16, step: 37
	action: tensor([[ 1.3943, -0.8934, -0.9618, -0.3920,  0.1166,  0.1329,  0.2127]],
       dtype=torch.float64)
	q_value: tensor([[-32.2576]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.546667227904279, distance: 1.4231646081154787 entropy 1.0271300077438354
epoch: 16, step: 38
	action: tensor([[ 2.0203,  0.7133,  1.5660, -0.1991,  0.5955, -1.9353,  0.4470]],
       dtype=torch.float64)
	q_value: tensor([[-28.6969]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 16, step: 39
	action: tensor([[-0.0213,  0.2567,  0.2833, -0.0810,  0.7327, -0.9841,  0.3127]],
       dtype=torch.float64)
	q_value: tensor([[-32.2576]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11097830865331604, distance: 1.0789787015239762 entropy 1.0271300077438354
epoch: 16, step: 40
	action: tensor([[ 0.2895,  1.1414,  1.1117, -0.1594, -0.7796, -0.2850, -0.1726]],
       dtype=torch.float64)
	q_value: tensor([[-24.1092]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 16, step: 41
	action: tensor([[-0.0318, -0.0301, -0.7832, -0.8142, -0.3859,  0.8364,  0.3796]],
       dtype=torch.float64)
	q_value: tensor([[-32.2576]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20530449681766538, distance: 1.0201336445048808 entropy 1.0271300077438354
epoch: 16, step: 42
	action: tensor([[ 0.4265,  0.1707,  0.3441,  0.0469, -1.3552, -0.9535,  0.1431]],
       dtype=torch.float64)
	q_value: tensor([[-23.9254]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.76600283267434, distance: 0.553556549157097 entropy 1.0271300077438354
epoch: 16, step: 43
	action: tensor([[ 1.9555,  0.5518,  0.3353, -0.0470, -0.6503, -0.7187,  1.2351]],
       dtype=torch.float64)
	q_value: tensor([[-29.4425]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 16, step: 44
	action: tensor([[ 0.3172, -0.4565, -0.6219,  0.2359, -0.5934, -0.3602,  0.3401]],
       dtype=torch.float64)
	q_value: tensor([[-32.2576]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27109384364463474, distance: 0.9769953628644942 entropy 1.0271300077438354
epoch: 16, step: 45
	action: tensor([[ 1.7125,  1.2005, -0.0687, -0.2444,  0.3440, -0.9231,  1.4567]],
       dtype=torch.float64)
	q_value: tensor([[-21.5433]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7908183543963768, distance: 0.5233816365841385 entropy 1.0271300077438354
epoch: 16, step: 46
	action: tensor([[ 1.7116,  1.9801,  1.2312,  0.2552, -0.2957, -2.1754,  1.2596]],
       dtype=torch.float64)
	q_value: tensor([[-34.4528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 16, step: 47
	action: tensor([[-0.0686,  1.1283,  0.9408, -0.2914, -0.3827,  0.0419,  0.4265]],
       dtype=torch.float64)
	q_value: tensor([[-32.2576]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 16, step: 48
	action: tensor([[-0.1928,  0.1524, -0.0363, -0.6693, -0.8120, -0.1591,  0.9442]],
       dtype=torch.float64)
	q_value: tensor([[-32.2576]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11066130733674018, distance: 1.2060005738902293 entropy 1.0271300077438354
epoch: 16, step: 49
	action: tensor([[ 1.6922,  0.0082,  0.8183, -1.0722, -0.6903, -2.0126, -0.0043]],
       dtype=torch.float64)
	q_value: tensor([[-24.2740]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28172197146994926, distance: 0.9698464584784563 entropy 1.0271300077438354
epoch: 16, step: 50
	action: tensor([[ 3.8368,  1.0145,  1.5469, -0.6360, -0.9558, -2.1072,  1.2227]],
       dtype=torch.float64)
	q_value: tensor([[-46.6092]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 16, step: 51
	action: tensor([[ 0.1071,  0.8686,  0.2119, -0.8217, -0.5434,  1.2852, -0.4026]],
       dtype=torch.float64)
	q_value: tensor([[-32.2576]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5547013309685844, distance: 0.763628962642494 entropy 1.0271300077438354
epoch: 16, step: 52
	action: tensor([[ 1.7271, -0.3671,  1.1774,  0.2343,  0.5683, -1.1350,  0.0884]],
       dtype=torch.float64)
	q_value: tensor([[-31.9929]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4359690989151631, distance: 0.8594253155004178 entropy 1.0271300077438354
epoch: 16, step: 53
	action: tensor([[ 2.5192, -0.0991,  1.1119,  0.1973, -0.8749, -1.8245, -0.1331]],
       dtype=torch.float64)
	q_value: tensor([[-42.2642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 16, step: 54
	action: tensor([[ 0.3815, -0.4466,  1.0625,  0.5962, -0.5082,  0.2412,  1.0461]],
       dtype=torch.float64)
	q_value: tensor([[-32.2576]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.658380351435242, distance: 0.6688490492350629 entropy 1.0271300077438354
epoch: 16, step: 55
	action: tensor([[ 1.7616, -0.5402,  0.3212, -0.1539,  1.2546, -1.3148,  0.7051]],
       dtype=torch.float64)
	q_value: tensor([[-32.2639]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3043140659371065, distance: 0.9544722459384608 entropy 1.0271300077438354
epoch: 16, step: 56
	action: tensor([[ 2.6044,  0.9398,  1.8290, -0.6060, -0.8543, -1.5354, -0.0717]],
       dtype=torch.float64)
	q_value: tensor([[-43.5265]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 16, step: 57
	action: tensor([[ 0.5481,  0.6753,  0.3632, -0.2629,  0.0717,  1.0376, -0.0245]],
       dtype=torch.float64)
	q_value: tensor([[-32.2576]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9413580892366329, distance: 0.27711545637217505 entropy 1.0271300077438354
epoch: 16, step: 58
	action: tensor([[ 1.8091e+00, -1.8332e-01,  1.6168e+00,  6.5566e-01,  3.5722e-01,
         -1.5795e+00, -1.4577e-03]], dtype=torch.float64)
	q_value: tensor([[-24.3487]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5189584203257325, distance: 0.793684687858904 entropy 1.0271300077438354
epoch: 16, step: 59
	action: tensor([[ 2.4384,  1.7469,  1.9194, -0.8785, -1.6567, -1.0413,  0.7977]],
       dtype=torch.float64)
	q_value: tensor([[-48.7564]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 16, step: 60
	action: tensor([[ 0.4750,  1.0081,  0.7160,  0.3578, -0.0149, -0.2261,  1.2142]],
       dtype=torch.float64)
	q_value: tensor([[-32.2576]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 16, step: 61
	action: tensor([[ 0.4257,  0.3203, -0.4427,  0.1002, -0.8190,  0.4222,  0.6465]],
       dtype=torch.float64)
	q_value: tensor([[-32.2576]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7587786432710663, distance: 0.5620365654201647 entropy 1.0271300077438354
epoch: 16, step: 62
	action: tensor([[-0.4543,  1.7164,  0.5521,  0.7444,  0.3096, -0.2246, -0.6030]],
       dtype=torch.float64)
	q_value: tensor([[-22.3003]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 16, step: 63
	action: tensor([[ 1.0069,  0.8121,  0.4109,  0.3781,  0.1564, -0.5307, -0.0221]],
       dtype=torch.float64)
	q_value: tensor([[-32.2576]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9339291304139402, distance: 0.29414515861625024 entropy 1.0271300077438354
epoch: 16, step: 64
	action: tensor([[ 1.0700,  1.3961, -0.0741, -0.0974,  0.0519, -0.4895,  0.1877]],
       dtype=torch.float64)
	q_value: tensor([[-25.2094]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 16, step: 65
	action: tensor([[ 1.1440,  0.3060, -0.4548,  0.0369, -0.3798, -0.2177, -0.2213]],
       dtype=torch.float64)
	q_value: tensor([[-32.2576]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.856914971223184, distance: 0.4328663148736555 entropy 1.0271300077438354
epoch: 16, step: 66
	action: tensor([[ 1.1347,  1.5767,  0.5393, -0.3054, -1.2395, -0.6092,  0.7311]],
       dtype=torch.float64)
	q_value: tensor([[-23.0182]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8800335828384903, distance: 0.3963570045731688 entropy 1.0271300077438354
epoch: 16, step: 67
	action: tensor([[ 0.8472,  0.5954, -0.0443,  0.0020, -0.5689,  0.0441,  1.2263]],
       dtype=torch.float64)
	q_value: tensor([[-36.1636]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.09617741128136414 entropy 1.0271300077438354
epoch: 16, step: 68
	action: tensor([[ 0.7069, -0.1346,  0.1348,  0.0902, -0.3937,  0.5074,  0.5335]],
       dtype=torch.float64)
	q_value: tensor([[-32.2576]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7921687948770116, distance: 0.5216894704725065 entropy 1.0271300077438354
epoch: 16, step: 69
	action: tensor([[ 1.4321, -0.0561, -0.0388,  1.0363, -0.4132, -0.2442,  1.0680]],
       dtype=torch.float64)
	q_value: tensor([[-22.8694]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8419781702907052, distance: 0.45489926626229565 entropy 1.0271300077438354
epoch: 16, step: 70
	action: tensor([[ 2.6915,  1.4395,  1.6781,  0.5862, -0.2623, -1.4505,  0.6834]],
       dtype=torch.float64)
	q_value: tensor([[-33.0538]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 16, step: 71
	action: tensor([[-0.1212, -0.4019,  0.9895,  0.1902, -0.4136, -1.5638,  0.2788]],
       dtype=torch.float64)
	q_value: tensor([[-32.2576]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.26408452419296635, distance: 1.2866035651364292 entropy 1.0271300077438354
epoch: 16, step: 72
	action: tensor([[ 0.9733,  0.4127,  0.8114, -1.0122, -1.3950, -1.1564,  0.3261]],
       dtype=torch.float64)
	q_value: tensor([[-35.3038]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5966561353981449, distance: 0.7267656973540219 entropy 1.0271300077438354
epoch: 16, step: 73
	action: tensor([[ 1.4936,  0.9140,  0.0095,  1.0902, -0.7395, -2.0772,  1.4501]],
       dtype=torch.float64)
	q_value: tensor([[-39.6708]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6293819952819497, distance: 0.6966585156190432 entropy 1.0271300077438354
epoch: 16, step: 74
	action: tensor([[ 1.6792,  1.2981,  0.7452,  0.4844, -0.1361, -1.6645,  1.2642]],
       dtype=torch.float64)
	q_value: tensor([[-46.1600]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 16, step: 75
	action: tensor([[ 1.0495, -0.8944,  1.5169, -0.3908,  1.0325,  0.1376,  0.5455]],
       dtype=torch.float64)
	q_value: tensor([[-32.2576]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2318023879345632, distance: 1.2700687024690172 entropy 1.0271300077438354
epoch: 16, step: 76
	action: tensor([[ 2.5936,  1.0922,  0.3476,  0.5066,  0.4968, -0.6978,  1.2293]],
       dtype=torch.float64)
	q_value: tensor([[-39.9282]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 16, step: 77
	action: tensor([[-0.5381, -0.3125,  0.4283,  0.5617,  1.8406,  0.8920, -0.5698]],
       dtype=torch.float64)
	q_value: tensor([[-32.2576]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.01106174211447375, distance: 1.1379974329531282 entropy 1.0271300077438354
epoch: 16, step: 78
	action: tensor([[ 2.7503,  0.7644,  0.5165,  0.5160, -0.5496, -1.0454,  0.5993]],
       dtype=torch.float64)
	q_value: tensor([[-31.4641]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 16, step: 79
	action: tensor([[ 0.5864,  0.9661, -0.9364, -0.0075, -0.1236,  0.2009,  0.3063]],
       dtype=torch.float64)
	q_value: tensor([[-32.2576]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 16, step: 80
	action: tensor([[ 0.3546, -0.5838,  0.0773,  0.4435,  0.4449, -1.1867,  0.3605]],
       dtype=torch.float64)
	q_value: tensor([[-32.2576]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.014450223152539188, distance: 1.13604615264466 entropy 1.0271300077438354
epoch: 16, step: 81
	action: tensor([[ 0.8990, -0.6621,  1.5179, -0.2832, -0.9598, -1.1503,  0.3545]],
       dtype=torch.float64)
	q_value: tensor([[-31.9404]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09952025698425682, distance: 1.1999366291374278 entropy 1.0271300077438354
epoch: 16, step: 82
	action: tensor([[ 2.6271,  2.5053, -0.0981, -0.3961, -0.0743, -1.5883,  2.2000]],
       dtype=torch.float64)
	q_value: tensor([[-42.5313]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 16, step: 83
	action: tensor([[ 0.6120, -1.1696,  0.0015, -0.7863,  0.3297,  0.4109, -0.2760]],
       dtype=torch.float64)
	q_value: tensor([[-32.2576]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8835465902999238, distance: 1.5705250825861374 entropy 1.0271300077438354
epoch: 16, step: 84
	action: tensor([[ 1.0498,  0.7285,  0.8165,  0.5116, -0.5639, -1.6677, -0.0357]],
       dtype=torch.float64)
	q_value: tensor([[-25.0511]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9516943024074646, distance: 0.2515103562563188 entropy 1.0271300077438354
epoch: 16, step: 85
	action: tensor([[ 1.9915, -0.1044,  0.5141, -1.0415, -0.1530, -0.8729,  1.4543]],
       dtype=torch.float64)
	q_value: tensor([[-37.8653]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 16, step: 86
	action: tensor([[ 0.3387,  0.0665,  0.1553,  0.0966, -0.6593, -1.0021, -0.2060]],
       dtype=torch.float64)
	q_value: tensor([[-32.2576]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6089715997227924, distance: 0.7155843664853826 entropy 1.0271300077438354
epoch: 16, step: 87
	action: tensor([[ 1.9981,  0.1726,  1.8029, -1.0343, -1.3228, -0.9806, -0.1360]],
       dtype=torch.float64)
	q_value: tensor([[-24.7553]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 16, step: 88
	action: tensor([[-0.6360,  0.9790,  0.1424,  0.5010, -0.1726, -0.5345, -1.0998]],
       dtype=torch.float64)
	q_value: tensor([[-32.2576]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 16, step: 89
	action: tensor([[-0.6347, -0.0393,  1.0397,  1.7561,  0.4934,  1.0139,  0.7974]],
       dtype=torch.float64)
	q_value: tensor([[-32.2576]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7732394434983993, distance: 0.5449296644653651 entropy 1.0271300077438354
epoch: 16, step: 90
	action: tensor([[ 0.2158, -0.9329,  0.5474,  0.2904, -0.9761, -1.8119,  0.7716]],
       dtype=torch.float64)
	q_value: tensor([[-35.7581]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17519630522157725, distance: 1.2405432270009642 entropy 1.0271300077438354
epoch: 16, step: 91
	action: tensor([[ 1.9661, -1.4393,  1.0534,  0.1611,  0.6820, -0.2543,  0.3649]],
       dtype=torch.float64)
	q_value: tensor([[-41.8454]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 16, step: 92
	action: tensor([[-0.2106,  1.7858,  1.1544,  0.3774,  0.1719,  0.0347, -0.2028]],
       dtype=torch.float64)
	q_value: tensor([[-32.2576]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 16, step: 93
	action: tensor([[ 0.1696, -0.5030, -0.9872, -0.6012,  0.2695, -1.0736,  1.3131]],
       dtype=torch.float64)
	q_value: tensor([[-32.2576]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07779872954530886, distance: 1.098928790194373 entropy 1.0271300077438354
epoch: 16, step: 94
	action: tensor([[ 0.8187,  2.4532, -0.6065, -0.5911, -0.8138, -0.4831,  0.4252]],
       dtype=torch.float64)
	q_value: tensor([[-31.4818]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 16, step: 95
	action: tensor([[ 0.3573,  0.1193,  0.8457, -0.6180,  0.1900,  0.7795, -0.1646]],
       dtype=torch.float64)
	q_value: tensor([[-32.2576]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5566995575441216, distance: 0.7619136876212536 entropy 1.0271300077438354
epoch: 16, step: 96
	action: tensor([[ 1.8378,  0.3525,  1.7124, -0.8159,  0.3268, -1.6683,  0.4610]],
       dtype=torch.float64)
	q_value: tensor([[-24.2833]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9139515348622322, distance: 0.3356821571263597 entropy 1.0271300077438354
epoch: 16, step: 97
	action: tensor([[ 2.9028,  1.8689,  1.4664, -0.6309, -0.9964, -1.6837,  0.6593]],
       dtype=torch.float64)
	q_value: tensor([[-47.1736]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 16, step: 98
	action: tensor([[ 0.8020,  0.1124, -0.3087,  0.8613, -0.2558,  0.3143, -1.1219]],
       dtype=torch.float64)
	q_value: tensor([[-32.2576]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9413824173889712, distance: 0.2770579684227673 entropy 1.0271300077438354
epoch: 16, step: 99
	action: tensor([[ 1.7465,  1.8323,  0.4249,  0.4955, -0.6349,  0.8074,  0.7891]],
       dtype=torch.float64)
	q_value: tensor([[-27.1869]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 16, step: 100
	action: tensor([[ 0.4687,  0.1060, -0.6673,  0.5871, -0.4940,  0.4947, -0.4939]],
       dtype=torch.float64)
	q_value: tensor([[-32.2576]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6698321973655577, distance: 0.6575428398115816 entropy 1.0271300077438354
epoch: 16, step: 101
	action: tensor([[ 3.1644,  0.1367,  0.3562, -0.0739, -0.5179, -1.2396,  0.7790]],
       dtype=torch.float64)
	q_value: tensor([[-20.8291]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 16, step: 102
	action: tensor([[ 0.3355,  0.4181,  0.4748,  0.0244, -1.1095, -1.2864,  0.1214]],
       dtype=torch.float64)
	q_value: tensor([[-32.2576]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8123726032168492, distance: 0.49568390764956993 entropy 1.0271300077438354
epoch: 16, step: 103
	action: tensor([[ 0.3428,  0.8345, -0.6872,  0.4321,  0.2076, -1.9204,  0.5496]],
       dtype=torch.float64)
	q_value: tensor([[-30.3261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9002392962546025, distance: 0.3614401928845349 entropy 1.0271300077438354
epoch: 16, step: 104
	action: tensor([[ 2.1940,  1.2383,  1.1838, -1.3833, -1.6070, -0.7220,  0.8533]],
       dtype=torch.float64)
	q_value: tensor([[-32.9863]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 16, step: 105
	action: tensor([[ 0.7535, -0.8322, -0.4580,  1.1085,  0.0701, -0.2945,  0.1116]],
       dtype=torch.float64)
	q_value: tensor([[-32.2576]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5112589284373524, distance: 0.8000112820449751 entropy 1.0271300077438354
epoch: 16, step: 106
	action: tensor([[ 1.1731,  1.4145,  0.4466, -0.5246, -0.3452, -3.1131,  0.8351]],
       dtype=torch.float64)
	q_value: tensor([[-30.5698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 16, step: 107
	action: tensor([[ 1.9963,  0.4512, -1.2692,  1.2183, -0.2187, -2.0891, -0.2196]],
       dtype=torch.float64)
	q_value: tensor([[-32.2576]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7640016242631581, distance: 0.5559185939516754 entropy 1.0271300077438354
epoch: 16, step: 108
	action: tensor([[ 3.1745,  1.5403,  0.1533, -0.3204, -1.5149, -0.5435,  1.7999]],
       dtype=torch.float64)
	q_value: tensor([[-44.8862]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 16, step: 109
	action: tensor([[-0.9646, -0.1567, -1.0926,  1.6490, -1.5561, -0.2353, -0.2143]],
       dtype=torch.float64)
	q_value: tensor([[-32.2576]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7270057231558644, distance: 1.5038467502609225 entropy 1.0271300077438354
epoch: 16, step: 110
	action: tensor([[ 2.0336, -0.1096,  0.9230, -0.1119,  0.4450,  0.1586,  0.5990]],
       dtype=torch.float64)
	q_value: tensor([[-31.9235]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22721581558534898, distance: 1.0059717987379404 entropy 1.0271300077438354
epoch: 16, step: 111
	action: tensor([[ 2.8784,  1.3102,  1.6760, -0.1223, -0.1865, -1.9739,  0.8084]],
       dtype=torch.float64)
	q_value: tensor([[-33.9432]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 16, step: 112
	action: tensor([[ 0.5762, -0.9611,  0.0218,  1.0851, -0.0632, -0.5054,  0.2488]],
       dtype=torch.float64)
	q_value: tensor([[-32.2576]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3831674294689441, distance: 0.8987529773226337 entropy 1.0271300077438354
epoch: 16, step: 113
	action: tensor([[ 1.8952,  1.9611, -0.6007, -0.0997, -0.1133, -1.2844,  0.3691]],
       dtype=torch.float64)
	q_value: tensor([[-32.4327]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 16, step: 114
	action: tensor([[-0.3439,  1.8331,  0.3813,  1.1239, -0.1876, -0.4572, -0.2402]],
       dtype=torch.float64)
	q_value: tensor([[-32.2576]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 16, step: 115
	action: tensor([[-4.5494e-01,  6.3150e-01, -2.7960e-01, -1.4869e-01,  1.8065e-04,
         -1.3136e-01,  2.7031e-01]], dtype=torch.float64)
	q_value: tensor([[-32.2576]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1180608260802084, distance: 1.0746721888132371 entropy 1.0271300077438354
epoch: 16, step: 116
	action: tensor([[-0.7101, -0.7326,  0.4135, -0.1997, -0.5010, -0.6611,  0.6081]],
       dtype=torch.float64)
	q_value: tensor([[-16.5679]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0954136927834366, distance: 1.65650054095001 entropy 1.0271300077438354
epoch: 16, step: 117
	action: tensor([[ 1.2285,  0.7064,  1.0833, -1.1580,  0.1408,  0.2025,  1.3437]],
       dtype=torch.float64)
	q_value: tensor([[-28.1493]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9054788534177052, distance: 0.35182053432729116 entropy 1.0271300077438354
epoch: 16, step: 118
	action: tensor([[ 2.5575,  1.1121,  2.0595, -0.0415,  0.5449, -0.9294,  1.3974]],
       dtype=torch.float64)
	q_value: tensor([[-35.8275]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 16, step: 119
	action: tensor([[-0.6730,  0.3792, -0.6685, -1.0175,  1.0842, -0.4767,  0.7794]],
       dtype=torch.float64)
	q_value: tensor([[-32.2576]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2284161870258763, distance: 1.2683218039621416 entropy 1.0271300077438354
epoch: 16, step: 120
	action: tensor([[ 1.4945, -0.1196, -0.1036, -0.0249, -0.6603,  0.2885,  1.1799]],
       dtype=torch.float64)
	q_value: tensor([[-26.6290]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5197216219031969, distance: 0.7930548235358169 entropy 1.0271300077438354
epoch: 16, step: 121
	action: tensor([[ 2.4597,  0.7380,  0.4771, -0.9301, -0.8824, -2.4582,  1.0121]],
       dtype=torch.float64)
	q_value: tensor([[-31.5974]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 16, step: 122
	action: tensor([[ 0.8519, -0.1034,  0.1324,  0.3447,  0.8767,  0.4570, -0.0979]],
       dtype=torch.float64)
	q_value: tensor([[-32.2576]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8927622404299921, distance: 0.37474043733868256 entropy 1.0271300077438354
epoch: 16, step: 123
	action: tensor([[ 1.3267,  1.3875, -0.1325, -0.0756, -1.1020, -0.6566,  1.0685]],
       dtype=torch.float64)
	q_value: tensor([[-24.4430]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 16, step: 124
	action: tensor([[ 1.1207, -0.8805,  0.9438, -0.0908,  0.0398,  0.2209,  0.1803]],
       dtype=torch.float64)
	q_value: tensor([[-32.2576]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1863223996197778, distance: 1.2464017743722022 entropy 1.0271300077438354
epoch: 16, step: 125
	action: tensor([[ 1.0400,  0.4797,  0.8937,  0.7387, -0.3869, -1.2384,  0.0442]],
       dtype=torch.float64)
	q_value: tensor([[-32.2544]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9201187234604296, distance: 0.3234291739149991 entropy 1.0271300077438354
epoch: 16, step: 126
	action: tensor([[ 1.6503,  1.6812,  0.1252,  0.2820, -1.1483, -0.4268, -0.4042]],
       dtype=torch.float64)
	q_value: tensor([[-34.9846]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 16, step: 127
	action: tensor([[ 1.1392,  0.2005, -0.4679, -0.0786, -0.2479,  0.7820,  0.1942]],
       dtype=torch.float64)
	q_value: tensor([[-32.2576]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9381762945620056, distance: 0.2845340259500318 entropy 1.0271300077438354
LOSS epoch 16 actor 361.3023448126223 critic 660.4601432077279 
epoch: 17, step: 0
	action: tensor([[ 1.1948, -0.5507, -0.5807,  0.6169, -0.7269,  0.0160,  0.2713]],
       dtype=torch.float64)
	q_value: tensor([[-28.0948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6159594966217637, distance: 0.709161595445745 entropy 1.0271300077438354
epoch: 17, step: 1
	action: tensor([[ 0.5580,  0.5838,  1.3505,  0.4100,  0.3898, -0.5428, -0.8475]],
       dtype=torch.float64)
	q_value: tensor([[-32.4504]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9693427237219334, distance: 0.20036574572656543 entropy 1.0271300077438354
epoch: 17, step: 2
	action: tensor([[ 0.8109,  0.9956, -1.2976, -0.3024,  0.2716, -0.4086,  0.1682]],
       dtype=torch.float64)
	q_value: tensor([[-36.6481]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9320548874371787, distance: 0.298288015296157 entropy 1.0271300077438354
epoch: 17, step: 3
	action: tensor([[ 0.6771, -0.7266, -0.7264,  0.8475, -0.5935,  0.4996, -1.1389]],
       dtype=torch.float64)
	q_value: tensor([[-28.5564]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.409874078381271, distance: 0.8790812946674236 entropy 1.0271300077438354
epoch: 17, step: 4
	action: tensor([[ 0.5291,  0.5098,  0.3499,  0.1111, -0.5826, -0.3532,  0.1789]],
       dtype=torch.float64)
	q_value: tensor([[-34.3343]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9334405854471729, distance: 0.2952306481124327 entropy 1.0271300077438354
epoch: 17, step: 5
	action: tensor([[ 0.9610, -0.7304,  0.8429, -0.6655,  0.2657, -0.2640, -0.2313]],
       dtype=torch.float64)
	q_value: tensor([[-24.9132]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2978984388531998, distance: 1.3036981471499924 entropy 1.0271300077438354
epoch: 17, step: 6
	action: tensor([[ 0.4597,  0.6184,  0.8465, -0.1995, -0.4097, -0.8579,  0.2962]],
       dtype=torch.float64)
	q_value: tensor([[-35.0794]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.739670573771041, distance: 0.583872907497996 entropy 1.0271300077438354
epoch: 17, step: 7
	action: tensor([[ 0.1062, -0.1886, -0.7767,  0.4873, -0.6895,  0.6163,  0.0812]],
       dtype=torch.float64)
	q_value: tensor([[-31.5055]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1280579778355363, distance: 1.0685638986677921 entropy 1.0271300077438354
epoch: 17, step: 8
	action: tensor([[ 0.8933, -0.9096,  0.2704,  0.0625, -0.0961, -0.7146,  0.1227]],
       dtype=torch.float64)
	q_value: tensor([[-22.5082]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3875841540768725, distance: 1.3479890336891858 entropy 1.0271300077438354
epoch: 17, step: 9
	action: tensor([[ 1.1788, -0.0494,  0.1178,  0.7560, -0.1962,  0.0273,  0.8528]],
       dtype=torch.float64)
	q_value: tensor([[-36.0924]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8946977327990412, distance: 0.37134326722002825 entropy 1.0271300077438354
epoch: 17, step: 10
	action: tensor([[ 0.9861,  0.1687, -1.2760,  0.4883, -1.1015, -0.3005, -0.3533]],
       dtype=torch.float64)
	q_value: tensor([[-32.8341]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9370708417417033, distance: 0.2870665927445598 entropy 1.0271300077438354
epoch: 17, step: 11
	action: tensor([[ 1.1117,  0.4195, -0.0406, -0.7765, -0.0466, -0.4516,  1.2196]],
       dtype=torch.float64)
	q_value: tensor([[-30.9704]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5779981678674923, distance: 0.7433851161159876 entropy 1.0271300077438354
epoch: 17, step: 12
	action: tensor([[ 0.3952, -0.3761,  0.8386,  0.6717, -0.8606,  0.1435,  0.9994]],
       dtype=torch.float64)
	q_value: tensor([[-33.4731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7249743704320752, distance: 0.600127152973209 entropy 1.0271300077438354
epoch: 17, step: 13
	action: tensor([[-0.0634,  0.6418,  0.8631, -1.0800,  0.5875,  1.0123, -0.0683]],
       dtype=torch.float64)
	q_value: tensor([[-36.2262]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27630313948987373, distance: 0.9734979416083835 entropy 1.0271300077438354
epoch: 17, step: 14
	action: tensor([[ 0.4061,  0.5136, -0.5827,  0.7162,  0.4568,  0.4937, -0.9529]],
       dtype=torch.float64)
	q_value: tensor([[-29.4315]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6662743706347503, distance: 0.6610761263779905 entropy 1.0271300077438354
epoch: 17, step: 15
	action: tensor([[-0.1096, -0.0191,  0.6250,  1.4631, -0.0836, -0.8485,  0.4267]],
       dtype=torch.float64)
	q_value: tensor([[-26.5631]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8044823305901927, distance: 0.5059990473037025 entropy 1.0271300077438354
epoch: 17, step: 16
	action: tensor([[-0.0732, -0.5302,  0.5018, -0.2665, -0.1728, -0.9771,  0.4873]],
       dtype=torch.float64)
	q_value: tensor([[-36.8261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5917592926199657, distance: 1.4437612795926598 entropy 1.0271300077438354
epoch: 17, step: 17
	action: tensor([[-0.6297, -0.2321, -0.2614, -0.3619, -0.4479,  0.5110,  0.0601]],
       dtype=torch.float64)
	q_value: tensor([[-31.3206]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7726473214902518, distance: 1.5235891198961429 entropy 1.0271300077438354
epoch: 17, step: 18
	action: tensor([[ 0.5475,  0.3041, -0.1180, -0.8428, -0.4855,  0.2909,  0.5168]],
       dtype=torch.float64)
	q_value: tensor([[-21.4095]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5819317954439269, distance: 0.7399123272676099 entropy 1.0271300077438354
epoch: 17, step: 19
	action: tensor([[ 0.8169,  0.0676, -0.9463,  0.5279, -1.2073, -0.0725,  0.4038]],
       dtype=torch.float64)
	q_value: tensor([[-27.2060]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8611032499054132, distance: 0.42648399273076004 entropy 1.0271300077438354
epoch: 17, step: 20
	action: tensor([[ 0.0400,  0.8241, -0.0550, -0.4586,  0.4061,  0.5626,  1.0326]],
       dtype=torch.float64)
	q_value: tensor([[-29.1524]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7036389655109188, distance: 0.6229701595245438 entropy 1.0271300077438354
epoch: 17, step: 21
	action: tensor([[ 0.6363, -1.4087, -0.1189, -0.3546, -0.7634, -0.4190, -1.1830]],
       dtype=torch.float64)
	q_value: tensor([[-25.7202]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8658563661223357, distance: 1.5631325170776782 entropy 1.0271300077438354
epoch: 17, step: 22
	action: tensor([[ 1.1141,  0.7560,  0.2250, -0.1985, -0.9277, -0.3440,  0.5906]],
       dtype=torch.float64)
	q_value: tensor([[-36.8458]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9533945559676215, distance: 0.24704440258645957 entropy 1.0271300077438354
epoch: 17, step: 23
	action: tensor([[ 1.1517,  1.1097,  0.0958, -1.6147, -0.0222, -0.3651,  0.2211]],
       dtype=torch.float64)
	q_value: tensor([[-33.2937]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7345257838703552, distance: 0.5896141083669234 entropy 1.0271300077438354
epoch: 17, step: 24
	action: tensor([[ 0.1570, -0.4640, -0.3122,  0.0894, -0.5598,  0.2107,  0.2154]],
       dtype=torch.float64)
	q_value: tensor([[-38.9458]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07894750196823153, distance: 1.0982441172717567 entropy 1.0271300077438354
epoch: 17, step: 25
	action: tensor([[ 0.4445,  0.3418,  0.2649,  0.0727, -0.5213,  0.0986,  0.2672]],
       dtype=torch.float64)
	q_value: tensor([[-21.1426]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8377685230061063, distance: 0.46091862148916607 entropy 1.0271300077438354
epoch: 17, step: 26
	action: tensor([[-0.0155,  0.0977,  0.6528,  1.0314, -0.1135,  0.5467,  1.1537]],
       dtype=torch.float64)
	q_value: tensor([[-22.6173]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9243265791620618, distance: 0.3147953993792227 entropy 1.0271300077438354
epoch: 17, step: 27
	action: tensor([[-0.7099,  1.2378,  0.6464,  0.1026,  0.6673,  1.4543, -0.6981]],
       dtype=torch.float64)
	q_value: tensor([[-31.7844]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7196914063493245, distance: 0.605863653627557 entropy 1.0271300077438354
epoch: 17, step: 28
	action: tensor([[ 0.2195,  0.6688,  0.1713,  0.5331, -0.1261, -0.7610,  0.3568]],
       dtype=torch.float64)
	q_value: tensor([[-31.8924]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 17, step: 29
	action: tensor([[ 0.8419,  0.5136,  0.5126,  0.2998, -0.1126,  0.3414,  1.0514]],
       dtype=torch.float64)
	q_value: tensor([[-33.2423]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9678371501977222, distance: 0.2052267438237607 entropy 1.0271300077438354
epoch: 17, step: 30
	action: tensor([[-0.4736,  0.6776,  0.1463, -0.3191, -1.4172,  0.4167,  0.9310]],
       dtype=torch.float64)
	q_value: tensor([[-29.1325]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09779585739911778, distance: 1.1989953176293509 entropy 1.0271300077438354
epoch: 17, step: 31
	action: tensor([[ 0.1300,  0.7095, -0.5939,  0.3842, -0.7292, -0.2186, -0.4745]],
       dtype=torch.float64)
	q_value: tensor([[-32.6984]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6459398007072965, distance: 0.6809186791192245 entropy 1.0271300077438354
epoch: 17, step: 32
	action: tensor([[ 0.8009, -0.1323,  0.5694,  0.3924, -0.1269,  1.7172, -0.2622]],
       dtype=torch.float64)
	q_value: tensor([[-22.8631]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9403395655731432, distance: 0.2795116403831861 entropy 1.0271300077438354
epoch: 17, step: 33
	action: tensor([[-0.4620, -0.5549,  0.7711, -0.3086,  0.1132,  0.3540,  0.6958]],
       dtype=torch.float64)
	q_value: tensor([[-37.4957]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7783653607813068, distance: 1.5260444667980164 entropy 1.0271300077438354
epoch: 17, step: 34
	action: tensor([[ 0.5638,  0.5323,  0.0998,  1.3383, -0.5366, -0.3428,  0.2211]],
       dtype=torch.float64)
	q_value: tensor([[-29.4224]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 17, step: 35
	action: tensor([[ 0.2388,  0.6345,  0.2037, -0.0661, -0.1965, -0.3323,  0.2416]],
       dtype=torch.float64)
	q_value: tensor([[-33.2423]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7286620530551573, distance: 0.5960901707614843 entropy 1.0271300077438354
epoch: 17, step: 36
	action: tensor([[ 0.0493, -0.2799,  0.1690,  1.2374, -1.3299, -0.0393,  0.4406]],
       dtype=torch.float64)
	q_value: tensor([[-21.0493]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7355564181824119, distance: 0.5884684839936567 entropy 1.0271300077438354
epoch: 17, step: 37
	action: tensor([[-1.2613, -0.7914,  0.7435,  0.9112,  0.7890,  0.0803, -0.1642]],
       dtype=torch.float64)
	q_value: tensor([[-33.1124]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8641808880025583, distance: 1.5624305384620618 entropy 1.0271300077438354
epoch: 17, step: 38
	action: tensor([[ 0.4043,  0.0387,  0.2109, -0.0642, -0.1174, -0.0489, -0.2122]],
       dtype=torch.float64)
	q_value: tensor([[-37.3070]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5715688917327242, distance: 0.7490265154157384 entropy 1.0271300077438354
epoch: 17, step: 39
	action: tensor([[-0.0979,  0.4233,  1.6557, -1.0872,  0.2855,  0.3328, -0.2683]],
       dtype=torch.float64)
	q_value: tensor([[-20.9444]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16742156929947516, distance: 1.2364328920526066 entropy 1.0271300077438354
epoch: 17, step: 40
	action: tensor([[ 0.4740,  0.1765,  1.5991, -0.5492, -0.0413, -0.2007,  1.4603]],
       dtype=torch.float64)
	q_value: tensor([[-33.5837]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4979349772549164, distance: 0.8108428221333155 entropy 1.0271300077438354
epoch: 17, step: 41
	action: tensor([[ 0.4661,  0.3772,  0.8806,  0.5466, -0.9051,  0.2367,  0.4877]],
       dtype=torch.float64)
	q_value: tensor([[-41.2491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9910061928485976, distance: 0.10852467121586908 entropy 1.0271300077438354
epoch: 17, step: 42
	action: tensor([[-0.1804,  0.2253,  0.9091, -0.7778, -1.1501,  0.5888, -0.2897]],
       dtype=torch.float64)
	q_value: tensor([[-31.6893]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21769496929653642, distance: 1.2627749240969421 entropy 1.0271300077438354
epoch: 17, step: 43
	action: tensor([[ 0.0563,  1.8817, -0.1910,  0.7871, -0.2501, -1.5884,  0.1242]],
       dtype=torch.float64)
	q_value: tensor([[-36.7502]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 17, step: 44
	action: tensor([[ 0.3643, -0.8852, -0.1869,  0.2865, -1.2908,  0.4177,  0.7746]],
       dtype=torch.float64)
	q_value: tensor([[-33.2423]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14451826027653314, distance: 1.224244203145063 entropy 1.0271300077438354
epoch: 17, step: 45
	action: tensor([[ 1.4005,  0.3549, -0.6641, -0.4857,  0.9277, -0.8772, -0.3847]],
       dtype=torch.float64)
	q_value: tensor([[-32.7098]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.495814274091698, distance: 0.8125535018754116 entropy 1.0271300077438354
epoch: 17, step: 46
	action: tensor([[ 0.2112,  0.0642,  0.1030, -0.4029,  0.0956,  0.2365,  1.3419]],
       dtype=torch.float64)
	q_value: tensor([[-37.3117]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3508719270379209, distance: 0.9219808253957984 entropy 1.0271300077438354
epoch: 17, step: 47
	action: tensor([[ 0.3516,  0.1789,  0.2867,  0.4992,  0.2914, -1.1318,  0.1462]],
       dtype=torch.float64)
	q_value: tensor([[-27.9454]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6744625029494407, distance: 0.6529158375742783 entropy 1.0271300077438354
epoch: 17, step: 48
	action: tensor([[ 0.6204,  1.2310, -0.2866, -0.6258, -0.0218,  0.6010,  1.1212]],
       dtype=torch.float64)
	q_value: tensor([[-31.6605]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9124213245686215, distance: 0.3386537425869268 entropy 1.0271300077438354
epoch: 17, step: 49
	action: tensor([[ 0.2868,  0.8613,  1.8798,  0.3404, -0.0155, -0.7225, -0.5646]],
       dtype=torch.float64)
	q_value: tensor([[-30.3909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9171430369413923, distance: 0.32939818271058285 entropy 1.0271300077438354
epoch: 17, step: 50
	action: tensor([[ 0.5719,  0.4425, -0.0069,  0.1300, -0.0597, -0.2823, -0.2760]],
       dtype=torch.float64)
	q_value: tensor([[-40.8112]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.973385742731561, distance: 0.1866869105164332 entropy 1.0271300077438354
epoch: 17, step: 51
	action: tensor([[ 0.3720, -0.9493, -0.5243, -0.6205, -1.0357,  0.3337,  0.7580]],
       dtype=torch.float64)
	q_value: tensor([[-21.9786]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6240773341526942, distance: 1.4583442241431643 entropy 1.0271300077438354
epoch: 17, step: 52
	action: tensor([[ 0.6764,  0.6793,  0.7792,  0.4523,  0.5408, -0.1488, -0.0301]],
       dtype=torch.float64)
	q_value: tensor([[-31.7240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9148302782789366, distance: 0.33396373338977897 entropy 1.0271300077438354
epoch: 17, step: 53
	action: tensor([[ 0.2838, -0.0938,  0.4935,  0.2473, -0.0824, -0.3787, -1.3311]],
       dtype=torch.float64)
	q_value: tensor([[-28.4437]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6840417606980289, distance: 0.6432377672514727 entropy 1.0271300077438354
epoch: 17, step: 54
	action: tensor([[ 0.8084, -0.8771,  0.1108,  0.6637, -0.0938,  0.3220, -0.4081]],
       dtype=torch.float64)
	q_value: tensor([[-30.7329]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4527688742577669, distance: 0.8465294842787568 entropy 1.0271300077438354
epoch: 17, step: 55
	action: tensor([[ 1.2830,  0.5468, -0.3796,  0.8744, -0.0704, -1.6807,  0.2019]],
       dtype=torch.float64)
	q_value: tensor([[-32.7046]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7763535521286669, distance: 0.5411749624062994 entropy 1.0271300077438354
epoch: 17, step: 56
	action: tensor([[ 0.6423,  0.8939,  0.3398,  0.1860,  0.1634, -0.9855,  0.9697]],
       dtype=torch.float64)
	q_value: tensor([[-43.5783]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9900524906199673, distance: 0.11413369354398273 entropy 1.0271300077438354
epoch: 17, step: 57
	action: tensor([[ 1.0388,  0.2389,  0.3871,  0.1498, -0.2441, -1.0937,  0.4171]],
       dtype=torch.float64)
	q_value: tensor([[-31.9071]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6976813910371901, distance: 0.6292006079565046 entropy 1.0271300077438354
epoch: 17, step: 58
	action: tensor([[ 1.2283, -0.5921,  0.2599,  0.6218, -0.2769, -1.2792, -0.9189]],
       dtype=torch.float64)
	q_value: tensor([[-34.9359]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.004057330102983481, distance: 1.1420204032518317 entropy 1.0271300077438354
epoch: 17, step: 59
	action: tensor([[ 0.8544,  0.7765, -0.2488, -0.5893, -0.2870,  0.5132,  0.0983]],
       dtype=torch.float64)
	q_value: tensor([[-45.9496]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9564521254567303, distance: 0.23880321798460258 entropy 1.0271300077438354
epoch: 17, step: 60
	action: tensor([[ 0.3121,  0.1599,  0.1152, -0.8887,  0.4460,  0.5125,  0.6781]],
       dtype=torch.float64)
	q_value: tensor([[-28.0366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5471979409761311, distance: 0.7700357525138856 entropy 1.0271300077438354
epoch: 17, step: 61
	action: tensor([[ 0.5308,  0.7198,  0.0138,  0.1874, -1.4997, -0.5351,  0.0184]],
       dtype=torch.float64)
	q_value: tensor([[-25.7298]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.964864498543815, distance: 0.21450122234189664 entropy 1.0271300077438354
epoch: 17, step: 62
	action: tensor([[ 0.2090,  1.2128,  0.1493, -0.4190, -1.6207, -0.1532,  0.0626]],
       dtype=torch.float64)
	q_value: tensor([[-33.1293]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9338533440199658, distance: 0.29431380943111934 entropy 1.0271300077438354
epoch: 17, step: 63
	action: tensor([[ 0.5334,  0.3651, -0.2884, -1.2364, -0.7623,  0.4189,  0.4591]],
       dtype=torch.float64)
	q_value: tensor([[-37.5014]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.48624121168142675, distance: 0.8202312760458824 entropy 1.0271300077438354
epoch: 17, step: 64
	action: tensor([[-0.2022, -0.0543, -0.6683,  0.0949,  1.1908,  0.0613, -0.1706]],
       dtype=torch.float64)
	q_value: tensor([[-32.2343]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5790813206961187, distance: 0.7424304792270788 entropy 1.0271300077438354
epoch: 17, step: 65
	action: tensor([[-0.2395,  0.3337, -0.9315, -0.0398,  0.0689, -0.9508,  0.4887]],
       dtype=torch.float64)
	q_value: tensor([[-25.4276]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7794562707062078, distance: 0.537407904855697 entropy 1.0271300077438354
epoch: 17, step: 66
	action: tensor([[ 1.2537,  0.2937, -0.3551, -0.3321, -0.8986, -1.7497,  0.8698]],
       dtype=torch.float64)
	q_value: tensor([[-24.9847]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.027573342491251096, distance: 1.1284572761989364 entropy 1.0271300077438354
epoch: 17, step: 67
	action: tensor([[ 0.8515,  0.2908, -0.3804,  0.8811, -0.3154,  0.3177,  0.5998]],
       dtype=torch.float64)
	q_value: tensor([[-42.8596]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9350190400752424, distance: 0.2917089503716658 entropy 1.0271300077438354
epoch: 17, step: 68
	action: tensor([[ 0.5957,  0.5717,  0.3583, -0.6686, -0.0107, -1.3298,  1.2305]],
       dtype=torch.float64)
	q_value: tensor([[-27.1022]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7161324157065585, distance: 0.6096977526912495 entropy 1.0271300077438354
epoch: 17, step: 69
	action: tensor([[-0.0525,  1.3855,  0.6303,  0.3347, -1.4430, -1.9841,  1.2747]],
       dtype=torch.float64)
	q_value: tensor([[-35.7829]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8623354152874823, distance: 0.4245880900321293 entropy 1.0271300077438354
epoch: 17, step: 70
	action: tensor([[-0.0283,  0.5163,  0.0034, -0.1132, -0.1855, -1.0650, -0.0237]],
       dtype=torch.float64)
	q_value: tensor([[-46.5106]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8390964973444197, distance: 0.45902827981649835 entropy 1.0271300077438354
epoch: 17, step: 71
	action: tensor([[-1.0875, -1.1799,  0.1661,  0.6599, -0.0447, -0.5688, -0.1185]],
       dtype=torch.float64)
	q_value: tensor([[-24.4216]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8283861121024056, distance: 1.547357449840293 entropy 1.0271300077438354
epoch: 17, step: 72
	action: tensor([[-0.2759, -0.0423, -1.2104,  0.1049, -0.3770,  0.8540,  0.9340]],
       dtype=torch.float64)
	q_value: tensor([[-36.1680]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3425620685118401, distance: 0.9278634596084574 entropy 1.0271300077438354
epoch: 17, step: 73
	action: tensor([[ 0.4772,  0.2132, -0.7919, -0.0712,  0.8195, -1.1940,  0.4425]],
       dtype=torch.float64)
	q_value: tensor([[-27.2175]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6773784142096919, distance: 0.6499851043246785 entropy 1.0271300077438354
epoch: 17, step: 74
	action: tensor([[ 0.3870,  0.6614,  0.0995,  0.3593, -1.4769,  0.1820, -0.9936]],
       dtype=torch.float64)
	q_value: tensor([[-32.9221]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9433841338225225, distance: 0.2722862879630328 entropy 1.0271300077438354
epoch: 17, step: 75
	action: tensor([[ 0.3534,  0.0597, -1.1742,  0.1282, -0.1456, -1.0854,  0.2454]],
       dtype=torch.float64)
	q_value: tensor([[-35.2829]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7738165307275405, distance: 0.54423582183595 entropy 1.0271300077438354
epoch: 17, step: 76
	action: tensor([[-0.6833,  0.3322,  0.6538, -0.4510, -0.2259, -0.3081,  0.6134]],
       dtype=torch.float64)
	q_value: tensor([[-28.9995]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0301391443380421, distance: 1.1614610185585479 entropy 1.0271300077438354
epoch: 17, step: 77
	action: tensor([[-1.1015, -0.6834, -0.7142,  0.4206, -0.2772,  0.4441, -1.1156]],
       dtype=torch.float64)
	q_value: tensor([[-25.5876]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8841187663852845, distance: 1.5707636083184353 entropy 1.0271300077438354
epoch: 17, step: 78
	action: tensor([[ 0.0490,  0.3661,  0.5949,  0.9690, -0.8403,  0.2353, -0.0028]],
       dtype=torch.float64)
	q_value: tensor([[-27.8142]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9355961386001063, distance: 0.29041072206452395 entropy 1.0271300077438354
epoch: 17, step: 79
	action: tensor([[-0.6342,  1.0249,  0.1013, -0.1914,  1.4406, -0.9386, -0.0870]],
       dtype=torch.float64)
	q_value: tensor([[-27.8592]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3094125975197338, distance: 0.9509682539174404 entropy 1.0271300077438354
epoch: 17, step: 80
	action: tensor([[ 1.4700, -0.0180,  1.2659, -1.3989, -1.8750, -0.8911,  0.5977]],
       dtype=torch.float64)
	q_value: tensor([[-29.8477]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24892504088277678, distance: 0.9917411523500888 entropy 1.0271300077438354
epoch: 17, step: 81
	action: tensor([[ 0.3453,  0.5136,  0.1884, -0.2698, -1.4307, -1.0207, -0.2659]],
       dtype=torch.float64)
	q_value: tensor([[-59.1758]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8756616561513931, distance: 0.4035145809000411 entropy 1.0271300077438354
epoch: 17, step: 82
	action: tensor([[ 0.1204, -1.0198,  0.8922,  0.8100, -0.4796, -0.4685, -1.3061]],
       dtype=torch.float64)
	q_value: tensor([[-35.1206]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.02157392098452182, distance: 1.1319329526458755 entropy 1.0271300077438354
epoch: 17, step: 83
	action: tensor([[ 0.6718,  0.2738,  0.1583, -0.8760, -0.2333,  0.3393,  0.5759]],
       dtype=torch.float64)
	q_value: tensor([[-41.5902]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5859237731478769, distance: 0.7363712802271013 entropy 1.0271300077438354
epoch: 17, step: 84
	action: tensor([[ 1.1142,  1.1203,  0.6056,  0.7596, -0.1211, -0.4739,  0.0640]],
       dtype=torch.float64)
	q_value: tensor([[-27.9163]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 17, step: 85
	action: tensor([[ 0.8356,  0.1697, -0.2202,  1.0486, -0.7369,  0.1711,  0.1556]],
       dtype=torch.float64)
	q_value: tensor([[-33.2423]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9334494814576784, distance: 0.29521091790050247 entropy 1.0271300077438354
epoch: 17, step: 86
	action: tensor([[ 1.3578, -0.1268,  0.0991, -1.0717,  0.8717, -0.7320, -1.0588]],
       dtype=torch.float64)
	q_value: tensor([[-28.7740]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03204034695793334, distance: 1.1258624141826026 entropy 1.0271300077438354
epoch: 17, step: 87
	action: tensor([[ 1.5491, -0.3150,  0.8019,  0.4505,  0.1465, -0.1839, -0.8040]],
       dtype=torch.float64)
	q_value: tensor([[-39.6670]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2490278228129328, distance: 0.9916732919080987 entropy 1.0271300077438354
epoch: 17, step: 88
	action: tensor([[ 0.9105,  1.3072,  0.7251, -1.1596,  1.0292, -1.0752, -0.4399]],
       dtype=torch.float64)
	q_value: tensor([[-42.2632]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7045398748466096, distance: 0.6220225538248878 entropy 1.0271300077438354
epoch: 17, step: 89
	action: tensor([[ 0.8420,  1.1880, -0.1332, -0.6228, -1.0438, -0.8242,  0.3726]],
       dtype=torch.float64)
	q_value: tensor([[-39.5454]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9635265703452403, distance: 0.2185470731146483 entropy 1.0271300077438354
epoch: 17, step: 90
	action: tensor([[ 0.4582, -0.1831, -1.1939,  0.6240,  0.2638, -0.6358,  0.4651]],
       dtype=torch.float64)
	q_value: tensor([[-35.3763]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44927351250574776, distance: 0.8492287248192257 entropy 1.0271300077438354
epoch: 17, step: 91
	action: tensor([[-0.5352,  0.6428,  0.3785, -0.0733, -0.3736, -1.0964,  0.4202]],
       dtype=torch.float64)
	q_value: tensor([[-30.5387]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.037970221552315, distance: 1.1658673506882662 entropy 1.0271300077438354
epoch: 17, step: 92
	action: tensor([[ 0.6470, -0.1868,  1.2877, -0.5855,  0.3543,  0.7490,  0.5832]],
       dtype=torch.float64)
	q_value: tensor([[-26.9873]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4365709169652279, distance: 0.8589666919504021 entropy 1.0271300077438354
epoch: 17, step: 93
	action: tensor([[ 1.0796, -0.4340, -1.6756, -0.4224, -1.2786,  0.5661,  0.4042]],
       dtype=torch.float64)
	q_value: tensor([[-34.9726]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14748195776935213, distance: 1.0565948334496427 entropy 1.0271300077438354
epoch: 17, step: 94
	action: tensor([[ 0.9310,  1.1464,  0.7426,  0.1567,  0.1689, -1.0729,  1.3466]],
       dtype=torch.float64)
	q_value: tensor([[-37.1728]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8969355087922217, distance: 0.36737637546652196 entropy 1.0271300077438354
epoch: 17, step: 95
	action: tensor([[ 0.0643, -0.0628,  1.0602, -1.2891, -0.5389, -2.0292,  0.1590]],
       dtype=torch.float64)
	q_value: tensor([[-38.8739]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08139138561451409, distance: 1.096786131108538 entropy 1.0271300077438354
epoch: 17, step: 96
	action: tensor([[ 0.7151,  0.6558,  1.3564, -0.0118, -0.4736,  0.6859,  0.5699]],
       dtype=torch.float64)
	q_value: tensor([[-46.3433]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9832011760915499, distance: 0.14831877576386998 entropy 1.0271300077438354
epoch: 17, step: 97
	action: tensor([[ 2.3686, -0.6878,  0.7628, -0.7821, -0.8876, -0.5669,  1.2203]],
       dtype=torch.float64)
	q_value: tensor([[-36.7977]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 17, step: 98
	action: tensor([[-0.5184, -1.0342, -0.2564,  0.2537, -0.3874,  0.1430, -0.3995]],
       dtype=torch.float64)
	q_value: tensor([[-33.2423]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9675217924686625, distance: 1.605153126185271 entropy 1.0271300077438354
epoch: 17, step: 99
	action: tensor([[ 0.5576,  1.1930,  0.4210, -0.7999,  0.0174, -0.8157,  0.3431]],
       dtype=torch.float64)
	q_value: tensor([[-25.8872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7965372284878907, distance: 0.5161776205372597 entropy 1.0271300077438354
epoch: 17, step: 100
	action: tensor([[ 1.7266,  0.1546, -0.2650, -0.7863, -0.9227,  0.0036,  0.9438]],
       dtype=torch.float64)
	q_value: tensor([[-31.4004]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09203200645150855, distance: 1.0904153664823053 entropy 1.0271300077438354
epoch: 17, step: 101
	action: tensor([[ 0.8510,  0.3489, -0.2968, -0.4381,  0.4633,  0.2064,  0.7677]],
       dtype=torch.float64)
	q_value: tensor([[-39.3566]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8417701941578154, distance: 0.455198519479065 entropy 1.0271300077438354
epoch: 17, step: 102
	action: tensor([[ 1.4316,  1.3205,  1.0756,  0.5663, -0.3089,  0.0260,  0.1508]],
       dtype=torch.float64)
	q_value: tensor([[-25.9862]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 17, step: 103
	action: tensor([[-1.0908, -0.2574, -1.2366,  1.0137,  0.1310, -0.9266,  0.0632]],
       dtype=torch.float64)
	q_value: tensor([[-33.2423]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.3312639017392143, distance: 1.7472393932563761 entropy 1.0271300077438354
epoch: 17, step: 104
	action: tensor([[-0.0233,  0.9208, -1.5043,  0.0696,  0.0736, -0.9489, -0.2685]],
       dtype=torch.float64)
	q_value: tensor([[-34.1360]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8151864710866706, distance: 0.49195295449554727 entropy 1.0271300077438354
epoch: 17, step: 105
	action: tensor([[ 0.7055, -0.5043,  0.5277, -0.1987,  0.6190, -1.1863, -1.1171]],
       dtype=torch.float64)
	q_value: tensor([[-28.7376]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03458700280172755, distance: 1.124380393507191 entropy 1.0271300077438354
epoch: 17, step: 106
	action: tensor([[-1.2014,  0.7506,  0.2949,  1.7269, -0.3551, -0.7192,  0.4124]],
       dtype=torch.float64)
	q_value: tensor([[-40.1200]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6118183753444942, distance: 0.7129747998415356 entropy 1.0271300077438354
epoch: 17, step: 107
	action: tensor([[ 0.2790,  0.1011, -0.2293, -0.6544,  0.4224, -1.5311,  0.2773]],
       dtype=torch.float64)
	q_value: tensor([[-33.0412]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19653988827774593, distance: 1.0257436891753973 entropy 1.0271300077438354
epoch: 17, step: 108
	action: tensor([[ 0.1158,  0.7918,  0.5241, -0.7805, -0.1564, -2.2126,  0.8434]],
       dtype=torch.float64)
	q_value: tensor([[-33.3554]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44385461511717794, distance: 0.8533965092526579 entropy 1.0271300077438354
epoch: 17, step: 109
	action: tensor([[-1.0057, -0.8844,  0.7176,  0.6548,  0.0698, -1.0168,  0.7379]],
       dtype=torch.float64)
	q_value: tensor([[-42.5047]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.142613268745131, distance: 1.6750531361428267 entropy 1.0271300077438354
epoch: 17, step: 110
	action: tensor([[-0.3265, -0.7400,  1.0189,  0.5484, -0.9546, -1.1193,  0.7851]],
       dtype=torch.float64)
	q_value: tensor([[-41.5846]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.36420985700712016, distance: 1.336587160378086 entropy 1.0271300077438354
epoch: 17, step: 111
	action: tensor([[ 1.6487,  0.8847,  0.5440,  0.9955, -0.0672, -0.8018, -0.0862]],
       dtype=torch.float64)
	q_value: tensor([[-42.1223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 17, step: 112
	action: tensor([[-0.0121,  0.4828,  0.5499, -1.0569, -0.7425,  0.9131, -0.2816]],
       dtype=torch.float64)
	q_value: tensor([[-33.2423]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11651324121121198, distance: 1.075614667333637 entropy 1.0271300077438354
epoch: 17, step: 113
	action: tensor([[-1.0865, -0.3987, -0.1544,  0.9208, -0.5839, -0.6198,  0.5740]],
       dtype=torch.float64)
	q_value: tensor([[-34.4950]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8689552612736624, distance: 1.5644300376853388 entropy 1.0271300077438354
epoch: 17, step: 114
	action: tensor([[-0.2423,  0.9502, -0.4874,  0.5141, -0.7519, -0.6492,  0.0728]],
       dtype=torch.float64)
	q_value: tensor([[-32.0941]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 17, step: 115
	action: tensor([[ 0.6989, -0.7735,  1.1631, -1.0635, -0.4130, -0.5540,  1.0329]],
       dtype=torch.float64)
	q_value: tensor([[-33.2423]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.40506196306337916, distance: 1.3564520047870168 entropy 1.0271300077438354
epoch: 17, step: 116
	action: tensor([[ 1.5275,  0.3086,  0.8421,  0.5352, -0.7069, -0.0015, -0.8132]],
       dtype=torch.float64)
	q_value: tensor([[-43.4146]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5492865245635494, distance: 0.7682577759403396 entropy 1.0271300077438354
epoch: 17, step: 117
	action: tensor([[ 0.0072,  0.6764,  1.2946, -0.8675, -1.0965, -0.1467,  0.2501]],
       dtype=torch.float64)
	q_value: tensor([[-42.7633]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03856124014201634, distance: 1.122063684035825 entropy 1.0271300077438354
epoch: 17, step: 118
	action: tensor([[ 0.8481, -0.1037, -0.3376,  0.2495, -0.3289, -0.1282,  1.2076]],
       dtype=torch.float64)
	q_value: tensor([[-38.6591]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7757490534792826, distance: 0.5419058454074531 entropy 1.0271300077438354
epoch: 17, step: 119
	action: tensor([[ 0.2770, -0.5600,  0.2191, -0.0820,  0.7862,  0.7216, -0.4553]],
       dtype=torch.float64)
	q_value: tensor([[-29.5721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22549174755033696, distance: 1.007093326326741 entropy 1.0271300077438354
epoch: 17, step: 120
	action: tensor([[ 0.8547, -0.2276,  0.2306,  1.5396,  1.3110, -0.8610,  1.0127]],
       dtype=torch.float64)
	q_value: tensor([[-25.3871]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9659635109044937, distance: 0.21111985120899388 entropy 1.0271300077438354
epoch: 17, step: 121
	action: tensor([[ 0.3710, -0.4625,  0.7602,  0.7081, -0.4778, -0.3627, -1.0949]],
       dtype=torch.float64)
	q_value: tensor([[-50.5822]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5339048172303802, distance: 0.7812571418517351 entropy 1.0271300077438354
epoch: 17, step: 122
	action: tensor([[ 0.2257,  0.2018,  0.9924,  0.3647, -0.9718, -0.6731,  0.8595]],
       dtype=torch.float64)
	q_value: tensor([[-35.7577]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6966569550498874, distance: 0.6302657600812174 entropy 1.0271300077438354
epoch: 17, step: 123
	action: tensor([[ 1.2294,  0.4670,  0.0785,  1.6534,  0.4450,  0.1700, -0.3279]],
       dtype=torch.float64)
	q_value: tensor([[-34.8141]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4938789342370078, distance: 0.8141115199749712 entropy 1.0271300077438354
epoch: 17, step: 124
	action: tensor([[ 0.3308,  1.7059,  0.6282, -0.3613, -0.6541, -1.0223,  1.4755]],
       dtype=torch.float64)
	q_value: tensor([[-39.2784]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 17, step: 125
	action: tensor([[ 0.0814,  1.1361,  0.5342,  0.1680, -0.1144, -0.4141, -0.8016]],
       dtype=torch.float64)
	q_value: tensor([[-33.2423]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 17, step: 126
	action: tensor([[-0.4712,  1.7072, -0.1258,  0.3960,  0.3351, -0.3073,  0.7293]],
       dtype=torch.float64)
	q_value: tensor([[-33.2423]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 1.0271300077438354
epoch: 17, step: 127
	action: tensor([[-0.9324,  0.0122, -0.8267,  0.2942,  0.8642,  0.6828, -0.3825]],
       dtype=torch.float64)
	q_value: tensor([[-33.2423]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6628085265914281, distance: 1.4756311508283828 entropy 1.0271300077438354
LOSS epoch 17 actor 443.47243874480677 critic 132.63838417525932 
epoch: 18, step: 0
	action: tensor([[ 0.5476, -0.4041,  0.2841, -0.4320,  0.5446,  0.1006, -1.0459]],
       dtype=torch.float64)
	q_value: tensor([[-25.5985]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.017175603576681686, distance: 1.1344742881557497 entropy 0.9217694997787476
epoch: 18, step: 1
	action: tensor([[-0.1943, -0.0118, -0.1854, -0.0925,  0.3622,  0.1643,  0.5286]],
       dtype=torch.float64)
	q_value: tensor([[-28.1724]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05538228195309525, distance: 1.112204727968869 entropy 0.9217694997787476
epoch: 18, step: 2
	action: tensor([[-0.3486,  0.2907, -0.1274, -0.1532, -1.3008,  0.6004, -0.9490]],
       dtype=torch.float64)
	q_value: tensor([[-19.9917]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1508033169823848, distance: 1.2276010345779498 entropy 0.9217694997787476
epoch: 18, step: 3
	action: tensor([[ 0.6846,  0.1168,  0.9056, -0.4539,  0.3631,  0.3053,  0.6494]],
       dtype=torch.float64)
	q_value: tensor([[-30.8687]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6939582167118566, distance: 0.6330631803682029 entropy 0.9217694997787476
epoch: 18, step: 4
	action: tensor([[ 0.7521,  0.4004,  1.0947, -0.3017, -0.3453, -1.0352,  0.9270]],
       dtype=torch.float64)
	q_value: tensor([[-30.4623]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7344981247687702, distance: 0.5896448227843186 entropy 0.9217694997787476
epoch: 18, step: 5
	action: tensor([[ 0.2149,  0.7808, -0.5039, -0.4618, -0.4215, -0.7282,  0.6450]],
       dtype=torch.float64)
	q_value: tensor([[-39.4157]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8765699339081604, distance: 0.402038064970441 entropy 0.9217694997787476
epoch: 18, step: 6
	action: tensor([[ 1.3149,  0.1790,  0.1868,  0.2860, -0.5722,  0.8046,  1.7383]],
       dtype=torch.float64)
	q_value: tensor([[-25.3984]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9187203206989082, distance: 0.32624786926186955 entropy 0.9217694997787476
epoch: 18, step: 7
	action: tensor([[-0.0410, -0.0070, -0.1076, -0.5712, -0.3662,  0.2386,  0.8463]],
       dtype=torch.float64)
	q_value: tensor([[-40.7756]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.009854537148349829, distance: 1.1499689223267362 entropy 0.9217694997787476
epoch: 18, step: 8
	action: tensor([[-0.2424, -0.6641, -0.7372, -0.3134,  1.0745, -0.1197, -0.5349]],
       dtype=torch.float64)
	q_value: tensor([[-23.9231]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5588795935541739, distance: 1.4287721599048815 entropy 0.9217694997787476
epoch: 18, step: 9
	action: tensor([[-0.1409, -0.1628,  0.4421, -0.2382, -0.1232, -1.2284, -0.5263]],
       dtype=torch.float64)
	q_value: tensor([[-27.1449]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.29660289506881954, distance: 1.3030473182119855 entropy 0.9217694997787476
epoch: 18, step: 10
	action: tensor([[ 0.3250,  0.0707, -0.5927, -0.5531, -0.0063, -0.1243, -0.9306]],
       dtype=torch.float64)
	q_value: tensor([[-31.4852]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4634115963834158, distance: 0.838257281587532 entropy 0.9217694997787476
epoch: 18, step: 11
	action: tensor([[ 0.2266, -0.5133, -0.6062,  0.0372,  0.4767, -0.0477,  0.0375]],
       dtype=torch.float64)
	q_value: tensor([[-22.6788]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.02354179708438564, distance: 1.1307940700026469 entropy 0.9217694997787476
epoch: 18, step: 12
	action: tensor([[ 0.1993,  0.7283, -0.5709, -0.4298, -0.8391, -0.0132,  1.2063]],
       dtype=torch.float64)
	q_value: tensor([[-23.2276]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8184959814797037, distance: 0.48752828318432573 entropy 0.9217694997787476
epoch: 18, step: 13
	action: tensor([[ 0.7917, -0.1642, -0.6175, -0.1293, -0.0356,  0.5436, -0.2271]],
       dtype=torch.float64)
	q_value: tensor([[-29.0447]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6872137329555443, distance: 0.6400008219872756 entropy 0.9217694997787476
epoch: 18, step: 14
	action: tensor([[ 1.6811, -0.3748, -0.5954,  0.0418, -0.6063,  0.0553, -0.1306]],
       dtype=torch.float64)
	q_value: tensor([[-23.6435]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15273595077638347, distance: 1.0533339514767481 entropy 0.9217694997787476
epoch: 18, step: 15
	action: tensor([[-1.2183, -0.5063, -0.1985, -0.1999,  0.0830,  0.9561,  1.3663]],
       dtype=torch.float64)
	q_value: tensor([[-34.6005]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.3624601015848588, distance: 1.7588910320512539 entropy 0.9217694997787476
epoch: 18, step: 16
	action: tensor([[-0.3241, -0.1323,  0.1571, -0.0828, -0.1404,  0.1933,  0.2053]],
       dtype=torch.float64)
	q_value: tensor([[-35.0394]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.244332337388953, distance: 1.2765119567240997 entropy 0.9217694997787476
epoch: 18, step: 17
	action: tensor([[ 0.0821,  0.0628,  0.2322, -0.1246, -0.7126, -0.4674, -0.5457]],
       dtype=torch.float64)
	q_value: tensor([[-19.0926]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27962896685252914, distance: 0.9712584580449819 entropy 0.9217694997787476
epoch: 18, step: 18
	action: tensor([[-0.6438,  0.3374,  0.3465,  0.1835,  0.1297,  0.1882, -0.4754]],
       dtype=torch.float64)
	q_value: tensor([[-25.1985]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09627298727147737, distance: 1.198163401654302 entropy 0.9217694997787476
epoch: 18, step: 19
	action: tensor([[-1.4262, -0.7861, -0.8910, -0.5211, -0.3459,  0.0235,  0.0933]],
       dtype=torch.float64)
	q_value: tensor([[-20.4161]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.3728344591146833, distance: 1.7627487428534332 entropy 0.9217694997787476
epoch: 18, step: 20
	action: tensor([[ 0.2732,  0.0942, -0.6625,  0.2277, -0.0263,  0.2521, -0.4714]],
       dtype=torch.float64)
	q_value: tensor([[-29.6160]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5645045267659463, distance: 0.7551765850789741 entropy 0.9217694997787476
epoch: 18, step: 21
	action: tensor([[ 0.2615,  0.7653,  0.1924, -0.1598, -0.7043, -0.8297,  0.9067]],
       dtype=torch.float64)
	q_value: tensor([[-19.1683]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8212409437526135, distance: 0.48382768994339626 entropy 0.9217694997787476
epoch: 18, step: 22
	action: tensor([[ 0.8283,  1.0683,  0.8124, -0.1147,  0.4016, -0.8019, -0.7111]],
       dtype=torch.float64)
	q_value: tensor([[-29.5212]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8890255815962015, distance: 0.3812133771868843 entropy 0.9217694997787476
epoch: 18, step: 23
	action: tensor([[-0.7727, -0.0511,  1.1625,  0.1241, -0.7783, -0.4379,  0.2985]],
       dtype=torch.float64)
	q_value: tensor([[-35.6053]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.761976329146761, distance: 1.5189963423671797 entropy 0.9217694997787476
epoch: 18, step: 24
	action: tensor([[ 0.0595, -0.7015,  0.3972, -0.6225, -1.6222, -0.2330,  0.4392]],
       dtype=torch.float64)
	q_value: tensor([[-31.8566]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6943402537540706, distance: 1.489556591599045 entropy 0.9217694997787476
epoch: 18, step: 25
	action: tensor([[ 0.8926,  1.7537, -0.0508, -0.1034,  1.8695,  0.8114, -0.2774]],
       dtype=torch.float64)
	q_value: tensor([[-37.4821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 18, step: 26
	action: tensor([[-1.3433, -1.3262,  0.1416, -0.6892,  0.0591, -0.2746,  0.6668]],
       dtype=torch.float64)
	q_value: tensor([[-32.4677]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.044894839772276, distance: 1.6364102185064788 entropy 0.9217694997787476
epoch: 18, step: 27
	action: tensor([[-0.9542,  1.0142, -0.1227, -0.1545,  0.5737,  0.2883,  0.1373]],
       dtype=torch.float64)
	q_value: tensor([[-37.5957]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 18, step: 28
	action: tensor([[-0.2736, -0.2253,  0.3406, -0.3842, -0.2880, -0.4261, -0.5843]],
       dtype=torch.float64)
	q_value: tensor([[-32.4677]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5111706416192026, distance: 1.4067387384440995 entropy 0.9217694997787476
epoch: 18, step: 29
	action: tensor([[-0.5407,  1.1091,  1.2245, -0.2927, -0.0308, -0.3080, -0.6349]],
       dtype=torch.float64)
	q_value: tensor([[-23.4198]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08626915279315495, distance: 1.1926840646234294 entropy 0.9217694997787476
epoch: 18, step: 30
	action: tensor([[ 0.0024,  0.1017,  0.9293, -0.0769,  0.3856, -0.2918,  0.3393]],
       dtype=torch.float64)
	q_value: tensor([[-29.0047]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2103895629375112, distance: 1.016864611286279 entropy 0.9217694997787476
epoch: 18, step: 31
	action: tensor([[-0.0401,  0.5575, -0.7269, -0.0184,  0.0669,  0.9206, -0.7397]],
       dtype=torch.float64)
	q_value: tensor([[-27.8296]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.46029192691004217, distance: 0.8406905206255347 entropy 0.9217694997787476
epoch: 18, step: 32
	action: tensor([[ 0.5346, -0.0320, -0.5810,  1.3953,  0.6976, -0.2902,  0.4579]],
       dtype=torch.float64)
	q_value: tensor([[-22.9556]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6792999072292607, distance: 0.6480465994492631 entropy 0.9217694997787476
epoch: 18, step: 33
	action: tensor([[ 0.5407,  0.7419, -0.1288,  1.0806,  1.0208, -0.4802,  0.9511]],
       dtype=torch.float64)
	q_value: tensor([[-36.2343]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 18, step: 34
	action: tensor([[ 0.5491, -0.5186,  0.0294, -0.5126,  1.5335, -0.2898, -0.4038]],
       dtype=torch.float64)
	q_value: tensor([[-32.4677]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20389629133738874, distance: 1.2555997818266034 entropy 0.9217694997787476
epoch: 18, step: 35
	action: tensor([[-0.3709, -0.3875, -0.2197, -0.1409,  0.7877, -0.7406,  0.8528]],
       dtype=torch.float64)
	q_value: tensor([[-34.4918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6905804489183496, distance: 1.4879029826460375 entropy 0.9217694997787476
epoch: 18, step: 36
	action: tensor([[ 0.7722,  0.3148, -0.2020,  0.0658, -0.0793, -0.3478, -0.2856]],
       dtype=torch.float64)
	q_value: tensor([[-31.4146]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8976384293759418, distance: 0.36612144165146415 entropy 0.9217694997787476
epoch: 18, step: 37
	action: tensor([[ 1.4427,  1.1300,  0.5239,  0.5453,  0.8327, -0.2291,  0.1362]],
       dtype=torch.float64)
	q_value: tensor([[-23.8846]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 18, step: 38
	action: tensor([[ 0.3731, -0.3456, -0.2178,  0.9239, -0.7272, -0.0541, -0.4178]],
       dtype=torch.float64)
	q_value: tensor([[-32.4677]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.704294046234587, distance: 0.6222812674972902 entropy 0.9217694997787476
epoch: 18, step: 39
	action: tensor([[ 0.2581, -0.4057,  0.8729,  0.8424,  0.1626,  0.4570,  0.2242]],
       dtype=torch.float64)
	q_value: tensor([[-27.9661]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8330261039687284, distance: 0.46760697824145614 entropy 0.9217694997787476
epoch: 18, step: 40
	action: tensor([[-0.3135, -0.8902,  0.9215,  0.3683,  0.6701, -0.1976,  1.1846]],
       dtype=torch.float64)
	q_value: tensor([[-34.2607]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.48563809179901885, distance: 1.3948040712941014 entropy 0.9217694997787476
epoch: 18, step: 41
	action: tensor([[-0.0928,  0.7472,  0.0954, -0.1944, -0.4538, -1.0486,  0.1821]],
       dtype=torch.float64)
	q_value: tensor([[-41.7977]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5178220716938399, distance: 0.7946215824454089 entropy 0.9217694997787476
epoch: 18, step: 42
	action: tensor([[ 0.0683,  0.0327, -0.7403, -0.5873, -0.7308, -0.1314,  0.8277]],
       dtype=torch.float64)
	q_value: tensor([[-25.8840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3745274316928473, distance: 0.9050255226904992 entropy 0.9217694997787476
epoch: 18, step: 43
	action: tensor([[ 0.2652,  0.7968, -0.1223,  0.6749, -0.7667, -0.4608, -0.5812]],
       dtype=torch.float64)
	q_value: tensor([[-25.9203]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 18, step: 44
	action: tensor([[ 0.3888, -0.3629, -0.3754, -0.1064, -1.4931, -0.3687, -0.7094]],
       dtype=torch.float64)
	q_value: tensor([[-32.4677]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3286539681344701, distance: 0.9376265715834924 entropy 0.9217694997787476
epoch: 18, step: 45
	action: tensor([[-0.4884,  0.8182,  1.0538, -0.7991,  0.1447,  1.0523,  0.6251]],
       dtype=torch.float64)
	q_value: tensor([[-33.3587]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03118935868009487, distance: 1.1263572102349744 entropy 0.9217694997787476
epoch: 18, step: 46
	action: tensor([[-0.3774,  0.1708,  0.8725, -0.3203, -1.2658, -0.2701,  0.2734]],
       dtype=torch.float64)
	q_value: tensor([[-32.4352]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3651746022829683, distance: 1.3370596823477543 entropy 0.9217694997787476
epoch: 18, step: 47
	action: tensor([[-0.5179, -0.0583, -0.3371,  0.1283,  0.3525,  0.0535, -0.3852]],
       dtype=torch.float64)
	q_value: tensor([[-32.9628]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4080095194598128, distance: 1.3578740502933855 entropy 0.9217694997787476
epoch: 18, step: 48
	action: tensor([[-0.5482,  0.2698, -0.2605,  0.1606, -0.1648, -0.0762,  0.4848]],
       dtype=torch.float64)
	q_value: tensor([[-18.9476]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20600810947629244, distance: 1.2567005562891016 entropy 0.9217694997787476
epoch: 18, step: 49
	action: tensor([[-0.6121,  0.4005, -1.0110, -0.5232, -0.0541,  0.9535,  0.6299]],
       dtype=torch.float64)
	q_value: tensor([[-19.1333]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09763910395150743, distance: 1.1989097127520372 entropy 0.9217694997787476
epoch: 18, step: 50
	action: tensor([[ 0.4629,  0.7896, -0.8338,  0.3710, -0.4397,  0.0118, -1.0562]],
       dtype=torch.float64)
	q_value: tensor([[-25.9527]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 18, step: 51
	action: tensor([[-0.4844,  0.6267,  0.3411, -0.5320,  0.6340,  0.5330,  0.3034]],
       dtype=torch.float64)
	q_value: tensor([[-32.4677]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.018135402930130162, distance: 1.1339202046075056 entropy 0.9217694997787476
epoch: 18, step: 52
	action: tensor([[-0.3071, -0.0674, -0.2517,  0.3229,  0.5112, -0.0702, -0.0805]],
       dtype=torch.float64)
	q_value: tensor([[-22.1384]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0952144194032214, distance: 1.197584785005391 entropy 0.9217694997787476
epoch: 18, step: 53
	action: tensor([[ 0.0908,  0.7934,  0.9039, -1.0711, -0.3708,  0.0556, -0.6363]],
       dtype=torch.float64)
	q_value: tensor([[-20.6026]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14065784422261607, distance: 1.0608152435662184 entropy 0.9217694997787476
epoch: 18, step: 54
	action: tensor([[ 1.4206,  0.7986,  1.7857,  0.6665,  0.0681, -0.6391, -0.1073]],
       dtype=torch.float64)
	q_value: tensor([[-32.9699]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6716031384309915, distance: 0.6557770175631956 entropy 0.9217694997787476
epoch: 18, step: 55
	action: tensor([[-0.1231, -0.8359, -1.1476,  0.1936, -0.1279,  0.3303, -0.6709]],
       dtype=torch.float64)
	q_value: tensor([[-50.1968]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.670634921682596, distance: 1.4790997747661943 entropy 0.9217694997787476
epoch: 18, step: 56
	action: tensor([[ 0.2329, -0.4187, -1.1711, -0.2482,  0.5920,  0.9267,  0.7489]],
       dtype=torch.float64)
	q_value: tensor([[-25.8212]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31444418605780367, distance: 0.9474975647245522 entropy 0.9217694997787476
epoch: 18, step: 57
	action: tensor([[ 0.3418,  0.4836, -0.8382,  0.0755, -0.2616, -0.8221, -1.9248]],
       dtype=torch.float64)
	q_value: tensor([[-29.8277]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8895898412462495, distance: 0.3802429848414059 entropy 0.9217694997787476
epoch: 18, step: 58
	action: tensor([[ 0.6365, -0.2302,  0.0037, -0.8254, -0.7634,  0.9927, -0.8470]],
       dtype=torch.float64)
	q_value: tensor([[-36.7853]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.263833739039247, distance: 0.9818488776915918 entropy 0.9217694997787476
epoch: 18, step: 59
	action: tensor([[ 0.4641,  0.6042,  0.1577, -0.3477, -1.0408,  0.3804,  0.4287]],
       dtype=torch.float64)
	q_value: tensor([[-34.4895]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7943863823839508, distance: 0.5188987570285796 entropy 0.9217694997787476
epoch: 18, step: 60
	action: tensor([[-0.8380, -0.1832,  0.0790, -0.7308, -0.5098,  0.9635,  1.1118]],
       dtype=torch.float64)
	q_value: tensor([[-30.1759]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0567521060380773, distance: 1.6411477006273456 entropy 0.9217694997787476
epoch: 18, step: 61
	action: tensor([[-0.3729, -1.4683,  0.1279,  0.8375, -0.1174,  1.1534,  0.8350]],
       dtype=torch.float64)
	q_value: tensor([[-31.8489]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.026431038568652587, distance: 1.1593687269278068 entropy 0.9217694997787476
epoch: 18, step: 62
	action: tensor([[-0.3564,  0.5945,  0.3175,  0.1708, -0.0285,  0.8524,  0.0951]],
       dtype=torch.float64)
	q_value: tensor([[-38.1160]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4945127864417874, distance: 0.8136015747440992 entropy 0.9217694997787476
epoch: 18, step: 63
	action: tensor([[ 0.1348, -1.0933, -0.1479, -0.5623,  0.8302,  0.9147,  0.1653]],
       dtype=torch.float64)
	q_value: tensor([[-22.1123]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4500466410161499, distance: 1.377995118434919 entropy 0.9217694997787476
epoch: 18, step: 64
	action: tensor([[ 0.2709,  0.0932, -0.0530,  0.6750, -0.7635, -0.9263, -0.6048]],
       dtype=torch.float64)
	q_value: tensor([[-31.1065]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8145579604089151, distance: 0.49278875708579506 entropy 0.9217694997787476
epoch: 18, step: 65
	action: tensor([[ 0.4045, -0.3573, -0.9897, -0.4686,  0.2424, -0.3898, -0.3343]],
       dtype=torch.float64)
	q_value: tensor([[-31.4837]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24949324097979686, distance: 0.9913659474334252 entropy 0.9217694997787476
epoch: 18, step: 66
	action: tensor([[ 0.7684,  0.6811,  1.1412, -0.6103,  0.3726, -0.6969,  0.6484]],
       dtype=torch.float64)
	q_value: tensor([[-24.4589]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.743246106442562, distance: 0.5798494003449943 entropy 0.9217694997787476
epoch: 18, step: 67
	action: tensor([[ 0.7443,  0.4249,  0.6804,  0.3841, -0.4118, -0.2765, -0.2277]],
       dtype=torch.float64)
	q_value: tensor([[-35.8984]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9877678682498527, distance: 0.1265632936713061 entropy 0.9217694997787476
epoch: 18, step: 68
	action: tensor([[ 0.7205,  0.2854, -0.2954, -0.3200,  0.5357,  0.8602, -0.4869]],
       dtype=torch.float64)
	q_value: tensor([[-30.4072]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9503511556578218, distance: 0.2549830227714569 entropy 0.9217694997787476
epoch: 18, step: 69
	action: tensor([[-0.7908,  1.1955,  0.4173, -0.5108,  0.8700, -0.0837,  0.0416]],
       dtype=torch.float64)
	q_value: tensor([[-25.2791]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23435745958889953, distance: 1.2713852429775123 entropy 0.9217694997787476
epoch: 18, step: 70
	action: tensor([[ 0.6547,  0.4060,  0.0368,  0.9083,  0.3415,  0.1415, -1.2717]],
       dtype=torch.float64)
	q_value: tensor([[-24.8588]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3390895260550293, distance: 0.9303106881960873 entropy 0.9217694997787476
epoch: 18, step: 71
	action: tensor([[-0.6707,  0.1777,  1.5409, -0.8546,  0.3131, -0.3347, -0.9030]],
       dtype=torch.float64)
	q_value: tensor([[-32.7493]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.6371569720862036, distance: 1.8583378983794143 entropy 0.9217694997787476
epoch: 18, step: 72
	action: tensor([[ 0.7164, -0.0942,  0.3614, -1.6373,  0.6590,  0.2201,  1.1438]],
       dtype=torch.float64)
	q_value: tensor([[-34.0606]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1385229519322344, distance: 1.2210335253951805 entropy 0.9217694997787476
epoch: 18, step: 73
	action: tensor([[ 0.3559,  0.0393, -0.4935, -0.7414,  1.2954,  0.3809,  0.4088]],
       dtype=torch.float64)
	q_value: tensor([[-38.1501]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4998604566556464, distance: 0.8092864889259036 entropy 0.9217694997787476
epoch: 18, step: 74
	action: tensor([[ 0.8194, -0.8060, -0.6676,  0.3713,  0.1798,  0.0164,  0.2242]],
       dtype=torch.float64)
	q_value: tensor([[-28.1654]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14736204687766263, distance: 1.0566691384892561 entropy 0.9217694997787476
epoch: 18, step: 75
	action: tensor([[-0.2806, -0.4459, -0.1564,  0.2572, -1.7653, -0.0835, -0.1886]],
       dtype=torch.float64)
	q_value: tensor([[-30.0093]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.26549715127750706, distance: 1.2873222605740455 entropy 0.9217694997787476
epoch: 18, step: 76
	action: tensor([[ 1.5505, -0.5182, -0.7872,  0.7368, -1.5555,  0.5118, -0.5199]],
       dtype=torch.float64)
	q_value: tensor([[-32.7532]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5527738959582547, distance: 0.7652798276126279 entropy 0.9217694997787476
epoch: 18, step: 77
	action: tensor([[ 0.8663, -0.0282,  0.4767, -0.2859,  0.5854,  0.4262,  0.0918]],
       dtype=torch.float64)
	q_value: tensor([[-43.4280]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6839952596023109, distance: 0.643285099710005 entropy 0.9217694997787476
epoch: 18, step: 78
	action: tensor([[ 0.1333,  0.6208,  0.2811, -0.1541, -0.0440, -0.2419,  0.0428]],
       dtype=torch.float64)
	q_value: tensor([[-28.0523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6296133515739821, distance: 0.6964410390121969 entropy 0.9217694997787476
epoch: 18, step: 79
	action: tensor([[-0.2242,  0.5528, -0.4170, -0.3558, -0.0586,  0.3789,  1.8871]],
       dtype=torch.float64)
	q_value: tensor([[-20.7750]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3290254173228464, distance: 0.9373671458917634 entropy 0.9217694997787476
epoch: 18, step: 80
	action: tensor([[-0.3703,  0.3764, -1.3721, -0.4663, -0.5515,  0.0720,  0.8063]],
       dtype=torch.float64)
	q_value: tensor([[-33.2541]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32122781492659946, distance: 0.9427981289211019 entropy 0.9217694997787476
epoch: 18, step: 81
	action: tensor([[ 0.0251, -0.0359,  0.4438,  0.7154,  0.1864, -0.0378,  0.9864]],
       dtype=torch.float64)
	q_value: tensor([[-27.2856]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6935170389679854, distance: 0.6335193154758831 entropy 0.9217694997787476
epoch: 18, step: 82
	action: tensor([[-0.2927,  1.3273,  1.2140, -0.2423, -0.6360,  0.0794, -0.1903]],
       dtype=torch.float64)
	q_value: tensor([[-29.6723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 18, step: 83
	action: tensor([[-0.3325, -0.3199,  0.7255, -0.6241,  0.8350, -0.4924,  0.1792]],
       dtype=torch.float64)
	q_value: tensor([[-32.4677]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8864544302942661, distance: 1.5717369120272568 entropy 0.9217694997787476
epoch: 18, step: 84
	action: tensor([[ 0.1358, -0.4705,  0.0249,  0.4598, -0.6556, -0.4409, -0.5549]],
       dtype=torch.float64)
	q_value: tensor([[-29.2535]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2202372131792636, distance: 1.0105037880185306 entropy 0.9217694997787476
epoch: 18, step: 85
	action: tensor([[ 0.8281, -0.0860,  0.9674, -0.7815,  0.0017,  0.3820, -0.0043]],
       dtype=torch.float64)
	q_value: tensor([[-27.6831]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4659860321960294, distance: 0.8362439747103003 entropy 0.9217694997787476
epoch: 18, step: 86
	action: tensor([[-0.1655,  1.1131,  0.0521,  0.4835,  0.6156, -0.2309,  1.1982]],
       dtype=torch.float64)
	q_value: tensor([[-32.7273]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 18, step: 87
	action: tensor([[ 0.5352, -0.6896,  0.6125,  0.3374,  0.0373, -1.2420, -0.8605]],
       dtype=torch.float64)
	q_value: tensor([[-32.4677]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0511418336086884, distance: 1.1732413300339442 entropy 0.9217694997787476
epoch: 18, step: 88
	action: tensor([[ 0.4909,  0.0939,  0.3741, -0.0906,  1.1210, -0.4179,  0.3394]],
       dtype=torch.float64)
	q_value: tensor([[-42.2913]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5915033313652547, distance: 0.7313932582559586 entropy 0.9217694997787476
epoch: 18, step: 89
	action: tensor([[-0.1417, -0.6560,  0.0472,  0.2154, -1.0967, -0.2528, -0.3121]],
       dtype=torch.float64)
	q_value: tensor([[-30.8315]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.29377062360620987, distance: 1.3016233659132577 entropy 0.9217694997787476
epoch: 18, step: 90
	action: tensor([[-0.4550, -0.0638,  0.3886, -0.1865, -0.2496, -0.9393, -0.4844]],
       dtype=torch.float64)
	q_value: tensor([[-27.6896]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5130091717654377, distance: 1.4075942160902446 entropy 0.9217694997787476
epoch: 18, step: 91
	action: tensor([[ 0.4323, -0.9992, -1.2656, -0.2564, -0.0335,  0.9765,  0.2948]],
       dtype=torch.float64)
	q_value: tensor([[-26.5709]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3304792202598219, distance: 1.3199599109948927 entropy 0.9217694997787476
epoch: 18, step: 92
	action: tensor([[ 0.4770,  1.1141, -0.2891,  0.5981,  0.1059, -0.5223, -0.2602]],
       dtype=torch.float64)
	q_value: tensor([[-30.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 18, step: 93
	action: tensor([[ 0.2930, -0.4732,  0.2505, -0.5552, -0.2834,  0.2446, -0.1333]],
       dtype=torch.float64)
	q_value: tensor([[-32.4677]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2051504119201546, distance: 1.2562536021196826 entropy 0.9217694997787476
epoch: 18, step: 94
	action: tensor([[ 0.4028,  0.8311,  1.1044,  0.7245, -0.3230, -0.5472, -0.6947]],
       dtype=torch.float64)
	q_value: tensor([[-24.2883]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9453903750131678, distance: 0.26741840266116407 entropy 0.9217694997787476
epoch: 18, step: 95
	action: tensor([[ 0.8214,  0.0020, -0.0468, -0.4331,  0.2085, -0.0203,  0.1979]],
       dtype=torch.float64)
	q_value: tensor([[-35.4301]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5279480043112099, distance: 0.7862336229234564 entropy 0.9217694997787476
epoch: 18, step: 96
	action: tensor([[-0.0915, -0.5516, -0.8169,  0.1420, -0.4592, -0.1231, -0.0534]],
       dtype=torch.float64)
	q_value: tensor([[-24.7023]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2575532599456154, distance: 1.2832754529059038 entropy 0.9217694997787476
epoch: 18, step: 97
	action: tensor([[-0.5510,  0.2914, -0.0232,  1.1473,  0.2536,  0.7482,  0.8107]],
       dtype=torch.float64)
	q_value: tensor([[-22.6800]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.48933861347732976, distance: 0.8177549907654068 entropy 0.9217694997787476
epoch: 18, step: 98
	action: tensor([[ 0.4150, -0.8108,  0.4457, -0.9050, -0.9536,  1.1915, -0.0923]],
       dtype=torch.float64)
	q_value: tensor([[-27.6611]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.345277401087845, distance: 1.3272802025967638 entropy 0.9217694997787476
epoch: 18, step: 99
	action: tensor([[-0.2714,  1.1849,  0.1026, -0.6437, -0.1733, -0.3164,  0.7327]],
       dtype=torch.float64)
	q_value: tensor([[-38.0201]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3380262270953063, distance: 0.9310587477898584 entropy 0.9217694997787476
epoch: 18, step: 100
	action: tensor([[ 0.8031,  0.4093,  0.1489, -0.0628,  0.2774,  0.2626,  0.5288]],
       dtype=torch.float64)
	q_value: tensor([[-25.9074]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.975245036031149, distance: 0.18004780971693865 entropy 0.9217694997787476
epoch: 18, step: 101
	action: tensor([[ 0.2379,  0.4288,  0.2242,  0.2761, -0.7091,  0.7693,  0.3539]],
       dtype=torch.float64)
	q_value: tensor([[-24.8937]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7604794265457248, distance: 0.5600516804194798 entropy 0.9217694997787476
epoch: 18, step: 102
	action: tensor([[ 0.4452, -1.0443, -0.7372,  0.6168,  0.8029, -0.1279,  0.2336]],
       dtype=torch.float64)
	q_value: tensor([[-26.1656]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.25399109999957803, distance: 1.2814566535404355 entropy 0.9217694997787476
epoch: 18, step: 103
	action: tensor([[ 0.9115,  1.0347, -0.0844, -0.6894,  0.1596,  0.1656, -0.3835]],
       dtype=torch.float64)
	q_value: tensor([[-35.6673]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.08706911631449159 entropy 0.9217694997787476
epoch: 18, step: 104
	action: tensor([[-0.5150,  0.3734,  0.3435, -0.8825, -0.6863, -0.0119,  0.4137]],
       dtype=torch.float64)
	q_value: tensor([[-32.4677]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5477952869364968, distance: 1.423683504825076 entropy 0.9217694997787476
epoch: 18, step: 105
	action: tensor([[ 0.5478, -0.6080,  0.3827, -0.3657, -0.8950,  1.3460, -0.2232]],
       dtype=torch.float64)
	q_value: tensor([[-26.9593]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2591471154932109, distance: 0.9849692703666482 entropy 0.9217694997787476
epoch: 18, step: 106
	action: tensor([[ 1.3745,  0.5089,  1.6057, -2.0311, -0.5126, -0.9722,  0.1128]],
       dtype=torch.float64)
	q_value: tensor([[-37.1256]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41997415140444905, distance: 0.8715260391490156 entropy 0.9217694997787476
epoch: 18, step: 107
	action: tensor([[0.0248, 0.9412, 0.7221, 0.2276, 0.5192, 1.1058, 0.2020]],
       dtype=torch.float64)
	q_value: tensor([[-55.6960]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8989881357832558, distance: 0.36369965275183197 entropy 0.9217694997787476
epoch: 18, step: 108
	action: tensor([[ 0.1479, -0.3172,  0.4255, -0.4496,  0.2291, -0.7169, -0.3224]],
       dtype=torch.float64)
	q_value: tensor([[-26.6857]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3400302602990173, distance: 1.3246892015144816 entropy 0.9217694997787476
epoch: 18, step: 109
	action: tensor([[ 0.9363, -0.3979, -0.7748,  0.9626,  0.1522, -0.7264, -0.6331]],
       dtype=torch.float64)
	q_value: tensor([[-27.2634]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6427118804727561, distance: 0.6840155596575502 entropy 0.9217694997787476
epoch: 18, step: 110
	action: tensor([[ 0.2994, -0.3671,  0.2210, -0.8771, -0.2762, -0.2166,  0.6456]],
       dtype=torch.float64)
	q_value: tensor([[-37.7960]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3648491552418147, distance: 1.3369003005172337 entropy 0.9217694997787476
epoch: 18, step: 111
	action: tensor([[-0.8780, -0.2795,  0.8983, -0.6146,  0.2310, -0.7572, -1.6113]],
       dtype=torch.float64)
	q_value: tensor([[-27.8169]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.3709579314984932, distance: 1.762051580660957 entropy 0.9217694997787476
epoch: 18, step: 112
	action: tensor([[ 0.1653, -0.8666, -0.5288,  0.0535, -0.0096, -0.5264, -0.1416]],
       dtype=torch.float64)
	q_value: tensor([[-35.0866]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.45896411002897697, distance: 1.382225807922074 entropy 0.9217694997787476
epoch: 18, step: 113
	action: tensor([[ 0.7651, -1.3098,  0.1065, -0.6620,  0.0150, -0.4801, -1.4191]],
       dtype=torch.float64)
	q_value: tensor([[-27.1159]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 18, step: 114
	action: tensor([[-0.2657,  0.0601, -0.3688,  0.5502,  0.3456, -0.2505, -0.6745]],
       dtype=torch.float64)
	q_value: tensor([[-32.4677]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06974437724551219, distance: 1.103717287950134 entropy 0.9217694997787476
epoch: 18, step: 115
	action: tensor([[ 2.0122, -0.8694, -0.0201, -0.2591,  0.2602,  0.0702, -0.2441]],
       dtype=torch.float64)
	q_value: tensor([[-23.1241]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5621114912759224, distance: 1.430252471353737 entropy 0.9217694997787476
epoch: 18, step: 116
	action: tensor([[ 0.7824,  0.3345, -0.8771,  0.4935,  0.2207, -0.8805,  0.1307]],
       dtype=torch.float64)
	q_value: tensor([[-38.1721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9420560715826072, distance: 0.27546134336074185 entropy 0.9217694997787476
epoch: 18, step: 117
	action: tensor([[-0.5583, -0.1750,  0.3761, -0.0741, -0.2231, -1.1407, -0.2296]],
       dtype=torch.float64)
	q_value: tensor([[-31.1120]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5783525900394693, distance: 1.437668333351702 entropy 0.9217694997787476
epoch: 18, step: 118
	action: tensor([[-0.0775,  1.4551, -1.1887,  0.9633, -0.5420, -0.7846,  0.0016]],
       dtype=torch.float64)
	q_value: tensor([[-29.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 18, step: 119
	action: tensor([[ 0.1620, -0.5773,  0.0913, -0.6787,  0.3358,  0.1420,  0.3783]],
       dtype=torch.float64)
	q_value: tensor([[-32.4677]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.46503153369982586, distance: 1.3850969711957197 entropy 0.9217694997787476
epoch: 18, step: 120
	action: tensor([[-0.0811, -0.6199,  0.7418, -0.3951, -0.5949, -0.5704,  0.3714]],
       dtype=torch.float64)
	q_value: tensor([[-24.4361]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7287933414518297, distance: 1.5046248625234009 entropy 0.9217694997787476
epoch: 18, step: 121
	action: tensor([[ 0.7891,  0.0822,  0.6785, -0.4306, -1.4129,  0.2128, -0.3995]],
       dtype=torch.float64)
	q_value: tensor([[-31.4773]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6315237328206771, distance: 0.6946426643713316 entropy 0.9217694997787476
epoch: 18, step: 122
	action: tensor([[ 0.1358,  0.0433, -0.5222, -0.4134,  0.1318,  0.7184,  0.4098]],
       dtype=torch.float64)
	q_value: tensor([[-39.5319]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4756265010814924, distance: 0.8286613072080727 entropy 0.9217694997787476
epoch: 18, step: 123
	action: tensor([[-0.0364, -0.8430,  0.7739,  0.1059,  0.4353, -0.0256, -0.2504]],
       dtype=torch.float64)
	q_value: tensor([[-21.3243]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.43682854362282697, distance: 1.371700088823266 entropy 0.9217694997787476
epoch: 18, step: 124
	action: tensor([[ 0.0326,  0.2720,  0.2080,  0.3942, -0.0256, -0.3547,  0.7767]],
       dtype=torch.float64)
	q_value: tensor([[-31.1264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5776070248419582, distance: 0.7437295489308174 entropy 0.9217694997787476
epoch: 18, step: 125
	action: tensor([[ 0.3218,  0.2088, -0.1434,  0.4863,  0.8675,  0.2028, -0.0408]],
       dtype=torch.float64)
	q_value: tensor([[-24.4729]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8286729550680394, distance: 0.4736632116463616 entropy 0.9217694997787476
epoch: 18, step: 126
	action: tensor([[ 0.0884, -0.4156,  0.1063,  0.3438, -0.6417, -0.2183,  1.2637]],
       dtype=torch.float64)
	q_value: tensor([[-24.7569]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19903688946045195, distance: 1.0241485407286046 entropy 0.9217694997787476
epoch: 18, step: 127
	action: tensor([[-0.4371, -0.7478,  0.8567, -1.4856,  0.1125,  1.1199, -0.3367]],
       dtype=torch.float64)
	q_value: tensor([[-31.6261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0177125913702514, distance: 1.625497647732142 entropy 0.9217694997787476
LOSS epoch 18 actor 426.6977700334278 critic 1047.9317913596533 
epoch: 19, step: 0
	action: tensor([[-0.8723,  0.8887, -0.2738, -0.1499,  0.1782,  0.3950,  0.4647]],
       dtype=torch.float64)
	q_value: tensor([[-31.5853]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08047281448012611, distance: 1.1894977239142381 entropy 0.9217694997787476
epoch: 19, step: 1
	action: tensor([[ 0.2226, -1.3539,  0.3052, -0.2150, -0.0155, -0.1579,  0.1870]],
       dtype=torch.float64)
	q_value: tensor([[-19.4962]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9450863673425351, distance: 1.595975199046753 entropy 0.9217694997787476
epoch: 19, step: 2
	action: tensor([[-0.0982,  0.3710, -0.4940, -0.2870,  0.9369,  0.4714,  0.5416]],
       dtype=torch.float64)
	q_value: tensor([[-29.0978]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5347169837451986, distance: 0.7805761784327261 entropy 0.9217694997787476
epoch: 19, step: 3
	action: tensor([[-0.2894,  0.3206, -0.3806, -0.6186,  0.1076,  0.6330, -0.7321]],
       dtype=torch.float64)
	q_value: tensor([[-20.9511]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20704153509579937, distance: 1.019018135134653 entropy 0.9217694997787476
epoch: 19, step: 4
	action: tensor([[-0.7599,  1.3817,  0.3364, -0.2295, -0.1535, -0.3301,  0.3883]],
       dtype=torch.float64)
	q_value: tensor([[-19.5680]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 19, step: 5
	action: tensor([[ 0.2294, -0.4490, -0.2299, -0.5577,  0.2375,  0.6498, -0.0030]],
       dtype=torch.float64)
	q_value: tensor([[-28.9037]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.027769437998216073, distance: 1.1283434904666378 entropy 0.9217694997787476
epoch: 19, step: 6
	action: tensor([[-0.0868, -0.1386, -0.6190,  1.1290, -0.3422, -1.0389,  0.2877]],
       dtype=torch.float64)
	q_value: tensor([[-19.5169]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3059636815030884, distance: 0.9533399485330778 entropy 0.9217694997787476
epoch: 19, step: 7
	action: tensor([[ 0.0270, -0.7616,  0.4655,  1.1651,  1.3419,  0.3518, -0.1407]],
       dtype=torch.float64)
	q_value: tensor([[-29.6236]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6595997756875748, distance: 0.6676542410682912 entropy 0.9217694997787476
epoch: 19, step: 8
	action: tensor([[-0.2307, -0.1257, -0.0862,  0.1082, -0.6364,  1.4039, -1.1739]],
       dtype=torch.float64)
	q_value: tensor([[-37.3272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10769823973618453, distance: 1.0809673290909592 entropy 0.9217694997787476
epoch: 19, step: 9
	action: tensor([[ 0.2573,  0.1713, -0.0533, -0.4716, -0.4178, -0.0652,  0.3476]],
       dtype=torch.float64)
	q_value: tensor([[-27.8402]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.43160525103325587, distance: 0.8627435513872419 entropy 0.9217694997787476
epoch: 19, step: 10
	action: tensor([[-0.4631,  0.9904, -0.9715,  0.3794,  1.2750,  0.1495,  0.1372]],
       dtype=torch.float64)
	q_value: tensor([[-18.9553]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 19, step: 11
	action: tensor([[-0.2673,  0.5671, -0.2250, -1.5898, -0.0791,  0.9613,  0.2728]],
       dtype=torch.float64)
	q_value: tensor([[-28.9037]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19537449204653856, distance: 1.0264873257626352 entropy 0.9217694997787476
epoch: 19, step: 12
	action: tensor([[ 0.1937, -1.2395,  1.1615,  0.0193, -0.8051,  0.4717,  0.7565]],
       dtype=torch.float64)
	q_value: tensor([[-28.8380]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5052118776312549, distance: 1.4039625120163823 entropy 0.9217694997787476
epoch: 19, step: 13
	action: tensor([[ 1.1011,  0.7796, -0.6531, -0.4693, -0.4624,  0.2667,  0.5678]],
       dtype=torch.float64)
	q_value: tensor([[-37.9153]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9354540578751842, distance: 0.29073088170474526 entropy 0.9217694997787476
epoch: 19, step: 14
	action: tensor([[ 0.2330,  0.5746, -0.1983,  1.0317,  0.5168,  1.3076,  0.9673]],
       dtype=torch.float64)
	q_value: tensor([[-26.6332]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 19, step: 15
	action: tensor([[-0.5938,  0.1929, -0.3889, -0.9950,  0.7411, -0.0701, -0.9225]],
       dtype=torch.float64)
	q_value: tensor([[-28.9037]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.31347973498182347, distance: 1.3115002617445692 entropy 0.9217694997787476
epoch: 19, step: 16
	action: tensor([[ 0.7135, -0.6748, -0.4792, -0.2629,  0.5006, -0.4254,  0.1167]],
       dtype=torch.float64)
	q_value: tensor([[-20.5468]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23827158163207707, distance: 1.2733994157065247 entropy 0.9217694997787476
epoch: 19, step: 17
	action: tensor([[ 0.4641,  0.0258,  1.0566, -0.0492,  0.3374, -0.6524,  0.4144]],
       dtype=torch.float64)
	q_value: tensor([[-26.9326]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5026407842049219, distance: 0.8070339002416388 entropy 0.9217694997787476
epoch: 19, step: 18
	action: tensor([[-0.2533,  0.6536, -0.0201, -0.2993,  0.0101, -0.4062, -0.1590]],
       dtype=torch.float64)
	q_value: tensor([[-30.3729]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21934401962277983, distance: 1.0110823724082578 entropy 0.9217694997787476
epoch: 19, step: 19
	action: tensor([[ 0.6416,  0.4876, -0.1565, -0.2954,  0.6165,  0.6119,  0.2701]],
       dtype=torch.float64)
	q_value: tensor([[-17.0051]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9730406286942552, distance: 0.18789342083733837 entropy 0.9217694997787476
epoch: 19, step: 20
	action: tensor([[ 0.8590,  0.0374,  0.1155,  0.0298, -0.2061,  0.4832,  0.4643]],
       dtype=torch.float64)
	q_value: tensor([[-21.5342]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8954575219258107, distance: 0.37000116241261954 entropy 0.9217694997787476
epoch: 19, step: 21
	action: tensor([[-0.2577,  1.2094, -0.1833,  0.2546, -1.1627,  1.5797, -0.3227]],
       dtype=torch.float64)
	q_value: tensor([[-23.1478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11708150924239435, distance: 1.0752686883229425 entropy 0.9217694997787476
epoch: 19, step: 22
	action: tensor([[ 1.0492, -0.1875,  0.6123,  0.0685,  0.1810,  0.6568, -0.1975]],
       dtype=torch.float64)
	q_value: tensor([[-31.8330]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7196757875467896, distance: 0.6058805327621418 entropy 0.9217694997787476
epoch: 19, step: 23
	action: tensor([[ 1.4202, -0.1502, -0.2714,  0.1465, -0.5876, -0.4171, -0.0669]],
       dtype=torch.float64)
	q_value: tensor([[-28.8135]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3989149477460735, distance: 0.8872063822570839 entropy 0.9217694997787476
epoch: 19, step: 24
	action: tensor([[-0.9274,  1.1706, -0.4419,  0.6950,  1.0559,  0.1308, -0.5328]],
       dtype=torch.float64)
	q_value: tensor([[-30.1963]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 19, step: 25
	action: tensor([[-0.1140, -0.1616, -0.4538, -0.0825, -0.9577,  0.6438, -0.0299]],
       dtype=torch.float64)
	q_value: tensor([[-28.9037]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10775002848280568, distance: 1.2044189451176845 entropy 0.9217694997787476
epoch: 19, step: 26
	action: tensor([[ 0.6603,  0.1581, -0.1915,  0.5237, -0.1081, -0.7419,  0.1978]],
       dtype=torch.float64)
	q_value: tensor([[-20.5912]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.885619199687002, distance: 0.3870198664560885 entropy 0.9217694997787476
epoch: 19, step: 27
	action: tensor([[-0.5024, -1.1701,  0.6342, -0.4861,  0.1717,  0.7778, -0.4899]],
       dtype=torch.float64)
	q_value: tensor([[-25.2270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.978209484910932, distance: 1.609506864172912 entropy 0.9217694997787476
epoch: 19, step: 28
	action: tensor([[ 0.7387, -0.3402, -0.0893, -0.2077, -0.3705,  0.8967, -0.0663]],
       dtype=torch.float64)
	q_value: tensor([[-27.4776]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5552925190405942, distance: 0.7631218891845727 entropy 0.9217694997787476
epoch: 19, step: 29
	action: tensor([[-0.4301, -0.8552, -0.0301,  0.1558, -0.8373,  0.1763,  0.5725]],
       dtype=torch.float64)
	q_value: tensor([[-24.1215]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7931333499919804, distance: 1.5323676933346901 entropy 0.9217694997787476
epoch: 19, step: 30
	action: tensor([[ 1.0998, -0.6448,  0.9397,  0.2468, -0.1145, -0.4410, -0.1930]],
       dtype=torch.float64)
	q_value: tensor([[-24.8859]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10595947891433155, distance: 1.082020016077103 entropy 0.9217694997787476
epoch: 19, step: 31
	action: tensor([[-0.6652, -0.2107,  0.0455,  0.5815,  0.5261,  0.3563,  0.8898]],
       dtype=torch.float64)
	q_value: tensor([[-37.0844]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.22767448393077294, distance: 1.267938847331064 entropy 0.9217694997787476
epoch: 19, step: 32
	action: tensor([[ 0.1290, -1.0498,  0.5474, -0.0845,  0.4435,  0.1934,  0.8493]],
       dtype=torch.float64)
	q_value: tensor([[-25.7397]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5749807984316471, distance: 1.4361318865127688 entropy 0.9217694997787476
epoch: 19, step: 33
	action: tensor([[ 1.0351, -0.2563, -0.3944,  0.2099, -0.4777, -0.7571, -0.0359]],
       dtype=torch.float64)
	q_value: tensor([[-31.9343]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.47686830730922125, distance: 0.8276795196746033 entropy 0.9217694997787476
epoch: 19, step: 34
	action: tensor([[-0.5545, -0.2868, -0.3207, -0.4606, -0.7244, -0.1902, -0.1963]],
       dtype=torch.float64)
	q_value: tensor([[-28.6653]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.589321813989079, distance: 1.4426554322640481 entropy 0.9217694997787476
epoch: 19, step: 35
	action: tensor([[ 0.0796,  0.7434, -1.0205, -0.4194, -0.6595,  0.2313, -0.0962]],
       dtype=torch.float64)
	q_value: tensor([[-19.8998]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7670071731834165, distance: 0.5523673101689073 entropy 0.9217694997787476
epoch: 19, step: 36
	action: tensor([[-0.2018, -0.0701, -0.3362, -0.1963,  0.7777, -0.2018,  0.5613]],
       dtype=torch.float64)
	q_value: tensor([[-21.8976]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13173350877708012, distance: 1.217387338906954 entropy 0.9217694997787476
epoch: 19, step: 37
	action: tensor([[ 0.4796,  0.1895,  0.2720, -0.3762, -0.5750, -0.1625, -0.2240]],
       dtype=torch.float64)
	q_value: tensor([[-21.2324]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5483662188775951, distance: 0.7690417234882322 entropy 0.9217694997787476
epoch: 19, step: 38
	action: tensor([[ 0.3257,  0.1505,  0.0172, -0.9314, -0.4716, -0.3868,  0.8335]],
       dtype=torch.float64)
	q_value: tensor([[-22.2908]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18757390122736817, distance: 1.0314510599451845 entropy 0.9217694997787476
epoch: 19, step: 39
	action: tensor([[-0.6604,  0.4314,  0.3053,  0.9949, -1.2390, -0.0039, -0.2638]],
       dtype=torch.float64)
	q_value: tensor([[-24.8489]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3927976895460382, distance: 0.8917095153400846 entropy 0.9217694997787476
epoch: 19, step: 40
	action: tensor([[-0.7890, -0.1423, -0.3303,  0.2575, -1.5392, -0.4038, -0.1398]],
       dtype=torch.float64)
	q_value: tensor([[-24.6114]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5861874924992276, distance: 1.441232190780505 entropy 0.9217694997787476
epoch: 19, step: 41
	action: tensor([[ 0.9868,  0.1868,  0.3248, -0.7526,  0.5315, -0.0526,  0.7499]],
       dtype=torch.float64)
	q_value: tensor([[-26.5045]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5522602620514658, distance: 0.7657191589360197 entropy 0.9217694997787476
epoch: 19, step: 42
	action: tensor([[-0.8175, -0.1489, -0.3510,  0.0867,  0.3793, -0.8210,  0.2348]],
       dtype=torch.float64)
	q_value: tensor([[-28.4805]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9372501436294731, distance: 1.5927570796389512 entropy 0.9217694997787476
epoch: 19, step: 43
	action: tensor([[ 0.1291,  0.3781,  0.7739, -0.0573, -0.1364,  0.2998,  0.8790]],
       dtype=torch.float64)
	q_value: tensor([[-24.0049]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6515023773824737, distance: 0.6755486075669372 entropy 0.9217694997787476
epoch: 19, step: 44
	action: tensor([[ 0.3182,  0.7971,  0.6167,  0.0252, -0.3229, -0.2147, -1.2805]],
       dtype=torch.float64)
	q_value: tensor([[-23.1794]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8174374559098428, distance: 0.48894784108461076 entropy 0.9217694997787476
epoch: 19, step: 45
	action: tensor([[ 0.3412,  0.1760,  0.4000, -1.2045,  0.3688, -0.1123,  0.8659]],
       dtype=torch.float64)
	q_value: tensor([[-28.4144]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.003101014874636876, distance: 1.1425685620426458 entropy 0.9217694997787476
epoch: 19, step: 46
	action: tensor([[ 0.1077,  0.2512, -0.3086,  0.5262,  0.0539, -0.6733,  1.6760]],
       dtype=torch.float64)
	q_value: tensor([[-26.5525]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5632081280518322, distance: 0.7562997683360921 entropy 0.9217694997787476
epoch: 19, step: 47
	action: tensor([[-0.3264,  0.8661,  0.2032,  1.1618, -0.0341, -0.0176, -0.0983]],
       dtype=torch.float64)
	q_value: tensor([[-30.0313]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 19, step: 48
	action: tensor([[ 0.2390,  0.4236, -0.5677,  0.5995, -0.5529,  0.3713,  0.2383]],
       dtype=torch.float64)
	q_value: tensor([[-28.9037]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5467508820586734, distance: 0.7704157932112833 entropy 0.9217694997787476
epoch: 19, step: 49
	action: tensor([[-0.3208,  0.0918,  0.0194, -0.1827,  0.7795, -1.0660,  0.4072]],
       dtype=torch.float64)
	q_value: tensor([[-18.7597]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3534172306786312, distance: 1.3312896140779154 entropy 0.9217694997787476
epoch: 19, step: 50
	action: tensor([[ 0.1875, -0.0845,  0.6058, -1.7677,  0.7212, -0.2153,  0.2110]],
       dtype=torch.float64)
	q_value: tensor([[-26.7827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4597264802126557, distance: 1.382586896351967 entropy 0.9217694997787476
epoch: 19, step: 51
	action: tensor([[-0.1030, -0.0427, -0.5501, -0.1690,  0.4031,  0.1829,  0.4055]],
       dtype=torch.float64)
	q_value: tensor([[-29.2963]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15812931112936246, distance: 1.0499760376990757 entropy 0.9217694997787476
epoch: 19, step: 52
	action: tensor([[-0.6797,  0.1814, -0.3186,  1.0130,  0.1005,  0.0920, -0.7465]],
       dtype=torch.float64)
	q_value: tensor([[-17.5729]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07333068379866625, distance: 1.1855598024939356 entropy 0.9217694997787476
epoch: 19, step: 53
	action: tensor([[ 0.2243,  0.5471,  1.6962, -0.6484, -0.6205,  0.2920, -0.3181]],
       dtype=torch.float64)
	q_value: tensor([[-21.9866]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.49302365573717566, distance: 0.8147991006885563 entropy 0.9217694997787476
epoch: 19, step: 54
	action: tensor([[-0.6598, -0.1882, -0.2128,  0.4455, -0.1003, -0.5041,  0.3786]],
       dtype=torch.float64)
	q_value: tensor([[-34.4728]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5981662214177867, distance: 1.4466639755019588 entropy 0.9217694997787476
epoch: 19, step: 55
	action: tensor([[ 0.0761,  0.6324,  0.2885,  0.4564,  0.0146, -0.4292,  0.1020]],
       dtype=torch.float64)
	q_value: tensor([[-21.6423]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6870202710009937, distance: 0.640198715378893 entropy 0.9217694997787476
epoch: 19, step: 56
	action: tensor([[-0.4726,  0.5837,  0.1943, -0.2190, -0.9764, -1.0487,  0.9341]],
       dtype=torch.float64)
	q_value: tensor([[-19.9209]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16156377046709158, distance: 1.0478321304468026 entropy 0.9217694997787476
epoch: 19, step: 57
	action: tensor([[-0.6068,  0.2298,  0.0885,  0.0219,  0.4115, -0.5296,  0.2695]],
       dtype=torch.float64)
	q_value: tensor([[-27.6547]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.46568916044516584, distance: 1.3854078090841917 entropy 0.9217694997787476
epoch: 19, step: 58
	action: tensor([[-0.0013,  0.7710, -0.8642,  0.6213, -0.3826, -1.0345, -0.7177]],
       dtype=torch.float64)
	q_value: tensor([[-19.6586]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6564224751539929, distance: 0.6707629508216134 entropy 0.9217694997787476
epoch: 19, step: 59
	action: tensor([[-0.2140, -0.0277,  0.1452,  0.6187, -0.0420, -0.0846,  0.9063]],
       dtype=torch.float64)
	q_value: tensor([[-26.0895]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32660536132739304, distance: 0.939056061719465 entropy 0.9217694997787476
epoch: 19, step: 60
	action: tensor([[ 0.5591,  1.1006,  0.0981, -0.0490,  0.2586,  0.5559,  1.0024]],
       dtype=torch.float64)
	q_value: tensor([[-23.1989]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 19, step: 61
	action: tensor([[ 0.5705,  0.4271,  0.9996, -0.7564,  0.2767, -1.2402, -0.3393]],
       dtype=torch.float64)
	q_value: tensor([[-28.9037]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.426481776140167, distance: 0.8666231866424345 entropy 0.9217694997787476
epoch: 19, step: 62
	action: tensor([[ 0.6944,  0.4860,  0.4470, -0.5104, -0.1488, -0.1684, -0.8452]],
       dtype=torch.float64)
	q_value: tensor([[-33.9629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7792670732136249, distance: 0.5376383680648791 entropy 0.9217694997787476
epoch: 19, step: 63
	action: tensor([[-0.7218, -0.1579,  0.1023, -0.8105, -0.6610, -0.7197, -0.4771]],
       dtype=torch.float64)
	q_value: tensor([[-26.9186]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6933143564684494, distance: 1.4891055713727481 entropy 0.9217694997787476
epoch: 19, step: 64
	action: tensor([[ 1.7121, -0.2300, -0.5285,  0.9256,  0.0451,  1.1587, -0.3011]],
       dtype=torch.float64)
	q_value: tensor([[-23.7245]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9224142258038093, distance: 0.3187482010188847 entropy 0.9217694997787476
epoch: 19, step: 65
	action: tensor([[ 0.3907,  0.7139,  0.2447,  1.5008, -0.4054, -0.1965, -0.4083]],
       dtype=torch.float64)
	q_value: tensor([[-35.7642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7205944347452031, distance: 0.6048869560802603 entropy 0.9217694997787476
epoch: 19, step: 66
	action: tensor([[-0.3904,  0.7942,  0.5216, -0.3901,  0.5037,  0.9621, -0.8185]],
       dtype=torch.float64)
	q_value: tensor([[-28.1002]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22911186210303047, distance: 1.0047369515793498 entropy 0.9217694997787476
epoch: 19, step: 67
	action: tensor([[ 0.0982,  0.0188, -0.7762,  0.4319, -0.7252, -0.2548,  0.1818]],
       dtype=torch.float64)
	q_value: tensor([[-24.1163]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13582915377856097, distance: 1.0637914579018515 entropy 0.9217694997787476
epoch: 19, step: 68
	action: tensor([[ 0.4031, -0.7471,  0.9481, -0.7563, -1.2602, -1.0888, -0.2807]],
       dtype=torch.float64)
	q_value: tensor([[-19.6798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5394651350030122, distance: 1.419847241759673 entropy 0.9217694997787476
epoch: 19, step: 69
	action: tensor([[ 0.0409,  0.7979, -0.0578,  0.2580,  0.0421, -0.6594, -0.9939]],
       dtype=torch.float64)
	q_value: tensor([[-39.2514]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6332194828037966, distance: 0.6930424274539119 entropy 0.9217694997787476
epoch: 19, step: 70
	action: tensor([[ 0.5333,  0.2449, -0.2260, -1.2247, -0.5722,  0.7174, -0.1429]],
       dtype=torch.float64)
	q_value: tensor([[-23.8418]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4775444042615111, distance: 0.8271444990192522 entropy 0.9217694997787476
epoch: 19, step: 71
	action: tensor([[ 0.8646,  0.2321, -0.2038,  0.8244,  0.7455, -0.6898,  0.0887]],
       dtype=torch.float64)
	q_value: tensor([[-27.7589]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9698587635174486, distance: 0.1986722569490052 entropy 0.9217694997787476
epoch: 19, step: 72
	action: tensor([[ 0.3553, -0.3058, -0.6172, -0.5132,  0.0305,  0.3586,  0.3849]],
       dtype=torch.float64)
	q_value: tensor([[-31.3207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21888060293014555, distance: 1.0113824296327778 entropy 0.9217694997787476
epoch: 19, step: 73
	action: tensor([[ 0.5799, -0.2275,  0.6158,  0.1018,  0.2367, -0.3176, -0.2589]],
       dtype=torch.float64)
	q_value: tensor([[-19.6795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4854398381906153, distance: 0.8208707351587426 entropy 0.9217694997787476
epoch: 19, step: 74
	action: tensor([[-0.2769, -0.0785, -0.7466,  0.8022,  0.0496, -0.4218,  0.2306]],
       dtype=torch.float64)
	q_value: tensor([[-26.2780]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12285350466730849, distance: 1.2126018952882869 entropy 0.9217694997787476
epoch: 19, step: 75
	action: tensor([[ 0.3290, -0.0611, -0.1349,  0.1825, -1.6696,  0.3229, -0.3277]],
       dtype=torch.float64)
	q_value: tensor([[-22.3230]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5153897281340831, distance: 0.7966232930315296 entropy 0.9217694997787476
epoch: 19, step: 76
	action: tensor([[ 0.7334, -0.5713, -0.8722,  0.3228,  0.3897,  0.8313,  0.2977]],
       dtype=torch.float64)
	q_value: tensor([[-28.9986]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5576554121123438, distance: 0.7610918164750411 entropy 0.9217694997787476
epoch: 19, step: 77
	action: tensor([[-0.1381, -0.7551,  0.0049, -0.0247,  0.0093, -0.3100, -1.4736]],
       dtype=torch.float64)
	q_value: tensor([[-25.8613]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5942767349623082, distance: 1.4449025167955525 entropy 0.9217694997787476
epoch: 19, step: 78
	action: tensor([[ 0.4694, -0.0368, -0.4167,  0.5952, -0.2299, -0.9711,  0.4920]],
       dtype=torch.float64)
	q_value: tensor([[-27.9051]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7148714231936663, distance: 0.6110504474331733 entropy 0.9217694997787476
epoch: 19, step: 79
	action: tensor([[ 0.6661, -0.6417, -0.1651,  0.6495, -0.2145,  0.1852, -0.1574]],
       dtype=torch.float64)
	q_value: tensor([[-27.8663]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.589292184305817, distance: 0.7333700620145627 entropy 0.9217694997787476
epoch: 19, step: 80
	action: tensor([[ 0.8042, -0.6929,  0.0601, -0.1553, -0.0443,  0.8615, -1.1373]],
       dtype=torch.float64)
	q_value: tensor([[-25.2857]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3043979251675457, distance: 0.9544147173064109 entropy 0.9217694997787476
epoch: 19, step: 81
	action: tensor([[-0.5211,  0.0023, -0.7307, -0.8419,  0.3313,  1.6176,  0.0060]],
       dtype=torch.float64)
	q_value: tensor([[-29.7801]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06623712356984135, distance: 1.1057959503369243 entropy 0.9217694997787476
epoch: 19, step: 82
	action: tensor([[ 1.5431,  0.5072, -0.0481, -0.0187, -0.8485,  0.0260,  0.2282]],
       dtype=torch.float64)
	q_value: tensor([[-27.8161]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7334528863869494, distance: 0.5908043513248691 entropy 0.9217694997787476
epoch: 19, step: 83
	action: tensor([[ 0.0687, -0.4931, -0.5800,  1.2333,  0.3893,  0.6613, -0.2034]],
       dtype=torch.float64)
	q_value: tensor([[-30.2692]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4229021348085318, distance: 0.8693235146043681 entropy 0.9217694997787476
epoch: 19, step: 84
	action: tensor([[-0.8081,  0.4941, -0.3913, -1.3134, -0.4455,  0.4278,  0.1894]],
       dtype=torch.float64)
	q_value: tensor([[-27.2615]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.411540286662176, distance: 1.359575507297725 entropy 0.9217694997787476
epoch: 19, step: 85
	action: tensor([[ 0.5564, -0.6388, -0.5395, -0.3353,  0.5032,  0.3680, -0.6365]],
       dtype=torch.float64)
	q_value: tensor([[-25.4587]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.005343370412605353, distance: 1.1473975083809982 entropy 0.9217694997787476
epoch: 19, step: 86
	action: tensor([[-0.2863,  1.1367, -0.3275,  0.6397, -0.9631, -0.9393, -0.0506]],
       dtype=torch.float64)
	q_value: tensor([[-22.8848]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 19, step: 87
	action: tensor([[ 0.0790, -0.2956, -1.2056,  0.0644, -0.3483, -0.0921,  0.2924]],
       dtype=torch.float64)
	q_value: tensor([[-28.9037]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12973267675798839, distance: 1.0675372347074281 entropy 0.9217694997787476
epoch: 19, step: 88
	action: tensor([[-1.0149, -0.5035, -0.2198, -0.1229, -1.0554, -0.1406, -0.3023]],
       dtype=torch.float64)
	q_value: tensor([[-20.5285]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2898260050584747, distance: 1.7316412984783462 entropy 0.9217694997787476
epoch: 19, step: 89
	action: tensor([[ 0.9445, -0.9289, -0.2862,  0.3229, -0.3771,  0.0764,  0.9262]],
       dtype=torch.float64)
	q_value: tensor([[-23.6861]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.049604687899940214, distance: 1.1156008487232025 entropy 0.9217694997787476
epoch: 19, step: 90
	action: tensor([[ 0.0546,  0.3601,  0.1477, -0.8140,  0.2661, -1.1767, -0.3178]],
       dtype=torch.float64)
	q_value: tensor([[-31.1499]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08227594320922227, distance: 1.0962579387144067 entropy 0.9217694997787476
epoch: 19, step: 91
	action: tensor([[ 0.4271, -0.6015,  0.3113, -0.1490,  1.5697, -0.1293,  0.1563]],
       dtype=torch.float64)
	q_value: tensor([[-25.5672]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.112914018994416, distance: 1.207222996670454 entropy 0.9217694997787476
epoch: 19, step: 92
	action: tensor([[ 0.3796,  0.0367,  0.3404, -0.7468,  1.0074, -0.0455,  0.2064]],
       dtype=torch.float64)
	q_value: tensor([[-33.0069]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16390103013423873, distance: 1.0463706210048174 entropy 0.9217694997787476
epoch: 19, step: 93
	action: tensor([[-0.2659,  0.3572,  0.0141, -1.4436,  0.2552, -0.5812, -1.0656]],
       dtype=torch.float64)
	q_value: tensor([[-24.2645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2358269183383226, distance: 1.2721417873903977 entropy 0.9217694997787476
epoch: 19, step: 94
	action: tensor([[ 0.1395,  0.5202,  0.2253, -0.1211,  0.2673,  0.3079, -0.0621]],
       dtype=torch.float64)
	q_value: tensor([[-25.3744]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6817658241065192, distance: 0.64555032204162 entropy 0.9217694997787476
epoch: 19, step: 95
	action: tensor([[-0.0083, -0.8093, -0.1006, -1.2355, -0.2232, -0.3376, -0.7794]],
       dtype=torch.float64)
	q_value: tensor([[-16.9999]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6268569032688911, distance: 1.4595916510875115 entropy 0.9217694997787476
epoch: 19, step: 96
	action: tensor([[ 0.9375, -0.7841,  0.5102,  0.5833,  0.3601,  0.4628, -1.1205]],
       dtype=torch.float64)
	q_value: tensor([[-26.1150]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4359952450109187, distance: 0.8594053956015545 entropy 0.9217694997787476
epoch: 19, step: 97
	action: tensor([[ 0.3315, -1.3071, -0.2853,  0.1823,  0.4281, -0.1609,  0.4995]],
       dtype=torch.float64)
	q_value: tensor([[-35.6074]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6692888812168301, distance: 1.4785037949649864 entropy 0.9217694997787476
epoch: 19, step: 98
	action: tensor([[-0.8025,  0.4858,  0.0571,  0.5385,  0.3334,  1.0312,  0.3759]],
       dtype=torch.float64)
	q_value: tensor([[-29.8910]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3625015869267284, distance: 0.9136844785355449 entropy 0.9217694997787476
epoch: 19, step: 99
	action: tensor([[ 1.0944, -0.3878,  0.0046, -0.5373, -0.2328,  0.5092,  0.4499]],
       dtype=torch.float64)
	q_value: tensor([[-22.0410]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21005983918566495, distance: 1.0170768991465295 entropy 0.9217694997787476
epoch: 19, step: 100
	action: tensor([[ 0.9929, -0.5822,  1.3817,  0.2230, -0.7215, -0.4743,  0.5451]],
       dtype=torch.float64)
	q_value: tensor([[-26.8971]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09982838571449448, distance: 1.085723780472253 entropy 0.9217694997787476
epoch: 19, step: 101
	action: tensor([[ 1.1171, -0.2582, -0.1792, -0.1582, -1.0317, -0.4168,  0.6531]],
       dtype=torch.float64)
	q_value: tensor([[-40.5479]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30838131922631074, distance: 0.9516780460845059 entropy 0.9217694997787476
epoch: 19, step: 102
	action: tensor([[ 0.3653,  0.9092, -0.5066,  0.7523,  0.1687, -0.8551,  1.3080]],
       dtype=torch.float64)
	q_value: tensor([[-29.5948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 19, step: 103
	action: tensor([[ 1.0713, -0.1208,  0.4243, -0.1821,  0.4357, -0.9895,  0.8332]],
       dtype=torch.float64)
	q_value: tensor([[-28.9037]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.45662823721397827, distance: 0.8435391159979626 entropy 0.9217694997787476
epoch: 19, step: 104
	action: tensor([[-0.8123,  1.0262,  0.5326,  0.1806, -0.4673,  0.3093,  0.2707]],
       dtype=torch.float64)
	q_value: tensor([[-35.7607]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 19, step: 105
	action: tensor([[ 0.8386, -0.4643,  0.4019, -0.4626, -0.1539,  0.1106,  0.8761]],
       dtype=torch.float64)
	q_value: tensor([[-28.9037]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06611438635393241, distance: 1.105868622885848 entropy 0.9217694997787476
epoch: 19, step: 106
	action: tensor([[-0.8006, -0.6098, -0.5202, -0.2682, -0.6698,  0.4592,  0.8985]],
       dtype=torch.float64)
	q_value: tensor([[-28.4243]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.169283688907409, distance: 1.6854461006228167 entropy 0.9217694997787476
epoch: 19, step: 107
	action: tensor([[-0.1110,  0.9195, -1.5242,  0.5378, -0.6936,  0.0266,  0.3888]],
       dtype=torch.float64)
	q_value: tensor([[-24.5383]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15688159558055115, distance: 1.0507538212700882 entropy 0.9217694997787476
epoch: 19, step: 108
	action: tensor([[-1.0604,  0.6443,  0.2416,  0.1791,  0.5870,  0.3491,  0.3957]],
       dtype=torch.float64)
	q_value: tensor([[-23.0485]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.047169743342434955, distance: 1.171022489909126 entropy 0.9217694997787476
epoch: 19, step: 109
	action: tensor([[ 0.3734,  0.2257,  1.4988,  0.3386,  0.5745, -0.1442, -0.0967]],
       dtype=torch.float64)
	q_value: tensor([[-21.1072]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8942756450577951, distance: 0.37208675875464103 entropy 0.9217694997787476
epoch: 19, step: 110
	action: tensor([[-0.4106, -0.8453, -0.9786,  0.3105, -0.3015, -0.3598,  0.7076]],
       dtype=torch.float64)
	q_value: tensor([[-32.8876]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7118968409922066, distance: 1.497254022507128 entropy 0.9217694997787476
epoch: 19, step: 111
	action: tensor([[-0.3220, -0.2182, -0.6967, -0.5355, -0.6451, -0.3602, -0.2019]],
       dtype=torch.float64)
	q_value: tensor([[-26.7913]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04097126918740701, distance: 1.1206564686409082 entropy 0.9217694997787476
epoch: 19, step: 112
	action: tensor([[-0.0563, -0.4885, -0.6635,  0.5022, -0.9777, -0.4389, -0.0612]],
       dtype=torch.float64)
	q_value: tensor([[-20.6073]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12676485293518425, distance: 1.0693559666139483 entropy 0.9217694997787476
epoch: 19, step: 113
	action: tensor([[ 0.1722, -0.0813, -0.7385,  0.1965,  0.0988,  0.1814,  0.5786]],
       dtype=torch.float64)
	q_value: tensor([[-24.0051]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5033269986747851, distance: 0.8064769692796481 entropy 0.9217694997787476
epoch: 19, step: 114
	action: tensor([[ 0.0807, -0.1049, -0.2336,  0.0301,  1.1264, -0.2632, -0.4149]],
       dtype=torch.float64)
	q_value: tensor([[-19.0952]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31096172436198843, distance: 0.9499010482853107 entropy 0.9217694997787476
epoch: 19, step: 115
	action: tensor([[-0.0748, -0.6906, -0.1961, -0.2700,  0.4113,  0.3163, -0.6860]],
       dtype=torch.float64)
	q_value: tensor([[-23.7100]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.32720881071562813, distance: 1.318336636672307 entropy 0.9217694997787476
epoch: 19, step: 116
	action: tensor([[-0.7405,  0.3057, -0.3992, -1.3488, -0.2007, -1.0453, -0.2594]],
       dtype=torch.float64)
	q_value: tensor([[-20.6653]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10370729935193412, distance: 1.083382018434175 entropy 0.9217694997787476
epoch: 19, step: 117
	action: tensor([[ 0.9276,  0.1920,  0.6955, -1.6297,  0.0060,  0.2767,  0.2328]],
       dtype=torch.float64)
	q_value: tensor([[-24.9341]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31110528422895234, distance: 0.9498020880557624 entropy 0.9217694997787476
epoch: 19, step: 118
	action: tensor([[ 0.6622, -1.1749, -0.0102,  1.1400, -0.3681,  0.3323, -0.4048]],
       dtype=torch.float64)
	q_value: tensor([[-32.6622]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5517315169683782, distance: 0.7661711523002798 entropy 0.9217694997787476
epoch: 19, step: 119
	action: tensor([[ 0.7651, -0.1874, -0.0084,  0.6019, -0.9423,  0.0765,  0.4080]],
       dtype=torch.float64)
	q_value: tensor([[-34.7392]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8906068522639653, distance: 0.3784876846310325 entropy 0.9217694997787476
epoch: 19, step: 120
	action: tensor([[-0.1049, -0.3422, -0.1480,  0.7927, -0.1811,  0.4675,  0.3824]],
       dtype=torch.float64)
	q_value: tensor([[-25.4471]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.48891793176063714, distance: 0.8180917537966285 entropy 0.9217694997787476
epoch: 19, step: 121
	action: tensor([[ 1.3632, -0.3524, -0.4326,  0.2288,  0.2393, -0.1768, -0.4301]],
       dtype=torch.float64)
	q_value: tensor([[-21.6343]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30263708745243967, distance: 0.9556219500707631 entropy 0.9217694997787476
epoch: 19, step: 122
	action: tensor([[ 0.5833, -0.1051,  0.4465, -0.8420,  0.8981, -0.1438, -0.3414]],
       dtype=torch.float64)
	q_value: tensor([[-30.8368]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13145551336928918, distance: 1.0664800288751746 entropy 0.9217694997787476
epoch: 19, step: 123
	action: tensor([[-1.1975, -0.6997,  0.2623, -1.5438, -0.3452, -0.4466,  0.8874]],
       dtype=torch.float64)
	q_value: tensor([[-25.8072]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5817791738723663, distance: 1.4392280673010218 entropy 0.9217694997787476
epoch: 19, step: 124
	action: tensor([[ 0.9336, -0.2769,  0.4555, -0.6106,  0.3218,  0.2985,  0.9299]],
       dtype=torch.float64)
	q_value: tensor([[-32.9248]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19867663632947463, distance: 1.0243788330082582 entropy 0.9217694997787476
epoch: 19, step: 125
	action: tensor([[ 0.1720,  0.6087, -0.0433, -0.1728, -1.1683,  0.2247,  0.6896]],
       dtype=torch.float64)
	q_value: tensor([[-29.3667]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7747453515887374, distance: 0.5431172219439356 entropy 0.9217694997787476
epoch: 19, step: 126
	action: tensor([[-0.0250, -0.4439,  0.5795,  0.8467, -0.1739,  0.7678, -0.8747]],
       dtype=torch.float64)
	q_value: tensor([[-24.6971]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8069318596369486, distance: 0.5028193705060986 entropy 0.9217694997787476
epoch: 19, step: 127
	action: tensor([[ 0.4445,  0.0812, -0.2955,  0.4750,  0.2207, -0.4453,  1.4271]],
       dtype=torch.float64)
	q_value: tensor([[-28.3243]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8216740381831759, distance: 0.48324122972255473 entropy 0.9217694997787476
LOSS epoch 19 actor 318.1556301607046 critic 198.0807053083616 
epoch: 20, step: 0
	action: tensor([[-0.0233, -0.8792, -0.3015,  0.1594,  0.2434,  0.8637, -0.4699]],
       dtype=torch.float64)
	q_value: tensor([[-24.7085]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.011269399234464128, distance: 1.150774225403361 entropy 0.8164090514183044
epoch: 20, step: 1
	action: tensor([[-0.2689,  0.1542,  0.0615, -0.0858, -0.1241,  0.1461, -0.0074]],
       dtype=torch.float64)
	q_value: tensor([[-20.6386]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24164371672694607, distance: 0.9965367932675264 entropy 0.8164090514183044
epoch: 20, step: 2
	action: tensor([[-0.6331, -0.1770,  0.4811,  0.2061, -0.1375, -1.0616,  0.3004]],
       dtype=torch.float64)
	q_value: tensor([[-13.4923]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4710748151701727, distance: 1.387950806174277 entropy 0.8164090514183044
epoch: 20, step: 3
	action: tensor([[ 0.2820, -0.2977, -0.3390,  0.5413,  0.6761, -0.0149,  0.3185]],
       dtype=torch.float64)
	q_value: tensor([[-24.2502]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.58317409635889, distance: 0.7388121746850603 entropy 0.8164090514183044
epoch: 20, step: 4
	action: tensor([[ 0.1557, -0.1408,  0.2600,  1.2095,  0.8218, -0.3820,  0.6395]],
       dtype=torch.float64)
	q_value: tensor([[-21.3260]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.862233422779344, distance: 0.4247453446608568 entropy 0.8164090514183044
epoch: 20, step: 5
	action: tensor([[ 1.1435,  0.3604, -0.7937, -0.3439, -0.2468,  0.3200,  0.6932]],
       dtype=torch.float64)
	q_value: tensor([[-28.8316]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.774148808718373, distance: 0.5438359162851019 entropy 0.8164090514183044
epoch: 20, step: 6
	action: tensor([[ 0.2083,  0.2957, -0.0964,  0.0638,  0.8743, -0.0337, -0.7359]],
       dtype=torch.float64)
	q_value: tensor([[-22.6181]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7647407746194999, distance: 0.5550473386466646 entropy 0.8164090514183044
epoch: 20, step: 7
	action: tensor([[ 0.5990, -0.1064, -0.7097, -0.3662, -0.1374,  0.3745, -0.1535]],
       dtype=torch.float64)
	q_value: tensor([[-18.7174]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6009510204042154, distance: 0.7228859694574726 entropy 0.8164090514183044
epoch: 20, step: 8
	action: tensor([[-0.0628,  1.0021, -0.7527, -1.0879,  0.8696,  0.2283,  0.2207]],
       dtype=torch.float64)
	q_value: tensor([[-17.2542]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9315619054542232, distance: 0.299368187513306 entropy 0.8164090514183044
epoch: 20, step: 9
	action: tensor([[-0.5353,  0.1412, -0.2979, -0.3876, -0.0228,  0.5129, -1.0554]],
       dtype=torch.float64)
	q_value: tensor([[-21.1834]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07831543290506038, distance: 1.1883095946511824 entropy 0.8164090514183044
epoch: 20, step: 10
	action: tensor([[-0.3606,  0.1956,  0.4208, -0.3495, -1.0976, -0.2744,  0.2515]],
       dtype=torch.float64)
	q_value: tensor([[-17.7572]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.016063667218906508, distance: 1.1534988191350937 entropy 0.8164090514183044
epoch: 20, step: 11
	action: tensor([[ 0.1817,  0.8576, -0.9619,  0.7152,  0.9766,  0.7437, -0.7691]],
       dtype=torch.float64)
	q_value: tensor([[-21.2133]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5611017905984187, distance: 0.7581211239317998 entropy 0.8164090514183044
epoch: 20, step: 12
	action: tensor([[ 0.7699,  0.3993, -1.1800, -0.5939,  0.0939,  0.6053, -0.6573]],
       dtype=torch.float64)
	q_value: tensor([[-24.9162]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8921975688254025, distance: 0.3757257593859918 entropy 0.8164090514183044
epoch: 20, step: 13
	action: tensor([[-0.1808, -0.5337,  0.1020, -0.1981, -0.1751, -0.7043,  0.8647]],
       dtype=torch.float64)
	q_value: tensor([[-21.9187]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5205425859950104, distance: 1.4110941365430296 entropy 0.8164090514183044
epoch: 20, step: 14
	action: tensor([[-0.2237, -0.3595,  0.2742, -1.2252, -0.5865,  0.1209,  0.3101]],
       dtype=torch.float64)
	q_value: tensor([[-23.2722]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7568533799326493, distance: 1.5167864926705967 entropy 0.8164090514183044
epoch: 20, step: 15
	action: tensor([[-0.4306, -0.7819,  0.2184,  0.6515, -0.8591, -0.6687, -0.3915]],
       dtype=torch.float64)
	q_value: tensor([[-22.7280]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.359705683796953, distance: 1.3343788500261666 entropy 0.8164090514183044
epoch: 20, step: 16
	action: tensor([[-0.1495,  0.5292,  0.4635,  0.1529, -0.4279,  0.7199, -0.7331]],
       dtype=torch.float64)
	q_value: tensor([[-25.2751]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5475800158112856, distance: 0.7697108054785493 entropy 0.8164090514183044
epoch: 20, step: 17
	action: tensor([[-0.5337, -0.9255, -0.5260, -0.3480,  1.0233, -0.5925, -0.1028]],
       dtype=torch.float64)
	q_value: tensor([[-20.1912]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9566577833019185, distance: 1.6007154276269515 entropy 0.8164090514183044
epoch: 20, step: 18
	action: tensor([[ 0.3415, -0.8179,  0.2169,  0.8559, -0.3145,  0.5969,  1.0634]],
       dtype=torch.float64)
	q_value: tensor([[-25.5899]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6141120170995209, distance: 0.7108653084848013 entropy 0.8164090514183044
epoch: 20, step: 19
	action: tensor([[-0.4141,  0.1296,  0.2863, -0.0264,  0.5192,  0.0553, -0.1804]],
       dtype=torch.float64)
	q_value: tensor([[-27.4561]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1276343552405339, distance: 1.2151806425100558 entropy 0.8164090514183044
epoch: 20, step: 20
	action: tensor([[ 0.6498, -0.5078, -0.3014,  0.0597,  0.2017, -0.5979, -0.1056]],
       dtype=torch.float64)
	q_value: tensor([[-15.4993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0857549960276579, distance: 1.094178032052256 entropy 0.8164090514183044
epoch: 20, step: 21
	action: tensor([[ 1.1539,  0.5455, -0.4800, -0.0210,  0.0393,  0.6255, -0.4888]],
       dtype=torch.float64)
	q_value: tensor([[-22.7632]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9748606570178747, distance: 0.18144025804401243 entropy 0.8164090514183044
epoch: 20, step: 22
	action: tensor([[-0.5837,  0.6502, -0.8120, -0.6818, -0.7380, -0.1827,  0.3444]],
       dtype=torch.float64)
	q_value: tensor([[-22.7342]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28501248571385074, distance: 0.9676224193350523 entropy 0.8164090514183044
epoch: 20, step: 23
	action: tensor([[-0.3014, -0.7645,  0.6325, -0.5235, -1.3098,  0.4280, -0.0026]],
       dtype=torch.float64)
	q_value: tensor([[-20.2369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9903355760196735, distance: 1.6144323306921127 entropy 0.8164090514183044
epoch: 20, step: 24
	action: tensor([[-0.2821,  0.3588,  1.2319,  0.7149, -0.7368, -0.4406, -0.0580]],
       dtype=torch.float64)
	q_value: tensor([[-26.3442]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5607858963685609, distance: 0.7583939013170828 entropy 0.8164090514183044
epoch: 20, step: 25
	action: tensor([[-0.4242, -0.9569,  0.0224,  0.2270, -0.1579,  0.7293, -0.1219]],
       dtype=torch.float64)
	q_value: tensor([[-24.8605]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.553017717806056, distance: 1.4260833142844656 entropy 0.8164090514183044
epoch: 20, step: 26
	action: tensor([[-0.1754, -0.9232,  0.3492,  0.3739, -0.0592,  0.8774, -0.5239]],
       dtype=torch.float64)
	q_value: tensor([[-20.5582]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.030986953432761366, distance: 1.161938864051989 entropy 0.8164090514183044
epoch: 20, step: 27
	action: tensor([[ 0.3317, -0.1759,  0.8624,  0.2852,  0.5498, -0.1520, -0.1464]],
       dtype=torch.float64)
	q_value: tensor([[-22.9778]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5825399115705662, distance: 0.7393739984059522 entropy 0.8164090514183044
