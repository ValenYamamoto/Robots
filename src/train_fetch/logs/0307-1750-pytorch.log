epoch: 0, step: 0
	action: tensor([[-0.0053,  0.0016,  0.0229, -0.0270,  0.0077,  0.0427, -0.0066]],
       dtype=torch.float64)
	q_value: tensor([[-0.0333]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24485313641434225, distance: 0.9944258489007799 entropy -16.122505359320677
epoch: 0, step: 1
	action: tensor([[-0.0075,  0.0031,  0.0261,  0.0008,  0.0074,  0.0380, -0.0053]],
       dtype=torch.float64)
	q_value: tensor([[-0.0479]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2553568062060746, distance: 0.9874856777088297 entropy -16.14996510215206
epoch: 0, step: 2
	action: tensor([[-0.0075,  0.0031,  0.0168,  0.0077,  0.0073,  0.0380, -0.0053]],
       dtype=torch.float64)
	q_value: tensor([[-0.0479]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25766868847661506, distance: 0.9859515702556126 entropy -16.149710210192275
epoch: 0, step: 3
	action: tensor([[-0.0075,  0.0031,  0.0157, -0.0195,  0.0073,  0.0380, -0.0053]],
       dtype=torch.float64)
	q_value: tensor([[-0.0481]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24386409497347716, distance: 0.9950768524290017 entropy -16.149787909202704
epoch: 0, step: 4
	action: tensor([[-0.0075,  0.0031,  0.0015, -0.0444,  0.0074,  0.0380, -0.0053]],
       dtype=torch.float64)
	q_value: tensor([[-0.0481]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23137295682099057, distance: 1.003262370761415 entropy -16.149991129407
epoch: 0, step: 5
	action: tensor([[-0.0075,  0.0030,  0.0161, -0.0647,  0.0074,  0.0380, -0.0052]],
       dtype=torch.float64)
	q_value: tensor([[-0.0484]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22098914940324255, distance: 1.0100164489946313 entropy -16.15036665616427
epoch: 0, step: 6
	action: tensor([[-0.0075,  0.0030,  0.0293, -0.0198,  0.0074,  0.0380, -0.0052]],
       dtype=torch.float64)
	q_value: tensor([[-0.0478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24208257143740408, distance: 0.9962484076073967 entropy -16.150173024063836
epoch: 0, step: 7
	action: tensor([[-0.0075,  0.0031,  0.0130,  0.0570,  0.0074,  0.0380, -0.0053]],
       dtype=torch.float64)
	q_value: tensor([[-0.0477]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27791591606560007, distance: 0.972412604325687 entropy -16.149754740076393
epoch: 0, step: 8
	action: tensor([[-0.0075,  0.0031,  0.0209,  0.0165,  0.0073,  0.0381, -0.0053]],
       dtype=torch.float64)
	q_value: tensor([[-0.0481]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2580830976600874, distance: 0.9856763262920316 entropy -16.149666854562803
epoch: 0, step: 9
	action: tensor([[-0.0075,  0.0031,  0.0243,  0.0142,  0.0073,  0.0380, -0.0053]],
       dtype=torch.float64)
	q_value: tensor([[-0.0480]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2562354996495515, distance: 0.9869028795822968 entropy -16.14968787470803
epoch: 0, step: 10
	action: tensor([[-0.0076,  0.0031,  0.0151,  0.0052,  0.0073,  0.0380, -0.0053]],
       dtype=torch.float64)
	q_value: tensor([[-0.0479]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2508536373563913, distance: 0.990467046567698 entropy -16.149668765986096
epoch: 0, step: 11
	action: tensor([[-0.0075,  0.0031,  0.0255, -0.0125,  0.0073,  0.0380, -0.0053]],
       dtype=torch.float64)
	q_value: tensor([[-0.0482]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2417069885520846, distance: 0.9964952204485626 entropy -16.149831191435847
epoch: 0, step: 12
	action: tensor([[-0.0075,  0.0031,  0.0193,  0.0360,  0.0074,  0.0380, -0.0053]],
       dtype=torch.float64)
	q_value: tensor([[-0.0479]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.264220460932608, distance: 0.9815909520323931 entropy -16.14977673224262
epoch: 0, step: 13
	action: tensor([[-0.0076,  0.0031,  0.0120, -0.0608,  0.0073,  0.0380, -0.0053]],
       dtype=torch.float64)
	q_value: tensor([[-0.0480]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21664698083410316, distance: 1.0128274286380576 entropy -16.149607916945268
epoch: 0, step: 14
	action: tensor([[-0.0075,  0.0030,  0.0198, -0.0862,  0.0074,  0.0380, -0.0052]],
       dtype=torch.float64)
	q_value: tensor([[-0.0480]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20354173912226825, distance: 1.0212644249931624 entropy -16.150236900562344
epoch: 0, step: 15
	action: tensor([[-0.0075,  0.0030,  0.0174, -0.0152,  0.0074,  0.0380, -0.0052]],
       dtype=torch.float64)
	q_value: tensor([[-0.0477]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23737023736833496, distance: 0.9993406842232214 entropy -16.150092145599093
epoch: 0, step: 16
	action: tensor([[-0.0075,  0.0031,  0.0233, -0.0806,  0.0073,  0.0380, -0.0053]],
       dtype=torch.float64)
	q_value: tensor([[-0.0482]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20464936817380397, distance: 1.020554045204586 entropy -16.150066756112427
epoch: 0, step: 17
	action: tensor([[-0.0075,  0.0030,  0.0095,  0.0423,  0.0074,  0.0380, -0.0052]],
       dtype=torch.float64)
	q_value: tensor([[-0.0475]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2632128024521737, distance: 0.9822628720692369 entropy -16.149995332499063
epoch: 0, step: 18
	action: tensor([[-0.0075,  0.0031,  0.0110,  0.0267,  0.0073,  0.0381, -0.0053]],
       dtype=torch.float64)
	q_value: tensor([[-0.0482]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25523512032715434, distance: 0.9875663594063886 entropy -16.14972335701444
epoch: 0, step: 19
	action: tensor([[-0.0075,  0.0031,  0.0260, -0.0401,  0.0073,  0.0380, -0.0053]],
       dtype=torch.float64)
	q_value: tensor([[-0.0482]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22246235216240595, distance: 1.0090609660501793 entropy -16.149745481814602
epoch: 0, step: 20
	action: tensor([[-0.0075,  0.0031,  0.0232,  0.0273,  0.0074,  0.0380, -0.0052]],
       dtype=torch.float64)
	q_value: tensor([[-0.0476]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2544416897484112, distance: 0.9880922681790485 entropy -16.149959742094513
epoch: 0, step: 21
	action: tensor([[-0.0076,  0.0031, -0.0052, -0.0238,  0.0073,  0.0380, -0.0053]],
       dtype=torch.float64)
	q_value: tensor([[-0.0479]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22858693623571702, distance: 1.0050789743948345 entropy -16.149597873952235
epoch: 0, step: 22
	action: tensor([[-0.0075,  0.0030,  0.0137, -0.0753,  0.0073,  0.0380, -0.0052]],
       dtype=torch.float64)
	q_value: tensor([[-0.0487]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20359208881976798, distance: 1.0212321438492047 entropy -16.150418457591734
epoch: 0, step: 23
	action: tensor([[-0.0075,  0.0030,  0.0124, -0.0617,  0.0074,  0.0380, -0.0052]],
       dtype=torch.float64)
	q_value: tensor([[-0.0479]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2098107118188175, distance: 1.0172372668097538 entropy -16.15016852669844
epoch: 0, step: 24
	action: tensor([[-0.0075,  0.0030,  0.0407, -0.0657,  0.0074,  0.0380, -0.0052]],
       dtype=torch.float64)
	q_value: tensor([[-0.0480]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20764402853344066, distance: 1.018630934271311 entropy -16.15023429761724
epoch: 0, step: 25
	action: tensor([[-0.0075,  0.0031,  0.0092,  0.0411,  0.0074,  0.0380, -0.0052]],
       dtype=torch.float64)
	q_value: tensor([[-0.0471]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2588613146526737, distance: 0.985159239189915 entropy -16.149891703024625
epoch: 0, step: 26
	action: tensor([[-0.0075,  0.0031,  0.0166, -0.0068,  0.0073,  0.0381, -0.0053]],
       dtype=torch.float64)
	q_value: tensor([[-0.0482]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2362784985148747, distance: 1.000055729009534 entropy -16.14983264809517
epoch: 0, step: 27
	action: tensor([[-0.0075,  0.0031,  0.0126, -0.0039,  0.0073,  0.0380, -0.0053]],
       dtype=torch.float64)
	q_value: tensor([[-0.0481]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23758440158597605, distance: 0.999200355289016 entropy -16.149887334965218
epoch: 0, step: 28
	action: tensor([[-0.0075,  0.0031,  0.0026,  0.0702,  0.0073,  0.0380, -0.0053]],
       dtype=torch.float64)
	q_value: tensor([[-0.0483]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2721529182151683, distance: 0.9762853352031091 entropy -16.150047108423262
epoch: 0, step: 29
	action: tensor([[-0.0075,  0.0031,  0.0159, -0.0045,  0.0073,  0.0381, -0.0053]],
       dtype=torch.float64)
	q_value: tensor([[-0.0483]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2373766176890365, distance: 0.9993365038674945 entropy -16.14999329303355
epoch: 0, step: 30
	action: tensor([[-0.0075,  0.0031,  0.0160,  0.0550,  0.0073,  0.0380, -0.0053]],
       dtype=torch.float64)
	q_value: tensor([[-0.0482]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.265881980094537, distance: 0.9804820234314551 entropy -16.149997685708968
epoch: 0, step: 31
	action: tensor([[-0.0075,  0.0031,  0.0322, -0.0153,  0.0073,  0.0381, -0.0053]],
       dtype=torch.float64)
	q_value: tensor([[-0.0481]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23249876057194452, distance: 1.0025273651186353 entropy -16.149732502837338
epoch: 0, step: 32
	action: tensor([[-0.0075,  0.0031,  0.0274, -0.0465,  0.0074,  0.0380, -0.0053]],
       dtype=torch.float64)
	q_value: tensor([[-0.0477]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2170720594075558, distance: 1.0125525910809863 entropy -16.149783766997423
epoch: 0, step: 33
	action: tensor([[-0.0075,  0.0031,  0.0079,  0.0002,  0.0074,  0.0380, -0.0052]],
       dtype=torch.float64)
	q_value: tensor([[-0.0476]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.239403348137339, distance: 0.9980077135279577 entropy -16.15005952779105
epoch: 0, step: 34
	action: tensor([[-0.0075,  0.0031,  0.0302, -0.0081,  0.0073,  0.0380, -0.0053]],
       dtype=torch.float64)
	q_value: tensor([[-0.0484]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2360421329863699, distance: 1.0002104715524474 entropy -16.150105037654956
epoch: 0, step: 35
	action: tensor([[-0.0075,  0.0031,  0.0139,  0.0166,  0.0073,  0.0380, -0.0053]],
       dtype=torch.float64)
	q_value: tensor([[-0.0478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24745407087480908, distance: 0.9927118330399404 entropy -16.14979108553634
epoch: 0, step: 36
	action: tensor([[-0.0075,  0.0031,  0.0021,  0.0135,  0.0073,  0.0380, -0.0053]],
       dtype=torch.float64)
	q_value: tensor([[-0.0482]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24550285491284285, distance: 0.9939979613376283 entropy -16.14986785120632
epoch: 0, step: 37
	action: tensor([[-0.0075,  0.0031,  0.0333, -0.0394,  0.0073,  0.0381, -0.0053]],
       dtype=torch.float64)
	q_value: tensor([[-0.0485]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22067021517372942, distance: 1.0102231828635042 entropy -16.150083161739154
epoch: 0, step: 38
	action: tensor([[-0.0075,  0.0031,  0.0149, -0.0109,  0.0074,  0.0380, -0.0052]],
       dtype=torch.float64)
	q_value: tensor([[-0.0474]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23427381040940287, distance: 1.0013673890504302 entropy -16.149927243809053
epoch: 0, step: 39
	action: tensor([[-0.0075,  0.0031,  0.0138, -0.0419,  0.0073,  0.0380, -0.0053]],
       dtype=torch.float64)
	q_value: tensor([[-0.0482]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21930114718441807, distance: 1.0111101355796484 entropy -16.150073696561453
epoch: 0, step: 40
	action: tensor([[-0.0075,  0.0031,  0.0263, -0.0141,  0.0074,  0.0380, -0.0052]],
       dtype=torch.float64)
	q_value: tensor([[-0.0481]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23299238857524718, distance: 1.0022049192690237 entropy -16.150277899076848
epoch: 0, step: 41
	action: tensor([[-0.0075,  0.0031,  0.0180,  0.0514,  0.0073,  0.0380, -0.0053]],
       dtype=torch.float64)
	q_value: tensor([[-0.0479]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26428348910289756, distance: 0.9815489087277216 entropy -16.14988326495014
epoch: 0, step: 42
	action: tensor([[-0.0076,  0.0031,  0.0352, -0.0220,  0.0073,  0.0381, -0.0053]],
       dtype=torch.float64)
	q_value: tensor([[-0.0480]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2292368735911967, distance: 1.0046554814230997 entropy -16.149696515748413
epoch: 0, step: 43
	action: tensor([[-0.0075,  0.0031,  0.0259, -0.0271,  0.0074,  0.0380, -0.0053]],
       dtype=torch.float64)
	q_value: tensor([[-0.0474]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22660636614733198, distance: 1.0063683959578407 entropy -16.149673109299155
epoch: 0, step: 44
	action: tensor([[-0.0075,  0.0031,  0.0383,  0.0471,  0.0074,  0.0380, -0.0053]],
       dtype=torch.float64)
	q_value: tensor([[-0.0477]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2634105646727041, distance: 0.9821310378543103 entropy -16.14987333484235
epoch: 0, step: 45
	action: tensor([[-0.0076,  0.0031,  0.0030, -0.0334,  0.0073,  0.0380, -0.0053]],
       dtype=torch.float64)
	q_value: tensor([[-0.0476]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22322211199380437, distance: 1.0085678507894995 entropy -16.1493120245353
epoch: 0, step: 46
	action: tensor([[-0.0075,  0.0030, -0.0010,  0.0251,  0.0073,  0.0380, -0.0052]],
       dtype=torch.float64)
	q_value: tensor([[-0.0485]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2508590249528252, distance: 0.9904634850164523 entropy -16.150318390517505
epoch: 0, step: 47
	action: tensor([[-0.0075,  0.0031, -0.0081, -0.0094,  0.0073,  0.0381, -0.0053]],
       dtype=torch.float64)
	q_value: tensor([[-0.0485]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23444808458226174, distance: 1.0012534302970102 entropy -16.150065256787983
epoch: 0, step: 48
	action: tensor([[-0.0075,  0.0031,  0.0390,  0.0084,  0.0073,  0.0380, -0.0053]],
       dtype=torch.float64)
	q_value: tensor([[-0.0488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2444317388066174, distance: 0.9947032719091028 entropy -16.1505067705197
epoch: 0, step: 49
	action: tensor([[-0.0076,  0.0031,  0.0269, -0.0675,  0.0073,  0.0380, -0.0053]],
       dtype=torch.float64)
	q_value: tensor([[-0.0476]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20681790454104476, distance: 1.01916181701476 entropy -16.149620899152655
epoch: 0, step: 50
	action: tensor([[-0.0075,  0.0030,  0.0286, -0.0111,  0.0074,  0.0380, -0.0052]],
       dtype=torch.float64)
	q_value: tensor([[-0.0475]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23451282951266228, distance: 1.001211089958594 entropy -16.150094495592477
epoch: 0, step: 51
	action: tensor([[-0.0075,  0.0031,  0.0193,  0.0773,  0.0073,  0.0380, -0.0053]],
       dtype=torch.float64)
	q_value: tensor([[-0.0478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2767464444779075, distance: 0.973199734841637 entropy -16.14983423205351
epoch: 0, step: 52
	action: tensor([[-0.0076,  0.0031,  0.0105, -0.0237,  0.0073,  0.0381, -0.0053]],
       dtype=torch.float64)
	q_value: tensor([[-0.0479]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22797829849927176, distance: 1.0054753952010853 entropy -16.149745827676092
epoch: 0, step: 53
	action: tensor([[-0.0075,  0.0031,  0.0172,  0.0025,  0.0073,  0.0380, -0.0053]],
       dtype=torch.float64)
	q_value: tensor([[-0.0483]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24079872080793296, distance: 0.9970918325705164 entropy -16.15024458264781
epoch: 0, step: 54
	action: tensor([[-0.0075,  0.0031, -0.0044, -0.0519,  0.0073,  0.0380, -0.0053]],
       dtype=torch.float64)
	q_value: tensor([[-0.0482]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21451072879392297, distance: 1.0142075098472003 entropy -16.149919775710973
epoch: 0, step: 55
	action: tensor([[-0.0075,  0.0030,  0.0089, -0.0547,  0.0073,  0.0380, -0.0052]],
       dtype=torch.float64)
	q_value: tensor([[-0.0487]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21316870016580236, distance: 1.0150735399079394 entropy -16.15055245118115
epoch: 0, step: 56
	action: tensor([[-0.0075,  0.0030,  0.0331, -0.0234,  0.0074,  0.0380, -0.0052]],
       dtype=torch.float64)
	q_value: tensor([[-0.0482]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22857593849527802, distance: 1.0050861388821914 entropy -16.150370390983806
epoch: 0, step: 57
	action: tensor([[-0.0075,  0.0031,  0.0196,  0.0575,  0.0074,  0.0380, -0.0053]],
       dtype=torch.float64)
	q_value: tensor([[-0.0476]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26731392195182546, distance: 0.9795253117558944 entropy -16.149815163967915
epoch: 0, step: 58
	action: tensor([[-0.0076,  0.0031,  0.0262,  0.0889,  0.0073,  0.0381, -0.0053]],
       dtype=torch.float64)
	q_value: tensor([[-0.0480]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2828414608344474, distance: 0.9690903750880359 entropy -16.149680376865103
epoch: 0, step: 59
	action: tensor([[-0.0076,  0.0031,  0.0212, -0.0464,  0.0073,  0.0380, -0.0053]],
       dtype=torch.float64)
	q_value: tensor([[-0.0478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21709367368266985, distance: 1.0125386142255266 entropy -16.149683930387656
epoch: 0, step: 60
	action: tensor([[-0.0075,  0.0031,  0.0276, -0.0349,  0.0074,  0.0380, -0.0052]],
       dtype=torch.float64)
	q_value: tensor([[-0.0478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22283554865030597, distance: 1.0088187763586256 entropy -16.15017436764472
epoch: 0, step: 61
	action: tensor([[-0.0075,  0.0031,  0.0046,  0.0055,  0.0074,  0.0380, -0.0053]],
       dtype=torch.float64)
	q_value: tensor([[-0.0476]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24180538249420547, distance: 0.9964305671599173 entropy -16.15000008060743
epoch: 0, step: 62
	action: tensor([[-0.0075,  0.0031,  0.0195, -0.0182,  0.0073,  0.0380, -0.0053]],
       dtype=torch.float64)
	q_value: tensor([[-0.0484]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23085276094524132, distance: 1.0036018102071307 entropy -16.150110827044884
epoch: 0, step: 63
	action: tensor([[-0.0075,  0.0031,  0.0255, -0.0644,  0.0073,  0.0380, -0.0053]],
       dtype=torch.float64)
	q_value: tensor([[-0.0481]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20831832481329648, distance: 1.0181974136908956 entropy -16.150036597736015
epoch: 0, step: 64
	action: tensor([[-0.0075,  0.0031,  0.0094,  0.0392,  0.0074,  0.0380, -0.0052]],
       dtype=torch.float64)
	q_value: tensor([[-0.0476]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25802485797065344, distance: 0.9857150128055896 entropy -16.150120627078056
epoch: 0, step: 65
	action: tensor([[-0.0075,  0.0031,  0.0286,  0.0165,  0.0073,  0.0381, -0.0053]],
       dtype=torch.float64)
	q_value: tensor([[-0.0482]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2479613116397117, distance: 0.9923772163940092 entropy -16.149835357687184
epoch: 0, step: 66
	action: tensor([[-0.0076,  0.0031,  0.0196,  0.0171,  0.0073,  0.0380, -0.0053]],
       dtype=torch.float64)
	q_value: tensor([[-0.0478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2478648091772233, distance: 0.9924408858398441 entropy -16.149607695732048
epoch: 0, step: 67
	action: tensor([[-0.0075,  0.0031,  0.0010, -0.0433,  0.0073,  0.0380, -0.0053]],
       dtype=torch.float64)
	q_value: tensor([[-0.0480]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21858424509099839, distance: 1.0115742714231035 entropy -16.14969709350734
epoch: 0, step: 68
	action: tensor([[-0.0075,  0.0030,  0.0226, -0.0440,  0.0074,  0.0380, -0.0052]],
       dtype=torch.float64)
	q_value: tensor([[-0.0485]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21831869128360548, distance: 1.0117461418896636 entropy -16.15038250144475
epoch: 0, step: 69
	action: tensor([[-0.0075,  0.0031,  0.0036, -0.0265,  0.0074,  0.0380, -0.0052]],
       dtype=torch.float64)
	q_value: tensor([[-0.0477]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22660682555650213, distance: 1.0063680970576958 entropy -16.15004150499568
epoch: 0, step: 70
	action: tensor([[-0.0075,  0.0031,  0.0179,  0.0444,  0.0073,  0.0380, -0.0052]],
       dtype=torch.float64)
	q_value: tensor([[-0.0484]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2609767228112134, distance: 0.9837522804501522 entropy -16.150288336334537
epoch: 0, step: 71
	action: tensor([[-0.0076,  0.0031,  0.0322, -0.0424,  0.0073,  0.0380, -0.0053]],
       dtype=torch.float64)
	q_value: tensor([[-0.0480]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21910276249860183, distance: 1.0112385948597418 entropy -16.149607358943218
epoch: 0, step: 72
	action: tensor([[-0.0075,  0.0031,  0.0062,  0.0015,  0.0074,  0.0380, -0.0052]],
       dtype=torch.float64)
	q_value: tensor([[-0.0474]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23996853129071527, distance: 0.9976368454936958 entropy -16.14986120638053
epoch: 0, step: 73
	action: tensor([[-0.0075,  0.0031,  0.0204, -0.0614,  0.0073,  0.0380, -0.0053]],
       dtype=torch.float64)
	q_value: tensor([[-0.0484]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2098601830047192, distance: 1.0172054233499876 entropy -16.149989085761046
epoch: 0, step: 74
	action: tensor([[-0.0075,  0.0030,  0.0216,  0.0116,  0.0074,  0.0380, -0.0052]],
       dtype=torch.float64)
	q_value: tensor([[-0.0477]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2453636497415247, distance: 0.9940896537155695 entropy -16.15011402706059
epoch: 0, step: 75
	action: tensor([[-0.0075,  0.0031,  0.0082, -0.0041,  0.0073,  0.0380, -0.0053]],
       dtype=torch.float64)
	q_value: tensor([[-0.0480]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23730934734481934, distance: 0.9993805781968085 entropy -16.149712951013047
epoch: 0, step: 76
	action: tensor([[-0.0075,  0.0031,  0.0144, -0.0759,  0.0073,  0.0380, -0.0053]],
       dtype=torch.float64)
	q_value: tensor([[-0.0483]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2028686620211554, distance: 1.0216958628773456 entropy -16.15000700617048
epoch: 0, step: 77
	action: tensor([[-0.0075,  0.0030, -0.0012,  0.0089,  0.0074,  0.0380, -0.0052]],
       dtype=torch.float64)
	q_value: tensor([[-0.0478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24322510471461256, distance: 0.995497220073393 entropy -16.150152669333615
epoch: 0, step: 78
	action: tensor([[-0.0075,  0.0031,  0.0141, -0.0142,  0.0073,  0.0380, -0.0053]],
       dtype=torch.float64)
	q_value: tensor([[-0.0485]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23266632235815732, distance: 1.0024179226597285 entropy -16.15005916679
epoch: 0, step: 79
	action: tensor([[-0.0075,  0.0031,  0.0008, -0.0044,  0.0073,  0.0380, -0.0053]],
       dtype=torch.float64)
	q_value: tensor([[-0.0482]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23702200810691798, distance: 0.9995688158598939 entropy -16.14998096361072
epoch: 0, step: 80
	action: tensor([[-0.0075,  0.0031,  0.0148,  0.0072,  0.0073,  0.0380, -0.0053]],
       dtype=torch.float64)
	q_value: tensor([[-0.0485]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24296810790158274, distance: 0.9956662385607727 entropy -16.150154427513574
epoch: 0, step: 81
	action: tensor([[-0.0075,  0.0031,  0.0229, -0.0271,  0.0073,  0.0380, -0.0053]],
       dtype=torch.float64)
	q_value: tensor([[-0.0482]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22654598051948704, distance: 1.006407683196967 entropy -16.14982048348779
epoch: 0, step: 82
	action: tensor([[-0.0075,  0.0031,  0.0046,  0.0181,  0.0074,  0.0380, -0.0053]],
       dtype=torch.float64)
	q_value: tensor([[-0.0478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2477650030443651, distance: 0.9925067306522966 entropy -16.149926871511905
epoch: 0, step: 83
	action: tensor([[-0.0075,  0.0031, -0.0074,  0.0217,  0.0073,  0.0380, -0.0053]],
       dtype=torch.float64)
	q_value: tensor([[-0.0484]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24892543279438373, distance: 0.9917408936043306 entropy -16.149883749663054
epoch: 0, step: 84
	action: tensor([[-0.0075,  0.0031,  0.0096,  0.0007,  0.0073,  0.0381, -0.0053]],
       dtype=torch.float64)
	q_value: tensor([[-0.0486]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2396744713797564, distance: 0.9978298221291575 entropy -16.150071464057564
epoch: 0, step: 85
	action: tensor([[-0.0075,  0.0031,  0.0087, -0.0358,  0.0073,  0.0380, -0.0053]],
       dtype=torch.float64)
	q_value: tensor([[-0.0483]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22218099585611584, distance: 1.0092435166883733 entropy -16.150068029342822
epoch: 0, step: 86
	action: tensor([[-0.0075,  0.0031,  0.0308, -0.0021,  0.0073,  0.0380, -0.0052]],
       dtype=torch.float64)
	q_value: tensor([[-0.0483]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23899170015457638, distance: 0.9982777464615179 entropy -16.15033807896482
epoch: 0, step: 87
	action: tensor([[-0.0075,  0.0031,  0.0034,  0.0450,  0.0073,  0.0380, -0.0053]],
       dtype=torch.float64)
	q_value: tensor([[-0.0478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26038620635139376, distance: 0.9841452355382141 entropy -16.14975740987557
epoch: 0, step: 88
	action: tensor([[-0.0075,  0.0031,  0.0165,  0.0633,  0.0073,  0.0381, -0.0053]],
       dtype=torch.float64)
	q_value: tensor([[-0.0484]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2698812854433317, distance: 0.9778076563659417 entropy -16.14992716420045
epoch: 0, step: 89
	action: tensor([[-0.0076,  0.0031,  0.0119, -0.0245,  0.0073,  0.0381, -0.0053]],
       dtype=torch.float64)
	q_value: tensor([[-0.0480]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22765068861871984, distance: 1.0056887109246047 entropy -16.1497490029463
epoch: 0, step: 90
	action: tensor([[-0.0075,  0.0031,  0.0159, -0.0689,  0.0073,  0.0380, -0.0053]],
       dtype=torch.float64)
	q_value: tensor([[-0.0483]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20628246323858346, distance: 1.0195057539684267 entropy -16.150226167431672
epoch: 0, step: 91
	action: tensor([[-0.0075,  0.0030,  0.0213, -0.0477,  0.0074,  0.0380, -0.0052]],
       dtype=torch.float64)
	q_value: tensor([[-0.0478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21651316957577038, distance: 1.0129139298209313 entropy -16.15016538554382
epoch: 0, step: 92
	action: tensor([[-0.0075,  0.0031,  0.0219,  0.0184,  0.0074,  0.0380, -0.0052]],
       dtype=torch.float64)
	q_value: tensor([[-0.0477]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24866010811633865, distance: 0.9919160493813097 entropy -16.150079239580137
epoch: 0, step: 93
	action: tensor([[-0.0075,  0.0031,  0.0178,  0.0055,  0.0073,  0.0380, -0.0053]],
       dtype=torch.float64)
	q_value: tensor([[-0.0480]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2422641912748611, distance: 0.9961290348974465 entropy -16.14975458765247
epoch: 0, step: 94
	action: tensor([[-0.0075,  0.0031,  0.0120,  0.0663,  0.0073,  0.0380, -0.0053]],
       dtype=torch.float64)
	q_value: tensor([[-0.0482]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27100459317015557, distance: 0.9770551748374742 entropy -16.149892902771608
epoch: 0, step: 95
	action: tensor([[-0.0076,  0.0031,  0.0342, -0.0108,  0.0073,  0.0381, -0.0053]],
       dtype=torch.float64)
	q_value: tensor([[-0.0481]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23476726657064406, distance: 1.0010446819599297 entropy -16.14971086816288
epoch: 0, step: 96
	action: tensor([[-0.0075,  0.0031,  0.0079,  0.0011,  0.0074,  0.0380, -0.0053]],
       dtype=torch.float64)
	q_value: tensor([[-0.0476]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23979961749655265, distance: 0.9977476996292349 entropy -16.14973659791891
epoch: 0, step: 97
	action: tensor([[-0.0075,  0.0031, -0.0027,  0.0221,  0.0073,  0.0380, -0.0053]],
       dtype=torch.float64)
	q_value: tensor([[-0.0484]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24934683067894592, distance: 0.9914626415049401 entropy -16.150093783196805
epoch: 0, step: 98
	action: tensor([[-0.0075,  0.0031,  0.0260, -0.0533,  0.0073,  0.0381, -0.0053]],
       dtype=torch.float64)
	q_value: tensor([[-0.0486]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21377717601867918, distance: 1.014680973423376 entropy -16.150110022466595
epoch: 0, step: 99
	action: tensor([[-0.0075,  0.0031,  0.0142,  0.0519,  0.0074,  0.0380, -0.0052]],
       dtype=torch.float64)
	q_value: tensor([[-0.0476]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2643277875318586, distance: 0.981519358127393 entropy -16.150115405383424
epoch: 0, step: 100
	action: tensor([[-0.0076,  0.0031,  0.0070, -0.0001,  0.0073,  0.0381, -0.0053]],
       dtype=torch.float64)
	q_value: tensor([[-0.0481]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23917448195509772, distance: 0.9981578542332252 entropy -16.149652381904946
epoch: 0, step: 101
	action: tensor([[-0.0075,  0.0031, -0.0010, -0.0666,  0.0073,  0.0380, -0.0053]],
       dtype=torch.float64)
	q_value: tensor([[-0.0484]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20751153087156748, distance: 1.0187160983769503 entropy -16.150124349906044
epoch: 0, step: 102
	action: tensor([[-0.0074,  0.0030,  0.0131, -0.0074,  0.0074,  0.0380, -0.0052]],
       dtype=torch.float64)
	q_value: tensor([[-0.0484]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23597686386708694, distance: 1.0002531973787565 entropy -16.150461108420657
epoch: 0, step: 103
	action: tensor([[-0.0075,  0.0031,  0.0180, -0.1026,  0.0073,  0.0380, -0.0053]],
       dtype=torch.float64)
	q_value: tensor([[-0.0483]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18984943545154942, distance: 1.030005545083252 entropy -16.150074661886823
epoch: 0, step: 104
	action: tensor([[-0.0074,  0.0030,  0.0263, -0.0605,  0.0074,  0.0380, -0.0052]],
       dtype=torch.float64)
	q_value: tensor([[-0.0476]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21030204440727784, distance: 1.0169209631442342 entropy -16.149919231016277
epoch: 0, step: 105
	action: tensor([[-0.0075,  0.0030, -0.0118,  0.0268,  0.0074,  0.0380, -0.0052]],
       dtype=torch.float64)
	q_value: tensor([[-0.0475]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2511161884719819, distance: 0.9902934683261301 entropy -16.150026009641714
epoch: 0, step: 106
	action: tensor([[-0.0075,  0.0031, -0.0012, -0.0457,  0.0073,  0.0381, -0.0053]],
       dtype=torch.float64)
	q_value: tensor([[-0.0487]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21744518352725883, distance: 1.0123112835488337 entropy -16.150103367046174
epoch: 0, step: 107
	action: tensor([[-0.0075,  0.0030,  0.0222,  0.0380,  0.0074,  0.0380, -0.0052]],
       dtype=torch.float64)
	q_value: tensor([[-0.0485]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25815836827168814, distance: 0.9856263245050616 entropy -16.15041350680131
epoch: 0, step: 108
	action: tensor([[-0.0076,  0.0031,  0.0163,  0.0116,  0.0073,  0.0380, -0.0053]],
       dtype=torch.float64)
	q_value: tensor([[-0.0479]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.245103445424113, distance: 0.9942610237575848 entropy -16.149565569878924
epoch: 0, step: 109
	action: tensor([[-0.0075,  0.0031,  0.0172, -0.0314,  0.0073,  0.0380, -0.0053]],
       dtype=torch.float64)
	q_value: tensor([[-0.0481]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22440368692088186, distance: 1.007800480844865 entropy -16.149768020404796
epoch: 0, step: 110
	action: tensor([[-0.0075,  0.0031,  0.0150, -0.0124,  0.0074,  0.0380, -0.0053]],
       dtype=torch.float64)
	q_value: tensor([[-0.0480]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23352061324694173, distance: 1.0018597593688787 entropy -16.150063187348298
epoch: 0, step: 111
	action: tensor([[-0.0075,  0.0031,  0.0361,  0.0041,  0.0073,  0.0380, -0.0053]],
       dtype=torch.float64)
	q_value: tensor([[-0.0482]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24216090320212547, distance: 0.996196924484459 entropy -16.149952170937986
epoch: 0, step: 112
	action: tensor([[-0.0076,  0.0031, -0.0087, -0.0077,  0.0074,  0.0380, -0.0053]],
       dtype=torch.float64)
	q_value: tensor([[-0.0476]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23511237526163353, distance: 1.0008189283010183 entropy -16.1495677924255
epoch: 0, step: 113
	action: tensor([[-0.0075,  0.0030,  0.0167,  0.0788,  0.0073,  0.0380, -0.0053]],
       dtype=torch.float64)
	q_value: tensor([[-0.0487]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2772856698801188, distance: 0.9728368801671494 entropy -16.15034852076146
epoch: 0, step: 114
	action: tensor([[-0.0076,  0.0031,  0.0159, -0.0083,  0.0073,  0.0380, -0.0053]],
       dtype=torch.float64)
	q_value: tensor([[-0.0480]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23550608006309037, distance: 1.0005613231826789 entropy -16.14969714212794
epoch: 0, step: 115
	action: tensor([[-0.0075,  0.0031,  0.0141,  0.0319,  0.0073,  0.0380, -0.0053]],
       dtype=torch.float64)
	q_value: tensor([[-0.0482]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2547469733679778, distance: 0.9878899504895751 entropy -16.14990691500024
epoch: 0, step: 116
	action: tensor([[-0.0075,  0.0031,  0.0196,  0.0127,  0.0073,  0.0381, -0.0053]],
       dtype=torch.float64)
	q_value: tensor([[-0.0482]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2457758400888007, distance: 0.9938181254968802 entropy -16.14979029727244
epoch: 0, step: 117
	action: tensor([[-0.0075,  0.0031,  0.0041, -0.0092,  0.0073,  0.0380, -0.0053]],
       dtype=torch.float64)
	q_value: tensor([[-0.0481]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23478581547427835, distance: 1.0010325494458256 entropy -16.149811407425528
epoch: 0, step: 118
	action: tensor([[-0.0075,  0.0031,  0.0267, -0.0241,  0.0073,  0.0380, -0.0053]],
       dtype=torch.float64)
	q_value: tensor([[-0.0485]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2281193155740111, distance: 1.0053835612059694 entropy -16.150270195145737
epoch: 0, step: 119
	action: tensor([[-0.0075,  0.0031,  0.0198,  0.0304,  0.0074,  0.0380, -0.0053]],
       dtype=torch.float64)
	q_value: tensor([[-0.0477]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25430582188126816, distance: 0.9881822972696109 entropy -16.149832609807138
epoch: 0, step: 120
	action: tensor([[-0.0076,  0.0031,  0.0158, -0.0490,  0.0073,  0.0380, -0.0053]],
       dtype=torch.float64)
	q_value: tensor([[-0.0480]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2158467488065946, distance: 1.013344622063219 entropy -16.149618740908526
epoch: 0, step: 121
	action: tensor([[-0.0075,  0.0031,  0.0284,  0.0445,  0.0074,  0.0380, -0.0052]],
       dtype=torch.float64)
	q_value: tensor([[-0.0479]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2616001738352276, distance: 0.9833372388730434 entropy -16.15017321110297
epoch: 0, step: 122
	action: tensor([[-0.0076,  0.0031,  0.0155, -0.0192,  0.0073,  0.0380, -0.0053]],
       dtype=torch.float64)
	q_value: tensor([[-0.0478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23022043334281983, distance: 1.004014263582883 entropy -16.149458399444505
epoch: 0, step: 123
	action: tensor([[-0.0075,  0.0031,  0.0071, -0.0258,  0.0074,  0.0380, -0.0053]],
       dtype=torch.float64)
	q_value: tensor([[-0.0482]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22698644607163787, distance: 1.006121078542114 entropy -16.14999177665638
epoch: 0, step: 124
	action: tensor([[-0.0075,  0.0031,  0.0105,  0.0673,  0.0073,  0.0380, -0.0052]],
       dtype=torch.float64)
	q_value: tensor([[-0.0484]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27135420812620925, distance: 0.9768208564423704 entropy -16.15031940074152
epoch: 0, step: 125
	action: tensor([[-0.0075,  0.0031,  0.0159,  0.0070,  0.0073,  0.0381, -0.0053]],
       dtype=torch.float64)
	q_value: tensor([[-0.0481]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2428884231352103, distance: 0.9957186388260298 entropy -16.14984551687427
epoch: 0, step: 126
	action: tensor([[-0.0075,  0.0031,  0.0251, -0.0724,  0.0073,  0.0380, -0.0053]],
       dtype=torch.float64)
	q_value: tensor([[-0.0482]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2044521330114928, distance: 1.0206805784951032 entropy -16.149906072769422
epoch: 0, step: 127
	action: tensor([[-0.0075,  0.0030,  0.0250,  0.0206,  0.0074,  0.0380, -0.0052]],
       dtype=torch.float64)
	q_value: tensor([[-0.0476]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24986852918714852, distance: 0.9911180519503321 entropy -16.150111904971727
LOSS epoch 0 actor 0.2629040485006653 critic 12.946248816840203 entropy 0.01
epoch: 1, step: 0
	action: tensor([[ 0.0229,  0.0193,  0.0202, -0.1364,  0.0191,  0.0314,  0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0906]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21794314572576712, distance: 1.0119891508627703 entropy -13.573705085844699
epoch: 1, step: 1
	action: tensor([[0.0229, 0.0195, 0.0300, 0.0262, 0.0193, 0.0232, 0.0366]],
       dtype=torch.float64)
	q_value: tensor([[0.0937]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.294712591332711, distance: 0.9610362259063299 entropy -13.57175739135349
epoch: 1, step: 2
	action: tensor([[ 0.0229,  0.0194,  0.0230, -0.0264,  0.0190,  0.0274,  0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0908]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27022386144713295, distance: 0.9775782328935503 entropy -13.573731584170192
epoch: 1, step: 3
	action: tensor([[0.0229, 0.0194, 0.0394, 0.0418, 0.0191, 0.0289, 0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0916]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30400189033104863, distance: 0.9546863723987453 entropy -13.573168859205236
epoch: 1, step: 4
	action: tensor([[ 0.0229,  0.0194,  0.0254, -0.0186,  0.0190,  0.0239,  0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0904]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2731127574767064, distance: 0.975641390779772 entropy -13.57328042896694
epoch: 1, step: 5
	action: tensor([[0.0229, 0.0194, 0.0158, 0.0732, 0.0191, 0.0197, 0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0916]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31528922937561354, distance: 0.946913423099599 entropy -13.573138380642275
epoch: 1, step: 6
	action: tensor([[0.0229, 0.0195, 0.0336, 0.0310, 0.0189, 0.0292, 0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0900]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2985874627845533, distance: 0.958392608081864 entropy -13.575060292536877
epoch: 1, step: 7
	action: tensor([[0.0229, 0.0194, 0.0297, 0.0473, 0.0190, 0.0294, 0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0904]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30620721890932223, distance: 0.9531726703207027 entropy -13.57350122321473
epoch: 1, step: 8
	action: tensor([[ 0.0229,  0.0194,  0.0134, -0.0572,  0.0189,  0.0226,  0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0901]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2542648674891572, distance: 0.9882094329579479 entropy -13.573942519477225
epoch: 1, step: 9
	action: tensor([[0.0230, 0.0195, 0.0176, 0.0434, 0.0192, 0.0227, 0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0923]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30218332898627664, distance: 0.9559328004389648 entropy -13.57328842434444
epoch: 1, step: 10
	action: tensor([[ 0.0229,  0.0195,  0.0044, -0.0486,  0.0190,  0.0087,  0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0904]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2551167024746508, distance: 0.9876448679682547 entropy -13.574782406359835
epoch: 1, step: 11
	action: tensor([[ 0.0230,  0.0195,  0.0005, -0.0081,  0.0191,  0.0260,  0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0925]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27823462730971404, distance: 0.9721979804036838 entropy -13.573908647110553
epoch: 1, step: 12
	action: tensor([[ 0.0230,  0.0195,  0.0161, -0.0132,  0.0191,  0.0196,  0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0907]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2746050927585386, distance: 0.9746393563138303 entropy -13.574960866868581
epoch: 1, step: 13
	action: tensor([[0.0230, 0.0195, 0.0360, 0.0533, 0.0191, 0.0216, 0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0915]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3076289913477155, distance: 0.9521955128230379 entropy -13.573899565037944
epoch: 1, step: 14
	action: tensor([[0.0229, 0.0194, 0.0289, 0.0013, 0.0189, 0.0230, 0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0905]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2825635500120671, distance: 0.9692781261931889 entropy -13.573678948614049
epoch: 1, step: 15
	action: tensor([[ 0.0229,  0.0194,  0.0010, -0.0054,  0.0190,  0.0182,  0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0913]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27755896137238034, distance: 0.9726529256151153 entropy -13.573281369020957
epoch: 1, step: 16
	action: tensor([[ 0.0230,  0.0195,  0.0184, -0.0146,  0.0191,  0.0178,  0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0912]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27354052086118186, distance: 0.9753542726015354 entropy -13.57502498920891
epoch: 1, step: 17
	action: tensor([[0.0230, 0.0195, 0.0283, 0.0271, 0.0191, 0.0218, 0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0916]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2946961533239566, distance: 0.9610474251916526 entropy -13.573736782683941
epoch: 1, step: 18
	action: tensor([[ 0.0229,  0.0194,  0.0018, -0.0008,  0.0190,  0.0220,  0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2806448080829608, distance: 0.9705733996377761 entropy -13.573883574968603
epoch: 1, step: 19
	action: tensor([[0.0230, 0.0195, 0.0339, 0.0181, 0.0190, 0.0238, 0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0908]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2910793255347104, distance: 0.9635084200127852 entropy -13.575070806852622
epoch: 1, step: 20
	action: tensor([[0.0229, 0.0194, 0.0102, 0.0653, 0.0190, 0.0211, 0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31149814066003534, distance: 0.9495312273115257 entropy -13.573339231928651
epoch: 1, step: 21
	action: tensor([[ 0.0229,  0.0195,  0.0224, -0.0019,  0.0189,  0.0140,  0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0900]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27871702496236084, distance: 0.9718730379568447 entropy -13.575415846113215
epoch: 1, step: 22
	action: tensor([[ 0.0229,  0.0195,  0.0304, -0.0309,  0.0190,  0.0200,  0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0916]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2663138505608227, distance: 0.980193579672077 entropy -13.573746276036855
epoch: 1, step: 23
	action: tensor([[0.0229, 0.0194, 0.0255, 0.0370, 0.0191, 0.0170, 0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0923]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2981359100846326, distance: 0.9587010536244663 entropy -13.572599753982075
epoch: 1, step: 24
	action: tensor([[0.0229, 0.0195, 0.0044, 0.0031, 0.0190, 0.0232, 0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28278298991637485, distance: 0.9691298799157093 entropy -13.574291382645686
epoch: 1, step: 25
	action: tensor([[0.0230, 0.0195, 0.0095, 0.0135, 0.0190, 0.0149, 0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0908]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2858959983357825, distance: 0.9670243872927075 entropy -13.574972236511229
epoch: 1, step: 26
	action: tensor([[ 0.0230,  0.0195,  0.0054, -0.0177,  0.0190,  0.0217,  0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0911]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27275106576477726, distance: 0.9758840952169845 entropy -13.574914032206062
epoch: 1, step: 27
	action: tensor([[ 0.0230,  0.0195,  0.0082, -0.0245,  0.0191,  0.0225,  0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0912]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26981738655390874, distance: 0.9778504435559344 entropy -13.574483512845424
epoch: 1, step: 28
	action: tensor([[0.0230, 0.0195, 0.0381, 0.0071, 0.0191, 0.0157, 0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0913]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2839104653651776, distance: 0.9683678372618866 entropy -13.574192460974249
epoch: 1, step: 29
	action: tensor([[ 0.0229,  0.0194,  0.0165, -0.0602,  0.0190,  0.0196,  0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0917]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25208026282425644, distance: 0.9896558367066007 entropy -13.572916475400536
epoch: 1, step: 30
	action: tensor([[0.0230, 0.0195, 0.0102, 0.0400, 0.0192, 0.0220, 0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0926]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3000166958529562, distance: 0.9574156759462616 entropy -13.57303062444306
epoch: 1, step: 31
	action: tensor([[ 0.0229,  0.0195,  0.0290, -0.0476,  0.0190,  0.0158,  0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0903]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25717812313809396, distance: 0.9862772966371351 entropy -13.575201375879479
epoch: 1, step: 32
	action: tensor([[0.0229, 0.0194, 0.0295, 0.0454, 0.0191, 0.0134, 0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0927]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30146802095874903, distance: 0.9564226219727294 entropy -13.572396938264836
epoch: 1, step: 33
	action: tensor([[ 0.0229,  0.0194,  0.0301, -0.0505,  0.0190,  0.0229,  0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0910]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2574881836405931, distance: 0.9860714346654501 entropy -13.574160930484442
epoch: 1, step: 34
	action: tensor([[ 0.0229,  0.0194,  0.0327, -0.0659,  0.0191,  0.0142,  0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0925]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24784465976212156, distance: 0.992454179307355 entropy -13.572294018691826
epoch: 1, step: 35
	action: tensor([[0.0229, 0.0195, 0.0184, 0.0702, 0.0192, 0.0162, 0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0933]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3132285099554435, distance: 0.9483372775092415 entropy -13.571854418213993
epoch: 1, step: 36
	action: tensor([[0.0229, 0.0195, 0.0353, 0.0286, 0.0189, 0.0187, 0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0903]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2949145334398363, distance: 0.9608986312377088 entropy -13.574914715596728
epoch: 1, step: 37
	action: tensor([[0.0229, 0.0194, 0.0135, 0.0510, 0.0190, 0.0238, 0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0911]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30567512676985664, distance: 0.9535381097531678 entropy -13.573544116338027
epoch: 1, step: 38
	action: tensor([[ 0.0229,  0.0195,  0.0422, -0.0309,  0.0189,  0.0258,  0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0901]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.267890638705385, distance: 0.9791397306167183 entropy -13.575097701703928
epoch: 1, step: 39
	action: tensor([[0.0229, 0.0194, 0.0219, 0.0004, 0.0191, 0.0269, 0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0922]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28289830113656234, distance: 0.9690519704130744 entropy -13.571795749575307
epoch: 1, step: 40
	action: tensor([[0.0229, 0.0194, 0.0327, 0.0027, 0.0190, 0.0205, 0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2827507878638704, distance: 0.9691516359676065 entropy -13.573674077824167
epoch: 1, step: 41
	action: tensor([[0.0229, 0.0194, 0.0262, 0.0017, 0.0190, 0.0180, 0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0915]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2814741506634427, distance: 0.9700137526140735 entropy -13.573101888381228
epoch: 1, step: 42
	action: tensor([[ 0.0229,  0.0195,  0.0118, -0.0282,  0.0190,  0.0160,  0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0915]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2665143317614107, distance: 0.9800596505990942 entropy -13.573536473417567
epoch: 1, step: 43
	action: tensor([[ 0.0230,  0.0195,  0.0359, -0.0287,  0.0191,  0.0339,  0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0919]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2709409217505244, distance: 0.9770978425491839 entropy -13.573917721855818
epoch: 1, step: 44
	action: tensor([[ 0.0229,  0.0194,  0.0110, -0.0182,  0.0191,  0.0175,  0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0917]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2715260685696219, distance: 0.9767056517931841 entropy -13.572244425438166
epoch: 1, step: 45
	action: tensor([[0.0230, 0.0195, 0.0115, 0.0261, 0.0191, 0.0181, 0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0916]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29265495411513376, distance: 0.9624370900761037 entropy -13.574148247005885
epoch: 1, step: 46
	action: tensor([[ 0.0230,  0.0195, -0.0024,  0.0102,  0.0190,  0.0252,  0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0908]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.286430627061975, distance: 0.9666623281130146 entropy -13.57496376736153
epoch: 1, step: 47
	action: tensor([[ 0.0230,  0.0195,  0.0117, -0.0053,  0.0190,  0.0316,  0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0905]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28113803677224813, distance: 0.9702406038663515 entropy -13.575494103864267
epoch: 1, step: 48
	action: tensor([[ 0.0230,  0.0195,  0.0107, -0.0165,  0.0190,  0.0222,  0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0906]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27348123097924903, distance: 0.9753940734861374 entropy -13.574192057253255
epoch: 1, step: 49
	action: tensor([[0.0230, 0.0195, 0.0450, 0.0355, 0.0191, 0.0224, 0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0913]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2997040674899103, distance: 0.9576294538185122 entropy -13.574155137130882
epoch: 1, step: 50
	action: tensor([[ 0.0229,  0.0194,  0.0017, -0.0317,  0.0190,  0.0212,  0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0910]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2658850616794892, distance: 0.9804799655594657 entropy -13.573001083030885
epoch: 1, step: 51
	action: tensor([[ 0.0230,  0.0195,  0.0183, -0.0653,  0.0191,  0.0253,  0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0915]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2511400865120147, distance: 0.990277667302397 entropy -13.574504744985912
epoch: 1, step: 52
	action: tensor([[0.0230, 0.0195, 0.0166, 0.0353, 0.0192, 0.0170, 0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0925]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2969613641574457, distance: 0.9595028952708913 entropy -13.572839258612907
epoch: 1, step: 53
	action: tensor([[0.0229, 0.0195, 0.0235, 0.0107, 0.0190, 0.0185, 0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0907]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2858621794752537, distance: 0.9670472854109199 entropy -13.57481015984567
epoch: 1, step: 54
	action: tensor([[ 0.0229,  0.0194,  0.0288, -0.0304,  0.0190,  0.0280,  0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0912]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26849505567397014, distance: 0.9787354666526707 entropy -13.573903474854744
epoch: 1, step: 55
	action: tensor([[0.0229, 0.0194, 0.0194, 0.0242, 0.0191, 0.0238, 0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2934027581231825, distance: 0.9619282120974899 entropy -13.572725195275549
epoch: 1, step: 56
	action: tensor([[ 0.0229,  0.0194,  0.0283, -0.0097,  0.0190,  0.0169,  0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0906]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2758004594155723, distance: 0.9738359789115406 entropy -13.57434566259882
epoch: 1, step: 57
	action: tensor([[ 0.0229,  0.0194,  0.0105, -0.0455,  0.0191,  0.0194,  0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2590671475543642, distance: 0.9850224279088766 entropy -13.573176095763838
epoch: 1, step: 58
	action: tensor([[ 0.0230,  0.0195,  0.0072, -0.0343,  0.0191,  0.0210,  0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0921]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2648225278845623, distance: 0.9811892662102164 entropy -13.57368772257568
epoch: 1, step: 59
	action: tensor([[0.0230, 0.0195, 0.0294, 0.0041, 0.0191, 0.0257, 0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0916]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2846909839262155, distance: 0.9678399457727297 entropy -13.574128631436523
epoch: 1, step: 60
	action: tensor([[ 0.0229,  0.0194, -0.0015,  0.0365,  0.0190,  0.0174,  0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0910]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2966508999957558, distance: 0.9597147316954165 entropy -13.573259391602424
epoch: 1, step: 61
	action: tensor([[0.0230, 0.0195, 0.0025, 0.0142, 0.0190, 0.0236, 0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0905]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28809683734089664, distance: 0.9655330727758819 entropy -13.57591744702755
epoch: 1, step: 62
	action: tensor([[ 0.0230,  0.0195, -0.0021, -0.0424,  0.0190,  0.0188,  0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0905]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26046131175872234, distance: 0.9840952658743409 entropy -13.575284968222338
epoch: 1, step: 63
	action: tensor([[ 0.0230,  0.0195,  0.0243, -0.0274,  0.0191,  0.0176,  0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0917]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2674696037964146, distance: 0.9794212409858061 entropy -13.574531725869377
epoch: 1, step: 64
	action: tensor([[0.0229, 0.0194, 0.0402, 0.0404, 0.0191, 0.0113, 0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0921]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2990463400489951, distance: 0.9580790575625245 entropy -13.573043350485843
epoch: 1, step: 65
	action: tensor([[0.0229, 0.0194, 0.0501, 0.0667, 0.0190, 0.0157, 0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0913]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31344876271912725, distance: 0.9481851958788545 entropy -13.573510683897764
epoch: 1, step: 66
	action: tensor([[0.0229, 0.0194, 0.0047, 0.0014, 0.0189, 0.0179, 0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0908]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2806105349716558, distance: 0.9705965204648187 entropy -13.573000747414289
epoch: 1, step: 67
	action: tensor([[0.0230, 0.0195, 0.0288, 0.0024, 0.0190, 0.0280, 0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0911]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28441177887928004, distance: 0.9680288147981395 entropy -13.574936060109255
epoch: 1, step: 68
	action: tensor([[ 0.0229,  0.0194, -0.0040,  0.0232,  0.0190,  0.0192,  0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2908221654535461, distance: 0.9636831599000478 entropy -13.573230503569983
epoch: 1, step: 69
	action: tensor([[ 0.0230,  0.0195,  0.0132, -0.0067,  0.0190,  0.0212,  0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0905]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27798336110868527, distance: 0.9723671899875397 entropy -13.57584805790646
epoch: 1, step: 70
	action: tensor([[0.0229, 0.0195, 0.0314, 0.0129, 0.0191, 0.0201, 0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0912]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28757597038295035, distance: 0.9658862263679615 entropy -13.574194661626994
epoch: 1, step: 71
	action: tensor([[ 0.0229,  0.0194,  0.0051, -0.0101,  0.0190,  0.0258,  0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0912]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2772469703387632, distance: 0.9728629263070467 entropy -13.573433893495991
epoch: 1, step: 72
	action: tensor([[0.0230, 0.0195, 0.0027, 0.0126, 0.0191, 0.0261, 0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0908]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28792659698845324, distance: 0.9656485118367125 entropy -13.57462469786766
epoch: 1, step: 73
	action: tensor([[0.0230, 0.0195, 0.0149, 0.0823, 0.0190, 0.0243, 0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0904]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32057640863483783, distance: 0.9432504142173506 entropy -13.575196398430707
epoch: 1, step: 74
	action: tensor([[0.0229, 0.0195, 0.0042, 0.0361, 0.0189, 0.0186, 0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0896]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29707089481120985, distance: 0.959428148971247 entropy -13.575159205243596
epoch: 1, step: 75
	action: tensor([[ 0.0230,  0.0195,  0.0356, -0.0787,  0.0190,  0.0165,  0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0905]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24206908014641304, distance: 0.9962572744158695 entropy -13.575591921932439
epoch: 1, step: 76
	action: tensor([[ 0.0229,  0.0195,  0.0229, -0.0514,  0.0192,  0.0266,  0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0935]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2579769390568575, distance: 0.9857468424636479 entropy -13.571512862419905
epoch: 1, step: 77
	action: tensor([[ 0.0229,  0.0194,  0.0236, -0.0468,  0.0191,  0.0262,  0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0922]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2600995349658257, distance: 0.9843359424640673 entropy -13.57278495723601
epoch: 1, step: 78
	action: tensor([[0.0229, 0.0194, 0.0128, 0.0538, 0.0191, 0.0231, 0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0921]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30680406847017117, distance: 0.9527625888741964 entropy -13.57280889836787
epoch: 1, step: 79
	action: tensor([[0.0229, 0.0195, 0.0166, 0.0419, 0.0189, 0.0223, 0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0900]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.301260882220839, distance: 0.9565644175498668 entropy -13.575162778257246
epoch: 1, step: 80
	action: tensor([[ 0.0229,  0.0194,  0.0167, -0.0267,  0.0190,  0.0212,  0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0903]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26846858844771593, distance: 0.9787531727385466 entropy -13.574821757173467
epoch: 1, step: 81
	action: tensor([[0.0230, 0.0195, 0.0244, 0.0208, 0.0191, 0.0243, 0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0917]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2921180686913414, distance: 0.9628022728365744 entropy -13.573599983858676
epoch: 1, step: 82
	action: tensor([[ 0.0229,  0.0194, -0.0025, -0.0313,  0.0190,  0.0277,  0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0907]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2676437497094559, distance: 0.979304814182648 entropy -13.573951906075061
epoch: 1, step: 83
	action: tensor([[0.0230, 0.0195, 0.0292, 0.0511, 0.0191, 0.0187, 0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0910]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3054821918867989, distance: 0.953670582312043 entropy -13.574766409639631
epoch: 1, step: 84
	action: tensor([[ 0.0229,  0.0194,  0.0269, -0.0283,  0.0189,  0.0193,  0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0906]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26734957828926, distance: 0.9795014770529963 entropy -13.574144549155159
epoch: 1, step: 85
	action: tensor([[ 0.0229,  0.0194,  0.0171, -0.0015,  0.0191,  0.0152,  0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0921]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2790390668963447, distance: 0.97165605043114 entropy -13.572877171516106
epoch: 1, step: 86
	action: tensor([[0.0229, 0.0195, 0.0336, 0.0496, 0.0190, 0.0309, 0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0915]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3079629943327452, distance: 0.9519658133523626 entropy -13.574094963026072
epoch: 1, step: 87
	action: tensor([[ 0.0229,  0.0194,  0.0042, -0.0357,  0.0189,  0.0183,  0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0900]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26337462388980526, distance: 0.9821549983716231 entropy -13.573657553866097
epoch: 1, step: 88
	action: tensor([[ 0.0230,  0.0195,  0.0277, -0.0291,  0.0191,  0.0217,  0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0917]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26764383240211587, distance: 0.9793047588944449 entropy -13.574299934973359
epoch: 1, step: 89
	action: tensor([[ 0.0229,  0.0194,  0.0266, -0.0310,  0.0191,  0.0138,  0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0920]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26471850362774096, distance: 0.9812586806562682 entropy -13.572809080333991
epoch: 1, step: 90
	action: tensor([[ 0.0229,  0.0194,  0.0462, -0.1026,  0.0191,  0.0098,  0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0924]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2282543889012999, distance: 1.0052955900680132 entropy -13.57282596197601
epoch: 1, step: 91
	action: tensor([[0.0229, 0.0195, 0.0231, 0.0245, 0.0193, 0.0278, 0.0366]],
       dtype=torch.float64)
	q_value: tensor([[0.0945]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29467533015910297, distance: 0.9610616119151166 entropy -13.570596734178318
epoch: 1, step: 92
	action: tensor([[0.0229, 0.0194, 0.0120, 0.0419, 0.0190, 0.0227, 0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0904]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30111131226785826, distance: 0.956666791694721 entropy -13.574039596937187
epoch: 1, step: 93
	action: tensor([[0.0229, 0.0195, 0.0313, 0.0070, 0.0190, 0.0158, 0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0903]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28370237572804125, distance: 0.9685085268441008 entropy -13.575137731254523
epoch: 1, step: 94
	action: tensor([[ 0.0229,  0.0194,  0.0059, -0.0338,  0.0190,  0.0265,  0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0916]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26631300998970064, distance: 0.9801941411670579 entropy -13.573336681692865
epoch: 1, step: 95
	action: tensor([[0.0230, 0.0195, 0.0367, 0.0339, 0.0191, 0.0140, 0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2964658156740164, distance: 0.9598409964948805 entropy -13.574223645109603
epoch: 1, step: 96
	action: tensor([[0.0229, 0.0194, 0.0112, 0.0406, 0.0190, 0.0253, 0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0913]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3010909011142209, distance: 0.9566807613945029 entropy -13.573613451752708
epoch: 1, step: 97
	action: tensor([[ 0.0229,  0.0195,  0.0214, -0.0054,  0.0190,  0.0221,  0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0902]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.279009766175047, distance: 0.9716757948670474 entropy -13.575122166453625
epoch: 1, step: 98
	action: tensor([[0.0229, 0.0195, 0.0131, 0.0144, 0.0191, 0.0241, 0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0913]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28863110691620764, distance: 0.96517069781189 entropy -13.573660675234185
epoch: 1, step: 99
	action: tensor([[ 0.0230,  0.0195,  0.0226, -0.0321,  0.0190,  0.0233,  0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0907]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2664913751823501, distance: 0.9800749873955639 entropy -13.574603214519561
epoch: 1, step: 100
	action: tensor([[ 0.0230,  0.0195,  0.0243, -0.0137,  0.0191,  0.0166,  0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0919]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2737622637374383, distance: 0.9752054037557502 entropy -13.57313601818348
epoch: 1, step: 101
	action: tensor([[0.0229, 0.0195, 0.0378, 0.0071, 0.0191, 0.0180, 0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28448216083851374, distance: 0.9679812082012635 entropy -13.573357273435358
epoch: 1, step: 102
	action: tensor([[ 0.0229,  0.0194,  0.0111, -0.0120,  0.0190,  0.0205,  0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0916]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2751925571541691, distance: 0.9742446182990973 entropy -13.572918445178251
epoch: 1, step: 103
	action: tensor([[ 0.0230,  0.0195,  0.0252, -0.0076,  0.0191,  0.0209,  0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0913]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.277755075957801, distance: 0.9725208979739535 entropy -13.574226433595294
epoch: 1, step: 104
	action: tensor([[ 0.0229,  0.0195,  0.0085, -0.0052,  0.0191,  0.0243,  0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0915]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2792956703993603, distance: 0.9714831197464788 entropy -13.573393758422847
epoch: 1, step: 105
	action: tensor([[ 0.0230,  0.0195,  0.0018, -0.0049,  0.0191,  0.0280,  0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0910]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28019104632917824, distance: 0.9708794651694308 entropy -13.574509498944314
epoch: 1, step: 106
	action: tensor([[ 0.0230,  0.0195,  0.0277, -0.0519,  0.0190,  0.0224,  0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0906]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2567529813017684, distance: 0.9865594960279143 entropy -13.574916512829143
epoch: 1, step: 107
	action: tensor([[ 0.0229,  0.0194, -0.0023, -0.0288,  0.0191,  0.0262,  0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0925]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2684805575774356, distance: 0.9787451656513441 entropy -13.572427562463831
epoch: 1, step: 108
	action: tensor([[0.0230, 0.0195, 0.0091, 0.0056, 0.0191, 0.0175, 0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0910]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28284761437084616, distance: 0.9690862174678144 entropy -13.57478410667614
epoch: 1, step: 109
	action: tensor([[0.0230, 0.0195, 0.0078, 0.0452, 0.0190, 0.0166, 0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0911]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30104387871572325, distance: 0.9567129434532591 entropy -13.57476279811405
epoch: 1, step: 110
	action: tensor([[0.0229, 0.0195, 0.0359, 0.0013, 0.0190, 0.0316, 0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0905]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2849533678353443, distance: 0.9676624218533313 entropy -13.575510475039323
epoch: 1, step: 111
	action: tensor([[ 0.0229,  0.0194, -0.0003, -0.0818,  0.0190,  0.0191,  0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0910]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24190823854494892, distance: 0.9963629773968554 entropy -13.572723510151338
epoch: 1, step: 112
	action: tensor([[0.0230, 0.0195, 0.0305, 0.0223, 0.0192, 0.0187, 0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0927]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29178597803860107, distance: 0.9630280874322327 entropy -13.573716855306756
epoch: 1, step: 113
	action: tensor([[ 0.0229,  0.0194,  0.0216, -0.0259,  0.0190,  0.0211,  0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0911]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2689036645519818, distance: 0.9784620741984599 entropy -13.573704771340953
epoch: 1, step: 114
	action: tensor([[0.0229, 0.0194, 0.0421, 0.0492, 0.0191, 0.0195, 0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3054370682471794, distance: 0.9537015623590731 entropy -13.573277905833832
epoch: 1, step: 115
	action: tensor([[0.0229, 0.0194, 0.0001, 0.0219, 0.0189, 0.0307, 0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0908]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29304554239510006, distance: 0.9621713297103636 entropy -13.573324390500442
epoch: 1, step: 116
	action: tensor([[ 0.0230,  0.0195,  0.0167, -0.0012,  0.0190,  0.0213,  0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0900]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2807053987033612, distance: 0.9705325235285401 entropy -13.575442801020822
epoch: 1, step: 117
	action: tensor([[0.0229, 0.0194, 0.0219, 0.0734, 0.0190, 0.0232, 0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0911]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3166414032876558, distance: 0.945977973826536 entropy -13.574066554248905
epoch: 1, step: 118
	action: tensor([[ 0.0229,  0.0194,  0.0185, -0.0793,  0.0189,  0.0275,  0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0899]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24475781548349285, distance: 0.9944886092796242 entropy -13.574648200346756
epoch: 1, step: 119
	action: tensor([[0.0229, 0.0195, 0.0248, 0.0147, 0.0192, 0.0267, 0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0926]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2898376398947624, distance: 0.9643518508811905 entropy -13.572701670008948
epoch: 1, step: 120
	action: tensor([[ 0.0229,  0.0194,  0.0148, -0.0186,  0.0190,  0.0247,  0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0907]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2731401306281529, distance: 0.9756230202338213 entropy -13.573759375224594
epoch: 1, step: 121
	action: tensor([[0.0229, 0.0194, 0.0184, 0.0181, 0.0191, 0.0306, 0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0912]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29214481599258335, distance: 0.9627840829347301 entropy -13.57381974585485
epoch: 1, step: 122
	action: tensor([[0.0229, 0.0194, 0.0330, 0.0471, 0.0190, 0.0259, 0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0903]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3054704892385588, distance: 0.9536786169687632 entropy -13.574178273842781
epoch: 1, step: 123
	action: tensor([[0.0229, 0.0194, 0.0071, 0.0434, 0.0189, 0.0229, 0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0903]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30154070310445535, distance: 0.9563728628644858 entropy -13.573770760870989
epoch: 1, step: 124
	action: tensor([[ 0.0229,  0.0195,  0.0187, -0.0542,  0.0190,  0.0111,  0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0901]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25289998079438036, distance: 0.9891133579926548 entropy -13.575448835177436
epoch: 1, step: 125
	action: tensor([[ 0.0230,  0.0195,  0.0358, -0.0197,  0.0192,  0.0242,  0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0928]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2729063393480251, distance: 0.9757799100458556 entropy -13.572955462970624
epoch: 1, step: 126
	action: tensor([[ 0.0229,  0.0194,  0.0224, -0.0522,  0.0191,  0.0250,  0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0919]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2572072865580527, distance: 0.9862579356690401 entropy -13.572399005508872
epoch: 1, step: 127
	action: tensor([[ 0.0229,  0.0194,  0.0019, -0.0295,  0.0191,  0.0195,  0.0365]],
       dtype=torch.float64)
	q_value: tensor([[0.0923]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26665372588165137, distance: 0.979966519233353 entropy -13.572807063070703
LOSS epoch 1 actor 0.2108619744777235 critic 18.00016730847844 entropy 0.01
epoch: 2, step: 0
	action: tensor([[0.0237, 0.0575, 0.0114, 0.0244, 0.0213, 0.0229, 0.0414]],
       dtype=torch.float64)
	q_value: tensor([[0.2803]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32047104034430407, distance: 0.943323553295706 entropy -13.476336640638761
epoch: 2, step: 1
	action: tensor([[ 0.0237,  0.0574,  0.0398, -0.0382,  0.0212,  0.0119,  0.0413]],
       dtype=torch.float64)
	q_value: tensor([[0.2792]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2893471560246693, distance: 0.9646848151523284 entropy -13.476681453931334
epoch: 2, step: 2
	action: tensor([[0.0236, 0.0574, 0.0346, 0.0081, 0.0213, 0.0333, 0.0414]],
       dtype=torch.float64)
	q_value: tensor([[0.2844]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31620689510726774, distance: 0.9462786723561715 entropy -13.475330480406013
epoch: 2, step: 3
	action: tensor([[0.0237, 0.0574, 0.0315, 0.0499, 0.0212, 0.0254, 0.0413]],
       dtype=torch.float64)
	q_value: tensor([[0.2794]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33354392143914613, distance: 0.9342055863912502 entropy -13.475812365941703
epoch: 2, step: 4
	action: tensor([[ 0.0237,  0.0574,  0.0153, -0.0015,  0.0212,  0.0399,  0.0413]],
       dtype=torch.float64)
	q_value: tensor([[0.2779]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31268971405783663, distance: 0.9487092062110658 entropy -13.47637464543604
epoch: 2, step: 5
	action: tensor([[ 0.0237,  0.0574,  0.0464, -0.0656,  0.0213,  0.0138,  0.0414]],
       dtype=torch.float64)
	q_value: tensor([[0.2792]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27661204692255437, distance: 0.9732901523643507 entropy -13.47646405122792
epoch: 2, step: 6
	action: tensor([[0.0236, 0.0575, 0.0362, 0.0220, 0.0214, 0.0050, 0.0415]],
       dtype=torch.float64)
	q_value: tensor([[0.2861]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31602120553021185, distance: 0.9464071484680698 entropy -13.475082370980877
epoch: 2, step: 7
	action: tensor([[ 0.0237,  0.0574,  0.0285,  0.0588,  0.0212, -0.0038,  0.0414]],
       dtype=torch.float64)
	q_value: tensor([[0.2819]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33070992479380223, distance: 0.9361897583398946 entropy -13.476036506304931
epoch: 2, step: 8
	action: tensor([[0.0237, 0.0574, 0.0322, 0.0360, 0.0212, 0.0028, 0.0414]],
       dtype=torch.float64)
	q_value: tensor([[0.2810]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3219265582359564, distance: 0.9423127352016619 entropy -13.476676225877638
epoch: 2, step: 9
	action: tensor([[ 0.0237,  0.0574,  0.0295, -0.0070,  0.0212,  0.0053,  0.0414]],
       dtype=torch.float64)
	q_value: tensor([[0.2813]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3023857208287559, distance: 0.9557941630006769 entropy -13.476320547916766
epoch: 2, step: 10
	action: tensor([[ 0.0237,  0.0575, -0.0025, -0.0056,  0.0213,  0.0128,  0.0414]],
       dtype=torch.float64)
	q_value: tensor([[0.2833]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30416237497401966, distance: 0.954576299301904 entropy -13.476010285582076
epoch: 2, step: 11
	action: tensor([[ 0.0237,  0.0575, -0.0034,  0.0583,  0.0213,  0.0246,  0.0414]],
       dtype=torch.float64)
	q_value: tensor([[0.2819]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3352270521190809, distance: 0.9330251754419223 entropy -13.477032474504302
epoch: 2, step: 12
	action: tensor([[0.0237, 0.0574, 0.0043, 0.0321, 0.0212, 0.0160, 0.0413]],
       dtype=torch.float64)
	q_value: tensor([[0.2775]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3220302543423653, distance: 0.942240679660386 entropy -13.477491522293723
epoch: 2, step: 13
	action: tensor([[ 0.0237,  0.0574,  0.0112,  0.0252,  0.0212, -0.0008,  0.0414]],
       dtype=torch.float64)
	q_value: tensor([[0.2797]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3153396580994907, distance: 0.9468785525258814 entropy -13.476974773487614
epoch: 2, step: 14
	action: tensor([[0.0237, 0.0574, 0.0115, 0.0167, 0.0212, 0.0096, 0.0414]],
       dtype=torch.float64)
	q_value: tensor([[0.2819]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31386847129676065, distance: 0.9478953250882926 entropy -13.47674965869015
epoch: 2, step: 15
	action: tensor([[ 0.0237,  0.0574,  0.0376, -0.0017,  0.0213,  0.0158,  0.0414]],
       dtype=torch.float64)
	q_value: tensor([[0.2811]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30754263156413186, distance: 0.9522548948836369 entropy -13.476633523351198
epoch: 2, step: 16
	action: tensor([[ 0.0237,  0.0574, -0.0035, -0.0301,  0.0213,  0.0253,  0.0414]],
       dtype=torch.float64)
	q_value: tensor([[0.2819]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29593638393866895, distance: 0.9602020839360823 entropy -13.475675247805354
epoch: 2, step: 17
	action: tensor([[ 0.0237,  0.0575,  0.0301, -0.0360,  0.0213,  0.0030,  0.0414]],
       dtype=torch.float64)
	q_value: tensor([[0.2817]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2882325844763166, distance: 0.9654410134872399 entropy -13.476814895928156
epoch: 2, step: 18
	action: tensor([[ 0.0236,  0.0575,  0.0151,  0.0236,  0.0213, -0.0043,  0.0414]],
       dtype=torch.float64)
	q_value: tensor([[0.2850]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31390927937409485, distance: 0.9478671363523977 entropy -13.475700026077837
epoch: 2, step: 19
	action: tensor([[ 0.0237,  0.0575,  0.0062,  0.0434,  0.0212, -0.0013,  0.0414]],
       dtype=torch.float64)
	q_value: tensor([[0.2824]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32321729332711047, distance: 0.941415446356417 entropy -13.476628066405228
epoch: 2, step: 20
	action: tensor([[0.0237, 0.0575, 0.0342, 0.0150, 0.0212, 0.0089, 0.0414]],
       dtype=torch.float64)
	q_value: tensor([[0.2811]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31368843200859564, distance: 0.9480196796754408 entropy -13.477059292872374
epoch: 2, step: 21
	action: tensor([[ 0.0237,  0.0574,  0.0344, -0.0023,  0.0212,  0.0305,  0.0414]],
       dtype=torch.float64)
	q_value: tensor([[0.2815]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3106790183005511, distance: 0.9500958961291105 entropy -13.4759098713016
epoch: 2, step: 22
	action: tensor([[ 0.0236,  0.0574,  0.0356,  0.0482,  0.0213, -0.0003,  0.0414]],
       dtype=torch.float64)
	q_value: tensor([[0.2803]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3269442501404297, distance: 0.938819739937039 entropy -13.475758873455192
epoch: 2, step: 23
	action: tensor([[ 0.0237,  0.0574,  0.0194, -0.0413,  0.0212, -0.0003,  0.0413]],
       dtype=torch.float64)
	q_value: tensor([[0.2808]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2850074581279829, distance: 0.967625821349879 entropy -13.47626399279379
epoch: 2, step: 24
	action: tensor([[0.0237, 0.0575, 0.0256, 0.0450, 0.0213, 0.0037, 0.0414]],
       dtype=torch.float64)
	q_value: tensor([[0.2854]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3260061387126172, distance: 0.9394737800016766 entropy -13.476058645409196
epoch: 2, step: 25
	action: tensor([[ 0.0237,  0.0574,  0.0161, -0.0543,  0.0212,  0.0498,  0.0413]],
       dtype=torch.float64)
	q_value: tensor([[0.2805]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.290868094905646, distance: 0.9636519532308315 entropy -13.47652266593633
epoch: 2, step: 26
	action: tensor([[ 0.0236,  0.0574,  0.0201,  0.0481,  0.0213, -0.0095,  0.0414]],
       dtype=torch.float64)
	q_value: tensor([[0.2812]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32406860525820524, distance: 0.9408231658646502 entropy -13.476073982697006
epoch: 2, step: 27
	action: tensor([[ 0.0237,  0.0575,  0.0429,  0.0591,  0.0212, -0.0229,  0.0414]],
       dtype=torch.float64)
	q_value: tensor([[0.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32717540615606233, distance: 0.9386585107471278 entropy -13.476832564697457
epoch: 2, step: 28
	action: tensor([[ 0.0237,  0.0575,  0.0411, -0.0326,  0.0212,  0.0083,  0.0414]],
       dtype=torch.float64)
	q_value: tensor([[0.2832]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2911614126256319, distance: 0.9634526352855263 entropy -13.476241177517776
epoch: 2, step: 29
	action: tensor([[ 0.0236,  0.0575,  0.0267,  0.0205,  0.0213, -0.0053,  0.0414]],
       dtype=torch.float64)
	q_value: tensor([[0.2847]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3125777923093539, distance: 0.9487864470679558 entropy -13.475428251689419
epoch: 2, step: 30
	action: tensor([[ 0.0237,  0.0575,  0.0152, -0.0407,  0.0212,  0.0341,  0.0414]],
       dtype=torch.float64)
	q_value: tensor([[0.2829]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29342149805048034, distance: 0.9619154561856242 entropy -13.476355120792338
epoch: 2, step: 31
	action: tensor([[0.0236, 0.0574, 0.0159, 0.0547, 0.0213, 0.0320, 0.0414]],
       dtype=torch.float64)
	q_value: tensor([[0.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3364211450268795, distance: 0.9321868294285721 entropy -13.476207391768213
epoch: 2, step: 32
	action: tensor([[0.0237, 0.0574, 0.0361, 0.0380, 0.0212, 0.0079, 0.0413]],
       dtype=torch.float64)
	q_value: tensor([[0.2771]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3242033340471776, distance: 0.9407293972634725 entropy -13.476999730838271
epoch: 2, step: 33
	action: tensor([[ 0.0237,  0.0574,  0.0471, -0.0213,  0.0212,  0.0126,  0.0413]],
       dtype=torch.float64)
	q_value: tensor([[0.2804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2975875748814777, distance: 0.9590754757598762 entropy -13.476092166035048
epoch: 2, step: 34
	action: tensor([[ 0.0236,  0.0574,  0.0382, -0.0540,  0.0213, -0.0006,  0.0414]],
       dtype=torch.float64)
	q_value: tensor([[0.2835]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.278758732179217, distance: 0.9718449389257235 entropy -13.4752010310163
epoch: 2, step: 35
	action: tensor([[ 0.0236,  0.0575,  0.0430, -0.0626,  0.0213,  0.0173,  0.0415]],
       dtype=torch.float64)
	q_value: tensor([[0.2865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27896215099544586, distance: 0.971707879735514 entropy -13.475323927974296
epoch: 2, step: 36
	action: tensor([[0.0236, 0.0574, 0.0060, 0.0343, 0.0213, 0.0005, 0.0414]],
       dtype=torch.float64)
	q_value: tensor([[0.2852]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3194354311674794, distance: 0.9440420970685792 entropy -13.475109577128071
epoch: 2, step: 37
	action: tensor([[ 0.0237,  0.0575,  0.0400, -0.0400,  0.0212,  0.0370,  0.0414]],
       dtype=torch.float64)
	q_value: tensor([[0.2813]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2946352571603458, distance: 0.9610889128698533 entropy -13.476978430511434
epoch: 2, step: 38
	action: tensor([[ 0.0236,  0.0574,  0.0196, -0.0341,  0.0213,  0.0333,  0.0414]],
       dtype=torch.float64)
	q_value: tensor([[0.2819]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2962030129597678, distance: 0.9600202523665067 entropy -13.475296365252301
epoch: 2, step: 39
	action: tensor([[0.0236, 0.0574, 0.0223, 0.0032, 0.0213, 0.0115, 0.0414]],
       dtype=torch.float64)
	q_value: tensor([[0.2818]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3083905307533804, distance: 0.9516717084612486 entropy -13.47610082619401
epoch: 2, step: 40
	action: tensor([[0.0237, 0.0574, 0.0379, 0.0373, 0.0213, 0.0014, 0.0414]],
       dtype=torch.float64)
	q_value: tensor([[0.2820]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3224611703940471, distance: 0.9419411890312461 entropy -13.476306492882307
epoch: 2, step: 41
	action: tensor([[ 0.0237,  0.0574,  0.0461, -0.0149,  0.0212, -0.0091,  0.0414]],
       dtype=torch.float64)
	q_value: tensor([[0.2811]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2954181520572092, distance: 0.9605554012982283 entropy -13.47605417259887
epoch: 2, step: 42
	action: tensor([[ 0.0236,  0.0574,  0.0314, -0.0265,  0.0213,  0.0224,  0.0414]],
       dtype=torch.float64)
	q_value: tensor([[0.2853]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2973106986197096, distance: 0.9592644808657451 entropy -13.47527053043885
epoch: 2, step: 43
	action: tensor([[ 0.0236,  0.0574,  0.0257, -0.0255,  0.0213,  0.0159,  0.0414]],
       dtype=torch.float64)
	q_value: tensor([[0.2825]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29614905983361495, distance: 0.9600570492710857 entropy -13.475699422200922
epoch: 2, step: 44
	action: tensor([[ 0.0237,  0.0575,  0.0237, -0.0242,  0.0213,  0.0082,  0.0414]],
       dtype=torch.float64)
	q_value: tensor([[0.2833]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29496529370334934, distance: 0.9608640422779781 entropy -13.475984680345078
epoch: 2, step: 45
	action: tensor([[ 0.0237,  0.0575, -0.0013,  0.0048,  0.0213,  0.0111,  0.0414]],
       dtype=torch.float64)
	q_value: tensor([[0.2839]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3084647680719431, distance: 0.9516206308987577 entropy -13.476062859555691
epoch: 2, step: 46
	action: tensor([[0.0237, 0.0575, 0.0037, 0.0085, 0.0213, 0.0089, 0.0414]],
       dtype=torch.float64)
	q_value: tensor([[0.2817]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3098303967768179, distance: 0.9506805467136955 entropy -13.477053066313927
epoch: 2, step: 47
	action: tensor([[0.0237, 0.0575, 0.0108, 0.0391, 0.0213, 0.0139, 0.0414]],
       dtype=torch.float64)
	q_value: tensor([[0.2815]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3249908811624277, distance: 0.9401810927725424 entropy -13.476828999430472
epoch: 2, step: 48
	action: tensor([[ 0.0237,  0.0574, -0.0039,  0.0516,  0.0212,  0.0253,  0.0413]],
       dtype=torch.float64)
	q_value: tensor([[0.2796]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3323335220606529, distance: 0.9350535409290542 entropy -13.476850949337546
epoch: 2, step: 49
	action: tensor([[ 0.0237,  0.0574,  0.0368,  0.0274,  0.0212, -0.0013,  0.0413]],
       dtype=torch.float64)
	q_value: tensor([[0.2777]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.317132365928121, distance: 0.9456380912993464 entropy -13.477431501898169
epoch: 2, step: 50
	action: tensor([[ 0.0237,  0.0574,  0.0157, -0.0058,  0.0212,  0.0004,  0.0414]],
       dtype=torch.float64)
	q_value: tensor([[0.2819]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3015295507214204, distance: 0.9563804980938849 entropy -13.475970410815235
epoch: 2, step: 51
	action: tensor([[0.0237, 0.0575, 0.0165, 0.0287, 0.0213, 0.0255, 0.0414]],
       dtype=torch.float64)
	q_value: tensor([[0.2832]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32324236495213576, distance: 0.9413980086814965 entropy -13.47635805928945
epoch: 2, step: 52
	action: tensor([[0.0237, 0.0574, 0.0260, 0.0128, 0.0212, 0.0152, 0.0413]],
       dtype=torch.float64)
	q_value: tensor([[0.2788]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3138255582989843, distance: 0.9479249669224571 entropy -13.47656098027889
epoch: 2, step: 53
	action: tensor([[ 0.0237,  0.0574,  0.0319, -0.0215,  0.0212, -0.0080,  0.0414]],
       dtype=torch.float64)
	q_value: tensor([[0.2808]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29242554727862724, distance: 0.9625931466927994 entropy -13.4761450185708
epoch: 2, step: 54
	action: tensor([[ 0.0237,  0.0575,  0.0250, -0.0301,  0.0213,  0.0089,  0.0414]],
       dtype=torch.float64)
	q_value: tensor([[0.2853]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29239149596954706, distance: 0.9626163083277999 entropy -13.475728457093854
epoch: 2, step: 55
	action: tensor([[0.0237, 0.0575, 0.0464, 0.0462, 0.0213, 0.0042, 0.0414]],
       dtype=torch.float64)
	q_value: tensor([[0.2840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32762818054278453, distance: 0.9383426245116455 entropy -13.475897885823112
epoch: 2, step: 56
	action: tensor([[0.0237, 0.0574, 0.0259, 0.0226, 0.0212, 0.0444, 0.0413]],
       dtype=torch.float64)
	q_value: tensor([[0.2805]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3252385579155538, distance: 0.9400085896474909 entropy -13.475889509475369
epoch: 2, step: 57
	action: tensor([[ 0.0237,  0.0574,  0.0234, -0.0238,  0.0212,  0.0196,  0.0413]],
       dtype=torch.float64)
	q_value: tensor([[0.2773]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2977982926504006, distance: 0.9589316077158562 entropy -13.476244760314723
epoch: 2, step: 58
	action: tensor([[0.0237, 0.0574, 0.0312, 0.0471, 0.0213, 0.0094, 0.0414]],
       dtype=torch.float64)
	q_value: tensor([[0.2825]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32853204320553453, distance: 0.9377117101392163 entropy -13.475985051108381
epoch: 2, step: 59
	action: tensor([[0.0237, 0.0574, 0.0076, 0.0323, 0.0212, 0.0010, 0.0413]],
       dtype=torch.float64)
	q_value: tensor([[0.2800]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3188078260696007, distance: 0.9444772865625355 entropy -13.476460681559887
epoch: 2, step: 60
	action: tensor([[ 0.0237,  0.0574,  0.0350,  0.0700,  0.0212, -0.0027,  0.0414]],
       dtype=torch.float64)
	q_value: tensor([[0.2814]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33654661235947503, distance: 0.9320986978348874 entropy -13.476922578578598
epoch: 2, step: 61
	action: tensor([[ 0.0237,  0.0574,  0.0362, -0.0057,  0.0212,  0.0021,  0.0413]],
       dtype=torch.float64)
	q_value: tensor([[0.2801]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30235640610224146, distance: 0.9558142446926573 entropy -13.476520779846412
epoch: 2, step: 62
	action: tensor([[ 0.0237,  0.0574,  0.0263,  0.0368,  0.0213, -0.0034,  0.0414]],
       dtype=torch.float64)
	q_value: tensor([[0.2835]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3205501218731376, distance: 0.9432686611249892 entropy -13.47569584881701
epoch: 2, step: 63
	action: tensor([[0.0237, 0.0574, 0.0309, 0.0118, 0.0212, 0.0140, 0.0414]],
       dtype=torch.float64)
	q_value: tensor([[0.2817]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31324103824939786, distance: 0.9483286275415757 entropy -13.476440619582865
epoch: 2, step: 64
	action: tensor([[ 0.0237,  0.0574,  0.0100, -0.0152,  0.0212, -0.0010,  0.0414]],
       dtype=torch.float64)
	q_value: tensor([[0.2811]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2968083362884908, distance: 0.9596073153458899 entropy -13.475988614846514
epoch: 2, step: 65
	action: tensor([[ 0.0237,  0.0575,  0.0296, -0.0262,  0.0213,  0.0216,  0.0414]],
       dtype=torch.float64)
	q_value: tensor([[0.2838]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29732917858940666, distance: 0.9592518669729079 entropy -13.47650474084691
epoch: 2, step: 66
	action: tensor([[0.0236, 0.0574, 0.0369, 0.0600, 0.0213, 0.0165, 0.0414]],
       dtype=torch.float64)
	q_value: tensor([[0.2825]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3364638695295914, distance: 0.9321568195363698 entropy -13.47576436284852
epoch: 2, step: 67
	action: tensor([[0.0237, 0.0574, 0.0058, 0.0367, 0.0212, 0.0192, 0.0413]],
       dtype=torch.float64)
	q_value: tensor([[0.2784]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32480536166932616, distance: 0.9403102835730849 entropy -13.476360734047256
epoch: 2, step: 68
	action: tensor([[ 0.0237,  0.0574,  0.0151, -0.0487,  0.0212,  0.0212,  0.0413]],
       dtype=torch.float64)
	q_value: tensor([[0.2791]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28664279637029866, distance: 0.9665186060495712 entropy -13.47696531366985
epoch: 2, step: 69
	action: tensor([[0.0236, 0.0575, 0.0278, 0.0069, 0.0213, 0.0128, 0.0414]],
       dtype=torch.float64)
	q_value: tensor([[0.2835]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3105768014587371, distance: 0.9501663366093314 entropy -13.476113187869448
epoch: 2, step: 70
	action: tensor([[0.0237, 0.0574, 0.0010, 0.0003, 0.0213, 0.0067, 0.0414]],
       dtype=torch.float64)
	q_value: tensor([[0.2817]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3054726731372883, distance: 0.9536771175802384 entropy -13.47615676950674
epoch: 2, step: 71
	action: tensor([[0.0237, 0.0575, 0.0215, 0.0596, 0.0213, 0.0086, 0.0414]],
       dtype=torch.float64)
	q_value: tensor([[0.2824]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3336459147673395, distance: 0.9341340990226186 entropy -13.476964242786897
epoch: 2, step: 72
	action: tensor([[0.0237, 0.0574, 0.0258, 0.0349, 0.0212, 0.0145, 0.0413]],
       dtype=torch.float64)
	q_value: tensor([[0.2796]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32388937075615476, distance: 0.9409478950870341 entropy -13.47687127268428
epoch: 2, step: 73
	action: tensor([[0.0237, 0.0574, 0.0205, 0.0667, 0.0212, 0.0211, 0.0414]],
       dtype=torch.float64)
	q_value: tensor([[0.2800]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33961548795454566, distance: 0.9299404373192254 entropy -13.47646181421978
epoch: 2, step: 74
	action: tensor([[ 0.0237,  0.0574,  0.0265, -0.0449,  0.0212,  0.0059,  0.0413]],
       dtype=torch.float64)
	q_value: tensor([[0.2778]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2847672096626792, distance: 0.9677883762576655 entropy -13.47697837146209
epoch: 2, step: 75
	action: tensor([[ 0.0236,  0.0575,  0.0348, -0.0765,  0.0213,  0.0102,  0.0414]],
       dtype=torch.float64)
	q_value: tensor([[0.2854]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27078419672919163, distance: 0.9772028597253076 entropy -13.475861449620757
epoch: 2, step: 76
	action: tensor([[0.0236, 0.0575, 0.0270, 0.0240, 0.0214, 0.0094, 0.0415]],
       dtype=torch.float64)
	q_value: tensor([[0.2868]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.317670772673426, distance: 0.9452652237782105 entropy -13.475435353076177
epoch: 2, step: 77
	action: tensor([[0.0237, 0.0574, 0.0089, 0.0175, 0.0212, 0.0085, 0.0414]],
       dtype=torch.float64)
	q_value: tensor([[0.2811]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3138804103339726, distance: 0.9478870781222836 entropy -13.476319810557365
epoch: 2, step: 78
	action: tensor([[ 0.0237,  0.0575,  0.0206, -0.0078,  0.0213, -0.0056,  0.0414]],
       dtype=torch.float64)
	q_value: tensor([[0.2814]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29937446956311464, distance: 0.9578547839526467 entropy -13.47682919999804
epoch: 2, step: 79
	action: tensor([[ 0.0237,  0.0575,  0.0299, -0.0397,  0.0213,  0.0024,  0.0414]],
       dtype=torch.float64)
	q_value: tensor([[0.2843]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28639950562378447, distance: 0.9666834077690545 entropy -13.476292252242345
epoch: 2, step: 80
	action: tensor([[ 0.0236,  0.0575,  0.0081, -0.0511,  0.0213,  0.0202,  0.0414]],
       dtype=torch.float64)
	q_value: tensor([[0.2855]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28536667764931734, distance: 0.9673827182871292 entropy -13.475783500772138
epoch: 2, step: 81
	action: tensor([[ 0.0237,  0.0575,  0.0038, -0.0444,  0.0214,  0.0198,  0.0414]],
       dtype=torch.float64)
	q_value: tensor([[0.2838]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2883581693692189, distance: 0.9653558380828552 entropy -13.476421548086817
epoch: 2, step: 82
	action: tensor([[ 0.0237,  0.0575,  0.0169, -0.0582,  0.0214,  0.0142,  0.0414]],
       dtype=torch.float64)
	q_value: tensor([[0.2834]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28061963630286135, distance: 0.9705903807110627 entropy -13.476600891594392
epoch: 2, step: 83
	action: tensor([[0.0236, 0.0575, 0.0488, 0.0179, 0.0214, 0.0198, 0.0415]],
       dtype=torch.float64)
	q_value: tensor([[0.2850]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3180920988772409, distance: 0.9449733364518548 entropy -13.47609015168287
epoch: 2, step: 84
	action: tensor([[0.0236, 0.0574, 0.0290, 0.0758, 0.0212, 0.0248, 0.0414]],
       dtype=torch.float64)
	q_value: tensor([[0.2808]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34513196357988996, distance: 0.9260481958269154 entropy -13.47555800083594
epoch: 2, step: 85
	action: tensor([[0.0237, 0.0574, 0.0082, 0.0047, 0.0212, 0.0212, 0.0413]],
       dtype=torch.float64)
	q_value: tensor([[0.2770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31099382880352466, distance: 0.9498789186018208 entropy -13.476844497139796
epoch: 2, step: 86
	action: tensor([[0.0237, 0.0575, 0.0301, 0.0108, 0.0213, 0.0096, 0.0414]],
       dtype=torch.float64)
	q_value: tensor([[0.2807]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3117559038897889, distance: 0.9493534666092022 entropy -13.476733489813196
epoch: 2, step: 87
	action: tensor([[ 0.0237,  0.0574,  0.0173, -0.0946,  0.0212,  0.0162,  0.0414]],
       dtype=torch.float64)
	q_value: tensor([[0.2819]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26407518661068385, distance: 0.981687851248976 entropy -13.476113897928474
epoch: 2, step: 88
	action: tensor([[0.0236, 0.0575, 0.0252, 0.0041, 0.0214, 0.0084, 0.0415]],
       dtype=torch.float64)
	q_value: tensor([[0.2866]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3081690875125781, distance: 0.9518240519515445 entropy -13.475864054835304
epoch: 2, step: 89
	action: tensor([[ 0.0237,  0.0574,  0.0259, -0.0382,  0.0213,  0.0295,  0.0414]],
       dtype=torch.float64)
	q_value: tensor([[0.2823]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29351727423814267, distance: 0.9618502605157908 entropy -13.476227957037596
epoch: 2, step: 90
	action: tensor([[0.0236, 0.0574, 0.0199, 0.0251, 0.0213, 0.0299, 0.0414]],
       dtype=torch.float64)
	q_value: tensor([[0.2826]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3226988961761341, distance: 0.9417759266570884 entropy -13.475873764016182
epoch: 2, step: 91
	action: tensor([[0.0237, 0.0574, 0.0354, 0.0663, 0.0212, 0.0079, 0.0413]],
       dtype=torch.float64)
	q_value: tensor([[0.2786]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3372814073318391, distance: 0.9315823908929814 entropy -13.476428644914902
epoch: 2, step: 92
	action: tensor([[ 0.0237,  0.0574,  0.0304, -0.0114,  0.0212,  0.0200,  0.0413]],
       dtype=torch.float64)
	q_value: tensor([[0.2791]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3038046797034675, distance: 0.9548216177112444 entropy -13.476491919753032
epoch: 2, step: 93
	action: tensor([[0.0237, 0.0574, 0.0230, 0.0282, 0.0213, 0.0108, 0.0414]],
       dtype=torch.float64)
	q_value: tensor([[0.2818]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31979167002471365, distance: 0.9437949871586003 entropy -13.475839485640195
epoch: 2, step: 94
	action: tensor([[ 0.0237,  0.0574,  0.0135, -0.0497,  0.0212,  0.0157,  0.0414]],
       dtype=torch.float64)
	q_value: tensor([[0.2804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2848524083214351, distance: 0.9677307329781845 entropy -13.476369007531478
epoch: 2, step: 95
	action: tensor([[ 0.0236,  0.0575,  0.0294,  0.0316,  0.0213, -0.0070,  0.0414]],
       dtype=torch.float64)
	q_value: tensor([[0.2841]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31743474456442256, distance: 0.945428700486711 entropy -13.476164818919829
epoch: 2, step: 96
	action: tensor([[ 0.0237,  0.0574,  0.0353, -0.0314,  0.0212,  0.0087,  0.0414]],
       dtype=torch.float64)
	q_value: tensor([[0.2823]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29180571777709985, distance: 0.9630146663091758 entropy -13.476293234899527
epoch: 2, step: 97
	action: tensor([[ 0.0236,  0.0574,  0.0270, -0.0642,  0.0213, -0.0004,  0.0414]],
       dtype=torch.float64)
	q_value: tensor([[0.2842]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27416519154678853, distance: 0.974934836781314 entropy -13.47554276020662
epoch: 2, step: 98
	action: tensor([[ 0.0236,  0.0575,  0.0207,  0.0297,  0.0214, -0.0132,  0.0415]],
       dtype=torch.float64)
	q_value: tensor([[0.2868]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31482871390109846, distance: 0.9472318020459913 entropy -13.475653984383767
epoch: 2, step: 99
	action: tensor([[0.0237, 0.0575, 0.0409, 0.0002, 0.0212, 0.0047, 0.0414]],
       dtype=torch.float64)
	q_value: tensor([[0.2831]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3058834932521802, distance: 0.9533950209067718 entropy -13.476556228112397
epoch: 2, step: 100
	action: tensor([[0.0237, 0.0574, 0.0130, 0.0854, 0.0213, 0.0083, 0.0414]],
       dtype=torch.float64)
	q_value: tensor([[0.2830]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3445545643251312, distance: 0.9264563557146834 entropy -13.475566800349942
epoch: 2, step: 101
	action: tensor([[0.0237, 0.0574, 0.0259, 0.0636, 0.0212, 0.0201, 0.0413]],
       dtype=torch.float64)
	q_value: tensor([[0.2782]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3383688356991099, distance: 0.9308177790120696 entropy -13.477290200407486
epoch: 2, step: 102
	action: tensor([[0.0237, 0.0574, 0.0121, 0.0200, 0.0212, 0.0089, 0.0413]],
       dtype=torch.float64)
	q_value: tensor([[0.2778]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.315202040470973, distance: 0.9469737096611774 entropy -13.476715680002275
epoch: 2, step: 103
	action: tensor([[0.0237, 0.0574, 0.0233, 0.0470, 0.0212, 0.0214, 0.0414]],
       dtype=torch.float64)
	q_value: tensor([[0.2810]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33087771562022517, distance: 0.9360723997344662 entropy -13.476645815940525
epoch: 2, step: 104
	action: tensor([[0.0237, 0.0574, 0.0177, 0.0029, 0.0212, 0.0199, 0.0413]],
       dtype=torch.float64)
	q_value: tensor([[0.2784]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3101024037817809, distance: 0.950493188960759 entropy -13.476583230458534
epoch: 2, step: 105
	action: tensor([[0.0237, 0.0574, 0.0267, 0.0181, 0.0213, 0.0136, 0.0414]],
       dtype=torch.float64)
	q_value: tensor([[0.2808]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31593583275706183, distance: 0.9464662108851831 entropy -13.476325143024116
epoch: 2, step: 106
	action: tensor([[0.0237, 0.0574, 0.0159, 0.0182, 0.0212, 0.0193, 0.0414]],
       dtype=torch.float64)
	q_value: tensor([[0.2810]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31696137709262806, distance: 0.945756476933168 entropy -13.476261480724235
epoch: 2, step: 107
	action: tensor([[ 0.0237,  0.0574,  0.0274, -0.0460,  0.0212,  0.0140,  0.0414]],
       dtype=torch.float64)
	q_value: tensor([[0.2802]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28618592728046466, distance: 0.9668280595709259 entropy -13.47659401247324
epoch: 2, step: 108
	action: tensor([[ 0.0236,  0.0575,  0.0120, -0.0022,  0.0213,  0.0166,  0.0414]],
       dtype=torch.float64)
	q_value: tensor([[0.2846]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30688019803017763, distance: 0.9527102693315036 entropy -13.475811063192111
epoch: 2, step: 109
	action: tensor([[ 0.0237,  0.0575,  0.0203,  0.0269,  0.0213, -0.0044,  0.0414]],
       dtype=torch.float64)
	q_value: tensor([[0.2816]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3155972702257649, distance: 0.9467003982413252 entropy -13.476581539541652
epoch: 2, step: 110
	action: tensor([[0.0237, 0.0575, 0.0290, 0.0012, 0.0212, 0.0099, 0.0414]],
       dtype=torch.float64)
	q_value: tensor([[0.2825]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30734895064699674, distance: 0.9523880588263129 entropy -13.476613581101853
epoch: 2, step: 111
	action: tensor([[ 0.0237,  0.0574,  0.0172, -0.0562,  0.0213,  0.0085,  0.0414]],
       dtype=torch.float64)
	q_value: tensor([[0.2824]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28016289008342965, distance: 0.9708984535783381 entropy -13.476086455724584
epoch: 2, step: 112
	action: tensor([[0.0236, 0.0575, 0.0322, 0.0235, 0.0214, 0.0004, 0.0415]],
       dtype=torch.float64)
	q_value: tensor([[0.2855]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.315571317145087, distance: 0.9467183478766952 entropy -13.47610341802778
epoch: 2, step: 113
	action: tensor([[0.0237, 0.0574, 0.0306, 0.0477, 0.0212, 0.0110, 0.0414]],
       dtype=torch.float64)
	q_value: tensor([[0.2822]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3291662394010335, distance: 0.9372687748657316 entropy -13.47619096526042
epoch: 2, step: 114
	action: tensor([[ 2.3676e-02,  5.7396e-02,  1.5853e-02, -1.1515e-01,  2.1199e-02,
          8.3571e-05,  4.1336e-02]], dtype=torch.float64)
	q_value: tensor([[0.2796]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2506869905753827, distance: 0.9905772045922218 entropy -13.476390286885328
epoch: 2, step: 115
	action: tensor([[ 0.0236,  0.0575,  0.0270, -0.0345,  0.0215, -0.0083,  0.0415]],
       dtype=torch.float64)
	q_value: tensor([[0.2890]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28627531948557894, distance: 0.9667675188772548 entropy -13.475684925305796
epoch: 2, step: 116
	action: tensor([[0.0237, 0.0575, 0.0457, 0.0534, 0.0213, 0.0294, 0.0414]],
       dtype=torch.float64)
	q_value: tensor([[0.2860]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3370322113286899, distance: 0.9317575216287824 entropy -13.475842459619106
epoch: 2, step: 117
	action: tensor([[0.0237, 0.0573, 0.0186, 0.0594, 0.0212, 0.0094, 0.0413]],
       dtype=torch.float64)
	q_value: tensor([[0.2775]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33343249358524363, distance: 0.9342836801893365 entropy -13.4759785076702
epoch: 2, step: 118
	action: tensor([[ 0.0237,  0.0574,  0.0256, -0.0539,  0.0212,  0.0362,  0.0413]],
       dtype=torch.float64)
	q_value: tensor([[0.2792]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2877967531574488, distance: 0.9657365489592705 entropy -13.476861784758402
epoch: 2, step: 119
	action: tensor([[0.0236, 0.0574, 0.0132, 0.0482, 0.0213, 0.0194, 0.0414]],
       dtype=torch.float64)
	q_value: tensor([[0.2825]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33038617495141387, distance: 0.9364161584120544 entropy -13.475709490623803
epoch: 2, step: 120
	action: tensor([[ 0.0237,  0.0574,  0.0461, -0.0023,  0.0212,  0.0185,  0.0413]],
       dtype=torch.float64)
	q_value: tensor([[0.2786]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30810373147487513, distance: 0.9518690094537433 entropy -13.476893648268453
epoch: 2, step: 121
	action: tensor([[0.0236, 0.0574, 0.0303, 0.0411, 0.0212, 0.0143, 0.0414]],
       dtype=torch.float64)
	q_value: tensor([[0.2818]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32682822238036335, distance: 0.9389006577936929 entropy -13.47537155840185
epoch: 2, step: 122
	action: tensor([[ 0.0237,  0.0574,  0.0142, -0.0165,  0.0212,  0.0106,  0.0413]],
       dtype=torch.float64)
	q_value: tensor([[0.2795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29896151653160863, distance: 0.9581370251442687 entropy -13.476307654285305
epoch: 2, step: 123
	action: tensor([[ 0.0237,  0.0575,  0.0245, -0.0760,  0.0213,  0.0304,  0.0414]],
       dtype=torch.float64)
	q_value: tensor([[0.2830]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2760898932743644, distance: 0.973641357638523 entropy -13.476430244809949
epoch: 2, step: 124
	action: tensor([[ 0.0236,  0.0575,  0.0204, -0.0067,  0.0214,  0.0073,  0.0415]],
       dtype=torch.float64)
	q_value: tensor([[0.2844]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30279660864152047, distance: 0.9555126449544572 entropy -13.47573593262803
epoch: 2, step: 125
	action: tensor([[ 0.0237,  0.0575,  0.0177, -0.0636,  0.0213,  0.0196,  0.0414]],
       dtype=torch.float64)
	q_value: tensor([[0.2830]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2793892639001122, distance: 0.9714200373876605 entropy -13.476298899711876
epoch: 2, step: 126
	action: tensor([[ 0.0236,  0.0575,  0.0357, -0.0223,  0.0214,  0.0147,  0.0415]],
       dtype=torch.float64)
	q_value: tensor([[0.2848]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2975111472834755, distance: 0.9591276515461473 entropy -13.476027239972877
epoch: 2, step: 127
	action: tensor([[ 0.0236,  0.0574,  0.0171, -0.0116,  0.0213,  0.0029,  0.0414]],
       dtype=torch.float64)
	q_value: tensor([[0.2834]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29946406971674666, distance: 0.9577935339152409 entropy -13.475693345434905
LOSS epoch 2 actor 0.19907482908598426 critic 21.46811078481555 entropy 0.01
epoch: 3, step: 0
	action: tensor([[ 0.0179,  0.0703,  0.0187, -0.0251,  0.0344,  0.0117,  0.0184]],
       dtype=torch.float64)
	q_value: tensor([[0.5731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2981576526862839, distance: 0.9586862040140881 entropy -13.423348066775171
epoch: 3, step: 1
	action: tensor([[0.0179, 0.0704, 0.0380, 0.0737, 0.0344, 0.0103, 0.0184]],
       dtype=torch.float64)
	q_value: tensor([[0.5761]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3437928833018159, distance: 0.9269945083173836 entropy -13.423693583902892
epoch: 3, step: 2
	action: tensor([[0.0180, 0.0703, 0.0390, 0.0092, 0.0343, 0.0220, 0.0183]],
       dtype=torch.float64)
	q_value: tensor([[0.5673]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3168616725772996, distance: 0.94582550139759 entropy -13.423565410611333
epoch: 3, step: 3
	action: tensor([[ 0.0179,  0.0703,  0.0289, -0.0698,  0.0344,  0.0144,  0.0183]],
       dtype=torch.float64)
	q_value: tensor([[0.5717]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27809912300960205, distance: 0.972289236391703 entropy -13.423195275881772
epoch: 3, step: 4
	action: tensor([[0.0178, 0.0704, 0.0424, 0.0162, 0.0345, 0.0207, 0.0184]],
       dtype=torch.float64)
	q_value: tensor([[0.5805]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31986199536522153, distance: 0.9437461973860118 entropy -13.423433854983816
epoch: 3, step: 5
	action: tensor([[ 0.0179,  0.0703,  0.0053, -0.0351,  0.0344,  0.0076,  0.0183]],
       dtype=torch.float64)
	q_value: tensor([[0.5713]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2925351735849455, distance: 0.9625185753043013 entropy -13.423145749743705
epoch: 3, step: 6
	action: tensor([[ 0.0179,  0.0704,  0.0458,  0.0252,  0.0344, -0.0026,  0.0184]],
       dtype=torch.float64)
	q_value: tensor([[0.5774]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31873268527799936, distance: 0.9445293767190903 entropy -13.424019162876652
epoch: 3, step: 7
	action: tensor([[0.0179, 0.0703, 0.0350, 0.0086, 0.0343, 0.0137, 0.0183]],
       dtype=torch.float64)
	q_value: tensor([[0.5740]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3144455163545604, distance: 0.9474966454313866 entropy -13.422960698874885
epoch: 3, step: 8
	action: tensor([[ 0.0179,  0.0703,  0.0140,  0.0149,  0.0344, -0.0003,  0.0183]],
       dtype=torch.float64)
	q_value: tensor([[0.5725]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.313425498464763, distance: 0.9482012606920229 entropy -13.423194571030235
epoch: 3, step: 9
	action: tensor([[ 0.0180,  0.0704,  0.0338, -0.0369,  0.0344,  0.0185,  0.0184]],
       dtype=torch.float64)
	q_value: tensor([[0.5735]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2944818748707769, distance: 0.9611934020766575 entropy -13.423693260086123
epoch: 3, step: 10
	action: tensor([[ 0.0179,  0.0703,  0.0120, -0.1008,  0.0344,  0.0146,  0.0184]],
       dtype=torch.float64)
	q_value: tensor([[0.5762]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2641768478832549, distance: 0.9816200433144906 entropy -13.423172204186189
epoch: 3, step: 11
	action: tensor([[ 0.0178,  0.0704,  0.0250, -0.0693,  0.0345,  0.0057,  0.0184]],
       dtype=torch.float64)
	q_value: tensor([[0.5825]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2762902710097185, distance: 0.9735065967400675 entropy -13.423875204013987
epoch: 3, step: 12
	action: tensor([[0.0179, 0.0704, 0.0493, 0.0626, 0.0345, 0.0464, 0.0184]],
       dtype=torch.float64)
	q_value: tensor([[0.5818]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3480156392928576, distance: 0.924007044939803 entropy -13.423562742927967
epoch: 3, step: 13
	action: tensor([[0.0179, 0.0703, 0.0258, 0.0413, 0.0343, 0.0076, 0.0183]],
       dtype=torch.float64)
	q_value: tensor([[0.5625]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3275964428827509, distance: 0.9383647703295739 entropy -13.423366267037093
epoch: 3, step: 14
	action: tensor([[ 0.0180,  0.0703,  0.0117,  0.0050,  0.0344, -0.0035,  0.0183]],
       dtype=torch.float64)
	q_value: tensor([[0.5704]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3082469503436752, distance: 0.9517704884294353 entropy -13.42364902624928
epoch: 3, step: 15
	action: tensor([[ 0.0180,  0.0704,  0.0283, -0.0111,  0.0344,  0.0077,  0.0184]],
       dtype=torch.float64)
	q_value: tensor([[0.5755]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30388385090367487, distance: 0.9547673251007364 entropy -13.423833930413462
epoch: 3, step: 16
	action: tensor([[ 0.0179,  0.0704,  0.0362, -0.0270,  0.0344,  0.0056,  0.0184]],
       dtype=torch.float64)
	q_value: tensor([[0.5758]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29597304684146175, distance: 0.9601770831739581 entropy -13.42344642146908
epoch: 3, step: 17
	action: tensor([[0.0179, 0.0704, 0.0430, 0.0979, 0.0344, 0.0177, 0.0184]],
       dtype=torch.float64)
	q_value: tensor([[0.5780]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35690938525451843, distance: 0.9176831955214513 entropy -13.423258740216168
epoch: 3, step: 18
	action: tensor([[0.0180, 0.0703, 0.0508, 0.0478, 0.0343, 0.0164, 0.0183]],
       dtype=torch.float64)
	q_value: tensor([[0.5639]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3340094561222511, distance: 0.9338792474996754 entropy -13.423551670891124
epoch: 3, step: 19
	action: tensor([[0.0179, 0.0703, 0.0152, 0.0572, 0.0343, 0.0040, 0.0183]],
       dtype=torch.float64)
	q_value: tensor([[0.5691]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3334082110728045, distance: 0.9343006976318733 entropy -13.423128046149019
epoch: 3, step: 20
	action: tensor([[ 0.0180,  0.0704,  0.0561, -0.0580,  0.0344,  0.0058,  0.0183]],
       dtype=torch.float64)
	q_value: tensor([[0.5693]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2813319507045918, distance: 0.9701097332193311 entropy -13.423979392021097
epoch: 3, step: 21
	action: tensor([[ 0.0178,  0.0703,  0.0130, -0.0622,  0.0344,  0.0056,  0.0184]],
       dtype=torch.float64)
	q_value: tensor([[0.5817]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2796784986226475, distance: 0.9712250662403094 entropy -13.422695451105554
epoch: 3, step: 22
	action: tensor([[0.0179, 0.0704, 0.0458, 0.0653, 0.0345, 0.0232, 0.0184]],
       dtype=torch.float64)
	q_value: tensor([[0.5807]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3434698136407246, distance: 0.927222673283872 entropy -13.423857970745193
epoch: 3, step: 23
	action: tensor([[ 0.0179,  0.0703,  0.0163, -0.0103,  0.0343,  0.0131,  0.0183]],
       dtype=torch.float64)
	q_value: tensor([[0.5661]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30516365676660084, distance: 0.9538892539909443 entropy -13.423366442008971
epoch: 3, step: 24
	action: tensor([[ 0.0179,  0.0704,  0.0477, -0.0007,  0.0344, -0.0106,  0.0184]],
       dtype=torch.float64)
	q_value: tensor([[0.5743]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3046037201319036, distance: 0.9542735243174051 entropy -13.423740175647918
epoch: 3, step: 25
	action: tensor([[ 0.0179,  0.0703,  0.0359, -0.0140,  0.0344,  0.0224,  0.0184]],
       dtype=torch.float64)
	q_value: tensor([[0.5786]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3060851134973228, distance: 0.953256544370906 entropy -13.422996490993105
epoch: 3, step: 26
	action: tensor([[ 0.0179,  0.0703,  0.0618, -0.0023,  0.0344,  0.0080,  0.0183]],
       dtype=torch.float64)
	q_value: tensor([[0.5738]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3085996153315891, distance: 0.9515278448159569 entropy -13.42324440608267
epoch: 3, step: 27
	action: tensor([[0.0179, 0.0703, 0.0283, 0.0178, 0.0344, 0.0182, 0.0184]],
       dtype=torch.float64)
	q_value: tensor([[0.5755]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3194566881852974, distance: 0.9440273536648678 entropy -13.4225311245161
epoch: 3, step: 28
	action: tensor([[0.0179, 0.0703, 0.0218, 0.0898, 0.0344, 0.0319, 0.0183]],
       dtype=torch.float64)
	q_value: tensor([[0.5705]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3548208540517651, distance: 0.9191721430724101 entropy -13.423368647259595
epoch: 3, step: 29
	action: tensor([[0.0179, 0.0703, 0.0503, 0.0948, 0.0343, 0.0040, 0.0183]],
       dtype=torch.float64)
	q_value: tensor([[0.5611]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35283153595300265, distance: 0.9205881206815868 entropy -13.42401220821289
epoch: 3, step: 30
	action: tensor([[ 0.0180,  0.0702,  0.0426, -0.0392,  0.0343,  0.0086,  0.0183]],
       dtype=torch.float64)
	q_value: tensor([[0.5663]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2909537824590287, distance: 0.9635937303020949 entropy -13.423123102902835
epoch: 3, step: 31
	action: tensor([[ 0.0179,  0.0703,  0.0285, -0.0433,  0.0344,  0.0123,  0.0184]],
       dtype=torch.float64)
	q_value: tensor([[0.5784]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2898778989358445, distance: 0.9643245159834197 entropy -13.422962220180102
epoch: 3, step: 32
	action: tensor([[ 0.0179,  0.0704,  0.0342,  0.0243,  0.0344, -0.0002,  0.0184]],
       dtype=torch.float64)
	q_value: tensor([[0.5777]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3183495745937387, distance: 0.9447949174469402 entropy -13.42332823689075
epoch: 3, step: 33
	action: tensor([[ 0.0179,  0.0703,  0.0144, -0.0216,  0.0343, -0.0048,  0.0183]],
       dtype=torch.float64)
	q_value: tensor([[0.5733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29588007990585685, distance: 0.9602404768934724 entropy -13.423258624500923
epoch: 3, step: 34
	action: tensor([[ 0.0179,  0.0704, -0.0046, -0.0587,  0.0344,  0.0092,  0.0184]],
       dtype=torch.float64)
	q_value: tensor([[0.5779]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2825065179926577, distance: 0.9693166514105398 entropy -13.423718031880227
epoch: 3, step: 35
	action: tensor([[ 0.0179,  0.0704,  0.0237, -0.0085,  0.0345,  0.0022,  0.0184]],
       dtype=torch.float64)
	q_value: tensor([[0.5787]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30365063665633685, distance: 0.9549272457526776 entropy -13.424162332367988
epoch: 3, step: 36
	action: tensor([[0.0179, 0.0704, 0.0248, 0.0121, 0.0344, 0.0010, 0.0184]],
       dtype=torch.float64)
	q_value: tensor([[0.5758]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3128055330359427, distance: 0.9486292691270869 entropy -13.42346141341846
epoch: 3, step: 37
	action: tensor([[0.0179, 0.0703, 0.0160, 0.0371, 0.0344, 0.0202, 0.0184]],
       dtype=torch.float64)
	q_value: tensor([[0.5739]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32828846626072283, distance: 0.9378817735363416 entropy -13.423444568430138
epoch: 3, step: 38
	action: tensor([[ 0.0179,  0.0703,  0.0148,  0.0282,  0.0344, -0.0210,  0.0183]],
       dtype=torch.float64)
	q_value: tensor([[0.5679]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31470579110350183, distance: 0.9473167670493478 entropy -13.42376164021462
epoch: 3, step: 39
	action: tensor([[ 0.0180,  0.0704,  0.0264, -0.0078,  0.0344,  0.0082,  0.0184]],
       dtype=torch.float64)
	q_value: tensor([[0.5758]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3055155344827891, distance: 0.9536476900015713 entropy -13.423626149218224
epoch: 3, step: 40
	action: tensor([[ 0.0179,  0.0703,  0.0392, -0.0508,  0.0344, -0.0063,  0.0184]],
       dtype=torch.float64)
	q_value: tensor([[0.5748]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28190997790929484, distance: 0.96971952343018 entropy -13.423379133758356
epoch: 3, step: 41
	action: tensor([[ 0.0179,  0.0704,  0.0391, -0.0101,  0.0344,  0.0283,  0.0184]],
       dtype=torch.float64)
	q_value: tensor([[0.5825]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3093680159473233, distance: 0.9509989487836646 entropy -13.423218903544775
epoch: 3, step: 42
	action: tensor([[0.0179, 0.0703, 0.0189, 0.0043, 0.0344, 0.0352, 0.0183]],
       dtype=torch.float64)
	q_value: tensor([[0.5725]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31707136978638606, distance: 0.9456803242184182 entropy -13.423169377953485
epoch: 3, step: 43
	action: tensor([[ 0.0179,  0.0704,  0.0438, -0.0034,  0.0344,  0.0069,  0.0183]],
       dtype=torch.float64)
	q_value: tensor([[0.5692]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30746125147926473, distance: 0.9523108494528034 entropy -13.423668819670453
epoch: 3, step: 44
	action: tensor([[ 0.0179,  0.0703,  0.0116, -0.0401,  0.0344,  0.0026,  0.0184]],
       dtype=torch.float64)
	q_value: tensor([[0.5752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2891198222054442, distance: 0.9648391014183972 entropy -13.422986797749035
epoch: 3, step: 45
	action: tensor([[ 0.0179,  0.0704,  0.0105, -0.0253,  0.0344,  0.0157,  0.0184]],
       dtype=torch.float64)
	q_value: tensor([[0.5784]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2990084210834082, distance: 0.9581049714549695 entropy -13.42377799218363
epoch: 3, step: 46
	action: tensor([[ 0.0179,  0.0704,  0.0016, -0.0421,  0.0344,  0.0026,  0.0184]],
       dtype=torch.float64)
	q_value: tensor([[0.5747]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2883484233985789, distance: 0.9653624483585855 entropy -13.423778421323709
epoch: 3, step: 47
	action: tensor([[ 0.0179,  0.0704,  0.0349, -0.0221,  0.0344,  0.0221,  0.0184]],
       dtype=torch.float64)
	q_value: tensor([[0.5783]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30228494567168773, distance: 0.9558631960118239 entropy -13.424009119306948
epoch: 3, step: 48
	action: tensor([[0.0179, 0.0703, 0.0480, 0.0880, 0.0344, 0.0041, 0.0184]],
       dtype=torch.float64)
	q_value: tensor([[0.5746]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34952054321503645, distance: 0.9229400368595055 entropy -13.423268745332736
epoch: 3, step: 49
	action: tensor([[0.0180, 0.0703, 0.0439, 0.0040, 0.0343, 0.0074, 0.0183]],
       dtype=torch.float64)
	q_value: tensor([[0.5673]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3110341826299966, distance: 0.9498511018623509 entropy -13.423306054479541
epoch: 3, step: 50
	action: tensor([[0.0179, 0.0703, 0.0250, 0.0599, 0.0344, 0.0180, 0.0183]],
       dtype=torch.float64)
	q_value: tensor([[0.5748]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3384816384512581, distance: 0.9307384271853801 entropy -13.423094826002272
epoch: 3, step: 51
	action: tensor([[0.0180, 0.0703, 0.0499, 0.0397, 0.0343, 0.0133, 0.0183]],
       dtype=torch.float64)
	q_value: tensor([[0.5669]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32945049543346894, distance: 0.9370701768312748 entropy -13.423834730648545
epoch: 3, step: 52
	action: tensor([[ 0.0179,  0.0702,  0.0460,  0.0432,  0.0343, -0.0053,  0.0183]],
       dtype=torch.float64)
	q_value: tensor([[0.5700]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32635509186351175, distance: 0.9392305472376554 entropy -13.422988655436798
epoch: 3, step: 53
	action: tensor([[ 0.0179,  0.0703,  0.0476, -0.0132,  0.0343,  0.0014,  0.0183]],
       dtype=torch.float64)
	q_value: tensor([[0.5726]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3015343551606803, distance: 0.9563772088495665 entropy -13.423013386538845
epoch: 3, step: 54
	action: tensor([[ 0.0179,  0.0703,  0.0548, -0.0296,  0.0344,  0.0104,  0.0184]],
       dtype=torch.float64)
	q_value: tensor([[0.5772]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29596512793472085, distance: 0.9601824832025799 entropy -13.42287861849499
epoch: 3, step: 55
	action: tensor([[ 0.0178,  0.0703,  0.0235, -0.0281,  0.0344,  0.0184,  0.0184]],
       dtype=torch.float64)
	q_value: tensor([[0.5776]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2982966416006043, distance: 0.9585912728997911 entropy -13.422642698408149
epoch: 3, step: 56
	action: tensor([[0.0179, 0.0703, 0.0319, 0.0031, 0.0344, 0.0022, 0.0184]],
       dtype=torch.float64)
	q_value: tensor([[0.5749]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3090835896331908, distance: 0.9511947558766701 entropy -13.423442182620537
epoch: 3, step: 57
	action: tensor([[0.0179, 0.0704, 0.0437, 0.0373, 0.0344, 0.0237, 0.0184]],
       dtype=torch.float64)
	q_value: tensor([[0.5754]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33053846365590167, distance: 0.9363096688897895 entropy -13.423378902718655
epoch: 3, step: 58
	action: tensor([[ 0.0179,  0.0703,  0.0440, -0.0480,  0.0343,  0.0012,  0.0183]],
       dtype=torch.float64)
	q_value: tensor([[0.5687]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2849444186389817, distance: 0.9676684772443652 entropy -13.423263604289238
epoch: 3, step: 59
	action: tensor([[0.0179, 0.0704, 0.0211, 0.0171, 0.0344, 0.0143, 0.0184]],
       dtype=torch.float64)
	q_value: tensor([[0.5811]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3180257195218257, distance: 0.9450193288757719 entropy -13.423074042861748
epoch: 3, step: 60
	action: tensor([[0.0179, 0.0704, 0.0257, 0.0522, 0.0344, 0.0373, 0.0183]],
       dtype=torch.float64)
	q_value: tensor([[0.5715]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33964942654219044, distance: 0.9299165411875275 entropy -13.423642836588513
epoch: 3, step: 61
	action: tensor([[ 0.0179,  0.0703,  0.0274,  0.0424,  0.0344, -0.0026,  0.0183]],
       dtype=torch.float64)
	q_value: tensor([[0.5643]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3258299236143064, distance: 0.9395965842694419 entropy -13.423801272064404
epoch: 3, step: 62
	action: tensor([[ 0.0180,  0.0704,  0.0307, -0.0674,  0.0343, -0.0145,  0.0183]],
       dtype=torch.float64)
	q_value: tensor([[0.5722]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27235775136688267, distance: 0.9761479508001093 entropy -13.423577788543275
epoch: 3, step: 63
	action: tensor([[0.0179, 0.0704, 0.0201, 0.0419, 0.0345, 0.0052, 0.0184]],
       dtype=torch.float64)
	q_value: tensor([[0.5853]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3271377580761313, distance: 0.9386847718236633 entropy -13.423461552182047
epoch: 3, step: 64
	action: tensor([[0.0180, 0.0704, 0.0242, 0.0245, 0.0344, 0.0316, 0.0183]],
       dtype=torch.float64)
	q_value: tensor([[0.5707]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3257046048158265, distance: 0.9396839091505442 entropy -13.42377506530674
epoch: 3, step: 65
	action: tensor([[0.0179, 0.0703, 0.0412, 0.0402, 0.0344, 0.0150, 0.0183]],
       dtype=torch.float64)
	q_value: tensor([[0.5679]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3296469749854074, distance: 0.9369328800200618 entropy -13.423616019065902
epoch: 3, step: 66
	action: tensor([[0.0179, 0.0703, 0.0506, 0.0409, 0.0343, 0.0334, 0.0183]],
       dtype=torch.float64)
	q_value: tensor([[0.5698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3348823050026378, distance: 0.9332670746108128 entropy -13.423310872549104
epoch: 3, step: 67
	action: tensor([[ 0.0179,  0.0703,  0.0224, -0.0093,  0.0343,  0.0159,  0.0183]],
       dtype=torch.float64)
	q_value: tensor([[0.5669]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30636164824507794, distance: 0.9530665824380188 entropy -13.423156948448455
epoch: 3, step: 68
	action: tensor([[ 0.0179,  0.0704,  0.0256, -0.0169,  0.0344,  0.0309,  0.0184]],
       dtype=torch.float64)
	q_value: tensor([[0.5740]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3066151561501167, distance: 0.9528924052232405 entropy -13.42358000848566
epoch: 3, step: 69
	action: tensor([[ 0.0179,  0.0703,  0.0260, -0.0211,  0.0344,  0.0129,  0.0183]],
       dtype=torch.float64)
	q_value: tensor([[0.5718]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3003373734423843, distance: 0.9571963443417665 entropy -13.42337882965225
epoch: 3, step: 70
	action: tensor([[ 0.0179,  0.0703,  0.0239, -0.1001,  0.0344,  0.0128,  0.0184]],
       dtype=torch.float64)
	q_value: tensor([[0.5753]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2637184867789697, distance: 0.981925732527063 entropy -13.42339637334933
epoch: 3, step: 71
	action: tensor([[ 0.0178,  0.0704,  0.0086, -0.0274,  0.0345,  0.0119,  0.0184]],
       dtype=torch.float64)
	q_value: tensor([[0.5832]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29704260272193783, distance: 0.9594474567175864 entropy -13.423568689478083
epoch: 3, step: 72
	action: tensor([[0.0179, 0.0704, 0.0096, 0.0322, 0.0344, 0.0381, 0.0184]],
       dtype=torch.float64)
	q_value: tensor([[0.5755]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3299826903600024, distance: 0.9366982408434121 entropy -13.42383533141557
epoch: 3, step: 73
	action: tensor([[ 0.0179,  0.0703,  0.0191, -0.0121,  0.0344, -0.0121,  0.0183]],
       dtype=torch.float64)
	q_value: tensor([[0.5650]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2985544430874475, distance: 0.9584151664619308 entropy -13.423906838918075
epoch: 3, step: 74
	action: tensor([[0.0179, 0.0704, 0.0301, 0.0144, 0.0344, 0.0084, 0.0184]],
       dtype=torch.float64)
	q_value: tensor([[0.5784]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3157931294253411, distance: 0.9465649273835048 entropy -13.423564842690412
epoch: 3, step: 75
	action: tensor([[ 0.0179,  0.0703,  0.0425, -0.0014,  0.0344,  0.0055,  0.0183]],
       dtype=torch.float64)
	q_value: tensor([[0.5727]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3080127185935634, distance: 0.9519316124018865 entropy -13.423317637433716
epoch: 3, step: 76
	action: tensor([[0.0179, 0.0703, 0.0267, 0.0137, 0.0344, 0.0215, 0.0184]],
       dtype=torch.float64)
	q_value: tensor([[0.5752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.318384934056134, distance: 0.9447704123103193 entropy -13.423012777217796
epoch: 3, step: 77
	action: tensor([[0.0179, 0.0703, 0.0364, 0.0463, 0.0344, 0.0162, 0.0183]],
       dtype=torch.float64)
	q_value: tensor([[0.5703]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3324831302672193, distance: 0.9349487734100879 entropy -13.423397759800599
epoch: 3, step: 78
	action: tensor([[0.0179, 0.0703, 0.0276, 0.0543, 0.0343, 0.0253, 0.0183]],
       dtype=torch.float64)
	q_value: tensor([[0.5683]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3378005082366725, distance: 0.9312174697926972 entropy -13.423347670295119
epoch: 3, step: 79
	action: tensor([[ 0.0179,  0.0703,  0.0357, -0.0022,  0.0343,  0.0133,  0.0183]],
       dtype=torch.float64)
	q_value: tensor([[0.5657]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30934895046818345, distance: 0.9510120752576106 entropy -13.42362699206923
epoch: 3, step: 80
	action: tensor([[0.0179, 0.0703, 0.0348, 0.0371, 0.0344, 0.0032, 0.0183]],
       dtype=torch.float64)
	q_value: tensor([[0.5737]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3250923718537001, distance: 0.9401104098637288 entropy -13.423164511668768
epoch: 3, step: 81
	action: tensor([[ 0.0179,  0.0703,  0.0306, -0.0171,  0.0343,  0.0041,  0.0183]],
       dtype=torch.float64)
	q_value: tensor([[0.5714]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3001936282793647, distance: 0.9572946669282543 entropy -13.423299957270602
epoch: 3, step: 82
	action: tensor([[ 0.0179,  0.0703,  0.0425, -0.0387,  0.0344,  0.0401,  0.0184]],
       dtype=torch.float64)
	q_value: tensor([[0.5765]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29891715672548436, distance: 0.9581673388155822 entropy -13.42328852807457
epoch: 3, step: 83
	action: tensor([[0.0178, 0.0703, 0.0180, 0.0016, 0.0344, 0.0103, 0.0183]],
       dtype=torch.float64)
	q_value: tensor([[0.5730]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3098882020992373, distance: 0.9506407336402192 entropy -13.422927120038812
epoch: 3, step: 84
	action: tensor([[0.0179, 0.0704, 0.0501, 0.0047, 0.0344, 0.0043, 0.0184]],
       dtype=torch.float64)
	q_value: tensor([[0.5732]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3108358797253383, distance: 0.9499877883829675 entropy -13.42358837067304
epoch: 3, step: 85
	action: tensor([[ 0.0179,  0.0703, -0.0083, -0.0156,  0.0344,  0.0054,  0.0184]],
       dtype=torch.float64)
	q_value: tensor([[0.5750]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30063885337834595, distance: 0.9569900973774077 entropy -13.422832664673399
epoch: 3, step: 86
	action: tensor([[0.0180, 0.0704, 0.0063, 0.0587, 0.0344, 0.0034, 0.0184]],
       dtype=torch.float64)
	q_value: tensor([[0.5748]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3336362286707081, distance: 0.9341408882661079 entropy -13.424236486514099
epoch: 3, step: 87
	action: tensor([[0.0180, 0.0704, 0.0077, 0.0164, 0.0343, 0.0062, 0.0183]],
       dtype=torch.float64)
	q_value: tensor([[0.5687]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31556983244432235, distance: 0.9467193747131852 entropy -13.424077050890773
epoch: 3, step: 88
	action: tensor([[0.0180, 0.0704, 0.0292, 0.0456, 0.0344, 0.0055, 0.0184]],
       dtype=torch.float64)
	q_value: tensor([[0.5720]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3293688003011801, distance: 0.93712725818021 entropy -13.423853730068421
epoch: 3, step: 89
	action: tensor([[0.0180, 0.0703, 0.0291, 0.0345, 0.0343, 0.0002, 0.0183]],
       dtype=torch.float64)
	q_value: tensor([[0.5700]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32300849655613784, distance: 0.9415606549625398 entropy -13.423495168320974
epoch: 3, step: 90
	action: tensor([[ 0.0180,  0.0703,  0.0169, -0.0088,  0.0343,  0.0069,  0.0183]],
       dtype=torch.float64)
	q_value: tensor([[0.5720]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3044841542108485, distance: 0.9543555593326124 entropy -13.423407189382292
epoch: 3, step: 91
	action: tensor([[ 0.0179,  0.0704,  0.0118, -0.0440,  0.0344,  0.0019,  0.0184]],
       dtype=torch.float64)
	q_value: tensor([[0.5747]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28723937202788163, distance: 0.9661143751310225 entropy -13.423627228561344
epoch: 3, step: 92
	action: tensor([[ 0.0179,  0.0704,  0.0511, -0.0208,  0.0344,  0.0256,  0.0184]],
       dtype=torch.float64)
	q_value: tensor([[0.5790]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3039336659215707, distance: 0.9547331622657164 entropy -13.423780593032278
epoch: 3, step: 93
	action: tensor([[ 0.0178,  0.0703,  0.0135,  0.0114,  0.0344, -0.0105,  0.0183]],
       dtype=torch.float64)
	q_value: tensor([[0.5740]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3093372017163307, distance: 0.9510201641162114 entropy -13.422772795407864
epoch: 3, step: 94
	action: tensor([[ 0.0180,  0.0704,  0.0182,  0.0296,  0.0344, -0.0183,  0.0184]],
       dtype=torch.float64)
	q_value: tensor([[0.5755]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31614016857112004, distance: 0.9463248415535992 entropy -13.423667886940397
epoch: 3, step: 95
	action: tensor([[ 0.0180,  0.0704,  0.0178, -0.0366,  0.0344,  0.0019,  0.0184]],
       dtype=torch.float64)
	q_value: tensor([[0.5753]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2907116336515213, distance: 0.9637582563574838 entropy -13.42356800326543
epoch: 3, step: 96
	action: tensor([[ 0.0179,  0.0704,  0.0194, -0.0316,  0.0344,  0.0012,  0.0184]],
       dtype=torch.float64)
	q_value: tensor([[0.5784]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29274790087784985, distance: 0.9623738547772805 entropy -13.423622771349715
epoch: 3, step: 97
	action: tensor([[ 0.0179,  0.0704,  0.0256, -0.0533,  0.0344,  0.0028,  0.0184]],
       dtype=torch.float64)
	q_value: tensor([[0.5781]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2831181692261113, distance: 0.96890340017546 entropy -13.423586466826125
epoch: 3, step: 98
	action: tensor([[0.0179, 0.0704, 0.0246, 0.0507, 0.0344, 0.0119, 0.0184]],
       dtype=torch.float64)
	q_value: tensor([[0.5802]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3328809864359493, distance: 0.9346701058610858 entropy -13.423420744281028
epoch: 3, step: 99
	action: tensor([[ 0.0180,  0.0703,  0.0261, -0.0261,  0.0343,  0.0004,  0.0183]],
       dtype=torch.float64)
	q_value: tensor([[0.5683]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2950956247772881, distance: 0.9607752266286151 entropy -13.423658334885957
epoch: 3, step: 100
	action: tensor([[ 0.0179,  0.0704,  0.0158, -0.0039,  0.0344,  0.0309,  0.0184]],
       dtype=torch.float64)
	q_value: tensor([[0.5779]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31233445210158794, distance: 0.9489543624044865 entropy -13.423416915244559
epoch: 3, step: 101
	action: tensor([[0.0179, 0.0703, 0.0006, 0.0026, 0.0344, 0.0130, 0.0183]],
       dtype=torch.float64)
	q_value: tensor([[0.5701]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31067052883455104, distance: 0.9501017466560691 entropy -13.423627559141902
epoch: 3, step: 102
	action: tensor([[ 0.0180,  0.0704,  0.0369,  0.0777,  0.0344, -0.0109,  0.0184]],
       dtype=torch.float64)
	q_value: tensor([[0.5720]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.340600618687821, distance: 0.9292465576196945 entropy -13.424025258015357
epoch: 3, step: 103
	action: tensor([[ 0.0180,  0.0703,  0.0237, -0.0165,  0.0343,  0.0236,  0.0184]],
       dtype=torch.float64)
	q_value: tensor([[0.5706]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3050275768645765, distance: 0.9539826564177918 entropy -13.423462051506002
epoch: 3, step: 104
	action: tensor([[ 0.0179,  0.0704,  0.0252,  0.0309,  0.0344, -0.0023,  0.0183]],
       dtype=torch.float64)
	q_value: tensor([[0.5734]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32060797834152943, distance: 0.9432284996965614 entropy -13.423535965164048
epoch: 3, step: 105
	action: tensor([[0.0180, 0.0704, 0.0295, 0.0448, 0.0344, 0.0070, 0.0183]],
       dtype=torch.float64)
	q_value: tensor([[0.5731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3293721448924327, distance: 0.9371249213432573 entropy -13.423596230542852
epoch: 3, step: 106
	action: tensor([[ 0.0180,  0.0703,  0.0407, -0.0341,  0.0343,  0.0090,  0.0183]],
       dtype=torch.float64)
	q_value: tensor([[0.5703]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29352293053454237, distance: 0.9618464100882738 entropy -13.42359702762222
epoch: 3, step: 107
	action: tensor([[0.0179, 0.0704, 0.0056, 0.0598, 0.0344, 0.0123, 0.0184]],
       dtype=torch.float64)
	q_value: tensor([[0.5782]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33588132335520404, distance: 0.9325659195314889 entropy -13.423137215475473
epoch: 3, step: 108
	action: tensor([[0.0180, 0.0704, 0.0215, 0.0558, 0.0344, 0.0028, 0.0183]],
       dtype=torch.float64)
	q_value: tensor([[0.5674]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33302959123034304, distance: 0.9345659983540657 entropy -13.424211869868769
epoch: 3, step: 109
	action: tensor([[ 0.0180,  0.0704,  0.0269, -0.0043,  0.0343,  0.0041,  0.0183]],
       dtype=torch.float64)
	q_value: tensor([[0.5698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30611978151320585, distance: 0.9532327317056658 entropy -13.423833455264555
epoch: 3, step: 110
	action: tensor([[ 0.0179,  0.0703,  0.0233, -0.0665,  0.0344, -0.0194,  0.0184]],
       dtype=torch.float64)
	q_value: tensor([[0.5751]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27171628850676577, distance: 0.9765781242252022 entropy -13.423374239586323
epoch: 3, step: 111
	action: tensor([[ 0.0179,  0.0704,  0.0176, -0.0154,  0.0345,  0.0203,  0.0185]],
       dtype=torch.float64)
	q_value: tensor([[0.5853]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3046441814413, distance: 0.954245761932781 entropy -13.423550749876085
epoch: 3, step: 112
	action: tensor([[ 0.0179,  0.0704,  0.0317, -0.0043,  0.0344, -0.0004,  0.0184]],
       dtype=torch.float64)
	q_value: tensor([[0.5731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3050255232970882, distance: 0.953984065873948 entropy -13.423592839449002
epoch: 3, step: 113
	action: tensor([[ 0.0179,  0.0703,  0.0220, -0.0333,  0.0344,  0.0091,  0.0184]],
       dtype=torch.float64)
	q_value: tensor([[0.5761]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2938277689274258, distance: 0.9616388737346797 entropy -13.42326753566603
epoch: 3, step: 114
	action: tensor([[ 0.0179,  0.0704,  0.0289, -0.0098,  0.0344,  0.0205,  0.0184]],
       dtype=torch.float64)
	q_value: tensor([[0.5770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3074374746296573, distance: 0.9523271970994508 entropy -13.423500227535367
epoch: 3, step: 115
	action: tensor([[ 0.0179,  0.0703,  0.0427, -0.0049,  0.0344,  0.0098,  0.0183]],
       dtype=torch.float64)
	q_value: tensor([[0.5729]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30739431500496295, distance: 0.9523568705531605 entropy -13.423303849824379
epoch: 3, step: 116
	action: tensor([[ 0.0179,  0.0703,  0.0322, -0.0288,  0.0344,  0.0225,  0.0183]],
       dtype=torch.float64)
	q_value: tensor([[0.5748]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2991258245789382, distance: 0.9580247354105504 entropy -13.423009363917846
epoch: 3, step: 117
	action: tensor([[ 0.0179,  0.0703,  0.0365, -0.0618,  0.0344,  0.0079,  0.0184]],
       dtype=torch.float64)
	q_value: tensor([[0.5746]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2801350245265447, distance: 0.9709172455827325 entropy -13.423211177956219
epoch: 3, step: 118
	action: tensor([[ 0.0178,  0.0704,  0.0238, -0.0135,  0.0345,  0.0157,  0.0184]],
       dtype=torch.float64)
	q_value: tensor([[0.5805]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3044452540830639, distance: 0.9543822474622619 entropy -13.42312473873534
epoch: 3, step: 119
	action: tensor([[0.0179, 0.0704, 0.0177, 0.0706, 0.0344, 0.0060, 0.0184]],
       dtype=torch.float64)
	q_value: tensor([[0.5745]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34005936264852055, distance: 0.9296278569933629 entropy -13.42354938514707
epoch: 3, step: 120
	action: tensor([[ 0.0180,  0.0704,  0.0300, -0.0517,  0.0343, -0.0032,  0.0183]],
       dtype=torch.float64)
	q_value: tensor([[0.5678]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2824314539381414, distance: 0.9693673549617443 entropy -13.424013207618398
epoch: 3, step: 121
	action: tensor([[ 0.0179,  0.0704,  0.0348,  0.0487,  0.0344, -0.0080,  0.0184]],
       dtype=torch.float64)
	q_value: tensor([[0.5817]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32781035630027544, distance: 0.9382154962962123 entropy -13.42344421933643
epoch: 3, step: 122
	action: tensor([[0.0180, 0.0703, 0.0320, 0.0117, 0.0343, 0.0160, 0.0183]],
       dtype=torch.float64)
	q_value: tensor([[0.5727]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31643549340223864, distance: 0.9461204843225479 entropy -13.423418616971201
epoch: 3, step: 123
	action: tensor([[0.0179, 0.0703, 0.0185, 0.0858, 0.0344, 0.0279, 0.0183]],
       dtype=torch.float64)
	q_value: tensor([[0.5717]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3518654378473297, distance: 0.9212749949798881 entropy -13.42327218671176
epoch: 3, step: 124
	action: tensor([[ 0.0180,  0.0703,  0.0249, -0.0152,  0.0343,  0.0075,  0.0183]],
       dtype=torch.float64)
	q_value: tensor([[0.5621]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3017743987200652, distance: 0.9562128543685439 entropy -13.424049656738598
epoch: 3, step: 125
	action: tensor([[ 0.0179,  0.0704,  0.0253, -0.0349,  0.0344, -0.0113,  0.0184]],
       dtype=torch.float64)
	q_value: tensor([[0.5761]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2883147890319149, distance: 0.9653852607656562 entropy -13.423535269717567
epoch: 3, step: 126
	action: tensor([[ 0.0179,  0.0704,  0.0322, -0.0090,  0.0344,  0.0252,  0.0184]],
       dtype=torch.float64)
	q_value: tensor([[0.5813]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30906205901031725, distance: 0.9512095765233599 entropy -13.42356543659966
epoch: 3, step: 127
	action: tensor([[ 0.0179,  0.0703,  0.0526, -0.0220,  0.0344,  0.0038,  0.0183]],
       dtype=torch.float64)
	q_value: tensor([[0.5727]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2979968867552807, distance: 0.9587959973713199 entropy -13.423327669056874
LOSS epoch 3 actor 0.22676840277784388 critic 21.360202669771038 entropy 0.01
epoch: 4, step: 0
	action: tensor([[-0.0018,  0.0808,  0.0242,  0.0181,  0.0505,  0.0175,  0.0164]],
       dtype=torch.float64)
	q_value: tensor([[1.0553]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3058004003067972, distance: 0.9534520848354469 entropy -13.350667669128086
epoch: 4, step: 1
	action: tensor([[-0.0018,  0.0809,  0.0472, -0.0578,  0.0506,  0.0047,  0.0164]],
       dtype=torch.float64)
	q_value: tensor([[1.0490]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2678580052397598, distance: 0.979161552743554 entropy -13.351297982032449
epoch: 4, step: 2
	action: tensor([[-0.0019,  0.0809,  0.0188,  0.0402,  0.0506, -0.0160,  0.0164]],
       dtype=torch.float64)
	q_value: tensor([[1.0669]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3075507387870138, distance: 0.9522493204139137 entropy -13.350999668002986
epoch: 4, step: 3
	action: tensor([[-0.0018,  0.0810,  0.0010,  0.0471,  0.0505,  0.0278,  0.0164]],
       dtype=torch.float64)
	q_value: tensor([[1.0543]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3201467847218674, distance: 0.9435485926268519 entropy -13.351220341234113
epoch: 4, step: 4
	action: tensor([[-0.0018,  0.0810,  0.0094,  0.0665,  0.0506, -0.0038,  0.0164]],
       dtype=torch.float64)
	q_value: tensor([[1.0400]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32173864376633676, distance: 0.9424432977045589 entropy -13.351616697252817
epoch: 4, step: 5
	action: tensor([[-0.0018,  0.0810,  0.0192,  0.0207,  0.0505, -0.0031,  0.0164]],
       dtype=torch.float64)
	q_value: tensor([[1.0464]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30204034933468227, distance: 0.9560307286942762 entropy -13.351354378938868
epoch: 4, step: 6
	action: tensor([[-0.0018,  0.0809,  0.0463,  0.0795,  0.0505,  0.0295,  0.0164]],
       dtype=torch.float64)
	q_value: tensor([[1.0539]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33791146597226396, distance: 0.9311394494138383 entropy -13.351286184315105
epoch: 4, step: 7
	action: tensor([[-0.0018,  0.0809,  0.0368,  0.0412,  0.0505,  0.0018,  0.0163]],
       dtype=torch.float64)
	q_value: tensor([[1.0370]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3131007015107723, distance: 0.9484255163744574 entropy -13.350953409935618
epoch: 4, step: 8
	action: tensor([[-0.0018,  0.0809,  0.0502, -0.0056,  0.0505,  0.0044,  0.0163]],
       dtype=torch.float64)
	q_value: tensor([[1.0504]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.292434113552543, distance: 0.9625873198420068 entropy -13.351080103137248
epoch: 4, step: 9
	action: tensor([[-0.0018,  0.0809,  0.0203,  0.0453,  0.0505,  0.0170,  0.0164]],
       dtype=torch.float64)
	q_value: tensor([[1.0586]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3176940900952254, distance: 0.9452490722377517 entropy -13.350898682315044
epoch: 4, step: 10
	action: tensor([[-0.0018,  0.0809,  0.0461,  0.0290,  0.0505,  0.0258,  0.0164]],
       dtype=torch.float64)
	q_value: tensor([[1.0443]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3137242637517841, distance: 0.9479949317018735 entropy -13.351345027447422
epoch: 4, step: 11
	action: tensor([[-0.0018,  0.0809,  0.0326,  0.0430,  0.0505, -0.0087,  0.0163]],
       dtype=torch.float64)
	q_value: tensor([[1.0465]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31118484432470095, distance: 0.9497472403952684 entropy -13.351044988488983
epoch: 4, step: 12
	action: tensor([[-0.0018,  0.0809,  0.0431,  0.0345,  0.0505,  0.0046,  0.0163]],
       dtype=torch.float64)
	q_value: tensor([[1.0526]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3110210777271787, distance: 0.949860135436206 entropy -13.351080408992454
epoch: 4, step: 13
	action: tensor([[-0.0018,  0.0809,  0.0189,  0.0272,  0.0505,  0.0115,  0.0163]],
       dtype=torch.float64)
	q_value: tensor([[1.0512]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3083024281702412, distance: 0.9517323221959615 entropy -13.351006630362262
epoch: 4, step: 14
	action: tensor([[-0.0018,  0.0809,  0.0383,  0.0800,  0.0505,  0.0239,  0.0164]],
       dtype=torch.float64)
	q_value: tensor([[1.0478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3361777999716745, distance: 0.9323577377244215 entropy -13.351311577844239
epoch: 4, step: 15
	action: tensor([[-0.0018,  0.0809,  0.0302,  0.0482,  0.0505, -0.0051,  0.0163]],
       dtype=torch.float64)
	q_value: tensor([[1.0372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3143526115622931, distance: 0.9475608445570691 entropy -13.351015876191369
epoch: 4, step: 16
	action: tensor([[-0.0018,  0.0809,  0.0138,  0.0004,  0.0505,  0.0252,  0.0163]],
       dtype=torch.float64)
	q_value: tensor([[1.0497]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29941664753123487, distance: 0.9578259518769981 entropy -13.351081449264118
epoch: 4, step: 17
	action: tensor([[-0.0018,  0.0809,  0.0375, -0.0084,  0.0506,  0.0179,  0.0164]],
       dtype=torch.float64)
	q_value: tensor([[1.0482]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2941298545225177, distance: 0.9614331673131802 entropy -13.351428352171395
epoch: 4, step: 18
	action: tensor([[-0.0019,  0.0809,  0.0619, -0.0294,  0.0505, -0.0138,  0.0164]],
       dtype=torch.float64)
	q_value: tensor([[1.0534]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2766551441767585, distance: 0.9732611590989068 entropy -13.351085931290134
epoch: 4, step: 19
	action: tensor([[-0.0018,  0.0808,  0.0244,  0.0287,  0.0505,  0.0323,  0.0164]],
       dtype=torch.float64)
	q_value: tensor([[1.0674]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31405607477285724, distance: 0.9477657284965757 entropy -13.350633084810053
epoch: 4, step: 20
	action: tensor([[-0.0018,  0.0809,  0.0292,  0.0176,  0.0506, -0.0019,  0.0164]],
       dtype=torch.float64)
	q_value: tensor([[1.0431]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30113259959374206, distance: 0.9566522221121898 entropy -13.35132448196548
epoch: 4, step: 21
	action: tensor([[-0.0018,  0.0809,  0.0562,  0.0173,  0.0505,  0.0191,  0.0164]],
       dtype=torch.float64)
	q_value: tensor([[1.0548]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3070105140914332, distance: 0.9526207038006588 entropy -13.351163357119022
epoch: 4, step: 22
	action: tensor([[-0.0018,  0.0808,  0.0388, -0.0023,  0.0505,  0.0035,  0.0163]],
       dtype=torch.float64)
	q_value: tensor([[1.0511]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2934509196372894, distance: 0.9618954291301478 entropy -13.35087637036527
epoch: 4, step: 23
	action: tensor([[-0.0018,  0.0809,  0.0380,  0.0353,  0.0505, -0.0080,  0.0164]],
       dtype=torch.float64)
	q_value: tensor([[1.0574]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30804943244046856, distance: 0.9519063593835184 entropy -13.351057570460467
epoch: 4, step: 24
	action: tensor([[-0.0018,  0.0809,  0.0229,  0.0064,  0.0505, -0.0135,  0.0163]],
       dtype=torch.float64)
	q_value: tensor([[1.0533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29315000285640413, distance: 0.9621002412739188 entropy -13.350963267887783
epoch: 4, step: 25
	action: tensor([[-0.0018,  0.0809,  0.0315, -0.0759,  0.0505,  0.0081,  0.0164]],
       dtype=torch.float64)
	q_value: tensor([[1.0595]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2604731422940293, distance: 0.9840873944671142 entropy -13.351193480487831
epoch: 4, step: 26
	action: tensor([[-0.0019,  0.0809,  0.0311, -0.0020,  0.0506,  0.0180,  0.0165]],
       dtype=torch.float64)
	q_value: tensor([[1.0681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29691404556397794, distance: 0.9595351847932471 entropy -13.351272645305723
epoch: 4, step: 27
	action: tensor([[-0.0019,  0.0809,  0.0239,  0.0327,  0.0506,  0.0304,  0.0164]],
       dtype=torch.float64)
	q_value: tensor([[1.0528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31544900943325427, distance: 0.9468029335941466 entropy -13.351202027888997
epoch: 4, step: 28
	action: tensor([[-0.0018,  0.0809,  0.0232,  0.0122,  0.0505,  0.0118,  0.0164]],
       dtype=torch.float64)
	q_value: tensor([[1.0430]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30173170434360874, distance: 0.9562420886786589 entropy -13.351330887699666
epoch: 4, step: 29
	action: tensor([[-0.0018,  0.0809,  0.0272,  0.0059,  0.0505, -0.0103,  0.0164]],
       dtype=torch.float64)
	q_value: tensor([[1.0506]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29376484258192237, distance: 0.9616817181495153 entropy -13.351257885626508
epoch: 4, step: 30
	action: tensor([[-0.0018,  0.0809,  0.0303,  0.0989,  0.0505,  0.0103,  0.0164]],
       dtype=torch.float64)
	q_value: tensor([[1.0590]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3408240959528518, distance: 0.9290890786448134 entropy -13.35114896576262
epoch: 4, step: 31
	action: tensor([[-0.0018,  0.0809,  0.0467,  0.0406,  0.0505, -0.0144,  0.0163]],
       dtype=torch.float64)
	q_value: tensor([[1.0385]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3094201798364985, distance: 0.9509630333166152 entropy -13.351037595803309
epoch: 4, step: 32
	action: tensor([[-0.0018,  0.0809,  0.0327,  0.0477,  0.0505,  0.0194,  0.0163]],
       dtype=torch.float64)
	q_value: tensor([[1.0557]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32011418527005686, distance: 0.9435712142733064 entropy -13.350840055082426
epoch: 4, step: 33
	action: tensor([[-0.0018,  0.0809,  0.0376, -0.0072,  0.0505, -0.0058,  0.0163]],
       dtype=torch.float64)
	q_value: tensor([[1.0441]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28896644827005036, distance: 0.9649431788660987 entropy -13.351186424120609
epoch: 4, step: 34
	action: tensor([[-0.0018,  0.0809,  0.0433,  0.0911,  0.0506,  0.0023,  0.0164]],
       dtype=torch.float64)
	q_value: tensor([[1.0607]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33640456873114366, distance: 0.9321984724367477 entropy -13.35103375962834
epoch: 4, step: 35
	action: tensor([[-0.0018,  0.0809,  0.0436,  0.0398,  0.0505,  0.0040,  0.0163]],
       dtype=torch.float64)
	q_value: tensor([[1.0427]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31336296500746186, distance: 0.9482444409680458 entropy -13.350881085527176
epoch: 4, step: 36
	action: tensor([[-0.0018,  0.0809,  0.0635,  0.0705,  0.0505, -0.0213,  0.0163]],
       dtype=torch.float64)
	q_value: tensor([[1.0505]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32251060950374544, distance: 0.941906822304732 entropy -13.350993965978187
epoch: 4, step: 37
	action: tensor([[-0.0017,  0.0809,  0.0085,  0.0086,  0.0504, -0.0028,  0.0163]],
       dtype=torch.float64)
	q_value: tensor([[1.0537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2963252762373134, distance: 0.9599368616140875 entropy -13.350457647927806
epoch: 4, step: 38
	action: tensor([[-0.0018,  0.0810,  0.0558,  0.0647,  0.0506, -0.0049,  0.0164]],
       dtype=torch.float64)
	q_value: tensor([[1.0550]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3233535774305166, distance: 0.9413206549127358 entropy -13.35142797431588
epoch: 4, step: 39
	action: tensor([[-0.0018,  0.0809,  0.0548, -0.0304,  0.0505,  0.0156,  0.0163]],
       dtype=torch.float64)
	q_value: tensor([[1.0496]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2835269057975869, distance: 0.9686271463338576 entropy -13.350723566150757
epoch: 4, step: 40
	action: tensor([[-0.0019,  0.0809,  0.0760,  0.0251,  0.0506,  0.0425,  0.0164]],
       dtype=torch.float64)
	q_value: tensor([[1.0599]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31729787816029387, distance: 0.9455234833221963 entropy -13.350901961846358
epoch: 4, step: 41
	action: tensor([[-0.0019,  0.0808,  0.0373,  0.0189,  0.0505,  0.0282,  0.0163]],
       dtype=torch.float64)
	q_value: tensor([[1.0447]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.309072587153148, distance: 0.9512023294843435 entropy -13.350666104871888
epoch: 4, step: 42
	action: tensor([[-0.0018,  0.0809,  0.0443,  0.1267,  0.0505, -0.0176,  0.0164]],
       dtype=torch.float64)
	q_value: tensor([[1.0468]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34787609335934055, distance: 0.9241059234825664 entropy -13.35113897254344
epoch: 4, step: 43
	action: tensor([[-0.0017,  0.0809,  0.0287,  0.1044,  0.0504,  0.0146,  0.0163]],
       dtype=torch.float64)
	q_value: tensor([[1.0427]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34422040586345404, distance: 0.9266924880958087 entropy -13.35062114820938
epoch: 4, step: 44
	action: tensor([[-0.0018,  0.0809,  0.0171, -0.0111,  0.0505, -0.0016,  0.0163]],
       dtype=torch.float64)
	q_value: tensor([[1.0364]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2880135283413816, distance: 0.9655895658815369 entropy -13.351053330279218
epoch: 4, step: 45
	action: tensor([[-0.0018,  0.0809,  0.0504,  0.0737,  0.0506,  0.0196,  0.0164]],
       dtype=torch.float64)
	q_value: tensor([[1.0587]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33312444710222133, distance: 0.9344995394650685 entropy -13.35134353548539
epoch: 4, step: 46
	action: tensor([[-0.0018,  0.0808,  0.0151,  0.0187,  0.0505,  0.0248,  0.0163]],
       dtype=torch.float64)
	q_value: tensor([[1.0400]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3074622600935937, distance: 0.9523101559791746 entropy -13.350850712832571
epoch: 4, step: 47
	action: tensor([[-0.0018,  0.0809,  0.0474,  0.0078,  0.0506,  0.0077,  0.0164]],
       dtype=torch.float64)
	q_value: tensor([[1.0452]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29945995161134187, distance: 0.9577963491091243 entropy -13.351399699395426
epoch: 4, step: 48
	action: tensor([[-0.0018,  0.0809,  0.0138, -0.0372,  0.0505, -0.0064,  0.0164]],
       dtype=torch.float64)
	q_value: tensor([[1.0552]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27489473972511114, distance: 0.9744447523013917 entropy -13.35095123268628
epoch: 4, step: 49
	action: tensor([[-0.0018,  0.0809,  0.0231, -0.0181,  0.0506,  0.0209,  0.0164]],
       dtype=torch.float64)
	q_value: tensor([[1.0634]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29020904262981095, distance: 0.964099648162767 entropy -13.351348416329028
epoch: 4, step: 50
	action: tensor([[-0.0019,  0.0809,  0.0245, -0.0690,  0.0506, -0.0106,  0.0164]],
       dtype=torch.float64)
	q_value: tensor([[1.0531]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2592282182579053, distance: 0.9849153555348258 entropy -13.351313878583047
epoch: 4, step: 51
	action: tensor([[-0.0019,  0.0809,  0.0386,  0.0617,  0.0506, -0.0150,  0.0165]],
       dtype=torch.float64)
	q_value: tensor([[1.0708]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3185211997289873, distance: 0.9446759702877742 entropy -13.35124399245632
epoch: 4, step: 52
	action: tensor([[-0.0017,  0.0809,  0.0313, -0.0250,  0.0505,  0.0369,  0.0163]],
       dtype=torch.float64)
	q_value: tensor([[1.0509]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29112513070928436, distance: 0.9634772921405901 entropy -13.350891095937483
epoch: 4, step: 53
	action: tensor([[-0.0019,  0.0809,  0.0435, -0.0032,  0.0506, -0.0071,  0.0164]],
       dtype=torch.float64)
	q_value: tensor([[1.0504]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29049823872243963, distance: 0.9639032225434263 entropy -13.351270245904812
epoch: 4, step: 54
	action: tensor([[-0.0018,  0.0809,  0.0409,  0.1549,  0.0505,  0.0146,  0.0164]],
       dtype=torch.float64)
	q_value: tensor([[1.0599]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3677377263136693, distance: 0.9099244349493157 entropy -13.350895280665242
epoch: 4, step: 55
	action: tensor([[-0.0017,  0.0809,  0.0409, -0.0042,  0.0504, -0.0171,  0.0163]],
       dtype=torch.float64)
	q_value: tensor([[1.0282]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28778339175689294, distance: 0.965745607842672 entropy -13.350705859189187
epoch: 4, step: 56
	action: tensor([[-0.0018,  0.0809,  0.0640, -0.0172,  0.0505,  0.0138,  0.0164]],
       dtype=torch.float64)
	q_value: tensor([[1.0626]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28945820235077147, distance: 0.9646094415781423 entropy -13.350882993211645
epoch: 4, step: 57
	action: tensor([[-0.0019,  0.0808,  0.0113,  0.0109,  0.0505,  0.0299,  0.0164]],
       dtype=torch.float64)
	q_value: tensor([[1.0589]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3049568852494263, distance: 0.9540311740695074 entropy -13.350746064394032
epoch: 4, step: 58
	action: tensor([[-0.0018,  0.0809,  0.0353, -0.0124,  0.0506,  0.0394,  0.0164]],
       dtype=torch.float64)
	q_value: tensor([[1.0458]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2975222679873597, distance: 0.9591200598122916 entropy -13.35150440874283
epoch: 4, step: 59
	action: tensor([[-0.0019,  0.0809,  0.0141,  0.0141,  0.0506, -0.0049,  0.0164]],
       dtype=torch.float64)
	q_value: tensor([[1.0489]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29830900374299085, distance: 0.9585828289513211 entropy -13.35123303901185
epoch: 4, step: 60
	action: tensor([[-0.0018,  0.0810,  0.0512,  0.0829,  0.0506,  0.0058,  0.0164]],
       dtype=torch.float64)
	q_value: tensor([[1.0552]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33407483189102016, distance: 0.9338334100830118 entropy -13.35134485486405
epoch: 4, step: 61
	action: tensor([[-0.0018,  0.0809,  0.0364,  0.0740,  0.0505,  0.0091,  0.0163]],
       dtype=torch.float64)
	q_value: tensor([[1.0434]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32980909673671277, distance: 0.9368195767464776 entropy -13.3507938637955
epoch: 4, step: 62
	action: tensor([[-0.0018,  0.0809,  0.0221, -0.0310,  0.0505, -0.0087,  0.0163]],
       dtype=torch.float64)
	q_value: tensor([[1.0430]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27729517431483097, distance: 0.9728304832447228 entropy -13.351040955750108
epoch: 4, step: 63
	action: tensor([[-0.0018,  0.0809,  0.0264,  0.0210,  0.0506,  0.0350,  0.0164]],
       dtype=torch.float64)
	q_value: tensor([[1.0645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3114575627120929, distance: 0.9495592079629201 entropy -13.351251772895358
epoch: 4, step: 64
	action: tensor([[-0.0018,  0.0809,  0.0350, -0.0184,  0.0506, -0.0132,  0.0164]],
       dtype=torch.float64)
	q_value: tensor([[1.0439]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28194287025236997, distance: 0.9696973140194278 entropy -13.351301580659358
epoch: 4, step: 65
	action: tensor([[-0.0018,  0.0809,  0.0430,  0.0208,  0.0505,  0.0130,  0.0164]],
       dtype=torch.float64)
	q_value: tensor([[1.0635]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3066555947756122, distance: 0.9528646181860004 entropy -13.350990943068041
epoch: 4, step: 66
	action: tensor([[-0.0018,  0.0809,  0.0487,  0.0014,  0.0505,  0.0268,  0.0163]],
       dtype=torch.float64)
	q_value: tensor([[1.0503]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30114922596596627, distance: 0.9566408424495195 entropy -13.350985542357899
epoch: 4, step: 67
	action: tensor([[-0.0019,  0.0808,  0.0213,  0.0322,  0.0505,  0.0016,  0.0164]],
       dtype=torch.float64)
	q_value: tensor([[1.0501]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30820232864820085, distance: 0.9518011850256132 entropy -13.350960955734225
epoch: 4, step: 68
	action: tensor([[-0.0018,  0.0809,  0.0182, -0.0170,  0.0505, -0.0127,  0.0164]],
       dtype=torch.float64)
	q_value: tensor([[1.0498]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2826534470986911, distance: 0.9692173974628406 entropy -13.351237882847034
epoch: 4, step: 69
	action: tensor([[-0.0018,  0.0809,  0.0073, -0.0434,  0.0506,  0.0048,  0.0164]],
       dtype=torch.float64)
	q_value: tensor([[1.0620]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2748397370926081, distance: 0.9744817097013609 entropy -13.35123924035806
epoch: 4, step: 70
	action: tensor([[-0.0019,  0.0810,  0.0338,  0.0191,  0.0506,  0.0053,  0.0164]],
       dtype=torch.float64)
	q_value: tensor([[1.0619]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30369728888784286, distance: 0.954895257331733 entropy -13.351550456916712
epoch: 4, step: 71
	action: tensor([[-0.0018,  0.0809,  0.0136, -0.0345,  0.0505, -0.0034,  0.0164]],
       dtype=torch.float64)
	q_value: tensor([[1.0530]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2769089072599882, distance: 0.9730904248941553 entropy -13.351137982341138
epoch: 4, step: 72
	action: tensor([[-0.0018,  0.0809,  0.0656,  0.0428,  0.0506, -0.0072,  0.0164]],
       dtype=torch.float64)
	q_value: tensor([[1.0621]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3129341504399311, distance: 0.9485404908050149 entropy -13.35137138692071
epoch: 4, step: 73
	action: tensor([[-0.0018,  0.0808,  0.0190, -0.0107,  0.0504,  0.0281,  0.0163]],
       dtype=torch.float64)
	q_value: tensor([[1.0538]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29514008729315, distance: 0.9607449252438431 entropy -13.35052646953723
epoch: 4, step: 74
	action: tensor([[-0.0019,  0.0809,  0.0687, -0.0345,  0.0506,  0.0146,  0.0164]],
       dtype=torch.float64)
	q_value: tensor([[1.0495]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2813473428856682, distance: 0.9700993444281181 entropy -13.351366047537434
epoch: 4, step: 75
	action: tensor([[-0.0019,  0.0808,  0.0087,  0.0089,  0.0505,  0.0175,  0.0164]],
       dtype=torch.float64)
	q_value: tensor([[1.0610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30101932027073997, distance: 0.9567297507857168 entropy -13.350657353353075
epoch: 4, step: 76
	action: tensor([[-0.0018,  0.0809,  0.0042,  0.0811,  0.0506, -0.0081,  0.0164]],
       dtype=torch.float64)
	q_value: tensor([[1.0484]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32678934113989677, distance: 0.9389277720432451 entropy -13.351477801491248
epoch: 4, step: 77
	action: tensor([[-0.0017,  0.0810,  0.0213, -0.0346,  0.0505, -0.0045,  0.0164]],
       dtype=torch.float64)
	q_value: tensor([[1.0442]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2767296290901544, distance: 0.9732110480496124 entropy -13.351329360487858
epoch: 4, step: 78
	action: tensor([[-0.0018,  0.0809,  0.0112, -0.0342,  0.0506,  0.0292,  0.0164]],
       dtype=torch.float64)
	q_value: tensor([[1.0629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2847534766360389, distance: 0.9677976673583643 entropy -13.351244347494802
epoch: 4, step: 79
	action: tensor([[-1.8789e-03,  8.0902e-02,  3.3370e-02,  3.0708e-05,  5.0598e-02,
          3.6039e-02,  1.6425e-02]], dtype=torch.float64)
	q_value: tensor([[1.0527]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3023106991527751, distance: 0.9558455548331286 entropy -13.351538226898663
epoch: 4, step: 80
	action: tensor([[-0.0019,  0.0808,  0.0423,  0.0331,  0.0506,  0.0039,  0.0164]],
       dtype=torch.float64)
	q_value: tensor([[1.0467]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3099949303987688, distance: 0.9505672207743482 entropy -13.351200640195145
epoch: 4, step: 81
	action: tensor([[-0.0018,  0.0809,  0.0074,  0.0203,  0.0505,  0.0338,  0.0163]],
       dtype=torch.float64)
	q_value: tensor([[1.0507]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3099723396840529, distance: 0.9505827813970794 entropy -13.350962020332473
epoch: 4, step: 82
	action: tensor([[-0.0018,  0.0809,  0.0162,  0.0066,  0.0506,  0.0042,  0.0164]],
       dtype=torch.float64)
	q_value: tensor([[1.0419]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29722091061600286, distance: 0.9593257651996486 entropy -13.351531300472416
epoch: 4, step: 83
	action: tensor([[-0.0018,  0.0809,  0.0217,  0.0271,  0.0506,  0.0392,  0.0164]],
       dtype=torch.float64)
	q_value: tensor([[1.0531]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3149723791678385, distance: 0.9471324900589874 entropy -13.351332940306353
epoch: 4, step: 84
	action: tensor([[-0.0018,  0.0809,  0.0444,  0.0377,  0.0505, -0.0089,  0.0164]],
       dtype=torch.float64)
	q_value: tensor([[1.0404]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30917642517058963, distance: 0.9511308497069156 entropy -13.35133871113094
epoch: 4, step: 85
	action: tensor([[-0.0018,  0.0809,  0.0184, -0.0345,  0.0505,  0.0020,  0.0163]],
       dtype=torch.float64)
	q_value: tensor([[1.0536]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2781730461755475, distance: 0.9722394535607362 entropy -13.350862625208695
epoch: 4, step: 86
	action: tensor([[-0.0018,  0.0809,  0.0307,  0.0031,  0.0506,  0.0512,  0.0164]],
       dtype=torch.float64)
	q_value: tensor([[1.0608]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3074039931772711, distance: 0.952350216618396 entropy -13.351320976884494
epoch: 4, step: 87
	action: tensor([[-0.0019,  0.0808,  0.0511, -0.0260,  0.0506,  0.0196,  0.0164]],
       dtype=torch.float64)
	q_value: tensor([[1.0418]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2864344285343079, distance: 0.9666597532092075 entropy -13.351277809038683
epoch: 4, step: 88
	action: tensor([[-0.0019,  0.0808,  0.0155, -0.0030,  0.0505,  0.0141,  0.0164]],
       dtype=torch.float64)
	q_value: tensor([[1.0568]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29512172209588783, distance: 0.9607574413160607 entropy -13.350926360208572
epoch: 4, step: 89
	action: tensor([[-0.0018,  0.0809,  0.0487, -0.0155,  0.0506,  0.0193,  0.0164]],
       dtype=torch.float64)
	q_value: tensor([[1.0519]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2914038834631326, distance: 0.9632878381423609 entropy -13.351374726005186
epoch: 4, step: 90
	action: tensor([[-0.0019,  0.0808, -0.0040,  0.0601,  0.0505,  0.0136,  0.0164]],
       dtype=torch.float64)
	q_value: tensor([[1.0550]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32196655726810486, distance: 0.9422849416390965 entropy -13.350944884268847
epoch: 4, step: 91
	action: tensor([[-0.0018,  0.0810,  0.0449,  0.0944,  0.0505,  0.0082,  0.0164]],
       dtype=torch.float64)
	q_value: tensor([[1.0406]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3395291347653655, distance: 0.9300012357601801 entropy -13.351561818322562
epoch: 4, step: 92
	action: tensor([[-0.0018,  0.0809,  0.0377, -0.0795,  0.0505,  0.0208,  0.0163]],
       dtype=torch.float64)
	q_value: tensor([[1.0406]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26178817462403003, distance: 0.9832120492815538 entropy -13.350863238736363
epoch: 4, step: 93
	action: tensor([[-0.0019,  0.0809, -0.0014, -0.0131,  0.0506,  0.0258,  0.0165]],
       dtype=torch.float64)
	q_value: tensor([[1.0655]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29304074573499916, distance: 0.962174593853442 entropy -13.351229461355643
epoch: 4, step: 94
	action: tensor([[-0.0019,  0.0809,  0.0264, -0.0243,  0.0506,  0.0199,  0.0164]],
       dtype=torch.float64)
	q_value: tensor([[1.0492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28715020875679975, distance: 0.9661748016057171 entropy -13.351668235080796
epoch: 4, step: 95
	action: tensor([[-0.0019,  0.0809,  0.0375,  0.0019,  0.0506, -0.0006,  0.0164]],
       dtype=torch.float64)
	q_value: tensor([[1.0547]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29438124109561514, distance: 0.9612619510396245 entropy -13.351276774459793
epoch: 4, step: 96
	action: tensor([[-0.0018,  0.0809,  0.0301, -0.0752,  0.0505,  0.0284,  0.0164]],
       dtype=torch.float64)
	q_value: tensor([[1.0568]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26570757306766657, distance: 0.9805984848141834 entropy -13.351008019956433
epoch: 4, step: 97
	action: tensor([[-0.0019,  0.0809,  0.0276,  0.0017,  0.0506,  0.0041,  0.0165]],
       dtype=torch.float64)
	q_value: tensor([[1.0612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2951221366017862, distance: 0.9607571588277947 entropy -13.351313512137732
epoch: 4, step: 98
	action: tensor([[-0.0018,  0.0809,  0.0301,  0.0737,  0.0505,  0.0055,  0.0164]],
       dtype=torch.float64)
	q_value: tensor([[1.0548]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32834862687317756, distance: 0.9378397727648077 entropy -13.351173103618379
epoch: 4, step: 99
	action: tensor([[-0.0018,  0.0809,  0.0075,  0.0025,  0.0505, -0.0094,  0.0163]],
       dtype=torch.float64)
	q_value: tensor([[1.0428]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2921070234461244, distance: 0.9628097842203562 entropy -13.351082453366713
epoch: 4, step: 100
	action: tensor([[-0.0018,  0.0809,  0.0186,  0.0263,  0.0506,  0.0191,  0.0164]],
       dtype=torch.float64)
	q_value: tensor([[1.0569]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.309717594909214, distance: 0.9507582335309888 entropy -13.351385741739302
epoch: 4, step: 101
	action: tensor([[-0.0018,  0.0809,  0.0354,  0.0784,  0.0505,  0.0120,  0.0164]],
       dtype=torch.float64)
	q_value: tensor([[1.0459]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33239683314768687, distance: 0.9350092069228418 entropy -13.351336764269846
epoch: 4, step: 102
	action: tensor([[-0.0018,  0.0809,  0.0383,  0.0546,  0.0505,  0.0081,  0.0163]],
       dtype=torch.float64)
	q_value: tensor([[1.0406]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32083575168548273, distance: 0.9430703727884617 entropy -13.351020317697435
epoch: 4, step: 103
	action: tensor([[-0.0018,  0.0809,  0.0454, -0.0438,  0.0505,  0.0166,  0.0163]],
       dtype=torch.float64)
	q_value: tensor([[1.0455]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2774133850113516, distance: 0.9727509184847898 entropy -13.35102500936762
epoch: 4, step: 104
	action: tensor([[-0.0019,  0.0808,  0.0461,  0.0335,  0.0506,  0.0230,  0.0164]],
       dtype=torch.float64)
	q_value: tensor([[1.0602]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31498122941753115, distance: 0.9471263717756887 entropy -13.351011222613105
epoch: 4, step: 105
	action: tensor([[-0.0018,  0.0808,  0.0271,  0.0192,  0.0505,  0.0187,  0.0163]],
       dtype=torch.float64)
	q_value: tensor([[1.0456]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3066956295891904, distance: 0.9528371078262421 entropy -13.350999658889892
epoch: 4, step: 106
	action: tensor([[-0.0018,  0.0809,  0.0583,  0.0234,  0.0505,  0.0225,  0.0164]],
       dtype=torch.float64)
	q_value: tensor([[1.0477]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31079151862053733, distance: 0.9500183629793879 entropy -13.351221715665677
epoch: 4, step: 107
	action: tensor([[-0.0018,  0.0808,  0.0309, -0.0464,  0.0505,  0.0213,  0.0163]],
       dtype=torch.float64)
	q_value: tensor([[1.0483]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2772294411643528, distance: 0.9728747238228046 entropy -13.350812915609177
epoch: 4, step: 108
	action: tensor([[-0.0019,  0.0809,  0.0354,  0.0246,  0.0506,  0.0198,  0.0164]],
       dtype=torch.float64)
	q_value: tensor([[1.0582]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3096898042979319, distance: 0.950777371992555 entropy -13.351247801583437
epoch: 4, step: 109
	action: tensor([[-0.0018,  0.0809,  0.0299, -0.0290,  0.0505, -0.0109,  0.0164]],
       dtype=torch.float64)
	q_value: tensor([[1.0481]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2775463356653224, distance: 0.9726614248396867 entropy -13.351160099603092
epoch: 4, step: 110
	action: tensor([[-0.0018,  0.0809,  0.0415,  0.0391,  0.0506, -0.0008,  0.0164]],
       dtype=torch.float64)
	q_value: tensor([[1.0653]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31170357586074326, distance: 0.9493895561679985 entropy -13.35112750136615
epoch: 4, step: 111
	action: tensor([[-0.0018,  0.0809,  0.0348,  0.0123,  0.0505, -0.0029,  0.0163]],
       dtype=torch.float64)
	q_value: tensor([[1.0518]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2986398672043975, distance: 0.958356805366577 entropy -13.351005854660254
epoch: 4, step: 112
	action: tensor([[-0.0018,  0.0809,  0.0233,  0.1876,  0.0505, -0.0060,  0.0164]],
       dtype=torch.float64)
	q_value: tensor([[1.0564]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3751694542378935, distance: 0.9045609172059833 entropy -13.351082677613903
epoch: 4, step: 113
	action: tensor([[-0.0017,  0.0810,  0.0188,  0.0057,  0.0504, -0.0027,  0.0163]],
       dtype=torch.float64)
	q_value: tensor([[1.0305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2954583737854771, distance: 0.9605279837953167 entropy -13.350779893286246
epoch: 4, step: 114
	action: tensor([[-0.0018,  0.0809,  0.0135,  0.0832,  0.0506,  0.0192,  0.0164]],
       dtype=torch.float64)
	q_value: tensor([[1.0563]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33464143510729405, distance: 0.9334360489109338 entropy -13.351301830852293
epoch: 4, step: 115
	action: tensor([[-0.0018,  0.0810,  0.0361,  0.0075,  0.0505,  0.0451,  0.0163]],
       dtype=torch.float64)
	q_value: tensor([[1.0376]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30819508413334107, distance: 0.9518061686502061 entropy -13.351332777127094
epoch: 4, step: 116
	action: tensor([[-0.0019,  0.0809,  0.0349,  0.0428,  0.0506,  0.0074,  0.0164]],
       dtype=torch.float64)
	q_value: tensor([[1.0441]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3149793998457239, distance: 0.9471276365837679 entropy -13.351216986335174
epoch: 4, step: 117
	action: tensor([[-0.0018,  0.0809,  0.0376,  0.0521,  0.0505, -0.0182,  0.0163]],
       dtype=torch.float64)
	q_value: tensor([[1.0485]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31336108767412185, distance: 0.9482457372640479 entropy -13.351125280630454
epoch: 4, step: 118
	action: tensor([[-0.0018,  0.0809,  0.0444,  0.0561,  0.0505,  0.0156,  0.0163]],
       dtype=torch.float64)
	q_value: tensor([[1.0541]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3237218354224446, distance: 0.9410644679142908 entropy -13.350932991374403
epoch: 4, step: 119
	action: tensor([[-0.0018,  0.0809,  0.0382,  0.0287,  0.0505,  0.0016,  0.0163]],
       dtype=torch.float64)
	q_value: tensor([[1.0446]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30737527770260076, distance: 0.9523699589391725 entropy -13.351006932473497
epoch: 4, step: 120
	action: tensor([[-0.0018,  0.0809,  0.0659, -0.0170,  0.0505,  0.0301,  0.0163]],
       dtype=torch.float64)
	q_value: tensor([[1.0527]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2937253450656566, distance: 0.9617086096939528 entropy -13.351062298532794
epoch: 4, step: 121
	action: tensor([[-0.0019,  0.0808,  0.0149, -0.0061,  0.0505,  0.0075,  0.0164]],
       dtype=torch.float64)
	q_value: tensor([[1.0545]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2921145086717869, distance: 0.9628046938551492 entropy -13.350784173347526
epoch: 4, step: 122
	action: tensor([[-0.0018,  0.0809, -0.0055, -0.0146,  0.0506,  0.0007,  0.0164]],
       dtype=torch.float64)
	q_value: tensor([[1.0551]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28663961194316023, distance: 0.9665207633170864 entropy -13.351399221869613
epoch: 4, step: 123
	action: tensor([[-0.0018,  0.0810,  0.0669,  0.0077,  0.0506,  0.0019,  0.0164]],
       dtype=torch.float64)
	q_value: tensor([[1.0573]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2985553154370083, distance: 0.958414570497425 entropy -13.351679180456179
epoch: 4, step: 124
	action: tensor([[-0.0018,  0.0808, -0.0006,  0.0364,  0.0505,  0.0289,  0.0163]],
       dtype=torch.float64)
	q_value: tensor([[1.0583]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3154836854600723, distance: 0.9467789530724988 entropy -13.35063357945938
epoch: 4, step: 125
	action: tensor([[-0.0018,  0.0810,  0.0597,  0.0826,  0.0506,  0.0152,  0.0164]],
       dtype=torch.float64)
	q_value: tensor([[1.0411]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33686565523215184, distance: 0.9318745560854974 entropy -13.3516627023507
epoch: 4, step: 126
	action: tensor([[-0.0018,  0.0808,  0.0451,  0.0694,  0.0504,  0.0264,  0.0163]],
       dtype=torch.float64)
	q_value: tensor([[1.0404]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33239917158730337, distance: 0.9350075693748234 entropy -13.350663429808046
epoch: 4, step: 127
	action: tensor([[-0.0018,  0.0808,  0.0457, -0.0094,  0.0505,  0.0246,  0.0163]],
       dtype=torch.float64)
	q_value: tensor([[1.0385]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2955064261351269, distance: 0.9604952274528726 entropy -13.350958860075533
LOSS epoch 4 actor 0.4686085154185331 critic 19.79548297591021 entropy 0.01
epoch: 5, step: 0
	action: tensor([[-2.8824e-05,  9.5876e-02, -8.4595e-03, -9.5573e-04,  6.4431e-02,
         -2.3131e-02,  2.2007e-02]], dtype=torch.float64)
	q_value: tensor([[1.8171]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2995684996878092, distance: 0.9577221413751825 entropy -13.27086895490493
epoch: 5, step: 1
	action: tensor([[ 3.7383e-05,  9.6085e-02,  9.4643e-03,  4.2462e-03,  6.4522e-02,
         -2.0977e-02,  2.2069e-02]], dtype=torch.float64)
	q_value: tensor([[1.8417]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3028555515568856, distance: 0.955472253662565 entropy -13.271255378699362
epoch: 5, step: 2
	action: tensor([[ 3.2333e-05,  9.6025e-02,  5.1990e-02, -7.9056e-02,  6.4492e-02,
          2.0268e-02,  2.2044e-02]], dtype=torch.float64)
	q_value: tensor([[1.8420]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27473213878294034, distance: 0.9745540031659549 entropy -13.271188402030619
epoch: 5, step: 3
	action: tensor([[-0.0001,  0.0959,  0.0244,  0.0433,  0.0644,  0.0013,  0.0221]],
       dtype=torch.float64)
	q_value: tensor([[1.8518]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32549606107548934, distance: 0.9398292090065361 entropy -13.27085654780014
epoch: 5, step: 4
	action: tensor([[ 1.7937e-05,  9.5997e-02,  3.8798e-02, -9.5719e-02,  6.4493e-02,
          3.0078e-02,  2.1988e-02]], dtype=torch.float64)
	q_value: tensor([[1.8251]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26969194305395616, distance: 0.9779344360475942 entropy -13.271159132005902
epoch: 5, step: 5
	action: tensor([[-0.0001,  0.0959,  0.0088,  0.0699,  0.0644, -0.0068,  0.0221]],
       dtype=torch.float64)
	q_value: tensor([[1.8500]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3343702779943556, distance: 0.9336262336137001 entropy -13.270881449756956
epoch: 5, step: 6
	action: tensor([[ 4.3950e-05,  9.6065e-02,  5.4208e-02, -4.3734e-02,  6.4523e-02,
          1.3120e-02,  2.1986e-02]], dtype=torch.float64)
	q_value: tensor([[1.8207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28958748862926, distance: 0.9645216800708344 entropy -13.271253521082285
epoch: 5, step: 7
	action: tensor([[-5.8056e-05,  9.5870e-02,  2.9905e-02, -6.0701e-02,  6.4434e-02,
          3.7696e-02,  2.2052e-02]], dtype=torch.float64)
	q_value: tensor([[1.8464]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28765485465634233, distance: 0.9658327502503338 entropy -13.270901728952131
epoch: 5, step: 8
	action: tensor([[-9.8658e-05,  9.5940e-02,  4.9255e-02,  2.9769e-02,  6.4491e-02,
          3.7474e-02,  2.2084e-02]], dtype=torch.float64)
	q_value: tensor([[1.8366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3293941570452926, distance: 0.9371095414972418 entropy -13.27095811584229
epoch: 5, step: 9
	action: tensor([[-3.7420e-05,  9.5918e-02,  5.0399e-02,  4.9982e-03,  6.4476e-02,
          1.6442e-02,  2.1974e-02]], dtype=torch.float64)
	q_value: tensor([[1.8172]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3128756870113728, distance: 0.9485808462888593 entropy -13.270945615711174
epoch: 5, step: 10
	action: tensor([[-2.9040e-05,  9.5907e-02,  2.2871e-02, -1.1758e-02,  6.4456e-02,
          2.1285e-03,  2.1999e-02]], dtype=torch.float64)
	q_value: tensor([[1.8323]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30119131325250337, distance: 0.9566120358537841 entropy -13.270935561471543
epoch: 5, step: 11
	action: tensor([[-1.2295e-05,  9.5984e-02,  7.2960e-04,  2.9157e-02,  6.4491e-02,
         -2.6419e-03,  2.2041e-02]], dtype=torch.float64)
	q_value: tensor([[1.8381]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3175904189893337, distance: 0.9453208811530242 entropy -13.271098559035106
epoch: 5, step: 12
	action: tensor([[2.5922e-05, 9.6061e-02, 8.5363e-02, 6.4651e-02, 6.4529e-02, 1.5845e-02,
         2.2026e-02]], dtype=torch.float64)
	q_value: tensor([[1.8271]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3424423250363263, distance: 0.9279479546851009 entropy -13.27124640821808
epoch: 5, step: 13
	action: tensor([[-5.6879e-06,  9.5814e-02,  6.9677e-02, -5.8987e-03,  6.4375e-02,
         -1.4061e-03,  2.1904e-02]], dtype=torch.float64)
	q_value: tensor([[1.8230]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3037214643152232, distance: 0.9548786803452006 entropy -13.270929417571008
epoch: 5, step: 14
	action: tensor([[-1.9194e-05,  9.5844e-02,  5.6562e-02, -9.2071e-03,  6.4403e-02,
          1.3025e-02,  2.1997e-02]], dtype=torch.float64)
	q_value: tensor([[1.8453]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30554689897433807, distance: 0.9536261553135293 entropy -13.270871474954888
epoch: 5, step: 15
	action: tensor([[-3.1461e-05,  9.5849e-02,  2.5325e-02,  4.9031e-02,  6.4424e-02,
          4.0232e-02,  2.2004e-02]], dtype=torch.float64)
	q_value: tensor([[1.8365]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3373936486549777, distance: 0.9315034988345835 entropy -13.270928198868818
epoch: 5, step: 16
	action: tensor([[-1.7968e-05,  9.5975e-02,  1.0674e-03, -1.9659e-02,  6.4514e-02,
          2.1511e-02,  2.1975e-02]], dtype=torch.float64)
	q_value: tensor([[1.8056]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30205332996994405, distance: 0.9560218385357429 entropy -13.271107811742098
epoch: 5, step: 17
	action: tensor([[-3.3184e-05,  9.6013e-02,  5.1814e-02, -3.3357e-02,  6.4527e-02,
          9.3919e-03,  2.2061e-02]], dtype=torch.float64)
	q_value: tensor([[1.8273]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29340143357500537, distance: 0.9619291136858257 entropy -13.27117817384939
epoch: 5, step: 18
	action: tensor([[-4.1848e-05,  9.5846e-02,  3.2027e-02,  3.3366e-02,  6.4422e-02,
         -1.5253e-02,  2.2036e-02]], dtype=torch.float64)
	q_value: tensor([[1.8434]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31749444993659526, distance: 0.9453873502960758 entropy -13.270948458614656
epoch: 5, step: 19
	action: tensor([[ 3.3254e-05,  9.5933e-02,  2.5662e-02,  4.1458e-02,  6.4441e-02,
         -1.3457e-02,  2.1985e-02]], dtype=torch.float64)
	q_value: tensor([[1.8339]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3214458380101458, distance: 0.9426467023686316 entropy -13.271137905873994
epoch: 5, step: 20
	action: tensor([[ 3.7140e-05,  9.5955e-02,  6.1947e-02, -3.4321e-02,  6.4456e-02,
         -1.5962e-02,  2.1984e-02]], dtype=torch.float64)
	q_value: tensor([[1.8304]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28666780767030475, distance: 0.9665016621548501 entropy -13.271180334259139
epoch: 5, step: 21
	action: tensor([[-1.5074e-05,  9.5815e-02,  3.6738e-02,  4.7185e-02,  6.4373e-02,
         -1.1081e-03,  2.2035e-02]], dtype=torch.float64)
	q_value: tensor([[1.8557]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3272922460966139, distance: 0.9385770054281022 entropy -13.27096750349485
epoch: 5, step: 22
	action: tensor([[ 2.1088e-05,  9.5929e-02,  6.6389e-02,  1.4194e-01,  6.4450e-02,
         -1.5680e-02,  2.1966e-02]], dtype=torch.float64)
	q_value: tensor([[1.8251]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36900767675631796, distance: 0.9090101468214475 entropy -13.271135126030128
epoch: 5, step: 23
	action: tensor([[5.9081e-05, 9.5961e-02, 5.0490e-02, 4.3894e-02, 6.4420e-02, 4.8502e-03,
         2.1874e-02]], dtype=torch.float64)
	q_value: tensor([[1.8153]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32807182348469743, distance: 0.9380330058142078 entropy -13.271197730879573
epoch: 5, step: 24
	action: tensor([[ 8.2742e-06,  9.5883e-02,  7.3225e-03, -2.2164e-02,  6.4426e-02,
         -1.5996e-03,  2.1955e-02]], dtype=torch.float64)
	q_value: tensor([[1.8257]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29551310035036915, distance: 0.9604906776831458 entropy -13.271047956988767
epoch: 5, step: 25
	action: tensor([[-7.4747e-06,  9.5993e-02,  8.8461e-03, -1.4968e-02,  6.4497e-02,
         -4.0756e-02,  2.2061e-02]], dtype=torch.float64)
	q_value: tensor([[1.8383]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2896716144213305, distance: 0.9644645699050529 entropy -13.271188704062041
epoch: 5, step: 26
	action: tensor([[4.2559e-05, 9.5981e-02, 2.9317e-02, 3.7142e-02, 6.4448e-02, 2.7088e-02,
         2.2065e-02]], dtype=torch.float64)
	q_value: tensor([[1.8537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3293872124056626, distance: 0.9371143937299501 entropy -13.271250657395601
epoch: 5, step: 27
	action: tensor([[-1.6003e-05,  9.5983e-02,  7.4116e-02, -4.0824e-02,  6.4508e-02,
         -1.3726e-02,  2.1988e-02]], dtype=torch.float64)
	q_value: tensor([[1.8166]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2840348318387922, distance: 0.9682837432228568 entropy -13.27107885222324
epoch: 5, step: 28
	action: tensor([[-2.4166e-05,  9.5772e-02,  4.4017e-02,  6.0424e-02,  6.4346e-02,
          2.6186e-02,  2.2034e-02]], dtype=torch.float64)
	q_value: tensor([[1.8583]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3402014446968743, distance: 0.9295277794010389 entropy -13.270866149105629
epoch: 5, step: 29
	action: tensor([[-2.6798e-06,  9.5916e-02, -4.1833e-03,  7.1223e-02,  6.4463e-02,
         -3.1881e-03,  2.1945e-02]], dtype=torch.float64)
	q_value: tensor([[1.8114]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.335094367289381, distance: 0.9331182840064067 entropy -13.27108307343882
epoch: 5, step: 30
	action: tensor([[ 4.6170e-05,  9.6082e-02,  2.7522e-02, -2.1464e-03,  6.4541e-02,
          4.0900e-03,  2.1993e-02]], dtype=torch.float64)
	q_value: tensor([[1.8155]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3062532894557446, distance: 0.953141022603993 entropy -13.271306738447622
epoch: 5, step: 31
	action: tensor([[-8.3461e-06,  9.5938e-02, -2.4153e-02, -4.2288e-02,  6.4469e-02,
          1.9903e-03,  2.2019e-02]], dtype=torch.float64)
	q_value: tensor([[1.8342]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2876779319833599, distance: 0.9658171054334292 entropy -13.27110253360824
epoch: 5, step: 32
	action: tensor([[-1.8641e-05,  9.6072e-02,  3.8774e-02,  7.1252e-02,  6.4531e-02,
          2.0161e-02,  2.2116e-02]], dtype=torch.float64)
	q_value: tensor([[1.8380]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34349437375520997, distance: 0.927205329901601 entropy -13.2713900338169
epoch: 5, step: 33
	action: tensor([[ 1.1581e-05,  9.5935e-02,  4.4961e-02,  8.8997e-02,  6.4468e-02,
         -2.3373e-04,  2.1942e-02]], dtype=torch.float64)
	q_value: tensor([[1.8112]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3469115136097929, distance: 0.9247891099744114 entropy -13.271152456420703
epoch: 5, step: 34
	action: tensor([[3.8118e-05, 9.5930e-02, 3.1772e-02, 3.0842e-02, 6.4435e-02, 5.9833e-03,
         2.1921e-02]], dtype=torch.float64)
	q_value: tensor([[1.8166]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3215662334475995, distance: 0.9425630720535948 entropy -13.271235009049757
epoch: 5, step: 35
	action: tensor([[5.8286e-06, 9.5936e-02, 3.2439e-02, 7.9828e-02, 6.4466e-02, 2.3496e-02,
         2.1984e-02]], dtype=torch.float64)
	q_value: tensor([[1.8257]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3475706991376323, distance: 0.9243222808136901 entropy -13.271124711109328
epoch: 5, step: 36
	action: tensor([[ 1.6037e-05,  9.5963e-02,  1.5459e-02, -7.2467e-02,  6.4486e-02,
          8.5415e-03,  2.1942e-02]], dtype=torch.float64)
	q_value: tensor([[1.8070]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2755524171070255, distance: 0.9740027367117706 entropy -13.27119000038263
epoch: 5, step: 37
	action: tensor([[-6.8346e-05,  9.5939e-02,  3.5683e-03, -1.6822e-02,  6.4458e-02,
          5.3394e-02,  2.2113e-02]], dtype=torch.float64)
	q_value: tensor([[1.8485]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.310828315076366, distance: 0.949993002166024 entropy -13.271191967540874
epoch: 5, step: 38
	action: tensor([[-6.4903e-05,  9.6017e-02,  5.9169e-02,  5.7362e-02,  6.4550e-02,
          1.9978e-02,  2.2054e-02]], dtype=torch.float64)
	q_value: tensor([[1.8134]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3383559938844364, distance: 0.9308268122408961 entropy -13.27106114158108
epoch: 5, step: 39
	action: tensor([[-2.7657e-06,  9.5863e-02,  4.3432e-02,  7.6311e-02,  6.4422e-02,
          3.8678e-02,  2.1932e-02]], dtype=torch.float64)
	q_value: tensor([[1.8175]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35042429483107185, distance: 0.922298664974285 entropy -13.271022801681047
epoch: 5, step: 40
	action: tensor([[-3.7910e-06,  9.5926e-02,  3.5901e-02,  3.0188e-03,  6.4475e-02,
         -3.3407e-03,  2.1932e-02]], dtype=torch.float64)
	q_value: tensor([[1.8026]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30681445038947586, distance: 0.9527554541371757 entropy -13.27111209640741
epoch: 5, step: 41
	action: tensor([[1.1588e-06, 9.5914e-02, 5.5389e-04, 4.8288e-02, 6.4447e-02, 5.6046e-03,
         2.2008e-02]], dtype=torch.float64)
	q_value: tensor([[1.8371]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32763815785508, distance: 0.9383356624618798 entropy -13.27107665250915
epoch: 5, step: 42
	action: tensor([[ 2.7901e-05,  9.6042e-02,  5.5256e-02, -1.5996e-02,  6.4529e-02,
         -3.2487e-02,  2.2004e-02]], dtype=torch.float64)
	q_value: tensor([[1.8175]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2913015225335034, distance: 0.9633574119627782 entropy -13.271280759530825
epoch: 5, step: 43
	action: tensor([[ 1.8367e-05,  9.5840e-02, -3.8277e-03, -6.7269e-02,  6.4372e-02,
          2.1480e-02,  2.2020e-02]], dtype=torch.float64)
	q_value: tensor([[1.8572]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2812058533990175, distance: 0.9701948370848373 entropy -13.271022975645327
epoch: 5, step: 44
	action: tensor([[-7.3308e-05,  9.6009e-02,  2.7412e-02, -3.1525e-02,  6.4509e-02,
          2.9579e-02,  2.2119e-02]], dtype=torch.float64)
	q_value: tensor([[1.8388]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2989980567264663, distance: 0.9581120543540413 entropy -13.271256146405701
epoch: 5, step: 45
	action: tensor([[-5.8111e-05,  9.5926e-02, -9.5544e-04, -4.0079e-02,  6.4484e-02,
          4.0082e-03,  2.2050e-02]], dtype=torch.float64)
	q_value: tensor([[1.8307]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28894692019280854, distance: 0.9649564295456946 entropy -13.271011140555595
epoch: 5, step: 46
	action: tensor([[-2.7367e-05,  9.6006e-02,  6.2035e-02,  4.1916e-02,  6.4503e-02,
         -5.6596e-03,  2.2089e-02]], dtype=torch.float64)
	q_value: tensor([[1.8398]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3250550559740637, distance: 0.940136399021755 entropy -13.27127022284864
epoch: 5, step: 47
	action: tensor([[1.4332e-05, 9.5843e-02, 5.0708e-02, 2.6903e-02, 6.4389e-02, 1.2623e-02,
         2.1946e-02]], dtype=torch.float64)
	q_value: tensor([[1.8325]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3220562374825263, distance: 0.9422226238353454 entropy -13.270991160633084
epoch: 5, step: 48
	action: tensor([[-9.9215e-06,  9.5878e-02,  1.8922e-02, -3.8097e-02,  6.4433e-02,
          2.7048e-02,  2.1969e-02]], dtype=torch.float64)
	q_value: tensor([[1.8267]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29532192526252266, distance: 0.9606209919829534 entropy -13.271000846212443
epoch: 5, step: 49
	action: tensor([[-6.0847e-05,  9.5988e-02,  6.3101e-03, -3.4883e-02,  6.4514e-02,
          1.6770e-02,  2.2072e-02]], dtype=torch.float64)
	q_value: tensor([[1.8337]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29428539057076986, distance: 0.961327237238869 entropy -13.271052110520982
epoch: 5, step: 50
	action: tensor([[-4.3019e-05,  9.6023e-02,  2.6541e-02, -1.7586e-02,  6.4523e-02,
          1.0265e-02,  2.2081e-02]], dtype=torch.float64)
	q_value: tensor([[1.8358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30064927247321915, distance: 0.9569829687230619 entropy -13.271168189939344
epoch: 5, step: 51
	action: tensor([[-2.7409e-05,  9.5971e-02,  4.6196e-02, -7.1508e-03,  6.4491e-02,
         -2.4504e-03,  2.2043e-02]], dtype=torch.float64)
	q_value: tensor([[1.8369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30257193936434, distance: 0.9556665864343552 entropy -13.271067176192332
epoch: 5, step: 52
	action: tensor([[-8.9110e-06,  9.5881e-02,  3.6989e-02,  9.0475e-02,  6.4430e-02,
         -5.3637e-03,  2.2011e-02]], dtype=torch.float64)
	q_value: tensor([[1.8410]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.345732154677871, distance: 0.9256237337514301 entropy -13.27102052604285
epoch: 5, step: 53
	action: tensor([[4.3505e-05, 9.5962e-02, 1.2171e-02, 2.9853e-04, 6.4450e-02, 9.9371e-03,
         2.1930e-02]], dtype=torch.float64)
	q_value: tensor([[1.8175]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3083199125687348, distance: 0.9517202934040322 entropy -13.27127047772868
epoch: 5, step: 54
	action: tensor([[-9.4995e-06,  9.5987e-02,  4.7378e-02,  5.4039e-02,  6.4502e-02,
          1.3805e-02,  2.2031e-02]], dtype=torch.float64)
	q_value: tensor([[1.8287]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3346698798954464, distance: 0.9334160960044506 entropy -13.271156848785926
epoch: 5, step: 55
	action: tensor([[ 5.4281e-06,  9.5898e-02,  1.8200e-02,  2.8922e-02,  6.4442e-02,
         -1.3641e-02,  2.1948e-02]], dtype=torch.float64)
	q_value: tensor([[1.8190]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3154777671373532, distance: 0.9467830459853757 entropy -13.27107868480715
epoch: 5, step: 56
	action: tensor([[ 3.4082e-05,  9.5975e-02,  4.0646e-02, -1.4311e-02,  6.4471e-02,
         -2.6942e-02,  2.2003e-02]], dtype=torch.float64)
	q_value: tensor([[1.8322]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29333273694783124, distance: 0.9619758726822184 entropy -13.271189837630939
epoch: 5, step: 57
	action: tensor([[1.8009e-05, 9.5888e-02, 3.2559e-02, 1.8290e-02, 6.4409e-02, 1.3570e-02,
         2.2030e-02]], dtype=torch.float64)
	q_value: tensor([[1.8521]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3177210416184987, distance: 0.9452304030795462 entropy -13.271086586757066
epoch: 5, step: 58
	action: tensor([[-1.0035e-05,  9.5928e-02,  1.3600e-02,  2.3884e-02,  6.4469e-02,
          2.4106e-02,  2.1994e-02]], dtype=torch.float64)
	q_value: tensor([[1.8258]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3220026553208014, distance: 0.9422598579912437 entropy -13.271078998658965
epoch: 5, step: 59
	action: tensor([[-1.2199e-05,  9.5989e-02,  2.2568e-02,  5.9665e-02,  6.4515e-02,
         -3.0540e-04,  2.2008e-02]], dtype=torch.float64)
	q_value: tensor([[1.8171]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3324371160203037, distance: 0.9349809974813387 entropy -13.27114690326984
epoch: 5, step: 60
	action: tensor([[3.1666e-05, 9.5978e-02, 1.9877e-03, 3.7742e-03, 6.4479e-02, 1.4909e-02,
         2.1970e-02]], dtype=torch.float64)
	q_value: tensor([[1.8205]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3107705603033877, distance: 0.9500328075453802 entropy -13.271223530138252
epoch: 5, step: 61
	action: tensor([[-9.5960e-06,  9.6018e-02,  5.8408e-02, -1.1631e-01,  6.4525e-02,
         -2.5073e-02,  2.2038e-02]], dtype=torch.float64)
	q_value: tensor([[1.8242]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2454225821338334, distance: 0.9940508368567935 entropy -13.271198995022827
epoch: 5, step: 62
	action: tensor([[-7.7596e-05,  9.5767e-02,  4.3099e-02, -2.6744e-02,  6.4303e-02,
         -4.6720e-03,  2.2151e-02]], dtype=torch.float64)
	q_value: tensor([[1.8800]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2927349526106692, distance: 0.962382664236299 entropy -13.27087065442579
epoch: 5, step: 63
	action: tensor([[-1.8007e-05,  9.5877e-02,  2.6550e-03, -9.6790e-03,  6.4427e-02,
          2.0864e-02,  2.2036e-02]], dtype=torch.float64)
	q_value: tensor([[1.8461]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3061685298458129, distance: 0.9531992465873376 entropy -13.271038193845033
epoch: 5, step: 64
	action: tensor([[-2.6880e-05,  9.6019e-02,  1.8451e-02,  3.4967e-02,  6.4530e-02,
          3.5494e-02,  2.2050e-02]], dtype=torch.float64)
	q_value: tensor([[1.8248]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32984489422369956, distance: 0.9367945568347815 entropy -13.271154401334332
epoch: 5, step: 65
	action: tensor([[-1.9669e-05,  9.5984e-02,  8.2903e-03,  3.8153e-02,  6.4518e-02,
          1.4527e-02,  2.1994e-02]], dtype=torch.float64)
	q_value: tensor([[1.8104]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32577589741845336, distance: 0.9396342319001392 entropy -13.271121420704063
epoch: 5, step: 66
	action: tensor([[8.5567e-06, 9.6011e-02, 4.1261e-03, 5.1700e-03, 6.4519e-02, 1.3612e-02,
         2.2002e-02]], dtype=torch.float64)
	q_value: tensor([[1.8172]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31112665037281984, distance: 0.949787358834483 entropy -13.27122182891786
epoch: 5, step: 67
	action: tensor([[-9.7576e-06,  9.6046e-02,  4.9960e-02,  1.3932e-01,  6.4536e-02,
          3.4987e-03,  2.2042e-02]], dtype=torch.float64)
	q_value: tensor([[1.8263]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37080256871284556, distance: 0.9077163617230386 entropy -13.271170765665795
epoch: 5, step: 68
	action: tensor([[4.3902e-05, 9.6007e-02, 4.2196e-02, 3.3016e-02, 6.4466e-02, 2.3775e-02,
         2.1896e-02]], dtype=torch.float64)
	q_value: tensor([[1.8058]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3273501666968781, distance: 0.9385365984954817 entropy -13.271291699025396
epoch: 5, step: 69
	action: tensor([[-1.9096e-05,  9.5941e-02,  3.5309e-02, -2.6752e-02,  6.4480e-02,
          3.0290e-02,  2.1978e-02]], dtype=torch.float64)
	q_value: tensor([[1.8210]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30143841585923037, distance: 0.9564428892530763 entropy -13.271019105497057
epoch: 5, step: 70
	action: tensor([[-6.0335e-05,  9.5939e-02,  2.6818e-02,  7.2803e-02,  6.4489e-02,
          1.2423e-02,  2.2045e-02]], dtype=torch.float64)
	q_value: tensor([[1.8320]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3413927179834326, distance: 0.9286882641613017 entropy -13.27095009457139
epoch: 5, step: 71
	action: tensor([[ 2.1839e-05,  9.6008e-02,  2.9748e-02, -4.1405e-02,  6.4503e-02,
         -5.5738e-03,  2.1961e-02]], dtype=torch.float64)
	q_value: tensor([[1.8142]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28609530553822626, distance: 0.9668894290969224 entropy -13.271193875283373
epoch: 5, step: 72
	action: tensor([[-2.8646e-05,  9.5944e-02,  3.9129e-02,  1.3982e-01,  6.4458e-02,
          4.7021e-02,  2.2073e-02]], dtype=torch.float64)
	q_value: tensor([[1.8501]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3800421849762733, distance: 0.9010269120296626 entropy -13.271116434464618
epoch: 5, step: 73
	action: tensor([[1.2618e-05, 9.6048e-02, 1.7279e-02, 2.4732e-02, 6.4530e-02, 3.7925e-02,
         2.1910e-02]], dtype=torch.float64)
	q_value: tensor([[1.7858]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3259256245778994, distance: 0.9395298922780512 entropy -13.271209832553833
epoch: 5, step: 74
	action: tensor([[-2.7906e-05,  9.5985e-02,  4.7809e-02,  5.6962e-02,  6.4521e-02,
         -1.3053e-02,  2.2004e-02]], dtype=torch.float64)
	q_value: tensor([[1.8117]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3295084698537927, distance: 0.9370296673131723 entropy -13.271093702523377
epoch: 5, step: 75
	action: tensor([[3.4951e-05, 9.5897e-02, 2.9452e-02, 5.1023e-02, 6.4414e-02, 2.2461e-02,
         2.1948e-02]], dtype=torch.float64)
	q_value: tensor([[1.8296]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33438245832828006, distance: 0.9336176913784798 entropy -13.271108857173987
epoch: 5, step: 76
	action: tensor([[3.9851e-07, 9.5954e-02, 1.6863e-02, 1.7748e-02, 6.4488e-02, 3.6590e-02,
         2.1969e-02]], dtype=torch.float64)
	q_value: tensor([[1.8134]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3224471825835896, distance: 0.9419509121838066 entropy -13.271146717971567
epoch: 5, step: 77
	action: tensor([[-3.0559e-05,  9.5982e-02,  5.5442e-02, -9.1994e-02,  6.4519e-02,
         -3.1980e-02,  2.2011e-02]], dtype=torch.float64)
	q_value: tensor([[1.8139]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25541028249579245, distance: 0.9874502191034548 entropy -13.271084976942864
epoch: 5, step: 78
	action: tensor([[-4.4399e-05,  9.5793e-02,  2.3113e-02,  1.2166e-01,  6.4320e-02,
          8.3300e-03,  2.2125e-02]], dtype=torch.float64)
	q_value: tensor([[1.8762]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36130356639394245, distance: 0.9145425975226054 entropy -13.27098726642829
epoch: 5, step: 79
	action: tensor([[3.9255e-05, 9.6083e-02, 5.9938e-02, 8.2685e-02, 6.4526e-02, 3.0205e-02,
         2.1937e-02]], dtype=torch.float64)
	q_value: tensor([[1.8041]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3526892916144886, distance: 0.9206892854175748 entropy -13.271322448410263
epoch: 5, step: 80
	action: tensor([[-2.8701e-07,  9.5908e-02,  2.9357e-02,  3.5812e-02,  6.4448e-02,
          2.9835e-02,  2.1915e-02]], dtype=torch.float64)
	q_value: tensor([[1.8085]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32933496185897315, distance: 0.9371509004791193 entropy -13.271061758581556
epoch: 5, step: 81
	action: tensor([[-2.0099e-05,  9.5985e-02,  2.7202e-02, -7.0694e-02,  6.4512e-02,
          9.0314e-03,  2.1989e-02]], dtype=torch.float64)
	q_value: tensor([[1.8156]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2762195383957128, distance: 0.9735541689787479 entropy -13.271063966569864
epoch: 5, step: 82
	action: tensor([[-7.3099e-05,  9.5941e-02,  4.7778e-02,  4.4720e-02,  6.4458e-02,
         -1.4288e-02,  2.2107e-02]], dtype=torch.float64)
	q_value: tensor([[1.8511]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3235306563928384, distance: 0.9411974746325662 entropy -13.27109126383463
epoch: 5, step: 83
	action: tensor([[ 2.8709e-05,  9.5924e-02,  2.8179e-02, -2.4880e-02,  6.4429e-02,
          6.1582e-03,  2.1965e-02]], dtype=torch.float64)
	q_value: tensor([[1.8347]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2963631928799293, distance: 0.9599109987601229 entropy -13.271056317260415
epoch: 5, step: 84
	action: tensor([[-2.5235e-05,  9.5927e-02,  4.8625e-02,  5.7675e-02,  6.4467e-02,
         -3.5547e-02,  2.2043e-02]], dtype=torch.float64)
	q_value: tensor([[1.8389]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3244337571322169, distance: 0.9405690056387663 entropy -13.27108420376672
epoch: 5, step: 85
	action: tensor([[5.9397e-05, 9.5891e-02, 3.3788e-02, 7.0794e-02, 6.4388e-02, 1.3425e-02,
         2.1948e-02]], dtype=torch.float64)
	q_value: tensor([[1.8391]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3412973255613577, distance: 0.9287555171493062 entropy -13.271068299198173
epoch: 5, step: 86
	action: tensor([[ 2.0231e-05,  9.5950e-02,  4.1371e-02, -8.7970e-02,  6.4471e-02,
          2.4572e-02,  2.1948e-02]], dtype=torch.float64)
	q_value: tensor([[1.8134]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2718003943983215, distance: 0.9765217325031151 entropy -13.27118561420521
epoch: 5, step: 87
	action: tensor([[-0.0001,  0.0959,  0.0242, -0.0616,  0.0644, -0.0113,  0.0221]],
       dtype=torch.float64)
	q_value: tensor([[1.8491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.275336891162005, distance: 0.9741476107194603 entropy -13.270921478676902
epoch: 5, step: 88
	action: tensor([[-3.5825e-05,  9.5913e-02,  6.7068e-02, -1.6620e-02,  6.4427e-02,
         -1.7571e-02,  2.2098e-02]], dtype=torch.float64)
	q_value: tensor([[1.8552]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29458293206665265, distance: 0.96112455977121 entropy -13.271198862207184
epoch: 5, step: 89
	action: tensor([[-3.7184e-06,  9.5808e-02,  8.6137e-02,  1.1708e-02,  6.4367e-02,
          3.7753e-04,  2.2009e-02]], dtype=torch.float64)
	q_value: tensor([[1.8528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3129854455041169, distance: 0.9485050820057309 entropy -13.270931792891032
epoch: 5, step: 90
	action: tensor([[-1.5509e-05,  9.5762e-02,  1.3939e-02, -2.2048e-03,  6.4350e-02,
          2.2826e-02,  2.1957e-02]], dtype=torch.float64)
	q_value: tensor([[1.8411]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3100753593684181, distance: 0.9505118187392423 entropy -13.270829966718116
epoch: 5, step: 91
	action: tensor([[-2.7963e-05,  9.5986e-02,  7.3125e-02, -6.3607e-02,  6.4513e-02,
          1.4905e-03,  2.2031e-02]], dtype=torch.float64)
	q_value: tensor([[1.8238]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2768958659434946, distance: 0.9730991999455398 entropy -13.271102098036824
epoch: 5, step: 92
	action: tensor([[-6.2517e-05,  9.5763e-02,  2.9658e-02, -5.5012e-02,  6.4354e-02,
          4.2390e-02,  2.2059e-02]], dtype=torch.float64)
	q_value: tensor([[1.8574]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2912954465238484, distance: 0.9633615416149339 entropy -13.270796733098729
epoch: 5, step: 93
	action: tensor([[-9.6318e-05,  9.5917e-02,  6.4462e-02,  1.5245e-02,  6.4488e-02,
          8.7163e-03,  2.2070e-02]], dtype=torch.float64)
	q_value: tensor([[1.8310]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3161033860777763, distance: 0.9463502910079646 entropy -13.270925476068117
epoch: 5, step: 94
	action: tensor([[-1.5560e-05,  9.5832e-02,  6.0418e-02, -3.0666e-02,  6.4403e-02,
          1.0909e-02,  2.1971e-02]], dtype=torch.float64)
	q_value: tensor([[1.8335]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2949513059582062, distance: 0.9608735739042639 entropy -13.27091738937577
epoch: 5, step: 95
	action: tensor([[-4.3815e-05,  9.5823e-02,  4.1760e-02, -2.0790e-02,  6.4409e-02,
          3.0094e-02,  2.2026e-02]], dtype=torch.float64)
	q_value: tensor([[1.8433]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30409253062420094, distance: 0.9546242056565097 entropy -13.270896485798257
epoch: 5, step: 96
	action: tensor([[-5.5783e-05,  9.5889e-02,  2.8522e-02, -8.5969e-03,  6.4463e-02,
          9.0590e-03,  2.2027e-02]], dtype=torch.float64)
	q_value: tensor([[1.8299]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30433490937675134, distance: 0.954457947364425 entropy -13.27093583461277
epoch: 5, step: 97
	action: tensor([[-1.8201e-05,  9.5934e-02,  1.5553e-02,  1.3859e-02,  6.4471e-02,
          2.3010e-03,  2.2025e-02]], dtype=torch.float64)
	q_value: tensor([[1.8337]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3125133500363064, distance: 0.9488309179344286 entropy -13.271082936029572
epoch: 5, step: 98
	action: tensor([[ 7.1003e-06,  9.5978e-02,  3.8949e-02, -4.2114e-02,  6.4490e-02,
         -7.3384e-03,  2.2015e-02]], dtype=torch.float64)
	q_value: tensor([[1.8290]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28524282745545637, distance: 0.9674665412372571 entropy -13.271175824855913
epoch: 5, step: 99
	action: tensor([[-3.0028e-05,  9.5916e-02, -6.7805e-03,  9.0471e-02,  6.4439e-02,
          1.7425e-02,  2.2066e-02]], dtype=torch.float64)
	q_value: tensor([[1.8523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34762290694498477, distance: 0.9242852976456067 entropy -13.271064935066574
epoch: 5, step: 100
	action: tensor([[ 2.8880e-05,  9.6124e-02,  5.7388e-02, -2.5238e-02,  6.4574e-02,
          7.7811e-03,  2.1986e-02]], dtype=torch.float64)
	q_value: tensor([[1.8020]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.296975665153298, distance: 0.9594931362647656 entropy -13.271315036414606
epoch: 5, step: 101
	action: tensor([[-3.5938e-05,  9.5837e-02,  4.2551e-02,  1.5122e-02,  6.4415e-02,
         -1.3639e-03,  2.2022e-02]], dtype=torch.float64)
	q_value: tensor([[1.8430]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.312889316915432, distance: 0.9485714381440744 entropy -13.270925987646413
epoch: 5, step: 102
	action: tensor([[2.9737e-06, 9.5897e-02, 5.2630e-02, 9.9208e-03, 6.4436e-02, 1.4473e-02,
         2.1990e-02]], dtype=torch.float64)
	q_value: tensor([[1.8343]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31475795629863956, distance: 0.9472807110723527 entropy -13.271049284934367
epoch: 5, step: 103
	action: tensor([[-2.4811e-05,  9.5902e-02,  7.3708e-02, -8.2012e-03,  6.4450e-02,
          1.5236e-02,  2.1992e-02]], dtype=torch.float64)
	q_value: tensor([[1.8322]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30694824530663267, distance: 0.9526635018534043 entropy -13.270935933171414
epoch: 5, step: 104
	action: tensor([[-4.2239e-05,  9.5835e-02,  3.7081e-02, -4.0059e-03,  6.4413e-02,
          1.4901e-02,  2.1996e-02]], dtype=torch.float64)
	q_value: tensor([[1.8397]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30792335236391977, distance: 0.951993078699713 entropy -13.270814898209258
epoch: 5, step: 105
	action: tensor([[-2.8522e-05,  9.5951e-02,  5.4860e-02, -1.4868e-02,  6.4483e-02,
         -2.2762e-03,  2.2019e-02]], dtype=torch.float64)
	q_value: tensor([[1.8328]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2991309881630807, distance: 0.9580212063531129 entropy -13.270993857640923
epoch: 5, step: 106
	action: tensor([[-1.9200e-05,  9.5887e-02,  4.8988e-02,  1.1378e-01,  6.4430e-02,
          1.4405e-02,  2.2019e-02]], dtype=torch.float64)
	q_value: tensor([[1.8458]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3618021565635452, distance: 0.9141855648630545 entropy -13.27095372121778
epoch: 5, step: 107
	action: tensor([[3.0216e-05, 9.5977e-02, 3.2466e-02, 1.2269e-01, 6.4465e-02, 9.9066e-03,
         2.1909e-02]], dtype=torch.float64)
	q_value: tensor([[1.8068]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36324534303842915, distance: 0.9131513346692207 entropy -13.27122310670383
epoch: 5, step: 108
	action: tensor([[ 3.7861e-05,  9.6049e-02,  3.7746e-02, -2.8610e-02,  6.4505e-02,
          3.5108e-02,  2.1925e-02]], dtype=torch.float64)
	q_value: tensor([[1.8047]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3019526190695522, distance: 0.9560908111028945 entropy -13.271299429777233
epoch: 5, step: 109
	action: tensor([[-6.8100e-05,  9.5933e-02,  3.1121e-02, -2.7210e-02,  6.4488e-02,
         -2.4749e-02,  2.2045e-02]], dtype=torch.float64)
	q_value: tensor([[1.8309]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2877767483828647, distance: 0.9657501119459558 entropy -13.270916553263541
epoch: 5, step: 110
	action: tensor([[ 8.4678e-06,  9.5909e-02,  5.0442e-02,  1.0031e-02,  6.4424e-02,
         -4.1641e-04,  2.2052e-02]], dtype=torch.float64)
	q_value: tensor([[1.8530]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31109192178394784, distance: 0.9498112996350734 entropy -13.271147182631411
epoch: 5, step: 111
	action: tensor([[-2.9867e-06,  9.5872e-02,  2.9952e-02, -8.3187e-02,  6.4423e-02,
         -3.0479e-02,  2.1989e-02]], dtype=torch.float64)
	q_value: tensor([[1.8365]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26082093440941123, distance: 0.9838559640040732 entropy -13.271003532300966
epoch: 5, step: 112
	action: tensor([[-3.3334e-05,  9.5878e-02,  4.7163e-02,  1.6066e-02,  6.4373e-02,
          3.8140e-03,  2.2131e-02]], dtype=torch.float64)
	q_value: tensor([[1.8698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3147669281217975, distance: 0.9472745097140752 entropy -13.271186464801387
epoch: 5, step: 113
	action: tensor([[-4.1464e-06,  9.5884e-02,  1.9440e-02,  8.2435e-02,  6.4432e-02,
         -1.5394e-03,  2.1985e-02]], dtype=torch.float64)
	q_value: tensor([[1.8327]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34184877187078344, distance: 0.9283666724868392 entropy -13.271017223282822
epoch: 5, step: 114
	action: tensor([[4.1803e-05, 9.6013e-02, 6.1272e-02, 4.9684e-02, 6.4492e-02, 2.3994e-02,
         2.1957e-02]], dtype=torch.float64)
	q_value: tensor([[1.8154]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3360189252010847, distance: 0.9324693031855827 entropy -13.271278966264205
epoch: 5, step: 115
	action: tensor([[-1.1442e-05,  9.5854e-02,  5.8160e-02,  4.8966e-02,  6.4422e-02,
         -1.6655e-02,  2.1937e-02]], dtype=torch.float64)
	q_value: tensor([[1.8180]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3253820303877383, distance: 0.9399086487516541 entropy -13.270977529216651
epoch: 5, step: 116
	action: tensor([[ 3.1217e-05,  9.5860e-02,  5.6938e-03, -2.1377e-02,  6.4387e-02,
          1.4186e-02,  2.1944e-02]], dtype=torch.float64)
	q_value: tensor([[1.8346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2995638915262966, distance: 0.9577252918124851 entropy -13.271033026512123
epoch: 5, step: 117
	action: tensor([[-2.6937e-05,  9.6001e-02,  5.5779e-02, -2.7300e-02,  6.4515e-02,
         -3.3042e-02,  2.2059e-02]], dtype=torch.float64)
	q_value: tensor([[1.8311]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2857436532488944, distance: 0.967127533023237 entropy -13.271156376980189
epoch: 5, step: 118
	action: tensor([[1.1210e-05, 9.5834e-02, 5.7271e-02, 5.2296e-02, 6.4366e-02, 9.5267e-03,
         2.2035e-02]], dtype=torch.float64)
	q_value: tensor([[1.8602]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33329017647083514, distance: 0.9343834131193482 entropy -13.27103302019145
epoch: 5, step: 119
	action: tensor([[5.5842e-06, 9.5866e-02, 2.0374e-02, 5.5862e-02, 6.4416e-02, 8.3582e-05,
         2.1940e-02]], dtype=torch.float64)
	q_value: tensor([[1.8227]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3306218842358206, distance: 0.9362513310132369 entropy -13.271027007477043
epoch: 5, step: 120
	action: tensor([[2.9384e-05, 9.5987e-02, 1.5425e-02, 7.4167e-02, 6.4487e-02, 1.6843e-02,
         2.1976e-02]], dtype=torch.float64)
	q_value: tensor([[1.8205]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34238026986013004, distance: 0.927991739905397 entropy -13.271210084401558
epoch: 5, step: 121
	action: tensor([[ 2.4035e-05,  9.6015e-02,  4.2221e-02, -2.9198e-03,  6.4515e-02,
          3.9742e-02,  2.1967e-02]], dtype=torch.float64)
	q_value: tensor([[1.8090]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31490334135034703, distance: 0.947180215370067 entropy -13.271251925339568
epoch: 5, step: 122
	action: tensor([[-5.6176e-05,  9.5897e-02,  2.7892e-02,  8.3311e-02,  6.4471e-02,
          1.1539e-02,  2.2006e-02]], dtype=torch.float64)
	q_value: tensor([[1.8217]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34586621534715656, distance: 0.9255288979014454 entropy -13.270911299389383
epoch: 5, step: 123
	action: tensor([[2.9152e-05, 9.5981e-02, 4.7921e-02, 3.0525e-03, 6.4485e-02, 1.5259e-02,
         2.1945e-02]], dtype=torch.float64)
	q_value: tensor([[1.8108]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3117270364830558, distance: 0.949373376031924 entropy -13.271250615897582
epoch: 5, step: 124
	action: tensor([[-2.4905e-05,  9.5879e-02,  2.4816e-02,  5.1937e-02,  6.4442e-02,
         -1.1142e-02,  2.1997e-02]], dtype=torch.float64)
	q_value: tensor([[1.8312]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32649386419701343, distance: 0.9391338004750845 entropy -13.270969820983044
epoch: 5, step: 125
	action: tensor([[3.9420e-05, 9.5965e-02, 1.1809e-02, 3.7356e-02, 6.4461e-02, 3.5113e-02,
         2.1975e-02]], dtype=torch.float64)
	q_value: tensor([[1.8270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33048301165963967, distance: 0.9363484456994621 entropy -13.27120039569912
epoch: 5, step: 126
	action: tensor([[-1.4990e-05,  9.6008e-02,  4.6455e-02, -9.1935e-02,  6.4532e-02,
          2.3134e-02,  2.2000e-02]], dtype=torch.float64)
	q_value: tensor([[1.8090]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26949517663146016, distance: 0.9780661692929828 entropy -13.271152296413883
epoch: 5, step: 127
	action: tensor([[-0.0001,  0.0959,  0.0250, -0.0768,  0.0644,  0.0281,  0.0221]],
       dtype=torch.float64)
	q_value: tensor([[1.8531]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2779227455613278, distance: 0.9724080057591367 entropy -13.270868701039433
LOSS epoch 5 actor 1.3390950360943177 critic 20.07946188245272 entropy 0.01
epoch: 6, step: 0
	action: tensor([[ 0.0084,  0.0731,  0.0021,  0.0113,  0.0853, -0.0012,  0.0322]],
       dtype=torch.float64)
	q_value: tensor([[3.0384]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30430036380391823, distance: 0.9544816454683359 entropy -13.206807488165774
epoch: 6, step: 1
	action: tensor([[0.0084, 0.0732, 0.0212, 0.0988, 0.0854, 0.0346, 0.0322]],
       dtype=torch.float64)
	q_value: tensor([[3.0221]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35305412274490944, distance: 0.9204297937620206 entropy -13.206848629551489
epoch: 6, step: 2
	action: tensor([[0.0084, 0.0732, 0.0576, 0.0803, 0.0856, 0.0073, 0.0321]],
       dtype=torch.float64)
	q_value: tensor([[2.9814]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3406896655307795, distance: 0.9291838116812015 entropy -13.207009942224065
epoch: 6, step: 3
	action: tensor([[ 0.0084,  0.0731, -0.0004, -0.0066,  0.0854, -0.0108,  0.0321]],
       dtype=torch.float64)
	q_value: tensor([[3.0130]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2940305592146115, distance: 0.9615007877146909 entropy -13.20710588112309
epoch: 6, step: 4
	action: tensor([[0.0084, 0.0733, 0.0327, 0.0286, 0.0855, 0.0447, 0.0322]],
       dtype=torch.float64)
	q_value: tensor([[3.0346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32483745730836533, distance: 0.9402879343004497 entropy -13.206844866324031
epoch: 6, step: 5
	action: tensor([[ 0.0084,  0.0732,  0.0115, -0.0459,  0.0854,  0.0230,  0.0321]],
       dtype=torch.float64)
	q_value: tensor([[3.0012]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2847912186878766, distance: 0.9677721326997477 entropy -13.20681699663903
epoch: 6, step: 6
	action: tensor([[ 0.0084,  0.0732,  0.0526, -0.1106,  0.0854, -0.0059,  0.0322]],
       dtype=torch.float64)
	q_value: tensor([[3.0304]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24629211649585692, distance: 0.9934779264474597 entropy -13.206691650023009
epoch: 6, step: 7
	action: tensor([[ 0.0084,  0.0731,  0.0375, -0.1278,  0.0851,  0.0188,  0.0322]],
       dtype=torch.float64)
	q_value: tensor([[3.0810]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24502701597055598, distance: 0.9943113544306776 entropy -13.206666048050243
epoch: 6, step: 8
	action: tensor([[0.0084, 0.0731, 0.0440, 0.1448, 0.0851, 0.0094, 0.0322]],
       dtype=torch.float64)
	q_value: tensor([[3.0675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36937192953405806, distance: 0.9087477369176798 entropy -13.206682613726034
epoch: 6, step: 9
	action: tensor([[0.0084, 0.0733, 0.0255, 0.0523, 0.0856, 0.0296, 0.0321]],
       dtype=torch.float64)
	q_value: tensor([[2.9912]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33139916994205376, distance: 0.9357075829095175 entropy -13.20706879566283
epoch: 6, step: 10
	action: tensor([[0.0084, 0.0732, 0.0338, 0.0571, 0.0855, 0.0027, 0.0321]],
       dtype=torch.float64)
	q_value: tensor([[3.0017]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3272917685793677, distance: 0.9385773385494501 entropy -13.206904676673748
epoch: 6, step: 11
	action: tensor([[ 0.0084,  0.0732, -0.0131,  0.0820,  0.0855, -0.0130,  0.0321]],
       dtype=torch.float64)
	q_value: tensor([[3.0184]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33159217364486604, distance: 0.9355725187164915 entropy -13.207011828844683
epoch: 6, step: 12
	action: tensor([[ 0.0085,  0.0734,  0.0129, -0.0179,  0.0857,  0.0272,  0.0322]],
       dtype=torch.float64)
	q_value: tensor([[3.0070]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29874144660193735, distance: 0.9582874024824919 entropy -13.206931174137852
epoch: 6, step: 13
	action: tensor([[0.0084, 0.0732, 0.0230, 0.0392, 0.0854, 0.0162, 0.0322]],
       dtype=torch.float64)
	q_value: tensor([[3.0172]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3219301897761625, distance: 0.9423102118384422 entropy -13.206704397352238
epoch: 6, step: 14
	action: tensor([[ 0.0084,  0.0732,  0.0674, -0.0726,  0.0854,  0.0172,  0.0321]],
       dtype=torch.float64)
	q_value: tensor([[3.0098]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2703299754557037, distance: 0.977507157319345 entropy -13.206918218474652
epoch: 6, step: 15
	action: tensor([[ 0.0084,  0.0730,  0.0027, -0.0080,  0.0851,  0.0207,  0.0321]],
       dtype=torch.float64)
	q_value: tensor([[3.0576]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3009697866523309, distance: 0.956763649753325 entropy -13.2065848294644
epoch: 6, step: 16
	action: tensor([[ 0.0084,  0.0732,  0.0099,  0.0043,  0.0854, -0.0452,  0.0322]],
       dtype=torch.float64)
	q_value: tensor([[3.0139]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29083812782896457, distance: 0.963672314412362 entropy -13.206717152751306
epoch: 6, step: 17
	action: tensor([[ 0.0085,  0.0732,  0.0781, -0.0357,  0.0854, -0.0098,  0.0322]],
       dtype=torch.float64)
	q_value: tensor([[3.0527]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2813094934633332, distance: 0.9701248902992624 entropy -13.2070138177731
epoch: 6, step: 18
	action: tensor([[0.0084, 0.0731, 0.0593, 0.0408, 0.0852, 0.0017, 0.0321]],
       dtype=torch.float64)
	q_value: tensor([[3.0650]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3206400689655231, distance: 0.9432062230524452 entropy -13.206706870470521
epoch: 6, step: 19
	action: tensor([[ 0.0084,  0.0731,  0.0145,  0.0400,  0.0853, -0.0028,  0.0321]],
       dtype=torch.float64)
	q_value: tensor([[3.0275]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3171456574250374, distance: 0.9456288881929834 entropy -13.207044585508656
epoch: 6, step: 20
	action: tensor([[ 0.0084,  0.0732,  0.0426, -0.0267,  0.0855, -0.0037,  0.0322]],
       dtype=torch.float64)
	q_value: tensor([[3.0178]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2871210365749479, distance: 0.9661945709449278 entropy -13.206942764096878
epoch: 6, step: 21
	action: tensor([[ 0.0084,  0.0731, -0.0040,  0.0352,  0.0853,  0.0148,  0.0321]],
       dtype=torch.float64)
	q_value: tensor([[3.0471]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31847501870949635, distance: 0.9447079781509049 entropy -13.206807893198445
epoch: 6, step: 22
	action: tensor([[ 0.0084,  0.0733, -0.0119, -0.0352,  0.0855,  0.0018,  0.0322]],
       dtype=torch.float64)
	q_value: tensor([[3.0036]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28451006669331247, distance: 0.9679623319378448 entropy -13.206818253597636
epoch: 6, step: 23
	action: tensor([[ 0.0084,  0.0733,  0.0598, -0.0093,  0.0854, -0.0269,  0.0322]],
       dtype=torch.float64)
	q_value: tensor([[3.0299]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28965162839852066, distance: 0.9644781380488151 entropy -13.206789858849882
epoch: 6, step: 24
	action: tensor([[0.0084, 0.0731, 0.0136, 0.0205, 0.0852, 0.0023, 0.0321]],
       dtype=torch.float64)
	q_value: tensor([[3.0605]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3096303792667211, distance: 0.9508182947251654 entropy -13.206863282916212
epoch: 6, step: 25
	action: tensor([[ 0.0084,  0.0732,  0.0246, -0.0255,  0.0855, -0.0216,  0.0322]],
       dtype=torch.float64)
	q_value: tensor([[3.0228]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2831264328827614, distance: 0.9688978157766398 entropy -13.206853206933273
epoch: 6, step: 26
	action: tensor([[0.0084, 0.0732, 0.0572, 0.0290, 0.0853, 0.0563, 0.0322]],
       dtype=torch.float64)
	q_value: tensor([[3.0541]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3292056399528369, distance: 0.9372412498279759 entropy -13.206868620762304
epoch: 6, step: 27
	action: tensor([[0.0084, 0.0731, 0.0077, 0.0023, 0.0854, 0.0176, 0.0321]],
       dtype=torch.float64)
	q_value: tensor([[3.0010]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3049834862830876, distance: 0.9540129173184778 entropy -13.206844333837148
epoch: 6, step: 28
	action: tensor([[ 0.0084,  0.0732,  0.0011,  0.0530,  0.0855, -0.0194,  0.0322]],
       dtype=torch.float64)
	q_value: tensor([[3.0172]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3184139153825887, distance: 0.9447503269335682 entropy -13.206738471042863
epoch: 6, step: 29
	action: tensor([[0.0085, 0.0733, 0.0801, 0.0238, 0.0855, 0.0365, 0.0322]],
       dtype=torch.float64)
	q_value: tensor([[3.0240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3227779360813544, distance: 0.9417209732166008 entropy -13.20697689693041
epoch: 6, step: 30
	action: tensor([[ 0.0084,  0.0731,  0.0167,  0.0568,  0.0853, -0.0016,  0.0320]],
       dtype=torch.float64)
	q_value: tensor([[3.0218]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3249842025673211, distance: 0.9401857438751268 entropy -13.206873397150245
epoch: 6, step: 31
	action: tensor([[0.0084, 0.0732, 0.0700, 0.0663, 0.0855, 0.0473, 0.0321]],
       dtype=torch.float64)
	q_value: tensor([[3.0129]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3453001865173976, distance: 0.9259292462486969 entropy -13.20698500499879
epoch: 6, step: 32
	action: tensor([[0.0084, 0.0731, 0.0412, 0.0399, 0.0854, 0.0050, 0.0320]],
       dtype=torch.float64)
	q_value: tensor([[2.9963]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3201954565307026, distance: 0.943514816925221 entropy -13.207027056283717
epoch: 6, step: 33
	action: tensor([[ 0.0084,  0.0732,  0.0676,  0.0468,  0.0854, -0.0343,  0.0321]],
       dtype=torch.float64)
	q_value: tensor([[3.0235]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3148033500446673, distance: 0.9472493343255691 entropy -13.206972899982492
epoch: 6, step: 34
	action: tensor([[0.0084, 0.0732, 0.0351, 0.0093, 0.0853, 0.0080, 0.0321]],
       dtype=torch.float64)
	q_value: tensor([[3.0524]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3066800161424462, distance: 0.9528478368713164 entropy -13.207129126498119
epoch: 6, step: 35
	action: tensor([[ 0.0084,  0.0732, -0.0269,  0.1292,  0.0854, -0.0106,  0.0321]],
       dtype=torch.float64)
	q_value: tensor([[3.0289]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35076009081448456, distance: 0.9220602445559837 entropy -13.206838620748353
epoch: 6, step: 36
	action: tensor([[0.0084, 0.0735, 0.0109, 0.0007, 0.0858, 0.0320, 0.0322]],
       dtype=torch.float64)
	q_value: tensor([[2.9880]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30832266793893104, distance: 0.9517183977702046 entropy -13.20691696191402
epoch: 6, step: 37
	action: tensor([[0.0084, 0.0732, 0.0580, 0.0334, 0.0854, 0.0344, 0.0322]],
       dtype=torch.float64)
	q_value: tensor([[3.0085]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3256004469395549, distance: 0.9397564824631068 entropy -13.206712916424621
epoch: 6, step: 38
	action: tensor([[ 0.0084,  0.0731,  0.0746,  0.0089,  0.0853, -0.0046,  0.0321]],
       dtype=torch.float64)
	q_value: tensor([[3.0101]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30425621348500975, distance: 0.9545119315273536 entropy -13.206939927076712
epoch: 6, step: 39
	action: tensor([[ 0.0084,  0.0731, -0.0051,  0.0820,  0.0852, -0.0051,  0.0321]],
       dtype=torch.float64)
	q_value: tensor([[3.0461]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3339016670335233, distance: 0.9339548175819722 entropy -13.206911126654566
epoch: 6, step: 40
	action: tensor([[0.0084, 0.0733, 0.0628, 0.0131, 0.0856, 0.0171, 0.0322]],
       dtype=torch.float64)
	q_value: tensor([[3.0016]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31179891559847417, distance: 0.9493238012964419 entropy -13.206959051866832
epoch: 6, step: 41
	action: tensor([[0.0084, 0.0731, 0.0366, 0.0498, 0.0853, 0.0020, 0.0321]],
       dtype=torch.float64)
	q_value: tensor([[3.0286]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3238478908435555, distance: 0.9409767585866909 entropy -13.206891243437179
epoch: 6, step: 42
	action: tensor([[0.0084, 0.0732, 0.0327, 0.0741, 0.0854, 0.0104, 0.0321]],
       dtype=torch.float64)
	q_value: tensor([[3.0185]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.336862479274072, distance: 0.9318767876023881 entropy -13.207016631680052
epoch: 6, step: 43
	action: tensor([[ 0.0084,  0.0732,  0.0085,  0.0171,  0.0855, -0.0080,  0.0321]],
       dtype=torch.float64)
	q_value: tensor([[3.0059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30551856147099876, distance: 0.9536456117092533 entropy -13.207057509336169
epoch: 6, step: 44
	action: tensor([[ 0.0084,  0.0732,  0.0912, -0.0640,  0.0854, -0.0201,  0.0322]],
       dtype=torch.float64)
	q_value: tensor([[3.0266]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.264367783483331, distance: 0.9814926768598866 entropy -13.206901952506836
epoch: 6, step: 45
	action: tensor([[0.0084, 0.0730, 0.0372, 0.0514, 0.0851, 0.0050, 0.0321]],
       dtype=torch.float64)
	q_value: tensor([[3.0843]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3252219806625972, distance: 0.9400201364427682 entropy -13.206620060515776
epoch: 6, step: 46
	action: tensor([[ 0.0084,  0.0732, -0.0193,  0.0730,  0.0854, -0.0029,  0.0321]],
       dtype=torch.float64)
	q_value: tensor([[3.0158]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32969952528813373, distance: 0.9368961552872652 entropy -13.207012871928685
epoch: 6, step: 47
	action: tensor([[ 0.0085,  0.0734,  0.0078,  0.0951,  0.0856, -0.0179,  0.0322]],
       dtype=torch.float64)
	q_value: tensor([[2.9996]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3377730628216028, distance: 0.9312367671385026 entropy -13.206910015354193
epoch: 6, step: 48
	action: tensor([[ 0.0084,  0.0733,  0.0370, -0.0761,  0.0856, -0.0047,  0.0321]],
       dtype=torch.float64)
	q_value: tensor([[3.0104]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2636554050740306, distance: 0.981967795394016 entropy -13.207061657806932
epoch: 6, step: 49
	action: tensor([[ 0.0084,  0.0731,  0.0367, -0.0404,  0.0852,  0.0339,  0.0322]],
       dtype=torch.float64)
	q_value: tensor([[3.0621]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2901122742944058, distance: 0.964165365496567 entropy -13.206740509346517
epoch: 6, step: 50
	action: tensor([[0.0084, 0.0731, 0.0575, 0.0177, 0.0853, 0.0242, 0.0322]],
       dtype=torch.float64)
	q_value: tensor([[3.0300]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31545034323739984, distance: 0.9468020112009937 entropy -13.206676160111337
epoch: 6, step: 51
	action: tensor([[ 0.0084,  0.0731,  0.0227, -0.0278,  0.0853,  0.0067,  0.0321]],
       dtype=torch.float64)
	q_value: tensor([[3.0235]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2889399754764471, distance: 0.9649611418045483 entropy -13.206884679462595
epoch: 6, step: 52
	action: tensor([[0.0084, 0.0732, 0.0806, 0.1127, 0.0854, 0.0166, 0.0322]],
       dtype=torch.float64)
	q_value: tensor([[3.0370]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3601188135556649, distance: 0.9153904219996315 entropy -13.206757614958331
epoch: 6, step: 53
	action: tensor([[ 0.0084,  0.0732,  0.0115, -0.0037,  0.0854,  0.0185,  0.0320]],
       dtype=torch.float64)
	q_value: tensor([[3.0064]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3027100340699057, distance: 0.9555719680523356 entropy -13.207140303997306
epoch: 6, step: 54
	action: tensor([[ 0.0084,  0.0732, -0.0043, -0.0392,  0.0854,  0.0079,  0.0322]],
       dtype=torch.float64)
	q_value: tensor([[3.0194]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28418069145665836, distance: 0.9681851066571651 entropy -13.206732297625845
epoch: 6, step: 55
	action: tensor([[0.0084, 0.0733, 0.0526, 0.1054, 0.0854, 0.0044, 0.0322]],
       dtype=torch.float64)
	q_value: tensor([[3.0322]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3512426192768612, distance: 0.9217175337914835 entropy -13.20674873255362
epoch: 6, step: 56
	action: tensor([[ 0.0084,  0.0732,  0.0623,  0.0155,  0.0855, -0.0075,  0.0321]],
       dtype=torch.float64)
	q_value: tensor([[3.0085]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30649565123672506, distance: 0.9529745172128419 entropy -13.20708073125739
epoch: 6, step: 57
	action: tensor([[0.0084, 0.0731, 0.0177, 0.0198, 0.0853, 0.0289, 0.0321]],
       dtype=torch.float64)
	q_value: tensor([[3.0444]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31606415691427603, distance: 0.946377432535775 entropy -13.206940812061754
epoch: 6, step: 58
	action: tensor([[ 0.0084,  0.0732,  0.0302, -0.0100,  0.0855,  0.0374,  0.0322]],
       dtype=torch.float64)
	q_value: tensor([[3.0082]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30511894765538916, distance: 0.9539199424078216 entropy -13.206775284565497
epoch: 6, step: 59
	action: tensor([[ 0.0084,  0.0732, -0.0224,  0.0596,  0.0854, -0.0186,  0.0321]],
       dtype=torch.float64)
	q_value: tensor([[3.0164]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32018740680745583, distance: 0.943520403097559 entropy -13.206697856621238
epoch: 6, step: 60
	action: tensor([[ 0.0085,  0.0734,  0.0414, -0.0718,  0.0856,  0.0058,  0.0322]],
       dtype=torch.float64)
	q_value: tensor([[3.0118]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26836966153701625, distance: 0.978819350180098 entropy -13.206952072601782
epoch: 6, step: 61
	action: tensor([[0.0084, 0.0731, 0.0738, 0.0294, 0.0852, 0.0217, 0.0322]],
       dtype=torch.float64)
	q_value: tensor([[3.0561]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32106197939539416, distance: 0.9429132926560271 entropy -13.206692743966574
epoch: 6, step: 62
	action: tensor([[0.0084, 0.0731, 0.0779, 0.1285, 0.0853, 0.0309, 0.0320]],
       dtype=torch.float64)
	q_value: tensor([[3.0235]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3709078983061542, distance: 0.907640381281823 entropy -13.206956904386255
epoch: 6, step: 63
	action: tensor([[ 0.0084,  0.0732,  0.0586, -0.0318,  0.0855,  0.0058,  0.0320]],
       dtype=torch.float64)
	q_value: tensor([[2.9900]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28708458908957324, distance: 0.966219270027238 entropy -13.20713115127697
epoch: 6, step: 64
	action: tensor([[ 0.0084,  0.0731,  0.0529, -0.0625,  0.0852,  0.0053,  0.0321]],
       dtype=torch.float64)
	q_value: tensor([[3.0483]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27221691534214887, distance: 0.976242413529318 entropy -13.206737947545788
epoch: 6, step: 65
	action: tensor([[0.0084, 0.0731, 0.0259, 0.0766, 0.0852, 0.0323, 0.0321]],
       dtype=torch.float64)
	q_value: tensor([[3.0564]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3427906734169275, distance: 0.9277021268268414 entropy -13.20666731319001
epoch: 6, step: 66
	action: tensor([[0.0084, 0.0732, 0.0161, 0.0807, 0.0855, 0.0131, 0.0321]],
       dtype=torch.float64)
	q_value: tensor([[2.9898]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33930267486102317, distance: 0.9301606598558806 entropy -13.206983362888277
epoch: 6, step: 67
	action: tensor([[0.0084, 0.0732, 0.0681, 0.0126, 0.0855, 0.0205, 0.0321]],
       dtype=torch.float64)
	q_value: tensor([[2.9981]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31257440449789053, distance: 0.9487887850093297 entropy -13.207012217608213
epoch: 6, step: 68
	action: tensor([[ 0.0084,  0.0731, -0.0021,  0.0903,  0.0853,  0.0287,  0.0321]],
       dtype=torch.float64)
	q_value: tensor([[3.0284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34568309498935423, distance: 0.9256584366343136 entropy -13.206875522505554
epoch: 6, step: 69
	action: tensor([[ 0.0084,  0.0733,  0.0306, -0.0182,  0.0856,  0.0234,  0.0322]],
       dtype=torch.float64)
	q_value: tensor([[2.9801]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29784817037027356, distance: 0.9588975504279207 entropy -13.206890133129056
epoch: 6, step: 70
	action: tensor([[0.0084, 0.0732, 0.0025, 0.0037, 0.0854, 0.0044, 0.0322]],
       dtype=torch.float64)
	q_value: tensor([[3.0276]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30234712256107765, distance: 0.9558206041801119 entropy -13.206721993838938
epoch: 6, step: 71
	action: tensor([[ 0.0084,  0.0733,  0.0762, -0.0670,  0.0855,  0.0298,  0.0322]],
       dtype=torch.float64)
	q_value: tensor([[3.0238]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27639914823265876, distance: 0.9734333652514666 entropy -13.206792649101677
epoch: 6, step: 72
	action: tensor([[0.0083, 0.0730, 0.0438, 0.0245, 0.0852, 0.0042, 0.0321]],
       dtype=torch.float64)
	q_value: tensor([[3.0539]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3128759628047697, distance: 0.9485806559212977 entropy -13.206547953467323
epoch: 6, step: 73
	action: tensor([[ 0.0084,  0.0732,  0.0342, -0.0353,  0.0854, -0.0113,  0.0321]],
       dtype=torch.float64)
	q_value: tensor([[3.0288]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28109465797045186, distance: 0.9702698773862254 entropy -13.206919152390752
epoch: 6, step: 74
	action: tensor([[0.0084, 0.0732, 0.0369, 0.0398, 0.0853, 0.0212, 0.0322]],
       dtype=torch.float64)
	q_value: tensor([[3.0542]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32416024409574073, distance: 0.9407593880432036 entropy -13.206817970920328
epoch: 6, step: 75
	action: tensor([[0.0084, 0.0731, 0.0736, 0.0830, 0.0854, 0.0023, 0.0321]],
       dtype=torch.float64)
	q_value: tensor([[3.0103]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34174180217823413, distance: 0.928442113402622 entropy -13.206947561756406
epoch: 6, step: 76
	action: tensor([[0.0084, 0.0731, 0.0058, 0.0051, 0.0854, 0.0218, 0.0320]],
       dtype=torch.float64)
	q_value: tensor([[3.0192]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3072695443090513, distance: 0.9524426487102059 entropy -13.207150336824492
epoch: 6, step: 77
	action: tensor([[ 0.0084,  0.0732,  0.0380, -0.1171,  0.0854,  0.0013,  0.0322]],
       dtype=torch.float64)
	q_value: tensor([[3.0107]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24570484940729875, distance: 0.9938648955220857 entropy -13.206732004069798
epoch: 6, step: 78
	action: tensor([[ 0.0084,  0.0731,  0.0408, -0.0138,  0.0851,  0.0245,  0.0322]],
       dtype=torch.float64)
	q_value: tensor([[3.0725]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30016675808762183, distance: 0.9573130451864289 entropy -13.206717161566342
epoch: 6, step: 79
	action: tensor([[ 0.0084,  0.0731,  0.0564, -0.0116,  0.0853, -0.0024,  0.0321]],
       dtype=torch.float64)
	q_value: tensor([[3.0256]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2946064803668522, distance: 0.9611085174612223 entropy -13.206765049837253
epoch: 6, step: 80
	action: tensor([[ 0.0084,  0.0731, -0.0362,  0.0014,  0.0853,  0.0275,  0.0321]],
       dtype=torch.float64)
	q_value: tensor([[3.0457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3058292468785012, distance: 0.953432274891973 entropy -13.206823915000598
epoch: 6, step: 81
	action: tensor([[0.0084, 0.0733, 0.0647, 0.0435, 0.0856, 0.0043, 0.0323]],
       dtype=torch.float64)
	q_value: tensor([[2.9968]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32307404619821734, distance: 0.9415150705957711 entropy -13.20669567402706
epoch: 6, step: 82
	action: tensor([[ 0.0084,  0.0731,  0.0706, -0.0042,  0.0853,  0.0047,  0.0321]],
       dtype=torch.float64)
	q_value: tensor([[3.0273]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30029924976358724, distance: 0.9572224221594945 entropy -13.207057400062398
epoch: 6, step: 83
	action: tensor([[ 0.0084,  0.0731,  0.0535,  0.0289,  0.0853, -0.0050,  0.0321]],
       dtype=torch.float64)
	q_value: tensor([[3.0463]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31308558856420743, distance: 0.9484359497999549 entropy -13.206821929773303
epoch: 6, step: 84
	action: tensor([[ 0.0084,  0.0731,  0.0397,  0.0556,  0.0854, -0.0053,  0.0321]],
       dtype=torch.float64)
	q_value: tensor([[3.0360]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3249186671161328, distance: 0.9402313828081059 entropy -13.206973009734785
epoch: 6, step: 85
	action: tensor([[ 0.0084,  0.0732,  0.0215,  0.0183,  0.0854, -0.0154,  0.0321]],
       dtype=torch.float64)
	q_value: tensor([[3.0251]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3046320052333851, distance: 0.954254116679822 entropy -13.207038048377765
epoch: 6, step: 86
	action: tensor([[0.0084, 0.0732, 0.0280, 0.0162, 0.0854, 0.0042, 0.0322]],
       dtype=torch.float64)
	q_value: tensor([[3.0370]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3087571196156863, distance: 0.9514194573666945 entropy -13.206935737279467
epoch: 6, step: 87
	action: tensor([[0.0084, 0.0732, 0.0404, 0.0316, 0.0854, 0.0203, 0.0321]],
       dtype=torch.float64)
	q_value: tensor([[3.0276]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32034578669558644, distance: 0.9434104879811969 entropy -13.206855472182948
epoch: 6, step: 88
	action: tensor([[0.0084, 0.0732, 0.0259, 0.0158, 0.0854, 0.0197, 0.0321]],
       dtype=torch.float64)
	q_value: tensor([[3.0169]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3123106699005952, distance: 0.9489707715646716 entropy -13.206909831897258
epoch: 6, step: 89
	action: tensor([[ 0.0084,  0.0732, -0.0004,  0.1096,  0.0854, -0.0175,  0.0321]],
       dtype=torch.float64)
	q_value: tensor([[3.0178]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34334010357351097, distance: 0.9273142640271044 entropy -13.206810219957703
epoch: 6, step: 90
	action: tensor([[ 0.0084,  0.0734,  0.0259, -0.0288,  0.0857, -0.0290,  0.0322]],
       dtype=torch.float64)
	q_value: tensor([[3.0055]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2798841394005973, distance: 0.9710864213891289 entropy -13.207006738564862
epoch: 6, step: 91
	action: tensor([[ 0.0084,  0.0732,  0.0129, -0.0245,  0.0853, -0.0055,  0.0322]],
       dtype=torch.float64)
	q_value: tensor([[3.0602]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28751201044279295, distance: 0.9659295830180433 entropy -13.206888744060732
epoch: 6, step: 92
	action: tensor([[0.0084, 0.0732, 0.0344, 0.0432, 0.0854, 0.0207, 0.0322]],
       dtype=torch.float64)
	q_value: tensor([[3.0406]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3255074387487885, distance: 0.9398212823554273 entropy -13.206809923339138
epoch: 6, step: 93
	action: tensor([[0.0084, 0.0732, 0.0145, 0.0218, 0.0854, 0.0177, 0.0321]],
       dtype=torch.float64)
	q_value: tensor([[3.0118]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3141171442496845, distance: 0.9477235378426647 entropy -13.206930356286508
epoch: 6, step: 94
	action: tensor([[ 0.0084,  0.0732, -0.0073,  0.0211,  0.0854,  0.0135,  0.0322]],
       dtype=torch.float64)
	q_value: tensor([[3.0116]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3119873998132062, distance: 0.9491937921984906 entropy -13.206828611360079
epoch: 6, step: 95
	action: tensor([[ 0.0084,  0.0733,  0.0491, -0.0077,  0.0855,  0.0115,  0.0322]],
       dtype=torch.float64)
	q_value: tensor([[3.0085]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30009344651300085, distance: 0.9573631859089934 entropy -13.206794989591115
epoch: 6, step: 96
	action: tensor([[0.0084, 0.0731, 0.0457, 0.0882, 0.0853, 0.0200, 0.0321]],
       dtype=torch.float64)
	q_value: tensor([[3.0343]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3465825611737453, distance: 0.9250219829614941 entropy -13.206819387955417
epoch: 6, step: 97
	action: tensor([[ 0.0084,  0.0732,  0.0631,  0.0896,  0.0855, -0.0374,  0.0321]],
       dtype=torch.float64)
	q_value: tensor([[2.9995]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3341212606751838, distance: 0.9338008557453321 entropy -13.207078146459585
epoch: 6, step: 98
	action: tensor([[ 0.0084,  0.0732,  0.0050, -0.0199,  0.0854, -0.0247,  0.0321]],
       dtype=torch.float64)
	q_value: tensor([[3.0381]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28481643507083365, distance: 0.967755071997714 entropy -13.207180168388595
epoch: 6, step: 99
	action: tensor([[0.0084, 0.0732, 0.0543, 0.0770, 0.0854, 0.0065, 0.0322]],
       dtype=torch.float64)
	q_value: tensor([[3.0453]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3387539263823167, distance: 0.9305468565961936 entropy -13.206907568808857
epoch: 6, step: 100
	action: tensor([[ 0.0084,  0.0732,  0.0118, -0.0171,  0.0854, -0.0036,  0.0321]],
       dtype=torch.float64)
	q_value: tensor([[3.0134]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29123795464823454, distance: 0.9634006159643537 entropy -13.20710317823286
epoch: 6, step: 101
	action: tensor([[ 0.0084,  0.0732,  0.0121, -0.1096,  0.0854, -0.0230,  0.0322]],
       dtype=torch.float64)
	q_value: tensor([[3.0362]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24437410823274008, distance: 0.9947412064216206 entropy -13.206799689203889
epoch: 6, step: 102
	action: tensor([[ 0.0084,  0.0732,  0.0372, -0.0200,  0.0852,  0.0165,  0.0323]],
       dtype=torch.float64)
	q_value: tensor([[3.0798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2952968102494381, distance: 0.9606381102917858 entropy -13.206836141194584
epoch: 6, step: 103
	action: tensor([[ 0.0084,  0.0731,  0.0399, -0.0265,  0.0853,  0.0023,  0.0322]],
       dtype=torch.float64)
	q_value: tensor([[3.0339]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2886103778211918, distance: 0.9651847601145487 entropy -13.20674593445514
epoch: 6, step: 104
	action: tensor([[0.0084, 0.0731, 0.0199, 0.0176, 0.0853, 0.0458, 0.0322]],
       dtype=torch.float64)
	q_value: tensor([[3.0452]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3194403159067316, distance: 0.9440387091403994 entropy -13.206779508782352
epoch: 6, step: 105
	action: tensor([[ 0.0084,  0.0732,  0.0025,  0.0112,  0.0855, -0.0219,  0.0322]],
       dtype=torch.float64)
	q_value: tensor([[3.0000]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29934205489847965, distance: 0.9578769414255995 entropy -13.206744925083056
epoch: 6, step: 106
	action: tensor([[0.0085, 0.0733, 0.0506, 0.1062, 0.0855, 0.0356, 0.0322]],
       dtype=torch.float64)
	q_value: tensor([[3.0376]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3592825899996699, distance: 0.9159883619968452 entropy -13.20691588964306
epoch: 6, step: 107
	action: tensor([[ 0.0084,  0.0732,  0.0898,  0.0188,  0.0855, -0.0331,  0.0321]],
       dtype=torch.float64)
	q_value: tensor([[2.9891]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30216299122154044, distance: 0.9559467305982412 entropy -13.207056917536423
epoch: 6, step: 108
	action: tensor([[ 0.0084,  0.0731,  0.0350, -0.0419,  0.0852,  0.0062,  0.0320]],
       dtype=torch.float64)
	q_value: tensor([[3.0644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2823174612717998, distance: 0.9694443485687552 entropy -13.20697178497412
epoch: 6, step: 109
	action: tensor([[0.0084, 0.0731, 0.0498, 0.0473, 0.0853, 0.0279, 0.0322]],
       dtype=torch.float64)
	q_value: tensor([[3.0429]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3299506620272765, distance: 0.9367206287160618 entropy -13.206756842094027
epoch: 6, step: 110
	action: tensor([[ 0.0084,  0.0731,  0.0416,  0.0083,  0.0854, -0.0209,  0.0321]],
       dtype=torch.float64)
	q_value: tensor([[3.0076]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29905539475496346, distance: 0.958072869455755 entropy -13.206986357978463
epoch: 6, step: 111
	action: tensor([[0.0084, 0.0732, 0.0455, 0.0294, 0.0853, 0.0216, 0.0321]],
       dtype=torch.float64)
	q_value: tensor([[3.0456]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3198740451816653, distance: 0.9437378373041317 entropy -13.206934882865522
epoch: 6, step: 112
	action: tensor([[0.0084, 0.0731, 0.0198, 0.0349, 0.0854, 0.0106, 0.0321]],
       dtype=torch.float64)
	q_value: tensor([[3.0153]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3183969594910251, distance: 0.9447620781890836 entropy -13.206930386683087
epoch: 6, step: 113
	action: tensor([[ 0.0084,  0.0732, -0.0010, -0.0059,  0.0854,  0.0336,  0.0321]],
       dtype=torch.float64)
	q_value: tensor([[3.0133]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30521554678434537, distance: 0.9538536353485537 entropy -13.206907897191272
epoch: 6, step: 114
	action: tensor([[ 0.0084,  0.0732,  0.0138,  0.1393,  0.0854, -0.0164,  0.0322]],
       dtype=torch.float64)
	q_value: tensor([[3.0055]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3577474238072712, distance: 0.9170850647219497 entropy -13.20667688821222
epoch: 6, step: 115
	action: tensor([[0.0084, 0.0734, 0.0567, 0.0190, 0.0857, 0.0355, 0.0321]],
       dtype=torch.float64)
	q_value: tensor([[2.9982]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3191915893604058, distance: 0.9442112039583777 entropy -13.207081493549646
epoch: 6, step: 116
	action: tensor([[ 0.0084,  0.0731, -0.0059,  0.0275,  0.0853,  0.0593,  0.0321]],
       dtype=torch.float64)
	q_value: tensor([[3.0141]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3257859806986766, distance: 0.9396272055785826 entropy -13.206876019382083
epoch: 6, step: 117
	action: tensor([[0.0084, 0.0732, 0.0103, 0.0477, 0.0855, 0.0489, 0.0322]],
       dtype=torch.float64)
	q_value: tensor([[2.9788]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3331682854778376, distance: 0.9344688233779898 entropy -13.206655504207362
epoch: 6, step: 118
	action: tensor([[0.0084, 0.0732, 0.0483, 0.0421, 0.0855, 0.0334, 0.0321]],
       dtype=torch.float64)
	q_value: tensor([[2.9846]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3289005818726277, distance: 0.9374543408544642 entropy -13.20680813088036
epoch: 6, step: 119
	action: tensor([[ 0.0084,  0.0731,  0.0283, -0.0551,  0.0854, -0.0104,  0.0321]],
       dtype=torch.float64)
	q_value: tensor([[3.0057]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2720490641888239, distance: 0.9763549841368804 entropy -13.20695077894911
epoch: 6, step: 120
	action: tensor([[ 0.0084,  0.0731,  0.0841, -0.0376,  0.0852, -0.0051,  0.0322]],
       dtype=torch.float64)
	q_value: tensor([[3.0555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2814648115798899, distance: 0.9700200565000473 entropy -13.206808077054728
epoch: 6, step: 121
	action: tensor([[ 0.0084,  0.0730,  0.0240, -0.0099,  0.0851,  0.0043,  0.0321]],
       dtype=torch.float64)
	q_value: tensor([[3.0646]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29652736662179546, distance: 0.9597990081902565 entropy -13.206677188401732
epoch: 6, step: 122
	action: tensor([[ 0.0084,  0.0732,  0.0139, -0.1162,  0.0854,  0.0246,  0.0322]],
       dtype=torch.float64)
	q_value: tensor([[3.0300]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2530622234432419, distance: 0.9890059526383266 entropy -13.20679781766704
epoch: 6, step: 123
	action: tensor([[ 0.0084,  0.0731,  0.0115,  0.0793,  0.0852, -0.0170,  0.0322]],
       dtype=torch.float64)
	q_value: tensor([[3.0513]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33108793342717613, distance: 0.9359253454892721 entropy -13.206739169916844
epoch: 6, step: 124
	action: tensor([[0.0084, 0.0733, 0.0199, 0.0808, 0.0856, 0.0403, 0.0322]],
       dtype=torch.float64)
	q_value: tensor([[3.0174]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3464208592511725, distance: 0.9251364339780093 entropy -13.207022212037884
epoch: 6, step: 125
	action: tensor([[ 0.0084,  0.0732,  0.0187,  0.0401,  0.0856, -0.0294,  0.0321]],
       dtype=torch.float64)
	q_value: tensor([[2.9855]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31099860928403644, distance: 0.9498756233588135 entropy -13.206925837258883
epoch: 6, step: 126
	action: tensor([[0.0085, 0.0733, 0.0296, 0.0050, 0.0855, 0.0326, 0.0322]],
       dtype=torch.float64)
	q_value: tensor([[3.0384]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3108538682506038, distance: 0.9499753900366628 entropy -13.207026152062241
epoch: 6, step: 127
	action: tensor([[ 0.0084,  0.0731,  0.0210, -0.0898,  0.0854,  0.0103,  0.0321]],
       dtype=torch.float64)
	q_value: tensor([[3.0115]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26132811858921945, distance: 0.9835183720532699 entropy -13.206774590230847
LOSS epoch 6 actor 3.8728318193867333 critic 17.99912788857486 entropy 0.01
epoch: 7, step: 0
	action: tensor([[ 0.0230,  0.0473,  0.0292, -0.0104,  0.0987, -0.0175,  0.0296]],
       dtype=torch.float64)
	q_value: tensor([[4.8000]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28851801799072596, distance: 0.9652474131283236 entropy -13.176916972528351
epoch: 7, step: 1
	action: tensor([[ 0.0230,  0.0474, -0.0204,  0.0270,  0.0988,  0.0067,  0.0296]],
       dtype=torch.float64)
	q_value: tensor([[4.7870]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3103698273493015, distance: 0.9503089522581706 entropy -13.176561127922328
epoch: 7, step: 2
	action: tensor([[ 0.0230,  0.0475,  0.0875, -0.0362,  0.0991,  0.0211,  0.0296]],
       dtype=torch.float64)
	q_value: tensor([[4.7298]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2868973141399237, distance: 0.9663461692221745 entropy -13.176411779201365
epoch: 7, step: 3
	action: tensor([[ 0.0229,  0.0473,  0.0318, -0.0217,  0.0987, -0.0027,  0.0295]],
       dtype=torch.float64)
	q_value: tensor([[4.8045]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2868308575633184, distance: 0.9663911967920753 entropy -13.176528472714859
epoch: 7, step: 4
	action: tensor([[0.0230, 0.0474, 0.0625, 0.0552, 0.0989, 0.0118, 0.0296]],
       dtype=torch.float64)
	q_value: tensor([[4.7835]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3286879327982667, distance: 0.9376028531338714 entropy -13.176570814482515
epoch: 7, step: 5
	action: tensor([[0.0229, 0.0474, 0.0092, 0.1171, 0.0990, 0.0166, 0.0295]],
       dtype=torch.float64)
	q_value: tensor([[4.7676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35446859847364254, distance: 0.9194230341108383 entropy -13.176409984510832
epoch: 7, step: 6
	action: tensor([[ 0.0230,  0.0475,  0.0559, -0.0512,  0.0993, -0.0028,  0.0296]],
       dtype=torch.float64)
	q_value: tensor([[4.7141]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27291125668743244, distance: 0.975776610437169 entropy -13.176098268129866
epoch: 7, step: 7
	action: tensor([[ 0.0230,  0.0473,  0.0420,  0.0250,  0.0987, -0.0186,  0.0295]],
       dtype=torch.float64)
	q_value: tensor([[4.8087]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30527922525913476, distance: 0.9538099229884179 entropy -13.176591532823826
epoch: 7, step: 8
	action: tensor([[ 0.0230,  0.0474,  0.0346, -0.0015,  0.0989,  0.0300,  0.0295]],
       dtype=torch.float64)
	q_value: tensor([[4.7887]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3052158128104867, distance: 0.9538534527379432 entropy -13.176411820527932
epoch: 7, step: 9
	action: tensor([[ 0.0230,  0.0474,  0.0200, -0.0135,  0.0989, -0.0444,  0.0295]],
       dtype=torch.float64)
	q_value: tensor([[4.7556]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2801150434924311, distance: 0.9709307201902276 entropy -13.176490558931317
epoch: 7, step: 10
	action: tensor([[0.0230, 0.0474, 0.0626, 0.0194, 0.0989, 0.0019, 0.0296]],
       dtype=torch.float64)
	q_value: tensor([[4.8084]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3087732822407274, distance: 0.951408334267801 entropy -13.176565712459212
epoch: 7, step: 11
	action: tensor([[ 0.0229,  0.0473,  0.0583, -0.0050,  0.0989, -0.0291,  0.0295]],
       dtype=torch.float64)
	q_value: tensor([[4.7863]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2884123627711561, distance: 0.9653190801867264 entropy -13.176438267940805
epoch: 7, step: 12
	action: tensor([[ 0.0230,  0.0474,  0.0027, -0.0998,  0.0988, -0.0095,  0.0295]],
       dtype=torch.float64)
	q_value: tensor([[4.8148]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24902788013841692, distance: 0.9916732540583925 entropy -13.176496470904349
epoch: 7, step: 13
	action: tensor([[ 0.0230,  0.0474,  0.0893, -0.0080,  0.0987, -0.0205,  0.0296]],
       dtype=torch.float64)
	q_value: tensor([[4.7963]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.289768275118252, distance: 0.9643989460393112 entropy -13.176840683099554
epoch: 7, step: 14
	action: tensor([[ 0.0229,  0.0473, -0.0056, -0.0426,  0.0988, -0.0228,  0.0295]],
       dtype=torch.float64)
	q_value: tensor([[4.8258]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2720753797940091, distance: 0.9763373362411029 entropy -13.176541525791789
epoch: 7, step: 15
	action: tensor([[0.0230, 0.0474, 0.0584, 0.0346, 0.0989, 0.0647, 0.0296]],
       dtype=torch.float64)
	q_value: tensor([[4.7843]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3327994257585234, distance: 0.934727239592543 entropy -13.176723524987866
epoch: 7, step: 16
	action: tensor([[ 0.0229,  0.0473,  0.0524, -0.1407,  0.0990,  0.0168,  0.0295]],
       dtype=torch.float64)
	q_value: tensor([[4.7271]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23400074562538398, distance: 1.0015459214032836 entropy -13.176432800249327
epoch: 7, step: 17
	action: tensor([[0.0230, 0.0472, 0.0377, 0.0923, 0.0985, 0.0023, 0.0296]],
       dtype=torch.float64)
	q_value: tensor([[4.8192]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34203201797097305, distance: 0.9282374230012619 entropy -13.176818046977854
epoch: 7, step: 18
	action: tensor([[ 0.0229,  0.0474, -0.0055, -0.0313,  0.0991,  0.0380,  0.0295]],
       dtype=torch.float64)
	q_value: tensor([[4.7455]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2926496797705096, distance: 0.962440678293381 entropy -13.176341770192677
epoch: 7, step: 19
	action: tensor([[ 0.0230,  0.0474,  0.0215,  0.0295,  0.0989, -0.0108,  0.0296]],
       dtype=torch.float64)
	q_value: tensor([[4.7308]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30882506204425575, distance: 0.9513726985943097 entropy -13.176574705084255
epoch: 7, step: 20
	action: tensor([[0.0230, 0.0474, 0.0344, 0.0024, 0.0990, 0.0241, 0.0295]],
       dtype=torch.float64)
	q_value: tensor([[4.7667]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.305555426810291, distance: 0.9536203000640048 entropy -13.176422690303594
epoch: 7, step: 21
	action: tensor([[0.0230, 0.0474, 0.0468, 0.0126, 0.0989, 0.0130, 0.0295]],
       dtype=torch.float64)
	q_value: tensor([[4.7547]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3077990456799472, distance: 0.9520785705233558 entropy -13.17650320432986
epoch: 7, step: 22
	action: tensor([[ 0.0229,  0.0473, -0.0089, -0.0513,  0.0989,  0.0150,  0.0295]],
       dtype=torch.float64)
	q_value: tensor([[4.7669]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2776996728456175, distance: 0.9725581980984445 entropy -13.176460635324789
epoch: 7, step: 23
	action: tensor([[0.0230, 0.0474, 0.0187, 0.0095, 0.0989, 0.0232, 0.0296]],
       dtype=torch.float64)
	q_value: tensor([[4.7521]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30816748183490394, distance: 0.9518251565001529 entropy -13.176686880149095
epoch: 7, step: 24
	action: tensor([[ 0.0230,  0.0474,  0.0111, -0.0579,  0.0989,  0.0162,  0.0295]],
       dtype=torch.float64)
	q_value: tensor([[4.7449]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2749156252671383, distance: 0.9744307185089928 entropy -13.1764900413975
epoch: 7, step: 25
	action: tensor([[ 0.0230,  0.0474,  0.0179, -0.0600,  0.0988,  0.0101,  0.0296]],
       dtype=torch.float64)
	q_value: tensor([[4.7652]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27229748714900603, distance: 0.9761883728446794 entropy -13.176686661673886
epoch: 7, step: 26
	action: tensor([[ 0.0230,  0.0474,  0.0550, -0.0945,  0.0988,  0.0261,  0.0296]],
       dtype=torch.float64)
	q_value: tensor([[4.7738]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25927176774073923, distance: 0.9848864038483209 entropy -13.176704425367216
epoch: 7, step: 27
	action: tensor([[ 0.0230,  0.0473, -0.0014,  0.0356,  0.0987, -0.0295,  0.0295]],
       dtype=torch.float64)
	q_value: tensor([[4.7972]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30591101290337697, distance: 0.9533761210835908 entropy -13.176674281480917
epoch: 7, step: 28
	action: tensor([[ 0.0230,  0.0475, -0.0148,  0.0151,  0.0990, -0.0207,  0.0296]],
       dtype=torch.float64)
	q_value: tensor([[4.7651]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29872740448470403, distance: 0.9582969968871671 entropy -13.176354996124088
epoch: 7, step: 29
	action: tensor([[0.0230, 0.0475, 0.0102, 0.0830, 0.0990, 0.0519, 0.0296]],
       dtype=torch.float64)
	q_value: tensor([[4.7592]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3484590131993063, distance: 0.923692811722684 entropy -13.176466869997364
epoch: 7, step: 30
	action: tensor([[0.0230, 0.0475, 0.0230, 0.0633, 0.0992, 0.0189, 0.0295]],
       dtype=torch.float64)
	q_value: tensor([[4.7005]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33205113804994535, distance: 0.935251256603658 entropy -13.176187600817736
epoch: 7, step: 31
	action: tensor([[ 0.0230,  0.0474, -0.0466, -0.0063,  0.0991,  0.0495,  0.0295]],
       dtype=torch.float64)
	q_value: tensor([[4.7391]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3056118359058454, distance: 0.9535815684264225 entropy -13.176347289744575
epoch: 7, step: 32
	action: tensor([[ 0.0231,  0.0475,  0.0414,  0.0184,  0.0991, -0.0453,  0.0296]],
       dtype=torch.float64)
	q_value: tensor([[4.6972]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2954856658886724, distance: 0.9605093794419621 entropy -13.176314621459854
epoch: 7, step: 33
	action: tensor([[ 0.0230,  0.0474,  0.0776, -0.1060,  0.0989, -0.0405,  0.0296]],
       dtype=torch.float64)
	q_value: tensor([[4.8119]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23482050720192194, distance: 1.0010098577976043 entropy -13.176395379228538
epoch: 7, step: 34
	action: tensor([[0.0230, 0.0473, 0.0169, 0.0072, 0.0985, 0.0085, 0.0295]],
       dtype=torch.float64)
	q_value: tensor([[4.8690]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30303929423118614, distance: 0.9553463309811555 entropy -13.176882820318808
epoch: 7, step: 35
	action: tensor([[ 0.0230,  0.0474, -0.0003,  0.0596,  0.0990, -0.0180,  0.0296]],
       dtype=torch.float64)
	q_value: tensor([[4.7583]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3197260356979521, distance: 0.943840520160495 entropy -13.176504178446779
epoch: 7, step: 36
	action: tensor([[ 0.0230,  0.0475, -0.0183, -0.0063,  0.0991, -0.0064,  0.0296]],
       dtype=torch.float64)
	q_value: tensor([[4.7556]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29267545123698335, distance: 0.9624231454437518 entropy -13.176216856989953
