epoch: 0, step: 0
	action: tensor([[-0.0329, -0.0289,  0.0350, -0.0236, -0.0161,  0.0250,  0.0258]],
       dtype=torch.float64)
	q_value: tensor([[-0.0662]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19538884697575698, distance: 1.0264781691932245 entropy -5.682730396897044
epoch: 0, step: 1
	action: tensor([[-0.0543, -0.0267,  0.0365,  0.0227,  0.0153,  0.0387,  0.0039]],
       dtype=torch.float64)
	q_value: tensor([[-0.0287]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20070334851405847, distance: 1.023082580113321 entropy -8.026599972204812
epoch: 0, step: 2
	action: tensor([[-0.0478, -0.0268,  0.0365, -0.0098,  0.0413,  0.0325,  0.0077]],
       dtype=torch.float64)
	q_value: tensor([[-0.0275]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18693538742066118, distance: 1.0318563068908686 entropy -8.027782700416827
epoch: 0, step: 3
	action: tensor([[-0.0408, -0.0267,  0.0365, -0.0420,  0.0327,  0.0246,  0.0151]],
       dtype=torch.float64)
	q_value: tensor([[-0.0273]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1727472142601888, distance: 1.040820439893726 entropy -8.028379960247943
epoch: 0, step: 4
	action: tensor([[-0.0291, -0.0267,  0.0366,  0.0150,  0.0073,  0.0405,  0.0049]],
       dtype=torch.float64)
	q_value: tensor([[-0.0278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21488769539694974, distance: 1.0139641148988816 entropy -8.028434956842391
epoch: 0, step: 5
	action: tensor([[-0.0298, -0.0268,  0.0365,  0.0431,  0.0442,  0.0482,  0.0074]],
       dtype=torch.float64)
	q_value: tensor([[-0.0275]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2288066267555765, distance: 1.0049358461177547 entropy -8.027519685150585
epoch: 0, step: 6
	action: tensor([[-0.0369, -0.0268,  0.0365,  0.0009,  0.0061,  0.0434,  0.0054]],
       dtype=torch.float64)
	q_value: tensor([[-0.0268]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19549951726600823, distance: 1.0264075732643312 entropy -8.027544590188873
epoch: 0, step: 7
	action: tensor([[-0.0323, -0.0267,  0.0365, -0.0281, -0.0291,  0.0264,  0.0109]],
       dtype=torch.float64)
	q_value: tensor([[-0.0276]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17910298190544416, distance: 1.0368144285656629 entropy -8.027588682995082
epoch: 0, step: 8
	action: tensor([[-0.0424, -0.0267,  0.0365,  0.0065, -0.0407,  0.0244, -0.0007]],
       dtype=torch.float64)
	q_value: tensor([[-0.0285]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18469363105710834, distance: 1.0332778287173354 entropy -8.027089348262205
epoch: 0, step: 9
	action: tensor([[-0.0442, -0.0268,  0.0365,  0.0288,  0.0489,  0.0502,  0.0076]],
       dtype=torch.float64)
	q_value: tensor([[-0.0284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2014457352534743, distance: 1.0226073501830004 entropy -8.027469310438374
epoch: 0, step: 10
	action: tensor([[-0.0395, -0.0268,  0.0365,  0.0007, -0.0396,  0.0111,  0.0085]],
       dtype=torch.float64)
	q_value: tensor([[-0.0268]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17786948614763765, distance: 1.0375931048821763 entropy -8.028024723840073
epoch: 0, step: 11
	action: tensor([[-0.0294, -0.0268,  0.0365, -0.0003,  0.0131,  0.0261,  0.0412]],
       dtype=torch.float64)
	q_value: tensor([[-0.0286]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19202533256139998, distance: 1.028621424000203 entropy -8.027310338564492
epoch: 0, step: 12
	action: tensor([[-0.0317, -0.0267,  0.0365, -0.0324, -0.0107,  0.0037,  0.0355]],
       dtype=torch.float64)
	q_value: tensor([[-0.0282]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16693216839762448, distance: 1.044472177174149 entropy -8.026808005718594
epoch: 0, step: 13
	action: tensor([[-0.0268, -0.0267,  0.0365,  0.0003,  0.0737,  0.0234,  0.0122]],
       dtype=torch.float64)
	q_value: tensor([[-0.0288]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19567990835478, distance: 1.0262924924384036 entropy -8.026861008368552
epoch: 0, step: 14
	action: tensor([[-0.0397, -0.0267,  0.0365, -0.0107,  0.0555,  0.0116,  0.0430]],
       dtype=torch.float64)
	q_value: tensor([[-0.0265]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17270577977894286, distance: 1.0408465052728608 entropy -8.029186440059327
epoch: 0, step: 15
	action: tensor([[-0.0359, -0.0267,  0.0365,  0.0021,  0.0031,  0.0190,  0.0133]],
       dtype=torch.float64)
	q_value: tensor([[-0.0276]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18339315882616536, distance: 1.0341015765482704 entropy -8.028012848334901
epoch: 0, step: 16
	action: tensor([[-0.0274, -0.0267,  0.0365,  0.0081, -0.0249,  0.0317, -0.0009]],
       dtype=torch.float64)
	q_value: tensor([[-0.0278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19788550556167295, distance: 1.024884382778024 entropy -8.027955273500892
epoch: 0, step: 17
	action: tensor([[-0.0411, -0.0268,  0.0365,  0.0168, -0.0154,  0.0342, -0.0040]],
       dtype=torch.float64)
	q_value: tensor([[-0.0278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18840922543662963, distance: 1.0309206623950125 entropy -8.027495866144898
epoch: 0, step: 18
	action: tensor([[-0.0360, -0.0268,  0.0365, -0.0271, -0.0568,  0.0357, -0.0003]],
       dtype=torch.float64)
	q_value: tensor([[-0.0276]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17069291717380064, distance: 1.0421119608608531 entropy -8.027836713024945
epoch: 0, step: 19
	action: tensor([[-0.0378, -0.0267,  0.0365,  0.0449, -0.0724,  0.0156,  0.0261]],
       dtype=torch.float64)
	q_value: tensor([[-0.0288]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20054795105528345, distance: 1.0231820279880075 entropy -8.027003400833753
epoch: 0, step: 20
	action: tensor([[-0.0347, -0.0268,  0.0364,  0.0109, -0.0155,  0.0191, -0.0056]],
       dtype=torch.float64)
	q_value: tensor([[-0.0292]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18825055305185656, distance: 1.031021434021979 entropy -8.025911239833041
epoch: 0, step: 21
	action: tensor([[-0.0289, -0.0268,  0.0365,  0.0431, -0.0376,  0.0054,  0.0189]],
       dtype=torch.float64)
	q_value: tensor([[-0.0276]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20760747529812096, distance: 1.0186544299147902 entropy -8.028153412059135
epoch: 0, step: 22
	action: tensor([[-0.0358, -0.0268,  0.0364,  0.0439, -0.0116,  0.0588,  0.0030]],
       dtype=torch.float64)
	q_value: tensor([[-0.0285]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21392780270703649, distance: 1.0145837711092953 entropy -8.026896470177231
epoch: 0, step: 23
	action: tensor([[-0.0316, -0.0268,  0.0365, -0.0440, -0.0260,  0.0187,  0.0265]],
       dtype=torch.float64)
	q_value: tensor([[-0.0276]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.164145503946865, distance: 1.0462176313907274 entropy -8.026855615221825
epoch: 0, step: 24
	action: tensor([[-0.0347, -0.0267,  0.0365, -0.0160,  0.0511,  0.0256,  0.0019]],
       dtype=torch.float64)
	q_value: tensor([[-0.0290]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17842855448333672, distance: 1.0372402508693215 entropy -8.026611680421798
epoch: 0, step: 25
	action: tensor([[-0.0294, -0.0267,  0.0365, -0.0308,  0.0274,  0.0304,  0.0056]],
       dtype=torch.float64)
	q_value: tensor([[-0.0268]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17736002875636903, distance: 1.0379145426592453 entropy -8.029226339053624
epoch: 0, step: 26
	action: tensor([[-0.0310, -0.0267,  0.0366, -0.0136,  0.0062,  0.0040,  0.0072]],
       dtype=torch.float64)
	q_value: tensor([[-0.0274]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17710106245535717, distance: 1.0380778970726547 entropy -8.028782931396828
epoch: 0, step: 27
	action: tensor([[-0.0445, -0.0267,  0.0365,  0.0172,  0.0554,  0.0304,  0.0435]],
       dtype=torch.float64)
	q_value: tensor([[-0.0277]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18658460852503966, distance: 1.032078868775481 entropy -8.028403306891043
epoch: 0, step: 28
	action: tensor([[-0.0402, -0.0267,  0.0365,  0.0183, -0.0503,  0.0300,  0.0206]],
       dtype=torch.float64)
	q_value: tensor([[-0.0274]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1880851752027246, distance: 1.0311264537569982 entropy -8.027538783940761
epoch: 0, step: 29
	action: tensor([[-0.0364, -0.0268,  0.0365,  0.0284, -0.0434, -0.0003,  0.0123]],
       dtype=torch.float64)
	q_value: tensor([[-0.0288]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19037824049650265, distance: 1.0296693353330988 entropy -8.026393865428336
epoch: 0, step: 30
	action: tensor([[-0.0214, -0.0268,  0.0364, -0.0114,  0.0178, -0.0043,  0.0062]],
       dtype=torch.float64)
	q_value: tensor([[-0.0286]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1867809768886557, distance: 1.0319542830633142 entropy -8.027233340335103
epoch: 0, step: 31
	action: tensor([[-0.0204, -0.0267,  0.0365,  0.0210, -0.0487,  0.0114,  0.0294]],
       dtype=torch.float64)
	q_value: tensor([[-0.0275]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20722955121205144, distance: 1.0188973197332585 entropy -8.028809267974149
epoch: 0, step: 32
	action: tensor([[-0.0361, -0.0272,  0.0350, -0.0167, -0.0559,  0.0120,  0.0237]],
       dtype=torch.float64)
	q_value: tensor([[-0.0663]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19441833371340445, distance: 1.0270970460565247 entropy -5.682631838418176
epoch: 0, step: 33
	action: tensor([[-0.0335, -0.0267,  0.0365,  0.0041, -0.0121,  0.0410, -0.0185]],
       dtype=torch.float64)
	q_value: tensor([[-0.0293]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21473074463859898, distance: 1.0140654599441365 entropy -8.026310141400247
epoch: 0, step: 34
	action: tensor([[-0.0314, -0.0267,  0.0365, -0.0253,  0.0041,  0.0424,  0.0102]],
       dtype=torch.float64)
	q_value: tensor([[-0.0273]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20141195851045923, distance: 1.022628976753644 entropy -8.028355482411705
epoch: 0, step: 35
	action: tensor([[-0.0262, -0.0267,  0.0365,  0.0158, -0.0223,  0.0112,  0.0042]],
       dtype=torch.float64)
	q_value: tensor([[-0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21791342915887157, distance: 1.0120083774429502 entropy -8.02753022595782
epoch: 0, step: 36
	action: tensor([[-0.0289, -0.0268,  0.0365, -0.0181, -0.0124,  0.0592, -0.0019]],
       dtype=torch.float64)
	q_value: tensor([[-0.0280]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2091175760998809, distance: 1.0176833174851174 entropy -8.027506049804932
epoch: 0, step: 37
	action: tensor([[-0.0393, -0.0267,  0.0365, -0.0498,  0.0656,  0.0192,  0.0060]],
       dtype=torch.float64)
	q_value: tensor([[-0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1709775038275072, distance: 1.0419331389206918 entropy -8.027402433114391
epoch: 0, step: 38
	action: tensor([[-0.0219, -0.0267,  0.0366, -0.0309,  0.0625,  0.0330,  0.0112]],
       dtype=torch.float64)
	q_value: tensor([[-0.0271]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19873933575252734, distance: 1.0243387560423574 entropy -8.029446533207656
epoch: 0, step: 39
	action: tensor([[-0.0106, -0.0267,  0.0366, -0.0162, -0.0251,  0.0049, -0.0017]],
       dtype=torch.float64)
	q_value: tensor([[-0.0270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2064342359504232, distance: 1.0194082756190899 entropy -8.02900974395301
epoch: 0, step: 40
	action: tensor([[-0.0304, -0.0267,  0.0365, -0.0010, -0.0439,  0.0182,  0.0246]],
       dtype=torch.float64)
	q_value: tensor([[-0.0280]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.196772604794481, distance: 1.0255951287288911 entropy -8.02805117117909
epoch: 0, step: 41
	action: tensor([[-0.0306, -0.0267,  0.0365, -0.0310,  0.0059,  0.0604,  0.0047]],
       dtype=torch.float64)
	q_value: tensor([[-0.0288]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19128499512924713, distance: 1.0290925727934592 entropy -8.026323445143728
epoch: 0, step: 42
	action: tensor([[-0.0290, -0.0267,  0.0366, -0.0049,  0.0173,  0.0222, -0.0047]],
       dtype=torch.float64)
	q_value: tensor([[-0.0277]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19470453352937378, distance: 1.0269145809440927 entropy -8.027793002915985
epoch: 0, step: 43
	action: tensor([[-0.0292, -0.0267,  0.0365, -0.0004,  0.0244,  0.0263,  0.0104]],
       dtype=torch.float64)
	q_value: tensor([[-0.0272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19607165192590004, distance: 1.0260425344578328 entropy -8.028773447488845
epoch: 0, step: 44
	action: tensor([[-0.0367, -0.0267,  0.0365, -0.0678, -0.0144,  0.0109,  0.0226]],
       dtype=torch.float64)
	q_value: tensor([[-0.0273]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14734688003427998, distance: 1.0566785365368114 entropy -8.028317855940587
epoch: 0, step: 45
	action: tensor([[-0.0409, -0.0266,  0.0365,  0.0007, -0.0165,  0.0209, -0.0033]],
       dtype=torch.float64)
	q_value: tensor([[-0.0290]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17841488398920868, distance: 1.0372488803852642 entropy -8.027534738646702
epoch: 0, step: 46
	action: tensor([[-0.0418, -0.0267,  0.0365, -0.0159, -0.0415,  0.0397,  0.0297]],
       dtype=torch.float64)
	q_value: tensor([[-0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1726133637643542, distance: 1.0409046394875534 entropy -8.028093709743022
epoch: 0, step: 47
	action: tensor([[-0.0313, -0.0267,  0.0365,  0.0236,  0.0249,  0.0136, -0.0098]],
       dtype=torch.float64)
	q_value: tensor([[-0.0291]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.198006903329229, distance: 1.024806823162224 entropy -8.026085640749004
epoch: 0, step: 48
	action: tensor([[-0.0311, -0.0268,  0.0365,  0.0042,  0.0039,  0.0569,  0.0103]],
       dtype=torch.float64)
	q_value: tensor([[-0.0268]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19932794366360485, distance: 1.0239624461271317 entropy -8.028917465041975
epoch: 0, step: 49
	action: tensor([[-0.0267, -0.0267,  0.0365,  0.0129,  0.0122,  0.0229,  0.0077]],
       dtype=torch.float64)
	q_value: tensor([[-0.0276]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20012857848392818, distance: 1.0234503606668106 entropy -8.02727177945366
epoch: 0, step: 50
	action: tensor([[-0.0403, -0.0268,  0.0365,  0.0304,  0.0150,  0.0179, -0.0103]],
       dtype=torch.float64)
	q_value: tensor([[-0.0274]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1923429940265895, distance: 1.0284191989029563 entropy -8.028085112572466
epoch: 0, step: 51
	action: tensor([[-0.0310, -0.0268,  0.0365,  0.0305,  0.0779,  0.0286,  0.0245]],
       dtype=torch.float64)
	q_value: tensor([[-0.0270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20808624715862722, distance: 1.018346642579279 entropy -8.028527823917189
epoch: 0, step: 52
	action: tensor([[-0.0491, -0.0268,  0.0365,  0.0298, -0.0176,  0.0430,  0.0304]],
       dtype=torch.float64)
	q_value: tensor([[-0.0265]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1883892707491044, distance: 1.030933336007058 entropy -8.028432064369413
epoch: 0, step: 53
	action: tensor([[-0.0364, -0.0268,  0.0365, -0.0099, -0.0055,  0.0263,  0.0080]],
       dtype=torch.float64)
	q_value: tensor([[-0.0284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17824496174492577, distance: 1.037356138015632 entropy -8.026558362406794
epoch: 0, step: 54
	action: tensor([[-0.0238, -0.0267,  0.0365, -0.0234,  0.0266,  0.0303,  0.0018]],
       dtype=torch.float64)
	q_value: tensor([[-0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18713960853814282, distance: 1.0317267107439085 entropy -8.027805552387319
epoch: 0, step: 55
	action: tensor([[-0.0503, -0.0267,  0.0366, -0.0052,  0.0246, -0.0022,  0.0049]],
       dtype=torch.float64)
	q_value: tensor([[-0.0272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15887925147319615, distance: 1.0495082729710479 entropy -8.028856787066598
epoch: 0, step: 56
	action: tensor([[-0.0403, -0.0267,  0.0365, -0.0111,  0.0355,  0.0288,  0.0245]],
       dtype=torch.float64)
	q_value: tensor([[-0.0274]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1755395585884083, distance: 1.0390623402010575 entropy -8.028908033993758
epoch: 0, step: 57
	action: tensor([[-0.0289, -0.0267,  0.0365,  0.0276,  0.0106,  0.0439,  0.0121]],
       dtype=torch.float64)
	q_value: tensor([[-0.0275]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21062098480598124, distance: 1.0167155871965134 entropy -8.02807010082962
epoch: 0, step: 58
	action: tensor([[-0.0395, -0.0268,  0.0365,  0.0110,  0.0216,  0.0278,  0.0354]],
       dtype=torch.float64)
	q_value: tensor([[-0.0274]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18714681307756387, distance: 1.0317221385367326 entropy -8.027327665098147
epoch: 0, step: 59
	action: tensor([[-0.0337, -0.0268,  0.0365,  0.0013, -0.0023,  0.0028,  0.0425]],
       dtype=torch.float64)
	q_value: tensor([[-0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1818410629144641, distance: 1.0350838502806872 entropy -8.02729036958472
epoch: 0, step: 60
	action: tensor([[-0.0252, -0.0267,  0.0365,  0.0169, -0.0786,  0.0098,  0.0226]],
       dtype=torch.float64)
	q_value: tensor([[-0.0285]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19868953665772782, distance: 1.024370587350521 entropy -8.026869323019076
epoch: 0, step: 61
	action: tensor([[-0.0208, -0.0268,  0.0364,  0.0070, -0.0230,  0.0260, -0.0063]],
       dtype=torch.float64)
	q_value: tensor([[-0.0292]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2031072240362174, distance: 1.0215429670801637 entropy -8.025767924610054
epoch: 0, step: 62
	action: tensor([[-0.0415, -0.0268,  0.0365, -0.0416,  0.0036,  0.0083,  0.0204]],
       dtype=torch.float64)
	q_value: tensor([[-0.0276]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15290332904111736, distance: 1.0532299024985436 entropy -8.02778997982872
epoch: 0, step: 63
	action: tensor([[-0.0476, -0.0267,  0.0365,  0.0115,  0.0170,  0.0272,  0.0092]],
       dtype=torch.float64)
	q_value: tensor([[-0.0284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1775937620864796, distance: 1.0377670829774572 entropy -8.027827834503032
epoch: 0, step: 64
	action: tensor([[-0.0440, -0.0273,  0.0350,  0.0086,  0.0052,  0.0106,  0.0045]],
       dtype=torch.float64)
	q_value: tensor([[-0.0663]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17065550141761787, distance: 1.0421354690208986 entropy -5.682631838418176
epoch: 0, step: 65
	action: tensor([[-0.0393, -0.0268,  0.0365,  0.0026,  0.0037,  0.0439,  0.0321]],
       dtype=torch.float64)
	q_value: tensor([[-0.0276]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17979097598834726, distance: 1.03637986022289 entropy -8.028498334244569
epoch: 0, step: 66
	action: tensor([[-0.0284, -0.0267,  0.0365, -0.0249,  0.0218,  0.0074, -0.0399]],
       dtype=torch.float64)
	q_value: tensor([[-0.0282]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16722371856896112, distance: 1.0442893933249102 entropy -8.026758040884848
epoch: 0, step: 67
	action: tensor([[-0.0338, -0.0267,  0.0365,  0.0043, -0.0031,  0.0118,  0.0239]],
       dtype=torch.float64)
	q_value: tensor([[-0.0267]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17785552496767687, distance: 1.0376019148961098 entropy -8.030547362063425
epoch: 0, step: 68
	action: tensor([[-0.0254, -0.0268,  0.0365, -0.0192,  0.0149,  0.0149,  0.0082]],
       dtype=torch.float64)
	q_value: tensor([[-0.0281]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1760839223530235, distance: 1.038719254434952 entropy -8.027532952495578
epoch: 0, step: 69
	action: tensor([[-0.0274, -0.0267,  0.0365, -0.0170, -0.0335,  0.0154,  0.0177]],
       dtype=torch.float64)
	q_value: tensor([[-0.0277]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17407521774872092, distance: 1.0399846806388473 entropy -8.028316535671573
epoch: 0, step: 70
	action: tensor([[-0.0392, -0.0267,  0.0365, -0.0015,  0.0325,  0.0245,  0.0322]],
       dtype=torch.float64)
	q_value: tensor([[-0.0288]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17328247162014343, distance: 1.040483664390629 entropy -8.026542479266878
epoch: 0, step: 71
	action: tensor([[-0.0273, -0.0267,  0.0365, -0.0061, -0.0003,  0.0211,  0.0072]],
       dtype=torch.float64)
	q_value: tensor([[-0.0278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18175238063411148, distance: 1.0351399464160094 entropy -8.027548997731618
epoch: 0, step: 72
	action: tensor([[-0.0236, -0.0267,  0.0365, -0.0274, -0.0247,  0.0258,  0.0253]],
       dtype=torch.float64)
	q_value: tensor([[-0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17590905261903644, distance: 1.0388294785995933 entropy -8.027792388513697
epoch: 0, step: 73
	action: tensor([[-0.0283, -0.0267,  0.0365,  0.0146,  0.0237,  0.0192, -0.0055]],
       dtype=torch.float64)
	q_value: tensor([[-0.0288]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19112399776015365, distance: 1.0291950025435725 entropy -8.026331956479082
epoch: 0, step: 74
	action: tensor([[-0.0338, -0.0268,  0.0365, -0.0590,  0.0437,  0.0463,  0.0329]],
       dtype=torch.float64)
	q_value: tensor([[-0.0270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15617011811485226, distance: 1.0511970745621524 entropy -8.028525027849552
epoch: 0, step: 75
	action: tensor([[-0.0407, -0.0266,  0.0366, -0.0260, -0.0335,  0.0390,  0.0064]],
       dtype=torch.float64)
	q_value: tensor([[-0.0281]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16035212741167348, distance: 1.048588980001471 entropy -8.027685070246957
epoch: 0, step: 76
	action: tensor([[-0.0234, -0.0267,  0.0365, -0.0032, -0.0382,  0.0190,  0.0221]],
       dtype=torch.float64)
	q_value: tensor([[-0.0287]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1863302703858173, distance: 1.0322402110033442 entropy -8.026912043270212
epoch: 0, step: 77
	action: tensor([[-0.0381, -0.0267,  0.0365, -0.0141,  0.0510,  0.0092,  0.0035]],
       dtype=torch.float64)
	q_value: tensor([[-0.0288]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1638853794106735, distance: 1.046380414331499 entropy -8.026160818119742
epoch: 0, step: 78
	action: tensor([[-0.0347, -0.0267,  0.0365, -0.0329, -0.0153,  0.0230,  0.0217]],
       dtype=torch.float64)
	q_value: tensor([[-0.0270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16035257040410922, distance: 1.0485887033872718 entropy -8.029279161906059
epoch: 0, step: 79
	action: tensor([[-0.0496, -0.0267,  0.0365,  0.0110, -0.1106,  0.0315,  0.0254]],
       dtype=torch.float64)
	q_value: tensor([[-0.0285]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.166662196691724, distance: 1.044641404146638 entropy -8.027059659072842
epoch: 0, step: 80
	action: tensor([[-0.0339, -0.0268,  0.0364,  0.0252,  0.0263,  0.0105,  0.0264]],
       dtype=torch.float64)
	q_value: tensor([[-0.0304]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18861406295479488, distance: 1.030790557074329 entropy -8.025765762120658
epoch: 0, step: 81
	action: tensor([[-0.0461, -0.0268,  0.0365, -0.0568,  0.0024, -0.0025,  0.0308]],
       dtype=torch.float64)
	q_value: tensor([[-0.0277]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1297853415798481, distance: 1.0675049328460393 entropy -8.027798339798293
epoch: 0, step: 82
	action: tensor([[-0.0352, -0.0267,  0.0365,  0.0181, -0.0050,  0.0152,  0.0259]],
       dtype=torch.float64)
	q_value: tensor([[-0.0289]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18411786387565454, distance: 1.0336426133484766 entropy -8.027625001527433
epoch: 0, step: 83
	action: tensor([[-0.0296, -0.0268,  0.0365, -0.0050, -0.0412,  0.0530,  0.0289]],
       dtype=torch.float64)
	q_value: tensor([[-0.0281]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18679411729349304, distance: 1.0319459456096598 entropy -8.02731940401886
epoch: 0, step: 84
	action: tensor([[-0.0346, -0.0267,  0.0365, -0.0139, -0.0207,  0.0198,  0.0291]],
       dtype=torch.float64)
	q_value: tensor([[-0.0289]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16936323888218596, distance: 1.042947066873859 entropy -8.025748005969213
epoch: 0, step: 85
	action: tensor([[-0.0380, -0.0267,  0.0365, -0.0160,  0.0303,  0.0242,  0.0283]],
       dtype=torch.float64)
	q_value: tensor([[-0.0288]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1669805249688917, distance: 1.0444418628201937 entropy -8.026397560901163
epoch: 0, step: 86
	action: tensor([[-0.0418, -0.0267,  0.0365, -0.0497,  0.0274,  0.0390,  0.0158]],
       dtype=torch.float64)
	q_value: tensor([[-0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14915807273178983, distance: 1.055555649733886 entropy -8.027695966782584
epoch: 0, step: 87
	action: tensor([[-0.0361, -0.0267,  0.0366,  0.0124,  0.0402,  0.0130, -0.0029]],
       dtype=torch.float64)
	q_value: tensor([[-0.0280]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1802297085508725, distance: 1.0361026416121077 entropy -8.028048519733806
epoch: 0, step: 88
	action: tensor([[-0.0314, -0.0268,  0.0365,  0.0142,  0.0183,  0.0204,  0.0205]],
       dtype=torch.float64)
	q_value: tensor([[-0.0269]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18813378716693074, distance: 1.0310955848585366 entropy -8.02895336096213
epoch: 0, step: 89
	action: tensor([[-0.0387, -0.0268,  0.0365,  0.0092,  0.0279,  0.0226,  0.0349]],
       dtype=torch.float64)
	q_value: tensor([[-0.0277]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1786290719791399, distance: 1.0371136657069293 entropy -8.02766713224562
epoch: 0, step: 90
	action: tensor([[-0.0313, -0.0267,  0.0365,  0.0051,  0.0024,  0.0369, -0.0140]],
       dtype=torch.float64)
	q_value: tensor([[-0.0278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18662081723241974, distance: 1.032055897328154 entropy -8.027556152019786
epoch: 0, step: 91
	action: tensor([[-0.0156, -0.0267,  0.0365, -0.0223,  0.0051,  0.0407,  0.0083]],
       dtype=torch.float64)
	q_value: tensor([[-0.0272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19157228876147203, distance: 1.0289097654983237 entropy -8.028256567354038
epoch: 0, step: 92
	action: tensor([[-0.0508, -0.0267,  0.0366, -0.0902, -0.0310,  0.0399,  0.0011]],
       dtype=torch.float64)
	q_value: tensor([[-0.0278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11873309641928032, distance: 1.0742625188559058 entropy -8.02761220108568
epoch: 0, step: 93
	action: tensor([[-0.0378, -0.0266,  0.0365, -0.0089,  0.0386, -0.0030,  0.0218]],
       dtype=torch.float64)
	q_value: tensor([[-0.0294]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1639220765341698, distance: 1.0463574512283025 entropy -8.027735213690713
epoch: 0, step: 94
	action: tensor([[-0.0364, -0.0267,  0.0365,  0.0167,  0.0011,  0.0195,  0.0055]],
       dtype=torch.float64)
	q_value: tensor([[-0.0276]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18274835432291747, distance: 1.03450976672272 entropy -8.028461619063135
epoch: 0, step: 95
	action: tensor([[-0.0486, -0.0268,  0.0365, -0.0253, -0.0516,  0.0083,  0.0081]],
       dtype=torch.float64)
	q_value: tensor([[-0.0277]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14446735223897156, distance: 1.0584613075954632 entropy -8.027892385177868
epoch: 0, step: 96
	action: tensor([[-0.0201, -0.0249,  0.0350, -0.0219, -0.0439,  0.0413,  0.0313]],
       dtype=torch.float64)
	q_value: tensor([[-0.0663]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.216052616858753, distance: 1.0132115938718431 entropy -5.682631838418176
epoch: 0, step: 97
	action: tensor([[-0.0247, -0.0267,  0.0365, -0.0082, -0.0058,  0.0340, -0.0104]],
       dtype=torch.float64)
	q_value: tensor([[-0.0291]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21443892611145277, distance: 1.0142538638575762 entropy -8.025330772965185
epoch: 0, step: 98
	action: tensor([[-0.0334, -0.0267,  0.0365, -0.0390, -0.0382,  0.0120,  0.0200]],
       dtype=torch.float64)
	q_value: tensor([[-0.0274]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18367855906496477, distance: 1.0339208539450553 entropy -8.028371311658178
epoch: 0, step: 99
	action: tensor([[-0.0404, -0.0267,  0.0365, -0.0149,  0.0395,  0.0617,  0.0003]],
       dtype=torch.float64)
	q_value: tensor([[-0.0290]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20067227311649793, distance: 1.0231024678413934 entropy -8.026851202902089
epoch: 0, step: 100
	action: tensor([[-0.0319, -0.0267,  0.0366, -0.0254,  0.0036,  0.0288,  0.0087]],
       dtype=torch.float64)
	q_value: tensor([[-0.0270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19254083185760984, distance: 1.028293234116389 entropy -8.028280517162752
epoch: 0, step: 101
	action: tensor([[-0.0371, -0.0267,  0.0365, -0.0197,  0.0122,  0.0116,  0.0195]],
       dtype=torch.float64)
	q_value: tensor([[-0.0278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18398368106634444, distance: 1.033727608080779 entropy -8.028078328628473
epoch: 0, step: 102
	action: tensor([[-0.0426, -0.0267,  0.0365, -0.0313, -0.0049,  0.0129,  0.0271]],
       dtype=torch.float64)
	q_value: tensor([[-0.0280]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17036177417911658, distance: 1.0423199981735987 entropy -8.028028685029554
epoch: 0, step: 103
	action: tensor([[-0.0525, -0.0267,  0.0365, -0.0167,  0.0369,  0.0071,  0.0069]],
       dtype=torch.float64)
	q_value: tensor([[-0.0286]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16406724996128674, distance: 1.0462666044977094 entropy -8.027233747552847
epoch: 0, step: 104
	action: tensor([[-0.0265, -0.0267,  0.0365, -0.0207,  0.0173,  0.0345,  0.0252]],
       dtype=torch.float64)
	q_value: tensor([[-0.0273]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19295264300354986, distance: 1.0280309814614046 entropy -8.0289823437339
epoch: 0, step: 105
	action: tensor([[-0.0338, -0.0267,  0.0366,  0.0070,  0.0364,  0.0403,  0.0103]],
       dtype=torch.float64)
	q_value: tensor([[-0.0279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1986849077437225, distance: 1.0243735460767123 entropy -8.02759339240856
epoch: 0, step: 106
	action: tensor([[-0.0341, -0.0267,  0.0365,  0.0476, -0.0299,  0.0221,  0.0096]],
       dtype=torch.float64)
	q_value: tensor([[-0.0271]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21057746697487245, distance: 1.0167436121671378 entropy -8.02811317902968
epoch: 0, step: 107
	action: tensor([[-0.0445, -0.0268,  0.0364,  0.0365,  0.0281,  0.0290,  0.0127]],
       dtype=torch.float64)
	q_value: tensor([[-0.0280]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19584247277729316, distance: 1.0261887731113233 entropy -8.027177194276563
epoch: 0, step: 108
	action: tensor([[-0.0227, -0.0268,  0.0365, -0.0237,  0.0140,  0.0330,  0.0253]],
       dtype=torch.float64)
	q_value: tensor([[-0.0272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18898959670956383, distance: 1.0305519890692918 entropy -8.028071343948696
epoch: 0, step: 109
	action: tensor([[-0.0187, -0.0267,  0.0366,  0.0073, -0.0399,  0.0130,  0.0166]],
       dtype=torch.float64)
	q_value: tensor([[-0.0280]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2025546221375415, distance: 1.0218970980062163 entropy -8.027501574563278
epoch: 0, step: 110
	action: tensor([[-0.0307, -0.0268,  0.0365, -0.0686, -0.0289,  0.0468,  0.0033]],
       dtype=torch.float64)
	q_value: tensor([[-0.0286]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15947975762526168, distance: 1.049133565354532 entropy -8.026368300357092
epoch: 0, step: 111
	action: tensor([[-0.0479, -0.0266,  0.0365,  0.0039, -0.0192,  0.0229,  0.0179]],
       dtype=torch.float64)
	q_value: tensor([[-0.0288]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17226782020055964, distance: 1.041121974610875 entropy -8.027674383549236
epoch: 0, step: 112
	action: tensor([[-0.0224, -0.0267,  0.0365,  0.0205, -0.0513,  0.0188,  0.0169]],
       dtype=torch.float64)
	q_value: tensor([[-0.0284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20611970269752633, distance: 1.0196102790636887 entropy -8.027306284526821
epoch: 0, step: 113
	action: tensor([[-0.0377, -0.0268,  0.0365, -0.0237,  0.0513,  0.0432,  0.0475]],
       dtype=torch.float64)
	q_value: tensor([[-0.0287]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17687354214091944, distance: 1.0382213943331535 entropy -8.025956237803884
epoch: 0, step: 114
	action: tensor([[-0.0352, -0.0267,  0.0366, -0.0400, -0.0305,  0.0451, -0.0097]],
       dtype=torch.float64)
	q_value: tensor([[-0.0280]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16778556760456986, distance: 1.0439370586116261 entropy -8.026972590536221
epoch: 0, step: 115
	action: tensor([[-0.0311, -0.0267,  0.0365, -0.0212,  0.0177,  0.0298,  0.0064]],
       dtype=torch.float64)
	q_value: tensor([[-0.0284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18000381248032526, distance: 1.0362453861078922 entropy -8.027643216159008
epoch: 0, step: 116
	action: tensor([[-0.0320, -0.0267,  0.0365, -0.0351,  0.0082,  0.0448,  0.0364]],
       dtype=torch.float64)
	q_value: tensor([[-0.0275]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1761391427355169, distance: 1.038684445406952 entropy -8.02842799787623
epoch: 0, step: 117
	action: tensor([[-0.0145, -0.0267,  0.0366,  0.0395, -0.0316,  0.0236,  0.0381]],
       dtype=torch.float64)
	q_value: tensor([[-0.0284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22661048245444948, distance: 1.0063657178087573 entropy -8.026758425315812
epoch: 0, step: 118
	action: tensor([[-0.0502, -0.0268,  0.0365, -0.0413, -0.0533,  0.0378,  0.0004]],
       dtype=torch.float64)
	q_value: tensor([[-0.0286]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15071652627980436, distance: 1.0545884968635724 entropy -8.025458215359944
epoch: 0, step: 119
	action: tensor([[-0.0159, -0.0267,  0.0365,  0.0196, -0.0142,  0.0184,  0.0246]],
       dtype=torch.float64)
	q_value: tensor([[-0.0291]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2139273802635635, distance: 1.0145840437332578 entropy -8.0271244781383
epoch: 0, step: 120
	action: tensor([[-0.0342, -0.0268,  0.0365,  0.0351,  0.0020,  0.0167, -0.0110]],
       dtype=torch.float64)
	q_value: tensor([[-0.0281]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20113311723568594, distance: 1.0228074957556976 entropy -8.026715937445697
epoch: 0, step: 121
	action: tensor([[-0.0228, -0.0268,  0.0365, -0.0229, -0.0125,  0.0350,  0.0054]],
       dtype=torch.float64)
	q_value: tensor([[-0.0271]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18856183104078328, distance: 1.0308237344408806 entropy -8.028472789627193
epoch: 0, step: 122
	action: tensor([[-0.0435, -0.0267,  0.0365,  0.0427,  0.0676,  0.0293, -0.0025]],
       dtype=torch.float64)
	q_value: tensor([[-0.0280]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19962042405965585, distance: 1.0237754055686652 entropy -8.027650657691055
epoch: 0, step: 123
	action: tensor([[-0.0387, -0.0268,  0.0365,  0.0211, -0.0336,  0.0119,  0.0318]],
       dtype=torch.float64)
	q_value: tensor([[-0.0261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18753781891252863, distance: 1.0314739646307431 entropy -8.029028208835571
epoch: 0, step: 124
	action: tensor([[-0.0299, -0.0268,  0.0365, -0.0088,  0.0153,  0.0106,  0.0116]],
       dtype=torch.float64)
	q_value: tensor([[-0.0288]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1826602075849153, distance: 1.0345655550488355 entropy -8.026570200016744
epoch: 0, step: 125
	action: tensor([[-0.0268, -0.0267,  0.0365, -0.0380, -0.0032,  0.0051, -0.0123]],
       dtype=torch.float64)
	q_value: tensor([[-0.0276]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16890536794136302, distance: 1.0432344785244336 entropy -8.028436076397469
epoch: 0, step: 126
	action: tensor([[-0.0224, -0.0267,  0.0365, -0.0128,  0.0422,  0.0272,  0.0264]],
       dtype=torch.float64)
	q_value: tensor([[-0.0278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19431677151429183, distance: 1.0271617886837883 entropy -8.029089770353385
epoch: 0, step: 127
	action: tensor([[-0.0275, -0.0267,  0.0366, -0.0286, -0.0194,  0.0568,  0.0202]],
       dtype=torch.float64)
	q_value: tensor([[-0.0274]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18609721130554713, distance: 1.0323880324849006 entropy -8.028152377303332
LOSS epoch 0 actor 0.1682794566464874 critic 3.738170466904651 entropy 0.01
epoch: 1, step: 0
	action: tensor([[-0.0261, -0.0039, -0.1486, -0.0377, -0.0244,  0.0023, -0.0363]],
       dtype=torch.float64)
	q_value: tensor([[0.2869]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22007874728750076, distance: 1.0106064617110166 entropy -5.491971707361256
epoch: 1, step: 1
	action: tensor([[-0.0124, -0.0211, -0.0760, -0.0330, -0.0059,  0.0081, -0.0026]],
       dtype=torch.float64)
	q_value: tensor([[0.1451]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22632431330688374, distance: 1.006551888022763 entropy -8.019964118949247
epoch: 1, step: 2
	action: tensor([[-0.0278, -0.0212, -0.0756, -0.0093, -0.0052, -0.0196,  0.0175]],
       dtype=torch.float64)
	q_value: tensor([[0.1432]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21746459116634653, distance: 1.0122987306302176 entropy -8.015829166570688
epoch: 1, step: 3
	action: tensor([[-0.0364, -0.0213, -0.0755, -0.0204, -0.0416, -0.0171,  0.0008]],
       dtype=torch.float64)
	q_value: tensor([[0.1443]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2080017700798349, distance: 1.018400956988616 entropy -8.01569148144055
epoch: 1, step: 4
	action: tensor([[-0.0325, -0.0212, -0.0757, -0.0201,  0.0417, -0.0233, -0.0093]],
       dtype=torch.float64)
	q_value: tensor([[0.1440]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21152786015135006, distance: 1.0161313942722445 entropy -8.016644138197625
epoch: 1, step: 5
	action: tensor([[-2.0791e-02, -2.1135e-02, -7.5502e-02, -1.4720e-02, -3.7819e-06,
          3.3650e-02, -2.2469e-02]], dtype=torch.float64)
	q_value: tensor([[0.1457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24068233112873738, distance: 0.9971682594379513 entropy -8.016584843960356
epoch: 1, step: 6
	action: tensor([[-0.0188, -0.0212, -0.0756, -0.0289,  0.0323, -0.0190,  0.0393]],
       dtype=torch.float64)
	q_value: tensor([[0.1444]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22481197547001552, distance: 1.0075351833213335 entropy -8.01498669930503
epoch: 1, step: 7
	action: tensor([[-0.0376, -0.0212, -0.0755, -0.0162, -0.0299, -0.0052, -0.0305]],
       dtype=torch.float64)
	q_value: tensor([[0.1435]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21738041628789306, distance: 1.0123531740651883 entropy -8.01579510824803
epoch: 1, step: 8
	action: tensor([[-0.0176, -0.0212, -0.0757, -0.0377,  0.0038, -0.0235, -0.0193]],
       dtype=torch.float64)
	q_value: tensor([[0.1451]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22473929238024948, distance: 1.0075824164127716 entropy -8.016494862156764
epoch: 1, step: 9
	action: tensor([[-0.0161, -0.0212, -0.0756, -0.0320,  0.0284,  0.0260,  0.0078]],
       dtype=torch.float64)
	q_value: tensor([[0.1445]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23875953656694593, distance: 0.9984300089726886 entropy -8.01729477290651
epoch: 1, step: 10
	action: tensor([[-0.0253, -0.0212, -0.0756, -0.0318,  0.0384,  0.0106,  0.0338]],
       dtype=torch.float64)
	q_value: tensor([[0.1437]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22495363710004224, distance: 1.0074431181750663 entropy -8.01528973255652
epoch: 1, step: 11
	action: tensor([[-0.0259, -0.0212, -0.0756, -0.0277,  0.0250,  0.0309,  0.0122]],
       dtype=torch.float64)
	q_value: tensor([[0.1436]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23125166070899394, distance: 1.003341529454395 entropy -8.015385783773946
epoch: 1, step: 12
	action: tensor([[-0.0440, -0.0212, -0.0757, -0.0327,  0.0300,  0.0065,  0.0036]],
       dtype=torch.float64)
	q_value: tensor([[0.1438]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20433492173110201, distance: 1.0207557662218114 entropy -8.015129735427388
epoch: 1, step: 13
	action: tensor([[-0.0315, -0.0211, -0.0757, -0.0094,  0.0162,  0.0236,  0.0297]],
       dtype=torch.float64)
	q_value: tensor([[0.1448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2311774798810926, distance: 1.0033899373052448 entropy -8.016412210019261
epoch: 1, step: 14
	action: tensor([[-0.0212, -0.0212, -0.0756,  0.0023, -0.0801,  0.0028, -0.0037]],
       dtype=torch.float64)
	q_value: tensor([[0.1441]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24484958314319827, distance: 0.9944281884857584 entropy -8.01449354294617
epoch: 1, step: 15
	action: tensor([[-0.0209, -0.0214, -0.0757, -0.0103,  0.0628, -0.0197,  0.0098]],
       dtype=torch.float64)
	q_value: tensor([[0.1437]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23180784526570342, distance: 1.0029785081592935 entropy -8.015276957832352
epoch: 1, step: 16
	action: tensor([[-0.0279, -0.0212, -0.0754, -0.0045, -0.0270, -0.0185,  0.0287]],
       dtype=torch.float64)
	q_value: tensor([[0.1457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22917274043755131, distance: 1.0046972779114764 entropy -8.01567070107291
epoch: 1, step: 17
	action: tensor([[-0.0234, -0.0213, -0.0755, -0.0147,  0.0344,  0.0007,  0.0122]],
       dtype=torch.float64)
	q_value: tensor([[0.1439]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2324915782160587, distance: 1.0025320559849857 entropy -8.015532225823895
epoch: 1, step: 18
	action: tensor([[-0.0373, -0.0212, -0.0755, -0.0252, -0.0169,  0.0226,  0.0255]],
       dtype=torch.float64)
	q_value: tensor([[0.1448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2185029906360726, distance: 1.011626863643637 entropy -8.01538983040409
epoch: 1, step: 19
	action: tensor([[-0.0304, -0.0212, -0.0758, -0.0257,  0.0603,  0.0659,  0.0141]],
       dtype=torch.float64)
	q_value: tensor([[0.1433]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2349852700904982, distance: 1.000902080369854 entropy -8.015630268955933
epoch: 1, step: 20
	action: tensor([[-0.0185, -0.0211, -0.0757, -0.0150, -0.0357,  0.0068, -0.0208]],
       dtype=torch.float64)
	q_value: tensor([[0.1440]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24082416464781686, distance: 0.9970751241835593 entropy -8.013885802139226
epoch: 1, step: 21
	action: tensor([[-0.0337, -0.0212, -0.0756, -0.0177, -0.1230,  0.0351, -0.0374]],
       dtype=torch.float64)
	q_value: tensor([[0.1443]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2295993916020982, distance: 1.0044191905775939 entropy -8.015866857859852
epoch: 1, step: 22
	action: tensor([[-0.0128, -0.0213, -0.0761, -0.0267, -0.0550, -0.0191,  0.0148]],
       dtype=torch.float64)
	q_value: tensor([[0.1428]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23620011998529067, distance: 1.000107044107014 entropy -8.016552033329557
epoch: 1, step: 23
	action: tensor([[-0.0295, -0.0213, -0.0756, -0.0117,  0.0509,  0.0122,  0.0040]],
       dtype=torch.float64)
	q_value: tensor([[0.1425]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22981198540173742, distance: 1.0042805951357485 entropy -8.016323851825055
epoch: 1, step: 24
	action: tensor([[-0.0190, -0.0212, -0.0755, -0.0041, -0.0192,  0.0345, -0.0103]],
       dtype=torch.float64)
	q_value: tensor([[0.1454]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2503539341854839, distance: 0.9907973271651559 entropy -8.015136968532612
epoch: 1, step: 25
	action: tensor([[-0.0285, -0.0213, -0.0756, -0.0185, -0.0365,  0.0221, -0.0113]],
       dtype=torch.float64)
	q_value: tensor([[0.1442]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23153078453855092, distance: 1.003159361935003 entropy -8.01453834297003
epoch: 1, step: 26
	action: tensor([[-0.0194, -0.0212, -0.0757, -0.0125, -0.0058,  0.0502, -0.0009]],
       dtype=torch.float64)
	q_value: tensor([[0.1439]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24936763117534477, distance: 0.991448904762851 entropy -8.015861648348286
epoch: 1, step: 27
	action: tensor([[-0.0185, -0.0212, -0.0756, -0.0190,  0.0257,  0.0050, -0.0021]],
       dtype=torch.float64)
	q_value: tensor([[0.1437]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2372950024963647, distance: 0.9993899764341443 entropy -8.014339528352016
epoch: 1, step: 28
	action: tensor([[-0.0220, -0.0212, -0.0755, -0.0078, -0.0397, -0.0507,  0.0092]],
       dtype=torch.float64)
	q_value: tensor([[0.1445]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22835917036671893, distance: 1.0052273422417433 entropy -8.015454338502792
epoch: 1, step: 29
	action: tensor([[-0.0300, -0.0213, -0.0755, -0.0163, -0.0049,  0.0309, -0.0104]],
       dtype=torch.float64)
	q_value: tensor([[0.1442]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23245071657793226, distance: 1.002558742699877 entropy -8.016391549867175
epoch: 1, step: 30
	action: tensor([[-0.0343, -0.0212, -0.0757, -0.0095, -0.0090, -0.0050,  0.0102]],
       dtype=torch.float64)
	q_value: tensor([[0.1442]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22292072704740018, distance: 1.0087634908133263 entropy -8.015136646488637
epoch: 1, step: 31
	action: tensor([[-0.0254, -0.0212, -0.0756, -0.0390, -0.0339,  0.0019, -0.0016]],
       dtype=torch.float64)
	q_value: tensor([[0.1445]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22163961859817605, distance: 1.0095946822196642 entropy -8.01555850995271
epoch: 1, step: 32
	action: tensor([[-0.0243,  0.0011, -0.1486, -0.0366,  0.0066, -0.0302, -0.0076]],
       dtype=torch.float64)
	q_value: tensor([[0.2869]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22321513500533208, distance: 1.0085723802377344 entropy -5.491971707361256
epoch: 1, step: 33
	action: tensor([[-0.0273, -0.0211, -0.0759, -0.0007, -0.0875,  0.0038,  0.0171]],
       dtype=torch.float64)
	q_value: tensor([[0.1453]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22840677552912014, distance: 1.0051963338067746 entropy -8.019726664248896
epoch: 1, step: 34
	action: tensor([[-0.0216, -0.0214, -0.0757,  0.0025, -0.0065,  0.0153, -0.0185]],
       dtype=torch.float64)
	q_value: tensor([[0.1430]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24117357623719027, distance: 0.9968456452647427 entropy -8.015548187301588
epoch: 1, step: 35
	action: tensor([[-0.0273, -0.0212, -0.0755, -0.0041, -0.0168, -0.0179,  0.0018]],
       dtype=torch.float64)
	q_value: tensor([[0.1452]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22864040029024202, distance: 1.0050441444634373 entropy -8.014853847912784
epoch: 1, step: 36
	action: tensor([[-0.0151, -0.0212, -0.0755, -0.0169,  0.0023,  0.0220,  0.0218]],
       dtype=torch.float64)
	q_value: tensor([[0.1449]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2449455234874679, distance: 0.9943650164268575 entropy -8.015792331823741
epoch: 1, step: 37
	action: tensor([[-0.0102, -0.0213, -0.0755, -0.0247, -0.0588,  0.0086, -0.0068]],
       dtype=torch.float64)
	q_value: tensor([[0.1433]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24714196587023085, distance: 0.9929176664659909 entropy -8.014724275642935
epoch: 1, step: 38
	action: tensor([[-0.0250, -0.0213, -0.0757, -0.0064, -0.0081, -0.0150, -0.0043]],
       dtype=torch.float64)
	q_value: tensor([[0.1427]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23735391523510418, distance: 0.9993513783275662 entropy -8.015836170800496
epoch: 1, step: 39
	action: tensor([[-0.0188, -0.0212, -0.0755, -0.0053, -0.0256,  0.0232, -0.0190]],
       dtype=torch.float64)
	q_value: tensor([[0.1449]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25293643694577617, distance: 0.989089224874394 entropy -8.015687051357913
epoch: 1, step: 40
	action: tensor([[-0.0392, -0.0213, -0.0756, -0.0063, -0.0633,  0.0197, -0.0152]],
       dtype=torch.float64)
	q_value: tensor([[0.1443]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2303076611962387, distance: 1.003957376849374 entropy -8.014851439086197
epoch: 1, step: 41
	action: tensor([[-0.0142, -0.0213, -0.0758, -0.0049, -0.0742, -0.0027,  0.0179]],
       dtype=torch.float64)
	q_value: tensor([[0.1440]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2527857803128176, distance: 0.9891889522254317 entropy -8.015681257688515
epoch: 1, step: 42
	action: tensor([[-0.0127, -0.0214, -0.0756, -0.0260, -0.0132, -0.0108,  0.0148]],
       dtype=torch.float64)
	q_value: tensor([[0.1426]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24246363243433477, distance: 0.9959979323403937 entropy -8.015183020528884
epoch: 1, step: 43
	action: tensor([[-0.0055, -0.0213, -0.0756, -0.0271,  0.0004,  0.0133,  0.0280]],
       dtype=torch.float64)
	q_value: tensor([[0.1433]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25423433557146125, distance: 0.9882296624060048 entropy -8.016040613160335
epoch: 1, step: 44
	action: tensor([[-0.0075, -0.0213, -0.0755, -0.0092, -0.0313,  0.0127, -0.0076]],
       dtype=torch.float64)
	q_value: tensor([[0.1426]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2608363685773225, distance: 0.983845692421636 entropy -8.015037873278029
epoch: 1, step: 45
	action: tensor([[-0.0173, -0.0213, -0.0755, -0.0142,  0.0337, -0.0075,  0.0055]],
       dtype=torch.float64)
	q_value: tensor([[0.1438]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24273814609557853, distance: 0.9958174526796036 entropy -8.015075751510603
epoch: 1, step: 46
	action: tensor([[-0.0271, -0.0212, -0.0754, -0.0273, -0.0314,  0.0002,  0.0551]],
       dtype=torch.float64)
	q_value: tensor([[0.1449]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22874022486580992, distance: 1.0049791090692086 entropy -8.015621758203048
epoch: 1, step: 47
	action: tensor([[-0.0062, -0.0213, -0.0757, -0.0358,  0.0043,  0.0195,  0.0062]],
       dtype=torch.float64)
	q_value: tensor([[0.1421]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25089544344019876, distance: 0.9904394096945015 entropy -8.015789838765508
epoch: 1, step: 48
	action: tensor([[-0.0375, -0.0212, -0.0756, -0.0252,  0.0069,  0.0039, -0.0038]],
       dtype=torch.float64)
	q_value: tensor([[0.1430]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21964884273424257, distance: 1.0108849542277787 entropy -8.01550854393908
epoch: 1, step: 49
	action: tensor([[-0.0075, -0.0212, -0.0757, -0.0003,  0.0644, -0.0043,  0.0104]],
       dtype=torch.float64)
	q_value: tensor([[0.1448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25751571497073833, distance: 0.9860531534009149 entropy -8.01628381328576
epoch: 1, step: 50
	action: tensor([[-0.0165, -0.0212, -0.0753,  0.0046, -0.0339,  0.0064, -0.0076]],
       dtype=torch.float64)
	q_value: tensor([[0.1456]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25621457786053625, distance: 0.9869167600704178 entropy -8.014649895899769
epoch: 1, step: 51
	action: tensor([[-0.0173, -0.0213, -0.0755,  0.0187, -0.1108,  0.0184,  0.0224]],
       dtype=torch.float64)
	q_value: tensor([[0.1445]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2640701382030465, distance: 0.9816912184081316 entropy -8.014850694738676
epoch: 1, step: 52
	action: tensor([[-0.0229, -0.0215, -0.0758, -0.0454,  0.0166,  0.0247,  0.0104]],
       dtype=torch.float64)
	q_value: tensor([[0.1424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23042818301425094, distance: 1.0038787717371978 entropy -8.01459343351144
epoch: 1, step: 53
	action: tensor([[-0.0160, -0.0212, -0.0757, -0.0129, -0.0606, -0.0291,  0.0176]],
       dtype=torch.float64)
	q_value: tensor([[0.1429]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24227542393659718, distance: 0.9961216515708089 entropy -8.015806575848929
epoch: 1, step: 54
	action: tensor([[-0.0379, -0.0213, -0.0756, -0.0217,  0.0489, -0.0330, -0.0080]],
       dtype=torch.float64)
	q_value: tensor([[0.1430]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21164085098847363, distance: 1.0160585840582532 entropy -8.01603785338313
epoch: 1, step: 55
	action: tensor([[-0.0238, -0.0211, -0.0755, -0.0425,  0.0177, -0.0087,  0.0058]],
       dtype=torch.float64)
	q_value: tensor([[0.1461]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2232724635456298, distance: 1.0085351620504337 entropy -8.016912752024393
epoch: 1, step: 56
	action: tensor([[-0.0162, -0.0212, -0.0757, -0.0414,  0.0068,  0.0053,  0.0086]],
       dtype=torch.float64)
	q_value: tensor([[0.1437]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2354397504592155, distance: 1.0006047279730954 entropy -8.016677447797147
epoch: 1, step: 57
	action: tensor([[-0.0386, -0.0212, -0.0756, -0.0249,  0.0465,  0.0367,  0.0043]],
       dtype=torch.float64)
	q_value: tensor([[0.1431]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22539295898563871, distance: 1.0071575516798255 entropy -8.016110817549354
epoch: 1, step: 58
	action: tensor([[-0.0235, -0.0211, -0.0757, -0.0139, -0.0197,  0.0168,  0.0120]],
       dtype=torch.float64)
	q_value: tensor([[0.1446]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24240744949654636, distance: 0.9960348659153129 entropy -8.015000226522037
epoch: 1, step: 59
	action: tensor([[-0.0108, -0.0213, -0.0756, -0.0409, -0.0091,  0.0040,  0.0180]],
       dtype=torch.float64)
	q_value: tensor([[0.1435]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24094842811012074, distance: 0.9969935191904831 entropy -8.015054939347959
epoch: 1, step: 60
	action: tensor([[-0.0380, -0.0212, -0.0757, -0.0065, -0.0330,  0.0345, -0.0007]],
       dtype=torch.float64)
	q_value: tensor([[0.1424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2341252863629314, distance: 1.0014644994323032 entropy -8.015955887386893
epoch: 1, step: 61
	action: tensor([[-0.0336, -0.0213, -0.0757, -0.0015, -0.0181,  0.0049,  0.0028]],
       dtype=torch.float64)
	q_value: tensor([[0.1439]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23465433034773175, distance: 1.0011185483991605 entropy -8.014977764437365
epoch: 1, step: 62
	action: tensor([[-0.0312, -0.0213, -0.0756, -0.0277,  0.0056,  0.0099,  0.0184]],
       dtype=torch.float64)
	q_value: tensor([[0.1446]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2262370820483518, distance: 1.0066086303469033 entropy -8.015144593440114
epoch: 1, step: 63
	action: tensor([[-0.0242, -0.0212, -0.0757, -0.0125,  0.0014,  0.0321,  0.0161]],
       dtype=torch.float64)
	q_value: tensor([[0.1436]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24525796818541123, distance: 0.9941592589390785 entropy -8.01566025937524
epoch: 1, step: 64
	action: tensor([[-0.0281, -0.0184, -0.1486, -0.0375, -0.0322, -0.0171, -0.0224]],
       dtype=torch.float64)
	q_value: tensor([[0.2869]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2079952960921937, distance: 1.0184051193096206 entropy -5.491971707361256
epoch: 1, step: 65
	action: tensor([[-0.0115, -0.0212, -0.0760, -0.0397,  0.0720, -0.0207,  0.0055]],
       dtype=torch.float64)
	q_value: tensor([[0.1450]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22026662961050725, distance: 1.0104847272887445 entropy -8.020409209805795
epoch: 1, step: 66
	action: tensor([[-0.0385, -0.0211, -0.0755, -0.0239,  0.0812,  0.0125,  0.0032]],
       dtype=torch.float64)
	q_value: tensor([[0.1447]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20802697221319566, distance: 1.018384753618492 entropy -8.016170224250397
epoch: 1, step: 67
	action: tensor([[-0.0173, -0.0211, -0.0756, -0.0156, -0.0507,  0.0182,  0.0093]],
       dtype=torch.float64)
	q_value: tensor([[0.1458]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23569148952786578, distance: 1.0004399848803818 entropy -8.015309633556305
epoch: 1, step: 68
	action: tensor([[-0.0321, -0.0213, -0.0757, -0.0081, -0.0521, -0.0029,  0.0006]],
       dtype=torch.float64)
	q_value: tensor([[0.1429]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22381729915036797, distance: 1.0081813814567504 entropy -8.015384382549785
epoch: 1, step: 69
	action: tensor([[-0.0233, -0.0213, -0.0757, -0.0138,  0.0147, -0.0249,  0.0032]],
       dtype=torch.float64)
	q_value: tensor([[0.1441]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22721193984881172, distance: 1.0059743213547627 entropy -8.015936829133555
epoch: 1, step: 70
	action: tensor([[-0.0352, -0.0212, -0.0755, -0.0101, -0.1057,  0.0240, -0.0301]],
       dtype=torch.float64)
	q_value: tensor([[0.1450]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23273604056389885, distance: 1.0023723829105176 entropy -8.016267271823718
epoch: 1, step: 71
	action: tensor([[-0.0349, -0.0213, -0.0760,  0.0033, -0.0203, -0.0143,  0.0308]],
       dtype=torch.float64)
	q_value: tensor([[0.1435]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23078818877175256, distance: 1.0036439369852244 entropy -8.016325851745872
epoch: 1, step: 72
	action: tensor([[-0.0394, -0.0213, -0.0755, -0.0142,  0.0367,  0.0079,  0.0301]],
       dtype=torch.float64)
	q_value: tensor([[0.1444]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22208653372905096, distance: 1.0093047985511752 entropy -8.015197808160512
epoch: 1, step: 73
	action: tensor([[-0.0210, -0.0212, -0.0756, -0.0042, -0.0475,  0.0292, -0.0103]],
       dtype=torch.float64)
	q_value: tensor([[0.1448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25238544017989306, distance: 0.9894539090456224 entropy -8.015245598833356
epoch: 1, step: 74
	action: tensor([[-0.0235, -0.0213, -0.0756, -0.0265, -0.0286, -0.0094,  0.0237]],
       dtype=torch.float64)
	q_value: tensor([[0.1438]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23131811346001951, distance: 1.003298162696775 entropy -8.01491163050173
epoch: 1, step: 75
	action: tensor([[-0.0151, -0.0213, -0.0757, -0.0053, -0.0706,  0.0194,  0.0083]],
       dtype=torch.float64)
	q_value: tensor([[0.1431]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25603333549788776, distance: 0.9870369965154351 entropy -8.01620152203915
epoch: 1, step: 76
	action: tensor([[-0.0200, -0.0214, -0.0757, -0.0067, -0.0514,  0.0221, -0.0101]],
       dtype=torch.float64)
	q_value: tensor([[0.1429]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25077731803057557, distance: 0.9905174972245222 entropy -8.015005076227874
epoch: 1, step: 77
	action: tensor([[-0.0238, -0.0213, -0.0757, -0.0272, -0.0591, -0.0067,  0.0234]],
       dtype=torch.float64)
	q_value: tensor([[0.1436]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23183857715876388, distance: 1.0029584456421567 entropy -8.015027157428142
epoch: 1, step: 78
	action: tensor([[-0.0295, -0.0213, -0.0757, -0.0203, -0.0915, -0.0049, -0.0011]],
       dtype=torch.float64)
	q_value: tensor([[0.1424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23006499053983687, distance: 1.0041156295584284 entropy -8.016346423925624
epoch: 1, step: 79
	action: tensor([[-0.0296, -0.0213, -0.0759, -0.0151,  0.0785, -0.0163, -0.0041]],
       dtype=torch.float64)
	q_value: tensor([[0.1428]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2263302296458939, distance: 1.006548039437569 entropy -8.016599911583386
epoch: 1, step: 80
	action: tensor([[-0.0248, -0.0211, -0.0754, -0.0027, -0.0337,  0.0018, -0.0275]],
       dtype=torch.float64)
	q_value: tensor([[0.1464]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24364113577120383, distance: 0.9952235490659861 entropy -8.015813805991993
epoch: 1, step: 81
	action: tensor([[-0.0280, -0.0212, -0.0756, -0.0090, -0.1036, -0.0372,  0.0007]],
       dtype=torch.float64)
	q_value: tensor([[0.1450]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23108840936902397, distance: 1.003448058565231 entropy -8.015468608122285
epoch: 1, step: 82
	action: tensor([[-0.0307, -0.0213, -0.0757,  0.0016, -0.0042,  0.0203,  0.0228]],
       dtype=torch.float64)
	q_value: tensor([[0.1434]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24162771287173324, distance: 0.9965473083425413 entropy -8.016977436270041
epoch: 1, step: 83
	action: tensor([[-0.0187, -0.0213, -0.0756,  0.0151, -0.0530, -0.0194, -0.0014]],
       dtype=torch.float64)
	q_value: tensor([[0.1442]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2534098510578875, distance: 0.9887757823333757 entropy -8.014282588963253
epoch: 1, step: 84
	action: tensor([[-0.0370, -0.0213, -0.0754, -0.0021, -0.0365,  0.0412,  0.0314]],
       dtype=torch.float64)
	q_value: tensor([[0.1448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23777619643836312, distance: 0.9990746669249563 entropy -8.015011463327053
epoch: 1, step: 85
	action: tensor([[-0.0354, -0.0213, -0.0758, -0.0240, -0.0408,  0.0108,  0.0252]],
       dtype=torch.float64)
	q_value: tensor([[0.1432]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22394026836933345, distance: 1.0081015161186018 entropy -8.01443770628361
epoch: 1, step: 86
	action: tensor([[-0.0147, -0.0213, -0.0758, -0.0059,  0.0058,  0.0304,  0.0104]],
       dtype=torch.float64)
	q_value: tensor([[0.1430]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2576403543588406, distance: 0.9859703865179131 entropy -8.01607699874025
epoch: 1, step: 87
	action: tensor([[-0.0352, -0.0213, -0.0755, -0.0152, -0.0062,  0.0374, -0.0183]],
       dtype=torch.float64)
	q_value: tensor([[0.1440]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2340452076494588, distance: 1.001516853878201 entropy -8.014236990760322
epoch: 1, step: 88
	action: tensor([[-0.0343, -0.0212, -0.0757, -0.0075, -0.0334,  0.0214, -0.0101]],
       dtype=torch.float64)
	q_value: tensor([[0.1447]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23508269473476784, distance: 1.0008383458886279 entropy -8.015363210924205
epoch: 1, step: 89
	action: tensor([[-0.0342, -0.0212, -0.0757, -0.0143, -0.0011,  0.0128,  0.0233]],
       dtype=torch.float64)
	q_value: tensor([[0.1444]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22939960192271347, distance: 1.00454942125066 entropy -8.015455789249042
epoch: 1, step: 90
	action: tensor([[-0.0395, -0.0212, -0.0756, -0.0179, -0.0180, -0.0151, -0.0075]],
       dtype=torch.float64)
	q_value: tensor([[0.1440]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2172358031633267, distance: 1.0124467014930592 entropy -8.015228804231754
epoch: 1, step: 91
	action: tensor([[-0.0200, -0.0212, -0.0756, -0.0140, -0.0745, -0.0783, -0.0243]],
       dtype=torch.float64)
	q_value: tensor([[0.1449]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22949265912396077, distance: 1.0044887650530883 entropy -8.016703855918584
epoch: 1, step: 92
	action: tensor([[-0.0312, -0.0212, -0.0755, -0.0041, -0.0018,  0.0004,  0.0030]],
       dtype=torch.float64)
	q_value: tensor([[0.1447]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2347115117895271, distance: 1.001081149303848 entropy -8.017845379625014
epoch: 1, step: 93
	action: tensor([[-0.0314, -0.0212, -0.0755, -0.0153, -0.0241, -0.0327,  0.0106]],
       dtype=torch.float64)
	q_value: tensor([[0.1450]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2232953481983615, distance: 1.008520304751142 entropy -8.015359052216935
epoch: 1, step: 94
	action: tensor([[-0.0305, -0.0212, -0.0756, -0.0227,  0.0414,  0.0277, -0.0094]],
       dtype=torch.float64)
	q_value: tensor([[0.1444]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23317544834217652, distance: 1.0020853152754017 entropy -8.01662958857641
epoch: 1, step: 95
	action: tensor([[-0.0233, -0.0211, -0.0756, -0.0195,  0.0556, -0.0054,  0.0174]],
       dtype=torch.float64)
	q_value: tensor([[0.1451]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2336400243394201, distance: 1.0017817156339333 entropy -8.015349090247417
epoch: 1, step: 96
	action: tensor([[-0.0165,  0.0054, -0.1486, -0.0183, -0.0270, -0.0338,  0.0013]],
       dtype=torch.float64)
	q_value: tensor([[0.2869]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24104093818727523, distance: 0.9969327626087288 entropy -5.491971707361256
epoch: 1, step: 97
	action: tensor([[-0.0228, -0.0212, -0.0758, -0.0231,  0.0243, -0.0009, -0.0125]],
       dtype=torch.float64)
	q_value: tensor([[0.1446]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22125629297176785, distance: 1.0098432533746482 entropy -8.019163211747555
epoch: 1, step: 98
	action: tensor([[-0.0194, -0.0212, -0.0755, -0.0115, -0.0089, -0.0095,  0.0154]],
       dtype=torch.float64)
	q_value: tensor([[0.1448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22978510533792507, distance: 1.0042981200065841 entropy -8.01596301703445
epoch: 1, step: 99
	action: tensor([[-0.0223, -0.0213, -0.0755, -0.0258, -0.0536,  0.0719, -0.0136]],
       dtype=torch.float64)
	q_value: tensor([[0.1439]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24101752895346218, distance: 0.9969481371218999 entropy -8.015362199381551
epoch: 1, step: 100
	action: tensor([[-0.0379, -0.0213, -0.0759, -0.0235, -0.0065,  0.0100, -0.0250]],
       dtype=torch.float64)
	q_value: tensor([[0.1424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2176791597002422, distance: 1.0121599366897323 entropy -8.01532034473755
epoch: 1, step: 101
	action: tensor([[-0.0101, -0.0212, -0.0757, -0.0211, -0.0298, -0.0173, -0.0030]],
       dtype=torch.float64)
	q_value: tensor([[0.1451]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24355537127589355, distance: 0.9952799722996895 entropy -8.016434209572976
epoch: 1, step: 102
	action: tensor([[-0.0232, -0.0213, -0.0755, -0.0113, -0.0234, -0.0239, -0.0183]],
       dtype=torch.float64)
	q_value: tensor([[0.1437]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23585969057742917, distance: 1.0003298956217577 entropy -8.016214537212475
epoch: 1, step: 103
	action: tensor([[-0.0392, -0.0212, -0.0755, -0.0315, -0.0214, -0.0044,  0.0063]],
       dtype=torch.float64)
	q_value: tensor([[0.1451]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21369004799303248, distance: 1.0147371945721573 entropy -8.016430323979431
epoch: 1, step: 104
	action: tensor([[-0.0471, -0.0212, -0.0758, -0.0133, -0.0044,  0.0313,  0.0183]],
       dtype=torch.float64)
	q_value: tensor([[0.1439]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22008489577442159, distance: 1.0106024781595768 entropy -8.016843064540222
epoch: 1, step: 105
	action: tensor([[-0.0345, -0.0212, -0.0758, -0.0054, -0.0325, -0.0064, -0.0179]],
       dtype=torch.float64)
	q_value: tensor([[0.1441]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23014198005680842, distance: 1.0040654251197358 entropy -8.014959517888267
epoch: 1, step: 106
	action: tensor([[-0.0248, -0.0212, -0.0756, -0.0117,  0.0124,  0.0105,  0.0167]],
       dtype=torch.float64)
	q_value: tensor([[0.1450]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24001675515442034, distance: 0.9976051950499903 entropy -8.01590839696689
epoch: 1, step: 107
	action: tensor([[-0.0189, -0.0212, -0.0755, -0.0305, -0.0243, -0.0115, -0.0074]],
       dtype=torch.float64)
	q_value: tensor([[0.1441]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2344445633119845, distance: 1.0012557330016465 entropy -8.014894524842994
epoch: 1, step: 108
	action: tensor([[-0.0331, -0.0212, -0.0756, -0.0001, -0.0303, -0.0506,  0.0327]],
       dtype=torch.float64)
	q_value: tensor([[0.1436]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22432248602520377, distance: 1.007853235195637 entropy -8.0165311351051
epoch: 1, step: 109
	action: tensor([[-0.0206, -0.0213, -0.0755, -0.0184, -0.0547,  0.0346, -0.0132]],
       dtype=torch.float64)
	q_value: tensor([[0.1443]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24767268145151577, distance: 0.9925676338217886 entropy -8.016185778323377
epoch: 1, step: 110
	action: tensor([[-0.0232, -0.0213, -0.0758, -0.0088, -0.0349,  0.0030, -0.0007]],
       dtype=torch.float64)
	q_value: tensor([[0.1432]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24228983334704024, distance: 0.9961121800593772 entropy -8.015581588954698
epoch: 1, step: 111
	action: tensor([[-0.0225, -0.0213, -0.0756, -0.0199,  0.0428, -0.0059,  0.0266]],
       dtype=torch.float64)
	q_value: tensor([[0.1441]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23444346335675315, distance: 1.001256452306894 entropy -8.015559152556373
epoch: 1, step: 112
	action: tensor([[-0.0213, -0.0212, -0.0755,  0.0054, -0.0343,  0.0385, -0.0102]],
       dtype=torch.float64)
	q_value: tensor([[0.1445]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2580060749953006, distance: 0.9857274893311289 entropy -8.015469432683583
epoch: 1, step: 113
	action: tensor([[-0.0216, -0.0213, -0.0756, -0.0144, -0.0190,  0.0179,  0.0070]],
       dtype=torch.float64)
	q_value: tensor([[0.1442]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24433625382834034, distance: 0.9947661227623129 entropy -8.014179062245484
epoch: 1, step: 114
	action: tensor([[-0.0184, -0.0213, -0.0756,  0.0003,  0.0357, -0.0187,  0.0168]],
       dtype=torch.float64)
	q_value: tensor([[0.1437]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24503432917238355, distance: 0.994306538617809 entropy -8.015212714786745
epoch: 1, step: 115
	action: tensor([[-0.0362, -0.0212, -0.0753, -0.0203,  0.0099,  0.0020, -0.0109]],
       dtype=torch.float64)
	q_value: tensor([[0.1452]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22280329693805967, distance: 1.0088397087335421 entropy -8.01508993442779
epoch: 1, step: 116
	action: tensor([[-0.0257, -0.0212, -0.0756, -0.0117,  0.0998,  0.0370, -0.0148]],
       dtype=torch.float64)
	q_value: tensor([[0.1450]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2443183060892976, distance: 0.9947779360171238 entropy -8.01603315137187
epoch: 1, step: 117
	action: tensor([[-0.0337, -0.0211, -0.0755,  0.0030,  0.0002,  0.0104, -0.0048]],
       dtype=torch.float64)
	q_value: tensor([[0.1463]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23750075398217607, distance: 0.9992551668794686 entropy -8.014275758817787
epoch: 1, step: 118
	action: tensor([[-0.0199, -0.0212, -0.0755, -0.0148,  0.1037,  0.0494, -0.0031]],
       dtype=torch.float64)
	q_value: tensor([[0.1454]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2512779564786506, distance: 0.9901865048359167 entropy -8.01495249602783
epoch: 1, step: 119
	action: tensor([[-0.0333, -0.0211, -0.0755, -0.0062, -0.0324, -0.0188, -0.0258]],
       dtype=torch.float64)
	q_value: tensor([[0.1458]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22890192500855933, distance: 1.004873753008172 entropy -8.013739193016479
epoch: 1, step: 120
	action: tensor([[-0.0248, -0.0212, -0.0756, -0.0194, -0.0040,  0.0072, -0.0274]],
       dtype=torch.float64)
	q_value: tensor([[0.1455]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23700384783596773, distance: 0.9995807115716367 entropy -8.016404250208183
epoch: 1, step: 121
	action: tensor([[-0.0402, -0.0212, -0.0756,  0.0015,  0.0313, -0.0078, -0.0121]],
       dtype=torch.float64)
	q_value: tensor([[0.1450]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2255162484741675, distance: 1.0070773969212474 entropy -8.016082640580755
epoch: 1, step: 122
	action: tensor([[-0.0197, -0.0212, -0.0755, -0.0148,  0.0389,  0.0305, -0.0016]],
       dtype=torch.float64)
	q_value: tensor([[0.1466]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24787160892901072, distance: 0.9924363997002366 entropy -8.015814832147289
epoch: 1, step: 123
	action: tensor([[-0.0157, -0.0212, -0.0755, -0.0089, -0.0204, -0.0093, -0.0164]],
       dtype=torch.float64)
	q_value: tensor([[0.1448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24783550458518955, distance: 0.992460219326932 entropy -8.014687385280244
epoch: 1, step: 124
	action: tensor([[-0.0168, -0.0212, -0.0755, -0.0245, -0.0002,  0.0082, -0.0173]],
       dtype=torch.float64)
	q_value: tensor([[0.1446]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2432151759665755, distance: 0.9955037504225843 entropy -8.01562649598375
epoch: 1, step: 125
	action: tensor([[-4.7358e-03, -2.1195e-02, -7.5553e-02, -3.2074e-02, -4.9103e-02,
         -1.3082e-02, -2.1940e-05]], dtype=torch.float64)
	q_value: tensor([[0.1442]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24870724195434002, distance: 0.9918849359370836 entropy -8.015758285019508
epoch: 1, step: 126
	action: tensor([[-0.0255, -0.0213, -0.0756, -0.0025,  0.0312,  0.0072, -0.0103]],
       dtype=torch.float64)
	q_value: tensor([[0.1426]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24268012451845467, distance: 0.9958556018298073 entropy -8.016287280745047
epoch: 1, step: 127
	action: tensor([[-0.0234, -0.0212, -0.0754, -0.0201,  0.0217, -0.0008, -0.0300]],
       dtype=torch.float64)
	q_value: tensor([[0.1456]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23601840540289154, distance: 1.000226004079967 entropy -8.015013092064855
LOSS epoch 1 actor 0.1545362149326468 critic 5.730121366260782 entropy 0.01
epoch: 2, step: 0
	action: tensor([[ 0.1201, -0.0680, -0.7623, -0.1548, -0.1294, -0.0883,  0.0300]],
       dtype=torch.float64)
	q_value: tensor([[0.7498]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3621846911006579, distance: 0.913911543276564 entropy -4.5057066881775745
epoch: 2, step: 1
	action: tensor([[ 0.0277, -0.0665, -0.3955, -0.0565, -0.0179, -0.1350, -0.0522]],
       dtype=torch.float64)
	q_value: tensor([[0.3901]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24125107007962598, distance: 0.9967947433922664 entropy -5.008853854427824
epoch: 2, step: 2
	action: tensor([[-0.0046, -0.0690, -0.3655, -0.0743, -0.0207, -0.0577, -0.0638]],
       dtype=torch.float64)
	q_value: tensor([[0.3680]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2078500360020784, distance: 1.0184985069096186 entropy -5.058837336557459
epoch: 2, step: 3
	action: tensor([[ 0.0056, -0.0626, -0.3661, -0.0746, -0.0585, -0.0487, -0.0141]],
       dtype=torch.float64)
	q_value: tensor([[0.3643]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22488431733276515, distance: 1.0074881697773905 entropy -5.055180181081608
epoch: 2, step: 4
	action: tensor([[ 0.0188, -0.0644, -0.3657, -0.0528,  0.0239, -0.0786, -0.0283]],
       dtype=torch.float64)
	q_value: tensor([[0.3605]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2363252328206925, distance: 1.0000251304044163 entropy -5.056138901409734
epoch: 2, step: 5
	action: tensor([[-0.0242, -0.0691, -0.3644, -0.0601, -0.0038, -0.0245, -0.0076]],
       dtype=torch.float64)
	q_value: tensor([[0.3638]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19388830440171478, distance: 1.0274348777423192 entropy -5.060050299242623
epoch: 2, step: 6
	action: tensor([[-0.0221, -0.0721, -0.3665, -0.0892, -0.0359, -0.0217, -0.0227]],
       dtype=torch.float64)
	q_value: tensor([[0.3627]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1876757951033211, distance: 1.0313863760052764 entropy -5.054990649281975
epoch: 2, step: 7
	action: tensor([[-0.0162, -0.0589, -0.3676, -0.0705,  0.1419, -0.0825,  0.0135]],
       dtype=torch.float64)
	q_value: tensor([[0.3619]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19532261638589532, distance: 1.02652041497581 entropy -5.0517788138778865
epoch: 2, step: 8
	action: tensor([[-0.0254, -0.0663, -0.3675, -0.0646, -0.0692, -0.0916, -0.0454]],
       dtype=torch.float64)
	q_value: tensor([[0.3673]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18955134062751622, distance: 1.030195022876993 entropy -5.052527275442515
epoch: 2, step: 9
	action: tensor([[-0.0240, -0.0601, -0.3665, -0.0937, -0.0616, -0.0152,  0.0363]],
       dtype=torch.float64)
	q_value: tensor([[0.3665]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19471052666664157, distance: 1.0269107597059555 entropy -5.054704219877779
epoch: 2, step: 10
	action: tensor([[ 0.0058, -0.0521, -0.3679, -0.0787, -0.0366, -0.0805, -0.0540]],
       dtype=torch.float64)
	q_value: tensor([[0.3582]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2291734148149409, distance: 1.004696838419186 entropy -5.051531592226036
epoch: 2, step: 11
	action: tensor([[-0.0033, -0.0515, -0.3659, -0.0687, -0.0070, -0.0203,  0.0173]],
       dtype=torch.float64)
	q_value: tensor([[0.3636]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2277008858699755, distance: 1.0056560290613294 entropy -5.05568553008153
epoch: 2, step: 12
	action: tensor([[ 0.0024, -0.0634, -0.3661, -0.0657, -0.0647,  0.0256,  0.0093]],
       dtype=torch.float64)
	q_value: tensor([[0.3592]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23059018339159687, distance: 1.0037731043415299 entropy -5.05588334720063
epoch: 2, step: 13
	action: tensor([[ 0.0298, -0.0628, -0.3667, -0.0635, -0.0681, -0.0603, -0.0556]],
       dtype=torch.float64)
	q_value: tensor([[0.3575]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2512438760665675, distance: 0.990209040288433 entropy -5.054847762715034
epoch: 2, step: 14
	action: tensor([[ 0.0308, -0.0657, -0.3647, -0.0586, -0.0581, -0.0067,  0.0074]],
       dtype=torch.float64)
	q_value: tensor([[0.3617]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2557399779430739, distance: 0.9872315793074271 entropy -5.058432885150038
epoch: 2, step: 15
	action: tensor([[ 0.0359, -0.0593, -0.3648, -0.0762,  0.0588,  0.0151, -0.0522]],
       dtype=torch.float64)
	q_value: tensor([[0.3572]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26376599524425637, distance: 0.9818940527033256 entropy -5.058899113050293
epoch: 2, step: 16
	action: tensor([[ 0.0024, -0.0641, -0.3659, -0.0623, -0.0757, -0.0674, -0.0476]],
       dtype=torch.float64)
	q_value: tensor([[0.3589]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22242014938864574, distance: 1.0090883503150703 entropy -5.055889988555103
epoch: 2, step: 17
	action: tensor([[ 3.5701e-03, -6.3703e-02, -3.6558e-01, -7.9035e-02, -2.4268e-02,
          3.4474e-04, -6.5955e-03]], dtype=torch.float64)
	q_value: tensor([[0.3636]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2253050180527384, distance: 1.0072147212227567 entropy -5.056588343037622
epoch: 2, step: 18
	action: tensor([[-0.0035, -0.0651, -0.3665, -0.0694, -0.0272, -0.0090, -0.0812]],
       dtype=torch.float64)
	q_value: tensor([[0.3587]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21906489636332216, distance: 1.0112631123195361 entropy -5.054778389450589
epoch: 2, step: 19
	action: tensor([[ 0.0163, -0.0571, -0.3667, -0.0794, -0.0616, -0.0890,  0.0452]],
       dtype=torch.float64)
	q_value: tensor([[0.3627]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23471794906661558, distance: 1.0010769389633052 entropy -5.054006351625544
epoch: 2, step: 20
	action: tensor([[-0.0482, -0.0629, -0.3651, -0.0655, -0.0711, -0.0046, -0.0584]],
       dtype=torch.float64)
	q_value: tensor([[0.3583]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17552640461691893, distance: 1.039070629102014 entropy -5.058653687974943
epoch: 2, step: 21
	action: tensor([[-0.0040, -0.0659, -0.3682, -0.0424, -0.0616, -0.0915, -0.0885]],
       dtype=torch.float64)
	q_value: tensor([[0.3640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21717292839827929, distance: 1.012487362569846 entropy -5.050538104335021
epoch: 2, step: 22
	action: tensor([[-0.0415, -0.0609, -0.3654, -0.0577,  0.0406, -0.0341,  0.0100]],
       dtype=torch.float64)
	q_value: tensor([[0.3680]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18052018442104167, distance: 1.0359190600131396 entropy -5.057824274036236
epoch: 2, step: 23
	action: tensor([[-0.0006, -0.0731, -0.3671, -0.0414, -0.0463, -0.0501, -0.0095]],
       dtype=torch.float64)
	q_value: tensor([[0.3639]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21810019899880084, distance: 1.0118875315317883 entropy -5.0538408278792755
epoch: 2, step: 24
	action: tensor([[ 0.0110, -0.0589, -0.3650, -0.0760, -0.0534, -0.0595, -0.0376]],
       dtype=torch.float64)
	q_value: tensor([[0.3632]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23183130385705164, distance: 1.0029631938642722 entropy -5.058619653766056
epoch: 2, step: 25
	action: tensor([[ 0.0088, -0.0481, -0.3656, -0.0552,  0.0062, -0.0210, -0.0876]],
       dtype=torch.float64)
	q_value: tensor([[0.3617]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24673422950761137, distance: 0.9931865045597882 entropy -5.056228737411408
epoch: 2, step: 26
	action: tensor([[-0.0115, -0.0619, -0.3659, -0.0796,  0.0231, -0.0820,  0.0224]],
       dtype=torch.float64)
	q_value: tensor([[0.3631]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2001751366906993, distance: 1.0234205741876075 entropy -5.0559475302781705
epoch: 2, step: 27
	action: tensor([[-0.0155, -0.0649, -0.3661, -0.0597,  0.0834,  0.0462, -0.0860]],
       dtype=torch.float64)
	q_value: tensor([[0.3625]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21644815657351846, distance: 1.0129559542730369 entropy -5.056207646878269
epoch: 2, step: 28
	action: tensor([[-0.0570, -0.0671, -0.3684, -0.0753, -0.0533, -0.1226, -0.0731]],
       dtype=torch.float64)
	q_value: tensor([[0.3637]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1503876555307906, distance: 1.0547926629345727 entropy -5.050920763242492
epoch: 2, step: 29
	action: tensor([[ 0.0122, -0.0763, -0.3682, -0.0659,  0.1026,  0.0678,  0.0220]],
       dtype=torch.float64)
	q_value: tensor([[0.3712]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23690963670303822, distance: 0.9996424214008658 entropy -5.050919339536912
epoch: 2, step: 30
	action: tensor([[-0.0218, -0.0593, -0.3675, -0.0569, -0.0400, -0.0668, -0.0506]],
       dtype=torch.float64)
	q_value: tensor([[0.3574]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2018675102355405, distance: 1.0223372576028857 entropy -5.053463269825623
epoch: 2, step: 31
	action: tensor([[ 0.0064, -0.0627, -0.3663, -0.0828,  0.0116, -0.0206,  0.0080]],
       dtype=torch.float64)
	q_value: tensor([[0.3657]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2249225189058811, distance: 1.0074633424464068 entropy -5.055182644445721
epoch: 2, step: 32
	action: tensor([[-0.0081, -0.1577, -0.7623, -0.1054,  0.0816, -0.1537, -0.0480]],
       dtype=torch.float64)
	q_value: tensor([[0.7498]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1643842417588397, distance: 1.0460682097228882 entropy -4.5057066881775745
epoch: 2, step: 33
	action: tensor([[ 0.0080, -0.0635, -0.3976, -0.0744, -0.1004, -0.1849, -0.0527]],
       dtype=torch.float64)
	q_value: tensor([[0.4107]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2224481406546489, distance: 1.0090701876050843 entropy -5.004872754316945
epoch: 2, step: 34
	action: tensor([[ 0.0285, -0.0668, -0.3671, -0.0681, -0.0446, -0.0311, -0.0239]],
       dtype=torch.float64)
	q_value: tensor([[0.3700]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2479608407225613, distance: 0.9923775271010147 entropy -5.055675579820685
epoch: 2, step: 35
	action: tensor([[ 0.0044, -0.0640, -0.3650, -0.0710,  0.0140, -0.0154, -0.0926]],
       dtype=torch.float64)
	q_value: tensor([[0.3598]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.226472220624653, distance: 1.0064556697309461 entropy -5.057860481707182
epoch: 2, step: 36
	action: tensor([[-0.0216, -0.0634, -0.3666, -0.0789, -0.0410, -0.1394, -0.0668]],
       dtype=torch.float64)
	q_value: tensor([[0.3636]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1872450494457636, distance: 1.031659792900455 entropy -5.054194586629314
epoch: 2, step: 37
	action: tensor([[-0.0088, -0.0719, -0.3667, -0.0528, -0.1271, -0.1235, -0.0270]],
       dtype=torch.float64)
	q_value: tensor([[0.3693]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20536040803737832, distance: 1.0200977578543606 entropy -5.0543963774485885
epoch: 2, step: 38
	action: tensor([[-0.0215, -0.0570, -0.3656, -0.0538, -0.0734,  0.0727, -0.0240]],
       dtype=torch.float64)
	q_value: tensor([[0.3659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21878313478703804, distance: 1.0114455278561867 entropy -5.057944123541903
epoch: 2, step: 39
	action: tensor([[ 0.0266, -0.0569, -0.3682, -0.0735,  0.1180,  0.0646, -0.0363]],
       dtype=torch.float64)
	q_value: tensor([[0.3583]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2654397136242602, distance: 0.9807773227030064 entropy -5.052145813595795
epoch: 2, step: 40
	action: tensor([[-0.0151, -0.0551, -0.3674, -0.0846,  0.0121,  0.0108,  0.0458]],
       dtype=torch.float64)
	q_value: tensor([[0.3576]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21238893695161432, distance: 1.0155763929348114 entropy -5.05298188662007
epoch: 2, step: 41
	action: tensor([[-0.0181, -0.0613, -0.3676, -0.0661, -0.1057, -0.0548, -0.0398]],
       dtype=torch.float64)
	q_value: tensor([[0.3572]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20497072683994255, distance: 1.0203478487118527 entropy -5.052847715366933
epoch: 2, step: 42
	action: tensor([[-0.0227, -0.0707, -0.3668, -0.0788, -0.1048, -0.0956, -0.0783]],
       dtype=torch.float64)
	q_value: tensor([[0.3637]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18674951937797113, distance: 1.031974242261287 entropy -5.053769934986334
epoch: 2, step: 43
	action: tensor([[ 0.0178, -0.0638, -0.3671, -0.0547,  0.0271, -0.1866, -0.0159]],
       dtype=torch.float64)
	q_value: tensor([[0.3671]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22337020936838337, distance: 1.0084717014441298 entropy -5.05296524418827
epoch: 2, step: 44
	action: tensor([[ 0.0233, -0.0628, -0.3644, -0.0798, -0.0764, -0.0305, -0.0021]],
       dtype=torch.float64)
	q_value: tensor([[0.3684]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24334302005298358, distance: 0.9954196613666082 entropy -5.061869204862657
epoch: 2, step: 45
	action: tensor([[-0.0259, -0.0630, -0.3656, -0.0695, -0.0316, -0.0996, -0.0041]],
       dtype=torch.float64)
	q_value: tensor([[0.3580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.187876013538552, distance: 1.0312592621687513 entropy -5.05650707680381
epoch: 2, step: 46
	action: tensor([[-0.0101, -0.0585, -0.3662, -0.0676,  0.0293, -0.1114, -0.0352]],
       dtype=torch.float64)
	q_value: tensor([[0.3649]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2043935085369566, distance: 1.020718185132461 entropy -5.055759982470247
epoch: 2, step: 47
	action: tensor([[-0.0076, -0.0570, -0.3659, -0.0598, -0.0544,  0.0381, -0.0399]],
       dtype=torch.float64)
	q_value: tensor([[0.3668]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22870168112335554, distance: 1.00500422069389 entropy -5.0567425127412875
epoch: 2, step: 48
	action: tensor([[-0.0027, -0.0507, -0.3672, -0.0610, -0.1082, -0.0374, -0.0127]],
       dtype=torch.float64)
	q_value: tensor([[0.3595]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23160516630025485, distance: 1.0031108118013248 entropy -5.053473638332639
epoch: 2, step: 49
	action: tensor([[-0.0063, -0.0652, -0.3662, -0.0610, -0.0646,  0.0318, -0.0231]],
       dtype=torch.float64)
	q_value: tensor([[0.3607]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2224165327953288, distance: 1.009090696992352 entropy -5.055329761475467
epoch: 2, step: 50
	action: tensor([[ 0.0005, -0.0617, -0.3671, -0.0899,  0.0701,  0.0126,  0.0202]],
       dtype=torch.float64)
	q_value: tensor([[0.3594]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22191982516125175, distance: 1.0094129408774692 entropy -5.053896394907026
epoch: 2, step: 51
	action: tensor([[ 0.0161, -0.0605, -0.3672, -0.0674, -0.0473, -0.0635, -0.0440]],
       dtype=torch.float64)
	q_value: tensor([[0.3579]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23765850769148789, distance: 0.9991517934242538 entropy -5.053149689575581
epoch: 2, step: 52
	action: tensor([[ 0.0200, -0.0584, -0.3651, -0.0671, -0.0741, -0.1093, -0.0166]],
       dtype=torch.float64)
	q_value: tensor([[0.3621]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23981926164372003, distance: 0.9977348082720882 entropy -5.057532890269999
epoch: 2, step: 53
	action: tensor([[ 0.0038, -0.0627, -0.3645, -0.0720, -0.0249, -0.0519, -0.0029]],
       dtype=torch.float64)
	q_value: tensor([[0.3620]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2223661931692672, distance: 1.0091233599979217 entropy -5.059778516338812
epoch: 2, step: 54
	action: tensor([[ 0.0169, -0.0571, -0.3656, -0.0568,  0.0429, -0.0126, -0.0163]],
       dtype=torch.float64)
	q_value: tensor([[0.3609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24749059017274744, distance: 0.9926877457532877 entropy -5.056752084487269
epoch: 2, step: 55
	action: tensor([[ 0.0076, -0.0709, -0.3653, -0.0714, -0.1485, -0.0808, -0.0754]],
       dtype=torch.float64)
	q_value: tensor([[0.3603]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22155016624792745, distance: 1.0096526939226709 entropy -5.057899236701545
epoch: 2, step: 56
	action: tensor([[ 0.0206, -0.0584, -0.3662, -0.0771, -0.1096, -0.0690, -0.0286]],
       dtype=torch.float64)
	q_value: tensor([[0.3647]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2426285190146218, distance: 0.9958895311879855 entropy -5.055170230459815
epoch: 2, step: 57
	action: tensor([[-0.0118, -0.0641, -0.3655, -0.0767, -0.0706,  0.0342, -0.0540]],
       dtype=torch.float64)
	q_value: tensor([[0.3605]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21416658015745416, distance: 1.0144296643192667 entropy -5.056666944477889
epoch: 2, step: 58
	action: tensor([[-0.0057, -0.0630, -0.3679, -0.0853, -0.0978,  0.0552, -0.0410]],
       dtype=torch.float64)
	q_value: tensor([[0.3600]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22101880753698433, distance: 1.0099972223766591 entropy -5.051544734597007
epoch: 2, step: 59
	action: tensor([[ 0.0057, -0.0687, -0.3685, -0.0681,  0.0491, -0.0674, -0.0419]],
       dtype=torch.float64)
	q_value: tensor([[0.3581]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2169160658962248, distance: 1.0126534584526594 entropy -5.05052444577637
epoch: 2, step: 60
	action: tensor([[-0.0228, -0.0790, -0.3658, -0.0616, -0.0075, -0.0078, -0.0403]],
       dtype=torch.float64)
	q_value: tensor([[0.3649]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1896585963492975, distance: 1.0301268520154248 entropy -5.056641456330533
epoch: 2, step: 61
	action: tensor([[-0.0047, -0.0568, -0.3670, -0.0584, -0.1343, -0.1057, -0.0807]],
       dtype=torch.float64)
	q_value: tensor([[0.3637]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22193727192549983, distance: 1.009401623863554 entropy -5.054011037410315
epoch: 2, step: 62
	action: tensor([[ 0.0076, -0.0558, -0.3660, -0.0565, -0.0278, -0.0514, -0.0434]],
       dtype=torch.float64)
	q_value: tensor([[0.3665]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.236193682909828, distance: 1.000111258398688 entropy -5.056235216254518
epoch: 2, step: 63
	action: tensor([[-0.0313, -0.0741, -0.3651, -0.0850, -0.0908, -0.0413, -0.0145]],
       dtype=torch.float64)
	q_value: tensor([[0.3624]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17678762492942157, distance: 1.038275577234642 entropy -5.057744965153241
epoch: 2, step: 64
	action: tensor([[-0.0389, -0.1107, -0.7623, -0.1176, -0.0689, -0.0720, -0.0655]],
       dtype=torch.float64)
	q_value: tensor([[0.7498]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17583449423723319, distance: 1.0388764707995128 entropy -4.5057066881775745
epoch: 2, step: 65
	action: tensor([[-4.4978e-03, -6.6440e-02, -4.0042e-01, -7.6974e-02, -7.5235e-03,
         -8.6846e-04, -2.7521e-04]], dtype=torch.float64)
	q_value: tensor([[0.4042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21737103409426684, distance: 1.0123592421889798 entropy -5.000150542031804
epoch: 2, step: 66
	action: tensor([[-0.0061, -0.0561, -0.3685, -0.0680, -0.0953, -0.0031, -0.0363]],
       dtype=torch.float64)
	q_value: tensor([[0.3617]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2247809711700256, distance: 1.0075553317303292 entropy -5.051381991953208
epoch: 2, step: 67
	action: tensor([[ 0.0400, -0.0693, -0.3671, -0.0698, -0.0102, -0.0430, -0.0393]],
       dtype=torch.float64)
	q_value: tensor([[0.3606]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25459244264885794, distance: 0.9879923663310536 entropy -5.053049642618669
epoch: 2, step: 68
	action: tensor([[-0.0025, -0.0643, -0.3647, -0.0485,  0.0547, -0.0047, -0.0034]],
       dtype=torch.float64)
	q_value: tensor([[0.3605]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22467274901816525, distance: 1.0076256576619589 entropy -5.058705698994828
epoch: 2, step: 69
	action: tensor([[-0.0216, -0.0644, -0.3658, -0.0748, -0.0091, -0.0234, -0.0191]],
       dtype=torch.float64)
	q_value: tensor([[0.3615]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19716679849529284, distance: 1.02534343614741 entropy -5.056967638483151
epoch: 2, step: 70
	action: tensor([[-0.0054, -0.0691, -0.3670, -0.0666, -0.1128, -0.0324, -0.0131]],
       dtype=torch.float64)
	q_value: tensor([[0.3622]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21314106800572463, distance: 1.0150913635686785 entropy -5.053492682187282
epoch: 2, step: 71
	action: tensor([[-0.0077, -0.0469, -0.3664, -0.0596, -0.0307, -0.0498, -0.0305]],
       dtype=torch.float64)
	q_value: tensor([[0.3611]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22685335727132627, distance: 1.0062076861336813 entropy -5.05482031587852
epoch: 2, step: 72
	action: tensor([[-0.0228, -0.0613, -0.3657, -0.0416, -0.1227, -0.0726, -0.0108]],
       dtype=torch.float64)
	q_value: tensor([[0.3621]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20480237445844174, distance: 1.0204558754872732 entropy -5.056320972761496
epoch: 2, step: 73
	action: tensor([[ 0.0199, -0.0647, -0.3660, -0.0720, -0.0098,  0.0339,  0.0295]],
       dtype=torch.float64)
	q_value: tensor([[0.3641]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24647870648101244, distance: 0.9933549447791704 entropy -5.05660147376832
epoch: 2, step: 74
	action: tensor([[ 0.0093, -0.0653, -0.3661, -0.0634, -0.0789,  0.0688,  0.0092]],
       dtype=torch.float64)
	q_value: tensor([[0.3550]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24114252396940716, distance: 0.996866041234281 entropy -5.056573588152808
epoch: 2, step: 75
	action: tensor([[-0.0147, -0.0594, -0.3671, -0.0628, -0.1317, -0.1124, -0.0096]],
       dtype=torch.float64)
	q_value: tensor([[0.3554]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2076521290276898, distance: 1.0186257273722583 entropy -5.05479800261751
epoch: 2, step: 76
	action: tensor([[-0.0235, -0.0607, -0.3661, -0.0472, -0.0557, -0.1751, -0.0335]],
       dtype=torch.float64)
	q_value: tensor([[0.3638]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19374714991812214, distance: 1.0275248284849938 entropy -5.056551407825482
epoch: 2, step: 77
	action: tensor([[-0.0009, -0.0700, -0.3654, -0.0714, -0.0154, -0.1284, -0.0288]],
       dtype=torch.float64)
	q_value: tensor([[0.3693]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20356473925707408, distance: 1.0212496788412395 entropy -5.059456419254935
epoch: 2, step: 78
	action: tensor([[-0.0033, -0.0649, -0.3652, -0.0475, -0.1594, -0.1075, -0.0773]],
       dtype=torch.float64)
	q_value: tensor([[0.3657]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2201576045281245, distance: 1.0105553695967857 entropy -5.058396086141633
epoch: 2, step: 79
	action: tensor([[-0.0052, -0.0504, -0.3657, -0.0474, -0.0846, -0.0502, -0.0613]],
       dtype=torch.float64)
	q_value: tensor([[0.3670]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2310552218388664, distance: 1.0034697135927897 entropy -5.057312190622311
epoch: 2, step: 80
	action: tensor([[ 0.0252, -0.0652, -0.3657, -0.0548, -0.0496, -0.0552, -0.0113]],
       dtype=torch.float64)
	q_value: tensor([[0.3638]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24652343128627485, distance: 0.9933254643476903 entropy -5.056317309149999
epoch: 2, step: 81
	action: tensor([[ 0.0200, -0.0755, -0.3644, -0.0387, -0.0627, -0.0982, -0.0526]],
       dtype=torch.float64)
	q_value: tensor([[0.3607]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23371007854744963, distance: 1.0017359273420052 entropy -5.059691767523144
epoch: 2, step: 82
	action: tensor([[ 0.0027, -0.0615, -0.3640, -0.0714, -0.0166, -0.1476, -0.0830]],
       dtype=torch.float64)
	q_value: tensor([[0.3656]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21251626020087433, distance: 1.0154943018377263 entropy -5.061221754514749
epoch: 2, step: 83
	action: tensor([[-0.0209, -0.0588, -0.3656, -0.0644,  0.1208, -0.0596, -0.0426]],
       dtype=torch.float64)
	q_value: tensor([[0.3694]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1969476416240059, distance: 1.0254833753812158 entropy -5.0570591797009685
epoch: 2, step: 84
	action: tensor([[ 0.0150, -0.0592, -0.3671, -0.0503, -0.0448, -0.0762, -0.0593]],
       dtype=torch.float64)
	q_value: tensor([[0.3672]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24041980040242117, distance: 0.997340627798455 entropy -5.053133564837959
epoch: 2, step: 85
	action: tensor([[ 0.0191, -0.0547, -0.3647, -0.0617, -0.0567,  0.0115, -0.0223]],
       dtype=torch.float64)
	q_value: tensor([[0.3642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2540363729202909, distance: 0.9883608159253863 entropy -5.059013253025266
epoch: 2, step: 86
	action: tensor([[-0.0011, -0.0605, -0.3656, -0.0723,  0.0162, -0.0673, -0.0817]],
       dtype=torch.float64)
	q_value: tensor([[0.3577]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21665761637134873, distance: 1.0128205530659082 entropy -5.056769434173999
epoch: 2, step: 87
	action: tensor([[-0.0025, -0.0589, -0.3660, -0.0736, -0.1050, -0.0512,  0.0087]],
       dtype=torch.float64)
	q_value: tensor([[0.3656]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22071438107327124, distance: 1.0101945569540167 entropy -5.055422066830145
epoch: 2, step: 88
	action: tensor([[ 0.0175, -0.0719, -0.3662, -0.0614,  0.0221, -0.1380, -0.0850]],
       dtype=torch.float64)
	q_value: tensor([[0.3597]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22039737615341348, distance: 1.0104000041457888 entropy -5.0554328198675105
epoch: 2, step: 89
	action: tensor([[-0.0285, -0.0583, -0.3649, -0.0693, -0.0699, -0.0577, -0.0168]],
       dtype=torch.float64)
	q_value: tensor([[0.3693]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1940327918720809, distance: 1.0273427948957556 entropy -5.059133206565963
epoch: 2, step: 90
	action: tensor([[-0.0177, -0.0649, -0.3667, -0.0737,  0.0104, -0.0707, -0.0278]],
       dtype=torch.float64)
	q_value: tensor([[0.3630]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1952309103190042, distance: 1.0265789076523582 entropy -5.053912086193827
epoch: 2, step: 91
	action: tensor([[-0.0050, -0.0629, -0.3664, -0.0638,  0.0283, -0.0063, -0.0996]],
       dtype=torch.float64)
	q_value: tensor([[0.3648]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22052132580910022, distance: 1.0103196787934 entropy -5.055023924810314
epoch: 2, step: 92
	action: tensor([[-0.0012, -0.0717, -0.3671, -0.0304, -0.0398, -0.1436, -0.0178]],
       dtype=torch.float64)
	q_value: tensor([[0.3647]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21230205577513828, distance: 1.0156324053772545 entropy -5.053221785231649
epoch: 2, step: 93
	action: tensor([[-0.0177, -0.0693, -0.3642, -0.0725, -0.0345, -0.0547, -0.0283]],
       dtype=torch.float64)
	q_value: tensor([[0.3678]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.194853048096149, distance: 1.026819883523859 entropy -5.062302090375233
epoch: 2, step: 94
	action: tensor([[ 0.0051, -0.0634, -0.3664, -0.0834, -0.1135, -0.0995, -0.0250]],
       dtype=torch.float64)
	q_value: tensor([[0.3637]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21925066232561985, distance: 1.0111428273961103 entropy -5.0545043011759265
epoch: 2, step: 95
	action: tensor([[ 0.0075, -0.0728, -0.3660, -0.0693, -0.0597, -0.0176, -0.0390]],
       dtype=torch.float64)
	q_value: tensor([[0.3624]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22330024797561, distance: 1.0085171236625352 entropy -5.055836612946214
epoch: 2, step: 96
	action: tensor([[ 0.0371, -0.0837, -0.7623, -0.1012, -0.0534, -0.1552,  0.0275]],
       dtype=torch.float64)
	q_value: tensor([[0.7498]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27601725617458595, distance: 0.9736902039730352 entropy -4.5057066881775745
epoch: 2, step: 97
	action: tensor([[-0.0214, -0.0606, -0.3952, -0.0687, -0.0640,  0.1578, -0.0516]],
       dtype=torch.float64)
	q_value: tensor([[0.3991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22249829341164862, distance: 1.0090376441371522 entropy -5.008960498882772
epoch: 2, step: 98
	action: tensor([[ 0.0075, -0.0565, -0.3727, -0.0576, -0.0976, -0.1388,  0.0129]],
       dtype=torch.float64)
	q_value: tensor([[0.3584]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2307269204640573, distance: 1.003683906692031 entropy -5.0444465157310825
epoch: 2, step: 99
	action: tensor([[ 0.0324, -0.0626, -0.3652, -0.0592, -0.0818, -0.0009, -0.0634]],
       dtype=torch.float64)
	q_value: tensor([[0.3636]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26102015613430674, distance: 0.9837233718489166 entropy -5.0599368772829285
epoch: 2, step: 100
	action: tensor([[-0.0046, -0.0576, -0.3654, -0.0797, -0.0989,  0.0746, -0.0125]],
       dtype=torch.float64)
	q_value: tensor([[0.3598]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22903799811713055, distance: 1.0047850857344423 entropy -5.056864832573811
epoch: 2, step: 101
	action: tensor([[ 0.0089, -0.0540, -0.3685, -0.0796, -0.0574,  0.0295, -0.0060]],
       dtype=torch.float64)
	q_value: tensor([[0.3559]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24176883351667144, distance: 0.9964545834735998 entropy -5.051319301701759
epoch: 2, step: 102
	action: tensor([[-0.0104, -0.0675, -0.3671, -0.0486,  0.0198, -0.0217, -0.0403]],
       dtype=torch.float64)
	q_value: tensor([[0.3567]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21266196721005293, distance: 1.0154003497501238 entropy -5.053745049298382
epoch: 2, step: 103
	action: tensor([[-0.0175, -0.0627, -0.3660, -0.0647, -0.0052,  0.0454, -0.0505]],
       dtype=torch.float64)
	q_value: tensor([[0.3639]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21377341168519148, distance: 1.0146834025012907 entropy -5.05642004356704
epoch: 2, step: 104
	action: tensor([[ 0.0177, -0.0594, -0.3676, -0.0672, -0.1325,  0.0168, -0.0226]],
       dtype=torch.float64)
	q_value: tensor([[0.3602]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24831991006264365, distance: 0.9921405880512156 entropy -5.0527994910279
epoch: 2, step: 105
	action: tensor([[-0.0372, -0.0588, -0.3666, -0.0684, -0.0197, -0.1459, -0.0749]],
       dtype=torch.float64)
	q_value: tensor([[0.3577]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17573942730323133, distance: 1.0389363859211138 entropy -5.0547428111598895
epoch: 2, step: 106
	action: tensor([[ 0.0062, -0.0669, -0.3669, -0.0664, -0.0330, -0.0471,  0.0256]],
       dtype=torch.float64)
	q_value: tensor([[0.3712]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22332409112647778, distance: 1.0085016438001977 entropy -5.0540734199286685
epoch: 2, step: 107
	action: tensor([[ 0.0094, -0.0674, -0.3653, -0.0772, -0.0460, -0.1185, -0.0496]],
       dtype=torch.float64)
	q_value: tensor([[0.3595]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21722169148239312, distance: 1.0124558276516642 entropy -5.0577111325079676
epoch: 2, step: 108
	action: tensor([[ 0.0157, -0.0583, -0.3652, -0.0514, -0.1235, -0.0404, -0.0304]],
       dtype=torch.float64)
	q_value: tensor([[0.3649]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24661102392266887, distance: 0.9932677249774977 entropy -5.0577853428245305
epoch: 2, step: 109
	action: tensor([[-0.0102, -0.0626, -0.3653, -0.0483, -0.0457,  0.0194,  0.0226]],
       dtype=torch.float64)
	q_value: tensor([[0.3611]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2216588242588451, distance: 1.009582226516012 entropy -5.057434446893813
epoch: 2, step: 110
	action: tensor([[ 0.0298, -0.0537, -0.3665, -0.0577, -0.1001, -0.0797, -0.0051]],
       dtype=torch.float64)
	q_value: tensor([[0.3586]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25891262518351255, distance: 0.9851251363158804 entropy -5.055939692682785
epoch: 2, step: 111
	action: tensor([[ 0.0022, -0.0641, -0.3645, -0.0579, -0.0139, -0.1051, -0.0541]],
       dtype=torch.float64)
	q_value: tensor([[0.3600]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21732268429051227, distance: 1.0123905128273463 entropy -5.05982452488468
epoch: 2, step: 112
	action: tensor([[ 0.0302, -0.0637, -0.3651, -0.0666, -0.0147, -0.1482, -0.0123]],
       dtype=torch.float64)
	q_value: tensor([[0.3663]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2386055542418204, distance: 0.99853098416242 entropy -5.058440195638268
epoch: 2, step: 113
	action: tensor([[-0.0006, -0.0590, -0.3640, -0.0474, -0.0527, -0.0637, -0.0204]],
       dtype=torch.float64)
	q_value: tensor([[0.3642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22662464552554273, distance: 1.0063565029861232 entropy -5.061729359463422
epoch: 2, step: 114
	action: tensor([[-0.0063, -0.0673, -0.3650, -0.0631, -0.1163, -0.0327, -0.0112]],
       dtype=torch.float64)
	q_value: tensor([[0.3630]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2144098913290612, distance: 1.0142726073831283 entropy -5.058382452547958
epoch: 2, step: 115
	action: tensor([[-0.0108, -0.0667, -0.3664, -0.0545, -0.0384,  0.0328,  0.0465]],
       dtype=torch.float64)
	q_value: tensor([[0.3613]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2175290392241639, distance: 1.0122570443181913 entropy -5.054935536707442
epoch: 2, step: 116
	action: tensor([[ 0.0008, -0.0560, -0.3668, -0.0718, -0.1581,  0.1413, -0.0294]],
       dtype=torch.float64)
	q_value: tensor([[0.3569]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2421037159646554, distance: 0.9962345107449339 entropy -5.055661770331608
epoch: 2, step: 117
	action: tensor([[-0.0020, -0.0642, -0.3702, -0.0589, -0.0841, -0.0598, -0.0446]],
       dtype=torch.float64)
	q_value: tensor([[0.3541]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21996894417636803, distance: 1.0106775995520487 entropy -5.04899530479111
epoch: 2, step: 118
	action: tensor([[ 0.0179, -0.0600, -0.3658, -0.0661, -0.0547,  0.0071, -0.0404]],
       dtype=torch.float64)
	q_value: tensor([[0.3634]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24734883030394728, distance: 0.9927812440244858 entropy -5.0561903866677165
epoch: 2, step: 119
	action: tensor([[-0.0568, -0.0661, -0.3658, -0.0740,  0.0370, -0.0468, -0.0343]],
       dtype=torch.float64)
	q_value: tensor([[0.3589]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1552609717551542, distance: 1.0517632044989846 entropy -5.056102102025002
epoch: 2, step: 120
	action: tensor([[ 0.0345, -0.0717, -0.3681, -0.0777, -0.0826, -0.1171, -0.0102]],
       dtype=torch.float64)
	q_value: tensor([[0.3665]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2402993547959139, distance: 0.9974196981423266 entropy -5.050999619535512
epoch: 2, step: 121
	action: tensor([[-0.0233, -0.0641, -0.3645, -0.0647,  0.0206, -0.0773, -0.0277]],
       dtype=torch.float64)
	q_value: tensor([[0.3613]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19087855085373862, distance: 1.0293511411657812 entropy -5.060047055092478
epoch: 2, step: 122
	action: tensor([[ 0.0014, -0.0653, -0.3661, -0.0656,  0.0160, -0.0796, -0.0201]],
       dtype=torch.float64)
	q_value: tensor([[0.3655]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21503710407645815, distance: 1.0138676304553584 entropy -5.0560167936808815
epoch: 2, step: 123
	action: tensor([[-0.0066, -0.0591, -0.3652, -0.0903,  0.0287, -0.0463,  0.0006]],
       dtype=torch.float64)
	q_value: tensor([[0.3637]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20915613919208487, distance: 1.0176585062787833 entropy -5.058001612528097
epoch: 2, step: 124
	action: tensor([[-0.0341, -0.0587, -0.3665, -0.0556, -0.0415, -0.1398, -0.0527]],
       dtype=torch.float64)
	q_value: tensor([[0.3608]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18348540200479202, distance: 1.0340431693040832 entropy -5.054614586417304
epoch: 2, step: 125
	action: tensor([[-0.0122, -0.0612, -0.3662, -0.0787, -0.0923, -0.0377,  0.0211]],
       dtype=torch.float64)
	q_value: tensor([[0.3695]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20840737241644947, distance: 1.0181401491414088 entropy -5.056310321174979
epoch: 2, step: 126
	action: tensor([[ 0.0050, -0.0616, -0.3667, -0.0688, -0.0370, -0.1386, -0.0006]],
       dtype=torch.float64)
	q_value: tensor([[0.3592]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21721535011705861, distance: 1.0124599286462979 entropy -5.054113487033583
epoch: 2, step: 127
	action: tensor([[-0.0152, -0.0763, -0.3648, -0.0628,  0.0240, -0.0183, -0.0162]],
       dtype=torch.float64)
	q_value: tensor([[0.3639]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19721849209824627, distance: 1.0253104252121246 entropy -5.059763428379539
LOSS epoch 2 actor 0.12880994264564463 critic 4.506429786892887 entropy 0.01
epoch: 3, step: 0
	action: tensor([[ 0.0411, -0.1049, -0.3941, -0.1201, -0.0465, -0.0200,  0.1216]],
       dtype=torch.float64)
	q_value: tensor([[1.6058]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20867262769237926, distance: 1.0179695502343384 entropy -4.459307527807132
epoch: 3, step: 1
	action: tensor([[-0.0059, -0.0611, -0.2818, -0.0103,  0.0313,  0.0515, -0.0166]],
       dtype=torch.float64)
	q_value: tensor([[0.7182]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24111404425649707, distance: 0.9968847471133125 entropy -4.700407205881647
epoch: 3, step: 2
	action: tensor([[-0.0043, -0.0992, -0.2849, -0.0134, -0.0887,  0.0100,  0.0255]],
       dtype=torch.float64)
	q_value: tensor([[0.7082]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2056141537862849, distance: 1.019934875121614 entropy -4.694201097169208
epoch: 3, step: 3
	action: tensor([[-0.0409, -0.0971, -0.2833, -0.0239, -0.1647,  0.0034,  0.0239]],
       dtype=torch.float64)
	q_value: tensor([[0.7166]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1651396706104853, distance: 1.045595259817975 entropy -4.697491162422819
epoch: 3, step: 4
	action: tensor([[-0.0063, -0.0977, -0.2828, -0.0211, -0.0912,  0.0270,  0.0519]],
       dtype=torch.float64)
	q_value: tensor([[0.7228]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20414153279441616, distance: 1.0208798076584942 entropy -4.697970572309683
epoch: 3, step: 5
	action: tensor([[-0.0345, -0.0991, -0.2834, -0.0520, -0.0699,  0.0414, -0.0430]],
       dtype=torch.float64)
	q_value: tensor([[0.7118]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16654686401203356, distance: 1.0447136900333813 entropy -4.69707819514196
epoch: 3, step: 6
	action: tensor([[ 0.0928, -0.1044, -0.2839, -0.0738,  0.0134,  0.0122,  0.0135]],
       dtype=torch.float64)
	q_value: tensor([[0.7201]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27454087265834426, distance: 0.9746824983673711 entropy -4.695280852776833
epoch: 3, step: 7
	action: tensor([[-0.0122, -0.1252, -0.2843, -0.0701, -0.1076, -0.0274,  0.0196]],
       dtype=torch.float64)
	q_value: tensor([[0.6996]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15382380884142732, distance: 1.0526575118894324 entropy -4.695309964103781
epoch: 3, step: 8
	action: tensor([[-0.0048, -0.0951, -0.2836, -0.0781, -0.0767, -0.0010, -0.0491]],
       dtype=torch.float64)
	q_value: tensor([[0.7212]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18694474977694497, distance: 1.0318503660135756 entropy -4.695579251646305
epoch: 3, step: 9
	action: tensor([[-0.0656, -0.0867, -0.2836, -0.0286, -0.0258, -0.0170, -0.0378]],
       dtype=torch.float64)
	q_value: tensor([[0.7193]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1419428311090305, distance: 1.0600218205812655 entropy -4.696069912485323
epoch: 3, step: 10
	action: tensor([[-0.0447, -0.0764, -0.2848, -0.0248, -0.0343,  0.0401, -0.0063]],
       dtype=torch.float64)
	q_value: tensor([[0.7271]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18158733431793805, distance: 1.0352443386662848 entropy -4.693175435119239
epoch: 3, step: 11
	action: tensor([[-0.0147, -0.0983, -0.2842, -0.0496, -0.0214,  0.0275, -0.0188]],
       dtype=torch.float64)
	q_value: tensor([[0.7165]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18656313605786579, distance: 1.0320924910486835 entropy -4.694783175650792
epoch: 3, step: 12
	action: tensor([[ 0.0140, -0.1134, -0.2844, -0.0919, -0.0598, -0.0807,  0.0560]],
       dtype=torch.float64)
	q_value: tensor([[0.7155]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.174659699521306, distance: 1.0396166327970044 entropy -4.69448344100752
epoch: 3, step: 13
	action: tensor([[ 0.0028, -0.0886, -0.2842, -0.0850, -0.0511,  0.0925,  0.0187]],
       dtype=torch.float64)
	q_value: tensor([[0.7154]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21047704381592336, distance: 1.0168082805507381 entropy -4.69408777283562
epoch: 3, step: 14
	action: tensor([[-0.0799, -0.0887, -0.2838, -0.0385, -0.0212,  0.0021,  0.0565]],
       dtype=torch.float64)
	q_value: tensor([[0.7024]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12320482294138912, distance: 1.0715335399498978 entropy -4.695128148919053
epoch: 3, step: 15
	action: tensor([[ 0.0211, -0.1179, -0.2851, -0.0639, -0.0726,  0.0472,  0.0324]],
       dtype=torch.float64)
	q_value: tensor([[0.7180]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20504815929577114, distance: 1.0202981587405224 entropy -4.691607653963053
epoch: 3, step: 16
	action: tensor([[ 0.0604, -0.0954, -0.2835, -0.0793, -0.0188, -0.0557, -0.0122]],
       dtype=torch.float64)
	q_value: tensor([[0.7084]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24153304413644705, distance: 0.9966095066394133 entropy -4.696289606822143
epoch: 3, step: 17
	action: tensor([[-0.0363, -0.0938, -0.2840, -0.0612, -0.1123, -0.0242,  0.0311]],
       dtype=torch.float64)
	q_value: tensor([[0.7115]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1576161842100069, distance: 1.0502959733527626 entropy -4.695608622680099
epoch: 3, step: 18
	action: tensor([[-0.0129, -0.1085, -0.2835, -0.0903, -0.0402, -0.0872, -0.0157]],
       dtype=torch.float64)
	q_value: tensor([[0.7196]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1513420580900342, distance: 1.0542000516702423 entropy -4.695677917955286
epoch: 3, step: 19
	action: tensor([[ 0.0386, -0.1044, -0.2846, -0.0758, -0.0231, -0.1570, -0.0264]],
       dtype=torch.float64)
	q_value: tensor([[0.7251]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20052766814148437, distance: 1.0231950074914509 entropy -4.693054751892928
epoch: 3, step: 20
	action: tensor([[ 0.0260, -0.1111, -0.2845, -0.0765, -0.0068, -0.0936, -0.0055]],
       dtype=torch.float64)
	q_value: tensor([[0.7277]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19001100982727637, distance: 1.0299028291136698 entropy -4.694337288704252
epoch: 3, step: 21
	action: tensor([[-0.0427, -0.1024, -0.2847, -0.0365,  0.0286, -0.0234,  0.0161]],
       dtype=torch.float64)
	q_value: tensor([[0.7213]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1489269868957004, distance: 1.0556989827032988 entropy -4.693530140250723
epoch: 3, step: 22
	action: tensor([[ 0.0142, -0.1116, -0.2856, -0.0210, -0.1661,  0.0222, -0.0004]],
       dtype=torch.float64)
	q_value: tensor([[0.7209]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21430232276664263, distance: 1.0143420457012222 entropy -4.691158692909697
epoch: 3, step: 23
	action: tensor([[-0.0530, -0.1142, -0.2823, -0.0713,  0.0713, -0.1648, -0.0067]],
       dtype=torch.float64)
	q_value: tensor([[0.7203]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0937544546376351, distance: 1.0893805967684342 entropy -4.700387548088355
epoch: 3, step: 24
	action: tensor([[ 0.0310, -0.1163, -0.2872, -0.0432,  0.0068,  0.0488, -0.0238]],
       dtype=torch.float64)
	q_value: tensor([[0.7377]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22387451638091715, distance: 1.0081442211273588 entropy -4.686230000926559
epoch: 3, step: 25
	action: tensor([[ 0.0078, -0.0911, -0.2845, -0.0284, -0.0708, -0.0170,  0.0235]],
       dtype=torch.float64)
	q_value: tensor([[0.7117]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21603772638136665, distance: 1.0132212164134886 entropy -4.694930864327893
epoch: 3, step: 26
	action: tensor([[ 0.0175, -0.1109, -0.2835, -0.0063,  0.0368, -0.0546,  0.0085]],
       dtype=torch.float64)
	q_value: tensor([[0.7152]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20888154892459576, distance: 1.0178351624326547 entropy -4.69702261306689
epoch: 3, step: 27
	action: tensor([[ 0.0019, -0.0947, -0.2853, -0.0596, -0.1538, -0.0989, -0.0330]],
       dtype=torch.float64)
	q_value: tensor([[0.7198]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1914450949084724, distance: 1.0289907039968476 entropy -4.693305775066628
epoch: 3, step: 28
	action: tensor([[ 0.0089, -0.1303, -0.2827, -0.0058, -0.0642,  0.0228, -0.0076]],
       dtype=torch.float64)
	q_value: tensor([[0.7301]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1987194824146783, distance: 1.024351446305546 entropy -4.698554054027677
epoch: 3, step: 29
	action: tensor([[-0.0096, -0.1037, -0.2838, -0.0869,  0.0523, -0.1467, -0.0370]],
       dtype=torch.float64)
	q_value: tensor([[0.7204]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1467535917115279, distance: 1.0570460986823949 entropy -4.6968864914861665
epoch: 3, step: 30
	action: tensor([[-0.0891, -0.1084, -0.2861, -0.0190, -0.0849,  0.1228, -0.0590]],
       dtype=torch.float64)
	q_value: tensor([[0.7316]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12088698755635574, distance: 1.0729489213252705 entropy -4.689391974170869
epoch: 3, step: 31
	action: tensor([[ 0.0252, -0.1247, -0.2845, -0.0097, -0.0605,  0.0116, -0.0193]],
       dtype=torch.float64)
	q_value: tensor([[0.7263]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21708607252978074, distance: 1.0125435295275707 entropy -4.693848098584721
epoch: 3, step: 32
	action: tensor([[ 0.0161, -0.1383, -0.3941, -0.0374, -0.1564, -0.1820, -0.0308]],
       dtype=torch.float64)
	q_value: tensor([[1.6058]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18070216047749443, distance: 1.0358040340367034 entropy -4.459307527807132
epoch: 3, step: 33
	action: tensor([[-0.0421, -0.1088, -0.2809, -0.0270, -0.0321, -0.0857, -0.0210]],
       dtype=torch.float64)
	q_value: tensor([[0.7600]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14040923237788694, distance: 1.0609686819614634 entropy -4.704938813693285
epoch: 3, step: 34
	action: tensor([[ 0.0246, -0.1414, -0.2850,  0.0026, -0.0172, -0.0092, -0.0325]],
       dtype=torch.float64)
	q_value: tensor([[0.7314]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20352440246399384, distance: 1.0212755399610558 entropy -4.692938226697363
epoch: 3, step: 35
	action: tensor([[ 0.0088, -0.1139, -0.2844, -0.0296, -0.0889, -0.1181,  0.0382]],
       dtype=torch.float64)
	q_value: tensor([[0.7241]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18634156826005865, distance: 1.0322330446063719 entropy -4.695871588373337
epoch: 3, step: 36
	action: tensor([[ 0.0229, -0.1167, -0.2837, -0.0223,  0.0085,  0.1266,  0.0301]],
       dtype=torch.float64)
	q_value: tensor([[0.7253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23601816577686718, distance: 1.0002261609424907 entropy -4.696501926835492
epoch: 3, step: 37
	action: tensor([[-0.0402, -0.0884, -0.2850, -0.0802,  0.0043, -0.0449,  0.0139]],
       dtype=torch.float64)
	q_value: tensor([[0.7015]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14680816453464685, distance: 1.0570122943137938 entropy -4.693648503860828
epoch: 3, step: 38
	action: tensor([[-0.0256, -0.1007, -0.2852, -0.0059,  0.0287,  0.0497, -0.0086]],
       dtype=torch.float64)
	q_value: tensor([[0.7188]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19060701175406392, distance: 1.0295238504887858 entropy -4.691291453915605
epoch: 3, step: 39
	action: tensor([[-0.0283, -0.1334, -0.2852, -0.0774,  0.0172,  0.0200, -0.0862]],
       dtype=torch.float64)
	q_value: tensor([[0.7152]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13444838937789483, distance: 1.0646409769075604 entropy -4.693173769657208
epoch: 3, step: 40
	action: tensor([[ 0.0021, -0.1036, -0.2854, -0.0485, -0.0534,  0.0236, -0.0857]],
       dtype=torch.float64)
	q_value: tensor([[0.7255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20033770068807233, distance: 1.0233165640462953 entropy -4.691366200811108
epoch: 3, step: 41
	action: tensor([[ 0.0250, -0.1250, -0.2839, -0.0369,  0.0561,  0.1393, -0.0308]],
       dtype=torch.float64)
	q_value: tensor([[0.7223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23168431146952306, distance: 1.0030591499369037 entropy -4.696330450408294
epoch: 3, step: 42
	action: tensor([[-0.0510, -0.1067, -0.2859, -0.0099, -0.0950, -0.0532, -0.0452]],
       dtype=torch.float64)
	q_value: tensor([[0.7051]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14463277088414406, distance: 1.0583589750309523 entropy -4.691586243193276
epoch: 3, step: 43
	action: tensor([[ 0.0055, -0.1086, -0.2839, -0.0943, -0.0369, -0.0346, -0.0080]],
       dtype=torch.float64)
	q_value: tensor([[0.7353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17544479749342146, distance: 1.0391220518931412 entropy -4.6959138217009535
epoch: 3, step: 44
	action: tensor([[ 0.0157, -0.0965, -0.2843, -0.0646,  0.0071, -0.0675, -0.0096]],
       dtype=torch.float64)
	q_value: tensor([[0.7165]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1991261844463853, distance: 1.0240914507844308 entropy -4.693847003841208
epoch: 3, step: 45
	action: tensor([[-0.0143, -0.1038, -0.2848, -0.0106,  0.0468, -0.0088, -0.0048]],
       dtype=torch.float64)
	q_value: tensor([[0.7184]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18832634357985722, distance: 1.0309733012618 entropy -4.693408310766541
epoch: 3, step: 46
	action: tensor([[-0.1213, -0.0693, -0.2855, -0.0718, -0.0539, -0.0299, -0.0113]],
       dtype=torch.float64)
	q_value: tensor([[0.7187]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08025370065628168, distance: 1.0974650987113457 entropy -4.692320687334273
epoch: 3, step: 47
	action: tensor([[ 0.0058, -0.0909, -0.2849, -0.1026, -0.0799, -0.0200,  0.0498]],
       dtype=torch.float64)
	q_value: tensor([[0.7291]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18891216065182437, distance: 1.0306011869453031 entropy -4.690637138963614
epoch: 3, step: 48
	action: tensor([[ 0.0371, -0.0929, -0.2836, -0.0637,  0.0671,  0.0074,  0.0217]],
       dtype=torch.float64)
	q_value: tensor([[0.7087]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23432960600158403, distance: 1.0013309054450283 entropy -4.695298108996327
epoch: 3, step: 49
	action: tensor([[ 0.0615, -0.0795, -0.2854, -0.0702, -0.0445, -0.0994, -0.0701]],
       dtype=torch.float64)
	q_value: tensor([[0.7040]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25441547742324044, distance: 0.9881096376911513 entropy -4.692041355069422
epoch: 3, step: 50
	action: tensor([[ 0.0358, -0.0857, -0.2836, -0.0550, -0.1021, -0.0335, -0.0456]],
       dtype=torch.float64)
	q_value: tensor([[0.7215]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23914421190564072, distance: 0.9981777102885087 entropy -4.697320831002146
epoch: 3, step: 51
	action: tensor([[ 0.0177, -0.1107, -0.2829, -0.0656, -0.0985,  0.0219,  0.0027]],
       dtype=torch.float64)
	q_value: tensor([[0.7186]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20360132674069753, distance: 1.0212262209490044 entropy -4.698940552356006
epoch: 3, step: 52
	action: tensor([[-0.0774, -0.0789, -0.2832, -0.0474, -0.1255, -0.0261,  0.0276]],
       dtype=torch.float64)
	q_value: tensor([[0.7134]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12966776945438252, distance: 1.0675770441255323 entropy -4.697195276445588
epoch: 3, step: 53
	action: tensor([[-0.0090, -0.1288, -0.2836, -0.0591, -0.0355, -0.0915,  0.0177]],
       dtype=torch.float64)
	q_value: tensor([[0.7246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14719268517972595, distance: 1.0567740777503412 entropy -4.69499383882183
epoch: 3, step: 54
	action: tensor([[-0.0505, -0.0731, -0.2848, -0.0436, -0.1230, -0.0079, -0.0309]],
       dtype=torch.float64)
	q_value: tensor([[0.7253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1667492585334962, distance: 1.0445868339712379 entropy -4.6930420425417685
epoch: 3, step: 55
	action: tensor([[-0.0340, -0.0955, -0.2832, -0.0574, -0.0925, -0.0013, -0.0129]],
       dtype=torch.float64)
	q_value: tensor([[0.7245]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16257379446337972, distance: 1.0472008036492313 entropy -4.696875958355546
epoch: 3, step: 56
	action: tensor([[ 0.0132, -0.1030, -0.2836, -0.0120,  0.0072,  0.0053, -0.0195]],
       dtype=torch.float64)
	q_value: tensor([[0.7211]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2199641331417831, distance: 1.0106807163493556 entropy -4.695579106911567
epoch: 3, step: 57
	action: tensor([[-0.0056, -0.0893, -0.2846, -0.0752, -0.0365, -0.0126,  0.0268]],
       dtype=torch.float64)
	q_value: tensor([[0.7160]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1889692151383775, distance: 1.0305649384326279 entropy -4.694959118572664
epoch: 3, step: 58
	action: tensor([[ 0.0213, -0.0947, -0.2842, -0.0602,  0.0085,  0.1104, -0.0429]],
       dtype=torch.float64)
	q_value: tensor([[0.7115]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23765104758002886, distance: 0.9991566821556054 entropy -4.694213003753835
epoch: 3, step: 59
	action: tensor([[ 0.0301, -0.0921, -0.2847, -0.0546, -0.0360, -0.0450, -0.0060]],
       dtype=torch.float64)
	q_value: tensor([[0.7051]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22503499388743808, distance: 1.0073902410244104 entropy -4.694144639188063
epoch: 3, step: 60
	action: tensor([[ 0.0176, -0.1140, -0.2839, -0.0534, -0.0793,  0.0608, -0.0426]],
       dtype=torch.float64)
	q_value: tensor([[0.7151]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21090979855700343, distance: 1.0165295749636625 entropy -4.695983263403361
epoch: 3, step: 61
	action: tensor([[ 0.0048, -0.0960, -0.2835, -0.0505, -0.2056,  0.0745,  0.0574]],
       dtype=torch.float64)
	q_value: tensor([[0.7153]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2117988241494384, distance: 1.0159567789177324 entropy -4.6971699419709285
epoch: 3, step: 62
	action: tensor([[ 0.0266, -0.0712, -0.2821, -0.0354, -0.0466, -0.0232, -0.0099]],
       dtype=torch.float64)
	q_value: tensor([[0.7092]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24766138875229105, distance: 0.9925750831895113 entropy -4.70020546148804
epoch: 3, step: 63
	action: tensor([[-0.0789, -0.1180, -0.2836, -0.0626, -0.0262, -0.1174, -0.0507]],
       dtype=torch.float64)
	q_value: tensor([[0.7124]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07891662283412615, distance: 1.0982625269407036 entropy -4.697132466894115
epoch: 3, step: 64
	action: tensor([[-0.0106, -0.1033, -0.3941, -0.0249, -0.0140, -0.0539, -0.0796]],
       dtype=torch.float64)
	q_value: tensor([[1.6058]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18270227428240537, distance: 1.0345389312892708 entropy -4.459307527807132
epoch: 3, step: 65
	action: tensor([[-0.0187, -0.1130, -0.2822, -0.0517, -0.0452, -0.0779, -0.0493]],
       dtype=torch.float64)
	q_value: tensor([[0.7494]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1555354353328553, distance: 1.0515923267875806 entropy -4.701483636023056
epoch: 3, step: 66
	action: tensor([[-0.0334, -0.1172, -0.2845, -0.0730, -0.1130,  0.0226, -0.0007]],
       dtype=torch.float64)
	q_value: tensor([[0.7300]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1436467037135809, distance: 1.0589688373266122 entropy -4.69390148966919
epoch: 3, step: 67
	action: tensor([[-0.0341, -0.0961, -0.2835, -0.0770, -0.0338,  0.0027, -0.0344]],
       dtype=torch.float64)
	q_value: tensor([[0.7210]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15603419179564026, distance: 1.0512817359367708 entropy -4.695529950306136
epoch: 3, step: 68
	action: tensor([[ 3.3943e-02, -9.9147e-02, -2.8443e-01, -2.9971e-02,  1.1063e-04,
         -6.8552e-02, -8.2543e-02]], dtype=torch.float64)
	q_value: tensor([[0.7199]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2273203261590795, distance: 1.0059037731191574 entropy -4.693495565689058
epoch: 3, step: 69
	action: tensor([[ 0.0546, -0.1051, -0.2845, -0.0353, -0.0257, -0.0314,  0.0012]],
       dtype=torch.float64)
	q_value: tensor([[0.7265]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24645900392843334, distance: 0.9933679314726208 entropy -4.695592890081541
epoch: 3, step: 70
	action: tensor([[-0.0085, -0.1214, -0.2839, -0.0599, -0.0893,  0.0163, -0.0312]],
       dtype=torch.float64)
	q_value: tensor([[0.7134]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16974941896584694, distance: 1.0427045949362115 entropy -4.69667106833627
epoch: 3, step: 71
	action: tensor([[-0.0036, -0.0909, -0.2836, -0.0007, -0.0212, -0.0690,  0.0058]],
       dtype=torch.float64)
	q_value: tensor([[0.7220]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2051545159101531, distance: 1.020229903607267 entropy -4.696256004262824
epoch: 3, step: 72
	action: tensor([[ 0.0085, -0.1210, -0.2844, -0.1107, -0.0313, -0.0249, -0.0304]],
       dtype=torch.float64)
	q_value: tensor([[0.7231]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16443404901964043, distance: 1.0460370335704317 entropy -4.69522011209293
epoch: 3, step: 73
	action: tensor([[-0.0747, -0.0999, -0.2844, -0.0878,  0.0169, -0.0706,  0.0698]],
       dtype=torch.float64)
	q_value: tensor([[0.7185]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09367908186010276, distance: 1.089425897920306 entropy -4.693418316168262
epoch: 3, step: 74
	action: tensor([[-0.0460, -0.0785, -0.2861, -0.0448, -0.0690,  0.0735, -0.0095]],
       dtype=torch.float64)
	q_value: tensor([[0.7219]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17691614674979972, distance: 1.0381945250784972 entropy -4.6879949327922645
epoch: 3, step: 75
	action: tensor([[-0.0257, -0.0771, -0.2838, -0.0728,  0.0262, -0.0173, -0.0561]],
       dtype=torch.float64)
	q_value: tensor([[0.7147]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17784780661307253, distance: 1.0376067854273634 entropy -4.6954245322106685
epoch: 3, step: 76
	action: tensor([[-0.0276, -0.0673, -0.2851, -0.0675, -0.0125, -0.0402, -0.0407]],
       dtype=torch.float64)
	q_value: tensor([[0.7197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18269564288695084, distance: 1.0345431283050184 entropy -4.692261781419753
epoch: 3, step: 77
	action: tensor([[ 0.0205, -0.0779, -0.2845, -0.0328, -0.0696,  0.0518, -0.0297]],
       dtype=torch.float64)
	q_value: tensor([[0.7211]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24800748096602232, distance: 0.9923467538051388 entropy -4.693601386132427
epoch: 3, step: 78
	action: tensor([[-0.0197, -0.0821, -0.2833, -0.0846, -0.0109, -0.0970, -0.0511]],
       dtype=torch.float64)
	q_value: tensor([[0.7112]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.165671965834021, distance: 1.0452618782415515 entropy -4.698121412058789
epoch: 3, step: 79
	action: tensor([[-0.0184, -0.1167, -0.2848, -0.0387, -0.0497,  0.0106, -0.0374]],
       dtype=torch.float64)
	q_value: tensor([[0.7270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16925953089828016, distance: 1.0430121726859287 entropy -4.692623906192958
epoch: 3, step: 80
	action: tensor([[-1.2549e-02, -9.8596e-02, -2.8412e-01, -6.3235e-02,  1.2191e-02,
         -1.5236e-01, -2.5302e-04]], dtype=torch.float64)
	q_value: tensor([[0.7231]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1568695881428388, distance: 1.0507613035027572 entropy -4.695263645542299
epoch: 3, step: 81
	action: tensor([[ 2.1194e-04, -9.0067e-02, -2.8547e-01, -7.3061e-02, -9.7062e-02,
         -1.1876e-01, -1.2306e-02]], dtype=torch.float64)
	q_value: tensor([[0.7299]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18454580388042308, distance: 1.0333714987944937 entropy -4.691321880956169
epoch: 3, step: 82
	action: tensor([[-0.0480, -0.0931, -0.2836, -0.0163, -0.0748, -0.1018, -0.0551]],
       dtype=torch.float64)
	q_value: tensor([[0.7269]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1506170985775137, distance: 1.0546502266897213 entropy -4.6962477972849275
epoch: 3, step: 83
	action: tensor([[-0.0051, -0.1162, -0.2842, -0.0652, -0.0494, -0.0392, -0.0501]],
       dtype=torch.float64)
	q_value: tensor([[0.7376]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16797451060133917, distance: 1.0438185460111997 entropy -4.695040738472334
epoch: 3, step: 84
	action: tensor([[ 0.0113, -0.0822, -0.2842, -0.0615, -0.0278, -0.0347, -0.0006]],
       dtype=torch.float64)
	q_value: tensor([[0.7253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2132340696898979, distance: 1.0150313731483818 entropy -4.694778282052262
epoch: 3, step: 85
	action: tensor([[-0.0356, -0.1061, -0.2841, -0.0470, -0.0257,  0.0008, -0.0043]],
       dtype=torch.float64)
	q_value: tensor([[0.7140]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15512247247300326, distance: 1.0518494219422796 entropy -4.695102790604407
epoch: 3, step: 86
	action: tensor([[-0.0606, -0.1012, -0.2846, -0.0808, -0.0474,  0.0453,  0.0159]],
       dtype=torch.float64)
	q_value: tensor([[0.7200]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12810714563453285, distance: 1.0685337707038207 entropy -4.6934717569744535
epoch: 3, step: 87
	action: tensor([[ 0.0080, -0.0609, -0.2845, -0.0140, -0.0257,  0.0417,  0.0073]],
       dtype=torch.float64)
	q_value: tensor([[0.7158]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25262141285485085, distance: 0.9892977440075068 entropy -4.6926202895986435
epoch: 3, step: 88
	action: tensor([[ 0.0392, -0.0645, -0.2838, -0.0725,  0.0114,  0.0346,  0.0636]],
       dtype=torch.float64)
	q_value: tensor([[0.7076]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26107865192802127, distance: 0.983684436539173 entropy -4.696792174808873
epoch: 3, step: 89
	action: tensor([[-0.0707, -0.0739, -0.2843, -0.0752, -0.0254,  0.0427, -0.0265]],
       dtype=torch.float64)
	q_value: tensor([[0.6952]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14078903704294254, distance: 1.0607342649625147 entropy -4.69444671248601
epoch: 3, step: 90
	action: tensor([[ 0.0399, -0.1022, -0.2847,  0.0040, -0.0559, -0.0250,  0.0070]],
       dtype=torch.float64)
	q_value: tensor([[0.7172]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24886079900202984, distance: 0.9917835648641574 entropy -4.69232084866365
epoch: 3, step: 91
	action: tensor([[ 0.0560, -0.0775, -0.2835, -0.0262, -0.0327, -0.1034, -0.0111]],
       dtype=torch.float64)
	q_value: tensor([[0.7167]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26330632760946593, distance: 0.9822005276057346 entropy -4.698308033411647
epoch: 3, step: 92
	action: tensor([[ 0.0246, -0.0957, -0.2838, -0.0361, -0.0206, -0.0626, -0.0865]],
       dtype=torch.float64)
	q_value: tensor([[0.7182]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22042767271432095, distance: 1.0103803711034285 entropy -4.697159410249003
epoch: 3, step: 93
	action: tensor([[-0.0713, -0.0955, -0.2842, -0.0806, -0.1371, -0.0953, -0.0093]],
       dtype=torch.float64)
	q_value: tensor([[0.7264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1075400128067413, distance: 1.0810631658175507 entropy -4.695903285991472
epoch: 3, step: 94
	action: tensor([[-0.0627, -0.1043, -0.2838,  0.0006, -0.1165,  0.0281, -0.0252]],
       dtype=torch.float64)
	q_value: tensor([[0.7337]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14630055157099142, distance: 1.0573266863052408 entropy -4.694069488918991
epoch: 3, step: 95
	action: tensor([[-0.0351, -0.0741, -0.2837, -0.0257, -0.0749, -0.0112,  0.0119]],
       dtype=torch.float64)
	q_value: tensor([[0.7282]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18655280015934095, distance: 1.0320990481460084 entropy -4.696333404543323
epoch: 3, step: 96
	action: tensor([[ 0.0266, -0.1438, -0.3941, -0.0699, -0.0469,  0.0415, -0.0159]],
       dtype=torch.float64)
	q_value: tensor([[1.6058]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18435479885829376, distance: 1.0334925157628205 entropy -4.459307527807132
epoch: 3, step: 97
	action: tensor([[ 0.0007, -0.1263, -0.2818,  0.0202, -0.0964,  0.0094, -0.0260]],
       dtype=torch.float64)
	q_value: tensor([[0.7338]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2005363737635234, distance: 1.0231894365836773 entropy -4.701797744624545
epoch: 3, step: 98
	action: tensor([[-0.0192, -0.1174, -0.2835, -0.0546, -0.0394, -0.0902, -0.0166]],
       dtype=torch.float64)
	q_value: tensor([[0.7263]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14837416115302626, distance: 1.056041798667268 entropy -4.698231326631228
epoch: 3, step: 99
	action: tensor([[ 0.0109, -0.0848, -0.2847, -0.0722, -0.1254, -0.0224, -0.0439]],
       dtype=torch.float64)
	q_value: tensor([[0.7295]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21120814828943335, distance: 1.0163373852756368 entropy -4.693416706190658
epoch: 3, step: 100
	action: tensor([[-0.0643, -0.1010, -0.2827, -0.0460, -0.0734, -0.0203,  0.0315]],
       dtype=torch.float64)
	q_value: tensor([[0.7206]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12618006585792585, distance: 1.0697139692047977 entropy -4.69869857411743
epoch: 3, step: 101
	action: tensor([[-3.0220e-03, -9.5876e-02, -2.8434e-01, -4.3040e-03,  2.5479e-04,
         -6.7851e-02, -5.1150e-03]], dtype=torch.float64)
	q_value: tensor([[0.7226]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20007302964160545, distance: 1.0234858979382837 entropy -4.69345465529828
epoch: 3, step: 102
	action: tensor([[ 0.0054, -0.0635, -0.2848, -0.0606,  0.0292,  0.0365, -0.0233]],
       dtype=torch.float64)
	q_value: tensor([[0.7233]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23304303710374163, distance: 1.0021718289569743 entropy -4.694270722703502
epoch: 3, step: 103
	action: tensor([[-0.0678, -0.1092, -0.2847, -0.0311, -0.1005, -0.0586,  0.0062]],
       dtype=torch.float64)
	q_value: tensor([[0.7070]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11711370796638676, distance: 1.0752490814193965 entropy -4.693829821811297
epoch: 3, step: 104
	action: tensor([[-0.0235, -0.0851, -0.2840, -0.0160, -0.1600,  0.0612, -0.0270]],
       dtype=torch.float64)
	q_value: tensor([[0.7323]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2018024283922364, distance: 1.0223789388010198 entropy -4.694511060506919
epoch: 3, step: 105
	action: tensor([[-0.0174, -0.1143, -0.2827, -0.1099, -0.0442,  0.0233, -0.0172]],
       dtype=torch.float64)
	q_value: tensor([[0.7199]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15084630471713123, distance: 1.0545079183051402 entropy -4.69913114068612
epoch: 3, step: 106
	action: tensor([[ 0.0568, -0.1228, -0.2844, -0.0765, -0.0176, -0.0551, -0.0289]],
       dtype=torch.float64)
	q_value: tensor([[0.7149]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2164287767480031, distance: 1.0129684810690813 entropy -4.6930378972359295
epoch: 3, step: 107
	action: tensor([[-0.0530, -0.1010, -0.2842, -0.0263, -0.0668,  0.0862, -0.0240]],
       dtype=torch.float64)
	q_value: tensor([[0.7176]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15932428621376327, distance: 1.0492305902244192 entropy -4.695135105998338
epoch: 3, step: 108
	action: tensor([[ 0.0069, -0.1003, -0.2841, -0.1073, -0.0042,  0.0423, -0.0158]],
       dtype=torch.float64)
	q_value: tensor([[0.7192]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1910107405314686, distance: 1.029267052952702 entropy -4.694880821917093
epoch: 3, step: 109
	action: tensor([[-0.0267, -0.0757, -0.2846, -0.1020,  0.0688, -0.0986,  0.0012]],
       dtype=torch.float64)
	q_value: tensor([[0.7075]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15405944513328773, distance: 1.0525109339154595 entropy -4.692958470735212
epoch: 3, step: 110
	action: tensor([[-0.0464, -0.0930, -0.2862, -0.0102, -0.0539, -0.1543, -0.0418]],
       dtype=torch.float64)
	q_value: tensor([[0.7206]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14748530964115314, distance: 1.056592756324155 entropy -4.688378654012355
epoch: 3, step: 111
	action: tensor([[-0.0594, -0.0849, -0.2847, -0.0541, -0.1918, -0.0160,  0.0280]],
       dtype=torch.float64)
	q_value: tensor([[0.7412]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14436182372878492, distance: 1.0585265852917325 entropy -4.694068796970758
epoch: 3, step: 112
	action: tensor([[-0.0341, -0.0820, -0.2826, -0.0699, -0.0462, -0.0160, -0.0681]],
       dtype=torch.float64)
	q_value: tensor([[0.7246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16746280117900214, distance: 1.0441394794995873 entropy -4.697527110744011
epoch: 3, step: 113
	action: tensor([[ 0.0832, -0.1005, -0.2842, -0.0234, -0.0857,  0.0128,  0.0064]],
       dtype=torch.float64)
	q_value: tensor([[0.7231]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28906852188767507, distance: 0.9648739143595254 entropy -4.694299269728162
epoch: 3, step: 114
	action: tensor([[ 0.0406, -0.0896, -0.2827, -0.0903, -0.1251, -0.0252, -0.0142]],
       dtype=torch.float64)
	q_value: tensor([[0.7078]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2302623463178769, distance: 1.0039869299152757 entropy -4.700224356944271
epoch: 3, step: 115
	action: tensor([[-0.0162, -0.0732, -0.2826,  0.0081, -0.1352, -0.0457, -0.0144]],
       dtype=torch.float64)
	q_value: tensor([[0.7132]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2151216737184195, distance: 1.0138130133948045 entropy -4.698796512494804
epoch: 3, step: 116
	action: tensor([[ 3.1686e-02, -9.5357e-02, -2.8275e-01, -5.3932e-02, -5.3087e-02,
          1.2810e-05,  9.5052e-04]], dtype=torch.float64)
	q_value: tensor([[0.7259]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23068774177642082, distance: 1.0037094649226836 entropy -4.699323044073808
epoch: 3, step: 117
	action: tensor([[ 0.0666, -0.0981, -0.2836, -0.0446, -0.0346, -0.1374, -0.0877]],
       dtype=torch.float64)
	q_value: tensor([[0.7114]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24669464109436, distance: 0.9932126030251153 entropy -4.696770447534566
epoch: 3, step: 118
	action: tensor([[ 0.0338, -0.0964, -0.2839, -0.0376, -0.0822, -0.1317,  0.0245]],
       dtype=torch.float64)
	q_value: tensor([[0.7301]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22120155125294572, distance: 1.0098787461712386 entropy -4.697218052641676
epoch: 3, step: 119
	action: tensor([[ 0.0069, -0.1269, -0.2835, -0.0283, -0.0677,  0.0020, -0.0031]],
       dtype=torch.float64)
	q_value: tensor([[0.7224]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18898506858632202, distance: 1.0305548660113637 entropy -4.6974201276904655
epoch: 3, step: 120
	action: tensor([[ 0.0338, -0.0975, -0.2838, -0.0231, -0.0036,  0.0348,  0.0045]],
       dtype=torch.float64)
	q_value: tensor([[0.7202]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24621635673981535, distance: 0.9935278554423203 entropy -4.696358304548112
epoch: 3, step: 121
	action: tensor([[ 0.0060, -0.1286, -0.2843, -0.0325, -0.0564,  0.0448, -0.0339]],
       dtype=torch.float64)
	q_value: tensor([[0.7081]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19180422628707516, distance: 1.0287621580494188 entropy -4.695745016619187
epoch: 3, step: 122
	action: tensor([[ 0.0208, -0.1120, -0.2840, -0.0791, -0.0790,  0.0633,  0.0012]],
       dtype=torch.float64)
	q_value: tensor([[0.7190]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2071184644409617, distance: 1.0189687036053057 entropy -4.6961588635884945
epoch: 3, step: 123
	action: tensor([[ 0.0602, -0.0955, -0.2834, -0.0595,  0.0030, -0.0966, -0.0602]],
       dtype=torch.float64)
	q_value: tensor([[0.7085]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24141946291944305, distance: 0.9966841255020248 entropy -4.696486202023467
epoch: 3, step: 124
	action: tensor([[-0.0285, -0.0912, -0.2844, -0.0957, -0.0776,  0.0009, -0.0243]],
       dtype=torch.float64)
	q_value: tensor([[0.7212]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1601651604585086, distance: 1.0487057197566008 entropy -4.69536400645747
epoch: 3, step: 125
	action: tensor([[-0.0569, -0.0667, -0.2838, -0.0672, -0.1289,  0.0729, -0.0304]],
       dtype=torch.float64)
	q_value: tensor([[0.7182]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16645152574025524, distance: 1.0447734404531956 entropy -4.6946651122559615
epoch: 3, step: 126
	action: tensor([[-0.0338, -0.0854, -0.2832, -0.0812, -0.0412,  0.0300, -0.0529]],
       dtype=torch.float64)
	q_value: tensor([[0.7163]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16777255661497648, distance: 1.0439452191292697 entropy -4.6963033337010955
epoch: 3, step: 127
	action: tensor([[ 0.0138, -0.0756, -0.2842, -0.0949, -0.1127, -0.0736,  0.0600]],
       dtype=torch.float64)
	q_value: tensor([[0.7172]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20771815613100264, distance: 1.0185832849616667 entropy -4.693978380312394
LOSS epoch 3 actor 0.27851499057681367 critic 2.7068167303002424 entropy 0.01
epoch: 4, step: 0
	action: tensor([[-0.0475, -0.0547, -0.0957, -0.0403,  0.0016, -0.0492, -0.0100]],
       dtype=torch.float64)
	q_value: tensor([[3.0280]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14816815720332277, distance: 1.0561695165084022 entropy -4.935048237859928
epoch: 4, step: 1
	action: tensor([[-0.0257, -0.0701, -0.0961,  0.0087, -0.1214,  0.0128,  0.0107]],
       dtype=torch.float64)
	q_value: tensor([[1.2539]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19670869981098882, distance: 1.0256359262269636 entropy -5.021667308964098
epoch: 4, step: 2
	action: tensor([[-0.0470, -0.0600, -0.0959, -0.0427,  0.0828,  0.1105, -0.0129]],
       dtype=torch.float64)
	q_value: tensor([[1.2554]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1875055761567792, distance: 1.0314944315730363 entropy -5.02942299522317
epoch: 4, step: 3
	action: tensor([[-0.0298, -0.0595, -0.0960, -0.0567,  0.0578,  0.0553,  0.0162]],
       dtype=torch.float64)
	q_value: tensor([[1.2216]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1850236338244049, distance: 1.0330686931904973 entropy -5.024178445768382
epoch: 4, step: 4
	action: tensor([[-0.0004, -0.0669, -0.0960,  0.0196,  0.0310,  0.0455,  0.0301]],
       dtype=torch.float64)
	q_value: tensor([[1.2214]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23995808057697354, distance: 0.9976437044078853 entropy -5.023619113871208
epoch: 4, step: 5
	action: tensor([[-0.0244, -0.0708, -0.0961, -0.0241, -0.0044,  0.0743,  0.0119]],
       dtype=torch.float64)
	q_value: tensor([[1.2255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20246714301722435, distance: 1.0219531471163958 entropy -5.024911323803678
epoch: 4, step: 6
	action: tensor([[-0.0124, -0.0687, -0.0959, -0.0164,  0.0136,  0.0538,  0.0132]],
       dtype=torch.float64)
	q_value: tensor([[1.2304]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21686832607904394, distance: 1.0126843256126685 entropy -5.026866347638028
epoch: 4, step: 7
	action: tensor([[-0.0548, -0.0757, -0.0960, -0.0367,  0.0280, -0.0097,  0.0592]],
       dtype=torch.float64)
	q_value: tensor([[1.2288]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14641849951849095, distance: 1.0572536431396924 entropy -5.025848673855997
epoch: 4, step: 8
	action: tensor([[-0.0670, -0.0692, -0.0961, -0.0080, -0.0867,  0.0223,  0.0532]],
       dtype=torch.float64)
	q_value: tensor([[1.2376]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1621090684045422, distance: 1.0474913331311886 entropy -5.019969531683932
epoch: 4, step: 9
	action: tensor([[-0.0267, -0.0583, -0.0959,  0.0355, -0.0336, -0.0003,  0.0092]],
       dtype=torch.float64)
	q_value: tensor([[1.2499]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22837375556756057, distance: 1.005217842023898 entropy -5.026469449605215
epoch: 4, step: 10
	action: tensor([[-0.0693, -0.0661, -0.0961, -0.0438, -0.0012,  0.1051,  0.0132]],
       dtype=torch.float64)
	q_value: tensor([[1.2471]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16537861178960878, distance: 1.0454456218281882 entropy -5.025216623075404
epoch: 4, step: 11
	action: tensor([[-0.0382, -0.0626, -0.0959,  0.0207, -0.0489, -0.0631,  0.0824]],
       dtype=torch.float64)
	q_value: tensor([[1.2317]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19280849529602817, distance: 1.028122786292634 entropy -5.0260921081281325
epoch: 4, step: 12
	action: tensor([[-0.0338, -0.0684, -0.0961, -0.0561, -0.0296,  0.0205,  0.0076]],
       dtype=torch.float64)
	q_value: tensor([[1.2484]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17817108097490675, distance: 1.0374027692768684 entropy -5.022891465297648
epoch: 4, step: 13
	action: tensor([[-0.0498, -0.0644, -0.0959, -0.0123, -0.0659,  0.0289,  0.0208]],
       dtype=torch.float64)
	q_value: tensor([[1.2398]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18485847773586217, distance: 1.0331733642880576 entropy -5.025632589144554
epoch: 4, step: 14
	action: tensor([[-0.0235, -0.0589, -0.0959, -0.0327, -0.1174,  0.0038, -0.0182]],
       dtype=torch.float64)
	q_value: tensor([[1.2465]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2049301313512487, distance: 1.0203738986902657 entropy -5.026594736825281
epoch: 4, step: 15
	action: tensor([[-0.0132, -0.0739, -0.0958,  0.0184, -0.0509, -0.0219, -0.0574]],
       dtype=torch.float64)
	q_value: tensor([[1.2550]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22106877855661689, distance: 1.0099648266006414 entropy -5.029975914190795
epoch: 4, step: 16
	action: tensor([[-0.0009, -0.0670, -0.0960, -0.0333,  0.0454,  0.0132,  0.0244]],
       dtype=torch.float64)
	q_value: tensor([[1.2654]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22087565283390898, distance: 1.0100900225699039 entropy -5.026726271200773
epoch: 4, step: 17
	action: tensor([[ 0.0105, -0.0739, -0.0961,  0.0033, -0.0711,  0.0478, -0.0530]],
       dtype=torch.float64)
	q_value: tensor([[1.2237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2530592010946531, distance: 0.9890079535531386 entropy -5.022616722995315
epoch: 4, step: 18
	action: tensor([[-0.0531, -0.0650, -0.0959, -0.0067, -0.0177, -0.0576,  0.0089]],
       dtype=torch.float64)
	q_value: tensor([[1.2509]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1654956703503656, distance: 1.0453723055599566 entropy -5.030676333234256
epoch: 4, step: 19
	action: tensor([[-0.0352, -0.0679, -0.0961, -0.0338, -0.0225,  0.0173,  0.0239]],
       dtype=torch.float64)
	q_value: tensor([[1.2595]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18571878659641006, distance: 1.0326280093869447 entropy -5.021385428445217
epoch: 4, step: 20
	action: tensor([[-0.0399, -0.0768, -0.0960, -0.0197, -0.0250, -0.0270,  0.0025]],
       dtype=torch.float64)
	q_value: tensor([[1.2385]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17110750015903875, distance: 1.0418514446382248 entropy -5.024701391027902
epoch: 4, step: 21
	action: tensor([[-0.0190, -0.0688, -0.0960, -0.0227, -0.0052, -0.0800, -0.0052]],
       dtype=torch.float64)
	q_value: tensor([[1.2548]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18701654461334905, distance: 1.0318048075083757 entropy -5.023146105290385
epoch: 4, step: 22
	action: tensor([[-0.0342, -0.0627, -0.0961, -0.0129, -0.0151, -0.0096,  0.0055]],
       dtype=torch.float64)
	q_value: tensor([[1.2575]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19460936408872287, distance: 1.0269752592932557 entropy -5.021736384774749
epoch: 4, step: 23
	action: tensor([[-0.0364, -0.0625, -0.0960,  0.0115, -0.0323, -0.0420, -0.0257]],
       dtype=torch.float64)
	q_value: tensor([[1.2459]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19739140932213295, distance: 1.025199994553969 entropy -5.023792532820648
epoch: 4, step: 24
	action: tensor([[ 0.0090, -0.0625, -0.0961,  0.0434, -0.0306, -0.0019, -0.0273]],
       dtype=torch.float64)
	q_value: tensor([[1.2624]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2667205297320926, distance: 0.9799218834111942 entropy -5.023880517168602
epoch: 4, step: 25
	action: tensor([[-0.0068, -0.0570, -0.0961, -0.0281,  0.0049,  0.0029,  0.0361]],
       dtype=torch.float64)
	q_value: tensor([[1.2503]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22303536952039937, distance: 1.0086890766506065 entropy -5.027023205146685
epoch: 4, step: 26
	action: tensor([[ 0.0020, -0.0689, -0.0960, -0.0284, -0.1114,  0.0370, -0.0374]],
       dtype=torch.float64)
	q_value: tensor([[1.2274]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2318680093341311, distance: 1.0029392312356895 entropy -5.02396838809392
epoch: 4, step: 27
	action: tensor([[-0.0404, -0.0729, -0.0958, -0.0497, -0.0407, -0.0144, -0.0106]],
       dtype=torch.float64)
	q_value: tensor([[1.2504]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16369818855496088, distance: 1.0464975405474937 entropy -5.031763403291126
epoch: 4, step: 28
	action: tensor([[-0.0265, -0.0668, -0.0960, -0.0510,  0.0151,  0.0839,  0.0566]],
       dtype=torch.float64)
	q_value: tensor([[1.2537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2024109906909397, distance: 1.0219891230865472 entropy -5.024759304036064
epoch: 4, step: 29
	action: tensor([[-0.0397, -0.0612, -0.0959, -0.0394,  0.0148, -0.0117, -0.0085]],
       dtype=torch.float64)
	q_value: tensor([[1.2147]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17747553634841595, distance: 1.0378416728605369 entropy -5.025436914234084
epoch: 4, step: 30
	action: tensor([[ 0.0543, -0.0610, -0.0961, -0.0128, -0.0938, -0.0561, -0.0161]],
       dtype=torch.float64)
	q_value: tensor([[1.2449]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27544059149817157, distance: 0.9740779072615772 entropy -5.022275840625139
epoch: 4, step: 31
	action: tensor([[-0.0532, -0.0465, -0.0959,  0.0216,  0.0883,  0.0420, -0.0184]],
       dtype=torch.float64)
	q_value: tensor([[1.2498]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2120524383437874, distance: 1.0157933169757336 entropy -5.030774468515299
epoch: 4, step: 32
	action: tensor([[-0.0422, -0.0753, -0.0957, -0.0186, -0.0821, -0.0265,  0.0600]],
       dtype=torch.float64)
	q_value: tensor([[3.0280]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15117000144305814, distance: 1.0543069103495573 entropy -4.935048237859928
epoch: 4, step: 33
	action: tensor([[-0.0360, -0.0640, -0.0959, -0.0074, -0.0554, -0.0212,  0.0069]],
       dtype=torch.float64)
	q_value: tensor([[1.2509]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17595652810439988, distance: 1.0387995549338902 entropy -5.025659095636494
epoch: 4, step: 34
	action: tensor([[-0.0157, -0.0601, -0.0960, -0.0361,  0.0489, -0.0473, -0.0344]],
       dtype=torch.float64)
	q_value: tensor([[1.2530]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18318589898842264, distance: 1.0342327988920355 entropy -5.025160042134039
epoch: 4, step: 35
	action: tensor([[-0.0455, -0.0672, -0.0962, -0.0413, -0.0360,  0.0738, -0.0402]],
       dtype=torch.float64)
	q_value: tensor([[1.2480]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17643589843714647, distance: 1.038497360846279 entropy -5.021075108755911
epoch: 4, step: 36
	action: tensor([[ 0.0068, -0.0748, -0.0959, -0.0164, -0.0762, -0.0136,  0.0675]],
       dtype=torch.float64)
	q_value: tensor([[1.2459]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21794171662552775, distance: 1.0119900754971298 entropy -5.027896159172711
epoch: 4, step: 37
	action: tensor([[ 0.0156, -0.0696, -0.0959, -0.0106, -0.0671,  0.0249, -0.0269]],
       dtype=torch.float64)
	q_value: tensor([[1.2370]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2457424887469133, distance: 0.9938400982759508 entropy -5.027466696988441
epoch: 4, step: 38
	action: tensor([[-0.0120, -0.0657, -0.0959,  0.0296, -0.0341,  0.0109, -0.0908]],
       dtype=torch.float64)
	q_value: tensor([[1.2456]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24032392311091832, distance: 0.9974035700005299 entropy -5.030206472015988
epoch: 4, step: 39
	action: tensor([[ 0.0183, -0.0709, -0.0960, -0.0019, -0.0403, -0.0475, -0.0314]],
       dtype=torch.float64)
	q_value: tensor([[1.2667]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24124503749870307, distance: 0.9967987059896181 entropy -5.028067829009086
epoch: 4, step: 40
	action: tensor([[-0.0394, -0.0724, -0.0960, -0.0688, -0.0065, -0.0166,  0.0069]],
       dtype=torch.float64)
	q_value: tensor([[1.2557]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15536637923330054, distance: 1.0516975823589352 entropy -5.027037555168242
epoch: 4, step: 41
	action: tensor([[-0.0422, -0.0682, -0.0960, -0.0055,  0.0178,  0.0084,  0.0271]],
       dtype=torch.float64)
	q_value: tensor([[1.2461]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18784300977118862, distance: 1.031280216539983 entropy -5.0231936893908875
epoch: 4, step: 42
	action: tensor([[ 0.0072, -0.0669, -0.0961, -0.0126, -0.0862, -0.0455, -0.0270]],
       dtype=torch.float64)
	q_value: tensor([[1.2395]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2289570238513451, distance: 1.0048378507203586 entropy -5.0223319035671805
epoch: 4, step: 43
	action: tensor([[-0.0285, -0.0627, -0.0959,  0.0047, -0.0336,  0.0572, -0.0253]],
       dtype=torch.float64)
	q_value: tensor([[1.2593]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22298806009016936, distance: 1.0087197857570571 entropy -5.028429443086131
epoch: 4, step: 44
	action: tensor([[-0.0140, -0.0640, -0.0960, -0.0240, -0.0011, -0.0223,  0.0189]],
       dtype=torch.float64)
	q_value: tensor([[1.2446]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20680576537984874, distance: 1.0191696158054269 entropy -5.027468234691514
epoch: 4, step: 45
	action: tensor([[ 0.0003, -0.0626, -0.0961, -0.0066,  0.0028, -0.0417,  0.0442]],
       dtype=torch.float64)
	q_value: tensor([[1.2392]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2257610847842123, distance: 1.0069182014550664 entropy -5.0232093838601495
epoch: 4, step: 46
	action: tensor([[-0.0189, -0.0719, -0.0961,  0.0038,  0.0301,  0.0117,  0.0278]],
       dtype=torch.float64)
	q_value: tensor([[1.2358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21436666687488493, distance: 1.0143005104691758 entropy -5.022501886978047
epoch: 4, step: 47
	action: tensor([[-0.0137, -0.0626, -0.0961,  0.0053, -0.0432,  0.0708, -0.0017]],
       dtype=torch.float64)
	q_value: tensor([[1.2333]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2413845870154505, distance: 0.9967070366226338 entropy -5.022364414485259
epoch: 4, step: 48
	action: tensor([[ 0.0106, -0.0622, -0.0959, -0.0242, -0.1167, -0.0106,  0.0104]],
       dtype=torch.float64)
	q_value: tensor([[1.2363]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23863612439458437, distance: 0.9985109383430024 entropy -5.0286031495143835
epoch: 4, step: 49
	action: tensor([[-0.0116, -0.0678, -0.0958, -0.0056, -0.0085, -0.0391, -0.0131]],
       dtype=torch.float64)
	q_value: tensor([[1.2473]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21156169345529585, distance: 1.0161095929622281 entropy -5.030984961730011
epoch: 4, step: 50
	action: tensor([[-0.0393, -0.0573, -0.0961, -0.0625,  0.0165,  0.0132, -0.0508]],
       dtype=torch.float64)
	q_value: tensor([[1.2515]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1771738216412596, distance: 1.0380320036045318 entropy -5.023967092416334
epoch: 4, step: 51
	action: tensor([[-0.0454, -0.0717, -0.0960, -0.0113,  0.0408,  0.0779,  0.0481]],
       dtype=torch.float64)
	q_value: tensor([[1.2478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19457002062950024, distance: 1.0270003429383854 entropy -5.024526074101455
epoch: 4, step: 52
	action: tensor([[ 0.0335, -0.0704, -0.0960, -0.0496,  0.0877,  0.0219,  0.0198]],
       dtype=torch.float64)
	q_value: tensor([[1.2236]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24049227687921648, distance: 0.9972930452921882 entropy -5.023735840949294
epoch: 4, step: 53
	action: tensor([[-0.0274, -0.0689, -0.0961, -0.0289, -0.0751, -0.0026,  0.0152]],
       dtype=torch.float64)
	q_value: tensor([[1.2132]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19130847420213137, distance: 1.0290776340852923 entropy -5.023166140294907
epoch: 4, step: 54
	action: tensor([[ 0.0093, -0.0634, -0.0959, -0.0177, -0.0663, -0.0045, -0.0114]],
       dtype=torch.float64)
	q_value: tensor([[1.2492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2397646779660655, distance: 0.9977706280810104 entropy -5.02733398723785
epoch: 4, step: 55
	action: tensor([[-0.0160, -0.0606, -0.0959,  0.0238, -0.0090,  0.0084,  0.0052]],
       dtype=torch.float64)
	q_value: tensor([[1.2457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23516177686356965, distance: 1.0007866079607106 entropy -5.028920625653746
epoch: 4, step: 56
	action: tensor([[-0.0452, -0.0625, -0.0961, -0.0383, -0.1016, -0.0239,  0.0170]],
       dtype=torch.float64)
	q_value: tensor([[1.2422]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17086712935946657, distance: 1.0420024971039221 entropy -5.025213922124139
epoch: 4, step: 57
	action: tensor([[-0.0025, -0.0744, -0.0959,  0.0089,  0.0932, -0.0452,  0.0261]],
       dtype=torch.float64)
	q_value: tensor([[1.2573]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21578262432687878, distance: 1.0133860545690283 entropy -5.027389415563727
epoch: 4, step: 58
	action: tensor([[-0.0334, -0.0680, -0.0963, -0.0282, -0.0689, -0.0186,  0.0204]],
       dtype=torch.float64)
	q_value: tensor([[1.2364]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18375113669573484, distance: 1.0338748909271798 entropy -5.018157620749725
epoch: 4, step: 59
	action: tensor([[-0.0524, -0.0681, -0.0959, -0.0019,  0.0938, -0.0047, -0.0136]],
       dtype=torch.float64)
	q_value: tensor([[1.2515]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.174702805847602, distance: 1.0395894836084039 entropy -5.026260114324357
epoch: 4, step: 60
	action: tensor([[ 0.0171, -0.0702, -0.0962, -0.0655,  0.0025,  0.0500,  0.0528]],
       dtype=torch.float64)
	q_value: tensor([[1.2443]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22565709456072292, distance: 1.0069858202036341 entropy -5.018169476448156
epoch: 4, step: 61
	action: tensor([[-0.0075, -0.0762, -0.0959, -0.0098, -0.0911,  0.0194,  0.0158]],
       dtype=torch.float64)
	q_value: tensor([[1.2121]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21979391606790777, distance: 1.010790984182699 entropy -5.026366375233559
epoch: 4, step: 62
	action: tensor([[-0.0046, -0.0665, -0.0959,  0.0658, -0.1005,  0.0778,  0.0806]],
       dtype=torch.float64)
	q_value: tensor([[1.2457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2738161022538217, distance: 0.9751692554102495 entropy -5.028758872717485
epoch: 4, step: 63
	action: tensor([[ 0.0197, -0.0661, -0.0958,  0.0067, -0.0011, -0.0594,  0.0587]],
       dtype=torch.float64)
	q_value: tensor([[1.2307]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24371202461786678, distance: 0.9951769098937366 entropy -5.0312125161453025
epoch: 4, step: 64
	action: tensor([[-0.0781, -0.0689, -0.0957,  0.0225, -0.0495,  0.0400,  0.0541]],
       dtype=torch.float64)
	q_value: tensor([[3.0280]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14332110464300374, distance: 1.0591701365464516 entropy -4.935048237859928
epoch: 4, step: 65
	action: tensor([[-0.0269, -0.0763, -0.0959, -0.0471, -0.0274,  0.0086,  0.0207]],
       dtype=torch.float64)
	q_value: tensor([[1.2478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1552794772865792, distance: 1.0517516840512615 entropy -5.024872934492522
epoch: 4, step: 66
	action: tensor([[-0.0184, -0.0630, -0.0959, -0.0254,  0.0203,  0.0376, -0.0374]],
       dtype=torch.float64)
	q_value: tensor([[1.2402]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.193281109757229, distance: 1.0278217568344985 entropy -5.025210159747378
epoch: 4, step: 67
	action: tensor([[-0.0271, -0.0654, -0.0960,  0.0177, -0.0341,  0.0298,  0.0046]],
       dtype=torch.float64)
	q_value: tensor([[1.2403]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20311186516489999, distance: 1.0215399923265889 entropy -5.025628550584522
epoch: 4, step: 68
	action: tensor([[-0.0174, -0.0720, -0.0960, -0.0011,  0.0024,  0.0627, -0.0836]],
       dtype=torch.float64)
	q_value: tensor([[1.2437]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21192542176990814, distance: 1.015875186248514 entropy -5.0261570032464515
epoch: 4, step: 69
	action: tensor([[-0.0066, -0.0548, -0.0960,  0.0355, -0.0033,  0.0730, -0.0114]],
       dtype=torch.float64)
	q_value: tensor([[1.2524]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25715099034074473, distance: 0.9862953091772139 entropy -5.027428173225549
epoch: 4, step: 70
	action: tensor([[-0.0064, -0.0573, -0.0960,  0.0272,  0.0189,  0.0135,  0.0287]],
       dtype=torch.float64)
	q_value: tensor([[1.2346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24059584570731052, distance: 0.9972250459880027 entropy -5.027962043338336
epoch: 4, step: 71
	action: tensor([[-0.0494, -0.0717, -0.0961,  0.0387,  0.0048,  0.0079, -0.0107]],
       dtype=torch.float64)
	q_value: tensor([[1.2313]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1940129986012612, distance: 1.0273554097696085 entropy -5.024169722837518
epoch: 4, step: 72
	action: tensor([[ 0.0144, -0.0659, -0.0961, -0.0140, -0.0542,  0.0558,  0.0483]],
       dtype=torch.float64)
	q_value: tensor([[1.2547]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2501719790032858, distance: 0.9909175637529505 entropy -5.0229659044815085
epoch: 4, step: 73
	action: tensor([[-0.0780, -0.0642, -0.0959, -0.0739, -0.0393, -0.0029,  0.0182]],
       dtype=torch.float64)
	q_value: tensor([[1.2234]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12129977770278821, distance: 1.0726969885590245 entropy -5.029365643757048
epoch: 4, step: 74
	action: tensor([[-0.0302, -0.0510, -0.0959,  0.0052, -0.0287, -0.0479, -0.0215]],
       dtype=torch.float64)
	q_value: tensor([[1.2514]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20778653071260722, distance: 1.0185393317213192 entropy -5.023565848287563
epoch: 4, step: 75
	action: tensor([[-0.0506, -0.0624, -0.0961, -0.0006,  0.0296,  0.0046, -0.0155]],
       dtype=torch.float64)
	q_value: tensor([[1.2586]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18518147806240726, distance: 1.0329686462175112 entropy -5.024363170102673
epoch: 4, step: 76
	action: tensor([[-0.0838, -0.0683, -0.0961,  0.0078, -0.1340, -0.0121, -0.0281]],
       dtype=torch.float64)
	q_value: tensor([[1.2485]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14745634731049373, distance: 1.0566107038910577 entropy -5.022032956110418
epoch: 4, step: 77
	action: tensor([[-0.0078, -0.0582, -0.0959, -0.0090,  0.0168,  0.0553,  0.0360]],
       dtype=torch.float64)
	q_value: tensor([[1.2802]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23759063422802995, distance: 0.9991962711183254 entropy -5.0275328109474415
epoch: 4, step: 78
	action: tensor([[ 0.0426, -0.0584, -0.0960, -0.0532, -0.0048,  0.0724,  0.0144]],
       dtype=torch.float64)
	q_value: tensor([[1.2197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27139067249284177, distance: 0.9767964141139556 entropy -5.025228897330636
epoch: 4, step: 79
	action: tensor([[-0.0243, -0.0743, -0.0959, -0.0252, -0.0101, -0.0074,  0.0498]],
       dtype=torch.float64)
	q_value: tensor([[1.2108]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19015555139433993, distance: 1.0298109325494063 entropy -5.029385212389821
epoch: 4, step: 80
	action: tensor([[ 0.0048, -0.0675, -0.0960, -0.0219, -0.0793, -0.0364,  0.0141]],
       dtype=torch.float64)
	q_value: tensor([[1.2362]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2238924081152982, distance: 1.0081326008720284 entropy -5.022969053024875
epoch: 4, step: 81
	action: tensor([[-0.0313, -0.0648, -0.0959, -0.0182, -0.0670,  0.0394,  0.0117]],
       dtype=torch.float64)
	q_value: tensor([[1.2488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2041875626088946, distance: 1.0208502850803243 entropy -5.0276856499290306
epoch: 4, step: 82
	action: tensor([[-0.0084, -0.0673, -0.0959, -0.0442,  0.0207,  0.0117,  0.0820]],
       dtype=torch.float64)
	q_value: tensor([[1.2430]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20605293410384196, distance: 1.0196531548683423 entropy -5.028166084036513
epoch: 4, step: 83
	action: tensor([[-0.0269, -0.0635, -0.0960,  0.0014, -0.0491, -0.0394,  0.0037]],
       dtype=torch.float64)
	q_value: tensor([[1.2178]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20274811623131872, distance: 1.0217731126811123 entropy -5.022805490832764
epoch: 4, step: 84
	action: tensor([[-0.0621, -0.0628, -0.0960, -0.0351, -0.0531, -0.0503,  0.0113]],
       dtype=torch.float64)
	q_value: tensor([[1.2563]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14775924542209184, distance: 1.0564229869231485 entropy -5.025094971151345
epoch: 4, step: 85
	action: tensor([[-0.0228, -0.0716, -0.0960, -0.0638, -0.0634, -0.0210,  0.0113]],
       dtype=torch.float64)
	q_value: tensor([[1.2617]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17614628703631618, distance: 1.0386799418004655 entropy -5.023486474311918
epoch: 4, step: 86
	action: tensor([[-0.0167, -0.0760, -0.0959,  0.0112, -0.0104,  0.0184, -0.0423]],
       dtype=torch.float64)
	q_value: tensor([[1.2487]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2199634199464957, distance: 1.0106811783874425 entropy -5.0266483165492035
epoch: 4, step: 87
	action: tensor([[-0.0541, -0.0563, -0.0961, -0.0656, -0.0622,  0.0002,  0.0018]],
       dtype=torch.float64)
	q_value: tensor([[1.2535]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15834017037806591, distance: 1.0498445382739288 entropy -5.026049958971781
epoch: 4, step: 88
	action: tensor([[-0.0621, -0.0685, -0.0959, -0.0240, -0.0034, -0.0139,  0.0011]],
       dtype=torch.float64)
	q_value: tensor([[1.2499]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15406763763357678, distance: 1.0525058373889025 entropy -5.0263983765884905
epoch: 4, step: 89
	action: tensor([[-0.0587, -0.0777, -0.0960, -0.0654, -0.0803, -0.0014,  0.0400]],
       dtype=torch.float64)
	q_value: tensor([[1.2539]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13578836318228693, distance: 1.0638165641502428 entropy -5.022071743425402
epoch: 4, step: 90
	action: tensor([[-0.0015, -0.0646, -0.0958, -0.0428, -0.0544,  0.0355,  0.0018]],
       dtype=torch.float64)
	q_value: tensor([[1.2515]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22274326402998434, distance: 1.0088786708211175 entropy -5.026094198489326
epoch: 4, step: 91
	action: tensor([[-0.0112, -0.0704, -0.0959, -0.0156, -0.0344,  0.0246, -0.0347]],
       dtype=torch.float64)
	q_value: tensor([[1.2351]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21977989185390356, distance: 1.0108000686320944 entropy -5.028822718999465
epoch: 4, step: 92
	action: tensor([[-7.7071e-02, -5.9158e-02, -9.5968e-02, -5.6790e-02, -2.0465e-02,
          3.2721e-02,  9.8669e-05]], dtype=torch.float64)
	q_value: tensor([[1.2485]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14088567061724766, distance: 1.0606746140560135 entropy -5.027720772328896
epoch: 4, step: 93
	action: tensor([[ 0.0040, -0.0707, -0.0959, -0.0276, -0.0503,  0.0995,  0.0624]],
       dtype=torch.float64)
	q_value: tensor([[1.2467]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24038307460400288, distance: 0.9973647382795716 entropy -5.024232936512974
epoch: 4, step: 94
	action: tensor([[-0.0152, -0.0736, -0.0958, -0.0690, -0.0786, -0.0643, -0.0176]],
       dtype=torch.float64)
	q_value: tensor([[1.2163]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17293196456709448, distance: 1.0407042102258712 entropy -5.0302201044427965
epoch: 4, step: 95
	action: tensor([[ 0.0151, -0.0684, -0.0959, -0.0019, -0.0645, -0.0392,  0.0649]],
       dtype=torch.float64)
	q_value: tensor([[1.2620]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24067185235431687, distance: 0.9971751400003487 entropy -5.026275560278147
epoch: 4, step: 96
	action: tensor([[-0.0180, -0.0592, -0.0957,  0.0063, -0.0018, -0.0142,  0.0002]],
       dtype=torch.float64)
	q_value: tensor([[3.0280]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19964854480606797, distance: 1.0237574206135853 entropy -4.935048237859928
epoch: 4, step: 97
	action: tensor([[-0.0069, -0.0739, -0.0961, -0.0576,  0.0021,  0.0199,  0.0339]],
       dtype=torch.float64)
	q_value: tensor([[1.2432]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18053149530964607, distance: 1.035911910840396 entropy -5.023698755517981
epoch: 4, step: 98
	action: tensor([[-0.0004, -0.0625, -0.0960,  0.0040, -0.0976,  0.0362,  0.0354]],
       dtype=torch.float64)
	q_value: tensor([[1.2275]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2288886736088913, distance: 1.004882387400437 entropy -5.0249333539074765
epoch: 4, step: 99
	action: tensor([[-0.0044, -0.0676, -0.0958, -0.0423, -0.0140,  0.0021, -0.0184]],
       dtype=torch.float64)
	q_value: tensor([[1.2370]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1987613380376515, distance: 1.0243246919873614 entropy -5.0300560962149135
epoch: 4, step: 100
	action: tensor([[-0.0219, -0.0679, -0.0960,  0.0148, -0.0339, -0.0150,  0.0098]],
       dtype=torch.float64)
	q_value: tensor([[1.2403]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2066710538859099, distance: 1.0192561570501355 entropy -5.025869386076769
epoch: 4, step: 101
	action: tensor([[-0.0379, -0.0638, -0.0960,  0.0022, -0.0082, -0.0106,  0.0233]],
       dtype=torch.float64)
	q_value: tensor([[1.2500]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19202503430243623, distance: 1.0286216138548754 entropy -5.025141039792163
epoch: 4, step: 102
	action: tensor([[-0.0416, -0.0673, -0.0961, -0.0153, -0.1091, -0.0140,  0.0426]],
       dtype=torch.float64)
	q_value: tensor([[1.2451]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18248350857400242, distance: 1.0346773793065764 entropy -5.023312223770614
epoch: 4, step: 103
	action: tensor([[ 0.0090, -0.0696, -0.0959, -0.0705, -0.0552,  0.0124, -0.0849]],
       dtype=torch.float64)
	q_value: tensor([[1.2541]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21555555688939365, distance: 1.013532755169826 entropy -5.027731851505991
epoch: 4, step: 104
	action: tensor([[-0.0356, -0.0617, -0.0959, -0.0092,  0.0500, -0.0488,  0.0093]],
       dtype=torch.float64)
	q_value: tensor([[1.2545]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1851381987463694, distance: 1.0329960790640063 entropy -5.030589890356228
epoch: 4, step: 105
	action: tensor([[-0.0003, -0.0547, -0.0962, -0.0230, -0.0194, -0.0683,  0.0633]],
       dtype=torch.float64)
	q_value: tensor([[1.2471]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2168933841340006, distance: 1.012668123929527 entropy -5.019431931571899
epoch: 4, step: 106
	action: tensor([[-0.0406, -0.0490, -0.0961, -0.0818, -0.0040,  0.0220,  0.0665]],
       dtype=torch.float64)
	q_value: tensor([[1.2372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17420844763492582, distance: 1.0399007973169383 entropy -5.023188464285304
epoch: 4, step: 107
	action: tensor([[-0.0528, -0.0570, -0.0959, -0.0062,  0.1354,  0.0994, -0.0298]],
       dtype=torch.float64)
	q_value: tensor([[1.2218]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2076529289986485, distance: 1.0186252131592244 entropy -5.023868574733405
epoch: 4, step: 108
	action: tensor([[-0.0889, -0.0645, -0.0961,  0.0034, -0.0688,  0.0390, -0.0149]],
       dtype=torch.float64)
	q_value: tensor([[1.2284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15116769961256837, distance: 1.054308339866746 entropy -5.02107252390091
epoch: 4, step: 109
	action: tensor([[-0.0499, -0.0746, -0.0959, -0.0136, -0.0986, -0.0896,  0.0368]],
       dtype=torch.float64)
	q_value: tensor([[1.2625]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15539810964637113, distance: 1.0516778275733631 entropy -5.026302410091625
epoch: 4, step: 110
	action: tensor([[ 0.0037, -0.0724, -0.0960,  0.0017, -0.0313, -0.0311,  0.0064]],
       dtype=torch.float64)
	q_value: tensor([[1.2701]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22834114311309095, distance: 1.005239084351361 entropy -5.0242213656250145
epoch: 4, step: 111
	action: tensor([[-0.0051, -0.0657, -0.0961,  0.0023, -0.1651,  0.0431, -0.0261]],
       dtype=torch.float64)
	q_value: tensor([[1.2468]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2413266019885445, distance: 0.9967451277153083 entropy -5.025140413575585
epoch: 4, step: 112
	action: tensor([[-0.0249, -0.0649, -0.0957, -0.0438, -0.0605, -0.0003, -0.0062]],
       dtype=torch.float64)
	q_value: tensor([[1.2569]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19214139387765128, distance: 1.028547543315137 entropy -5.033693103099886
epoch: 4, step: 113
	action: tensor([[-0.0069, -0.0714, -0.0959,  0.0286, -0.0206,  0.0258,  0.0210]],
       dtype=torch.float64)
	q_value: tensor([[1.2471]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24212821664860973, distance: 0.9962184078660349 entropy -5.026890824693257
epoch: 4, step: 114
	action: tensor([[-0.0121, -0.0589, -0.0960,  0.0061,  0.0101, -0.0901,  0.0266]],
       dtype=torch.float64)
	q_value: tensor([[1.2378]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2114248388175488, distance: 1.0161977756870249 entropy -5.025791415638971
epoch: 4, step: 115
	action: tensor([[-0.0212, -0.0757, -0.0962,  0.0088,  0.0287,  0.0433,  0.0379]],
       dtype=torch.float64)
	q_value: tensor([[1.2508]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21817185015566187, distance: 1.0118411671682068 entropy -5.0208262878084815
epoch: 4, step: 116
	action: tensor([[ 0.0010, -0.0697, -0.0961,  0.0304, -0.0739,  0.0566,  0.0109]],
       dtype=torch.float64)
	q_value: tensor([[1.2293]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25906288873071115, distance: 0.9850252588204583 entropy -5.023772724175928
epoch: 4, step: 117
	action: tensor([[-0.0386, -0.0611, -0.0959, -0.0597, -0.0746,  0.0219, -0.0183]],
       dtype=torch.float64)
	q_value: tensor([[1.2409]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17831750184212047, distance: 1.0373103509061459 entropy -5.0300879793649775
epoch: 4, step: 118
	action: tensor([[-0.0309, -0.0840, -0.0958,  0.0144, -0.0615,  0.0279,  0.0095]],
       dtype=torch.float64)
	q_value: tensor([[1.2493]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2016758415257861, distance: 1.022460005581977 entropy -5.028433459855814
epoch: 4, step: 119
	action: tensor([[-0.0573, -0.0671, -0.0960, -0.0518, -0.1092,  0.0859, -0.0507]],
       dtype=torch.float64)
	q_value: tensor([[1.2516]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16949089675091267, distance: 1.0428669202189933 entropy -5.027042120916863
epoch: 4, step: 120
	action: tensor([[-0.0540, -0.0644, -0.0957, -0.0105, -0.0686, -0.0396, -0.0257]],
       dtype=torch.float64)
	q_value: tensor([[1.2563]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16892044514165216, distance: 1.0432250156270018 entropy -5.031642352160448
epoch: 4, step: 121
	action: tensor([[-0.0253, -0.0642, -0.0960, -0.0080, -0.0601,  0.0030,  0.1182]],
       dtype=torch.float64)
	q_value: tensor([[1.2683]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20622458895695572, distance: 1.0195429221579098 entropy -5.0248792298116385
epoch: 4, step: 122
	action: tensor([[-0.0238, -0.0659, -0.0959, -0.0167, -0.0301, -0.0195,  0.0012]],
       dtype=torch.float64)
	q_value: tensor([[1.2285]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19993618599018403, distance: 1.0235734381533512 entropy -5.025357932850045
epoch: 4, step: 123
	action: tensor([[-0.0390, -0.0634, -0.0960, -0.0381, -0.0030,  0.0036,  0.0171]],
       dtype=torch.float64)
	q_value: tensor([[1.2485]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18032426540286872, distance: 1.0360428849763532 entropy -5.024612934422459
epoch: 4, step: 124
	action: tensor([[ 0.0101, -0.0701, -0.0960, -0.0116, -0.0657, -0.0043, -0.0418]],
       dtype=torch.float64)
	q_value: tensor([[1.2392]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23798507051852402, distance: 0.9989377680906641 entropy -5.023298285032223
epoch: 4, step: 125
	action: tensor([[-0.0449, -0.0671, -0.0959, -0.0310,  0.0084, -0.0135,  0.0149]],
       dtype=torch.float64)
	q_value: tensor([[1.2532]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1703483234128096, distance: 1.0423284476070087 entropy -5.028941977292284
epoch: 4, step: 126
	action: tensor([[-0.0095, -0.0689, -0.0961, -0.0728, -0.1632, -0.0112, -0.0087]],
       dtype=torch.float64)
	q_value: tensor([[1.2442]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19224105790282509, distance: 1.028484096352228 entropy -5.021801428464782
epoch: 4, step: 127
	action: tensor([[-0.0097, -0.0720, -0.0957, -0.0495, -0.0302,  0.1023,  0.0369]],
       dtype=torch.float64)
	q_value: tensor([[1.2574]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2206262622809575, distance: 1.0102516699084378 entropy -5.0323585282557906
LOSS epoch 4 actor 0.7752383262961178 critic 2.590171923155274 entropy 0.01
epoch: 5, step: 0
	action: tensor([[ 0.0147, -0.0484, -0.0636, -0.0239, -0.0150, -0.0115, -0.0162]],
       dtype=torch.float64)
	q_value: tensor([[4.8268]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22458257898940293, distance: 1.0076842490457272 entropy -5.084993815379126
epoch: 5, step: 1
	action: tensor([[-0.0169, -0.0576, -0.0692,  0.0313,  0.0162, -0.0157,  0.0545]],
       dtype=torch.float64)
	q_value: tensor([[2.0319]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21177231545938346, distance: 1.0159738630440631 entropy -5.089231759889474
epoch: 5, step: 2
	action: tensor([[-0.0333, -0.0555, -0.0693, -0.0245, -0.0144,  0.0244,  0.0363]],
       dtype=torch.float64)
	q_value: tensor([[2.0259]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18132328368753703, distance: 1.03541132952949 entropy -5.088676242169619
epoch: 5, step: 3
	action: tensor([[-0.0402, -0.0525, -0.0692, -0.0379, -0.0215,  0.0229,  0.0417]],
       dtype=torch.float64)
	q_value: tensor([[2.0257]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17166758067754362, distance: 1.0414993969953745 entropy -5.090478730392277
epoch: 5, step: 4
	action: tensor([[-0.0574, -0.0668, -0.0692,  0.0277, -0.0058,  0.0160,  0.0276]],
       dtype=torch.float64)
	q_value: tensor([[2.0260]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1728119369713632, distance: 1.040779723174755 entropy -5.0911822366661825
epoch: 5, step: 5
	action: tensor([[-0.0532, -0.0603, -0.0693, -0.0331, -0.0414,  0.0217,  0.0784]],
       dtype=torch.float64)
	q_value: tensor([[2.0483]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15647172186278424, distance: 1.051009197053236 entropy -5.088950150523567
epoch: 5, step: 6
	action: tensor([[-0.0246, -0.0547, -0.0691,  0.0213, -0.0299, -0.0078, -0.0515]],
       dtype=torch.float64)
	q_value: tensor([[2.0295]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21102084403896249, distance: 1.0164580463931816 entropy -5.092922437529798
epoch: 5, step: 7
	action: tensor([[-0.0383, -0.0599, -0.0693,  0.0111,  0.0364, -0.0077, -0.0576]],
       dtype=torch.float64)
	q_value: tensor([[2.0751]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19194180492549773, distance: 1.0286745915677173 entropy -5.088490655200667
epoch: 5, step: 8
	action: tensor([[-0.0397, -0.0592, -0.0693,  0.0087, -0.0665,  0.0376,  0.0060]],
       dtype=torch.float64)
	q_value: tensor([[2.0680]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20336549492955036, distance: 1.0213774139448142 entropy -5.0866921884289
epoch: 5, step: 9
	action: tensor([[-0.0080, -0.0528, -0.0692, -0.0215,  0.0923,  0.0243, -0.0203]],
       dtype=torch.float64)
	q_value: tensor([[2.0542]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22342964291351164, distance: 1.008433112791046 entropy -5.091247633486096
epoch: 5, step: 10
	action: tensor([[-0.0202, -0.0529, -0.0693, -0.0584, -0.0985,  0.0025,  0.0655]],
       dtype=torch.float64)
	q_value: tensor([[2.0152]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19224509809646406, distance: 1.0284815242481737 entropy -5.086302432248847
epoch: 5, step: 11
	action: tensor([[-0.0491, -0.0546, -0.0691, -0.0335, -0.0614,  0.0071,  0.0363]],
       dtype=torch.float64)
	q_value: tensor([[2.0347]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17337663245214652, distance: 1.0404244086025105 entropy -5.094653506273805
epoch: 5, step: 12
	action: tensor([[-0.0324, -0.0519, -0.0692, -0.0545,  0.0190, -0.0050, -0.0531]],
       dtype=torch.float64)
	q_value: tensor([[2.0464]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18168754422644895, distance: 1.0351809568781887 entropy -5.09236728369855
epoch: 5, step: 13
	action: tensor([[-0.0304, -0.0558, -0.0692, -0.0015, -0.0414,  0.0444,  0.1181]],
       dtype=torch.float64)
	q_value: tensor([[2.0546]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2129143472788072, distance: 1.0152375941488505 entropy -5.088619262608998
epoch: 5, step: 14
	action: tensor([[-0.0304, -0.0598, -0.0691,  0.0122, -0.0292,  0.0626,  0.0079]],
       dtype=torch.float64)
	q_value: tensor([[2.0040]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2221416848960891, distance: 1.0092690199417382 entropy -5.093972177377857
epoch: 5, step: 15
	action: tensor([[ 0.0006, -0.0503, -0.0692, -0.0246, -0.1249,  0.0310,  0.0574]],
       dtype=torch.float64)
	q_value: tensor([[2.0347]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23844704237585435, distance: 0.9986349189713647 entropy -5.08971924702063
epoch: 5, step: 16
	action: tensor([[-0.0304, -0.0603, -0.0691, -0.0150, -0.1108, -0.0008,  0.0359]],
       dtype=torch.float64)
	q_value: tensor([[2.0310]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19659544398789064, distance: 1.025708225744567 entropy -5.094868953602632
epoch: 5, step: 17
	action: tensor([[-0.0225, -0.0576, -0.0692,  0.0249, -0.1373,  0.0983, -0.0136]],
       dtype=torch.float64)
	q_value: tensor([[2.0595]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24446152420855705, distance: 0.994683665521248 entropy -5.093556175822216
epoch: 5, step: 18
	action: tensor([[-0.0081, -0.0504, -0.0692, -0.0006,  0.0354, -0.0275,  0.0283]],
       dtype=torch.float64)
	q_value: tensor([[2.0586]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22550103755663553, distance: 1.0070872864095608 entropy -5.092891162715896
epoch: 5, step: 19
	action: tensor([[-0.0244, -0.0519, -0.0693,  0.0096,  0.0502,  0.0014, -0.0436]],
       dtype=torch.float64)
	q_value: tensor([[2.0250]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2196829376916255, distance: 1.0108628702880087 entropy -5.088453741154406
epoch: 5, step: 20
	action: tensor([[-0.0167, -0.0518, -0.0693,  0.0219,  0.0287,  0.0066,  0.0339]],
       dtype=torch.float64)
	q_value: tensor([[2.0494]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23345090900166854, distance: 1.001905313295043 entropy -5.086639271243671
epoch: 5, step: 21
	action: tensor([[-0.0315, -0.0602, -0.0693,  0.0314, -0.0284,  0.0495, -0.0567]],
       dtype=torch.float64)
	q_value: tensor([[2.0196]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2274915343191668, distance: 1.0057923242995146 entropy -5.0882252441837394
epoch: 5, step: 22
	action: tensor([[ 0.0044, -0.0596, -0.0692, -0.0422,  0.0694, -0.0216, -0.0086]],
       dtype=torch.float64)
	q_value: tensor([[2.0661]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2126915627715208, distance: 1.0153812654276204 entropy -5.088213891187874
epoch: 5, step: 23
	action: tensor([[ 0.0022, -0.0487, -0.0693,  0.0083, -0.1191,  0.0075,  0.0052]],
       dtype=torch.float64)
	q_value: tensor([[2.0243]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25253183812596647, distance: 0.9893570268286151 entropy -5.087206676052887
epoch: 5, step: 24
	action: tensor([[-0.0192, -0.0558, -0.0692, -0.0498, -0.1017, -0.0207,  0.0847]],
       dtype=torch.float64)
	q_value: tensor([[2.0559]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19167727751744812, distance: 1.0288429521876055 entropy -5.0929302889657695
epoch: 5, step: 25
	action: tensor([[-0.0231, -0.0452, -0.0691,  0.0020, -0.0206, -0.0126,  0.0720]],
       dtype=torch.float64)
	q_value: tensor([[2.0379]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21893349532969586, distance: 1.0113481868848875 entropy -5.095011553913411
epoch: 5, step: 26
	action: tensor([[-0.0589, -0.0557, -0.0692, -0.0249, -0.0171,  0.0110, -0.0173]],
       dtype=torch.float64)
	q_value: tensor([[2.0217]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16683168024228723, distance: 1.0445351695941556 entropy -5.0917308912670975
epoch: 5, step: 27
	action: tensor([[-0.0458, -0.0547, -0.0692, -0.0058, -0.0388, -0.0229, -0.0516]],
       dtype=torch.float64)
	q_value: tensor([[2.0602]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18420621332388254, distance: 1.0335866467922272 entropy -5.0897486059591
epoch: 5, step: 28
	action: tensor([[-0.0376, -0.0528, -0.0692, -0.0451, -0.0121,  0.0658,  0.0136]],
       dtype=torch.float64)
	q_value: tensor([[2.0860]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1946850219826779, distance: 1.0269270214527426 entropy -5.089449330296254
epoch: 5, step: 29
	action: tensor([[-0.0224, -0.0514, -0.0692,  0.0033, -0.0062,  0.0237,  0.0160]],
       dtype=torch.float64)
	q_value: tensor([[2.0209]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22418343357226256, distance: 1.0079435679575255 entropy -5.090331358374874
epoch: 5, step: 30
	action: tensor([[-0.0511, -0.0560, -0.0692, -0.0196,  0.0155,  0.0303, -0.0087]],
       dtype=torch.float64)
	q_value: tensor([[2.0284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18137121303510284, distance: 1.0353810200588132 entropy -5.089523829397122
epoch: 5, step: 31
	action: tensor([[-0.0034, -0.0491, -0.0692, -0.0469, -0.0608,  0.0528,  0.0060]],
       dtype=torch.float64)
	q_value: tensor([[2.0399]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2286976694515982, distance: 1.005006834301172 entropy -5.08856710416539
epoch: 5, step: 32
	action: tensor([[-0.0360, -0.0532, -0.0636, -0.0218, -0.1087, -0.0191, -0.0195]],
       dtype=torch.float64)
	q_value: tensor([[4.8268]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1754876538397584, distance: 1.039095047301409 entropy -5.084993815379126
epoch: 5, step: 33
	action: tensor([[-0.0376, -0.0547, -0.0692,  0.0007, -0.0231, -0.0079, -0.0024]],
       dtype=torch.float64)
	q_value: tensor([[2.0790]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1890264947293765, distance: 1.0305285456214213 entropy -5.092452402600573
epoch: 5, step: 34
	action: tensor([[-0.0258, -0.0557, -0.0692,  0.0252, -0.0162, -0.0414,  0.0600]],
       dtype=torch.float64)
	q_value: tensor([[2.0557]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2049069940519762, distance: 1.0203887455140723 entropy -5.089740778300636
epoch: 5, step: 35
	action: tensor([[-0.0154, -0.0569, -0.0693,  0.0123, -0.0331, -0.0433, -0.0289]],
       dtype=torch.float64)
	q_value: tensor([[2.0425]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21111594568144032, distance: 1.0163967838476735 entropy -5.090224210094802
epoch: 5, step: 36
	action: tensor([[-0.0474, -0.0543, -0.0693,  0.0052, -0.0055, -0.0340, -0.0475]],
       dtype=torch.float64)
	q_value: tensor([[2.0747]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18384419098108407, distance: 1.033815957165959 entropy -5.089072615676922
epoch: 5, step: 37
	action: tensor([[-0.0314, -0.0507, -0.0693, -0.0297,  0.0734,  0.0368,  0.1004]],
       dtype=torch.float64)
	q_value: tensor([[2.0815]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20026068370419214, distance: 1.0233658416337377 entropy -5.088238323914526
epoch: 5, step: 38
	action: tensor([[-0.0181, -0.0585, -0.0692, -0.0077,  0.1161, -0.0155,  0.0605]],
       dtype=torch.float64)
	q_value: tensor([[1.9813]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20099802807505995, distance: 1.0228939709672626 entropy -5.089714613925581
epoch: 5, step: 39
	action: tensor([[-0.0022, -0.0542, -0.0693, -0.0276,  0.0068, -0.0108,  0.0063]],
       dtype=torch.float64)
	q_value: tensor([[2.0031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21242327665861727, distance: 1.015554253215381 entropy -5.086147660227369
epoch: 5, step: 40
	action: tensor([[-0.0368, -0.0508, -0.0692, -0.0604,  0.1048,  0.0053, -0.0331]],
       dtype=torch.float64)
	q_value: tensor([[2.0308]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17011292097044572, distance: 1.0424763104191 entropy -5.089416841292109
epoch: 5, step: 41
	action: tensor([[-0.0285, -0.0562, -0.0693, -0.0560,  0.0603, -0.0182,  0.0072]],
       dtype=torch.float64)
	q_value: tensor([[2.0281]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17006097787744834, distance: 1.0425089344941239 entropy -5.086099139179008
epoch: 5, step: 42
	action: tensor([[-6.1228e-05, -5.8611e-02, -6.9245e-02, -3.1835e-03,  1.3020e-02,
          4.3547e-02, -4.5221e-02]], dtype=torch.float64)
	q_value: tensor([[2.0295]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23630676744681545, distance: 1.0000372204505263 entropy -5.088151548663402
epoch: 5, step: 43
	action: tensor([[-0.0121, -0.0487, -0.0692, -0.0277, -0.0449, -0.0572,  0.0371]],
       dtype=torch.float64)
	q_value: tensor([[2.0390]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20210304266959145, distance: 1.0221863983450385 entropy -5.08742963624142
epoch: 5, step: 44
	action: tensor([[-0.0566, -0.0527, -0.0692,  0.0194,  0.0444,  0.0743,  0.0235]],
       dtype=torch.float64)
	q_value: tensor([[2.0482]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.205914622620893, distance: 1.0197419665816343 entropy -5.09151781093725
epoch: 5, step: 45
	action: tensor([[-0.0659, -0.0562, -0.0692,  0.0499, -0.0249, -0.0441,  0.0252]],
       dtype=torch.float64)
	q_value: tensor([[2.0149]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1822331883214443, distance: 1.0348357742028338 entropy -5.088192392069598
epoch: 5, step: 46
	action: tensor([[-0.0391, -0.0507, -0.0693, -0.0181, -0.0479, -0.0378, -0.0438]],
       dtype=torch.float64)
	q_value: tensor([[2.0730]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18894392822308748, distance: 1.030581004163831 entropy -5.088900582487104
epoch: 5, step: 47
	action: tensor([[ 0.0104, -0.0473, -0.0692,  0.0213, -0.0017,  0.0308,  0.0200]],
       dtype=torch.float64)
	q_value: tensor([[2.0831]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2730568765754313, distance: 0.9756788922444154 entropy -5.090139387599199
epoch: 5, step: 48
	action: tensor([[-0.0221, -0.0514, -0.0692, -0.0599, -0.0189,  0.0218,  0.0572]],
       dtype=torch.float64)
	q_value: tensor([[2.0146]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2002080158413032, distance: 1.0233995386168513 entropy -5.089235846760949
epoch: 5, step: 49
	action: tensor([[-0.0215, -0.0571, -0.0691, -0.0323, -0.0260, -0.0321, -0.0117]],
       dtype=torch.float64)
	q_value: tensor([[2.0125]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19864397047759774, distance: 1.0243997121362909 entropy -5.091937682572076
epoch: 5, step: 50
	action: tensor([[ 0.0074, -0.0526, -0.0692,  0.0147,  0.0073,  0.0313,  0.0175]],
       dtype=torch.float64)
	q_value: tensor([[2.0596]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2656013459482307, distance: 0.9806694118600665 entropy -5.090295036165377
epoch: 5, step: 51
	action: tensor([[-0.0303, -0.0462, -0.0692,  0.0185, -0.0537,  0.0295, -0.0291]],
       dtype=torch.float64)
	q_value: tensor([[2.0156]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23421772281017328, distance: 1.001404062253675 entropy -5.088860114697302
epoch: 5, step: 52
	action: tensor([[-0.0247, -0.0500, -0.0692,  0.1252, -0.0757,  0.0121,  0.0136]],
       dtype=torch.float64)
	q_value: tensor([[2.0571]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2807783314997211, distance: 0.9704833187624139 entropy -5.090052863969023
epoch: 5, step: 53
	action: tensor([[-0.0043, -0.0492, -0.0693, -0.0328, -0.0475,  0.0413,  0.0522]],
       dtype=torch.float64)
	q_value: tensor([[2.0674]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23733677646764084, distance: 0.9993626073596193 entropy -5.090084082076987
epoch: 5, step: 54
	action: tensor([[-0.0456, -0.0569, -0.0691,  0.0259, -0.0126, -0.0046,  0.0580]],
       dtype=torch.float64)
	q_value: tensor([[2.0119]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20372214407840494, distance: 1.021148755656113 entropy -5.092555242759322
epoch: 5, step: 55
	action: tensor([[ 0.0090, -0.0614, -0.0692, -0.0172, -0.0211,  0.0491, -0.0238]],
       dtype=torch.float64)
	q_value: tensor([[2.0381]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24876483061409438, distance: 0.9918469198520434 entropy -5.090267864718808
epoch: 5, step: 56
	action: tensor([[-0.0194, -0.0487, -0.0692,  0.0378, -0.1087,  0.0168,  0.0202]],
       dtype=torch.float64)
	q_value: tensor([[2.0340]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24962694607905012, distance: 0.99127763604588 entropy -5.089000108095791
epoch: 5, step: 57
	action: tensor([[-0.0496, -0.0587, -0.0692,  0.0508, -0.1148, -0.0403, -0.0126]],
       dtype=torch.float64)
	q_value: tensor([[2.0580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20576552456799124, distance: 1.0198376958380988 entropy -5.092918319860486
epoch: 5, step: 58
	action: tensor([[-0.0368, -0.0533, -0.0693, -0.0271, -0.0132, -0.0188, -0.0299]],
       dtype=torch.float64)
	q_value: tensor([[2.1032]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1905486031647211, distance: 1.0295609968144042 entropy -5.091078490073353
epoch: 5, step: 59
	action: tensor([[-0.0262, -0.0534, -0.0692,  0.0244, -0.0627, -0.0027, -0.0058]],
       dtype=torch.float64)
	q_value: tensor([[2.0623]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22884309642492928, distance: 1.00491208417127 entropy -5.089483899110303
epoch: 5, step: 60
	action: tensor([[-0.0523, -0.0521, -0.0692,  0.0126, -0.1634,  0.0068,  0.0097]],
       dtype=torch.float64)
	q_value: tensor([[2.0627]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1994977534289294, distance: 1.0238538573225704 entropy -5.090313597711058
epoch: 5, step: 61
	action: tensor([[ 0.0005, -0.0507, -0.0692, -0.0502, -0.0201,  0.0185, -0.0112]],
       dtype=torch.float64)
	q_value: tensor([[2.0870]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.228641657558877, distance: 1.005043325383079 entropy -5.095078589207604
epoch: 5, step: 62
	action: tensor([[-0.0655, -0.0508, -0.0692, -0.0132, -0.0155,  0.0337,  0.0064]],
       dtype=torch.float64)
	q_value: tensor([[2.0281]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17850986056401985, distance: 1.037188924824218 entropy -5.09014960955771
epoch: 5, step: 63
	action: tensor([[-0.0232, -0.0582, -0.0692,  0.0176, -0.0032,  0.0461,  0.0102]],
       dtype=torch.float64)
	q_value: tensor([[2.0460]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23304509374396343, distance: 1.0021704852647113 entropy -5.089899143648135
epoch: 5, step: 64
	action: tensor([[-0.0459, -0.0463, -0.0636, -0.0141, -0.0654, -0.0808, -0.0173]],
       dtype=torch.float64)
	q_value: tensor([[4.8268]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.162605012939957, distance: 1.0471812841272532 entropy -5.084993815379126
epoch: 5, step: 65
	action: tensor([[-0.0443, -0.0529, -0.0692,  0.0658,  0.0029,  0.0088,  0.0160]],
       dtype=torch.float64)
	q_value: tensor([[2.0895]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21517836525687561, distance: 1.0137763990207078 entropy -5.09074021701668
epoch: 5, step: 66
	action: tensor([[-0.0070, -0.0588, -0.0693, -0.0094,  0.0292,  0.0488,  0.0448]],
       dtype=torch.float64)
	q_value: tensor([[2.0469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22311370412493559, distance: 1.0086382266836158 entropy -5.088215520665513
epoch: 5, step: 67
	action: tensor([[-0.0564, -0.0573, -0.0692,  0.0052,  0.0187,  0.0171,  0.0699]],
       dtype=torch.float64)
	q_value: tensor([[2.0011]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17333827592515483, distance: 1.040448546925689 entropy -5.089143028290567
epoch: 5, step: 68
	action: tensor([[-0.0552, -0.0537, -0.0692, -0.0461, -0.0538,  0.0441,  0.1105]],
       dtype=torch.float64)
	q_value: tensor([[2.0220]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15947971510875802, distance: 1.0491335918889855 entropy -5.089708182311159
epoch: 5, step: 69
	action: tensor([[-0.0233, -0.0516, -0.0691,  0.0122, -0.0111, -0.0641, -0.0170]],
       dtype=torch.float64)
	q_value: tensor([[2.0132]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1979154982476905, distance: 1.0248652213471658 entropy -5.0950972052080346
epoch: 5, step: 70
	action: tensor([[-0.0429, -0.0563, -0.0693,  0.0038,  0.0409,  0.0073, -0.0070]],
       dtype=torch.float64)
	q_value: tensor([[2.0717]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18796915766837097, distance: 1.0312001218757922 entropy -5.088835985753868
epoch: 5, step: 71
	action: tensor([[ 0.0073, -0.0561, -0.0693, -0.0041,  0.0540,  0.0152,  0.0619]],
       dtype=torch.float64)
	q_value: tensor([[2.0411]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2348248474421507, distance: 1.0010070188366442 entropy -5.087615746213274
epoch: 5, step: 72
	action: tensor([[-0.0218, -0.0533, -0.0692, -0.0226, -0.0651,  0.0818,  0.0678]],
       dtype=torch.float64)
	q_value: tensor([[1.9938]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2148872857403158, distance: 1.01396437943242 entropy -5.088644647214563
epoch: 5, step: 73
	action: tensor([[-0.0021, -0.0518, -0.0691, -0.0301,  0.0213,  0.0459,  0.0061]],
       dtype=torch.float64)
	q_value: tensor([[2.0099]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2262739291003265, distance: 1.0065846624096715 entropy -5.093622035540199
epoch: 5, step: 74
	action: tensor([[-0.0532, -0.0535, -0.0692,  0.0141,  0.0024,  0.0054,  0.0062]],
       dtype=torch.float64)
	q_value: tensor([[2.0097]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1865346065962704, distance: 1.0321105900470302 entropy -5.089016793506965
epoch: 5, step: 75
	action: tensor([[-0.0410, -0.0514, -0.0693, -0.0318, -0.0594,  0.0127,  0.0682]],
       dtype=torch.float64)
	q_value: tensor([[2.0496]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18282718904072548, distance: 1.0344598694525056 entropy -5.088760424521061
epoch: 5, step: 76
	action: tensor([[-0.0386, -0.0535, -0.0692, -0.0087, -0.0347, -0.0288, -0.0090]],
       dtype=torch.float64)
	q_value: tensor([[2.0302]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18825261780936353, distance: 1.0310201227734113 entropy -5.093364107575828
epoch: 5, step: 77
	action: tensor([[-0.0460, -0.0538, -0.0692, -0.0280,  0.0155, -0.0190,  0.0232]],
       dtype=torch.float64)
	q_value: tensor([[2.0672]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17481712934690408, distance: 1.0395174770484583 entropy -5.090165000674405
epoch: 5, step: 78
	action: tensor([[-0.0281, -0.0529, -0.0692, -0.0098, -0.0605, -0.0416, -0.0803]],
       dtype=torch.float64)
	q_value: tensor([[2.0417]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20305012365124753, distance: 1.0215795651354982 entropy -5.089550199077411
epoch: 5, step: 79
	action: tensor([[ 0.0046, -0.0481, -0.0692, -0.0121,  0.0458,  0.0700,  0.0033]],
       dtype=torch.float64)
	q_value: tensor([[2.1003]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2625184335587921, distance: 0.9827256190910525 entropy -5.089752647524778
epoch: 5, step: 80
	action: tensor([[-0.0320, -0.0540, -0.0692, -0.0225, -0.0276,  0.0064, -0.0006]],
       dtype=torch.float64)
	q_value: tensor([[1.9973]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20238535035544303, distance: 1.0220055500514937 entropy -5.088021592820445
epoch: 5, step: 81
	action: tensor([[-0.0263, -0.0556, -0.0692, -0.0882, -0.0784, -0.0125, -0.0098]],
       dtype=torch.float64)
	q_value: tensor([[2.0465]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1749614764183015, distance: 1.0394265530415447 entropy -5.090238250759944
epoch: 5, step: 82
	action: tensor([[-0.0255, -0.0559, -0.0691, -0.0069,  0.0938,  0.0243,  0.0035]],
       dtype=torch.float64)
	q_value: tensor([[2.0607]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21702118219286815, distance: 1.0125854900337494 entropy -5.093028082851432
epoch: 5, step: 83
	action: tensor([[-0.0138, -0.0520, -0.0693,  0.0113, -0.0345, -0.0528,  0.0720]],
       dtype=torch.float64)
	q_value: tensor([[2.0132]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22323903079600305, distance: 1.008556867049391 entropy -5.0861670964080465
epoch: 5, step: 84
	action: tensor([[-0.0289, -0.0606, -0.0692,  0.0105,  0.0905,  0.0491,  0.0866]],
       dtype=torch.float64)
	q_value: tensor([[2.0382]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22144628758505502, distance: 1.0097200572058112 entropy -5.091406182889604
epoch: 5, step: 85
	action: tensor([[-0.0267, -0.0482, -0.0692, -0.0549,  0.0547, -0.0195,  0.0057]],
       dtype=torch.float64)
	q_value: tensor([[1.9850]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1863779237699048, distance: 1.032209983468948 entropy -5.088157261300107
epoch: 5, step: 86
	action: tensor([[-0.0082, -0.0515, -0.0692, -0.0461, -0.0257,  0.0037,  0.0503]],
       dtype=torch.float64)
	q_value: tensor([[2.0272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2121094962107446, distance: 1.0157565378472453 entropy -5.088544529050573
epoch: 5, step: 87
	action: tensor([[ 0.0010, -0.0546, -0.0692, -0.0280,  0.0604,  0.0044, -0.0123]],
       dtype=torch.float64)
	q_value: tensor([[2.0172]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2273781007508242, distance: 1.0058661658362256 entropy -5.0916158918184555
epoch: 5, step: 88
	action: tensor([[-0.0277, -0.0585, -0.0693,  0.0243, -0.0015, -0.0236, -0.0262]],
       dtype=torch.float64)
	q_value: tensor([[2.0195]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21671283337573088, distance: 1.0127848560937776 entropy -5.087441197161543
epoch: 5, step: 89
	action: tensor([[-0.0601, -0.0583, -0.0693, -0.0691, -0.0708,  0.0338,  0.0465]],
       dtype=torch.float64)
	q_value: tensor([[2.0663]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15319475384222203, distance: 1.0530487167327032 entropy -5.088017037171963
epoch: 5, step: 90
	action: tensor([[-0.0199, -0.0634, -0.0691,  0.0319, -0.0280, -0.0128,  0.0444]],
       dtype=torch.float64)
	q_value: tensor([[2.0409]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2258920146055906, distance: 1.006833059006304 entropy -5.093461561517292
epoch: 5, step: 91
	action: tensor([[-0.0257, -0.0554, -0.0693, -0.0141, -0.0786,  0.0195,  0.0449]],
       dtype=torch.float64)
	q_value: tensor([[2.0429]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21437722485026578, distance: 1.0142936949514616 entropy -5.090002347844355
epoch: 5, step: 92
	action: tensor([[-0.0384, -0.0613, -0.0692,  0.0689, -0.0777, -0.0034,  0.0814]],
       dtype=torch.float64)
	q_value: tensor([[2.0388]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22854578024410444, distance: 1.0051057852385168 entropy -5.092911180366807
epoch: 5, step: 93
	action: tensor([[-0.0086, -0.0535, -0.0693, -0.0202, -0.0026,  0.0354, -0.0168]],
       dtype=torch.float64)
	q_value: tensor([[2.0505]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23414027012764893, distance: 1.0014547029332415 entropy -5.092699452758507
epoch: 5, step: 94
	action: tensor([[-0.0170, -0.0500, -0.0692, -0.0160, -0.0860,  0.0360,  0.0171]],
       dtype=torch.float64)
	q_value: tensor([[2.0302]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23065185549674727, distance: 1.0037328747787875 entropy -5.0889768633038
epoch: 5, step: 95
	action: tensor([[-0.0045, -0.0519, -0.0692,  0.0254, -0.0607,  0.0500,  0.0901]],
       dtype=torch.float64)
	q_value: tensor([[2.0398]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2623682445293156, distance: 0.9828256806383495 entropy -5.092544946350336
epoch: 5, step: 96
	action: tensor([[-0.0152, -0.0518, -0.0636,  0.0606, -0.0179,  0.0047, -0.0215]],
       dtype=torch.float64)
	q_value: tensor([[4.8268]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23313933610599824, distance: 1.0021089107089731 entropy -5.084993815379126
epoch: 5, step: 97
	action: tensor([[-0.0406, -0.0587, -0.0693,  0.0087,  0.0307, -0.0262,  0.0296]],
       dtype=torch.float64)
	q_value: tensor([[2.0526]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17368891702784917, distance: 1.0402278625092087 entropy -5.087598856300317
epoch: 5, step: 98
	action: tensor([[-0.0402, -0.0517, -0.0693, -0.0588, -0.0256, -0.0131, -0.0022]],
       dtype=torch.float64)
	q_value: tensor([[2.0400]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15361029593540398, distance: 1.052790310348288 entropy -5.088166860171247
epoch: 5, step: 99
	action: tensor([[-0.0193, -0.0517, -0.0692, -0.0677, -0.0058, -0.0685, -0.0219]],
       dtype=torch.float64)
	q_value: tensor([[2.0514]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16129142728525825, distance: 1.0480022964990803 entropy -5.091012692145559
epoch: 5, step: 100
	action: tensor([[-0.0344, -0.0529, -0.0692, -0.0366, -0.0172,  0.0089, -0.0695]],
       dtype=torch.float64)
	q_value: tensor([[2.0639]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17973937982379373, distance: 1.0364124570264612 entropy -5.089835635369425
epoch: 5, step: 101
	action: tensor([[ 0.0060, -0.0512, -0.0692, -0.0063,  0.0506, -0.0203,  0.0457]],
       dtype=torch.float64)
	q_value: tensor([[2.0694]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22838515593646846, distance: 1.005210416215974 entropy -5.088983766218537
epoch: 5, step: 102
	action: tensor([[-0.0029, -0.0599, -0.0693, -0.0117,  0.0254, -0.0363, -0.0437]],
       dtype=torch.float64)
	q_value: tensor([[2.0068]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20819734278593027, distance: 1.0182752094044167 entropy -5.088257750720104
epoch: 5, step: 103
	action: tensor([[-0.0369, -0.0484, -0.0693, -0.0024, -0.0294,  0.0343, -0.0323]],
       dtype=torch.float64)
	q_value: tensor([[2.0578]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.206894609357399, distance: 1.019112536710451 entropy -5.087363474094433
epoch: 5, step: 104
	action: tensor([[-0.0334, -0.0528, -0.0692, -0.0160, -0.0062,  0.0062,  0.0215]],
       dtype=torch.float64)
	q_value: tensor([[2.0522]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1969706643371909, distance: 1.025468675481142 entropy -5.089463575051492
epoch: 5, step: 105
	action: tensor([[-0.0335, -0.0593, -0.0692,  0.0188, -0.0847,  0.0187, -0.0082]],
       dtype=torch.float64)
	q_value: tensor([[2.0339]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21437993258405164, distance: 1.0142919470135454 entropy -5.089981090776796
epoch: 5, step: 106
	action: tensor([[-0.0104, -0.0531, -0.0692, -0.0337,  0.1110, -0.0138,  0.0430]],
       dtype=torch.float64)
	q_value: tensor([[2.0666]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20831579082918283, distance: 1.0181990431930599 entropy -5.091191839884734
epoch: 5, step: 107
	action: tensor([[-0.0390, -0.0554, -0.0693, -0.0647, -0.0122,  0.0271,  0.0623]],
       dtype=torch.float64)
	q_value: tensor([[2.0017]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17435064347026386, distance: 1.0398112614478636 entropy -5.086462060174266
epoch: 5, step: 108
	action: tensor([[-0.0496, -0.0561, -0.0691,  0.0103, -0.0564,  0.0658, -0.0141]],
       dtype=torch.float64)
	q_value: tensor([[2.0142]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20893087421591416, distance: 1.017803431534195 entropy -5.091733756529121
epoch: 5, step: 109
	action: tensor([[-0.0316, -0.0576, -0.0692, -0.0068, -0.0673,  0.0098, -0.0294]],
       dtype=torch.float64)
	q_value: tensor([[2.0525]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20880780583452896, distance: 1.0178825994257623 entropy -5.0903915057137485
epoch: 5, step: 110
	action: tensor([[-0.0090, -0.0569, -0.0692, -0.0424,  0.0472,  0.0314,  0.0372]],
       dtype=torch.float64)
	q_value: tensor([[2.0697]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21844871914791908, distance: 1.0116619895248224 entropy -5.090717378551856
epoch: 5, step: 111
	action: tensor([[-0.0333, -0.0542, -0.0692, -0.0006, -0.0381, -0.0202, -0.0115]],
       dtype=torch.float64)
	q_value: tensor([[1.9998]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2054956085274917, distance: 1.0200109741192738 entropy -5.088812042067383
epoch: 5, step: 112
	action: tensor([[-0.0182, -0.0551, -0.0692, -0.0352,  0.0069,  0.0367,  0.0368]],
       dtype=torch.float64)
	q_value: tensor([[2.0657]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21604153090519707, distance: 1.013218757858358 entropy -5.0899812443320025
epoch: 5, step: 113
	action: tensor([[-0.0033, -0.0558, -0.0692,  0.0232,  0.0049,  0.0366, -0.0080]],
       dtype=torch.float64)
	q_value: tensor([[2.0118]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25819713194082294, distance: 0.985600573057437 entropy -5.090030407011718
epoch: 5, step: 114
	action: tensor([[-0.0322, -0.0537, -0.0692, -0.0367, -0.0361, -0.0545, -0.0214]],
       dtype=torch.float64)
	q_value: tensor([[2.0319]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18404603725301638, distance: 1.0336881109888278 entropy -5.088328532127517
epoch: 5, step: 115
	action: tensor([[-0.0392, -0.0618, -0.0692,  0.0184,  0.0456, -0.0605, -0.0068]],
       dtype=torch.float64)
	q_value: tensor([[2.0761]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1904582914818792, distance: 1.0296184300297901 entropy -5.090710262037463
epoch: 5, step: 116
	action: tensor([[-0.0462, -0.0578, -0.0693, -0.0432,  0.0066, -0.0081, -0.0195]],
       dtype=torch.float64)
	q_value: tensor([[2.0653]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17157902663884972, distance: 1.0415550669842066 entropy -5.086629592823572
epoch: 5, step: 117
	action: tensor([[-0.0189, -0.0499, -0.0692, -0.0271, -0.0444,  0.0021,  0.0288]],
       dtype=torch.float64)
	q_value: tensor([[2.0565]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21585376413439938, distance: 1.013340089173384 entropy -5.089396405774713
epoch: 5, step: 118
	action: tensor([[-0.0412, -0.0521, -0.0692, -0.0136,  0.0163, -0.0106,  0.0921]],
       dtype=torch.float64)
	q_value: tensor([[2.0356]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19147895585533425, distance: 1.0289691575538507 entropy -5.091707568197977
epoch: 5, step: 119
	action: tensor([[-0.0380, -0.0550, -0.0692, -0.0281, -0.0030,  0.0633, -0.0097]],
       dtype=torch.float64)
	q_value: tensor([[2.0123]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2052433804933076, distance: 1.0201728706108557 entropy -5.090894340674757
epoch: 5, step: 120
	action: tensor([[-0.0515, -0.0550, -0.0692, -0.0144, -0.0168, -0.0202, -0.0115]],
       dtype=torch.float64)
	q_value: tensor([[2.0301]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17864246896914193, distance: 1.0371052077389198 entropy -5.089158392816144
epoch: 5, step: 121
	action: tensor([[-0.0529, -0.0497, -0.0693,  0.0122, -0.0093,  0.0221,  0.0044]],
       dtype=torch.float64)
	q_value: tensor([[2.0649]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20189070343994975, distance: 1.0223224032713332 entropy -5.089530036028633
epoch: 5, step: 122
	action: tensor([[-0.0380, -0.0558, -0.0692, -0.0201, -0.0745,  0.0523,  0.0747]],
       dtype=torch.float64)
	q_value: tensor([[2.0448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2042901753969294, distance: 1.0207844682710308 entropy -5.089243646010603
epoch: 5, step: 123
	action: tensor([[-0.0286, -0.0545, -0.0691,  0.0343, -0.0343,  0.0241,  0.0469]],
       dtype=torch.float64)
	q_value: tensor([[2.0235]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.234090681423751, distance: 1.0014871240378815 entropy -5.093896864743661
epoch: 5, step: 124
	action: tensor([[-0.0040, -0.0556, -0.0692, -0.0332, -0.0334,  0.0385,  0.0398]],
       dtype=torch.float64)
	q_value: tensor([[2.0337]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23127960983710039, distance: 1.0033232902112021 entropy -5.090947097514998
epoch: 5, step: 125
	action: tensor([[-0.0096, -0.0527, -0.0692, -0.0528, -0.0944, -0.0186, -0.0512]],
       dtype=torch.float64)
	q_value: tensor([[2.0158]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21037836668935495, distance: 1.016871820555329 entropy -5.091437643317347
epoch: 5, step: 126
	action: tensor([[-0.0383, -0.0522, -0.0692, -0.0327, -0.0312, -0.0111, -0.0351]],
       dtype=torch.float64)
	q_value: tensor([[2.0787]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1894298280177652, distance: 1.030272249850278 entropy -5.092293629047027
epoch: 5, step: 127
	action: tensor([[-0.0094, -0.0519, -0.0692, -0.0207, -0.1519, -0.0041, -0.0094]],
       dtype=torch.float64)
	q_value: tensor([[2.0676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22871515995179537, distance: 1.0049954391768536 entropy -5.090211080806549
LOSS epoch 5 actor 2.084386579320234 critic 2.1942808508376794 entropy 0.01
epoch: 6, step: 0
	action: tensor([[-0.0508, -0.0510, -0.0829,  0.0315, -0.0975,  0.0507,  0.0536]],
       dtype=torch.float64)
	q_value: tensor([[5.3851]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19160871733278761, distance: 1.0288865833791934 entropy -4.869359102811282
epoch: 6, step: 1
	action: tensor([[-0.0540, -0.0312, -0.0935, -0.0141, -0.0740, -0.0233,  0.0114]],
       dtype=torch.float64)
	q_value: tensor([[2.6310]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1741520819396204, distance: 1.0399362867398718 entropy -4.824263769499292
epoch: 6, step: 2
	action: tensor([[-0.0042, -0.0677, -0.0935,  0.0028,  0.0786,  0.0197,  0.0546]],
       dtype=torch.float64)
	q_value: tensor([[2.6554]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21344179501340632, distance: 1.014897367829072 entropy -4.82457282949312
epoch: 6, step: 3
	action: tensor([[-0.0436, -0.0567, -0.0936,  0.0257, -0.0461, -0.0002, -0.0714]],
       dtype=torch.float64)
	q_value: tensor([[2.5878]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1912045974113996, distance: 1.0291437247058206 entropy -4.822911304219828
epoch: 6, step: 4
	action: tensor([[-0.0303, -0.0612, -0.0934, -0.0615, -0.0688,  0.0044, -0.0007]],
       dtype=torch.float64)
	q_value: tensor([[2.7005]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17056979222921698, distance: 1.0421893177398573 entropy -4.824653825496665
epoch: 6, step: 5
	action: tensor([[-0.0418, -0.0701, -0.0934,  0.0386, -0.0389, -0.0102, -0.0275]],
       dtype=torch.float64)
	q_value: tensor([[2.6546]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.198030370494081, distance: 1.0247918295876595 entropy -4.825189071976224
epoch: 6, step: 6
	action: tensor([[-0.0554, -0.0615, -0.0935,  0.0062, -0.0180, -0.0573,  0.0940]],
       dtype=torch.float64)
	q_value: tensor([[2.6907]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16797311722017982, distance: 1.0438194200448112 entropy -4.824195631470788
epoch: 6, step: 7
	action: tensor([[-0.0451, -0.0877, -0.0936,  0.0162, -0.0107, -0.0227,  0.0044]],
       dtype=torch.float64)
	q_value: tensor([[2.6412]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17214041629402332, distance: 1.0412020958785377 entropy -4.823490327378606
epoch: 6, step: 8
	action: tensor([[-0.0151, -0.0594, -0.0936, -0.0149, -0.0703,  0.0511,  0.0350]],
       dtype=torch.float64)
	q_value: tensor([[2.6808]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22842388526542723, distance: 1.0051851888543522 entropy -4.823845447789074
epoch: 6, step: 9
	action: tensor([[-0.0729, -0.0760, -0.0934,  0.0929, -0.0475,  0.0143,  0.0763]],
       dtype=torch.float64)
	q_value: tensor([[2.6225]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19273939380096494, distance: 1.028166792764954 entropy -4.824696642663531
epoch: 6, step: 10
	action: tensor([[ 0.0014, -0.0360, -0.0936, -0.0426, -0.0508, -0.0543,  0.0402]],
       dtype=torch.float64)
	q_value: tensor([[2.6643]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22641763357398204, distance: 1.0064911813692388 entropy -4.82371490206516
epoch: 6, step: 11
	action: tensor([[-0.0148, -0.0877, -0.0935, -0.0297, -0.0414,  0.0599, -0.0313]],
       dtype=torch.float64)
	q_value: tensor([[2.6290]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20360977443641848, distance: 1.0212208046722424 entropy -4.824640695581303
epoch: 6, step: 12
	action: tensor([[-0.0385, -0.0506, -0.0934, -0.0414, -0.0595, -0.0397, -0.0279]],
       dtype=torch.float64)
	q_value: tensor([[2.6562]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18269930979404392, distance: 1.0345408075185467 entropy -4.824749703624017
epoch: 6, step: 13
	action: tensor([[-0.0361, -0.0742, -0.0934, -0.0333,  0.0078,  0.0549, -0.0302]],
       dtype=torch.float64)
	q_value: tensor([[2.6819]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1889751822476181, distance: 1.0305611472665013 entropy -4.824935870446086
epoch: 6, step: 14
	action: tensor([[-0.0393, -0.0464, -0.0934, -0.0409, -0.0713,  0.0501, -0.0256]],
       dtype=torch.float64)
	q_value: tensor([[2.6403]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20261883177614537, distance: 1.0218559560258016 entropy -4.824514814072409
epoch: 6, step: 15
	action: tensor([[-0.0059, -0.0681, -0.0934, -0.0131,  0.0116,  0.0255,  0.0317]],
       dtype=torch.float64)
	q_value: tensor([[2.6439]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22620638631166934, distance: 1.0066285965964523 entropy -4.825196152074738
epoch: 6, step: 16
	action: tensor([[-0.0410, -0.0619, -0.0935, -0.0031, -0.0771,  0.0297,  0.0442]],
       dtype=torch.float64)
	q_value: tensor([[2.6101]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20021181673860877, distance: 1.0233971068338081 entropy -4.823747924786142
epoch: 6, step: 17
	action: tensor([[-0.0182, -0.0425, -0.0935, -0.0240,  0.0747,  0.0712,  0.0510]],
       dtype=torch.float64)
	q_value: tensor([[2.6410]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23708879434215135, distance: 0.9995250669577489 entropy -4.824636421480634
epoch: 6, step: 18
	action: tensor([[-0.0246, -0.0534, -0.0935,  0.0459, -0.0068,  0.0231,  0.0342]],
       dtype=torch.float64)
	q_value: tensor([[2.5522]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2430909210091703, distance: 0.9955854719339269 entropy -4.8234407817234315
epoch: 6, step: 19
	action: tensor([[-0.0106, -0.0730, -0.0936, -0.0232,  0.0546, -0.0044,  0.0426]],
       dtype=torch.float64)
	q_value: tensor([[2.6239]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20441405834994641, distance: 1.0207050029227307 entropy -4.823663055395629
epoch: 6, step: 20
	action: tensor([[-0.0287, -0.0643, -0.0936, -0.0252, -0.1127, -0.0165,  0.0441]],
       dtype=torch.float64)
	q_value: tensor([[2.6115]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1938030848148542, distance: 1.0274891848949943 entropy -4.823342417156682
epoch: 6, step: 21
	action: tensor([[-0.0159, -0.0269, -0.0935, -0.0303, -0.0137,  0.0702,  0.0100]],
       dtype=torch.float64)
	q_value: tensor([[2.6625]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2506021357453533, distance: 0.9906332912207243 entropy -4.82477319901704
epoch: 6, step: 22
	action: tensor([[-0.0202, -0.0546, -0.0934, -0.0499,  0.0085,  0.0420,  0.0283]],
       dtype=torch.float64)
	q_value: tensor([[2.5845]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20989340834118064, distance: 1.017184036408707 entropy -4.824736750037364
epoch: 6, step: 23
	action: tensor([[-0.0231, -0.0377, -0.0935, -0.0461, -0.0080,  0.0455, -0.0282]],
       dtype=torch.float64)
	q_value: tensor([[2.5972]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2233988802074347, distance: 1.0084530863993262 entropy -4.8244320940157195
epoch: 6, step: 24
	action: tensor([[-0.0310, -0.0351, -0.0934, -0.0034,  0.0802,  0.0500,  0.0949]],
       dtype=torch.float64)
	q_value: tensor([[2.6171]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23308470323864272, distance: 1.002144606312303 entropy -4.824974983835121
epoch: 6, step: 25
	action: tensor([[-0.0535, -0.0445, -0.0935, -0.0222, -0.0365,  0.0379, -0.0392]],
       dtype=torch.float64)
	q_value: tensor([[2.5460]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19450454831268382, distance: 1.0270420838265675 entropy -4.8230666057349465
epoch: 6, step: 26
	action: tensor([[ 0.0109, -0.0809, -0.0934, -0.0484, -0.1058, -0.0515,  0.0664]],
       dtype=torch.float64)
	q_value: tensor([[2.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2034667627757104, distance: 1.0213124933460593 entropy -4.824959615856238
epoch: 6, step: 27
	action: tensor([[-0.0168, -0.0581, -0.0935, -0.0132, -0.0168,  0.0065,  0.0909]],
       dtype=torch.float64)
	q_value: tensor([[2.6614]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21815529065332362, distance: 1.0118518827566998 entropy -4.824793372412997
epoch: 6, step: 28
	action: tensor([[-0.0311, -0.0525, -0.0935, -0.0212, -0.1427,  0.0167,  0.0268]],
       dtype=torch.float64)
	q_value: tensor([[2.5997]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2085591808554954, distance: 1.0180425170561496 entropy -4.823982888484717
epoch: 6, step: 29
	action: tensor([[ 3.1632e-05, -5.0643e-02, -9.3367e-02, -1.0646e-02, -1.8138e-01,
         -2.5842e-03,  5.2397e-02]], dtype=torch.float64)
	q_value: tensor([[2.6596]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24432630526511478, distance: 0.9947726709533202 entropy -4.8252809450831675
epoch: 6, step: 30
	action: tensor([[-0.0181, -0.0433, -0.0933,  0.0180,  0.0241,  0.0589,  0.0114]],
       dtype=torch.float64)
	q_value: tensor([[2.6579]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25422738556872315, distance: 0.9882342671921901 entropy -4.82559759241192
epoch: 6, step: 31
	action: tensor([[-0.0319, -0.0719, -0.0935,  0.0249,  0.0432,  0.0149,  0.0630]],
       dtype=torch.float64)
	q_value: tensor([[2.5989]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20952668356317417, distance: 1.017420069951034 entropy -4.823825566941147
epoch: 6, step: 32
	action: tensor([[-0.0343, -0.0452, -0.0829, -0.0342, -0.0592,  0.0337,  0.0267]],
       dtype=torch.float64)
	q_value: tensor([[5.3851]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18517312179402812, distance: 1.0329739429436362 entropy -4.869359102811282
epoch: 6, step: 33
	action: tensor([[-0.0414, -0.0363, -0.0935, -0.0162, -0.0068, -0.0465,  0.0166]],
       dtype=torch.float64)
	q_value: tensor([[2.6144]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17933306448821784, distance: 1.036669118219673 entropy -4.824435560393913
epoch: 6, step: 34
	action: tensor([[ 0.0015, -0.0318, -0.0935,  0.0510, -0.0507, -0.0607,  0.0423]],
       dtype=torch.float64)
	q_value: tensor([[2.6425]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25408328738030017, distance: 0.9883297358959096 entropy -4.824183533332406
epoch: 6, step: 35
	action: tensor([[-0.0597, -0.0646, -0.0935, -0.0380,  0.0348, -0.0162,  0.0503]],
       dtype=torch.float64)
	q_value: tensor([[2.6441]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1392096330677679, distance: 1.0617087396815175 entropy -4.8239657665622415
epoch: 6, step: 36
	action: tensor([[ 6.8594e-05, -4.8806e-02, -9.3575e-02, -2.4237e-02, -5.0459e-02,
          1.4302e-02, -8.2882e-03]], dtype=torch.float64)
	q_value: tensor([[2.6266]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2260269925231122, distance: 1.0067452765801026 entropy -4.823729898079277
epoch: 6, step: 37
	action: tensor([[-0.0545, -0.0558, -0.0934,  0.0874, -0.0388, -0.0648,  0.0214]],
       dtype=torch.float64)
	q_value: tensor([[2.6360]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20343336432834713, distance: 1.021333904815415 entropy -4.824852859359425
epoch: 6, step: 38
	action: tensor([[-0.0336, -0.0776, -0.0936,  0.0106,  0.0104,  0.0130,  0.0019]],
       dtype=torch.float64)
	q_value: tensor([[2.6940]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19248074523570957, distance: 1.02833149333728 entropy -4.823434256779026
epoch: 6, step: 39
	action: tensor([[-0.0567, -0.0621, -0.0935, -0.0578,  0.0260,  0.0527,  0.0272]],
       dtype=torch.float64)
	q_value: tensor([[2.6510]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16174904881350227, distance: 1.0477163486353003 entropy -4.823878830743821
epoch: 6, step: 40
	action: tensor([[-0.0405, -0.0602, -0.0935, -0.0304,  0.0412,  0.0148,  0.0113]],
       dtype=torch.float64)
	q_value: tensor([[2.6065]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1836527587138178, distance: 1.0339371926744954 entropy -4.824432249130827
epoch: 6, step: 41
	action: tensor([[-0.0601, -0.0501, -0.0935, -0.0213, -0.0673,  0.0164,  0.0302]],
       dtype=torch.float64)
	q_value: tensor([[2.6204]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17841436907796893, distance: 1.0372492054222036 entropy -4.823939461463096
epoch: 6, step: 42
	action: tensor([[-0.0574, -0.0474, -0.0934, -0.0687,  0.0096,  0.0559, -0.0413]],
       dtype=torch.float64)
	q_value: tensor([[2.6453]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1721325631529379, distance: 1.041207034329289 entropy -4.824824076707436
epoch: 6, step: 43
	action: tensor([[-0.0235, -0.0658, -0.0934, -0.0569, -0.0621,  0.0101, -0.0441]],
       dtype=torch.float64)
	q_value: tensor([[2.6271]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1898069212356226, distance: 1.0300325704940776 entropy -4.824963133102277
epoch: 6, step: 44
	action: tensor([[-0.0518, -0.0609, -0.0934,  0.0253,  0.0398,  0.0246,  0.0112]],
       dtype=torch.float64)
	q_value: tensor([[2.6709]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2001767072860796, distance: 1.023419569354869 entropy -4.825192282630089
epoch: 6, step: 45
	action: tensor([[-1.9265e-02, -5.9499e-02, -9.3565e-02,  3.6956e-05, -6.5205e-02,
          2.0717e-02,  4.8507e-03]], dtype=torch.float64)
	q_value: tensor([[2.6309]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2253001207380948, distance: 1.007217904823866 entropy -4.823461974933362
epoch: 6, step: 46
	action: tensor([[ 0.0170, -0.0624, -0.0935, -0.0703, -0.0953,  0.0402, -0.0226]],
       dtype=torch.float64)
	q_value: tensor([[2.6487]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2343627494750543, distance: 1.0013092329710016 entropy -4.824572302668215
epoch: 6, step: 47
	action: tensor([[ 0.0196, -0.0475, -0.0933,  0.0242,  0.0246, -0.0002,  0.0073]],
       dtype=torch.float64)
	q_value: tensor([[2.6394]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.279830910362865, distance: 0.9711223107809839 entropy -4.825442591195263
epoch: 6, step: 48
	action: tensor([[-0.0375, -0.0695, -0.0935, -0.0258, -0.1344,  0.0246, -0.0112]],
       dtype=torch.float64)
	q_value: tensor([[2.6133]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1884997866499032, distance: 1.0308631432403619 entropy -4.823672025865519
epoch: 6, step: 49
	action: tensor([[-0.0274, -0.0435, -0.0933, -0.0052, -0.0824,  0.0417,  0.1055]],
       dtype=torch.float64)
	q_value: tensor([[2.6852]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22918241673844675, distance: 1.0046909718399415 entropy -4.825481875453851
epoch: 6, step: 50
	action: tensor([[ 0.0093, -0.0456, -0.0934, -0.0671,  0.0810, -0.0519,  0.0507]],
       dtype=torch.float64)
	q_value: tensor([[2.5973]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.213549870658858, distance: 1.0148276403407241 entropy -4.824798128748961
epoch: 6, step: 51
	action: tensor([[-0.0360, -0.0613, -0.0936, -0.0358, -0.0478,  0.0565,  0.0207]],
       dtype=torch.float64)
	q_value: tensor([[2.5887]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1975371792199916, distance: 1.0251068918347097 entropy -4.823416960219802
epoch: 6, step: 52
	action: tensor([[-0.0251, -0.0391, -0.0934, -0.0400,  0.0262,  0.0144, -0.0324]],
       dtype=torch.float64)
	q_value: tensor([[2.6245]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21593270096489015, distance: 1.0132890834652934 entropy -4.824738268613612
epoch: 6, step: 53
	action: tensor([[-0.0700, -0.0480, -0.0935,  0.0329, -0.0017,  0.0076, -0.0188]],
       dtype=torch.float64)
	q_value: tensor([[2.6219]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19064120274948693, distance: 1.0295021052936086 entropy -4.824391343799038
epoch: 6, step: 54
	action: tensor([[-0.0184, -0.0327, -0.0935,  0.0102,  0.0040,  0.0094,  0.0611]],
       dtype=torch.float64)
	q_value: tensor([[2.6622]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24422199143016243, distance: 0.9948413282137941 entropy -4.823857585773551
epoch: 6, step: 55
	action: tensor([[-0.0261, -0.0422, -0.0935, -0.0224,  0.0280,  0.0057, -0.0096]],
       dtype=torch.float64)
	q_value: tensor([[2.5917]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2166479901004107, distance: 1.0128267761781173 entropy -4.823858288695931
epoch: 6, step: 56
	action: tensor([[-0.0066, -0.0739, -0.0935, -0.0152, -0.0163, -0.0271, -0.0614]],
       dtype=torch.float64)
	q_value: tensor([[2.6221]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2113875441411598, distance: 1.0162218053046022 entropy -4.8241888092957295
epoch: 6, step: 57
	action: tensor([[-0.0431, -0.0565, -0.0934, -0.0478, -0.0262, -0.0160, -0.0097]],
       dtype=torch.float64)
	q_value: tensor([[2.6891]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17396127834181874, distance: 1.0400564130494812 entropy -4.824707787870477
epoch: 6, step: 58
	action: tensor([[-0.0322, -0.0680, -0.0934, -0.0173, -0.0267, -0.0223,  0.0100]],
       dtype=torch.float64)
	q_value: tensor([[2.6585]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18819727878319936, distance: 1.0310552658970142 entropy -4.8247819004500565
epoch: 6, step: 59
	action: tensor([[-0.0490, -0.0511, -0.0935, -0.0109, -0.0669,  0.0598, -0.0101]],
       dtype=torch.float64)
	q_value: tensor([[2.6607]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20330215520397554, distance: 1.0214180175583976 entropy -4.824387447560176
epoch: 6, step: 60
	action: tensor([[-0.0298, -0.0542, -0.0934, -0.0151,  0.0604, -0.0490,  0.0536]],
       dtype=torch.float64)
	q_value: tensor([[2.6454]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19269935352612233, distance: 1.028192291079982 entropy -4.824980371125686
epoch: 6, step: 61
	action: tensor([[-0.0332, -0.0668, -0.0936,  0.0739,  0.0304,  0.1044,  0.0515]],
       dtype=torch.float64)
	q_value: tensor([[2.6191]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2548135591563063, distance: 0.9878458172233276 entropy -4.823227097945969
epoch: 6, step: 62
	action: tensor([[-0.0147, -0.0517, -0.0935, -0.0227,  0.0425,  0.0160, -0.0041]],
       dtype=torch.float64)
	q_value: tensor([[2.5990]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22260995951128482, distance: 1.008965181695529 entropy -4.823262906382302
epoch: 6, step: 63
	action: tensor([[-0.0299, -0.0391, -0.0935, -0.0150, -0.0931, -0.0607,  0.0792]],
       dtype=torch.float64)
	q_value: tensor([[2.6123]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20791702194472883, distance: 1.0184554427626447 entropy -4.823879995142116
epoch: 6, step: 64
	action: tensor([[-0.0089, -0.0673, -0.0829,  0.0694,  0.0880, -0.0116,  0.0401]],
       dtype=torch.float64)
	q_value: tensor([[5.3851]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22565298767310893, distance: 1.006988490578787 entropy -4.869359102811282
epoch: 6, step: 65
	action: tensor([[-0.0373, -0.0641, -0.0937, -0.0487, -0.0159, -0.0676, -0.0043]],
       dtype=torch.float64)
	q_value: tensor([[2.6090]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13584059882866417, distance: 1.0637844134690866 entropy -4.821880355898001
epoch: 6, step: 66
	action: tensor([[-0.0268, -0.0278, -0.0935,  0.0166, -0.0489,  0.0165,  0.0942]],
       dtype=torch.float64)
	q_value: tensor([[2.6789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22210616555244045, distance: 1.0092920628039332 entropy -4.824430358471612
epoch: 6, step: 67
	action: tensor([[-0.0349, -0.0454, -0.0935,  0.0073, -0.0051, -0.0007,  0.0497]],
       dtype=torch.float64)
	q_value: tensor([[2.5923]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1944813531005143, distance: 1.0270568711773185 entropy -4.824385798504752
epoch: 6, step: 68
	action: tensor([[-0.0349, -0.0678, -0.0935, -0.0003,  0.0220,  0.0535,  0.0046]],
       dtype=torch.float64)
	q_value: tensor([[2.6156]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18787354636397302, distance: 1.0312608286134974 entropy -4.823887602905907
epoch: 6, step: 69
	action: tensor([[-0.0243, -0.0620, -0.0935,  0.0566, -0.0267,  0.0273,  0.0173]],
       dtype=torch.float64)
	q_value: tensor([[2.6224]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22393561553377095, distance: 1.0081045381305445 entropy -4.823977206245769
epoch: 6, step: 70
	action: tensor([[-0.0323, -0.0724, -0.0935,  0.0408, -0.0373, -0.0824,  0.0016]],
       dtype=torch.float64)
	q_value: tensor([[2.6453]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18289852515107163, distance: 1.0344147162407438 entropy -4.823957200552636
epoch: 6, step: 71
	action: tensor([[-0.0544, -0.0467, -0.0936, -0.0004,  0.0705, -0.0452, -0.0390]],
       dtype=torch.float64)
	q_value: tensor([[2.7052]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17115431750872578, distance: 1.0418220213944485 entropy -4.823855619677627
epoch: 6, step: 72
	action: tensor([[-0.0032, -0.0579, -0.0936, -0.0010, -0.0904,  0.0228,  0.0427]],
       dtype=torch.float64)
	q_value: tensor([[2.6610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23091490261929715, distance: 1.0035612674130276 entropy -4.823513812616587
epoch: 6, step: 73
	action: tensor([[-0.0472, -0.0640, -0.0935, -0.0694, -0.0484, -0.0066, -0.0295]],
       dtype=torch.float64)
	q_value: tensor([[2.6319]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15333037970070795, distance: 1.05296438426314 entropy -4.824664950398832
epoch: 6, step: 74
	action: tensor([[-0.0271, -0.0521, -0.0934, -0.0191,  0.0354, -0.0211,  0.0639]],
       dtype=torch.float64)
	q_value: tensor([[2.6717]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20085594772773818, distance: 1.0229849135903615 entropy -4.825067159586394
epoch: 6, step: 75
	action: tensor([[ 0.0126, -0.0739, -0.0936,  0.0137, -0.0142, -0.0288,  0.0155]],
       dtype=torch.float64)
	q_value: tensor([[2.6045]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23776997193717442, distance: 0.9990787462575138 entropy -4.823487344398145
epoch: 6, step: 76
	action: tensor([[-0.0078, -0.0477, -0.0936,  0.0075, -0.0180,  0.0259,  0.0338]],
       dtype=torch.float64)
	q_value: tensor([[2.6505]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2493550948821428, distance: 0.9914571838104828 entropy -4.823839277023964
epoch: 6, step: 77
	action: tensor([[-0.0405, -0.0548, -0.0935,  0.0245, -0.1242,  0.0188,  0.0923]],
       dtype=torch.float64)
	q_value: tensor([[2.6108]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21576380773483417, distance: 1.0133982121405019 entropy -4.824194486467927
epoch: 6, step: 78
	action: tensor([[-0.0056, -0.0464, -0.0934, -0.0687, -0.1130,  0.0238, -0.0961]],
       dtype=torch.float64)
	q_value: tensor([[2.6417]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22239556870402732, distance: 1.009104299729429 entropy -4.824938845457338
epoch: 6, step: 79
	action: tensor([[-0.0535, -0.0725, -0.0932, -0.0024, -0.0197,  0.0165, -0.0106]],
       dtype=torch.float64)
	q_value: tensor([[2.6849]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1766231731420458, distance: 1.0383792793612505 entropy -4.826008056860595
epoch: 6, step: 80
	action: tensor([[-0.0306, -0.0594, -0.0935, -0.0218, -0.0543, -0.0020, -0.0065]],
       dtype=torch.float64)
	q_value: tensor([[2.6632]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19955482381829293, distance: 1.023817359748689 entropy -4.824228543720793
epoch: 6, step: 81
	action: tensor([[-0.0409, -0.0701, -0.0935, -0.0372, -0.0949,  0.0842, -0.0412]],
       dtype=torch.float64)
	q_value: tensor([[2.6595]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19101027809217774, distance: 1.0292673471305618 entropy -4.8246622716426915
epoch: 6, step: 82
	action: tensor([[-0.0038, -0.0560, -0.0933, -0.0165, -0.0055, -0.0101, -0.0196]],
       dtype=torch.float64)
	q_value: tensor([[2.6647]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2303215129535353, distance: 1.0039483429563578 entropy -4.825583490979169
epoch: 6, step: 83
	action: tensor([[-0.0125, -0.0391, -0.0935,  0.0113, -0.0275,  0.0278,  0.0384]],
       dtype=torch.float64)
	q_value: tensor([[2.6423]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.253494167952544, distance: 0.9887199465728774 entropy -4.824280404587968
epoch: 6, step: 84
	action: tensor([[-0.0729, -0.0662, -0.0935, -0.0400, -0.1064,  0.0210, -0.0153]],
       dtype=torch.float64)
	q_value: tensor([[2.6056]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1457436798253029, distance: 1.0576714794121886 entropy -4.824177347494369
epoch: 6, step: 85
	action: tensor([[-0.0089, -0.0572, -0.0934, -0.0220, -0.0655,  0.0534,  0.0571]],
       dtype=torch.float64)
	q_value: tensor([[2.6889]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23222076005561132, distance: 1.0027089139408238 entropy -4.825388814020649
epoch: 6, step: 86
	action: tensor([[-0.0683, -0.0614, -0.0934, -0.0705, -0.0106, -0.0402, -0.0057]],
       dtype=torch.float64)
	q_value: tensor([[2.6075]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12767034545775924, distance: 1.0688013937815934 entropy -4.824761563566539
epoch: 6, step: 87
	action: tensor([[-0.0364, -0.0535, -0.0935, -0.0722, -0.0417, -0.0267,  0.0141]],
       dtype=torch.float64)
	q_value: tensor([[2.6715]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17084904668385925, distance: 1.0420138596316137 entropy -4.824649960432781
epoch: 6, step: 88
	action: tensor([[-0.0094, -0.0741, -0.0934,  0.0161,  0.0404,  0.1248,  0.0596]],
       dtype=torch.float64)
	q_value: tensor([[2.6472]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2535664968864221, distance: 0.9886720468855086 entropy -4.824863022817178
epoch: 6, step: 89
	action: tensor([[-0.0467, -0.0585, -0.0935,  0.0063,  0.0007, -0.0353, -0.0161]],
       dtype=torch.float64)
	q_value: tensor([[2.5694]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18732213420403654, distance: 1.0316108684790104 entropy -4.823419544086546
epoch: 6, step: 90
	action: tensor([[-0.0539, -0.0400, -0.0936, -0.0407,  0.0324,  0.0042,  0.0028]],
       dtype=torch.float64)
	q_value: tensor([[2.6716]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18119544425060974, distance: 1.0354921680532587 entropy -4.823924725973515
epoch: 6, step: 91
	action: tensor([[-0.0460, -0.0503, -0.0935, -0.0479,  0.0411, -0.0091,  0.0247]],
       dtype=torch.float64)
	q_value: tensor([[2.6197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17508201532660495, distance: 1.0393506196669118 entropy -4.824100325636231
epoch: 6, step: 92
	action: tensor([[-0.0432, -0.0524, -0.0935,  0.0029, -0.1421,  0.0537,  0.0056]],
       dtype=torch.float64)
	q_value: tensor([[2.6162]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2126407695311816, distance: 1.0154140185840748 entropy -4.823843043621709
epoch: 6, step: 93
	action: tensor([[-0.0343, -0.0484, -0.0933,  0.0345, -0.0302, -0.0475,  0.0669]],
       dtype=torch.float64)
	q_value: tensor([[2.6643]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21767072102262286, distance: 1.012165395619069 entropy -4.825404861044078
epoch: 6, step: 94
	action: tensor([[-0.0185, -0.0510, -0.0936, -0.0593, -0.0813, -0.0119,  0.0527]],
       dtype=torch.float64)
	q_value: tensor([[2.6407]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20036342418595443, distance: 1.0233001049152437 entropy -4.823784458387154
epoch: 6, step: 95
	action: tensor([[-0.0175, -0.0559, -0.0934, -0.0357, -0.1174,  0.0072,  0.0398]],
       dtype=torch.float64)
	q_value: tensor([[2.6305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2120674967338232, distance: 1.0157836105651874 entropy -4.825058348963478
epoch: 6, step: 96
	action: tensor([[-0.0146, -0.0467, -0.0829, -0.0606,  0.0819, -0.0012,  0.0008]],
       dtype=torch.float64)
	q_value: tensor([[5.3851]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1855579925252323, distance: 1.0327299595872066 entropy -4.869359102811282
epoch: 6, step: 97
	action: tensor([[-0.0449, -0.0550, -0.0936,  0.0359,  0.0637,  0.0400, -0.0185]],
       dtype=torch.float64)
	q_value: tensor([[2.5910]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2020095209322934, distance: 1.0222463019741186 entropy -4.8234146797594315
epoch: 6, step: 98
	action: tensor([[-0.0537, -0.0607, -0.0936,  0.0509,  0.0641,  0.0145,  0.0410]],
       dtype=torch.float64)
	q_value: tensor([[2.6285]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1894348135210817, distance: 1.0302690814425361 entropy -4.823220244252522
epoch: 6, step: 99
	action: tensor([[ 0.0089, -0.0404, -0.0936,  0.0375,  0.0059, -0.0613,  0.0019]],
       dtype=torch.float64)
	q_value: tensor([[2.6191]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2453485464815237, distance: 0.994099601499941 entropy -4.822726052514822
epoch: 6, step: 100
	action: tensor([[-0.0103, -0.0713, -0.0936, -0.0483, -0.0660,  0.0233,  0.0452]],
       dtype=torch.float64)
	q_value: tensor([[2.6451]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18618816525026927, distance: 1.0323303459965245 entropy -4.823634834481085
epoch: 6, step: 101
	action: tensor([[-0.0854, -0.0570, -0.0934,  0.0198, -0.0005,  0.0002,  0.0358]],
       dtype=torch.float64)
	q_value: tensor([[2.6273]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14940951927246793, distance: 1.0553996657488602 entropy -4.8247370795172015
epoch: 6, step: 102
	action: tensor([[-0.0273, -0.0475, -0.0936, -0.0512, -0.1176,  0.0594,  0.0097]],
       dtype=torch.float64)
	q_value: tensor([[2.6488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20057123041433633, distance: 1.0231671307873327 entropy -4.823750897655964
epoch: 6, step: 103
	action: tensor([[-0.0285, -0.0475, -0.0933, -0.0093,  0.0852,  0.0337, -0.0055]],
       dtype=torch.float64)
	q_value: tensor([[2.6354]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21573203816847675, distance: 1.0134187384156295 entropy -4.825591731666785
epoch: 6, step: 104
	action: tensor([[ 0.0075, -0.0560, -0.0935, -0.0344, -0.1199,  0.0523, -0.0306]],
       dtype=torch.float64)
	q_value: tensor([[2.5973]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24137739237873346, distance: 0.9967117629489891 entropy -4.823363555300331
epoch: 6, step: 105
	action: tensor([[-0.0584, -0.0337, -0.0933, -0.0052,  0.0088,  0.0427, -0.0204]],
       dtype=torch.float64)
	q_value: tensor([[2.6515]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2052161178330184, distance: 1.020190368035359 entropy -4.825440735796925
epoch: 6, step: 106
	action: tensor([[-4.1138e-03, -4.7370e-02, -9.3463e-02,  2.9197e-02,  4.4751e-02,
          3.9063e-02,  9.1868e-05]], dtype=torch.float64)
	q_value: tensor([[2.6240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26373489750085854, distance: 0.9819147895659713 entropy -4.824262005603481
epoch: 6, step: 107
	action: tensor([[-0.0612, -0.0606, -0.0935,  0.0016, -0.0178,  0.0791,  0.0049]],
       dtype=torch.float64)
	q_value: tensor([[2.6053]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19233712005675097, distance: 1.0284229386664496 entropy -4.823453985998851
epoch: 6, step: 108
	action: tensor([[-0.0028, -0.0611, -0.0934, -0.0327, -0.0315,  0.0108, -0.0172]],
       dtype=torch.float64)
	q_value: tensor([[2.6301]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22348575095343082, distance: 1.0083966819468333 entropy -4.824348014649026
epoch: 6, step: 109
	action: tensor([[-0.0085, -0.0617, -0.0934, -0.0486, -0.0785,  0.0611,  0.0475]],
       dtype=torch.float64)
	q_value: tensor([[2.6422]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22104548201293295, distance: 1.0099799296759393 entropy -4.824667281394338
epoch: 6, step: 110
	action: tensor([[-0.0244, -0.0425, -0.0934,  0.0348, -0.0654,  0.0788,  0.0030]],
       dtype=torch.float64)
	q_value: tensor([[2.6097]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25940690573373526, distance: 0.9847965587221529 entropy -4.8249051899463735
epoch: 6, step: 111
	action: tensor([[-0.0140, -0.0639, -0.0934,  0.0304, -0.0978,  0.0011,  0.0156]],
       dtype=torch.float64)
	q_value: tensor([[2.6287]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2372916450050322, distance: 0.9993921761309483 entropy -4.824620900351414
epoch: 6, step: 112
	action: tensor([[-0.0099, -0.0518, -0.0935,  0.0495, -0.0723,  0.0402,  0.0242]],
       dtype=torch.float64)
	q_value: tensor([[2.6697]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26605855749277896, distance: 0.9803640986754382 entropy -4.824641378803521
epoch: 6, step: 113
	action: tensor([[-0.0174, -0.0706, -0.0935, -0.0035, -0.0575, -0.0225,  0.0486]],
       dtype=torch.float64)
	q_value: tensor([[2.6382]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20796250127446547, distance: 1.0184262038210943 entropy -4.824459653431126
epoch: 6, step: 114
	action: tensor([[-0.0728, -0.0645, -0.0935,  0.0258,  0.0424, -0.0428, -0.0248]],
       dtype=torch.float64)
	q_value: tensor([[2.6515]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16003877119177923, distance: 1.0487846282242486 entropy -4.824390052627419
epoch: 6, step: 115
	action: tensor([[-0.0503, -0.0759, -0.0936, -0.0029, -0.0149,  0.0282,  0.0696]],
       dtype=torch.float64)
	q_value: tensor([[2.6830]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17756088552696137, distance: 1.037787825688739 entropy -4.823411487381106
epoch: 6, step: 116
	action: tensor([[-0.0241, -0.0601, -0.0936, -0.0208, -0.0927, -0.0588,  0.0464]],
       dtype=torch.float64)
	q_value: tensor([[2.6255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19475430420042306, distance: 1.0268828466182065 entropy -4.823929213630038
epoch: 6, step: 117
	action: tensor([[-0.0297, -0.0582, -0.0935,  0.0171,  0.0792,  0.0136,  0.0790]],
       dtype=torch.float64)
	q_value: tensor([[2.6707]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2176404563316664, distance: 1.0121849734218327 entropy -4.824593946327502
epoch: 6, step: 118
	action: tensor([[-0.0287, -0.0566, -0.0937, -0.0024, -0.0668,  0.0782, -0.0224]],
       dtype=torch.float64)
	q_value: tensor([[2.5828]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22849090686737195, distance: 1.0051415310846519 entropy -4.822595605402074
epoch: 6, step: 119
	action: tensor([[ 0.0068, -0.0511, -0.0934, -0.0875,  0.0071,  0.0217, -0.0377]],
       dtype=torch.float64)
	q_value: tensor([[2.6424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22115044415767626, distance: 1.0099118812671475 entropy -4.824843852312402
epoch: 6, step: 120
	action: tensor([[-0.0151, -0.0296, -0.0934,  0.0550, -0.0560, -0.0292,  0.0661]],
       dtype=torch.float64)
	q_value: tensor([[2.6169]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2653236032140671, distance: 0.9808548343515276 entropy -4.82512657054331
epoch: 6, step: 121
	action: tensor([[-0.0443, -0.0631, -0.0935,  0.0019,  0.0875, -0.0157, -0.0092]],
       dtype=torch.float64)
	q_value: tensor([[2.6274]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18644168788368476, distance: 1.032169535178302 entropy -4.824026001841566
epoch: 6, step: 122
	action: tensor([[-0.0173, -0.0455, -0.0936, -0.0228, -0.0710,  0.0637,  0.0856]],
       dtype=torch.float64)
	q_value: tensor([[2.6393]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23455753267737234, distance: 1.0011818550089786 entropy -4.823242835806333
epoch: 6, step: 123
	action: tensor([[-0.0207, -0.0470, -0.0934, -0.0133,  0.0731,  0.0113,  0.0751]],
       dtype=torch.float64)
	q_value: tensor([[2.5896]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22157141357143406, distance: 1.00963891489487 entropy -4.824866850962823
epoch: 6, step: 124
	action: tensor([[-0.0380, -0.0566, -0.0936,  0.0180,  0.0014, -0.0274, -0.0213]],
       dtype=torch.float64)
	q_value: tensor([[2.5706]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2050618528023107, distance: 1.0202893710888932 entropy -4.822934476106383
epoch: 6, step: 125
	action: tensor([[-0.0110, -0.0544, -0.0935, -0.0008,  0.0034,  0.0247, -0.0066]],
       dtype=torch.float64)
	q_value: tensor([[2.6683]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2380640197032481, distance: 0.9988860188649193 entropy -4.823891093565076
epoch: 6, step: 126
	action: tensor([[-0.0390, -0.0649, -0.0935, -0.0924, -0.1586,  0.0228,  0.0040]],
       dtype=torch.float64)
	q_value: tensor([[2.6260]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16104257575847336, distance: 1.048157760290542 entropy -4.824090702409812
epoch: 6, step: 127
	action: tensor([[-0.0052, -0.0514, -0.0933, -0.0131,  0.0340, -0.0373,  0.0191]],
       dtype=torch.float64)
	q_value: tensor([[2.6722]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2263816557171876, distance: 1.0065145861003564 entropy -4.826086466412858
LOSS epoch 6 actor 3.397674082189115 critic 2.0398685603455453 entropy 0.01
epoch: 7, step: 0
	action: tensor([[-0.0556, -0.0326, -0.0964, -0.0074,  0.0187,  0.0175, -0.0233]],
       dtype=torch.float64)
	q_value: tensor([[3.9574]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1801011059097668, distance: 1.036183908469541 entropy -4.715746911497666
epoch: 7, step: 1
	action: tensor([[-0.0045, -0.0541, -0.1184, -0.0396, -0.0228,  0.0308,  0.0187]],
       dtype=torch.float64)
	q_value: tensor([[2.2333]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.206943993872717, distance: 1.0190808075337545 entropy -4.6238303529685725
epoch: 7, step: 2
	action: tensor([[-0.0671, -0.0352, -0.1181, -0.0278, -0.1044,  0.0467, -0.0172]],
       dtype=torch.float64)
	q_value: tensor([[2.2280]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17250518589768904, distance: 1.0409726845574188 entropy -4.625789362420774
epoch: 7, step: 3
	action: tensor([[-0.0222, -0.0530, -0.1179, -0.0236, -0.0451,  0.0035, -0.0211]],
       dtype=torch.float64)
	q_value: tensor([[2.2576]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20473859754571055, distance: 1.0204967962712093 entropy -4.627114081852476
epoch: 7, step: 4
	action: tensor([[-0.0121, -0.0591, -0.1181, -0.0897, -0.0255,  0.0351,  0.0348]],
       dtype=torch.float64)
	q_value: tensor([[2.2640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1923577828105183, distance: 1.028409783310179 entropy -4.625812528832033
epoch: 7, step: 5
	action: tensor([[ 0.0097, -0.0456, -0.1179,  0.0918,  0.0053, -0.0556,  0.0458]],
       dtype=torch.float64)
	q_value: tensor([[2.2146]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28359857896031937, distance: 0.9685786962887858 entropy -4.626660859744809
epoch: 7, step: 6
	action: tensor([[-0.0650, -0.0443, -0.1186, -0.0570, -0.0425, -0.0303, -0.0052]],
       dtype=torch.float64)
	q_value: tensor([[2.2632]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15504569537290458, distance: 1.0518972135517834 entropy -4.62277359554389
epoch: 7, step: 7
	action: tensor([[ 0.0096, -0.0114, -0.1180, -0.0247,  0.0595,  0.0286,  0.0829]],
       dtype=torch.float64)
	q_value: tensor([[2.2675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2770016196641498, distance: 0.9730280396388944 entropy -4.62629032623056
epoch: 7, step: 8
	action: tensor([[-0.0562, -0.0628, -0.1182, -0.0388,  0.0249,  0.0255,  0.0069]],
       dtype=torch.float64)
	q_value: tensor([[2.1574]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16745185610865165, distance: 1.0441463429421547 entropy -4.62458463950933
epoch: 7, step: 9
	action: tensor([[ 0.0188, -0.0976, -0.1182,  0.0502,  0.0323, -0.0087, -0.0193]],
       dtype=torch.float64)
	q_value: tensor([[2.2414]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2466115795545094, distance: 0.9932673587050319 entropy -4.625318324127443
epoch: 7, step: 10
	action: tensor([[-0.0433, -0.0831, -0.1185, -0.0095, -0.0370,  0.0600, -0.0163]],
       dtype=torch.float64)
	q_value: tensor([[2.2920]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18581236020420266, distance: 1.0325686751527179 entropy -4.623691468637516
epoch: 7, step: 11
	action: tensor([[-0.0045, -0.0278, -0.1181,  0.0379, -0.0133,  0.0780, -0.0133]],
       dtype=torch.float64)
	q_value: tensor([[2.2690]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2917603568301834, distance: 0.9630455071100563 entropy -4.625779498313494
epoch: 7, step: 12
	action: tensor([[-0.0152, -0.0532, -0.1182, -0.0031,  0.0994, -0.0550,  0.1027]],
       dtype=torch.float64)
	q_value: tensor([[2.2261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21114298318459612, distance: 1.0163793661634348 entropy -4.625248227231859
epoch: 7, step: 13
	action: tensor([[-0.0576, -0.0809, -0.1185, -0.0512, -0.0676,  0.0062,  0.0275]],
       dtype=torch.float64)
	q_value: tensor([[2.2130]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14330524443846837, distance: 1.0591799410235692 entropy -4.62296463101014
epoch: 7, step: 14
	action: tensor([[-0.0388, -0.1075, -0.1180, -0.0292, -0.0041,  0.0326, -0.0254]],
       dtype=torch.float64)
	q_value: tensor([[2.2682]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15715219210918552, distance: 1.050585189466764 entropy -4.626324287597349
epoch: 7, step: 15
	action: tensor([[-0.0503, -0.0338, -0.1181,  0.0627, -0.0686, -0.0272, -0.0031]],
       dtype=torch.float64)
	q_value: tensor([[2.2877]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22911509013238962, distance: 1.0047348479513305 entropy -4.625738284876323
epoch: 7, step: 16
	action: tensor([[-0.0331, -0.0490, -0.1183, -0.0069, -0.0543,  0.0465,  0.0059]],
       dtype=torch.float64)
	q_value: tensor([[2.2861]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22091195930783758, distance: 1.010066487665339 entropy -4.624625393835587
epoch: 7, step: 17
	action: tensor([[-0.0573, -0.0247, -0.1181,  0.0023,  0.0170, -0.0022,  0.0175]],
       dtype=torch.float64)
	q_value: tensor([[2.2448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2063920575384064, distance: 1.0194353662856088 entropy -4.625982419462184
epoch: 7, step: 18
	action: tensor([[ 0.0107, -0.0304, -0.1182,  0.0407, -0.0439,  0.0159,  0.0646]],
       dtype=torch.float64)
	q_value: tensor([[2.2338]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.290475525668052, distance: 0.9639186509837071 entropy -4.624815334777232
epoch: 7, step: 19
	action: tensor([[-0.0229, -0.0486, -0.1183, -0.0614, -0.0965, -0.0540,  0.0200]],
       dtype=torch.float64)
	q_value: tensor([[2.2183]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19192151835576643, distance: 1.0286875040955434 entropy -4.624602984171799
epoch: 7, step: 20
	action: tensor([[-0.0384, -0.0273, -0.1180, -0.0365, -0.1122,  0.0923, -0.0329]],
       dtype=torch.float64)
	q_value: tensor([[2.2700]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2278708785769883, distance: 1.0055453442269817 entropy -4.626657767475217
epoch: 7, step: 21
	action: tensor([[-0.0491, -0.0269, -0.1178,  0.0790, -0.0840, -0.0835,  0.0113]],
       dtype=torch.float64)
	q_value: tensor([[2.2377]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23323089674626263, distance: 1.002049084669683 entropy -4.62776381404961
epoch: 7, step: 22
	action: tensor([[-0.0497, -0.0477, -0.1184,  0.0386, -0.0671,  0.0687,  0.0187]],
       dtype=torch.float64)
	q_value: tensor([[2.3007]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2267241402433927, distance: 1.0062917670359226 entropy -4.624009837809351
epoch: 7, step: 23
	action: tensor([[ 0.0086, -0.0585, -0.1181, -0.0113,  0.0459, -0.0745,  0.0270]],
       dtype=torch.float64)
	q_value: tensor([[2.2473]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22592917371409094, distance: 1.0068088934708892 entropy -4.625462214005608
epoch: 7, step: 24
	action: tensor([[-0.0258, -0.0513, -0.1184,  0.1377,  0.0384,  0.0363,  0.0272]],
       dtype=torch.float64)
	q_value: tensor([[2.2571]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28393661217394905, distance: 0.9683501579364895 entropy -4.6237160409312565
epoch: 7, step: 25
	action: tensor([[-0.0564, -0.0272, -0.1186, -0.0308,  0.0204,  0.0862,  0.1410]],
       dtype=torch.float64)
	q_value: tensor([[2.2540]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20814822677587008, distance: 1.0183067910361552 entropy -4.622695448701087
epoch: 7, step: 26
	action: tensor([[ 0.0042, -0.0589, -0.1180,  0.0309, -0.0289,  0.1106, -0.0086]],
       dtype=torch.float64)
	q_value: tensor([[2.1506]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2786239431125578, distance: 0.9719357462314542 entropy -4.625458041279264
epoch: 7, step: 27
	action: tensor([[-0.0414, -0.0583, -0.1181,  0.0091,  0.0868,  0.1067,  0.0528]],
       dtype=torch.float64)
	q_value: tensor([[2.2346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2248819998144258, distance: 1.007489675920678 entropy -4.625739014362321
epoch: 7, step: 28
	action: tensor([[-0.0171, -0.0584, -0.1182,  0.0317,  0.0187, -0.0221,  0.0011]],
       dtype=torch.float64)
	q_value: tensor([[2.1910]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23070760548140246, distance: 1.0036965069090922 entropy -4.624342724088947
epoch: 7, step: 29
	action: tensor([[ 0.0019, -0.0440, -0.1184,  0.0327, -0.0325, -0.0420, -0.0382]],
       dtype=torch.float64)
	q_value: tensor([[2.2683]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26150500313783376, distance: 0.9834006068968352 entropy -4.624005703780581
epoch: 7, step: 30
	action: tensor([[-0.0136, -0.0301, -0.1183, -0.0316, -0.0291, -0.0315,  0.0173]],
       dtype=torch.float64)
	q_value: tensor([[2.2894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2304600940238667, distance: 1.0038579581389566 entropy -4.624826181998552
epoch: 7, step: 31
	action: tensor([[-0.0225, -0.0339, -0.1181,  0.0433,  0.0654, -0.0265,  0.0446]],
       dtype=torch.float64)
	q_value: tensor([[2.2413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24680449249737313, distance: 0.993140182343021 entropy -4.625686788432707
epoch: 7, step: 32
	action: tensor([[-2.8477e-02, -5.1638e-02, -9.6358e-02, -5.9253e-02, -4.4612e-02,
          9.8804e-05,  3.8304e-03]], dtype=torch.float64)
	q_value: tensor([[3.9574]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17241535887378912, distance: 1.0410291833651033 entropy -4.715746911497666
epoch: 7, step: 33
	action: tensor([[-0.0488, -0.0244, -0.1182, -0.0366, -0.0318,  0.0238,  0.0715]],
       dtype=torch.float64)
	q_value: tensor([[2.2360]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1909579255974858, distance: 1.029300650298779 entropy -4.625112325874218
epoch: 7, step: 34
	action: tensor([[-0.0463, -0.0474, -0.1180,  0.0021,  0.0984, -0.0945, -0.0256]],
       dtype=torch.float64)
	q_value: tensor([[2.2011]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16743733873016664, distance: 1.0441554464406648 entropy -4.625870305499109
epoch: 7, step: 35
	action: tensor([[-0.0189, -0.0865, -0.1184,  0.0184, -0.0500,  0.1141,  0.0299]],
       dtype=torch.float64)
	q_value: tensor([[2.2845]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21981299049390213, distance: 1.0107786282320965 entropy -4.6235418016110845
epoch: 7, step: 36
	action: tensor([[-0.0674, -0.0536, -0.1180, -0.0232, -0.0802,  0.0218,  0.0592]],
       dtype=torch.float64)
	q_value: tensor([[2.2416]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16439140464644497, distance: 1.0460637262716146 entropy -4.625797363475599
epoch: 7, step: 37
	action: tensor([[-0.0261, -0.0668, -0.1180,  0.0619, -0.0546,  0.0221,  0.0623]],
       dtype=torch.float64)
	q_value: tensor([[2.2430]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23473579440454684, distance: 1.0010652670180435 entropy -4.626076400722968
epoch: 7, step: 38
	action: tensor([[ 0.0044, -0.0131, -0.1183,  0.0113, -0.1431,  0.0530, -0.0093]],
       dtype=torch.float64)
	q_value: tensor([[2.2543]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.295148946005372, distance: 0.960738887881383 entropy -4.624294900439732
epoch: 7, step: 39
	action: tensor([[-0.0239, -0.0121, -0.1179, -0.0055,  0.0033, -0.0306,  0.0308]],
       dtype=torch.float64)
	q_value: tensor([[2.2386]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24300853651344856, distance: 0.995639651872837 entropy -4.6269628770101665
epoch: 7, step: 40
	action: tensor([[-0.0545, -0.0513, -0.1183, -0.0211, -0.0835, -0.0233, -0.0056]],
       dtype=torch.float64)
	q_value: tensor([[2.2219]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17796313057068824, distance: 1.037534009900864 entropy -4.624706418782284
epoch: 7, step: 41
	action: tensor([[-0.0466, -0.0565, -0.1181,  0.0186,  0.0296, -0.0729, -0.0513]],
       dtype=torch.float64)
	q_value: tensor([[2.2822]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18668346037568573, distance: 1.0320161541974924 entropy -4.626065496789627
epoch: 7, step: 42
	action: tensor([[-0.0350, -0.0402, -0.1184,  0.0490, -0.0891,  0.0726,  0.0485]],
       dtype=torch.float64)
	q_value: tensor([[2.3080]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25249309537818065, distance: 0.9893826666585391 entropy -4.624158440966977
epoch: 7, step: 43
	action: tensor([[ 0.0228, -0.0595, -0.1181, -0.0444, -0.0138,  0.0447, -0.0100]],
       dtype=torch.float64)
	q_value: tensor([[2.2334]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25401899400162775, distance: 0.9883723289144686 entropy -4.625459389712732
epoch: 7, step: 44
	action: tensor([[-0.0720, -0.0287, -0.1180, -0.0430, -0.0618, -0.0361, -0.0314]],
       dtype=torch.float64)
	q_value: tensor([[2.2320]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16504103642729395, distance: 1.0456570236752614 entropy -4.6261879401551
epoch: 7, step: 45
	action: tensor([[-0.0494, -0.0546, -0.1180, -0.0085, -0.0632,  0.0291, -0.0667]],
       dtype=torch.float64)
	q_value: tensor([[2.2804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19617818560420142, distance: 1.0259745484818006 entropy -4.62654152351992
epoch: 7, step: 46
	action: tensor([[-0.0161, -0.0769, -0.1180, -0.0867, -0.0891, -0.0597,  0.0359]],
       dtype=torch.float64)
	q_value: tensor([[2.2908]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16408814150848916, distance: 1.046253530321154 entropy -4.626541380548315
epoch: 7, step: 47
	action: tensor([[-0.0313, -0.0684, -0.1179, -0.0662, -0.0629, -0.0304,  0.0116]],
       dtype=torch.float64)
	q_value: tensor([[2.2795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16848853156627874, distance: 1.043496063396606 entropy -4.626983846869574
epoch: 7, step: 48
	action: tensor([[-0.0070, -0.0105, -0.1180, -0.0963,  0.0082, -0.0881,  0.0077]],
       dtype=torch.float64)
	q_value: tensor([[2.2718]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2143591770552451, distance: 1.014305345364449 entropy -4.626586981725772
epoch: 7, step: 49
	action: tensor([[-0.0437, -0.0732, -0.1180,  0.0293, -0.0032,  0.0124,  0.0554]],
       dtype=torch.float64)
	q_value: tensor([[2.2378]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19815143443612981, distance: 1.024714476271131 entropy -4.626363809362847
epoch: 7, step: 50
	action: tensor([[-0.0424, -0.0371, -0.1184, -0.0550,  0.0204, -0.0779,  0.0370]],
       dtype=torch.float64)
	q_value: tensor([[2.2503]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17403991555996556, distance: 1.0400069062361328 entropy -4.624028977038493
epoch: 7, step: 51
	action: tensor([[-0.0237, -0.0310, -0.1182,  0.0276, -0.0112, -0.0571,  0.0396]],
       dtype=torch.float64)
	q_value: tensor([[2.2498]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23773528085985762, distance: 0.9991014813390563 entropy -4.625266653817621
epoch: 7, step: 52
	action: tensor([[-0.0276, -0.0620, -0.1184,  0.0019, -0.0554,  0.0712,  0.0501]],
       dtype=torch.float64)
	q_value: tensor([[2.2521]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22466615554837488, distance: 1.0076299421335808 entropy -4.6241420367254955
epoch: 7, step: 53
	action: tensor([[-0.0540, -0.0740, -0.1181, -0.0017, -0.0665,  0.0300,  0.1336]],
       dtype=torch.float64)
	q_value: tensor([[2.2302]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1760334487006393, distance: 1.038751070267339 entropy -4.625744138466085
epoch: 7, step: 54
	action: tensor([[-0.0476, -0.0108, -0.1181,  0.0139, -0.1147, -0.0247,  0.0642]],
       dtype=torch.float64)
	q_value: tensor([[2.2259]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22997198207942915, distance: 1.004176276513934 entropy -4.6252522616721565
epoch: 7, step: 55
	action: tensor([[-0.0065, -0.0858, -0.1180,  0.0237, -0.0094, -0.0379,  0.0215]],
       dtype=torch.float64)
	q_value: tensor([[2.2397]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2158046701700015, distance: 1.0133718103634746 entropy -4.625933451915873
epoch: 7, step: 56
	action: tensor([[-0.0533, -0.0640, -0.1184, -0.0357, -0.0939,  0.0763,  0.0578]],
       dtype=torch.float64)
	q_value: tensor([[2.2837]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18016218222828884, distance: 1.036145313785776 entropy -4.624098948060544
epoch: 7, step: 57
	action: tensor([[-0.0288, -0.0605, -0.1179, -0.0718, -0.0458, -0.0624,  0.0522]],
       dtype=torch.float64)
	q_value: tensor([[2.2324]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1680931473589925, distance: 1.0437441254170463 entropy -4.6269444825780806
epoch: 7, step: 58
	action: tensor([[-0.0326, -0.0695, -0.1180, -0.0078, -0.0913,  0.0016,  0.0880]],
       dtype=torch.float64)
	q_value: tensor([[2.2576]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19592212881140036, distance: 1.026137947154038 entropy -4.626206270591856
epoch: 7, step: 59
	action: tensor([[-0.0658, -0.0690, -0.1181, -0.0681,  0.0952,  0.0273, -0.0463]],
       dtype=torch.float64)
	q_value: tensor([[2.2462]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1406330572425485, distance: 1.0608305426042755 entropy -4.6255369778731525
epoch: 7, step: 60
	action: tensor([[-0.0586, -0.0237, -0.1181, -0.0514, -0.0022, -0.0587, -0.0042]],
       dtype=torch.float64)
	q_value: tensor([[2.2526]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17387149763940346, distance: 1.0401129324690672 entropy -4.625765712464785
epoch: 7, step: 61
	action: tensor([[-0.0547, -0.0275, -0.1181,  0.0650, -0.0244, -0.0011,  0.0239]],
       dtype=torch.float64)
	q_value: tensor([[2.2583]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2334328926803606, distance: 1.0019170871932812 entropy -4.625778006161292
epoch: 7, step: 62
	action: tensor([[-0.0004, -0.0403, -0.1183, -0.0178, -0.0377, -0.0562,  0.0456]],
       dtype=torch.float64)
	q_value: tensor([[2.2535]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2353635769356056, distance: 1.0006545721087026 entropy -4.6241609439791365
epoch: 7, step: 63
	action: tensor([[-0.0378, -0.0576, -0.1182, -0.0377, -0.0140,  0.0163,  0.0252]],
       dtype=torch.float64)
	q_value: tensor([[2.2472]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18989658142997579, distance: 1.02997557452726 entropy -4.625281267832702
epoch: 7, step: 64
	action: tensor([[-0.0310, -0.0601, -0.0964,  0.0479,  0.0763,  0.0193,  0.0392]],
       dtype=torch.float64)
	q_value: tensor([[3.9574]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20561281579512125, distance: 1.0199357340639543 entropy -4.715746911497666
epoch: 7, step: 65
	action: tensor([[-0.0555, -0.0568, -0.1187,  0.0041,  0.0688,  0.0489,  0.0145]],
       dtype=torch.float64)
	q_value: tensor([[2.2196]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17048138427280957, distance: 1.042244859105378 entropy -4.621674436183228
epoch: 7, step: 66
	action: tensor([[-0.0482, -0.0675, -0.1183, -0.0461, -0.0451,  0.0309, -0.0102]],
       dtype=torch.float64)
	q_value: tensor([[2.2288]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14675928021527895, distance: 1.0570425750691332 entropy -4.62455154402489
epoch: 7, step: 67
	action: tensor([[-0.0479, -0.0645, -0.1180, -0.0627, -0.0566, -0.0324,  0.0608]],
       dtype=torch.float64)
	q_value: tensor([[2.2620]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1349680470996374, distance: 1.0643213356740222 entropy -4.626498396655504
epoch: 7, step: 68
	action: tensor([[-0.0574, -0.0224, -0.1180, -0.0231, -0.0867, -0.0139,  0.0687]],
       dtype=torch.float64)
	q_value: tensor([[2.2529]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18174155876887454, distance: 1.0351467915981387 entropy -4.626304448150182
epoch: 7, step: 69
	action: tensor([[-0.0050, -0.0265, -0.1180,  0.0434, -0.0620, -0.0009,  0.0236]],
       dtype=torch.float64)
	q_value: tensor([[2.2304]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26431316515575753, distance: 0.9815291125218405 entropy -4.6262049106799354
epoch: 7, step: 70
	action: tensor([[-0.0555, -0.0481, -0.1183, -0.0397, -0.0599, -0.0601, -0.0005]],
       dtype=torch.float64)
	q_value: tensor([[2.2451]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1596182725776727, distance: 1.04904711492411 entropy -4.6248651306452375
epoch: 7, step: 71
	action: tensor([[-0.0317, -0.0592, -0.1181,  0.0348,  0.0370,  0.0814,  0.0277]],
       dtype=torch.float64)
	q_value: tensor([[2.2847]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2367607808181993, distance: 0.9997399166702982 entropy -4.626061301647651
epoch: 7, step: 72
	action: tensor([[-0.0040, -0.0347, -0.1183, -0.0426,  0.0275,  0.0351,  0.0045]],
       dtype=torch.float64)
	q_value: tensor([[2.2226]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24149790030028817, distance: 0.9966325954999325 entropy -4.624569242664959
epoch: 7, step: 73
	action: tensor([[-0.0547, -0.0680, -0.1181, -0.0931, -0.0803,  0.0820,  0.1273]],
       dtype=torch.float64)
	q_value: tensor([[2.2110]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15149389501903798, distance: 1.0541057417895014 entropy -4.625783100056359
epoch: 7, step: 74
	action: tensor([[-0.0224, -0.0364, -0.1177, -0.0130, -0.0659,  0.0591,  0.0028]],
       dtype=torch.float64)
	q_value: tensor([[2.1927]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24165768711832136, distance: 0.9965276141564364 entropy -4.627732944585065
epoch: 7, step: 75
	action: tensor([[-0.0528, -0.0873, -0.1180,  0.1012, -0.0846, -0.0002,  0.0414]],
       dtype=torch.float64)
	q_value: tensor([[2.2327]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20704614517395892, distance: 1.0190151729617998 entropy -4.626343735865823
epoch: 7, step: 76
	action: tensor([[-0.0545, -0.0165, -0.1184,  0.0141,  0.0438,  0.0484,  0.0376]],
       dtype=torch.float64)
	q_value: tensor([[2.3048]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23040221623452406, distance: 1.0038957079597017 entropy -4.624295099441545
epoch: 7, step: 77
	action: tensor([[ 0.0207, -0.0777, -0.1183, -0.0209, -0.0910,  0.0037, -0.0157]],
       dtype=torch.float64)
	q_value: tensor([[2.1978]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23914350036700505, distance: 0.998178177027312 entropy -4.62449576729344
epoch: 7, step: 78
	action: tensor([[-0.0621, -0.0449, -0.1181, -0.0106,  0.0053, -0.1457,  0.0020]],
       dtype=torch.float64)
	q_value: tensor([[2.2806]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15351720490268816, distance: 1.0528482048631298 entropy -4.626307893972664
epoch: 7, step: 79
	action: tensor([[-0.0292, -0.0443, -0.1183, -0.0002,  0.0302, -0.0076,  0.0454]],
       dtype=torch.float64)
	q_value: tensor([[2.3115]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21819756639023258, distance: 1.011824526069531 entropy -4.624424393624323
epoch: 7, step: 80
	action: tensor([[-0.0140, -0.0283, -0.1183, -0.0531, -0.0779,  0.0531,  0.0429]],
       dtype=torch.float64)
	q_value: tensor([[2.2264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23852347243040606, distance: 0.9985848058205369 entropy -4.624180992695771
epoch: 7, step: 81
	action: tensor([[-0.0369, -0.0364, -0.1179,  0.0742,  0.0075, -0.0464,  0.0600]],
       dtype=torch.float64)
	q_value: tensor([[2.2041]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23962978163547677, distance: 0.9978591464794967 entropy -4.626897113180073
epoch: 7, step: 82
	action: tensor([[-0.0276, -0.0227, -0.1185, -0.0078, -0.0358,  0.0685,  0.0064]],
       dtype=torch.float64)
	q_value: tensor([[2.2514]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.250526948923511, distance: 0.9906829849176388 entropy -4.623039415089038
epoch: 7, step: 83
	action: tensor([[-0.0224, -0.0226, -0.1180, -0.1043, -0.0729,  0.0403, -0.0272]],
       dtype=torch.float64)
	q_value: tensor([[2.2162]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21188468530154114, distance: 1.0159014417792473 entropy -4.626130365648548
epoch: 7, step: 84
	action: tensor([[ 0.0065, -0.0516, -0.1177,  0.0020,  0.0082,  0.0541, -0.0418]],
       dtype=torch.float64)
	q_value: tensor([[2.2256]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2666237261673853, distance: 0.97998656325345 entropy -4.628187523788646
epoch: 7, step: 85
	action: tensor([[-0.0492, -0.0312, -0.1181,  0.0060, -0.1729, -0.1041, -0.0252]],
       dtype=torch.float64)
	q_value: tensor([[2.2482]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2016397330699321, distance: 1.0224831284163056 entropy -4.6255702996046955
epoch: 7, step: 86
	action: tensor([[ 0.0149, -0.0184, -0.1180,  0.0058, -0.0159,  0.0536, -0.0295]],
       dtype=torch.float64)
	q_value: tensor([[2.3335]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3010228186875279, distance: 0.9567273565538683 entropy -4.626766775387444
epoch: 7, step: 87
	action: tensor([[-0.0216, -0.0789, -0.1181, -0.1199, -0.0070,  0.0965,  0.0280]],
       dtype=torch.float64)
	q_value: tensor([[2.2258]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17241659724005043, distance: 1.0410284044865235 entropy -4.625787906412727
epoch: 7, step: 88
	action: tensor([[-0.0324, -0.0627, -0.1178,  0.0315,  0.0842,  0.1388, -0.0441]],
       dtype=torch.float64)
	q_value: tensor([[2.2051]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25054063438625307, distance: 0.99067393988463 entropy -4.627562200787622
epoch: 7, step: 89
	action: tensor([[-0.0417, -0.0351, -0.1182,  0.0290, -0.0584,  0.0001, -0.0094]],
       dtype=torch.float64)
	q_value: tensor([[2.2298]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22842063280113079, distance: 1.0051873074564126 entropy -4.6252069251017
epoch: 7, step: 90
	action: tensor([[-0.0253, -0.0459, -0.1182,  0.0714, -0.0471, -0.0378, -0.0665]],
       dtype=torch.float64)
	q_value: tensor([[2.2674]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24905574418363474, distance: 0.9916548563815807 entropy -4.625085791515394
epoch: 7, step: 91
	action: tensor([[-0.0251, -0.1086, -0.1184, -0.0518,  0.0161, -0.0020,  0.0242]],
       dtype=torch.float64)
	q_value: tensor([[2.3167]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15239991671064657, distance: 1.0535428125761153 entropy -4.624612397664754
epoch: 7, step: 92
	action: tensor([[-0.0208, -0.0505, -0.1182,  0.0275, -0.0583,  0.0476, -0.0033]],
       dtype=torch.float64)
	q_value: tensor([[2.2684]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24741293842158363, distance: 0.9927389623575797 entropy -4.625130821782058
epoch: 7, step: 93
	action: tensor([[-0.0533, -0.0820, -0.1182, -0.0451,  0.0621,  0.0396,  0.0052]],
       dtype=torch.float64)
	q_value: tensor([[2.2539]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1556537420916646, distance: 1.051518661862693 entropy -4.625385730047803
epoch: 7, step: 94
	action: tensor([[-0.0491, -0.0536, -0.1182, -0.0028, -0.0170,  0.0029,  0.0528]],
       dtype=torch.float64)
	q_value: tensor([[2.2403]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19231975717909777, distance: 1.028433992960349 entropy -4.62506329096069
epoch: 7, step: 95
	action: tensor([[ 0.0031, -0.0699, -0.1182,  0.0075,  0.0928, -0.0030, -0.0022]],
       dtype=torch.float64)
	q_value: tensor([[2.2379]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23392828673725152, distance: 1.001593290370912 entropy -4.624795846176377
epoch: 7, step: 96
	action: tensor([[ 0.0165, -0.0482, -0.0964, -0.0467,  0.0282,  0.0058, -0.0252]],
       dtype=torch.float64)
	q_value: tensor([[3.9574]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22591064196235733, distance: 1.0068209452238774 entropy -4.715746911497666
epoch: 7, step: 97
	action: tensor([[ 0.0059, -0.0774, -0.1183,  0.0148, -0.0251, -0.0095,  0.0393]],
       dtype=torch.float64)
	q_value: tensor([[2.2252]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21957801684301748, distance: 1.010930827936465 entropy -4.62441886843322
epoch: 7, step: 98
	action: tensor([[-0.0350, -0.0735, -0.1183,  0.0349, -0.0093, -0.0110,  0.0122]],
       dtype=torch.float64)
	q_value: tensor([[2.2605]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19418660986486969, distance: 1.0272447565727247 entropy -4.62457230096347
epoch: 7, step: 99
	action: tensor([[-0.0315, -0.0479, -0.1184,  0.0062, -0.0199, -0.0186, -0.0147]],
       dtype=torch.float64)
	q_value: tensor([[2.2783]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20803554622658715, distance: 1.0183792410139956 entropy -4.62425838091077
epoch: 7, step: 100
	action: tensor([[-0.0860, -0.0631, -0.1182,  0.0147, -0.0076,  0.0391,  0.0710]],
       dtype=torch.float64)
	q_value: tensor([[2.2716]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1587174440863134, distance: 1.049609215686593 entropy -4.625107608551232
epoch: 7, step: 101
	action: tensor([[-0.0370,  0.0100, -0.1182,  0.0035, -0.0596,  0.0962,  0.0022]],
       dtype=torch.float64)
	q_value: tensor([[2.2380]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2724118543543309, distance: 0.9761116599565192 entropy -4.624791259834119
epoch: 7, step: 102
	action: tensor([[-0.0578, -0.0525, -0.1179, -0.0130, -0.0148,  0.0855, -0.0391]],
       dtype=torch.float64)
	q_value: tensor([[2.1988]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19795725785177487, distance: 1.0248385417873058 entropy -4.626679651336491
epoch: 7, step: 103
	action: tensor([[-0.0830, -0.0565, -0.1180,  0.0233,  0.0047,  0.0287,  0.0208]],
       dtype=torch.float64)
	q_value: tensor([[2.2499]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16926291766877966, distance: 1.0430100466028807 entropy -4.626232900216047
epoch: 7, step: 104
	action: tensor([[-0.0259, -0.0547, -0.1183,  0.0356, -0.1318,  0.0336,  0.0863]],
       dtype=torch.float64)
	q_value: tensor([[2.2543]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23734618132271446, distance: 0.9993564454724494 entropy -4.624647948556512
epoch: 7, step: 105
	action: tensor([[-0.0172, -0.0257, -0.1180, -0.1128,  0.0586, -0.0172,  0.0399]],
       dtype=torch.float64)
	q_value: tensor([[2.2450]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19693197178119481, distance: 1.0254933803857103 entropy -4.625790488366524
epoch: 7, step: 106
	action: tensor([[-0.0098, -0.0398, -0.1180,  0.0124, -0.0117,  0.0028,  0.0685]],
       dtype=torch.float64)
	q_value: tensor([[2.1933]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2505454102545027, distance: 0.9906707833854456 entropy -4.626103720480208
epoch: 7, step: 107
	action: tensor([[-0.0256, -0.0915, -0.1183,  0.0246, -0.1086,  0.0976,  0.0605]],
       dtype=torch.float64)
	q_value: tensor([[2.2185]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21819112915612482, distance: 1.0118286916602628 entropy -4.624590002458288
epoch: 7, step: 108
	action: tensor([[-0.0554, -0.0697, -0.1180,  0.0193,  0.0257,  0.0713, -0.0003]],
       dtype=torch.float64)
	q_value: tensor([[2.2538]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19753483034476615, distance: 1.0251083921200546 entropy -4.626137498822694
epoch: 7, step: 109
	action: tensor([[-0.0568, -0.0572, -0.1182, -0.1000, -0.2057,  0.0045,  0.0505]],
       dtype=torch.float64)
	q_value: tensor([[2.2471]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1438075910182356, distance: 1.0588693558125444 entropy -4.6249003092075585
epoch: 7, step: 110
	action: tensor([[ 0.0033, -0.0552, -0.1175, -0.0080, -0.0525,  0.0466, -0.0069]],
       dtype=torch.float64)
	q_value: tensor([[2.2650]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25359983663906294, distance: 0.9886499669216384 entropy -4.629133656766635
epoch: 7, step: 111
	action: tensor([[-0.0480, -0.0690, -0.1181,  0.0516,  0.0977,  0.0260,  0.0451]],
       dtype=torch.float64)
	q_value: tensor([[2.2440]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20784957003321414, distance: 1.0184988064668643 entropy -4.625749908591693
epoch: 7, step: 112
	action: tensor([[-0.0076, -0.0804, -0.1185, -0.0518, -0.0695,  0.0055, -0.0138]],
       dtype=torch.float64)
	q_value: tensor([[2.2335]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19528727059507967, distance: 1.0265429598976414 entropy -4.622828845321611
epoch: 7, step: 113
	action: tensor([[-0.0392, -0.1082, -0.1180,  0.0322, -0.0214, -0.0043,  0.0213]],
       dtype=torch.float64)
	q_value: tensor([[2.2747]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17442746253893904, distance: 1.0397628878918743 entropy -4.6265309592451045
epoch: 7, step: 114
	action: tensor([[-0.0255, -0.0502, -0.1184,  0.0884, -0.0245, -0.0427,  0.0310]],
       dtype=torch.float64)
	q_value: tensor([[2.2972]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24970276383888135, distance: 0.9912275553907361 entropy -4.624198757793101
epoch: 7, step: 115
	action: tensor([[-0.0313, -0.0767, -0.1185, -0.0420,  0.0028,  0.0243,  0.1523]],
       dtype=torch.float64)
	q_value: tensor([[2.2785]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.178663546614239, distance: 1.0370919005767438 entropy -4.623255088983752
epoch: 7, step: 116
	action: tensor([[-0.0560, -0.0451, -0.1181, -0.0064, -0.0401, -0.0667, -0.0384]],
       dtype=torch.float64)
	q_value: tensor([[2.1965]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17878823377031794, distance: 1.0370131770845423 entropy -4.624937333439692
epoch: 7, step: 117
	action: tensor([[-0.0222, -0.0538, -0.1182, -0.0277,  0.1183,  0.0545, -0.0100]],
       dtype=torch.float64)
	q_value: tensor([[2.3033]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2208295769248123, distance: 1.0101198895148695 entropy -4.6252899152824485
epoch: 7, step: 118
	action: tensor([[-0.0196, -0.0723, -0.1182,  0.0145, -0.0607,  0.0632,  0.0402]],
       dtype=torch.float64)
	q_value: tensor([[2.2146]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22925398313942869, distance: 1.004644330593344 entropy -4.624841308128727
epoch: 7, step: 119
	action: tensor([[-0.0015, -0.0228, -0.1181,  0.0107, -0.0195,  0.0477, -0.0134]],
       dtype=torch.float64)
	q_value: tensor([[2.2452]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2817414125216692, distance: 0.9698333333639501 entropy -4.625540405234473
epoch: 7, step: 120
	action: tensor([[-0.0725, -0.0714, -0.1181, -0.0009, -0.0879,  0.0117, -0.0011]],
       dtype=torch.float64)
	q_value: tensor([[2.2274]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15731685139287777, distance: 1.0504825629588985 entropy -4.62555057280811
epoch: 7, step: 121
	action: tensor([[-0.0406, -0.0281, -0.1181,  0.0845, -0.0378, -0.0030,  0.0380]],
       dtype=torch.float64)
	q_value: tensor([[2.2892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2558631868918154, distance: 0.9871498600185172 entropy -4.625942871974063
epoch: 7, step: 122
	action: tensor([[ 0.0089, -0.0697, -0.1184, -0.0028,  0.0360,  0.0543, -0.0573]],
       dtype=torch.float64)
	q_value: tensor([[2.2528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2527901914387678, distance: 0.9891860324178439 entropy -4.623951404774681
epoch: 7, step: 123
	action: tensor([[ 0.0023, -0.0521, -0.1182, -0.0225, -0.0303,  0.0121,  0.0194]],
       dtype=torch.float64)
	q_value: tensor([[2.2598]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2421631841108982, distance: 0.9961954253298074 entropy -4.625350113254031
epoch: 7, step: 124
	action: tensor([[ 0.0039, -0.0621, -0.1182,  0.0240,  0.1028,  0.0572,  0.0955]],
       dtype=torch.float64)
	q_value: tensor([[2.2367]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2624754485019082, distance: 0.9827542583839096 entropy -4.625477032426878
epoch: 7, step: 125
	action: tensor([[ 0.0504, -0.0719, -0.1184, -0.0251,  0.0793,  0.0018,  0.0013]],
       dtype=torch.float64)
	q_value: tensor([[2.1822]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2643905037247405, distance: 0.981477519881817 entropy -4.62309376639628
epoch: 7, step: 126
	action: tensor([[ 0.0170, -0.0378, -0.1183, -0.0235, -0.0365,  0.0026,  0.0363]],
       dtype=torch.float64)
	q_value: tensor([[2.2324]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2637957848975341, distance: 0.9818741877017872 entropy -4.624327376527228
epoch: 7, step: 127
	action: tensor([[-0.0215, -0.0842, -0.1181, -0.0309, -0.0545, -0.0287,  0.0553]],
       dtype=torch.float64)
	q_value: tensor([[2.2216]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.180443886469265, distance: 1.035967283691955 entropy -4.62557230194872
LOSS epoch 7 actor 2.3330663805669554 critic 2.268023131105881 entropy 0.01
epoch: 8, step: 0
	action: tensor([[-0.0630, -0.0393, -0.1207, -0.0271,  0.0201, -0.0335,  0.0366]],
       dtype=torch.float64)
	q_value: tensor([[2.5755]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15098052197936185, distance: 1.0544245772329008 entropy -4.552159664225893
epoch: 8, step: 1
	action: tensor([[-0.0365,  0.0272, -0.1616,  0.0694,  0.0072,  0.1359,  0.0872]],
       dtype=torch.float64)
	q_value: tensor([[1.6608]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.298267114140532, distance: 0.958611441299614 entropy -4.4072176477395235
epoch: 8, step: 2
	action: tensor([[-8.0199e-02,  3.3836e-05, -1.6064e-01,  6.8897e-02,  3.2518e-02,
          9.1964e-02, -2.2624e-02]], dtype=torch.float64)
	q_value: tensor([[1.6043]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2312928201370309, distance: 1.0033146692231916 entropy -4.4113865932594285
epoch: 8, step: 3
	action: tensor([[ 0.0030, -0.0459, -0.1609, -0.0271, -0.0726, -0.0083,  0.0353]],
       dtype=torch.float64)
	q_value: tensor([[1.6648]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23114939780674337, distance: 1.0034082620894353 entropy -4.410704885522472
epoch: 8, step: 4
	action: tensor([[-0.0376, -0.0677, -0.1606,  0.0521, -0.0416,  0.0749,  0.0122]],
       dtype=torch.float64)
	q_value: tensor([[1.6765]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22763083830473008, distance: 1.0057016345513403 entropy -4.411845972686734
epoch: 8, step: 5
	action: tensor([[ 0.0054, -0.0730, -0.1609, -0.0143, -0.0058,  0.0879,  0.0300]],
       dtype=torch.float64)
	q_value: tensor([[1.6925]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24697670367483893, distance: 0.9930266396999083 entropy -4.410707791908345
epoch: 8, step: 6
	action: tensor([[-0.0685, -0.0405, -0.1606,  0.0466, -0.0280,  0.1082, -0.0461]],
       dtype=torch.float64)
	q_value: tensor([[1.6658]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22173404641335903, distance: 1.0095334402076717 entropy -4.411922856729905
epoch: 8, step: 7
	action: tensor([[-0.0374,  0.0142, -0.1607, -0.0092, -0.0631, -0.0488, -0.0646]],
       dtype=torch.float64)
	q_value: tensor([[1.6911]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2481578277965364, distance: 0.9922475482655748 entropy -4.411651902815787
epoch: 8, step: 8
	action: tensor([[-0.0681, -0.0949, -0.1607,  0.0297, -0.0325, -0.0145,  0.1071]],
       dtype=torch.float64)
	q_value: tensor([[1.6980]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1492516048894239, distance: 1.0554976300784809 entropy -4.41179047374752
epoch: 8, step: 9
	action: tensor([[ 0.0324, -0.0943, -0.1612, -0.0469, -0.2146, -0.1222,  0.0939]],
       dtype=torch.float64)
	q_value: tensor([[1.6925]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20966426079178846, distance: 1.0173315281106043 entropy -4.408928331549915
epoch: 8, step: 10
	action: tensor([[-0.0339, -0.0429, -0.1604,  0.0767, -0.0259, -0.0464,  0.0119]],
       dtype=torch.float64)
	q_value: tensor([[1.7295]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24003050416732818, distance: 0.9975961710689799 entropy -4.412841952022613
epoch: 8, step: 11
	action: tensor([[-0.0453,  0.0127, -0.1615, -0.1157, -0.0176,  0.0374,  0.1396]],
       dtype=torch.float64)
	q_value: tensor([[1.7104]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2104538567182891, distance: 1.0168232115034042 entropy -4.408225900659361
epoch: 8, step: 12
	action: tensor([[-0.0669, -0.1510, -0.1599,  0.0727, -0.0298, -0.0148,  0.0310]],
       dtype=torch.float64)
	q_value: tensor([[1.5832]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12504105890092088, distance: 1.0704109176377874 entropy -4.414293659338837
epoch: 8, step: 13
	action: tensor([[ 0.0056, -0.0338, -0.1616,  0.0576, -0.0442, -0.0713,  0.1066]],
       dtype=torch.float64)
	q_value: tensor([[1.7570]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2717455806919975, distance: 0.9765584846297377 entropy -4.4078426003818985
epoch: 8, step: 14
	action: tensor([[-0.0124, -0.0873, -0.1613,  0.1201, -0.0384, -0.0137, -0.0423]],
       dtype=torch.float64)
	q_value: tensor([[1.6740]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2536888949141137, distance: 0.9885909837571423 entropy -4.408642758520187
epoch: 8, step: 15
	action: tensor([[-0.0764, -0.0774, -0.1616, -0.0686,  0.0272, -0.0694,  0.0578]],
       dtype=torch.float64)
	q_value: tensor([[1.7542]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10454518011417324, distance: 1.082875511354161 entropy -4.407946475754161
epoch: 8, step: 16
	action: tensor([[-0.0168, -0.0212, -0.1609,  0.0636,  0.0339,  0.0049, -0.0105]],
       dtype=torch.float64)
	q_value: tensor([[1.6918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2757945617151135, distance: 0.9738399442421468 entropy -4.410400336296744
epoch: 8, step: 17
	action: tensor([[-0.0847, -0.0693, -0.1612, -0.0437, -0.0892,  0.0453,  0.0070]],
       dtype=torch.float64)
	q_value: tensor([[1.6851]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13463266852424283, distance: 1.0645276378694544 entropy -4.4092289203322546
epoch: 8, step: 18
	action: tensor([[-0.0967, -0.1453, -0.1603, -0.1109, -0.0697,  0.0996,  0.0366]],
       dtype=torch.float64)
	q_value: tensor([[1.6940]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04340799794363248, distance: 1.1192318644835568 entropy -4.413414287637459
epoch: 8, step: 19
	action: tensor([[-0.0349, -0.0404, -0.1600,  0.1281, -0.1238,  0.0657,  0.0404]],
       dtype=torch.float64)
	q_value: tensor([[1.6982]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27683544202435906, distance: 0.9731398560808898 entropy -4.414475317201089
epoch: 8, step: 20
	action: tensor([[-0.1076, -0.0917, -0.1611,  0.0598,  0.0608,  0.0817,  0.0024]],
       dtype=torch.float64)
	q_value: tensor([[1.6949]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14029580053295032, distance: 1.0610386825184335 entropy -4.410096650201553
epoch: 8, step: 21
	action: tensor([[-0.0451, -0.0728, -0.1612, -0.0223, -0.0939,  0.0524,  0.1137]],
       dtype=torch.float64)
	q_value: tensor([[1.7023]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18085152453792253, distance: 1.0357096123619525 entropy -4.409244369786066
epoch: 8, step: 22
	action: tensor([[-0.0410, -0.0419, -0.1605, -0.0439,  0.0654, -0.0405,  0.0061]],
       dtype=torch.float64)
	q_value: tensor([[1.6594]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18535343130431858, distance: 1.0328596454555383 entropy -4.412042017951804
epoch: 8, step: 23
	action: tensor([[-0.0425, -0.0930, -0.1610,  0.0510, -0.0383,  0.0142,  0.0315]],
       dtype=torch.float64)
	q_value: tensor([[1.6778]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19366635818291467, distance: 1.0275763095025712 entropy -4.4100468469580125
epoch: 8, step: 24
	action: tensor([[-0.0265, -0.0749, -0.1612,  0.0448, -0.1309,  0.0311, -0.0609]],
       dtype=torch.float64)
	q_value: tensor([[1.7128]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22673294540586753, distance: 1.0062860377818552 entropy -4.409194315162251
epoch: 8, step: 25
	action: tensor([[-0.1000, -0.0396, -0.1608, -0.0433, -0.0044, -0.1234, -0.0640]],
       dtype=torch.float64)
	q_value: tensor([[1.7384]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11197313651089413, distance: 1.0783748363227672 entropy -4.411910940287135
epoch: 8, step: 26
	action: tensor([[-0.0720, -0.0379, -0.1609,  0.0634,  0.0357,  0.0762, -0.0097]],
       dtype=torch.float64)
	q_value: tensor([[1.7423]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22018979323341337, distance: 1.010534513584836 entropy -4.410943796902871
epoch: 8, step: 27
	action: tensor([[-0.0408, -0.1521, -0.1611, -0.0541, -0.0496,  0.0464,  0.0445]],
       dtype=torch.float64)
	q_value: tensor([[1.6805]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1113619079282947, distance: 1.078745894997051 entropy -4.409844283966792
epoch: 8, step: 28
	action: tensor([[-0.0439, -0.0927, -0.1607,  0.0085, -0.0192, -0.1047,  0.0501]],
       dtype=torch.float64)
	q_value: tensor([[1.7138]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15324522109625904, distance: 1.053017336867509 entropy -4.411654136375245
epoch: 8, step: 29
	action: tensor([[-0.0434, -0.0598, -0.1614,  0.0062,  0.0377, -0.0521,  0.1046]],
       dtype=torch.float64)
	q_value: tensor([[1.7289]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18501990234185084, distance: 1.0330710582120815 entropy -4.408589330801001
epoch: 8, step: 30
	action: tensor([[-0.0039, -0.0764, -0.1613, -0.0581, -0.0195, -0.0602, -0.0550]],
       dtype=torch.float64)
	q_value: tensor([[1.6699]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19061827568747824, distance: 1.0295166867694217 entropy -4.408524364144968
epoch: 8, step: 31
	action: tensor([[-0.1027, -0.0773, -0.1608, -0.0720, -0.1166,  0.0518,  0.0134]],
       dtype=torch.float64)
	q_value: tensor([[1.7307]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09820967606281306, distance: 1.0866995289643122 entropy -4.411411388932008
epoch: 8, step: 32
	action: tensor([[-0.0008, -0.0698, -0.1207,  0.0165,  0.0401,  0.1214,  0.0013]],
       dtype=torch.float64)
	q_value: tensor([[2.5755]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23851355625066728, distance: 0.9985913077374203 entropy -4.552159664225893
epoch: 8, step: 33
	action: tensor([[-0.0443, -0.1291, -0.1614, -0.0406, -0.1158,  0.1063, -0.0019]],
       dtype=torch.float64)
	q_value: tensor([[1.6550]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12422693976933152, distance: 1.0709087920798632 entropy -4.408287228056006
epoch: 8, step: 34
	action: tensor([[-5.3046e-02, -3.7407e-02, -1.6020e-01, -3.4816e-03,  3.2340e-02,
          1.0962e-04,  1.1614e-01]], dtype=torch.float64)
	q_value: tensor([[1.7152]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18563465207820107, distance: 1.03268135546356 entropy -4.414088102251018
epoch: 8, step: 35
	action: tensor([[-0.0771, -0.0913, -0.1610, -0.1377, -0.0983,  0.1318,  0.0833]],
       dtype=torch.float64)
	q_value: tensor([[1.6402]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09466819393952575, distance: 1.0888312639149473 entropy -4.409759711268836
epoch: 8, step: 36
	action: tensor([[-0.0239, -0.1518, -0.1595,  0.0110,  0.0856, -0.0373,  0.0552]],
       dtype=torch.float64)
	q_value: tensor([[1.6452]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12771467228874822, distance: 1.068774238233021 entropy -4.416435542721239
epoch: 8, step: 37
	action: tensor([[ 1.2369e-02, -5.1363e-02, -1.6149e-01, -7.2919e-02,  1.4018e-02,
         -3.2821e-06,  4.1082e-02]], dtype=torch.float64)
	q_value: tensor([[1.7289]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21949303557415012, distance: 1.0109858672919392 entropy -4.407804419576231
epoch: 8, step: 38
	action: tensor([[-0.0835, -0.1359, -0.1606,  0.0238, -0.0273,  0.0323,  0.1137]],
       dtype=torch.float64)
	q_value: tensor([[1.6571]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10611783490143079, distance: 1.0819241860047324 entropy -4.411770860428493
epoch: 8, step: 39
	action: tensor([[-0.0543, -0.0235, -0.1611,  0.0167, -0.0261, -0.0260, -0.0174]],
       dtype=torch.float64)
	q_value: tensor([[1.7039]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21334183416705976, distance: 1.0149618556038966 entropy -4.409331719831297
epoch: 8, step: 40
	action: tensor([[-0.1209, -0.0686, -0.1610,  0.0169,  0.0312, -0.0272, -0.0556]],
       dtype=torch.float64)
	q_value: tensor([[1.6959]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10419878946606598, distance: 1.083084936594778 entropy -4.410302759230535
epoch: 8, step: 41
	action: tensor([[-0.1049, -0.0409, -0.1612,  0.0292, -0.1235,  0.0040,  0.0005]],
       dtype=torch.float64)
	q_value: tensor([[1.7326]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15531939913820103, distance: 1.0517268306413776 entropy -4.409608850317463
epoch: 8, step: 42
	action: tensor([[-0.0611, -0.0109, -0.1606,  0.0019, -0.0261,  0.1624, -0.0102]],
       dtype=torch.float64)
	q_value: tensor([[1.7108]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2445803890615874, distance: 0.9946054182961841 entropy -4.4122209117001665
epoch: 8, step: 43
	action: tensor([[-0.0093, -0.0412, -0.1602, -0.0438, -0.0487,  0.0738,  0.0154]],
       dtype=torch.float64)
	q_value: tensor([[1.6432]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24274950987308752, distance: 0.9958099808319409 entropy -4.414110330954552
epoch: 8, step: 44
	action: tensor([[-0.0378, -0.0604, -0.1604, -0.0095,  0.0031, -0.0217, -0.0568]],
       dtype=torch.float64)
	q_value: tensor([[1.6555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1940013728505846, distance: 1.027362819153854 entropy -4.4129681697268035
epoch: 8, step: 45
	action: tensor([[-0.0484, -0.0965, -0.1610, -0.0155, -0.0623,  0.0836, -0.0601]],
       dtype=torch.float64)
	q_value: tensor([[1.7179]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17227400019541406, distance: 1.0411180880032183 entropy -4.410416259070682
epoch: 8, step: 46
	action: tensor([[-0.0350, -0.0582, -0.1605,  0.0832,  0.0578,  0.0050,  0.0168]],
       dtype=torch.float64)
	q_value: tensor([[1.7210]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23785215039691476, distance: 0.9990248878666322 entropy -4.413023432142563
epoch: 8, step: 47
	action: tensor([[-0.0643, -0.0639, -0.1615, -0.0061, -0.0462,  0.0313, -0.0735]],
       dtype=torch.float64)
	q_value: tensor([[1.6966]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1745683319009993, distance: 1.0396741755275705 entropy -4.40797172132955
epoch: 8, step: 48
	action: tensor([[-0.1163, -0.0605, -0.1606,  0.0990, -0.1274, -0.0057,  0.0040]],
       dtype=torch.float64)
	q_value: tensor([[1.7221]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1530028101919465, distance: 1.0531680561929557 entropy -4.412231969847388
epoch: 8, step: 49
	action: tensor([[-0.0258, -0.0850, -0.1610, -0.0954, -0.1507, -0.0387,  0.0521]],
       dtype=torch.float64)
	q_value: tensor([[1.7369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15278535362616608, distance: 1.053303241772018 entropy -4.410459789453608
epoch: 8, step: 50
	action: tensor([[ 0.0665,  0.0170, -0.1601,  0.0825,  0.0776, -0.0215,  0.0723]],
       dtype=torch.float64)
	q_value: tensor([[1.7024]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3855781408552683, distance: 0.8969950001974865 entropy -4.414120161234286
epoch: 8, step: 51
	action: tensor([[-0.0929, -0.0715, -0.1614,  0.0556, -0.0933,  0.0143,  0.0446]],
       dtype=torch.float64)
	q_value: tensor([[1.6347]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1566681409936863, distance: 1.0508868239583948 entropy -4.4081513361128515
epoch: 8, step: 52
	action: tensor([[-0.0448, -0.0738, -0.1610,  0.0382,  0.0124, -0.0750, -0.0614]],
       dtype=torch.float64)
	q_value: tensor([[1.7089]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18439334402015395, distance: 1.0334680954591802 entropy -4.410397012621942
epoch: 8, step: 53
	action: tensor([[-0.0907, -0.0687, -0.1614, -0.0659, -0.1472, -0.0064,  0.0701]],
       dtype=torch.float64)
	q_value: tensor([[1.7511]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11179948215394298, distance: 1.0784802696955726 entropy -4.4085398623257515
epoch: 8, step: 54
	action: tensor([[-0.0788, -0.0436, -0.1602,  0.0026, -0.0301, -0.0670,  0.0042]],
       dtype=torch.float64)
	q_value: tensor([[1.6887]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1591624065556102, distance: 1.0493316047480812 entropy -4.413866713956108
epoch: 8, step: 55
	action: tensor([[-0.0348, -0.0860, -0.1611, -0.0371, -0.0090,  0.0685, -0.0321]],
       dtype=torch.float64)
	q_value: tensor([[1.7089]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1838907024194476, distance: 1.033786498971996 entropy -4.409822371678053
epoch: 8, step: 56
	action: tensor([[ 0.0113, -0.0786, -0.1606, -0.0036, -0.0693, -0.1390, -0.0035]],
       dtype=torch.float64)
	q_value: tensor([[1.6963]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21507655694665317, distance: 1.013842151231533 entropy -4.4122130692906945
epoch: 8, step: 57
	action: tensor([[ 0.0108, -0.1307, -0.1612, -0.1019,  0.0402, -0.1119, -0.0111]],
       dtype=torch.float64)
	q_value: tensor([[1.7526]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12986491740479278, distance: 1.0674561233213344 entropy -4.409492034706014
epoch: 8, step: 58
	action: tensor([[-0.0069, -0.0428, -0.1610, -0.0100, -0.0072, -0.0945, -0.0066]],
       dtype=torch.float64)
	q_value: tensor([[1.7475]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22653602948780516, distance: 1.0064141572486085 entropy -4.410537231106483
epoch: 8, step: 59
	action: tensor([[-0.0533, -0.0693, -0.1612, -0.0134, -0.0788, -0.0013,  0.0047]],
       dtype=torch.float64)
	q_value: tensor([[1.7126]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17314112881959065, distance: 1.040572605646059 entropy -4.409567662496851
epoch: 8, step: 60
	action: tensor([[-0.0066, -0.1257, -0.1607,  0.0090, -0.0175, -0.0450, -0.0265]],
       dtype=torch.float64)
	q_value: tensor([[1.7057]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17803911480062373, distance: 1.0374860570367173 entropy -4.4115114673338764
epoch: 8, step: 61
	action: tensor([[-0.0456,  0.0317, -0.1613, -0.0126,  0.0848,  0.0426, -0.0132]],
       dtype=torch.float64)
	q_value: tensor([[1.7539]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2645268030451662, distance: 0.9813865878608712 entropy -4.409249399218807
epoch: 8, step: 62
	action: tensor([[-0.0190, -0.0294, -0.1606, -0.0088, -0.1406,  0.0577,  0.0539]],
       dtype=torch.float64)
	q_value: tensor([[1.6288]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2507685618451412, distance: 0.9905232853097937 entropy -4.41160416688988
epoch: 8, step: 63
	action: tensor([[-0.0515, -0.0997, -0.1603,  0.0373, -0.0742, -0.0570,  0.0543]],
       dtype=torch.float64)
	q_value: tensor([[1.6590]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16132999329284525, distance: 1.0479782012826568 entropy -4.413160600872432
epoch: 8, step: 64
	action: tensor([[-0.0376, -0.0358, -0.1207, -0.0169,  0.0188,  0.0596, -0.0344]],
       dtype=torch.float64)
	q_value: tensor([[2.5755]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20191473361263534, distance: 1.02230701266699 entropy -4.552159664225893
epoch: 8, step: 65
	action: tensor([[-0.0141, -0.0516, -0.1613,  0.1030,  0.0018, -0.0411, -0.0572]],
       dtype=torch.float64)
	q_value: tensor([[1.6602]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24586749111145256, distance: 0.9937577407865928 entropy -4.408646710617164
epoch: 8, step: 66
	action: tensor([[ 0.0004, -0.0766, -0.1616, -0.0435, -0.0223,  0.0840, -0.0437]],
       dtype=torch.float64)
	q_value: tensor([[1.7407]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21547207606790875, distance: 1.0135866839705183 entropy -4.408025384401037
epoch: 8, step: 67
	action: tensor([[-0.0342, -0.1126, -0.1604, -0.0532, -0.1013, -0.0442,  0.0439]],
       dtype=torch.float64)
	q_value: tensor([[1.6910]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13418758925015029, distance: 1.064801358788398 entropy -4.41325260475865
epoch: 8, step: 68
	action: tensor([[-0.0163, -0.0970, -0.1607, -0.0556,  0.0232, -0.0052, -0.0219]],
       dtype=torch.float64)
	q_value: tensor([[1.7237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17167820334457362, distance: 1.0414927187967924 entropy -4.411884841538967
epoch: 8, step: 69
	action: tensor([[-0.0869, -0.0804, -0.1608,  0.0028, -0.1201,  0.0220,  0.0317]],
       dtype=torch.float64)
	q_value: tensor([[1.7091]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13761461530258745, distance: 1.0626919410609288 entropy -4.411280393681917
epoch: 8, step: 70
	action: tensor([[-0.0397, -0.0375, -0.1606,  0.0280, -0.1402,  0.0307, -0.0512]],
       dtype=torch.float64)
	q_value: tensor([[1.7086]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2339679598540021, distance: 1.0015673549125637 entropy -4.41205664167336
epoch: 8, step: 71
	action: tensor([[-0.0315, -0.0684, -0.1606, -0.0657, -0.0964, -0.1044,  0.0188]],
       dtype=torch.float64)
	q_value: tensor([[1.7133]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1611762879336226, distance: 1.0480742298069192 entropy -4.412676911841357
epoch: 8, step: 72
	action: tensor([[-0.0489, -0.1485, -0.1607,  0.1177, -0.0019,  0.0983,  0.0016]],
       dtype=torch.float64)
	q_value: tensor([[1.7233]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18911625696206913, distance: 1.0304715122520822 entropy -4.411757496105529
epoch: 8, step: 73
	action: tensor([[-0.0421, -0.0484, -0.1614, -0.0440, -0.0891,  0.0638, -0.0397]],
       dtype=torch.float64)
	q_value: tensor([[1.7446]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2009552431976933, distance: 1.02292135751247 entropy -4.408667887976567
epoch: 8, step: 74
	action: tensor([[-0.0531, -0.1253, -0.1602,  0.0012,  0.0674,  0.0242,  0.1350]],
       dtype=torch.float64)
	q_value: tensor([[1.6886]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13519887290024502, distance: 1.064179324032339 entropy -4.4140845776770075
epoch: 8, step: 75
	action: tensor([[-0.0041, -0.1318, -0.1613,  0.0058,  0.0088,  0.0599,  0.0639]],
       dtype=torch.float64)
	q_value: tensor([[1.6743]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19028694794315393, distance: 1.0297273862018488 entropy -4.408402577037257
epoch: 8, step: 76
	action: tensor([[-0.0282, -0.0935, -0.1611, -0.0723,  0.0007, -0.0295, -0.0353]],
       dtype=torch.float64)
	q_value: tensor([[1.6950]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15136277593943515, distance: 1.0541871837701564 entropy -4.409785237476318
epoch: 8, step: 77
	action: tensor([[-0.0348, -0.0551, -0.1607, -0.0959, -0.0296, -0.0033,  0.0186]],
       dtype=torch.float64)
	q_value: tensor([[1.7198]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17072025637297772, distance: 1.042094783423289 entropy -4.411607343043772
epoch: 8, step: 78
	action: tensor([[-0.0362, -0.0619, -0.1604,  0.0160, -0.0079, -0.0064, -0.0505]],
       dtype=torch.float64)
	q_value: tensor([[1.6711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20748366616653335, distance: 1.0187340077694418 entropy -4.412707678285776
epoch: 8, step: 79
	action: tensor([[-0.0420, -0.0518, -0.1611, -0.0185, -0.1290,  0.0643,  0.0907]],
       dtype=torch.float64)
	q_value: tensor([[1.7184]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2062326872682797, distance: 1.0195377213183927 entropy -4.41014338274866
epoch: 8, step: 80
	action: tensor([[-0.0140, -0.0725, -0.1603, -0.0122,  0.1115,  0.0438,  0.0290]],
       dtype=torch.float64)
	q_value: tensor([[1.6573]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21889722200981665, distance: 1.0113716705000237 entropy -4.413060856890814
epoch: 8, step: 81
	action: tensor([[-0.0275,  0.0036, -0.1610, -0.0827,  0.0498,  0.1446,  0.0547]],
       dtype=torch.float64)
	q_value: tensor([[1.6652]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2581532925401615, distance: 0.9856296963608846 entropy -4.409961707506994
epoch: 8, step: 82
	action: tensor([[ 0.0150, -0.0676, -0.1599, -0.0624, -0.0240,  0.0046, -0.0023]],
       dtype=torch.float64)
	q_value: tensor([[1.5879]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22719541595682524, distance: 1.0059850762583569 entropy -4.414579240425311
epoch: 8, step: 83
	action: tensor([[-0.0067, -0.0893, -0.1606,  0.0178, -0.1813, -0.1055,  0.0269]],
       dtype=torch.float64)
	q_value: tensor([[1.6849]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20726031277111134, distance: 1.018877551606068 entropy -4.41189693972063
epoch: 8, step: 84
	action: tensor([[-0.0121, -0.0327, -0.1608, -0.0457, -0.0006,  0.0208, -0.0213]],
       dtype=torch.float64)
	q_value: tensor([[1.7536]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23623943869291641, distance: 1.0000813021400619 entropy -4.411299314301993
epoch: 8, step: 85
	action: tensor([[-0.1509, -0.0368, -0.1606, -0.0416,  0.0115,  0.1042,  0.1232]],
       dtype=torch.float64)
	q_value: tensor([[1.6704]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09511772848147604, distance: 1.0885609055582302 entropy -4.411905935286673
epoch: 8, step: 86
	action: tensor([[-0.0191, -0.0917, -0.1602,  0.0249, -0.1093, -0.0798,  0.0360]],
       dtype=torch.float64)
	q_value: tensor([[1.6247]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1862626945096263, distance: 1.0322830742720195 entropy -4.413157583639132
epoch: 8, step: 87
	action: tensor([[-0.0669, -0.0380, -0.1611, -0.1285, -0.0877, -0.0650,  0.0879]],
       dtype=torch.float64)
	q_value: tensor([[1.7366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1287045581971431, distance: 1.0681676334650367 entropy -4.4097840398983355
epoch: 8, step: 88
	action: tensor([[-0.0701, -0.0623, -0.1601, -0.0488,  0.0515,  0.0590, -0.0851]],
       dtype=torch.float64)
	q_value: tensor([[1.6609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15875943495543365, distance: 1.0495830208291859 entropy -4.413913037027418
epoch: 8, step: 89
	action: tensor([[-0.0082, -0.1237, -0.1606,  0.0277, -0.0879,  0.0144,  0.0764]],
       dtype=torch.float64)
	q_value: tensor([[1.6975]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19644384002225712, distance: 1.0258049977246861 entropy -4.412209279883955
epoch: 8, step: 90
	action: tensor([[ 0.0282, -0.0498, -0.1610, -0.0202, -0.0892,  0.0244, -0.0161]],
       dtype=torch.float64)
	q_value: tensor([[1.7130]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27571521860983905, distance: 0.9738932891667453 entropy -4.4101478731628845
epoch: 8, step: 91
	action: tensor([[-0.0724, -0.1348, -0.1605,  0.0458,  0.0681,  0.0594, -0.0038]],
       dtype=torch.float64)
	q_value: tensor([[1.6906]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1360620375046253, distance: 1.0636481086946152 entropy -4.412600142952231
epoch: 8, step: 92
	action: tensor([[-0.0447, -0.0703, -0.1613,  0.0485,  0.0094,  0.0399,  0.0076]],
       dtype=torch.float64)
	q_value: tensor([[1.7263]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2126162530436727, distance: 1.015429827245709 entropy -4.409028284318071
epoch: 8, step: 93
	action: tensor([[ 0.0212, -0.0720, -0.1611,  0.0191, -0.0892, -0.1331,  0.0967]],
       dtype=torch.float64)
	q_value: tensor([[1.6981]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.237289806443855, distance: 0.9993933806821615 entropy -4.409669999442659
epoch: 8, step: 94
	action: tensor([[-0.1136, -0.0433, -0.1612, -0.0882, -0.0032,  0.0405, -0.0061]],
       dtype=torch.float64)
	q_value: tensor([[1.7161]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10522741667155255, distance: 1.0824629176561893 entropy -4.409289138098254
epoch: 8, step: 95
	action: tensor([[-0.0639, -0.0616, -0.1602, -0.1044, -0.1054,  0.1296,  0.0579]],
       dtype=torch.float64)
	q_value: tensor([[1.6694]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15334448479472373, distance: 1.052955613295612 entropy -4.41364805557799
epoch: 8, step: 96
	action: tensor([[-0.0478, -0.0046, -0.1207, -0.0090, -0.1240, -0.0407,  0.0640]],
       dtype=torch.float64)
	q_value: tensor([[2.5755]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21692072658715045, distance: 1.0126504449367375 entropy -4.552159664225893
epoch: 8, step: 97
	action: tensor([[-0.0255, -0.0543, -0.1612, -0.0420, -0.0959, -0.0858,  0.0188]],
       dtype=torch.float64)
	q_value: tensor([[1.6543]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18455733058425294, distance: 1.0333641952516568 entropy -4.409140508891063
epoch: 8, step: 98
	action: tensor([[-0.0776, -0.0079, -0.1607,  0.0502, -0.0728,  0.0384, -0.0382]],
       dtype=torch.float64)
	q_value: tensor([[1.7136]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21707983668477926, distance: 1.0125475619324833 entropy -4.411692610362314
epoch: 8, step: 99
	action: tensor([[-0.0040, -0.0343, -0.1607,  0.1678, -0.0216,  0.0492,  0.0291]],
       dtype=torch.float64)
	q_value: tensor([[1.6928]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32177614748736005, distance: 0.9424172416578752 entropy -4.411745116755552
epoch: 8, step: 100
	action: tensor([[ 0.0010, -0.0883, -0.1616,  0.0796, -0.0727, -0.1385,  0.0438]],
       dtype=torch.float64)
	q_value: tensor([[1.6931]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22399289351206297, distance: 1.0080673355114886 entropy -4.407557245247796
epoch: 8, step: 101
	action: tensor([[-0.0695, -0.0747, -0.1616,  0.0126, -0.0863, -0.0954,  0.0084]],
       dtype=torch.float64)
	q_value: tensor([[1.7547]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14067685358340842, distance: 1.0608035104450841 entropy -4.407688470626959
epoch: 8, step: 102
	action: tensor([[-0.0263, -0.0053, -0.1611, -0.0240, -0.0848,  0.0522, -0.0054]],
       dtype=torch.float64)
	q_value: tensor([[1.7416]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2493440509166427, distance: 0.9914644772581462 entropy -4.409908369241707
epoch: 8, step: 103
	action: tensor([[-8.8125e-02, -1.0136e-01, -1.6033e-01,  1.4922e-04,  2.3750e-02,
         -4.1734e-02,  2.6829e-02]], dtype=torch.float64)
	q_value: tensor([[1.6579]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09868654813382305, distance: 1.0864121643916862 entropy -4.413309540610204
epoch: 8, step: 104
	action: tensor([[-0.0366, -0.0110, -0.1613,  0.0266, -0.0064, -0.0372, -0.0609]],
       dtype=torch.float64)
	q_value: tensor([[1.7188]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23624974231398754, distance: 1.0000745562474005 entropy -4.408953914552558
epoch: 8, step: 105
	action: tensor([[-0.0858, -0.0344, -0.1611,  0.0242,  0.0189,  0.0731,  0.0023]],
       dtype=torch.float64)
	q_value: tensor([[1.7060]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18529275812942825, distance: 1.032898107353945 entropy -4.410091918156964
epoch: 8, step: 106
	action: tensor([[-0.0364, -0.0379, -0.1608,  0.0658, -0.1894,  0.0075, -0.0103]],
       dtype=torch.float64)
	q_value: tensor([[1.6709]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24200152227013672, distance: 0.996301673927874 entropy -4.411150577896408
epoch: 8, step: 107
	action: tensor([[-0.0825, -0.0429, -0.1607, -0.0454, -0.1098,  0.0719,  0.0312]],
       dtype=torch.float64)
	q_value: tensor([[1.7176]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15415950302377335, distance: 1.052448686552204 entropy -4.412147005324639
epoch: 8, step: 108
	action: tensor([[ 0.0087, -0.0807, -0.1601,  0.0127, -0.0473, -0.0041,  0.0482]],
       dtype=torch.float64)
	q_value: tensor([[1.6664]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22966021544364013, distance: 1.0043795398822926 entropy -4.414118410193576
epoch: 8, step: 109
	action: tensor([[-0.0274, -0.0890, -0.1611,  0.1088, -0.1086,  0.0771, -0.0246]],
       dtype=torch.float64)
	q_value: tensor([[1.6930]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24155954533602886, distance: 0.9965920954800037 entropy -4.409853367854043
epoch: 8, step: 110
	action: tensor([[-0.0167, -0.0687, -0.1611, -0.1144,  0.0289, -0.0219,  0.0557]],
       dtype=torch.float64)
	q_value: tensor([[1.7339]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16012126369727142, distance: 1.0487331264447353 entropy -4.410405791772989
epoch: 8, step: 111
	action: tensor([[-0.1139, -0.1073, -0.1606,  0.0268,  0.0439, -0.0160,  0.0403]],
       dtype=torch.float64)
	q_value: tensor([[1.6598]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07997923608378399, distance: 1.0976288356141473 entropy -4.411760314838796
epoch: 8, step: 112
	action: tensor([[-0.1289, -0.0611, -0.1613, -0.0182,  0.0328, -0.0244, -0.0282]],
       dtype=torch.float64)
	q_value: tensor([[1.7166]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08128424116614408, distance: 1.0968500925806486 entropy -4.408440557137779
epoch: 8, step: 113
	action: tensor([[ 0.0015, -0.0667, -0.1609, -0.0561, -0.0778,  0.0226,  0.0570]],
       dtype=torch.float64)
	q_value: tensor([[1.7131]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20688736702423782, distance: 1.019117189771543 entropy -4.410443078607798
epoch: 8, step: 114
	action: tensor([[-0.0486, -0.0639, -0.1605, -0.0324, -0.0090, -0.0217, -0.0012]],
       dtype=torch.float64)
	q_value: tensor([[1.6669]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16399538314160333, distance: 1.046311578353186 entropy -4.412475993732369
epoch: 8, step: 115
	action: tensor([[-0.0495, -0.0270, -0.1609,  0.0269, -0.0568,  0.0044, -0.0236]],
       dtype=torch.float64)
	q_value: tensor([[1.6974]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21961186705546742, distance: 1.010908903519218 entropy -4.4106533078955135
epoch: 8, step: 116
	action: tensor([[-0.0265, -0.0573, -0.1609,  0.0746, -0.0206, -0.0398, -0.0166]],
       dtype=torch.float64)
	q_value: tensor([[1.6966]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23168954535778186, distance: 1.0030557334321213 entropy -4.4108662647395285
epoch: 8, step: 117
	action: tensor([[-0.0298, -0.0549, -0.1615,  0.1134,  0.2123,  0.0844, -0.0007]],
       dtype=torch.float64)
	q_value: tensor([[1.7250]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26915962620212375, distance: 0.9782907761904216 entropy -4.408332211053419
epoch: 8, step: 118
	action: tensor([[-0.0185, -0.0198, -0.1615, -0.0044, -0.0048,  0.0607, -0.0062]],
       dtype=torch.float64)
	q_value: tensor([[1.6751]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2561641951050522, distance: 0.9869501855331296 entropy -4.40784610132615
epoch: 8, step: 119
	action: tensor([[-0.0206, -0.0324, -0.1607, -0.0466, -0.0129,  0.0578, -0.0623]],
       dtype=torch.float64)
	q_value: tensor([[1.6587]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22887186107279356, distance: 1.0048933420633672 entropy -4.41175340362379
epoch: 8, step: 120
	action: tensor([[-0.0199, -0.0518, -0.1604,  0.0525, -0.2198, -0.0206,  0.0281]],
       dtype=torch.float64)
	q_value: tensor([[1.6798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24103339994085493, distance: 0.9969377135385922 entropy -4.413207233018208
epoch: 8, step: 121
	action: tensor([[-0.0230, -0.0750, -0.1606,  0.0678, -0.0164, -0.0422, -0.1028]],
       dtype=torch.float64)
	q_value: tensor([[1.7175]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2198399127767603, distance: 1.0107611883730838 entropy -4.412393640490387
epoch: 8, step: 122
	action: tensor([[-1.1798e-02, -4.7693e-02, -1.6141e-01,  3.3321e-02,  6.1991e-05,
         -8.0073e-02, -7.6912e-02]], dtype=torch.float64)
	q_value: tensor([[1.7661]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23139037901493775, distance: 1.0032510004025346 entropy -4.408963390401853
epoch: 8, step: 123
	action: tensor([[-0.0261, -0.0893, -0.1613, -0.0592, -0.0285, -0.0610,  0.0211]],
       dtype=torch.float64)
	q_value: tensor([[1.7454]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1504131945791607, distance: 1.0547768094671313 entropy -4.409336658240931
epoch: 8, step: 124
	action: tensor([[-0.0027, -0.0787, -0.1608, -0.0129, -0.1193,  0.0395,  0.0423]],
       dtype=torch.float64)
	q_value: tensor([[1.7142]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2200100456091525, distance: 1.0106509718656638 entropy -4.411115073216357
epoch: 8, step: 125
	action: tensor([[-0.0950, -0.0333, -0.1605, -0.0048, -0.0131, -0.0525,  0.0680]],
       dtype=torch.float64)
	q_value: tensor([[1.6892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14076606099842182, distance: 1.0607484473436881 entropy -4.412512593382078
epoch: 8, step: 126
	action: tensor([[-0.0884, -0.0757, -0.1610, -0.0038, -0.0755, -0.0022,  0.0336]],
       dtype=torch.float64)
	q_value: tensor([[1.6765]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1263524916625447, distance: 1.0696084237803507 entropy -4.410104678334192
epoch: 8, step: 127
	action: tensor([[-0.0169, -0.0282, -0.1608,  0.0502,  0.0151, -0.0397,  0.0417]],
       dtype=torch.float64)
	q_value: tensor([[1.7055]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2495382973759982, distance: 0.991336188842454 entropy -4.411325446239506
LOSS epoch 8 actor 1.3010929429598568 critic 1.9868890489007471 entropy 0.01
epoch: 9, step: 0
	action: tensor([[-0.0428, -0.0711, -0.1187,  0.0274, -0.0688, -0.0327, -0.0487]],
       dtype=torch.float64)
	q_value: tensor([[1.9378]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1746519387809936, distance: 1.0396215205843453 entropy -4.502645561463483
epoch: 9, step: 1
	action: tensor([[-0.0436, -0.0925, -0.2200, -0.0113,  0.0277, -0.0445, -0.0661]],
       dtype=torch.float64)
	q_value: tensor([[1.4355]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1487899822293418, distance: 1.0557839518330467 entropy -4.1786973230702475
epoch: 9, step: 2
	action: tensor([[-0.0336, -0.0310, -0.2164, -0.1079, -0.0199, -0.0125,  0.0776]],
       dtype=torch.float64)
	q_value: tensor([[1.4676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1842216700289292, distance: 1.0335768551513285 entropy -4.190834998395645
epoch: 9, step: 3
	action: tensor([[-0.0951, -0.0996, -0.2147, -0.0266, -0.1482, -0.0529, -0.0214]],
       dtype=torch.float64)
	q_value: tensor([[1.3719]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09516854780445305, distance: 1.088530337656354 entropy -4.196287017333511
epoch: 9, step: 4
	action: tensor([[-0.0415, -0.1207, -0.2157, -0.0626, -0.1184, -0.0193,  0.1376]],
       dtype=torch.float64)
	q_value: tensor([[1.4721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12458467908156645, distance: 1.070690045167378 entropy -4.193439238757753
epoch: 9, step: 5
	action: tensor([[ 1.0647e-01, -6.2942e-02, -2.1546e-01, -9.1641e-02, -1.0316e-01,
         -4.9454e-02, -4.2349e-05]], dtype=torch.float64)
	q_value: tensor([[1.4155]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29790356507084903, distance: 0.95885972464143 entropy -4.1940429504873356
epoch: 9, step: 6
	action: tensor([[-0.0344, -0.0670, -0.2153, -0.0041, -0.0854,  0.0287,  0.0157]],
       dtype=torch.float64)
	q_value: tensor([[1.4284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20415517316236365, distance: 1.0208710590954975 entropy -4.195093652973485
epoch: 9, step: 7
	action: tensor([[-0.0955, -0.0812, -0.2162, -0.0028, -0.1134,  0.0058, -0.0471]],
       dtype=torch.float64)
	q_value: tensor([[1.4230]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12429015404220045, distance: 1.0708701416889546 entropy -4.191917197016094
epoch: 9, step: 8
	action: tensor([[ 0.0157, -0.1147, -0.2159, -0.1464, -0.1196,  0.0421,  0.1030]],
       dtype=torch.float64)
	q_value: tensor([[1.4588]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1662868142304137, distance: 1.0448766604312736 entropy -4.193012901094698
epoch: 9, step: 9
	action: tensor([[-0.0818, -0.0791, -0.2140, -0.1015,  0.0276, -0.0537,  0.0717]],
       dtype=torch.float64)
	q_value: tensor([[1.3965]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09311192290632686, distance: 1.0897667159496778 entropy -4.199084727750434
epoch: 9, step: 10
	action: tensor([[-0.0612, -0.1732, -0.2153,  0.0346, -0.0375, -0.1437,  0.0818]],
       dtype=torch.float64)
	q_value: tensor([[1.4040]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07607454269819336, distance: 1.099955612550413 entropy -4.1943263327850575
epoch: 9, step: 11
	action: tensor([[-0.1153, -0.1194, -0.2180,  0.0095, -0.1613,  0.0761,  0.1015]],
       dtype=torch.float64)
	q_value: tensor([[1.4961]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08371486016896046, distance: 1.0953981797639314 entropy -4.18541245800238
epoch: 9, step: 12
	action: tensor([[-0.0226, -0.1133, -0.2157,  0.0186, -0.0751,  0.0240, -0.0879]],
       dtype=torch.float64)
	q_value: tensor([[1.4317]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18930289116323518, distance: 1.0303529180043414 entropy -4.193467027932111
epoch: 9, step: 13
	action: tensor([[-0.0538, -0.0682, -0.2166, -0.0309,  0.0114,  0.0248, -0.0599]],
       dtype=torch.float64)
	q_value: tensor([[1.4827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17293738315452745, distance: 1.0407008011012588 entropy -4.190806553744294
epoch: 9, step: 14
	action: tensor([[-0.0895, -0.0478, -0.2158,  0.0018,  0.0819, -0.0121, -0.0784]],
       dtype=torch.float64)
	q_value: tensor([[1.4350]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15318565554505348, distance: 1.0530543738330025 entropy -4.192989717895219
epoch: 9, step: 15
	action: tensor([[-0.0389, -0.0458, -0.2164,  0.0520,  0.0422,  0.0966, -0.0021]],
       dtype=torch.float64)
	q_value: tensor([[1.4392]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24768102597548403, distance: 0.9925621292181586 entropy -4.190762731137597
epoch: 9, step: 16
	action: tensor([[-0.0603, -0.1505, -0.2169, -0.0601, -0.0577, -0.1082,  0.0159]],
       dtype=torch.float64)
	q_value: tensor([[1.4087]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06830000473058051, distance: 1.1045738055301866 entropy -4.189364289794438
epoch: 9, step: 17
	action: tensor([[-0.0333, -0.0431, -0.2161, -0.0076, -0.0697, -0.0735, -0.0805]],
       dtype=torch.float64)
	q_value: tensor([[1.4849]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20874372668888563, distance: 1.0179238180621548 entropy -4.192059635376629
epoch: 9, step: 18
	action: tensor([[ 0.0518, -0.0712, -0.2163,  0.0539,  0.0085,  0.0228,  0.0288]],
       dtype=torch.float64)
	q_value: tensor([[1.4637]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3089256108523011, distance: 0.9513034955135357 entropy -4.191435349228518
epoch: 9, step: 19
	action: tensor([[-0.0216,  0.0218, -0.2175, -0.0461, -0.1771, -0.0364,  0.0031]],
       dtype=torch.float64)
	q_value: tensor([[1.4241]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26426321255940843, distance: 0.9815624345095361 entropy -4.187449089253041
epoch: 9, step: 20
	action: tensor([[ 0.0076, -0.0556, -0.2149,  0.0480, -0.0370, -0.0435,  0.0278]],
       dtype=torch.float64)
	q_value: tensor([[1.4004]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26357159330345614, distance: 0.9820236782771058 entropy -4.196387256267451
epoch: 9, step: 21
	action: tensor([[-0.0447, -0.1435, -0.2175,  0.2254, -0.0908, -0.0218,  0.0123]],
       dtype=torch.float64)
	q_value: tensor([[1.4335]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21122667589102684, distance: 1.016325449043858 entropy -4.187249738895167
epoch: 9, step: 22
	action: tensor([[-0.0524, -0.0181, -0.2200,  0.1094, -0.0315,  0.0507, -0.1156]],
       dtype=torch.float64)
	q_value: tensor([[1.5145]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2638996255552728, distance: 0.9818049392141347 entropy -4.179064376896918
epoch: 9, step: 23
	action: tensor([[-0.1239,  0.0028, -0.2175,  0.0141,  0.0089,  0.0335,  0.0625]],
       dtype=torch.float64)
	q_value: tensor([[1.4561]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16298955677125038, distance: 1.046940816158238 entropy -4.187377928304586
epoch: 9, step: 24
	action: tensor([[ 0.0450, -0.0787, -0.2159, -0.0667,  0.1472,  0.1656,  0.0919]],
       dtype=torch.float64)
	q_value: tensor([[1.3793]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2717507593968653, distance: 0.976555012411077 entropy -4.192348100238663
epoch: 9, step: 25
	action: tensor([[-0.0274, -0.0255, -0.2149, -0.0884, -0.0462,  0.0159, -0.0071]],
       dtype=torch.float64)
	q_value: tensor([[1.3549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2075142684075605, distance: 1.0187143388722364 entropy -4.195426160879934
epoch: 9, step: 26
	action: tensor([[-0.0669, -0.0317, -0.2147,  0.0323, -0.2317,  0.0557, -0.0478]],
       dtype=torch.float64)
	q_value: tensor([[1.3954]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21059296231792268, distance: 1.0167336334372956 entropy -4.196799347867729
epoch: 9, step: 27
	action: tensor([[-6.1310e-02, -1.0581e-01, -2.1566e-01,  6.4305e-02, -3.9563e-05,
         -5.4656e-02,  3.9844e-02]], dtype=torch.float64)
	q_value: tensor([[1.4411]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1541794625221753, distance: 1.052436269038573 entropy -4.193894554599252
epoch: 9, step: 28
	action: tensor([[-0.0954, -0.0395, -0.2180,  0.0681, -0.1129,  0.0328, -0.0201]],
       dtype=torch.float64)
	q_value: tensor([[1.4578]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18327683009610807, distance: 1.0341752297637745 entropy -4.185512244269151
epoch: 9, step: 29
	action: tensor([[-0.0805, -0.1290, -0.2166, -0.0038, -0.0432, -0.1005, -0.0925]],
       dtype=torch.float64)
	q_value: tensor([[1.4396]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08617714900893225, distance: 1.093925384320837 entropy -4.190356916669909
epoch: 9, step: 30
	action: tensor([[-0.0852, -0.1295, -0.2165,  0.0159, -0.0454,  0.0581, -0.0268]],
       dtype=torch.float64)
	q_value: tensor([[1.5144]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11246693577333489, distance: 1.0780749722266811 entropy -4.190587340605538
epoch: 9, step: 31
	action: tensor([[-0.1207, -0.1044, -0.2164,  0.0155,  0.0410, -0.0345, -0.1366]],
       dtype=torch.float64)
	q_value: tensor([[1.4632]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07627502261743513, distance: 1.0998362679786324 entropy -4.191119188738689
epoch: 9, step: 32
	action: tensor([[-0.0416, -0.0322, -0.1187,  0.0450, -0.0511, -0.0518, -0.0186]],
       dtype=torch.float64)
	q_value: tensor([[1.9378]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2072636873382916, distance: 1.0188753830036499 entropy -4.502645561463483
epoch: 9, step: 33
	action: tensor([[-0.1571,  0.0425, -0.2204,  0.0598, -0.1147, -0.0276, -0.0531]],
       dtype=torch.float64)
	q_value: tensor([[1.4129]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15742457697433232, distance: 1.0504154158574432 entropy -4.177088549677776
epoch: 9, step: 34
	action: tensor([[-0.0505, -0.1094, -0.2161,  0.0564,  0.0765,  0.0762,  0.0921]],
       dtype=torch.float64)
	q_value: tensor([[1.4281]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1813485771629305, distance: 1.0353953345996545 entropy -4.192046377773492
epoch: 9, step: 35
	action: tensor([[ 0.0318, -0.1769, -0.2174, -0.0004, -0.2553,  0.0775, -0.0254]],
       dtype=torch.float64)
	q_value: tensor([[1.4129]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19224662636197354, distance: 1.0284805513084982 entropy -4.187189802542686
epoch: 9, step: 36
	action: tensor([[-0.0994, -0.0528, -0.2159, -0.0146, -0.1065, -0.0421,  0.1201]],
       dtype=torch.float64)
	q_value: tensor([[1.4975]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12975188312002384, distance: 1.0675254546375603 entropy -4.193256075458767
epoch: 9, step: 37
	action: tensor([[ 0.0092,  0.0107, -0.2159,  0.0306,  0.0025,  0.0741, -0.0923]],
       dtype=torch.float64)
	q_value: tensor([[1.4046]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3242175279086078, distance: 0.9407195180694093 entropy -4.192505194841977
epoch: 9, step: 38
	action: tensor([[ 0.0208, -0.0096, -0.2162, -0.0869,  0.0931, -0.1032, -0.0359]],
       dtype=torch.float64)
	q_value: tensor([[1.4121]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25138343756936166, distance: 0.9901167528950421 entropy -4.191928800804564
epoch: 9, step: 39
	action: tensor([[-0.0007, -0.1611, -0.2151,  0.0490,  0.0474,  0.1244,  0.0123]],
       dtype=torch.float64)
	q_value: tensor([[1.4131]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20559437438399242, distance: 1.0199475727148104 entropy -4.194876074374904
epoch: 9, step: 40
	action: tensor([[-0.1066, -0.0291, -0.2171,  0.0906, -0.0030, -0.0861,  0.0498]],
       dtype=torch.float64)
	q_value: tensor([[1.4523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16600259221708824, distance: 1.0450547502389005 entropy -4.1886451796350626
epoch: 9, step: 41
	action: tensor([[-0.0527, -0.0643, -0.2179, -0.0226, -0.1302,  0.0976,  0.0496]],
       dtype=torch.float64)
	q_value: tensor([[1.4320]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18894906893131436, distance: 1.0305777380982495 entropy -4.1856792251540345
epoch: 9, step: 42
	action: tensor([[-0.1781, -0.0148, -0.2151, -0.0326, -0.1269,  0.0625, -0.0645]],
       dtype=torch.float64)
	q_value: tensor([[1.4045]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07666626693927936, distance: 1.0996033251228294 entropy -4.1953115891073365
epoch: 9, step: 43
	action: tensor([[-0.1156,  0.0093, -0.2141, -0.0062,  0.0225, -0.0293,  0.0422]],
       dtype=torch.float64)
	q_value: tensor([[1.4284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1605242121268956, distance: 1.0484815210365903 entropy -4.198731896295117
epoch: 9, step: 44
	action: tensor([[-0.0450, -0.1049, -0.2161, -0.0843, -0.0281, -0.0465, -0.0008]],
       dtype=torch.float64)
	q_value: tensor([[1.3877]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12175024028129888, distance: 1.0724219961418153 entropy -4.191720838108194
epoch: 9, step: 45
	action: tensor([[-0.0811,  0.0596, -0.2153, -0.0102, -0.0118,  0.0087,  0.0091]],
       dtype=torch.float64)
	q_value: tensor([[1.4465]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24138588137668437, distance: 0.9967061863238672 entropy -4.194498408275448
epoch: 9, step: 46
	action: tensor([[-0.0033, -0.0052, -0.2155, -0.0278, -0.0601, -0.0640,  0.0369]],
       dtype=torch.float64)
	q_value: tensor([[1.3665]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26207855380418144, distance: 0.9830186546618678 entropy -4.193905979745993
epoch: 9, step: 47
	action: tensor([[-0.0963, -0.1062, -0.2160, -0.0608,  0.0163, -0.0434, -0.0164]],
       dtype=torch.float64)
	q_value: tensor([[1.3999]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07314294438893998, distance: 1.1016992998346182 entropy -4.192223802974956
epoch: 9, step: 48
	action: tensor([[-0.0113,  0.0170, -0.2157,  0.0819,  0.0053, -0.0202, -0.0117]],
       dtype=torch.float64)
	q_value: tensor([[1.4504]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3076257413726311, distance: 0.9521977476133613 entropy -4.193069566819433
epoch: 9, step: 49
	action: tensor([[-0.0820, -0.0295, -0.2177,  0.0623, -0.1607,  0.1231,  0.0368]],
       dtype=torch.float64)
	q_value: tensor([[1.4092]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.213438176498442, distance: 1.0148997023143433 entropy -4.186453542847825
epoch: 9, step: 50
	action: tensor([[-0.0425,  0.0066, -0.2162, -0.0501,  0.0445,  0.0052,  0.1180]],
       dtype=torch.float64)
	q_value: tensor([[1.4071]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2275250563907142, distance: 1.0057705014913956 entropy -4.191722467786935
epoch: 9, step: 51
	action: tensor([[-0.0721, -0.0319, -0.2155,  0.1113,  0.1529, -0.0126,  0.1824]],
       dtype=torch.float64)
	q_value: tensor([[1.3458]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21470160729993448, distance: 1.0140842731693147 entropy -4.193337944244058
epoch: 9, step: 52
	action: tensor([[-0.0961, -0.0263, -0.2180, -0.0791, -0.1444, -0.0189, -0.1015]],
       dtype=torch.float64)
	q_value: tensor([[1.3776]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13823861451418107, distance: 1.0623074036241882 entropy -4.184879516165137
epoch: 9, step: 53
	action: tensor([[ 0.0018,  0.0016, -0.2141,  0.0062, -0.0765, -0.0114,  0.0817]],
       dtype=torch.float64)
	q_value: tensor([[1.4480]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2894549761579793, distance: 0.9646116314651451 entropy -4.198979086772942
epoch: 9, step: 54
	action: tensor([[-0.0279, -0.0682, -0.2164,  0.0282, -0.0913,  0.0501, -0.0537]],
       dtype=torch.float64)
	q_value: tensor([[1.3775]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22557727348050605, distance: 1.0070377200945402 entropy -4.191159435774322
epoch: 9, step: 55
	action: tensor([[-0.0793, -0.1039, -0.2164, -0.0036, -0.1543, -0.0295, -0.0839]],
       dtype=torch.float64)
	q_value: tensor([[1.4482]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12059774355353314, distance: 1.0731254165616284 entropy -4.1915100770926585
epoch: 9, step: 56
	action: tensor([[ 0.0046, -0.1835, -0.2158,  0.1247, -0.1711, -0.0593, -0.0186]],
       dtype=torch.float64)
	q_value: tensor([[1.4943]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1917082524650634, distance: 1.0288232393544963 entropy -4.193378848382119
epoch: 9, step: 57
	action: tensor([[-0.0647,  0.0385, -0.2184,  0.0426, -0.1589,  0.0301, -0.0328]],
       dtype=torch.float64)
	q_value: tensor([[1.5410]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2644076808017406, distance: 0.9814660606677661 entropy -4.184801200534318
epoch: 9, step: 58
	action: tensor([[-0.0665, -0.1264, -0.2159, -0.0634, -0.1757,  0.0838,  0.0285]],
       dtype=torch.float64)
	q_value: tensor([[1.4057]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10787906015546422, distance: 1.0808577972724225 entropy -4.193046564433845
epoch: 9, step: 59
	action: tensor([[-0.0788,  0.0086, -0.2145,  0.0490,  0.0366,  0.1414,  0.1204]],
       dtype=torch.float64)
	q_value: tensor([[1.4381]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2478515979230831, distance: 0.9924496019133288 entropy -4.197490536264992
epoch: 9, step: 60
	action: tensor([[-0.0458, -0.0569, -0.2162,  0.0081, -0.0870,  0.0512, -0.0201]],
       dtype=torch.float64)
	q_value: tensor([[1.3437]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2076575657144264, distance: 1.0186222327213075 entropy -4.191267724604892
epoch: 9, step: 61
	action: tensor([[-0.0888, -0.0617, -0.2160, -0.1073,  0.0017,  0.0635, -0.0062]],
       dtype=torch.float64)
	q_value: tensor([[1.4288]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1196250016813567, distance: 1.0737187659139964 entropy -4.19269124752036
epoch: 9, step: 62
	action: tensor([[-0.0534, -0.0378, -0.2141, -0.0585, -0.0671,  0.0247, -0.0164]],
       dtype=torch.float64)
	q_value: tensor([[1.3978]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.187101517077495, distance: 1.031750884339655 entropy -4.198577163368021
epoch: 9, step: 63
	action: tensor([[-0.0462, -0.0756, -0.2150, -0.0056, -0.0965,  0.1066, -0.0284]],
       dtype=torch.float64)
	q_value: tensor([[1.4097]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1965190795421211, distance: 1.0257569719076312 entropy -4.195637152649312
epoch: 9, step: 64
	action: tensor([[-0.0409,  0.0093, -0.1187,  0.0186, -0.0771, -0.0065, -0.0135]],
       dtype=torch.float64)
	q_value: tensor([[1.9378]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23516031897388845, distance: 1.0007875617805704 entropy -4.502645561463483
epoch: 9, step: 65
	action: tensor([[-0.1044, -0.0764, -0.2195,  0.1383, -0.0522, -0.1112,  0.0793]],
       dtype=torch.float64)
	q_value: tensor([[1.3808]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13946172649342148, distance: 1.0615532608317602 entropy -4.180391260710971
epoch: 9, step: 66
	action: tensor([[-0.0995, -0.0378, -0.2187, -0.1131,  0.1052,  0.0049, -0.1017]],
       dtype=torch.float64)
	q_value: tensor([[1.4617]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11247769520415463, distance: 1.078068437535916 entropy -4.183021420851383
epoch: 9, step: 67
	action: tensor([[-0.1456, -0.0285, -0.2139, -0.0689, -0.1104,  0.0695, -0.1138]],
       dtype=torch.float64)
	q_value: tensor([[1.4187]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09416124855582753, distance: 1.0891360696944663 entropy -4.199181672165113
epoch: 9, step: 68
	action: tensor([[-0.0930, -0.0285, -0.2137,  0.0841,  0.0753, -0.0868,  0.0987]],
       dtype=torch.float64)
	q_value: tensor([[1.4346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17505855443191498, distance: 1.0393653992707736 entropy -4.200256845604128
epoch: 9, step: 69
	action: tensor([[-0.0189, -0.0172, -0.2179, -0.0420, -0.1037,  0.0858,  0.0126]],
       dtype=torch.float64)
	q_value: tensor([[1.4105]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25178872525632234, distance: 0.9898487008139095 entropy -4.185173237294859
epoch: 9, step: 70
	action: tensor([[-0.0917, -0.0339, -0.2149,  0.0642, -0.1078, -0.1175,  0.0607]],
       dtype=torch.float64)
	q_value: tensor([[1.3867]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17154888844555083, distance: 1.0415740128483597 entropy -4.196361687662258
epoch: 9, step: 71
	action: tensor([[-0.1130, -0.1079, -0.2175, -0.1585, -0.1810,  0.1085, -0.0018]],
       dtype=torch.float64)
	q_value: tensor([[1.4389]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04115622509029637, distance: 1.1205483999163837 entropy -4.187184651659258
epoch: 9, step: 72
	action: tensor([[-0.1025, -0.0543, -0.2123, -0.0822, -0.0791, -0.0151,  0.1488]],
       dtype=torch.float64)
	q_value: tensor([[1.4227]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10433967240011299, distance: 1.0829997646940455 entropy -4.204813442402605
epoch: 9, step: 73
	action: tensor([[-0.0514, -0.1613, -0.2149, -0.0672,  0.0019, -0.1738,  0.0031]],
       dtype=torch.float64)
	q_value: tensor([[1.3787]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0523127055892505, distance: 1.1140103411251148 entropy -4.19573622958129
epoch: 9, step: 74
	action: tensor([[-0.1087, -0.0615, -0.2164,  0.0323, -0.0322,  0.0278,  0.1622]],
       dtype=torch.float64)
	q_value: tensor([[1.5028]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13636520162273458, distance: 1.0634614701578298 entropy -4.190595473992575
epoch: 9, step: 75
	action: tensor([[-0.1093, -0.0536, -0.2167, -0.0426, -0.0785,  0.1281,  0.0152]],
       dtype=torch.float64)
	q_value: tensor([[1.3884]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13375069387256666, distance: 1.0650699782456992 entropy -4.189697986452626
epoch: 9, step: 76
	action: tensor([[-0.0601,  0.1057, -0.2145,  0.0934, -0.0792, -0.1182,  0.0971]],
       dtype=torch.float64)
	q_value: tensor([[1.3983]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3093792875181408, distance: 0.950991188286046 entropy -4.197344916284448
epoch: 9, step: 77
	action: tensor([[-0.0868, -0.0942, -0.2173,  0.0778, -0.0708,  0.1709,  0.0852]],
       dtype=torch.float64)
	q_value: tensor([[1.3715]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1769847596335793, distance: 1.0381512518417606 entropy -4.187437796634172
epoch: 9, step: 78
	action: tensor([[-0.0479,  0.0470, -0.2168, -0.0218, -0.0139,  0.1059,  0.0058]],
       dtype=torch.float64)
	q_value: tensor([[1.4126]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2790974490550814, distance: 0.9716167081286403 entropy -4.18950495597379
epoch: 9, step: 79
	action: tensor([[-0.0124, -0.0660, -0.2149,  0.0284, -0.1070, -0.0340,  0.0231]],
       dtype=torch.float64)
	q_value: tensor([[1.3535]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23109444040910376, distance: 1.0034441232315723 entropy -4.196148210453418
epoch: 9, step: 80
	action: tensor([[-0.0469, -0.1450, -0.2169,  0.0658,  0.0071,  0.1552,  0.0661]],
       dtype=torch.float64)
	q_value: tensor([[1.4410]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17948969325312014, distance: 1.0365701865186885 entropy -4.189598047525991
epoch: 9, step: 81
	action: tensor([[-0.0815,  0.0077, -0.2170,  0.1002, -0.2221, -0.0116,  0.0003]],
       dtype=torch.float64)
	q_value: tensor([[1.4333]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23878199039684167, distance: 0.9984152838339315 entropy -4.188819878492809
epoch: 9, step: 82
	action: tensor([[-0.0397, -0.0605, -0.2168,  0.0699, -0.0952, -0.0604,  0.0367]],
       dtype=torch.float64)
	q_value: tensor([[1.4338]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21691981791558634, distance: 1.0126510324675457 entropy -4.18978618326466
epoch: 9, step: 83
	action: tensor([[-0.0786, -0.0702, -0.2175,  0.0526, -0.0477,  0.0453, -0.0365]],
       dtype=torch.float64)
	q_value: tensor([[1.4456]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17644695412710043, distance: 1.0384903903242821 entropy -4.187262287259064
epoch: 9, step: 84
	action: tensor([[-0.0766, -0.0439, -0.2167,  0.0267, -0.1461,  0.0622,  0.0290]],
       dtype=torch.float64)
	q_value: tensor([[1.4477]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19014035107898375, distance: 1.0298205969840444 entropy -4.1899262922836495
epoch: 9, step: 85
	action: tensor([[-0.0875, -0.0678, -0.2159,  0.0791, -0.0890,  0.0586,  0.0601]],
       dtype=torch.float64)
	q_value: tensor([[1.4174]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1780389170708231, distance: 1.0374861818248322 entropy -4.192738576727986
epoch: 9, step: 86
	action: tensor([[ 0.0503, -0.0939, -0.2171, -0.0616, -0.0431, -0.0417,  0.0591]],
       dtype=torch.float64)
	q_value: tensor([[1.4260]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23029648923939183, distance: 1.0039646629604764 entropy -4.188517668234942
epoch: 9, step: 87
	action: tensor([[ 0.0542, -0.0279, -0.2159, -0.0312, -0.0359,  0.0625, -0.0753]],
       dtype=torch.float64)
	q_value: tensor([[1.4251]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3224294948060067, distance: 0.9419632070994695 entropy -4.1926615496451
epoch: 9, step: 88
	action: tensor([[-0.0393, -0.1047, -0.2154, -0.0372,  0.0256,  0.1595, -0.0050]],
       dtype=torch.float64)
	q_value: tensor([[1.4187]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1838783626536048, distance: 1.033794314490791 entropy -4.194641516845158
epoch: 9, step: 89
	action: tensor([[-0.0612,  0.0102, -0.2151,  0.1567,  0.0118, -0.0961, -0.0518]],
       dtype=torch.float64)
	q_value: tensor([[1.4129]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2655094298839653, distance: 0.980730779382967 entropy -4.1952748618756575
epoch: 9, step: 90
	action: tensor([[ 0.0176, -0.0165, -0.2189, -0.0232, -0.0230, -0.0289,  0.1212]],
       dtype=torch.float64)
	q_value: tensor([[1.4544]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27723109960231773, distance: 0.972873607664099 entropy -4.182399171742662
epoch: 9, step: 91
	action: tensor([[-0.0082, -0.0304, -0.2162,  0.0094, -0.0086,  0.0049,  0.1127]],
       dtype=torch.float64)
	q_value: tensor([[1.3686]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25863584569180265, distance: 0.9853090802003628 entropy -4.191464162280527
epoch: 9, step: 92
	action: tensor([[-0.0820, -0.0340, -0.2167, -0.0372, -0.0150, -0.0241,  0.0739]],
       dtype=torch.float64)
	q_value: tensor([[1.3772]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1567771481954615, distance: 1.0508189041159068 entropy -4.189798463734583
epoch: 9, step: 93
	action: tensor([[-0.0960, -0.1686, -0.2159, -0.1090, -0.0662,  0.0601,  0.0789]],
       dtype=torch.float64)
	q_value: tensor([[1.3910]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.021840128543685333, distance: 1.13177895552033 entropy -4.192414491081011
epoch: 9, step: 94
	action: tensor([[-0.0179, -0.0450, -0.2144, -0.0831,  0.0162,  0.0431, -0.0047]],
       dtype=torch.float64)
	q_value: tensor([[1.4316]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20963373155204834, distance: 1.017351176759052 entropy -4.197478296803714
epoch: 9, step: 95
	action: tensor([[-0.0969, -0.0216, -0.2150, -0.0778, -0.0397,  0.1288, -0.0017]],
       dtype=torch.float64)
	q_value: tensor([[1.3920]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16181980282741815, distance: 1.047672130551597 entropy -4.195610782739077
epoch: 9, step: 96
	action: tensor([[ 0.0132, -0.0837, -0.1187, -0.0039,  0.0550,  0.1091,  0.0275]],
       dtype=torch.float64)
	q_value: tensor([[1.9378]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2317895080901351, distance: 1.0029904789163024 entropy -4.502645561463483
epoch: 9, step: 97
	action: tensor([[-0.0289,  0.0064, -0.2193,  0.0023, -0.0652,  0.0059,  0.0521]],
       dtype=torch.float64)
	q_value: tensor([[1.3762]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24055402616584798, distance: 0.9972525036466138 entropy -4.180558656429741
epoch: 9, step: 98
	action: tensor([[-0.0729, -0.1051, -0.2161, -0.0077, -0.1603,  0.1070,  0.0053]],
       dtype=torch.float64)
	q_value: tensor([[1.3819]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13582671794619516, distance: 1.063792957151594 entropy -4.19214423523205
epoch: 9, step: 99
	action: tensor([[ 0.0268, -0.0745, -0.2153, -0.0369, -0.0847,  0.0299,  0.0415]],
       dtype=torch.float64)
	q_value: tensor([[1.4404]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2451517923782489, distance: 0.9942291847847519 entropy -4.194941825917077
epoch: 9, step: 100
	action: tensor([[-0.0401, -0.2314, -0.2158,  0.0071, -0.0360, -0.0431,  0.0843]],
       dtype=torch.float64)
	q_value: tensor([[1.4115]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05477324177946008, distance: 1.1125632159574477 entropy -4.19323975923498
epoch: 9, step: 101
	action: tensor([[ 0.0312, -0.0523, -0.2173,  0.0615, -0.0170, -0.0024, -0.0121]],
       dtype=torch.float64)
	q_value: tensor([[1.4988]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2983529498610248, distance: 0.9585528109989976 entropy -4.187902957004409
epoch: 9, step: 102
	action: tensor([[ 0.0712, -0.0844, -0.2175, -0.0261, -0.0017, -0.0086, -0.0093]],
       dtype=torch.float64)
	q_value: tensor([[1.4378]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2833827963279999, distance: 0.9687245549644224 entropy -4.187532746730926
epoch: 9, step: 103
	action: tensor([[-0.0850, -0.0101, -0.2163, -0.0204, -0.0770, -0.0844, -0.0819]],
       dtype=torch.float64)
	q_value: tensor([[1.4365]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17263708664181687, distance: 1.0408897169415903 entropy -4.1914372328165586
epoch: 9, step: 104
	action: tensor([[-0.1114, -0.1208, -0.2158,  0.0464, -0.0329,  0.0755, -0.0717]],
       dtype=torch.float64)
	q_value: tensor([[1.4512]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10469383577371416, distance: 1.0827856228130177 entropy -4.193071727647403
epoch: 9, step: 105
	action: tensor([[-0.0782, -0.0431, -0.2168,  0.0317,  0.0875,  0.0360, -0.0580]],
       dtype=torch.float64)
	q_value: tensor([[1.4749]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18863431012565746, distance: 1.030777695918015 entropy -4.189939055640045
epoch: 9, step: 106
	action: tensor([[-0.0430, -0.0389, -0.2167,  0.0226,  0.0842,  0.0585, -0.0322]],
       dtype=torch.float64)
	q_value: tensor([[1.4274]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23153565067806092, distance: 1.003156185801831 entropy -4.189815257249625
epoch: 9, step: 107
	action: tensor([[-0.0348, -0.1064, -0.2165, -0.0520, -0.1066,  0.0400,  0.0356]],
       dtype=torch.float64)
	q_value: tensor([[1.4124]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15695858898099713, distance: 1.0507058428625717 entropy -4.190478607032165
epoch: 9, step: 108
	action: tensor([[-0.0941,  0.0189, -0.2154, -0.0237, -0.0838, -0.1814,  0.0462]],
       dtype=torch.float64)
	q_value: tensor([[1.4275]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17222470188164374, distance: 1.041149091379934 entropy -4.19451863161627
epoch: 9, step: 109
	action: tensor([[-0.0834, -0.1042, -0.2161,  0.0503, -0.1554,  0.0872,  0.0377]],
       dtype=torch.float64)
	q_value: tensor([[1.4202]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1485634751030276, distance: 1.0559244146313527 entropy -4.191734635267925
epoch: 9, step: 110
	action: tensor([[-0.0236, -0.0211, -0.2165,  0.1082, -0.0467, -0.0465, -0.0169]],
       dtype=torch.float64)
	q_value: tensor([[1.4443]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2772237925965586, distance: 0.9728785254013477 entropy -4.190852803417001
epoch: 9, step: 111
	action: tensor([[-0.0610, -0.0968, -0.2182, -0.0338, -0.2170,  0.0815,  0.0008]],
       dtype=torch.float64)
	q_value: tensor([[1.4422]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14717683481191457, distance: 1.0567838983626325 entropy -4.185037831823243
epoch: 9, step: 112
	action: tensor([[-0.0099, -0.1022, -0.2148, -0.0011, -0.2122,  0.0184, -0.0404]],
       dtype=torch.float64)
	q_value: tensor([[1.4405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20307638616108048, distance: 1.0215627325433831 entropy -4.196654961663862
epoch: 9, step: 113
	action: tensor([[-0.1615,  0.0128, -0.2159,  0.0762, -0.1481, -0.0757,  0.0955]],
       dtype=torch.float64)
	q_value: tensor([[1.4707]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13552057928337224, distance: 1.0639813680425636 entropy -4.193207527537963
epoch: 9, step: 114
	action: tensor([[-0.1320,  0.0316, -0.2168,  0.0477, -0.0359, -0.0488,  0.1763]],
       dtype=torch.float64)
	q_value: tensor([[1.4142]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17196817412085297, distance: 1.0413104050580255 entropy -4.189450357182005
epoch: 9, step: 115
	action: tensor([[ 0.0205,  0.0244, -0.2166,  0.1074, -0.1797, -0.0863, -0.0640]],
       dtype=torch.float64)
	q_value: tensor([[1.3663]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3478909429639211, distance: 0.924095401950128 entropy -4.189948026442036
epoch: 9, step: 116
	action: tensor([[ 0.0126, -0.2084, -0.2176, -0.0331, -0.0567,  0.1234,  0.0672]],
       dtype=torch.float64)
	q_value: tensor([[1.4556]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1440719682208339, distance: 1.058705863045721 entropy -4.187353172850631
epoch: 9, step: 117
	action: tensor([[-0.0134, -0.0306, -0.2159, -0.0305, -0.1119,  0.0430,  0.0583]],
       dtype=torch.float64)
	q_value: tensor([[1.4509]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24648168405299364, distance: 0.993352982134718 entropy -4.192483272524061
epoch: 9, step: 118
	action: tensor([[-0.0975, -0.0047, -0.2155, -0.0776, -0.0065, -0.0038,  0.0456]],
       dtype=torch.float64)
	q_value: tensor([[1.3880]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15209641529286966, distance: 1.0537314175121208 entropy -4.194007787841919
epoch: 9, step: 119
	action: tensor([[-0.0769, -0.0414, -0.2148, -0.0423, -0.1755,  0.1202,  0.0650]],
       dtype=torch.float64)
	q_value: tensor([[1.3753]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17432414181669964, distance: 1.0398279492200275 entropy -4.195841713717286
epoch: 9, step: 120
	action: tensor([[-0.0462, -0.0803, -0.2144, -0.0407,  0.0189,  0.0403,  0.1629]],
       dtype=torch.float64)
	q_value: tensor([[1.3876]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16644418714895237, distance: 1.0447780395548945 entropy -4.197812572922839
epoch: 9, step: 121
	action: tensor([[-0.0373, -0.0596, -0.2159,  0.1262,  0.1206,  0.1525,  0.1029]],
       dtype=torch.float64)
	q_value: tensor([[1.3711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.276632913109782, distance: 0.9732761149443783 entropy -4.192043283696037
epoch: 9, step: 122
	action: tensor([[-0.0083,  0.0700, -0.2179,  0.0118, -0.1893, -0.2051, -0.0599]],
       dtype=torch.float64)
	q_value: tensor([[1.3871]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32023171405224526, distance: 0.9434896553068676 entropy -4.185726763497235
epoch: 9, step: 123
	action: tensor([[-0.0408, -0.0796, -0.2159, -0.1289, -0.0498,  0.1424,  0.0816]],
       dtype=torch.float64)
	q_value: tensor([[1.4491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16132966011345307, distance: 1.047978409448385 entropy -4.193058383121446
epoch: 9, step: 124
	action: tensor([[ 0.0071, -0.1628, -0.2135, -0.0227,  0.0399, -0.1118, -0.0195]],
       dtype=torch.float64)
	q_value: tensor([[1.3686]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13571838535837344, distance: 1.0638596334969814 entropy -4.200585600339243
epoch: 9, step: 125
	action: tensor([[ 0.0048, -0.0914, -0.2171, -0.0042,  0.0003,  0.0015,  0.0534]],
       dtype=torch.float64)
	q_value: tensor([[1.4990]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22045964330792633, distance: 1.0103596528260526 entropy -4.188604002262037
epoch: 9, step: 126
	action: tensor([[ 0.0370, -0.0657, -0.2166, -0.1325, -0.0110, -0.0238, -0.0264]],
       dtype=torch.float64)
	q_value: tensor([[1.4225]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22364800112745886, distance: 1.0082913258089334 entropy -4.190062918795279
epoch: 9, step: 127
	action: tensor([[-0.0131, -0.0340, -0.2143, -0.0213,  0.0089,  0.0132, -0.0076]],
       dtype=torch.float64)
	q_value: tensor([[1.4193]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2432049354904856, distance: 0.9955104857570831 entropy -4.197947988886182
LOSS epoch 9 actor 0.905632947517471 critic 2.2156455783409044 entropy 0.01
epoch: 10, step: 0
	action: tensor([[-0.0282, -0.0467, -0.1434, -0.1258, -0.0602,  0.0227,  0.0296]],
       dtype=torch.float64)
	q_value: tensor([[1.9115]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17100603411392035, distance: 1.041915210030478 entropy -4.365030335475394
epoch: 10, step: 1
	action: tensor([[-0.0744, -0.0532, -0.1921,  0.0788, -0.0325, -0.0015, -0.0023]],
       dtype=torch.float64)
	q_value: tensor([[1.4140]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19065521276914732, distance: 1.0294931949023198 entropy -4.201030575766083
epoch: 10, step: 2
	action: tensor([[-0.0747,  0.0172, -0.1932,  0.0214, -0.0703,  0.0665,  0.0600]],
       dtype=torch.float64)
	q_value: tensor([[1.4832]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23126433779648659, distance: 1.0033332565904798 entropy -4.197163391850152
epoch: 10, step: 3
	action: tensor([[ 0.0177, -0.0986, -0.1920, -0.0957, -0.1078,  0.1011,  0.0394]],
       dtype=torch.float64)
	q_value: tensor([[1.4116]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2050000196677748, distance: 1.0203290511969756 entropy -4.201640115006848
epoch: 10, step: 4
	action: tensor([[ 0.0118, -0.0038, -0.1906, -0.0556, -0.0403,  0.0210, -0.0738]],
       dtype=torch.float64)
	q_value: tensor([[1.4464]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2767319877756834, distance: 0.9732094611598548 entropy -4.20705864645128
epoch: 10, step: 5
	action: tensor([[-0.0764,  0.0208, -0.1920,  0.0866,  0.0304,  0.0293, -0.0104]],
       dtype=torch.float64)
	q_value: tensor([[1.4468]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24887375385085275, distance: 0.9917750122140426 entropy -4.202149205356132
epoch: 10, step: 6
	action: tensor([[-0.0547, -0.0271, -0.1933, -0.0823,  0.0275, -0.0400,  0.0626]],
       dtype=torch.float64)
	q_value: tensor([[1.4381]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1659494462858735, distance: 1.0450880474185815 entropy -4.1964520006690575
epoch: 10, step: 7
	action: tensor([[-0.0342, -0.1366, -0.1922,  0.1128,  0.0070,  0.0623,  0.2037]],
       dtype=torch.float64)
	q_value: tensor([[1.4185]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19271004759624577, distance: 1.028185480979604 entropy -4.200831470541493
epoch: 10, step: 8
	action: tensor([[-0.0927, -0.1553, -0.1928, -0.1085, -0.0384,  0.0761, -0.1895]],
       dtype=torch.float64)
	q_value: tensor([[1.4593]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.036460770285375976, distance: 1.1232887101012718 entropy -4.198520052424787
epoch: 10, step: 9
	action: tensor([[-0.0172, -0.0231, -0.1905, -0.0366, -0.0809,  0.0430, -0.0015]],
       dtype=torch.float64)
	q_value: tensor([[1.5448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23961846019454547, distance: 0.9978665752037287 entropy -4.20755142289947
epoch: 10, step: 10
	action: tensor([[-0.0542, -0.1227, -0.1918, -0.0358, -0.0312,  0.0702, -0.0788]],
       dtype=torch.float64)
	q_value: tensor([[1.4379]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13043697912364216, distance: 1.067105171384958 entropy -4.20266465384353
epoch: 10, step: 11
	action: tensor([[-0.0293, -0.1077, -0.1915,  0.0910,  0.0908, -0.0078, -0.0507]],
       dtype=torch.float64)
	q_value: tensor([[1.5046]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20095008373542578, distance: 1.022924660028119 entropy -4.203567032930641
epoch: 10, step: 12
	action: tensor([[ 0.0603, -0.0640, -0.1937,  0.0230, -0.0481,  0.1422, -0.0100]],
       dtype=torch.float64)
	q_value: tensor([[1.5147]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3273071256223491, distance: 0.9385666252460304 entropy -4.195067188851314
epoch: 10, step: 13
	action: tensor([[-0.0600,  0.0600, -0.1917,  0.0997, -0.1466,  0.1490,  0.0304]],
       dtype=torch.float64)
	q_value: tensor([[1.4567]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3108549123876011, distance: 0.949974670374474 entropy -4.203043283502695
epoch: 10, step: 14
	action: tensor([[-0.0635, -0.0214, -0.1921,  0.0321, -0.1499,  0.0959,  0.0121]],
       dtype=torch.float64)
	q_value: tensor([[1.4115]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22305147551998628, distance: 1.008678621843909 entropy -4.201486429892649
epoch: 10, step: 15
	action: tensor([[-0.0812, -0.0871, -0.1917, -0.0315, -0.0893,  0.0369, -0.0459]],
       dtype=torch.float64)
	q_value: tensor([[1.4498]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1240251251804293, distance: 1.0710321759665242 entropy -4.202971861025524
epoch: 10, step: 16
	action: tensor([[-0.0863, -0.0970, -0.1916, -0.0562,  0.0130, -0.0344, -0.0516]],
       dtype=torch.float64)
	q_value: tensor([[1.4893]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08821432153637354, distance: 1.0927053677043177 entropy -4.20330942883177
epoch: 10, step: 17
	action: tensor([[ 0.0103, -0.1619, -0.1922,  0.0756,  0.1176,  0.0106, -0.0039]],
       dtype=torch.float64)
	q_value: tensor([[1.4946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19406632202650054, distance: 1.02732142472059 entropy -4.200519217938604
epoch: 10, step: 18
	action: tensor([[-0.0409, -0.0673, -0.1932, -0.0993,  0.0228, -0.1050, -0.0408]],
       dtype=torch.float64)
	q_value: tensor([[1.5200]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13278258867905057, distance: 1.0656649639190892 entropy -4.196839598627613
epoch: 10, step: 19
	action: tensor([[-0.0616, -0.0930, -0.1923,  0.0294,  0.0142,  0.1959, -0.0824]],
       dtype=torch.float64)
	q_value: tensor([[1.4849]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1973358475895196, distance: 1.0252354794110035 entropy -4.200522608726646
epoch: 10, step: 20
	action: tensor([[-0.0929, -0.0941, -0.1913, -0.0026,  0.0493,  0.1549,  0.0633]],
       dtype=torch.float64)
	q_value: tensor([[1.4854]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.139684937794621, distance: 1.0614155760480992 entropy -4.204383498569368
epoch: 10, step: 21
	action: tensor([[-0.0961, -0.0907, -0.1912, -0.0688, -0.0036, -0.0363,  0.1469]],
       dtype=torch.float64)
	q_value: tensor([[1.4391]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07423307745842844, distance: 1.1010512213731554 entropy -4.204206941067063
epoch: 10, step: 22
	action: tensor([[-0.0360, -0.0325, -0.1918, -0.1193, -0.0365,  0.0496, -0.0246]],
       dtype=torch.float64)
	q_value: tensor([[1.4348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18395848844834628, distance: 1.0337435649330773 entropy -4.202000040423231
epoch: 10, step: 23
	action: tensor([[-0.0994,  0.0288, -0.1909,  0.0106, -0.1532,  0.1364, -0.1154]],
       dtype=torch.float64)
	q_value: tensor([[1.4301]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2172885375397059, distance: 1.0124125969409523 entropy -4.205853011675489
epoch: 10, step: 24
	action: tensor([[-0.0942, -0.1004, -0.1911,  0.1365, -0.0090, -0.0300, -0.0151]],
       dtype=torch.float64)
	q_value: tensor([[1.4563]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14928369237738182, distance: 1.0554777249061373 entropy -4.205503622512383
epoch: 10, step: 25
	action: tensor([[-0.1041, -0.0083, -0.1939, -0.0204, -0.0847,  0.0507, -0.0395]],
       dtype=torch.float64)
	q_value: tensor([[1.5249]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16435653653847437, distance: 1.0460855510126865 entropy -4.194162383476905
epoch: 10, step: 26
	action: tensor([[-0.0171, -0.0933, -0.1915, -0.0139, -0.1206,  0.0376,  0.0102]],
       dtype=torch.float64)
	q_value: tensor([[1.4514]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19457831243551016, distance: 1.0269950565014816 entropy -4.203488787903556
epoch: 10, step: 27
	action: tensor([[-0.0649, -0.0346, -0.1917,  0.0018,  0.0213, -0.0574,  0.1250]],
       dtype=torch.float64)
	q_value: tensor([[1.4810]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17474694574913363, distance: 1.0395616827196226 entropy -4.203016382158465
epoch: 10, step: 28
	action: tensor([[-0.0115, -0.0427, -0.1929, -0.0061, -0.0760, -0.1002, -0.0222]],
       dtype=torch.float64)
	q_value: tensor([[1.4233]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.221408127537152, distance: 1.0097448021225774 entropy -4.198042846061743
epoch: 10, step: 29
	action: tensor([[-0.1075, -0.1543, -0.1930,  0.0654, -0.0082,  0.1342, -0.0507]],
       dtype=torch.float64)
	q_value: tensor([[1.4907]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09833373269920809, distance: 1.0866247793681434 entropy -4.198078685099625
epoch: 10, step: 30
	action: tensor([[ 0.0052, -0.1116, -0.1919,  0.1125, -0.1033,  0.0054,  0.0897]],
       dtype=torch.float64)
	q_value: tensor([[1.5244]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2420050436940997, distance: 0.996299359672129 entropy -4.201796661108627
epoch: 10, step: 31
	action: tensor([[-0.1163, -0.1112, -0.1932, -0.0154,  0.1113,  0.0780,  0.0105]],
       dtype=torch.float64)
	q_value: tensor([[1.4946]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08000340251864235, distance: 1.0976144196623627 entropy -4.19751894905781
epoch: 10, step: 32
	action: tensor([[ 0.0066, -0.1834, -0.1434,  0.0175, -0.0877, -0.0049, -0.0501]],
       dtype=torch.float64)
	q_value: tensor([[1.9115]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13974934301520525, distance: 1.0613758452641289 entropy -4.365030335475394
epoch: 10, step: 33
	action: tensor([[-0.0517, -0.0364, -0.1935,  0.0766, -0.0513,  0.0031,  0.0290]],
       dtype=torch.float64)
	q_value: tensor([[1.5441]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2259750895744279, distance: 1.0067790323906576 entropy -4.196187917600521
epoch: 10, step: 34
	action: tensor([[-0.1255, -0.0081, -0.1931,  0.0299, -0.1110, -0.0253,  0.0935]],
       dtype=torch.float64)
	q_value: tensor([[1.4653]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1511044807029145, distance: 1.05434760026169 entropy -4.197695326289937
epoch: 10, step: 35
	action: tensor([[-0.0349, -0.1302, -0.1924,  0.0024, -0.0818,  0.0359,  0.1044]],
       dtype=torch.float64)
	q_value: tensor([[1.4412]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15529237235665527, distance: 1.0517436562698455 entropy -4.1999244461798
epoch: 10, step: 36
	action: tensor([[-0.1364, -0.0059, -0.1918, -0.0437,  0.1139, -0.0203,  0.0640]],
       dtype=torch.float64)
	q_value: tensor([[1.4736]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11092880120838089, distance: 1.0790087439489926 entropy -4.202257034546989
epoch: 10, step: 37
	action: tensor([[-0.0718, -0.0813, -0.1924,  0.0099,  0.1124,  0.0731,  0.0477]],
       dtype=torch.float64)
	q_value: tensor([[1.4114]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16191919009927036, distance: 1.0476100148144019 entropy -4.199795249672514
epoch: 10, step: 38
	action: tensor([[ 0.0783, -0.1380, -0.1922, -0.0978, -0.0695, -0.1023,  0.0173]],
       dtype=torch.float64)
	q_value: tensor([[1.4425]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18636222556391502, distance: 1.0322199412658535 entropy -4.200231355735591
epoch: 10, step: 39
	action: tensor([[-0.0697, -0.0754, -0.1922, -0.1184, -0.0452, -0.0898,  0.0576]],
       dtype=torch.float64)
	q_value: tensor([[1.5116]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09818514865563543, distance: 1.0867143072003858 entropy -4.201276541025115
epoch: 10, step: 40
	action: tensor([[-0.0391, -0.0899, -0.1918, -0.0942, -0.0423,  0.0929,  0.0332]],
       dtype=torch.float64)
	q_value: tensor([[1.4587]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15843079620060074, distance: 1.0497880156844346 entropy -4.202428830616629
epoch: 10, step: 41
	action: tensor([[-0.1559, -0.0876, -0.1908,  0.0816,  0.0779, -0.0366, -0.0393]],
       dtype=torch.float64)
	q_value: tensor([[1.4410]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06972490581417334, distance: 1.1037288389918085 entropy -4.206217348661743
epoch: 10, step: 42
	action: tensor([[-0.1190, -0.1028, -0.1937,  0.0231, -0.1308, -0.0359,  0.0946]],
       dtype=torch.float64)
	q_value: tensor([[1.5082]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08325914461044914, distance: 1.0956705447291724 entropy -4.194572746616063
epoch: 10, step: 43
	action: tensor([[-0.0629, -0.1246, -0.1922,  0.0502, -0.0846,  0.0025, -0.0521]],
       dtype=torch.float64)
	q_value: tensor([[1.4869]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14541260545641344, distance: 1.057876414387908 entropy -4.200606504693901
epoch: 10, step: 44
	action: tensor([[-0.1002, -0.1347, -0.1926,  0.0031,  0.0606,  0.0204,  0.1027]],
       dtype=torch.float64)
	q_value: tensor([[1.5327]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07730266605476444, distance: 1.0992243141648523 entropy -4.199474389002484
epoch: 10, step: 45
	action: tensor([[-0.0652, -0.1698, -0.1922,  0.0356, -0.0509,  0.1248,  0.0475]],
       dtype=torch.float64)
	q_value: tensor([[1.4657]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12243758813840011, distance: 1.0720022572040229 entropy -4.200160115188614
epoch: 10, step: 46
	action: tensor([[-0.0661, -0.1162, -0.1915,  0.0006, -0.0956, -0.0334, -0.0010]],
       dtype=torch.float64)
	q_value: tensor([[1.5029]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12369955873596572, distance: 1.071231188378059 entropy -4.20358679212685
epoch: 10, step: 47
	action: tensor([[-0.0229, -0.0873, -0.1924,  0.0366,  0.0264,  0.0233,  0.0182]],
       dtype=torch.float64)
	q_value: tensor([[1.5107]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21408751982098229, distance: 1.014480692394343 entropy -4.200225633515754
epoch: 10, step: 48
	action: tensor([[-0.0397, -0.0396, -0.1928,  0.0156, -0.1112,  0.0009,  0.0364]],
       dtype=torch.float64)
	q_value: tensor([[1.4729]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22216048743708572, distance: 1.0092568217454634 entropy -4.19839420265114
epoch: 10, step: 49
	action: tensor([[-0.0639, -0.0841, -0.1924, -0.0273,  0.0570,  0.0265,  0.0637]],
       dtype=torch.float64)
	q_value: tensor([[1.4566]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14810444013090562, distance: 1.0562090165410363 entropy -4.2004587561531705
epoch: 10, step: 50
	action: tensor([[ 1.0877e-02, -7.6336e-02, -1.9214e-01,  1.4524e-02, -2.3350e-01,
         -5.6929e-03,  9.8092e-05]], dtype=torch.float64)
	q_value: tensor([[1.4426]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24793967127134653, distance: 0.9923914944187722 entropy -4.200687576050159
epoch: 10, step: 51
	action: tensor([[-0.0018, -0.0833, -0.1920,  0.0562, -0.1373, -0.0834, -0.0284]],
       dtype=torch.float64)
	q_value: tensor([[1.4996]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23393225439731757, distance: 1.0015906966275474 entropy -4.20226059709332
epoch: 10, step: 52
	action: tensor([[ 0.0315, -0.0449, -0.1932,  0.0632, -0.0233,  0.1146,  0.0167]],
       dtype=torch.float64)
	q_value: tensor([[1.5265]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3293705851097637, distance: 0.9371260111506297 entropy -4.1977030269076225
epoch: 10, step: 53
	action: tensor([[ 0.0487, -0.1447, -0.1923, -0.0555, -0.0921, -0.1414, -0.0025]],
       dtype=torch.float64)
	q_value: tensor([[1.4487]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1795206279796322, distance: 1.0365506460466132 entropy -4.200829160801222
epoch: 10, step: 54
	action: tensor([[ 0.0209, -0.1454, -0.1926, -0.0577,  0.0095,  0.0414, -0.0294]],
       dtype=torch.float64)
	q_value: tensor([[1.5401]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18051907295358793, distance: 1.0359197625258236 entropy -4.199696324553774
epoch: 10, step: 55
	action: tensor([[ 0.0763,  0.0009, -0.1916,  0.0288,  0.0464, -0.0450, -0.0381]],
       dtype=torch.float64)
	q_value: tensor([[1.4988]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36436749003485536, distance: 0.9123463606772699 entropy -4.203178348406229
epoch: 10, step: 56
	action: tensor([[ 0.0049, -0.0877, -0.1935, -0.0316, -0.0392, -0.0755,  0.1721]],
       dtype=torch.float64)
	q_value: tensor([[1.4550]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19825165552353186, distance: 1.024650435994268 entropy -4.196496681917953
epoch: 10, step: 57
	action: tensor([[-0.0953,  0.0461, -0.1924,  0.0243, -0.0763,  0.0417,  0.0544]],
       dtype=torch.float64)
	q_value: tensor([[1.4394]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23131183650620757, distance: 1.0033022590879355 entropy -4.199893925204489
epoch: 10, step: 58
	action: tensor([[-0.1309, -0.1620, -0.1922, -0.0068,  0.0219, -0.0033, -0.0180]],
       dtype=torch.float64)
	q_value: tensor([[1.4063]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.014840887177905104, distance: 1.135820970536635 entropy -4.200792763528855
epoch: 10, step: 59
	action: tensor([[-0.0225, -0.0994, -0.1922,  0.0601, -0.1176,  0.0576,  0.0109]],
       dtype=torch.float64)
	q_value: tensor([[1.5197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2162478292873028, distance: 1.0130854350155325 entropy -4.200224798443819
epoch: 10, step: 60
	action: tensor([[ 0.0183, -0.0661, -0.1923, -0.0564, -0.1165, -0.0403,  0.0498]],
       dtype=torch.float64)
	q_value: tensor([[1.4955]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22967564500365134, distance: 1.00436948119631 entropy -4.200758735972824
epoch: 10, step: 61
	action: tensor([[-0.0398, -0.0205, -0.1920,  0.0316,  0.0582,  0.0077,  0.0150]],
       dtype=torch.float64)
	q_value: tensor([[1.4601]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24119261242030665, distance: 0.9968331415783529 entropy -4.20191146151963
epoch: 10, step: 62
	action: tensor([[-0.0689, -0.0647, -0.1930, -0.0915, -0.0629,  0.0247,  0.0641]],
       dtype=torch.float64)
	q_value: tensor([[1.4394]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13519036474720236, distance: 1.06418455886495 entropy -4.197579756312355
epoch: 10, step: 63
	action: tensor([[ 0.0827, -0.0674, -0.1911, -0.0223,  0.0674,  0.0468, -0.0179]],
       dtype=torch.float64)
	q_value: tensor([[1.4340]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3110339563087835, distance: 0.9498512578725843 entropy -4.204755178049655
epoch: 10, step: 64
	action: tensor([[-0.0529, -0.0708, -0.1434, -0.0605, -0.0248,  0.0565, -0.0302]],
       dtype=torch.float64)
	q_value: tensor([[1.9115]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14464034254125369, distance: 1.0583542907569243 entropy -4.365030335475394
epoch: 10, step: 65
	action: tensor([[-0.1210,  0.0230, -0.1926, -0.0018, -0.0021,  0.0241,  0.0224]],
       dtype=torch.float64)
	q_value: tensor([[1.4469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16647752569957075, distance: 1.044757146092632 entropy -4.199021012390783
epoch: 10, step: 66
	action: tensor([[-0.0698, -0.0848, -0.1922, -0.0442,  0.0909, -0.0706,  0.0373]],
       dtype=torch.float64)
	q_value: tensor([[1.4185]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10711210229627643, distance: 1.0813223050844105 entropy -4.200530025669548
epoch: 10, step: 67
	action: tensor([[-0.0428, -0.0815, -0.1927,  0.1001, -0.1814, -0.0824,  0.1108]],
       dtype=torch.float64)
	q_value: tensor([[1.4622]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2013062699717133, distance: 1.022696644049488 entropy -4.198624364708972
epoch: 10, step: 68
	action: tensor([[ 0.0258, -0.1236, -0.1932,  0.0267, -0.1158,  0.0166,  0.0594]],
       dtype=torch.float64)
	q_value: tensor([[1.4940]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22919781082548263, distance: 1.004680939390098 entropy -4.197447520410667
epoch: 10, step: 69
	action: tensor([[ 0.0698, -0.0364, -0.1922,  0.0129, -0.0797, -0.0340, -0.0142]],
       dtype=torch.float64)
	q_value: tensor([[1.4924]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33072023158089425, distance: 0.9361825498468789 entropy -4.2011381819133184
epoch: 10, step: 70
	action: tensor([[-0.0936, -0.0903, -0.1929, -0.0839, -0.0401,  0.0545,  0.1276]],
       dtype=torch.float64)
	q_value: tensor([[1.4736]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0942885368771339, distance: 1.0890595443899767 entropy -4.19895206387102
epoch: 10, step: 71
	action: tensor([[-0.1076, -0.0651, -0.1908, -0.0363,  0.0579, -0.0686,  0.0511]],
       dtype=torch.float64)
	q_value: tensor([[1.4264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09314570669637456, distance: 1.0897464175282163 entropy -4.205806012089284
epoch: 10, step: 72
	action: tensor([[-0.1055, -0.1069, -0.1927,  0.0330, -0.0952,  0.0272, -0.0389]],
       dtype=torch.float64)
	q_value: tensor([[1.4535]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1089885937692654, distance: 1.0801854550408747 entropy -4.198601092505508
epoch: 10, step: 73
	action: tensor([[-0.0653, -0.1393, -0.1921, -0.0255, -0.0526, -0.0356,  0.1086]],
       dtype=torch.float64)
	q_value: tensor([[1.5145]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09274569624935025, distance: 1.0899867328114015 entropy -4.20141835873958
epoch: 10, step: 74
	action: tensor([[-0.1189,  0.0057, -0.1921, -0.0213, -0.0880,  0.0655, -0.0339]],
       dtype=torch.float64)
	q_value: tensor([[1.4800]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16491791696350977, distance: 1.045734114881018 entropy -4.201022463930057
epoch: 10, step: 75
	action: tensor([[-0.1004, -0.1268, -0.1914,  0.0004, -0.0105, -0.0807,  0.0878]],
       dtype=torch.float64)
	q_value: tensor([[1.4414]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06562007680532711, distance: 1.106161254604364 entropy -4.20388548961421
epoch: 10, step: 76
	action: tensor([[ 0.0036, -0.0856, -0.1927,  0.1432, -0.1578, -0.1299,  0.1441]],
       dtype=torch.float64)
	q_value: tensor([[1.4900]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25750542241394203, distance: 0.9860599878707605 entropy -4.198448324699207
epoch: 10, step: 77
	action: tensor([[ 0.0595, -0.0753, -0.1940,  0.0668,  0.0629,  0.1586,  0.1289]],
       dtype=torch.float64)
	q_value: tensor([[1.5013]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34142405381218355, distance: 0.9286661709022663 entropy -4.194549291673774
epoch: 10, step: 78
	action: tensor([[-0.1654, -0.2039, -0.1919,  0.0069, -0.0395,  0.0303, -0.0078]],
       dtype=torch.float64)
	q_value: tensor([[1.4183]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.046616629402595144, distance: 1.1707131826456516 entropy -4.201978226237831
epoch: 10, step: 79
	action: tensor([[-0.0340, -0.2308, -0.1917, -0.0574, -0.0085, -0.0356,  0.0275]],
       dtype=torch.float64)
	q_value: tensor([[1.5437]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.030519057814455963, distance: 1.1267467949659473 entropy -4.202498562569378
epoch: 10, step: 80
	action: tensor([[-0.0174, -0.1023, -0.1918,  0.0128, -0.0958, -0.0737,  0.1233]],
       dtype=torch.float64)
	q_value: tensor([[1.5443]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1802899955388887, distance: 1.0360645427345048 entropy -4.202157915155687
epoch: 10, step: 81
	action: tensor([[-0.0603, -0.0436, -0.1927,  0.0271, -0.0308,  0.1737, -0.0067]],
       dtype=torch.float64)
	q_value: tensor([[1.4749]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23124947489123682, distance: 1.003342955877101 entropy -4.199084428297985
epoch: 10, step: 82
	action: tensor([[-0.0757, -0.0283, -0.1913,  0.0233, -0.0820, -0.0203,  0.0357]],
       dtype=torch.float64)
	q_value: tensor([[1.4451]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19068283315348988, distance: 1.029475628074947 entropy -4.204492984916554
epoch: 10, step: 83
	action: tensor([[-0.0996,  0.0081, -0.1927,  0.1076, -0.0551,  0.0476, -0.0599]],
       dtype=torch.float64)
	q_value: tensor([[1.4549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2299397240400337, distance: 1.0041973097866794 entropy -4.199018558640537
epoch: 10, step: 84
	action: tensor([[-0.1336, -0.0686, -0.1931,  0.1483, -0.1285,  0.2120,  0.0994]],
       dtype=torch.float64)
	q_value: tensor([[1.4705]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1731713038099162, distance: 1.0405536183945003 entropy -4.197392089739464
epoch: 10, step: 85
	action: tensor([[-0.0283, -0.1376, -0.1919,  0.0034, -0.0570, -0.0470,  0.1227]],
       dtype=torch.float64)
	q_value: tensor([[1.4614]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13601432461366514, distance: 1.0636774794525636 entropy -4.202128313782895
epoch: 10, step: 86
	action: tensor([[ 0.0321, -0.1505, -0.1926, -0.0129, -0.0565,  0.1119,  0.1622]],
       dtype=torch.float64)
	q_value: tensor([[1.4816]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20974570540278037, distance: 1.0172791084228996 entropy -4.199243606434698
epoch: 10, step: 87
	action: tensor([[-0.0466, -0.0082, -0.1913,  0.0432, -0.2337, -0.0624,  0.0020]],
       dtype=torch.float64)
	q_value: tensor([[1.4489]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24383251782386817, distance: 0.9950976300183132 entropy -4.204203537166623
epoch: 10, step: 88
	action: tensor([[ 0.0224, -0.0082, -0.1926, -0.0022, -0.0438, -0.0321,  0.1581]],
       dtype=torch.float64)
	q_value: tensor([[1.4818]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2962798425797707, distance: 0.9599678508888148 entropy -4.199783917450675
epoch: 10, step: 89
	action: tensor([[-0.0573,  0.0115, -0.1926, -0.0805,  0.0394,  0.0213, -0.0137]],
       dtype=torch.float64)
	q_value: tensor([[1.3976]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21055147311066114, distance: 1.0167603515410018 entropy -4.199632224400387
epoch: 10, step: 90
	action: tensor([[-0.0404, -0.0559, -0.1918, -0.0530,  0.0253,  0.0192, -0.0198]],
       dtype=torch.float64)
	q_value: tensor([[1.4105]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18640509769793057, distance: 1.0321927460839118 entropy -4.202383380978088
epoch: 10, step: 91
	action: tensor([[-0.0119, -0.0814, -0.1920, -0.0129, -0.1760, -0.0094, -0.0634]],
       dtype=torch.float64)
	q_value: tensor([[1.4524]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20945965281361778, distance: 1.0174632067544966 entropy -4.20162948675154
epoch: 10, step: 92
	action: tensor([[-0.0501, -0.0407, -0.1919,  0.0497, -0.1088,  0.1371, -0.1084]],
       dtype=torch.float64)
	q_value: tensor([[1.5138]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24394611691168722, distance: 0.9950228804214128 entropy -4.202650466187371
epoch: 10, step: 93
	action: tensor([[-0.0268, -0.1046, -0.1917,  0.0819, -0.1689,  0.0296,  0.1164]],
       dtype=torch.float64)
	q_value: tensor([[1.4902]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21463509482465493, distance: 1.014127217244872 entropy -4.203263059774881
epoch: 10, step: 94
	action: tensor([[-0.0721, -0.0270, -0.1923, -0.0245, -0.2415,  0.0152,  0.0563]],
       dtype=torch.float64)
	q_value: tensor([[1.4828]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18446984745783368, distance: 1.033419624964501 entropy -4.200590765126337
epoch: 10, step: 95
	action: tensor([[-0.0599, -0.0974, -0.1912, -0.0470, -0.1234,  0.0029,  0.0322]],
       dtype=torch.float64)
	q_value: tensor([[1.4528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13272430143158664, distance: 1.0657007759501784 entropy -4.20480240385835
epoch: 10, step: 96
	action: tensor([[ 0.0136, -0.0240, -0.1434,  0.0180,  0.0273, -0.0428,  0.0344]],
       dtype=torch.float64)
	q_value: tensor([[1.9115]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25845934824200156, distance: 0.9854263600753925 entropy -4.365030335475394
epoch: 10, step: 97
	action: tensor([[-0.0253, -0.0850, -0.1944, -0.0146,  0.0623,  0.0111,  0.0083]],
       dtype=torch.float64)
	q_value: tensor([[1.4278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17549327695283023, distance: 1.0390915040197755 entropy -4.192356237227874
epoch: 10, step: 98
	action: tensor([[-0.0135, -0.1085, -0.1924, -0.1835,  0.0116,  0.0616,  0.0293]],
       dtype=torch.float64)
	q_value: tensor([[1.4641]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11731248066767286, distance: 1.0751280340107239 entropy -4.199730232693484
epoch: 10, step: 99
	action: tensor([[-0.0461, -0.0987, -0.1903,  0.0409,  0.0152, -0.0406,  0.1111]],
       dtype=torch.float64)
	q_value: tensor([[1.4336]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16289151998985807, distance: 1.0470021270435932 entropy -4.207940421909317
epoch: 10, step: 100
	action: tensor([[-0.0685,  0.0034, -0.1931, -0.0035, -0.0064, -0.0446,  0.0538]],
       dtype=torch.float64)
	q_value: tensor([[1.4648]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2050048268845871, distance: 1.020325966322444 entropy -4.197160188881815
epoch: 10, step: 101
	action: tensor([[-0.0327, -0.0911, -0.1928,  0.0750, -0.0906,  0.1399,  0.0983]],
       dtype=torch.float64)
	q_value: tensor([[1.4263]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23395091946676927, distance: 1.001578494786459 entropy -4.198473132875139
epoch: 10, step: 102
	action: tensor([[-0.1358,  0.1680, -0.1918,  0.0765,  0.0534, -0.0023, -0.0079]],
       dtype=torch.float64)
	q_value: tensor([[1.4578]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2749343379779027, distance: 0.9744181445531348 entropy -4.202459969115913
epoch: 10, step: 103
	action: tensor([[-0.0415,  0.0440, -0.1932,  0.0190, -0.1555,  0.0003, -0.0096]],
       dtype=torch.float64)
	q_value: tensor([[1.3811]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2831147739491494, distance: 0.9689056946202044 entropy -4.196890909211167
epoch: 10, step: 104
	action: tensor([[-0.0142, -0.0877, -0.1923,  0.0447, -0.0241,  0.0993, -0.0082]],
       dtype=torch.float64)
	q_value: tensor([[1.4356]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24117444144155287, distance: 0.9968450769691353 entropy -4.200972371466945
epoch: 10, step: 105
	action: tensor([[-0.0661,  0.0404, -0.1921,  0.0035,  0.1024, -0.0252,  0.1524]],
       dtype=torch.float64)
	q_value: tensor([[1.4770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23750338901434265, distance: 0.999253440273314 entropy -4.2013085854642265
epoch: 10, step: 106
	action: tensor([[-0.0921, -0.0814, -0.1927,  0.0685, -0.1584,  0.0475,  0.0525]],
       dtype=torch.float64)
	q_value: tensor([[1.3731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15847279266856817, distance: 1.0497618217965128 entropy -4.198547610732418
epoch: 10, step: 107
	action: tensor([[-0.0932, -0.0964, -0.1922, -0.0093, -0.1076,  0.0068, -0.0143]],
       dtype=torch.float64)
	q_value: tensor([[1.4853]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11207983681462097, distance: 1.0783100486548296 entropy -4.2011919576512495
epoch: 10, step: 108
	action: tensor([[-0.0218, -0.0502, -0.1919,  0.0501, -0.1345, -0.1450, -0.0314]],
       dtype=torch.float64)
	q_value: tensor([[1.4965]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22890805013262083, distance: 1.0048697619538394 entropy -4.202080575684986
epoch: 10, step: 109
	action: tensor([[-0.0148, -0.1417, -0.1936, -0.0472,  0.0042,  0.0561,  0.0002]],
       dtype=torch.float64)
	q_value: tensor([[1.5238]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15393587612768445, distance: 1.0525878027803053 entropy -4.196177865239741
epoch: 10, step: 110
	action: tensor([[-0.0097, -0.0202, -0.1915, -0.0498, -0.1133, -0.1314,  0.0697]],
       dtype=torch.float64)
	q_value: tensor([[1.4874]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22802224860694142, distance: 1.0054467746449924 entropy -4.203308708457557
epoch: 10, step: 111
	action: tensor([[-0.1590, -0.0422, -0.1926,  0.0487, -0.0225, -0.0101, -0.0167]],
       dtype=torch.float64)
	q_value: tensor([[1.4524]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09533546093406342, distance: 1.0884299331088008 entropy -4.199596189787656
epoch: 10, step: 112
	action: tensor([[-0.0287, -0.0774, -0.1928,  0.0497,  0.0240, -0.0632,  0.0368]],
       dtype=torch.float64)
	q_value: tensor([[1.4805]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1979584942296161, distance: 1.0248377518741967 entropy -4.198037732650977
epoch: 10, step: 113
	action: tensor([[ 0.0040, -0.0196, -0.1935,  0.0327,  0.1163, -0.0014, -0.0109]],
       dtype=torch.float64)
	q_value: tensor([[1.4815]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28093300508652375, distance: 0.9703789585813073 entropy -4.195780240186461
epoch: 10, step: 114
	action: tensor([[-0.0531, -0.0990, -0.1932, -0.1450,  0.0791,  0.0232,  0.1801]],
       dtype=torch.float64)
	q_value: tensor([[1.4433]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09829385741660535, distance: 1.0866488065457653 entropy -4.197294139408142
epoch: 10, step: 115
	action: tensor([[-0.0617, -0.1539, -0.1909,  0.0196, -0.1434, -0.0179,  0.0691]],
       dtype=torch.float64)
	q_value: tensor([[1.3962]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1078407272260542, distance: 1.0808810183396196 entropy -4.205174302828772
epoch: 10, step: 116
	action: tensor([[ 0.0247, -0.1087, -0.1921,  0.0063, -0.0034, -0.0925,  0.1697]],
       dtype=torch.float64)
	q_value: tensor([[1.5131]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20722884683763743, distance: 1.0188977723769237 entropy -4.201215536243362
epoch: 10, step: 117
	action: tensor([[-0.1047, -0.0822, -0.1930, -0.0866,  0.1220, -0.0448,  0.0470]],
       dtype=torch.float64)
	q_value: tensor([[1.4577]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0673788297386676, distance: 1.1051197185885382 entropy -4.197553973005307
epoch: 10, step: 118
	action: tensor([[-0.0040, -0.0650, -0.1920,  0.0418,  0.0789,  0.0807,  0.1170]],
       dtype=torch.float64)
	q_value: tensor([[1.4438]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2580760720129255, distance: 0.9856809932551838 entropy -4.20112654285458
epoch: 10, step: 119
	action: tensor([[ 0.0034, -0.0768, -0.1924, -0.0027,  0.1442, -0.0101, -0.0926]],
       dtype=torch.float64)
	q_value: tensor([[1.4215]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22258148665717237, distance: 1.0089836588137586 entropy -4.199992445532275
epoch: 10, step: 120
	action: tensor([[-0.1371, -0.0763, -0.1928, -0.0649, -0.0508,  0.2502, -0.0374]],
       dtype=torch.float64)
	q_value: tensor([[1.4916]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1016461127421634, distance: 1.0846270191774843 entropy -4.198512619634671
epoch: 10, step: 121
	action: tensor([[-0.0708, -0.0524, -0.1896, -0.0267, -0.1881,  0.0306,  0.0176]],
       dtype=torch.float64)
	q_value: tensor([[1.4472]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16769873807918445, distance: 1.043991517048998 entropy -4.210775263132244
epoch: 10, step: 122
	action: tensor([[-0.0247, -0.0259, -0.1914,  0.1214, -0.2069, -0.0447,  0.0795]],
       dtype=torch.float64)
	q_value: tensor([[1.4652]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2821975182402845, distance: 0.9695253546054959 entropy -4.204005699201604
epoch: 10, step: 123
	action: tensor([[-0.0018, -0.0149, -0.1933, -0.0152, -0.0696,  0.0174,  0.0859]],
       dtype=torch.float64)
	q_value: tensor([[1.4743]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2710799024786741, distance: 0.97700470589806 entropy -4.197122469949836
epoch: 10, step: 124
	action: tensor([[-0.1059, -0.1403, -0.1921,  0.0452, -0.0668,  0.0794,  0.0321]],
       dtype=torch.float64)
	q_value: tensor([[1.4152]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09519605646320228, distance: 1.0885137907972031 entropy -4.2013433255929815
epoch: 10, step: 125
	action: tensor([[-0.0467, -0.0842, -0.1920,  0.0086, -0.0632, -0.0670,  0.0520]],
       dtype=torch.float64)
	q_value: tensor([[1.5030]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1652713456280549, distance: 1.0455128003942262 entropy -4.201612641029769
epoch: 10, step: 126
	action: tensor([[-0.1144, -0.1388, -0.1929,  0.0639, -0.0326, -0.0360,  0.0310]],
       dtype=torch.float64)
	q_value: tensor([[1.4812]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07371338852228271, distance: 1.101360221318652 entropy -4.198216219080361
epoch: 10, step: 127
	action: tensor([[-0.0177, -0.1482, -0.1930, -0.0012, -0.0338,  0.1269,  0.0199]],
       dtype=torch.float64)
	q_value: tensor([[1.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17480425931936971, distance: 1.0395255834736297 entropy -4.197681796570106
LOSS epoch 10 actor 0.9551596384184755 critic 1.8950101525299592 entropy 0.01
epoch: 11, step: 0
	action: tensor([[-0.0358, -0.0307, -0.1064, -0.0093, -0.1310, -0.0847, -0.0302]],
       dtype=torch.float64)
	q_value: tensor([[2.2265]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19394958063916767, distance: 1.0273958269877252 entropy -4.455330094037794
epoch: 11, step: 1
	action: tensor([[-0.0639, -0.1022, -0.1779,  0.0964,  0.0076,  0.0774,  0.0803]],
       dtype=torch.float64)
	q_value: tensor([[1.7093]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18644934304247462, distance: 1.0321646790787702 entropy -4.191979795588027
epoch: 11, step: 2
	action: tensor([[-0.0450, -0.0406, -0.1755,  0.0386,  0.0618,  0.0937,  0.1141]],
       dtype=torch.float64)
	q_value: tensor([[1.7174]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2331952856353331, distance: 1.0020723535172733 entropy -4.201206427159873
epoch: 11, step: 3
	action: tensor([[-0.0855, -0.0521, -0.1751, -0.0233, -0.0970, -0.0157,  0.0456]],
       dtype=torch.float64)
	q_value: tensor([[1.6451]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14435163428311593, distance: 1.0585328880534128 entropy -4.203144519859175
epoch: 11, step: 4
	action: tensor([[-0.0821, -0.0514, -0.1753, -0.1285, -0.0124, -0.0964,  0.0203]],
       dtype=torch.float64)
	q_value: tensor([[1.6997]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09637108189972454, distance: 1.0878067608790924 entropy -4.202268595113051
epoch: 11, step: 5
	action: tensor([[-0.1021, -0.0042, -0.1755, -0.0467, -0.0362, -0.1154,  0.0135]],
       dtype=torch.float64)
	q_value: tensor([[1.6921]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1393266993562341, distance: 1.0616365417733649 entropy -4.201704639340503
epoch: 11, step: 6
	action: tensor([[-0.0461, -0.0733, -0.1761,  0.0061,  0.1197,  0.0060,  0.0777]],
       dtype=torch.float64)
	q_value: tensor([[1.6918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1813079724227854, distance: 1.035421011851311 entropy -4.199026655685693
epoch: 11, step: 7
	action: tensor([[-0.0724, -0.0879, -0.1757,  0.0742, -0.0568,  0.0741, -0.0478]],
       dtype=torch.float64)
	q_value: tensor([[1.6734]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18530657389701188, distance: 1.0328893493977402 entropy -4.200517959967319
epoch: 11, step: 8
	action: tensor([[-0.1044, -0.1027, -0.1756, -0.0297, -0.0403,  0.0340, -0.0037]],
       dtype=torch.float64)
	q_value: tensor([[1.7539]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09052402208984534, distance: 1.091320490428744 entropy -4.201149710909751
epoch: 11, step: 9
	action: tensor([[-0.0612,  0.0397, -0.1750,  0.0082, -0.0909,  0.0124,  0.0675]],
       dtype=torch.float64)
	q_value: tensor([[1.7281]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25424388962749267, distance: 0.9882233322461059 entropy -4.203425253928514
epoch: 11, step: 10
	action: tensor([[-0.0762, -0.0941, -0.1755, -0.0740,  0.0432, -0.1023,  0.1129]],
       dtype=torch.float64)
	q_value: tensor([[1.6374]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08268513737139171, distance: 1.0960135121027357 entropy -4.2014977573234695
epoch: 11, step: 11
	action: tensor([[-0.0175, -0.1658, -0.1757,  0.1089, -0.0412,  0.0423, -0.0302]],
       dtype=torch.float64)
	q_value: tensor([[1.6907]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19313678807308, distance: 1.027913691171839 entropy -4.200568870810382
epoch: 11, step: 12
	action: tensor([[-0.0875, -0.0526, -0.1762, -0.1131, -0.0109,  0.0643,  0.0399]],
       dtype=torch.float64)
	q_value: tensor([[1.8046]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12259912791195582, distance: 1.071903586745086 entropy -4.198374967760231
epoch: 11, step: 13
	action: tensor([[-0.0411, -0.1177, -0.1743,  0.0322, -0.0008,  0.0508, -0.0095]],
       dtype=torch.float64)
	q_value: tensor([[1.6557]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17582630431470014, distance: 1.0388816325641224 entropy -4.206609545121565
epoch: 11, step: 14
	action: tensor([[-0.0291, -0.1555, -0.1758,  0.0283, -0.1735, -0.0006, -0.0151]],
       dtype=torch.float64)
	q_value: tensor([[1.7424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1489913213739431, distance: 1.0556590806468646 entropy -4.200354304054694
epoch: 11, step: 15
	action: tensor([[-0.1165, -0.1256, -0.1755, -0.0231,  0.0390,  0.0566,  0.1453]],
       dtype=torch.float64)
	q_value: tensor([[1.8018]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06284660303620593, distance: 1.1078017200829073 entropy -4.2014521538069545
epoch: 11, step: 16
	action: tensor([[-0.0652, -0.0482, -0.1744,  0.0523,  0.1654, -0.0267,  0.0091]],
       dtype=torch.float64)
	q_value: tensor([[1.6842]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18685779048527063, distance: 1.031905544660927 entropy -4.2057861942270245
epoch: 11, step: 17
	action: tensor([[-0.1343, -0.0468, -0.1765, -0.0625, -0.0841, -0.0547,  0.0150]],
       dtype=torch.float64)
	q_value: tensor([[1.6908]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07502846560328436, distance: 1.1005781264426118 entropy -4.1972492576666465
epoch: 11, step: 18
	action: tensor([[-0.0237,  0.0312, -0.1751,  0.0383, -0.1588, -0.0732, -0.0186]],
       dtype=torch.float64)
	q_value: tensor([[1.7087]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2868184086033566, distance: 0.9663996313359303 entropy -4.202974441763518
epoch: 11, step: 19
	action: tensor([[-0.0390, -0.0821, -0.1767, -0.0056, -0.0276,  0.0124,  0.0635]],
       dtype=torch.float64)
	q_value: tensor([[1.7019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18262054841510034, distance: 1.0345906544711982 entropy -4.197287202372048
epoch: 11, step: 20
	action: tensor([[-0.0899, -0.1413, -0.1756,  0.0517, -0.1594, -0.0088, -0.0748]],
       dtype=torch.float64)
	q_value: tensor([[1.6972]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.102841491773634, distance: 1.0839051588916702 entropy -4.20091220424423
epoch: 11, step: 21
	action: tensor([[-0.0069, -0.0442, -0.1755,  0.0109, -0.0660,  0.0340,  0.0664]],
       dtype=torch.float64)
	q_value: tensor([[1.8213]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25410944115456546, distance: 0.9883124090422822 entropy -4.201375625959076
epoch: 11, step: 22
	action: tensor([[-0.0232, -0.1923, -0.1756,  0.0556, -0.1046, -0.1190,  0.0123]],
       dtype=torch.float64)
	q_value: tensor([[1.6779]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11626468010256374, distance: 1.0757659639827504 entropy -4.201319570846497
epoch: 11, step: 23
	action: tensor([[-0.0374, -0.0615, -0.1767, -0.0636, -0.0033, -0.0609,  0.0136]],
       dtype=torch.float64)
	q_value: tensor([[1.8401]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1656702262596378, distance: 1.0452629679267622 entropy -4.196584621509631
epoch: 11, step: 24
	action: tensor([[-0.0949, -0.0798, -0.1760,  0.0352, -0.0259, -0.0957,  0.0653]],
       dtype=torch.float64)
	q_value: tensor([[1.7018]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12026034493595827, distance: 1.073331258829635 entropy -4.199519112321004
epoch: 11, step: 25
	action: tensor([[-0.1039,  0.0301, -0.1764,  0.1227, -0.1027, -0.0465,  0.1049]],
       dtype=torch.float64)
	q_value: tensor([[1.7296]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23206883388009292, distance: 1.002808115760321 entropy -4.197860240801071
epoch: 11, step: 26
	action: tensor([[-0.0701, -0.0968, -0.1763,  0.0538, -0.0507,  0.0524, -0.0616]],
       dtype=torch.float64)
	q_value: tensor([[1.6759]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16948722394392812, distance: 1.042869226180878 entropy -4.198427091835349
epoch: 11, step: 27
	action: tensor([[-0.1285, -0.1101, -0.1757, -0.0470, -0.0484,  0.0520,  0.0943]],
       dtype=torch.float64)
	q_value: tensor([[1.7614]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05249807760344083, distance: 1.113901383011311 entropy -4.2007334674411485
epoch: 11, step: 28
	action: tensor([[-0.0440, -0.0278, -0.1741,  0.0078,  0.0707,  0.0547,  0.0282]],
       dtype=torch.float64)
	q_value: tensor([[1.7001]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22656521091769088, distance: 1.0063951719549833 entropy -4.206925569310019
epoch: 11, step: 29
	action: tensor([[-0.0146, -0.0038, -0.1756, -0.0696, -0.0275,  0.0240,  0.0805]],
       dtype=torch.float64)
	q_value: tensor([[1.6590]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24095897611334294, distance: 0.9969865919092699 entropy -4.201062479360711
epoch: 11, step: 30
	action: tensor([[ 0.0720, -0.1621, -0.1752,  0.0474, -0.0384, -0.1125, -0.0757]],
       dtype=torch.float64)
	q_value: tensor([[1.6270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22680178421695318, distance: 1.0062412453160403 entropy -4.202993432891162
epoch: 11, step: 31
	action: tensor([[-0.0386, -0.1877, -0.1776,  0.1378, -0.1097,  0.1113,  0.0418]],
       dtype=torch.float64)
	q_value: tensor([[1.8371]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1765697670077755, distance: 1.0384129546616652 entropy -4.193469555597366
epoch: 11, step: 32
	action: tensor([[-0.0290, -0.0700, -0.1064,  0.0285, -0.0230, -0.0223,  0.0675]],
       dtype=torch.float64)
	q_value: tensor([[2.2265]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18667416960268735, distance: 1.032022048704717 entropy -4.455330094037794
epoch: 11, step: 33
	action: tensor([[-0.0504, -0.0203, -0.1778, -0.0377, -0.0443, -0.1628,  0.0715]],
       dtype=torch.float64)
	q_value: tensor([[1.6780]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1581842021348172, distance: 1.0499418072758877 entropy -4.192294068221256
epoch: 11, step: 34
	action: tensor([[-0.0927, -0.1194, -0.1766, -0.0051, -0.1651, -0.0051,  0.0562]],
       dtype=torch.float64)
	q_value: tensor([[1.6942]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0840781820651666, distance: 1.09518098667311 entropy -4.197382862688913
epoch: 11, step: 35
	action: tensor([[-0.0226, -0.0593, -0.1747,  0.0407, -0.0103,  0.0391,  0.0366]],
       dtype=torch.float64)
	q_value: tensor([[1.7490]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23154649308093633, distance: 1.0031491089205884 entropy -4.204388445901574
epoch: 11, step: 36
	action: tensor([[-0.0450, -0.0529, -0.1760,  0.1679, -0.0815,  0.0587,  0.1758]],
       dtype=torch.float64)
	q_value: tensor([[1.6950]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2674556226966823, distance: 0.9794305875745508 entropy -4.199674363200591
epoch: 11, step: 37
	action: tensor([[ 0.0658,  0.0707, -0.1759, -0.0248,  0.0481,  0.1325, -0.0031]],
       dtype=torch.float64)
	q_value: tensor([[1.6906]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4086586877837025, distance: 0.8799860825680853 entropy -4.199944888384545
epoch: 11, step: 38
	action: tensor([[-0.0260, -0.0211, -0.1753,  0.0964, -0.0105,  0.0202, -0.0483]],
       dtype=torch.float64)
	q_value: tensor([[1.5891]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2833619113813183, distance: 0.9687386710177145 entropy -4.20281799853092
epoch: 11, step: 39
	action: tensor([[-0.0684, -0.0182, -0.1767,  0.0959,  0.0331, -0.0690,  0.0272]],
       dtype=torch.float64)
	q_value: tensor([[1.7192]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22109988949559511, distance: 1.0099446571254846 entropy -4.196776021624381
epoch: 11, step: 40
	action: tensor([[-0.0463, -0.1218, -0.1771, -0.0182,  0.0071,  0.0262,  0.2227]],
       dtype=torch.float64)
	q_value: tensor([[1.7035]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13575485094385742, distance: 1.0638371901909112 entropy -4.195102928239657
epoch: 11, step: 41
	action: tensor([[-0.0726, -0.1083, -0.1747,  0.0279, -0.0282, -0.0402, -0.0426]],
       dtype=torch.float64)
	q_value: tensor([[1.6672]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13072723079549353, distance: 1.066927061861525 entropy -4.204606906525596
epoch: 11, step: 42
	action: tensor([[-0.0705, -0.0447, -0.1764,  0.0713,  0.0400,  0.0977,  0.0493]],
       dtype=torch.float64)
	q_value: tensor([[1.7695]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22236268557422123, distance: 1.0091256358712672 entropy -4.197823584128392
epoch: 11, step: 43
	action: tensor([[-0.0635, -0.1146, -0.1754,  0.0069, -0.0248,  0.0780,  0.0636]],
       dtype=torch.float64)
	q_value: tensor([[1.6798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14790543253289135, distance: 1.0563323775227778 entropy -4.201986966722954
epoch: 11, step: 44
	action: tensor([[-0.0813, -0.0933, -0.1749,  0.0973, -0.0675, -0.1223, -0.0468]],
       dtype=torch.float64)
	q_value: tensor([[1.7133]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1481990822939463, distance: 1.0561503446291536 entropy -4.203905157183137
epoch: 11, step: 45
	action: tensor([[-0.0209, -0.1088, -0.1773,  0.1406, -0.0930,  0.0853, -0.0085]],
       dtype=torch.float64)
	q_value: tensor([[1.8035]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2525014337278726, distance: 0.9893771484207748 entropy -4.194417584523671
epoch: 11, step: 46
	action: tensor([[-0.0209, -0.1102, -0.1759,  0.1474, -0.0193, -0.0647,  0.0391]],
       dtype=torch.float64)
	q_value: tensor([[1.7717]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22658734307139572, distance: 1.0063807726466687 entropy -4.199691043549676
epoch: 11, step: 47
	action: tensor([[-0.0062, -0.0480, -0.1773,  0.1011, -0.0217,  0.0198,  0.0507]],
       dtype=torch.float64)
	q_value: tensor([[1.7727]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28529652468993616, distance: 0.9674301993469903 entropy -4.194194454818116
epoch: 11, step: 48
	action: tensor([[ 0.0280, -0.1294, -0.1765,  0.0188,  0.0275,  0.1374, -0.0029]],
       dtype=torch.float64)
	q_value: tensor([[1.7042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2525261557428048, distance: 0.9893607874536907 entropy -4.197708713993771
epoch: 11, step: 49
	action: tensor([[-0.1187, -0.0025, -0.1751,  0.0372, -0.0376,  0.0825, -0.1045]],
       dtype=torch.float64)
	q_value: tensor([[1.7244]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18422514045903793, distance: 1.03357465666191 entropy -4.202982774817005
epoch: 11, step: 50
	action: tensor([[-0.1214, -0.1368, -0.1753, -0.0732, -0.0952, -0.1042,  0.0768]],
       dtype=torch.float64)
	q_value: tensor([[1.7106]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.00637797573189447, distance: 1.1406891166425723 entropy -4.202195760216236
epoch: 11, step: 51
	action: tensor([[-0.0413, -0.0284, -0.1750, -0.0509, -0.0752,  0.0923,  0.0418]],
       dtype=torch.float64)
	q_value: tensor([[1.7538]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2161134459497165, distance: 1.013172283876585 entropy -4.203293216289314
epoch: 11, step: 52
	action: tensor([[-0.0301, -0.0101, -0.1745, -0.0211, -0.1861, -0.0024,  0.0411]],
       dtype=torch.float64)
	q_value: tensor([[1.6559]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24154918143100224, distance: 0.9965989045514873 entropy -4.2054177634894945
epoch: 11, step: 53
	action: tensor([[-0.0574, -0.1017, -0.1752,  0.0050, -0.0988,  0.0886,  0.0357]],
       dtype=torch.float64)
	q_value: tensor([[1.6823]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16522365918856075, distance: 1.045542664033506 entropy -4.2029097925531245
epoch: 11, step: 54
	action: tensor([[-0.0333, -0.0677, -0.1747, -0.0713, -0.1501,  0.0386,  0.1077]],
       dtype=torch.float64)
	q_value: tensor([[1.7213]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17908579667719715, distance: 1.0368252812054946 entropy -4.204731854595786
epoch: 11, step: 55
	action: tensor([[-0.0513,  0.0122, -0.1743, -0.0670, -0.0233, -0.1194,  0.0534]],
       dtype=torch.float64)
	q_value: tensor([[1.6733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1983007358164891, distance: 1.024619072715756 entropy -4.206404115481265
epoch: 11, step: 56
	action: tensor([[-0.1600, -0.0727, -0.1762, -0.0401, -0.1242,  0.0447, -0.0431]],
       dtype=torch.float64)
	q_value: tensor([[1.6590]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.048353958849038925, distance: 1.1163346779062246 entropy -4.198772439413848
epoch: 11, step: 57
	action: tensor([[ 0.0394,  0.0374, -0.1741,  0.0155, -0.0253,  0.0800, -0.0052]],
       dtype=torch.float64)
	q_value: tensor([[1.7376]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3604887562771082, distance: 0.9151257705369137 entropy -4.2067008065474125
epoch: 11, step: 58
	action: tensor([[-0.0708, -0.1367, -0.1759, -0.0789, -0.1493, -0.0418, -0.0467]],
       dtype=torch.float64)
	q_value: tensor([[1.6379]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07113117708890027, distance: 1.1028942852143826 entropy -4.200504287187045
epoch: 11, step: 59
	action: tensor([[-0.0796, -0.1043, -0.1749,  0.1062, -0.0629,  0.0720, -0.0205]],
       dtype=torch.float64)
	q_value: tensor([[1.7859]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17664945097435825, distance: 1.0383627094424708 entropy -4.203885170131622
epoch: 11, step: 60
	action: tensor([[-0.0428, -0.1331, -0.1757, -0.0077, -0.0104,  0.0336, -0.0385]],
       dtype=torch.float64)
	q_value: tensor([[1.7629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14274448037373877, distance: 1.0595265364018729 entropy -4.200426293344233
epoch: 11, step: 61
	action: tensor([[-0.0646, -0.1055, -0.1756, -0.0066, -0.1078,  0.1482, -0.0221]],
       dtype=torch.float64)
	q_value: tensor([[1.7565]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16079258507122185, distance: 1.048313912530467 entropy -4.2009311767769475
epoch: 11, step: 62
	action: tensor([[-0.0737, -0.0473, -0.1740,  0.1520,  0.0530,  0.0535,  0.0291]],
       dtype=torch.float64)
	q_value: tensor([[1.7330]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2382375359464367, distance: 0.9987722738635584 entropy -4.207289534258327
epoch: 11, step: 63
	action: tensor([[-0.0086,  0.0380, -0.1764,  0.0542,  0.0354, -0.0570,  0.0683]],
       dtype=torch.float64)
	q_value: tensor([[1.7106]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3072926258936859, distance: 0.9524267810169693 entropy -4.197554260328507
epoch: 11, step: 64
	action: tensor([[-0.0462,  0.0008, -0.1064,  0.0036, -0.1468,  0.1150,  0.0906]],
       dtype=torch.float64)
	q_value: tensor([[2.2265]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23168722483269744, distance: 1.003057248193647 entropy -4.455330094037794
epoch: 11, step: 65
	action: tensor([[-0.0652, -0.0255, -0.1759,  0.0626, -0.1569, -0.0821,  0.0461]],
       dtype=torch.float64)
	q_value: tensor([[1.6219]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1933222193241918, distance: 1.0277955681297593 entropy -4.200168201614976
epoch: 11, step: 66
	action: tensor([[-0.1122, -0.0890, -0.1763,  0.0917, -0.1391, -0.0713, -0.0188]],
       dtype=torch.float64)
	q_value: tensor([[1.7232]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12198935780133169, distance: 1.0722759941974322 entropy -4.198423639859756
epoch: 11, step: 67
	action: tensor([[-0.0503, -0.0882, -0.1762, -0.0695, -0.0709, -0.0056,  0.0808]],
       dtype=torch.float64)
	q_value: tensor([[1.7895]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13851877302537086, distance: 1.0621347115900728 entropy -4.198454372576035
epoch: 11, step: 68
	action: tensor([[-0.0710, -0.0689, -0.1750, -0.0086, -0.0003,  0.0520,  0.0674]],
       dtype=torch.float64)
	q_value: tensor([[1.6911]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16419831796337614, distance: 1.0461845778937509 entropy -4.203626559948441
epoch: 11, step: 69
	action: tensor([[-0.0958, -0.1317, -0.1752,  0.0323, -0.1218, -0.0256,  0.0201]],
       dtype=torch.float64)
	q_value: tensor([[1.6797]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09217447289870495, distance: 1.090329816276466 entropy -4.2027084185668375
epoch: 11, step: 70
	action: tensor([[-0.0481, -0.0839, -0.1755, -0.0470, -0.0401, -0.0015,  0.1116]],
       dtype=torch.float64)
	q_value: tensor([[1.7742]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15208272926271527, distance: 1.0537399216286392 entropy -4.201114175622432
epoch: 11, step: 71
	action: tensor([[-0.0485,  0.0049, -0.1751, -0.0032,  0.0922,  0.0803,  0.1282]],
       dtype=torch.float64)
	q_value: tensor([[1.6793]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25059861790531246, distance: 0.9906356163443563 entropy -4.202874756024383
epoch: 11, step: 72
	action: tensor([[-0.0600, -0.1118, -0.1749, -0.0248, -0.0236,  0.0523, -0.0572]],
       dtype=torch.float64)
	q_value: tensor([[1.6018]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13850368475216301, distance: 1.0621440128424047 entropy -4.203874487905263
epoch: 11, step: 73
	action: tensor([[-0.0608, -0.1352, -0.1753, -0.1093, -0.0146,  0.0510, -0.0092]],
       dtype=torch.float64)
	q_value: tensor([[1.7440]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08452321258539264, distance: 1.0949148896261 entropy -4.202187562134529
epoch: 11, step: 74
	action: tensor([[-0.0983, -0.1116, -0.1745, -0.0503, -0.1069, -0.0555,  0.0092]],
       dtype=torch.float64)
	q_value: tensor([[1.7223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06901678903555464, distance: 1.1041488331637723 entropy -4.2054305454896
epoch: 11, step: 75
	action: tensor([[-0.0461, -0.1424, -0.1753, -0.0346, -0.0367,  0.0351, -0.1062]],
       dtype=torch.float64)
	q_value: tensor([[1.7536]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12228533297584276, distance: 1.0720952481963753 entropy -4.202319046001629
epoch: 11, step: 76
	action: tensor([[-0.0228,  0.0279, -0.1754,  0.1171, -0.1027,  0.0081,  0.0376]],
       dtype=torch.float64)
	q_value: tensor([[1.7832]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32553596962032505, distance: 0.9398014050392223 entropy -4.201851490340409
epoch: 11, step: 77
	action: tensor([[-0.0359, -0.0689, -0.1765, -0.1366,  0.0596, -0.0423, -0.0300]],
       dtype=torch.float64)
	q_value: tensor([[1.6796]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13586408949522233, distance: 1.0637699548086035 entropy -4.197687296564611
epoch: 11, step: 78
	action: tensor([[-0.0220,  0.0301, -0.1756, -0.0299, -0.0390,  0.0058, -0.0316]],
       dtype=torch.float64)
	q_value: tensor([[1.6917]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2770811714237079, distance: 0.9729745068619686 entropy -4.201179716768672
epoch: 11, step: 79
	action: tensor([[ 0.0072, -0.0528, -0.1759, -0.0090,  0.0259, -0.0897,  0.0181]],
       dtype=torch.float64)
	q_value: tensor([[1.6584]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23331247426554425, distance: 1.0019957786271443 entropy -4.200231716883502
epoch: 11, step: 80
	action: tensor([[-0.0560, -0.1339, -0.1770, -0.0569,  0.0091,  0.0734,  0.0633]],
       dtype=torch.float64)
	q_value: tensor([[1.7067]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11491884430459232, distance: 1.0765847914743802 entropy -4.195690698655531
epoch: 11, step: 81
	action: tensor([[-0.0519, -0.0719, -0.1746,  0.0265, -0.1271,  0.0576,  0.0517]],
       dtype=torch.float64)
	q_value: tensor([[1.7047]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19696003163973685, distance: 1.0254754644374093 entropy -4.205077449599561
epoch: 11, step: 82
	action: tensor([[ 0.0047, -0.1082, -0.1751,  0.0458,  0.1206, -0.0286, -0.0310]],
       dtype=torch.float64)
	q_value: tensor([[1.7110]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2179730873378809, distance: 1.0119697783091777 entropy -4.202933008652911
epoch: 11, step: 83
	action: tensor([[-0.0427, -0.0841, -0.1769, -0.1024, -0.0790, -0.0308,  0.0493]],
       dtype=torch.float64)
	q_value: tensor([[1.7445]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13403305344486027, distance: 1.0648963808691525 entropy -4.19587776384971
epoch: 11, step: 84
	action: tensor([[-0.0485, -0.1613, -0.1751,  0.0271, -0.0111, -0.0464,  0.0936]],
       dtype=torch.float64)
	q_value: tensor([[1.7001]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11022169818132754, distance: 1.0794377414845766 entropy -4.203343019224365
epoch: 11, step: 85
	action: tensor([[-0.0796, -0.0854, -0.1760,  0.0039, -0.0082,  0.0450,  0.1412]],
       dtype=torch.float64)
	q_value: tensor([[1.7551]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1439882155927693, distance: 1.0587576590064294 entropy -4.198967673313644
epoch: 11, step: 86
	action: tensor([[-8.4587e-02, -5.3124e-02, -1.7488e-01, -6.2901e-02, -1.8172e-01,
          1.9034e-02,  3.2556e-05]], dtype=torch.float64)
	q_value: tensor([[1.6748]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13702304824030387, distance: 1.063056363852095 entropy -4.203792274136131
epoch: 11, step: 87
	action: tensor([[ 0.0054, -0.0819, -0.1744, -0.0340, -0.1093, -0.0782, -0.0390]],
       dtype=torch.float64)
	q_value: tensor([[1.7125]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2068274650549936, distance: 1.0191556748311525 entropy -4.206045019359304
epoch: 11, step: 88
	action: tensor([[-0.0149, -0.2098, -0.1764, -0.0447, -0.0887,  0.0886, -0.0337]],
       dtype=torch.float64)
	q_value: tensor([[1.7566]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10386556994560525, distance: 1.0832863604614154 entropy -4.198238157256715
epoch: 11, step: 89
	action: tensor([[ 0.0047, -0.1034, -0.1744,  0.0763, -0.0663, -0.0560,  0.0471]],
       dtype=torch.float64)
	q_value: tensor([[1.7969]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23358024709500635, distance: 1.0018207851212866 entropy -4.20585043325729
epoch: 11, step: 90
	action: tensor([[-0.0856, -0.0793, -0.1769,  0.0211,  0.0145, -0.0246, -0.0065]],
       dtype=torch.float64)
	q_value: tensor([[1.7530]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13771447435617168, distance: 1.0626304126041943 entropy -4.1959573286886
epoch: 11, step: 91
	action: tensor([[-0.0512, -0.0848, -0.1762,  0.0593, -0.1819,  0.1575, -0.0486]],
       dtype=torch.float64)
	q_value: tensor([[1.7285]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2149452808507959, distance: 1.0139269287192045 entropy -4.198642793136172
epoch: 11, step: 92
	action: tensor([[ 0.0243, -0.0660, -0.1745, -0.0395, -0.2027, -0.0450,  0.0355]],
       dtype=torch.float64)
	q_value: tensor([[1.7550]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24327820524102362, distance: 0.995462293998236 entropy -4.205452443006096
epoch: 11, step: 93
	action: tensor([[-0.1122, -0.1714, -0.1755, -0.0540, -0.1004,  0.1618,  0.0160]],
       dtype=torch.float64)
	q_value: tensor([[1.7244]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.039232069413532855, distance: 1.1216721642950178 entropy -4.201728568285086
epoch: 11, step: 94
	action: tensor([[-0.0314, -0.0753, -0.1730, -0.0012, -0.1754,  0.0822, -0.0843]],
       dtype=torch.float64)
	q_value: tensor([[1.7501]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21031017807880392, distance: 1.0169157261277764 entropy -4.2111971122298515
epoch: 11, step: 95
	action: tensor([[-0.0512, -0.1107, -0.1749,  0.0802,  0.0049,  0.0583, -0.0816]],
       dtype=torch.float64)
	q_value: tensor([[1.7515]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19190512598201936, distance: 1.0286979378250034 entropy -4.204136614767575
epoch: 11, step: 96
	action: tensor([[-0.0212, -0.0815, -0.1064, -0.0365,  0.1034, -0.1221,  0.1720]],
       dtype=torch.float64)
	q_value: tensor([[2.2265]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13451727289424453, distance: 1.0645986122048765 entropy -4.455330094037794
epoch: 11, step: 97
	action: tensor([[-0.0584, -0.0291, -0.1776,  0.0196,  0.0154,  0.0092,  0.0353]],
       dtype=torch.float64)
	q_value: tensor([[1.6449]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1923575093766967, distance: 1.0284099573984 entropy -4.193185459946594
epoch: 11, step: 98
	action: tensor([[-0.0204,  0.0292, -0.1760,  0.0053, -0.1516,  0.1938,  0.1498]],
       dtype=torch.float64)
	q_value: tensor([[1.6769]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29884734786668665, distance: 0.9582150413831977 entropy -4.199715534627954
epoch: 11, step: 99
	action: tensor([[-0.0972, -0.0172, -0.1737,  0.0179, -0.1202, -0.0162, -0.0627]],
       dtype=torch.float64)
	q_value: tensor([[1.6042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17518746057726275, distance: 1.0392841899786835 entropy -4.208665586723321
epoch: 11, step: 100
	action: tensor([[-0.0753, -0.0766, -0.1758,  0.0535, -0.1213,  0.0443, -0.0796]],
       dtype=torch.float64)
	q_value: tensor([[1.7284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17738261799683075, distance: 1.037900292280613 entropy -4.200468544132237
epoch: 11, step: 101
	action: tensor([[-0.0379,  0.0286, -0.1755,  0.0512, -0.0627, -0.0548, -0.0703]],
       dtype=torch.float64)
	q_value: tensor([[1.7648]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2789427283081699, distance: 0.9717209671560996 entropy -4.201258464167398
epoch: 11, step: 102
	action: tensor([[-0.0838,  0.0190, -0.1769, -0.0127, -0.1635,  0.0056, -0.0229]],
       dtype=torch.float64)
	q_value: tensor([[1.7073]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20898260062896745, distance: 1.0177701549344365 entropy -4.196022795576782
epoch: 11, step: 103
	action: tensor([[-0.0019, -0.1173, -0.1751, -0.0156, -0.2248, -0.0615,  0.0686]],
       dtype=torch.float64)
	q_value: tensor([[1.6889]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18327572921902835, distance: 1.0341759267560144 entropy -4.203300853422503
epoch: 11, step: 104
	action: tensor([[-0.0706, -0.0073, -0.1752,  0.0226, -0.0360,  0.0750,  0.0371]],
       dtype=torch.float64)
	q_value: tensor([[1.7594]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22650856936578356, distance: 1.0064320223407972 entropy -4.2026591750657625
epoch: 11, step: 105
	action: tensor([[-0.0810, -0.0351, -0.1752,  0.0091, -0.0055,  0.0783, -0.0010]],
       dtype=torch.float64)
	q_value: tensor([[1.6606]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19137937782760384, distance: 1.029032519890178 entropy -4.202733996667336
epoch: 11, step: 106
	action: tensor([[-1.0645e-01,  4.0191e-03, -1.7519e-01,  5.6540e-02,  3.7053e-02,
         -1.5904e-04,  5.9590e-02]], dtype=torch.float64)
	q_value: tensor([[1.6825]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19252547468168446, distance: 1.0283030126944503 entropy -4.202728192369346
epoch: 11, step: 107
	action: tensor([[ 0.0073, -0.1230, -0.1760,  0.0283, -0.1933,  0.0884, -0.0200]],
       dtype=torch.float64)
	q_value: tensor([[1.6600]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22576474927093648, distance: 1.0069158185712594 entropy -4.199299426913567
epoch: 11, step: 108
	action: tensor([[-0.0096, -0.0789, -0.1750,  0.0308, -0.0638,  0.0582,  0.0136]],
       dtype=torch.float64)
	q_value: tensor([[1.7678]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23962387240231597, distance: 0.9978630239136462 entropy -4.203412348157224
epoch: 11, step: 109
	action: tensor([[-0.0227, -0.0917, -0.1756,  0.0090, -0.1832,  0.0122,  0.0761]],
       dtype=torch.float64)
	q_value: tensor([[1.7164]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19961220713446814, distance: 1.023780660740448 entropy -4.201151347897357
epoch: 11, step: 110
	action: tensor([[-0.0875,  0.0372, -0.1751, -0.0422, -0.0961, -0.0053,  0.0337]],
       dtype=torch.float64)
	q_value: tensor([[1.7249]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20527194949312777, distance: 1.0201545344436775 entropy -4.203111066435817
epoch: 11, step: 111
	action: tensor([[-0.0265,  0.0570, -0.1752,  0.0633, -0.1994,  0.1025,  0.0580]],
       dtype=torch.float64)
	q_value: tensor([[1.6457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3329568473356114, distance: 0.9346169617222481 entropy -4.202941995845842
epoch: 11, step: 112
	action: tensor([[ 0.0188, -0.0751, -0.1751,  0.0499, -0.0107, -0.0996, -0.0042]],
       dtype=torch.float64)
	q_value: tensor([[1.6426]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25072497151504114, distance: 0.9905520992428352 entropy -4.203332601493946
epoch: 11, step: 113
	action: tensor([[-0.0132, -0.0632, -0.1774, -0.1350,  0.0274,  0.0079,  0.0067]],
       dtype=torch.float64)
	q_value: tensor([[1.7488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1748065102085944, distance: 1.0395241657139305 entropy -4.194033965972688
epoch: 11, step: 114
	action: tensor([[-0.1110, -0.0510, -0.1752,  0.0350, -0.0747, -0.0271,  0.0668]],
       dtype=torch.float64)
	q_value: tensor([[1.6694]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13669741587596929, distance: 1.0632569097110804 entropy -4.2029790367481485
epoch: 11, step: 115
	action: tensor([[-0.0525, -0.0630, -0.1757, -0.0464, -0.0343, -0.0824, -0.0008]],
       dtype=torch.float64)
	q_value: tensor([[1.7066]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1526676547065643, distance: 1.053376404074327 entropy -4.2005580371649955
epoch: 11, step: 116
	action: tensor([[ 0.0111, -0.0285, -0.1762, -0.0563, -0.0092,  0.0285,  0.0151]],
       dtype=torch.float64)
	q_value: tensor([[1.7220]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26008423298223715, distance: 0.9843461210051505 entropy -4.198847750579996
epoch: 11, step: 117
	action: tensor([[-0.0693, -0.0496, -0.1757,  0.0056,  0.0521, -0.0576,  0.0360]],
       dtype=torch.float64)
	q_value: tensor([[1.6609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16466211103655215, distance: 1.0458942694977678 entropy -4.201079079991599
epoch: 11, step: 118
	action: tensor([[-0.0271,  0.0299, -0.1765,  0.1466, -0.0809,  0.0347,  0.0027]],
       dtype=torch.float64)
	q_value: tensor([[1.6936]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33601065944651354, distance: 0.9324751072202989 entropy -4.197633836797014
epoch: 11, step: 119
	action: tensor([[-0.0722, -0.0985, -0.1766,  0.0046, -0.0721,  0.0758,  0.0464]],
       dtype=torch.float64)
	q_value: tensor([[1.6908]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14929631936562004, distance: 1.0554698917694305 entropy -4.197339107212968
epoch: 11, step: 120
	action: tensor([[-0.0379, -0.0259, -0.1747,  0.0529, -0.2238, -0.0064,  0.0774]],
       dtype=torch.float64)
	q_value: tensor([[1.7161]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2480541138617257, distance: 0.99231598440322 entropy -4.204436843227957
epoch: 11, step: 121
	action: tensor([[-0.1242, -0.0476, -0.1754,  0.0427,  0.0016, -0.0385,  0.1108]],
       dtype=torch.float64)
	q_value: tensor([[1.7042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12237073772541907, distance: 1.0720430875927816 entropy -4.201870581995815
epoch: 11, step: 122
	action: tensor([[-0.1059, -0.2075, -0.1759,  0.0070,  0.0177, -0.1169,  0.0531]],
       dtype=torch.float64)
	q_value: tensor([[1.6846]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.012271779238337333, distance: 1.1513444134124322 entropy -4.199906828607964
epoch: 11, step: 123
	action: tensor([[-0.0571, -0.0781, -0.1760,  0.0175,  0.0802,  0.0498,  0.0980]],
       dtype=torch.float64)
	q_value: tensor([[1.8076]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17898175408439976, distance: 1.0368909826955708 entropy -4.1990404437154885
epoch: 11, step: 124
	action: tensor([[-0.0399, -0.0662, -0.1753,  0.0900, -0.2247,  0.1704, -0.0225]],
       dtype=torch.float64)
	q_value: tensor([[1.6713]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2515204379277689, distance: 0.9900261507340911 entropy -4.2020173350418375
epoch: 11, step: 125
	action: tensor([[-0.0369, -0.1194, -0.1746,  0.0465, -0.0503,  0.0363,  0.0863]],
       dtype=torch.float64)
	q_value: tensor([[1.7479]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18028613750485156, distance: 1.0360669808940646 entropy -4.204891798364051
epoch: 11, step: 126
	action: tensor([[-0.0608, -0.1314, -0.1756, -0.0205, -0.0045,  0.1097,  0.0880]],
       dtype=torch.float64)
	q_value: tensor([[1.7249]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13302262107635254, distance: 1.065517473903848 entropy -4.200837091734048
epoch: 11, step: 127
	action: tensor([[-0.0353, -0.1780, -0.1743,  0.0676, -0.0079,  0.0466, -0.0932]],
       dtype=torch.float64)
	q_value: tensor([[1.7021]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14932055237592712, distance: 1.0554548586882895 entropy -4.206128453270762
LOSS epoch 11 actor 1.3002377743989282 critic 1.8259800346547208 entropy 0.01
epoch: 12, step: 0
	action: tensor([[-0.0727, -0.0653, -0.1996, -0.0926, -0.0996,  0.0238,  0.0965]],
       dtype=torch.float64)
	q_value: tensor([[2.5984]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11507542345081989, distance: 1.0764895582957612 entropy -4.122689335937187
epoch: 12, step: 1
	action: tensor([[-0.0988, -0.2071, -0.3065,  0.1540, -0.1471, -0.0690, -0.0055]],
       dtype=torch.float64)
	q_value: tensor([[1.9145]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06280196248771486, distance: 1.1078281043907838 entropy -3.828886130020471
epoch: 12, step: 2
	action: tensor([[-0.1460, -0.2501, -0.3058, -0.0268, -0.2835,  0.1895,  0.0980]],
       dtype=torch.float64)
	q_value: tensor([[2.1904]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06563652593037772, distance: 1.1813028242825838 entropy -3.830404152711934
epoch: 12, step: 3
	action: tensor([[-0.1648,  0.0848, -0.2965, -0.0129, -0.2377, -0.0871,  0.2397]],
       dtype=torch.float64)
	q_value: tensor([[2.1152]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1578979039935121, distance: 1.0501203325750716 entropy -3.8541778698999485
epoch: 12, step: 4
	action: tensor([[-0.0952, -0.2707, -0.3003,  0.2182, -0.0878,  0.0550,  0.1211]],
       dtype=torch.float64)
	q_value: tensor([[1.9062]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.051188939111694975, distance: 1.1146706415124834 entropy -3.8443869988607036
epoch: 12, step: 5
	action: tensor([[-0.0725, -0.2618, -0.3048,  0.1161, -0.2261, -0.0224,  0.0462]],
       dtype=torch.float64)
	q_value: tensor([[2.1661]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04091811392379363, distance: 1.1206875250434447 entropy -3.8329924469920718
epoch: 12, step: 6
	action: tensor([[-0.1408, -0.0092, -0.3036, -0.0526, -0.1634,  0.2951,  0.1232]],
       dtype=torch.float64)
	q_value: tensor([[2.2027]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13886940123072944, distance: 1.0619185418828483 entropy -3.8358189810029506
epoch: 12, step: 7
	action: tensor([[-0.0397, -0.0619, -0.2954,  0.0549,  0.1470,  0.0008, -0.0286]],
       dtype=torch.float64)
	q_value: tensor([[1.9061]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20907910129778629, distance: 1.0177080712834439 entropy -3.857245480036512
epoch: 12, step: 8
	action: tensor([[-0.0999, -0.0895, -0.3073,  0.0900, -0.2503,  0.0531,  0.0817]],
       dtype=torch.float64)
	q_value: tensor([[1.9992]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14173652013197913, distance: 1.0601492485670234 entropy -3.8270255715716632
epoch: 12, step: 9
	action: tensor([[-0.0373, -0.2510, -0.3012,  0.1582, -0.0764,  0.0377,  0.1323]],
       dtype=torch.float64)
	q_value: tensor([[2.0514]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10752558939994183, distance: 1.0810719015317793 entropy -3.8420493226359946
epoch: 12, step: 10
	action: tensor([[-1.6423e-01, -3.2173e-02, -3.0465e-01, -7.3603e-02, -5.8387e-05,
          2.8598e-02,  1.1346e-01]], dtype=torch.float64)
	q_value: tensor([[2.1340]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06584572983064207, distance: 1.1060276774055444 entropy -3.8334079548772713
epoch: 12, step: 11
	action: tensor([[-0.0920, -0.0378, -0.3001, -0.0770, -0.1443,  0.0015,  0.0528]],
       dtype=torch.float64)
	q_value: tensor([[1.9287]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14094640601723296, distance: 1.0606371210045455 entropy -3.8450112825634366
epoch: 12, step: 12
	action: tensor([[-0.1390, -0.1046, -0.3013,  0.0058, -0.2001,  0.0973, -0.1314]],
       dtype=torch.float64)
	q_value: tensor([[1.9705]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0667781066763885, distance: 1.105475577995032 entropy -3.8419439429344244
epoch: 12, step: 13
	action: tensor([[ 0.0380, -0.1531, -0.3012,  0.1140, -0.2401, -0.1359,  0.1264]],
       dtype=torch.float64)
	q_value: tensor([[2.0955]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23666921276017672, distance: 0.9997998857434623 entropy -3.8421141879351492
epoch: 12, step: 14
	action: tensor([[-0.0419,  0.0076, -0.3060,  0.0614, -0.1348,  0.0865, -0.0018]],
       dtype=torch.float64)
	q_value: tensor([[2.1222]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27408830526612127, distance: 0.9749864718969469 entropy -3.829696449153229
epoch: 12, step: 15
	action: tensor([[-0.0322, -0.0649, -0.3033,  0.0696, -0.1518,  0.0574,  0.0668]],
       dtype=torch.float64)
	q_value: tensor([[1.9713]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23166841714869302, distance: 1.0030695251392623 entropy -3.8367604282653405
epoch: 12, step: 16
	action: tensor([[-0.1061,  0.0053, -0.3031,  0.1458, -0.2116, -0.1425,  0.0468]],
       dtype=torch.float64)
	q_value: tensor([[2.0091]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21110488464203692, distance: 1.0164039093342432 entropy -3.837161546419268
epoch: 12, step: 17
	action: tensor([[-0.0067,  0.0679, -0.3060, -0.0452, -0.1334, -0.0826, -0.1391]],
       dtype=torch.float64)
	q_value: tensor([[2.0493]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31493255252118535, distance: 0.9471600222063196 entropy -3.829918696041576
epoch: 12, step: 18
	action: tensor([[-0.1434,  0.0499, -0.3051, -0.1073, -0.0065, -0.1146, -0.1234]],
       dtype=torch.float64)
	q_value: tensor([[1.9947]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13053942525162654, distance: 1.0670423099186883 entropy -3.832254875140951
epoch: 12, step: 19
	action: tensor([[-0.1478, -0.0906, -0.3035,  0.0268, -0.2251,  0.2420,  0.0491]],
       dtype=torch.float64)
	q_value: tensor([[1.9844]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08255272723052232, distance: 1.0960926114789986 entropy -3.8365410900495247
epoch: 12, step: 20
	action: tensor([[-0.0667,  0.0603, -0.2978, -0.0731,  0.1125, -0.1630,  0.1050]],
       dtype=torch.float64)
	q_value: tensor([[2.0161]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20865092568825327, distance: 1.0179835089502172 entropy -3.8509194644360707
epoch: 12, step: 21
	action: tensor([[-0.1957, -0.0450, -0.3060, -0.1545, -0.0022, -0.0834,  0.0424]],
       dtype=torch.float64)
	q_value: tensor([[1.9013]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.011944406656875683, distance: 1.151158223754842 entropy -3.8301857951413303
epoch: 12, step: 22
	action: tensor([[-0.1142, -0.1274, -0.3005,  0.0151, -0.2126,  0.0218, -0.1263]],
       dtype=torch.float64)
	q_value: tensor([[1.9775]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07442910546034176, distance: 1.1009346432731146 entropy -3.8441688912461225
epoch: 12, step: 23
	action: tensor([[ 0.0315, -0.0441, -0.3028, -0.0576,  0.0226, -0.1187, -0.2008]],
       dtype=torch.float64)
	q_value: tensor([[2.1313]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.254830597817134, distance: 0.9878345236294553 entropy -3.838102095748686
epoch: 12, step: 24
	action: tensor([[-0.0359, -0.0886, -0.3075,  0.1123, -0.0757, -0.1290,  0.3213]],
       dtype=torch.float64)
	q_value: tensor([[2.0704]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19900165129643277, distance: 1.0241710690553523 entropy -3.8264528059556655
epoch: 12, step: 25
	action: tensor([[ 0.0117, -0.0551, -0.3043,  0.0591, -0.1171, -0.1372,  0.0330]],
       dtype=torch.float64)
	q_value: tensor([[1.9850]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26523731677566686, distance: 0.9809124324897814 entropy -3.8341292797144786
epoch: 12, step: 26
	action: tensor([[-0.0588, -0.0235, -0.3070,  0.1754, -0.3166, -0.0106,  0.0509]],
       dtype=torch.float64)
	q_value: tensor([[2.0536]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2567134413655119, distance: 0.9865857376271512 entropy -3.8273893494654017
epoch: 12, step: 27
	action: tensor([[-0.1469, -0.1591, -0.3043, -0.0394, -0.2393, -0.1592,  0.1194]],
       dtype=torch.float64)
	q_value: tensor([[2.0601]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.009591198576988758, distance: 1.1498189745338705 entropy -3.8340123628454945
epoch: 12, step: 28
	action: tensor([[-0.0227, -0.0389, -0.3013,  0.0915, -0.0124, -0.0768, -0.0048]],
       dtype=torch.float64)
	q_value: tensor([[2.1152]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24947748387106983, distance: 0.9913763543876885 entropy -3.8417497052403076
epoch: 12, step: 29
	action: tensor([[-0.3431, -0.0910, -0.3080,  0.1837, -0.0371, -0.0973,  0.1845]],
       dtype=torch.float64)
	q_value: tensor([[2.0284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14713419624639057, distance: 1.225642484279966 entropy -3.825185289616791
epoch: 12, step: 30
	action: tensor([[-0.0924, -0.2506, -0.3023, -0.0923, -0.0034,  0.2468,  0.0652]],
       dtype=torch.float64)
	q_value: tensor([[2.0790]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.011196750873941674, distance: 1.1507328895524058 entropy -3.839412479443801
epoch: 12, step: 31
	action: tensor([[-0.1223,  0.0778, -0.2975,  0.1174, -0.0261,  0.0426,  0.1093]],
       dtype=torch.float64)
	q_value: tensor([[2.0508]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23116400129130432, distance: 1.003398732717537 entropy -3.8516070580739727
epoch: 12, step: 32
	action: tensor([[-0.0252, -0.0739, -0.1996,  0.0810, -0.1589, -0.0727, -0.0569]],
       dtype=torch.float64)
	q_value: tensor([[2.5984]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21317381764542176, distance: 1.0150702389293178 entropy -4.122689335937187
epoch: 12, step: 33
	action: tensor([[-0.0706, -0.1923, -0.3123, -0.0106, -0.0762,  0.1876, -0.0971]],
       dtype=torch.float64)
	q_value: tensor([[2.0498]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08725866551834105, distance: 1.0932778580411178 entropy -3.814508703417992
epoch: 12, step: 34
	action: tensor([[-0.0082, -0.1344, -0.3004,  0.0572, -0.1540, -0.0924, -0.0235]],
       dtype=torch.float64)
	q_value: tensor([[2.1059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1870648621483928, distance: 1.0317741457492857 entropy -3.844322701369965
epoch: 12, step: 35
	action: tensor([[ 0.0254, -0.0227, -0.3065,  0.1298, -0.2216,  0.1429,  0.0530]],
       dtype=torch.float64)
	q_value: tensor([[2.1226]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3439246178133085, distance: 0.926901455914403 entropy -3.828753756998388
epoch: 12, step: 36
	action: tensor([[-0.1033, -0.0041, -0.3039,  0.0121, -0.1642, -0.2108, -0.0273]],
       dtype=torch.float64)
	q_value: tensor([[1.9979]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16876847869539724, distance: 1.0433203903422112 entropy -3.8352170399478274
epoch: 12, step: 37
	action: tensor([[-0.1156, -0.0160, -0.3059,  0.1229, -0.0610,  0.0165,  0.0779]],
       dtype=torch.float64)
	q_value: tensor([[2.0634]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18391681906437374, distance: 1.0337699575289514 entropy -3.8302511532349888
epoch: 12, step: 38
	action: tensor([[-0.0290, -0.0337, -0.3044,  0.1045, -0.2848, -0.0586,  0.1586]],
       dtype=torch.float64)
	q_value: tensor([[1.9854]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26135038160626145, distance: 0.9835035506881857 entropy -3.834167914113827
epoch: 12, step: 39
	action: tensor([[-0.0311, -0.0183, -0.3034, -0.2030, -0.0356,  0.1412, -0.0155]],
       dtype=torch.float64)
	q_value: tensor([[2.0129]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20572105415440567, distance: 1.0198662465814272 entropy -3.836446685741375
epoch: 12, step: 40
	action: tensor([[ 0.0546, -0.0759, -0.2988, -0.1133, -0.2681,  0.0367, -0.0058]],
       dtype=torch.float64)
	q_value: tensor([[1.8956]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25517617879548604, distance: 0.9876054371904026 entropy -3.8484980174715124
epoch: 12, step: 41
	action: tensor([[ 0.0096, -0.1454, -0.3017,  0.1521,  0.0443,  0.1087, -0.0726]],
       dtype=torch.float64)
	q_value: tensor([[2.0172]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2544882079812306, distance: 0.988061442271443 entropy -3.840729159464054
epoch: 12, step: 42
	action: tensor([[ 0.0065, -0.2687, -0.3071,  0.1139, -0.2860, -0.1368, -0.0613]],
       dtype=torch.float64)
	q_value: tensor([[2.0935]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1139593797883316, distance: 1.0771681645992166 entropy -3.827327396270433
epoch: 12, step: 43
	action: tensor([[ 0.0009, -0.0141, -0.3063, -0.0118, -0.0490, -0.0772, -0.0331]],
       dtype=torch.float64)
	q_value: tensor([[2.2784]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2676137733651247, distance: 0.9793248561211954 entropy -3.829192795379749
epoch: 12, step: 44
	action: tensor([[-0.0333, -0.1995, -0.3061, -0.0267, -0.1516,  0.0079,  0.0136]],
       dtype=torch.float64)
	q_value: tensor([[2.0032]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08950779152099031, distance: 1.0919300302295198 entropy -3.8298561700917793
epoch: 12, step: 45
	action: tensor([[-0.0856, -0.0758, -0.3025, -0.0423, -0.3594, -0.0423,  0.1383]],
       dtype=torch.float64)
	q_value: tensor([[2.1135]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12894738300130004, distance: 1.0680187771424734 entropy -3.838794366986808
epoch: 12, step: 46
	action: tensor([[-0.0270, -0.2487, -0.2997, -0.1144,  0.0860, -0.0800, -0.0944]],
       dtype=torch.float64)
	q_value: tensor([[2.0288]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.006804343504830168, distance: 1.140444352913671 entropy -3.8459273385810078
epoch: 12, step: 47
	action: tensor([[ 0.1355, -0.0475, -0.3057,  0.0894,  0.0976, -0.0548, -0.0257]],
       dtype=torch.float64)
	q_value: tensor([[2.1456]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3924053110137411, distance: 0.8919975833843844 entropy -3.831068858607256
epoch: 12, step: 48
	action: tensor([[-0.1678, -0.0476, -0.3098, -0.0362, -0.0059,  0.1373, -0.0571]],
       dtype=torch.float64)
	q_value: tensor([[2.0155]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07674140534712337, distance: 1.099558582836743 entropy -3.8207981327364506
epoch: 12, step: 49
	action: tensor([[-0.1838, -0.1183, -0.2998, -0.0571, -0.1082,  0.1456, -0.0137]],
       dtype=torch.float64)
	q_value: tensor([[1.9824]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.006017577379463734, distance: 1.1477821798098473 entropy -3.8457398141956687
epoch: 12, step: 50
	action: tensor([[-0.1021,  0.0075, -0.2985,  0.2054, -0.2098, -0.1480,  0.0540]],
       dtype=torch.float64)
	q_value: tensor([[2.0274]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22818852180388471, distance: 1.005338489236352 entropy -3.849225616133986
epoch: 12, step: 51
	action: tensor([[-0.0676, -0.1502, -0.3075,  0.1168,  0.2635,  0.0128, -0.0059]],
       dtype=torch.float64)
	q_value: tensor([[2.0568]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13445755665489945, distance: 1.064635338951101 entropy -3.8262103166209953
epoch: 12, step: 52
	action: tensor([[ 0.0876,  0.0554, -0.3074,  0.0502, -0.1970, -0.2264,  0.0816]],
       dtype=torch.float64)
	q_value: tensor([[2.0584]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.40214332269931774, distance: 0.8848206207885239 entropy -3.826870495139731
epoch: 12, step: 53
	action: tensor([[-0.0375, -0.0022, -0.3076, -0.0068, -0.0351,  0.0564,  0.0141]],
       dtype=torch.float64)
	q_value: tensor([[1.9889]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25249856754468725, distance: 0.9893790452360709 entropy -3.826014061135386
epoch: 12, step: 54
	action: tensor([[-0.0139, -0.2438, -0.3033,  0.0291, -0.2439, -0.0592, -0.0786]],
       dtype=torch.float64)
	q_value: tensor([[1.9435]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08688538661545553, distance: 1.0935013911649876 entropy -3.836924351367084
epoch: 12, step: 55
	action: tensor([[-0.0404, -0.0255, -0.3047,  0.1334, -0.0909, -0.1189,  0.1278]],
       dtype=torch.float64)
	q_value: tensor([[2.2225]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25170408843210457, distance: 0.9899046845208157 entropy -3.833193590681283
epoch: 12, step: 56
	action: tensor([[-0.0278, -0.0782, -0.3068, -0.0824, -0.0407, -0.0514,  0.0541]],
       dtype=torch.float64)
	q_value: tensor([[2.0081]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16940553977626982, distance: 1.0429205100459096 entropy -3.827934089275763
epoch: 12, step: 57
	action: tensor([[-0.1483,  0.0149, -0.3035,  0.1030,  0.1266,  0.0581,  0.1010]],
       dtype=torch.float64)
	q_value: tensor([[1.9935]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16819613406469192, distance: 1.043679517765543 entropy -3.8363860650777526
epoch: 12, step: 58
	action: tensor([[-0.2297,  0.0509, -0.3040,  0.1194,  0.1992, -0.0038,  0.3118]],
       dtype=torch.float64)
	q_value: tensor([[1.9230]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08228655335010959, distance: 1.0962516015781447 entropy -3.835348414909327
epoch: 12, step: 59
	action: tensor([[-0.0823, -0.1189, -0.3022, -0.1190, -0.1566,  0.0402,  0.1916]],
       dtype=torch.float64)
	q_value: tensor([[1.8843]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05624850109120172, distance: 1.1116946623796868 entropy -3.840048064753708
epoch: 12, step: 60
	action: tensor([[ 0.0198,  0.0730, -0.2984,  0.0766, -0.1602,  0.0306,  0.0089]],
       dtype=torch.float64)
	q_value: tensor([[1.9710]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3578752981660367, distance: 0.9169937630232133 entropy -3.8493465305174404
epoch: 12, step: 61
	action: tensor([[-0.0106, -0.1994, -0.3056,  0.1145, -0.0579, -0.1542,  0.1248]],
       dtype=torch.float64)
	q_value: tensor([[1.9348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13514469544609675, distance: 1.0642126574900437 entropy -3.8310501062637328
epoch: 12, step: 62
	action: tensor([[-0.0145,  0.0019, -0.3073,  0.0834, -0.1292,  0.1287,  0.0404]],
       dtype=torch.float64)
	q_value: tensor([[2.1353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.308919994187526, distance: 0.9513073613361309 entropy -3.826658983926346
epoch: 12, step: 63
	action: tensor([[-0.0367,  0.0778, -0.3032, -0.0107,  0.1716,  0.1829,  0.0069]],
       dtype=torch.float64)
	q_value: tensor([[1.9594]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.334525435442128, distance: 0.9335174137237585 entropy -3.8370538069616247
epoch: 12, step: 64
	action: tensor([[ 0.0507, -0.0672, -0.1996,  0.1400, -0.0072, -0.0855,  0.0783]],
       dtype=torch.float64)
	q_value: tensor([[2.5984]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3010597275957553, distance: 0.9567020966245673 entropy -4.122689335937187
epoch: 12, step: 65
	action: tensor([[ 0.0536, -0.2135, -0.3146,  0.1895, -0.0486, -0.1488,  0.1041]],
       dtype=torch.float64)
	q_value: tensor([[1.9855]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19919846834471966, distance: 1.0240452343947424 entropy -3.8090308332846337
epoch: 12, step: 66
	action: tensor([[-0.0565, -0.0051, -0.3087, -0.0180,  0.0602, -0.0833,  0.3246]],
       dtype=torch.float64)
	q_value: tensor([[2.1709]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19637318590501263, distance: 1.0258500946059521 entropy -3.823142759919996
epoch: 12, step: 67
	action: tensor([[-0.0581, -0.1142, -0.3025,  0.1537, -0.0914,  0.2214,  0.1072]],
       dtype=torch.float64)
	q_value: tensor([[1.8788]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21338486087137432, distance: 1.0149340982737691 entropy -3.8389333880259033
epoch: 12, step: 68
	action: tensor([[-0.0956, -0.3318, -0.3024,  0.2229, -0.2467,  0.0144,  0.0237]],
       dtype=torch.float64)
	q_value: tensor([[2.0194]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0010013498797185605, distance: 1.1449170551328423 entropy -3.8393057555129153
epoch: 12, step: 69
	action: tensor([[-0.0407, -0.2137, -0.3050, -0.1404,  0.1692,  0.1118, -0.0408]],
       dtype=torch.float64)
	q_value: tensor([[2.2773]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05210723881124546, distance: 1.1141310981046078 entropy -3.83235598449412
epoch: 12, step: 70
	action: tensor([[-0.1598, -0.3121, -0.3018, -0.0330, -0.1507, -0.0033, -0.0280]],
       dtype=torch.float64)
	q_value: tensor([[2.0383]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14599618761516697, distance: 1.2250343872385878 entropy -3.840953727495514
epoch: 12, step: 71
	action: tensor([[-0.1043, -0.0536, -0.3011,  0.0445, -0.2059, -0.0140,  0.0874]],
       dtype=torch.float64)
	q_value: tensor([[2.2138]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14347561746892612, distance: 1.0590746149036228 entropy -3.8424687048358943
epoch: 12, step: 72
	action: tensor([[-0.1321,  0.0288, -0.3023,  0.0726,  0.0495,  0.0164, -0.0615]],
       dtype=torch.float64)
	q_value: tensor([[2.0155]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.184772287083939, distance: 1.0332279851910187 entropy -3.8392460223136324
epoch: 12, step: 73
	action: tensor([[-0.1720,  0.0215, -0.3057,  0.0518, -0.0470,  0.0305, -0.1245]],
       dtype=torch.float64)
	q_value: tensor([[1.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12969284764100797, distance: 1.067561663166242 entropy -3.8307552669461304
epoch: 12, step: 74
	action: tensor([[-0.0662, -0.1109, -0.3039, -0.1110, -0.0341,  0.1528,  0.0404]],
       dtype=torch.float64)
	q_value: tensor([[2.0089]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12209096538790809, distance: 1.0722139479516617 entropy -3.8353963678227343
epoch: 12, step: 75
	action: tensor([[-0.0003, -0.3018, -0.2991, -0.0854, -0.2139, -0.0486, -0.0451]],
       dtype=torch.float64)
	q_value: tensor([[1.9668]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.008841335921338511, distance: 1.1392742567532048 entropy -3.847544191535509
epoch: 12, step: 76
	action: tensor([[-0.1382, -0.1028, -0.3028,  0.1494, -0.1424,  0.1813,  0.0567]],
       dtype=torch.float64)
	q_value: tensor([[2.2204]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12023548155366459, distance: 1.0733464260763934 entropy -3.83820951502381
epoch: 12, step: 77
	action: tensor([[-0.0440, -0.2828, -0.3018,  0.0273, -0.0494, -0.1476, -0.0822]],
       dtype=torch.float64)
	q_value: tensor([[2.0473]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.003918536624245239, distance: 1.1420999758354065 entropy -3.8406136764260417
epoch: 12, step: 78
	action: tensor([[-0.0567,  0.0060, -0.3073,  0.0327, -0.2878, -0.2009, -0.0229]],
       dtype=torch.float64)
	q_value: tensor([[2.2404]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.240299441833843, distance: 0.9974196410057768 entropy -3.826841933084144
epoch: 12, step: 79
	action: tensor([[-0.0848, -0.1077, -0.3052,  0.0094,  0.0142,  0.2048, -0.0310]],
       dtype=torch.float64)
	q_value: tensor([[2.0761]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15174828499055582, distance: 1.0539477146227985 entropy -3.8318023979483553
epoch: 12, step: 80
	action: tensor([[-0.0478, -0.0161, -0.3008,  0.1310, -0.1085, -0.1150, -0.0092]],
       dtype=torch.float64)
	q_value: tensor([[2.0055]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25242980375545887, distance: 0.9894245514281841 entropy -3.8433157135157474
epoch: 12, step: 81
	action: tensor([[-0.0501,  0.0027, -0.3080,  0.0791, -0.2069,  0.1739, -0.0705]],
       dtype=torch.float64)
	q_value: tensor([[2.0515]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2701342820711956, distance: 0.9776382294844638 entropy -3.8250435880098244
epoch: 12, step: 82
	action: tensor([[-0.0601, -0.0510, -0.3024,  0.0197, -0.0430,  0.1106, -0.0445]],
       dtype=torch.float64)
	q_value: tensor([[2.0068]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20693454245438037, distance: 1.0190868800748232 entropy -3.83904796101305
epoch: 12, step: 83
	action: tensor([[ 0.0277,  0.0827, -0.3029,  0.0323,  0.0057, -0.0428, -0.0211]],
       dtype=torch.float64)
	q_value: tensor([[1.9950]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37651323966786543, distance: 0.9035877013173471 entropy -3.8378742170150173
epoch: 12, step: 84
	action: tensor([[-0.0515,  0.0077, -0.3073,  0.0973,  0.0518,  0.1024,  0.0043]],
       dtype=torch.float64)
	q_value: tensor([[1.9191]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27915221543923996, distance: 0.9715798009596494 entropy -3.8269854542389843
epoch: 12, step: 85
	action: tensor([[ 0.0184, -0.1703, -0.3049,  0.0059,  0.0312,  0.1290,  0.1124]],
       dtype=torch.float64)
	q_value: tensor([[1.9474]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1970683034994214, distance: 1.0254063309679053 entropy -3.8330225902452875
epoch: 12, step: 86
	action: tensor([[-0.0717, -0.0534, -0.3025,  0.0189,  0.1959, -0.0429,  0.0575]],
       dtype=torch.float64)
	q_value: tensor([[2.0041]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16417013926678625, distance: 1.046202213578123 entropy -3.8389957173980416
epoch: 12, step: 87
	action: tensor([[-0.0327,  0.0447, -0.3063,  0.1305, -0.1530,  0.0848,  0.1143]],
       dtype=torch.float64)
	q_value: tensor([[1.9638]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3239676006551253, distance: 0.9408934569633076 entropy -3.8295048778965275
epoch: 12, step: 88
	action: tensor([[-0.1560, -0.1589, -0.3035, -0.0579, -0.1812,  0.0980,  0.1928]],
       dtype=torch.float64)
	q_value: tensor([[1.9318]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.01604298966838602, distance: 1.1534870818531557 entropy -3.8363462203908267
epoch: 12, step: 89
	action: tensor([[-0.2007, -0.2067, -0.2971,  0.0052, -0.2416,  0.0062, -0.2327]],
       dtype=torch.float64)
	q_value: tensor([[2.0162]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0937069375636117, distance: 1.1967603076523257 entropy -3.852583291865045
epoch: 12, step: 90
	action: tensor([[-0.1593, -0.1749, -0.3021,  0.1943, -0.1446,  0.1054,  0.0366]],
       dtype=torch.float64)
	q_value: tensor([[2.2320]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04789010914858194, distance: 1.1166067057407552 entropy -3.8396698563104867
epoch: 12, step: 91
	action: tensor([[-0.2456, -0.0875, -0.3036, -0.1060,  0.1189, -0.1174,  0.1380]],
       dtype=torch.float64)
	q_value: tensor([[2.1281]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10498961793313044, distance: 1.2029173588090112 entropy -3.836142529586183
epoch: 12, step: 92
	action: tensor([[-0.1893, -0.0035, -0.3011, -0.1126, -0.1931,  0.1747,  0.0276]],
       dtype=torch.float64)
	q_value: tensor([[1.9923]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05773748347447638, distance: 1.1108173407617477 entropy -3.842458005416593
epoch: 12, step: 93
	action: tensor([[-0.0579, -0.1640, -0.2957,  0.2042, -0.0356,  0.1361,  0.1405]],
       dtype=torch.float64)
	q_value: tensor([[1.9391]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18253585765138325, distance: 1.0346442513673335 entropy -3.856311979471073
epoch: 12, step: 94
	action: tensor([[ 0.0328,  0.0626, -0.3047, -0.0473, -0.3294, -0.0058, -0.1315]],
       dtype=torch.float64)
	q_value: tensor([[2.0574]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35653668816969863, distance: 0.9179490743143996 entropy -3.8333400247768656
epoch: 12, step: 95
	action: tensor([[-0.1069,  0.0610, -0.3034, -0.0168, -0.1563, -0.1870,  0.0974]],
       dtype=torch.float64)
	q_value: tensor([[2.0065]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2051293472472996, distance: 1.0202460561927211 entropy -3.836481109949992
epoch: 12, step: 96
	action: tensor([[-0.1482, -0.1048, -0.1996, -0.0069, -0.1086,  0.0019,  0.0842]],
       dtype=torch.float64)
	q_value: tensor([[2.5984]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0267893324538091, distance: 1.1289120886887407 entropy -4.122689335937187
epoch: 12, step: 97
	action: tensor([[-0.0733, -0.1792, -0.3075,  0.0386,  0.0014,  0.1865,  0.1120]],
       dtype=torch.float64)
	q_value: tensor([[1.9838]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10112196425414222, distance: 1.0849433882165371 entropy -3.826339499510628
epoch: 12, step: 98
	action: tensor([[-0.1396, -0.0259, -0.3007,  0.1267, -0.0070, -0.0023,  0.0588]],
       dtype=torch.float64)
	q_value: tensor([[2.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14695564418849894, distance: 1.0569209347051247 entropy -3.8434975733406973
epoch: 12, step: 99
	action: tensor([[-0.0382,  0.0050, -0.3053, -0.0848, -0.0563,  0.2037, -0.0785]],
       dtype=torch.float64)
	q_value: tensor([[1.9958]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2560777304691232, distance: 0.9870075461819104 entropy -3.831890004335827
epoch: 12, step: 100
	action: tensor([[-0.0504, -0.0962, -0.2997,  0.0618, -0.1416, -0.0210,  0.2566]],
       dtype=torch.float64)
	q_value: tensor([[1.9272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1764443757515849, distance: 1.0384920159731175 entropy -3.8460700171367828
epoch: 12, step: 101
	action: tensor([[-0.1513, -0.0030, -0.3023,  0.1134, -0.2484,  0.0802,  0.1644]],
       dtype=torch.float64)
	q_value: tensor([[1.9828]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15124767035214615, distance: 1.054258674106151 entropy -3.839321186326001
epoch: 12, step: 102
	action: tensor([[-0.0930, -0.2096, -0.3003,  0.0781,  0.2723,  0.0852,  0.2449]],
       dtype=torch.float64)
	q_value: tensor([[1.9793]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.054032831134042, distance: 1.1129988745917696 entropy -3.8442660704327416
epoch: 12, step: 103
	action: tensor([[-0.0120, -0.1559, -0.3037,  0.1354, -0.1371, -0.1043,  0.1270]],
       dtype=torch.float64)
	q_value: tensor([[1.9998]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1686688767658875, distance: 1.0433828961595166 entropy -3.8360619623088246
epoch: 12, step: 104
	action: tensor([[-0.0384, -0.2057, -0.3063, -0.0304, -0.0675, -0.1367, -0.0774]],
       dtype=torch.float64)
	q_value: tensor([[2.1065]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06073386426163452, distance: 1.1090497430986868 entropy -3.829125718792613
epoch: 12, step: 105
	action: tensor([[-0.0355, -0.1094, -0.3061,  0.0174, -0.1549,  0.0405,  0.1104]],
       dtype=torch.float64)
	q_value: tensor([[2.1749]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17662931949420446, distance: 1.0383754037016328 entropy -3.82972542636508
epoch: 12, step: 106
	action: tensor([[-0.1015, -0.0620, -0.3019,  0.0816,  0.0453, -0.0567,  0.0060]],
       dtype=torch.float64)
	q_value: tensor([[2.0176]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14522090935730658, distance: 1.0579950560928593 entropy -3.8403086244003366
epoch: 12, step: 107
	action: tensor([[ 0.0411, -0.0558, -0.3070, -0.0928, -0.2501,  0.0902,  0.1396]],
       dtype=torch.float64)
	q_value: tensor([[2.0288]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2661472586574334, distance: 0.9803048554852797 entropy -3.827740106130317
epoch: 12, step: 108
	action: tensor([[-0.0089, -0.0395, -0.2994,  0.2278, -0.0495, -0.0652,  0.0017]],
       dtype=torch.float64)
	q_value: tensor([[1.9479]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3067684743778194, distance: 0.9527870496959572 entropy -3.8465585961992197
epoch: 12, step: 109
	action: tensor([[-0.1533, -0.0459, -0.3098,  0.1923, -0.2017,  0.0693,  0.0879]],
       dtype=torch.float64)
	q_value: tensor([[2.0640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1399959132379348, distance: 1.061223725395402 entropy -3.820481406555415
epoch: 12, step: 110
	action: tensor([[-0.1043, -0.0625, -0.3027, -0.1053, -0.0832,  0.1018,  0.0061]],
       dtype=torch.float64)
	q_value: tensor([[2.0448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11142024990798838, distance: 1.0787104828283138 entropy -3.838347407583215
epoch: 12, step: 111
	action: tensor([[-0.1835,  0.0136, -0.2995, -0.1479, -0.1366,  0.1107,  0.1762]],
       dtype=torch.float64)
	q_value: tensor([[1.9652]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06508320846118909, distance: 1.1064789934954946 entropy -3.8465549339619005
epoch: 12, step: 112
	action: tensor([[-5.2668e-02, -2.3786e-01, -2.9566e-01,  1.4369e-01, -6.9149e-02,
          1.6356e-04,  1.7589e-01]], dtype=torch.float64)
	q_value: tensor([[1.8813]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08975457118467722, distance: 1.0917820419301094 entropy -3.8564854202699634
epoch: 12, step: 113
	action: tensor([[-0.1383, -0.0675, -0.3049,  0.0757,  0.0024, -0.1480, -0.0062]],
       dtype=torch.float64)
	q_value: tensor([[2.1105]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08793483740028463, distance: 1.0928728250486601 entropy -3.832931804096693
epoch: 12, step: 114
	action: tensor([[-0.1060, -0.0557, -0.3073, -0.1361, -0.2557, -0.1078, -0.0088]],
       dtype=torch.float64)
	q_value: tensor([[2.0702]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09482193128457939, distance: 1.0887388109967797 entropy -3.8268731335983195
epoch: 12, step: 115
	action: tensor([[-0.0256, -0.1836, -0.3008,  0.0436,  0.2784,  0.0247,  0.2187]],
       dtype=torch.float64)
	q_value: tensor([[2.0474]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12674646074111118, distance: 1.069367228012998 entropy -3.843017547632224
epoch: 12, step: 116
	action: tensor([[-0.0667, -0.1760, -0.3052, -0.0277, -0.1050, -0.2076, -0.0744]],
       dtype=torch.float64)
	q_value: tensor([[1.9853]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05276641942518123, distance: 1.1137436379526533 entropy -3.8321734788784574
epoch: 12, step: 117
	action: tensor([[-0.1327, -0.0849, -0.3065,  0.0573, -0.0617,  0.1364, -0.1120]],
       dtype=torch.float64)
	q_value: tensor([[2.1811]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1131396151870604, distance: 1.0776663472794246 entropy -3.8287518397987768
epoch: 12, step: 118
	action: tensor([[-0.0925,  0.0811, -0.3025,  0.0154,  0.1354, -0.0431, -0.0004]],
       dtype=torch.float64)
	q_value: tensor([[2.0569]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24125195639308572, distance: 0.9967941612017546 entropy -3.838906567714314
epoch: 12, step: 119
	action: tensor([[-0.0093,  0.0475, -0.3061, -0.0356, -0.2189,  0.0914,  0.0442]],
       dtype=torch.float64)
	q_value: tensor([[1.9007]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31002880744437566, distance: 0.95054388558032 entropy -3.8300119168057773
epoch: 12, step: 120
	action: tensor([[-0.0475, -0.0870, -0.3011,  0.1337, -0.1144, -0.0950,  0.1169]],
       dtype=torch.float64)
	q_value: tensor([[1.9130]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20218167079722338, distance: 1.022136031827147 entropy -3.8423636149721645
epoch: 12, step: 121
	action: tensor([[-0.0276, -0.1598, -0.3063,  0.1730,  0.0285,  0.1132,  0.0578]],
       dtype=torch.float64)
	q_value: tensor([[2.0521]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20832285559117958, distance: 1.0181945001253379 entropy -3.8291767324519097
epoch: 12, step: 122
	action: tensor([[-0.0530, -0.1208, -0.3055,  0.0399, -0.1101,  0.0543,  0.0177]],
       dtype=torch.float64)
	q_value: tensor([[2.0660]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15829767142456086, distance: 1.0498710434779521 entropy -3.831527828651681
epoch: 12, step: 123
	action: tensor([[-0.0167, -0.2084, -0.3030,  0.0161,  0.0632,  0.1071,  0.1445]],
       dtype=torch.float64)
	q_value: tensor([[2.0551]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12848326366647222, distance: 1.0683032732472317 entropy -3.8375920806875525
epoch: 12, step: 124
	action: tensor([[-0.1649, -0.0899, -0.3024, -0.1531, -0.2255, -0.0635, -0.1509]],
       dtype=torch.float64)
	q_value: tensor([[2.0258]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0033375086595996972, distance: 1.1462522927308336 entropy -3.8391451359242588
epoch: 12, step: 125
	action: tensor([[-0.0602, -0.0199, -0.2998,  0.1484, -0.1396, -0.0933, -0.1185]],
       dtype=torch.float64)
	q_value: tensor([[2.1088]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24106440541743024, distance: 0.9969173497672357 entropy -3.845654985727172
epoch: 12, step: 126
	action: tensor([[-0.0865, -0.2298, -0.3083,  0.0552, -0.0312, -0.1155,  0.1538]],
       dtype=torch.float64)
	q_value: tensor([[2.0982]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.01378324406416187, distance: 1.1364305020150445 entropy -3.824260639845267
epoch: 12, step: 127
	action: tensor([[-0.0102, -0.1718, -0.3047, -0.0190,  0.2038,  0.0701, -0.1684]],
       dtype=torch.float64)
	q_value: tensor([[2.1239]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15083598229799722, distance: 1.0545143276511122 entropy -3.8334476853081525
LOSS epoch 12 actor 1.922056259722967 critic 1.2545202771475399 entropy 0.01
epoch: 13, step: 0
	action: tensor([[-0.0192,  0.0023, -0.2060,  0.0205, -0.1572, -0.0789, -0.0109]],
       dtype=torch.float64)
	q_value: tensor([[2.3991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25435182463530004, distance: 0.9881518157333311 entropy -4.101923528241158
epoch: 13, step: 1
	action: tensor([[-0.0978, -0.1374, -0.3910, -0.0134, -0.4647,  0.1000,  0.0113]],
       dtype=torch.float64)
	q_value: tensor([[1.7795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06901946583618035, distance: 1.104147245815914 entropy -3.6353540862079763
epoch: 13, step: 2
	action: tensor([[-0.0450, -0.0815, -0.3590,  0.0917, -0.4255,  0.0404,  0.1326]],
       dtype=torch.float64)
	q_value: tensor([[1.9272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20239362724347354, distance: 1.0220002473357943 entropy -3.7008741884328225
epoch: 13, step: 3
	action: tensor([[-0.3068, -0.0853, -0.3648,  0.1660,  0.0115,  0.0325, -0.0533]],
       dtype=torch.float64)
	q_value: tensor([[1.8673]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09211283478498977, distance: 1.1958878369182495 entropy -3.688755801218472
epoch: 13, step: 4
	action: tensor([[-0.1462, -0.1295, -0.3749, -0.0160, -0.0163, -0.2136,  0.0100]],
       dtype=torch.float64)
	q_value: tensor([[1.9105]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.006563243310901079, distance: 1.1405827670660225 entropy -3.6683813273568724
epoch: 13, step: 5
	action: tensor([[-0.1484, -0.0990, -0.3790, -0.0778, -0.1920, -0.1229,  0.0334]],
       dtype=torch.float64)
	q_value: tensor([[1.9319]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.033227114851627326, distance: 1.1251720201377386 entropy -3.6600194229295013
epoch: 13, step: 6
	action: tensor([[-0.0787, -0.1548, -0.3688,  0.0868,  0.0623, -0.1545,  0.0771]],
       dtype=torch.float64)
	q_value: tensor([[1.8945]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08392631657800675, distance: 1.0952717767914124 entropy -3.680816800056351
epoch: 13, step: 7
	action: tensor([[-0.0143, -0.0377, -0.3850,  0.0048,  0.0585,  0.0021,  0.1331]],
       dtype=torch.float64)
	q_value: tensor([[1.9181]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24319977447453112, distance: 0.995513880227384 entropy -3.6479161659772763
epoch: 13, step: 8
	action: tensor([[ 0.0253, -0.0565, -0.3806, -0.0434, -0.0467,  0.0007, -0.0812]],
       dtype=torch.float64)
	q_value: tensor([[1.7790]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26336887497495665, distance: 0.9821588309265555 entropy -3.656514744075693
epoch: 13, step: 9
	action: tensor([[-0.0308,  0.0589, -0.3814, -0.0131, -0.0735,  0.2148,  0.1337]],
       dtype=torch.float64)
	q_value: tensor([[1.8577]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3128907364219159, distance: 0.9485704583135788 entropy -3.654936791047295
epoch: 13, step: 10
	action: tensor([[-0.1166,  0.0986, -0.3726,  0.1111, -0.0016,  0.2082, -0.0286]],
       dtype=torch.float64)
	q_value: tensor([[1.7017]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27056444768927057, distance: 0.977350088579776 entropy -3.6729153178842786
epoch: 13, step: 11
	action: tensor([[-0.0962, -0.3866, -0.3760, -0.1023,  0.3254, -0.0111, -0.0113]],
       dtype=torch.float64)
	q_value: tensor([[1.7466]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16682013263483975, distance: 1.236114355931555 entropy -3.6660425360029465
epoch: 13, step: 12
	action: tensor([[ 0.0475, -0.2640, -0.3952,  0.0179, -0.0624,  0.2048,  0.0122]],
       dtype=torch.float64)
	q_value: tensor([[2.0069]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15551664884498317, distance: 1.0516040239134559 entropy -3.62749556207716
epoch: 13, step: 13
	action: tensor([[-0.0099, -0.1501, -0.3810,  0.0977, -0.2795,  0.0655,  0.0471]],
       dtype=torch.float64)
	q_value: tensor([[1.9417]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19122336440168686, distance: 1.0291317847008061 entropy -3.6556353547084526
epoch: 13, step: 14
	action: tensor([[ 0.0117, -0.1537, -0.3718,  0.0625, -0.2788,  0.0878,  0.1979]],
       dtype=torch.float64)
	q_value: tensor([[1.9192]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20253779818068218, distance: 1.0219078775921864 entropy -3.6743384279440763
epoch: 13, step: 15
	action: tensor([[-0.0466, -0.0349, -0.3704, -0.0303,  0.0553, -0.0570,  0.3370]],
       dtype=torch.float64)
	q_value: tensor([[1.8621]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19373208654192742, distance: 1.0275344271616034 entropy -3.6771973970136167
epoch: 13, step: 16
	action: tensor([[-0.0333, -0.0455, -0.3766, -0.1769, -0.0657, -0.0642,  0.0509]],
       dtype=torch.float64)
	q_value: tensor([[1.7333]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17488630835149288, distance: 1.0394739022881638 entropy -3.664948917689819
epoch: 13, step: 17
	action: tensor([[ 0.0517,  0.1238, -0.3758, -0.1379, -0.0703,  0.0009,  0.1421]],
       dtype=torch.float64)
	q_value: tensor([[1.7987]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39905844947994895, distance: 0.8871004710774977 entropy -3.6664676316655305
epoch: 13, step: 18
	action: tensor([[-0.1839,  0.0114, -0.3744,  0.1402, -0.1187,  0.0644,  0.0899]],
       dtype=torch.float64)
	q_value: tensor([[1.6586]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12220822997311076, distance: 1.0721423363621976 entropy -3.669154433466127
epoch: 13, step: 19
	action: tensor([[-0.1159, -0.0448, -0.3693,  0.1734,  0.0219, -0.2896,  0.0500]],
       dtype=torch.float64)
	q_value: tensor([[1.8141]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14044447801700022, distance: 1.060946930388499 entropy -3.6799275931353574
epoch: 13, step: 20
	action: tensor([[-0.1374, -0.1299, -0.3816, -0.1172,  0.0721,  0.0999,  0.1651]],
       dtype=torch.float64)
	q_value: tensor([[1.9182]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.024353080945700434, distance: 1.13032421598902 entropy -3.6547162116539886
epoch: 13, step: 21
	action: tensor([[-0.0633, -0.1552, -0.3751, -0.0338, -0.1782, -0.0650, -0.0681]],
       dtype=torch.float64)
	q_value: tensor([[1.7884]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09100934869139743, distance: 1.0910292691343717 entropy -3.6679991003720884
epoch: 13, step: 22
	action: tensor([[-0.1305, -0.2054, -0.3756,  0.1035, -0.1001,  0.1564, -0.2642]],
       dtype=torch.float64)
	q_value: tensor([[1.9461]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03020104786686273, distance: 1.126931578025939 entropy -3.666616332742502
epoch: 13, step: 23
	action: tensor([[ 0.0507, -0.1080, -0.3809,  0.0124, -0.1660, -0.0630,  0.0262]],
       dtype=torch.float64)
	q_value: tensor([[2.0213]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25805376023672266, distance: 0.9856958142682007 entropy -3.6558805823013816
epoch: 13, step: 24
	action: tensor([[-0.1626, -0.1139, -0.3781, -0.0586, -0.2839, -0.1255,  0.1014]],
       dtype=torch.float64)
	q_value: tensor([[1.8905]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.014078596702941426, distance: 1.1362603199129417 entropy -3.6615469829742686
epoch: 13, step: 25
	action: tensor([[-0.1124,  0.0340, -0.3636,  0.2177, -0.2867,  0.0650,  0.0335]],
       dtype=torch.float64)
	q_value: tensor([[1.8990]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.234162803135331, distance: 1.0014399705014276 entropy -3.6915699025851496
epoch: 13, step: 26
	action: tensor([[-0.0831, -0.2069, -0.3689, -0.0299, -0.0199,  0.2516,  0.0432]],
       dtype=torch.float64)
	q_value: tensor([[1.8399]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0655737931688446, distance: 1.106188650597227 entropy -3.680421785572421
epoch: 13, step: 27
	action: tensor([[-0.0999, -0.0786, -0.3784,  0.1289, -0.4141,  0.0014, -0.0429]],
       dtype=torch.float64)
	q_value: tensor([[1.8698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1511959465178171, distance: 1.0542907974172377 entropy -3.661075941755968
epoch: 13, step: 28
	action: tensor([[ 0.0038, -0.2059, -0.3648, -0.1605, -0.0389, -0.0528,  0.0334]],
       dtype=torch.float64)
	q_value: tensor([[1.9367]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07711132002682297, distance: 1.099338285071248 entropy -3.6889004000192656
epoch: 13, step: 29
	action: tensor([[-0.2072,  0.0649, -0.3826,  0.0473,  0.1260,  0.0602,  0.0017]],
       dtype=torch.float64)
	q_value: tensor([[1.8971]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11757777363752098, distance: 1.074966456251214 entropy -3.6524445246786383
epoch: 13, step: 30
	action: tensor([[-0.0126, -0.0209, -0.3759, -0.1073,  0.1602,  0.2127, -0.2708]],
       dtype=torch.float64)
	q_value: tensor([[1.7693]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27013318658384744, distance: 0.9776389631741572 entropy -3.666316856975576
epoch: 13, step: 31
	action: tensor([[ 0.0356, -0.2677, -0.3867, -0.0052, -0.0031,  0.1342, -0.0038]],
       dtype=torch.float64)
	q_value: tensor([[1.8258]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1283880549808809, distance: 1.068361624950934 entropy -3.6445202070600424
epoch: 13, step: 32
	action: tensor([[-0.0990, -0.0030, -0.2060,  0.0776,  0.0858,  0.0482, -0.0317]],
       dtype=torch.float64)
	q_value: tensor([[2.3991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18859953889290992, distance: 1.0307997827696245 entropy -4.101923528241158
epoch: 13, step: 33
	action: tensor([[-0.1470,  0.1430, -0.3982,  0.2697, -0.1519, -0.0572,  0.1703]],
       dtype=torch.float64)
	q_value: tensor([[1.7565]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24922251756151215, distance: 0.9915447344837057 entropy -3.621354307453149
epoch: 13, step: 34
	action: tensor([[-0.1525, -0.1855, -0.3665,  0.0643, -0.0342,  0.1135,  0.1934]],
       dtype=torch.float64)
	q_value: tensor([[1.7803]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.004241367460436796, distance: 1.1419148830578474 entropy -3.6856547560783253
epoch: 13, step: 35
	action: tensor([[-0.0044, -0.0722, -0.3746, -0.1400, -0.1071,  0.1271,  0.0134]],
       dtype=torch.float64)
	q_value: tensor([[1.8573]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20099866640445974, distance: 1.02289356236788 entropy -3.668960348338731
epoch: 13, step: 36
	action: tensor([[-0.1578,  0.0178, -0.3749, -0.0310, -0.2452,  0.1655, -0.0197]],
       dtype=torch.float64)
	q_value: tensor([[1.7942]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12417615548165672, distance: 1.0709398415344762 entropy -3.6681018048111294
epoch: 13, step: 37
	action: tensor([[-0.2919, -0.0873, -0.3645, -0.0578, -0.1713, -0.1916,  0.0846]],
       dtype=torch.float64)
	q_value: tensor([[1.7966]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12317426059187198, distance: 1.2127750797096974 entropy -3.689672509559273
epoch: 13, step: 38
	action: tensor([[-0.0930, -0.2164, -0.3632,  0.1077, -0.0464, -0.1744,  0.1815]],
       dtype=torch.float64)
	q_value: tensor([[1.9078]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.028097494657970956, distance: 1.1281531077314082 entropy -3.6925497881870077
epoch: 13, step: 39
	action: tensor([[-0.0218, -0.1841, -0.3797,  0.0827, -0.0966,  0.1236,  0.1317]],
       dtype=torch.float64)
	q_value: tensor([[1.9415]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15673563411437574, distance: 1.0508447710881925 entropy -3.6586546184812647
epoch: 13, step: 40
	action: tensor([[-0.0979,  0.1098, -0.3769,  0.0708, -0.0971,  0.0690,  0.0374]],
       dtype=torch.float64)
	q_value: tensor([[1.8786]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27564183011316135, distance: 0.9739426280151147 entropy -3.664120942642157
epoch: 13, step: 41
	action: tensor([[-0.1084, -0.1844, -0.3722,  0.0939, -0.2128,  0.2124,  0.1568]],
       dtype=torch.float64)
	q_value: tensor([[1.7415]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.065001817503196, distance: 1.1065271557630627 entropy -3.6737651937502442
epoch: 13, step: 42
	action: tensor([[-0.0552, -0.1779, -0.3700, -0.0363, -0.1586,  0.0113, -0.1832]],
       dtype=torch.float64)
	q_value: tensor([[1.8809]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08436565716871391, distance: 1.095009104112121 entropy -3.6782646452836874
epoch: 13, step: 43
	action: tensor([[ 0.0072,  0.0029, -0.3790, -0.0421,  0.1557, -0.0553,  0.0687]],
       dtype=torch.float64)
	q_value: tensor([[1.9771]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27763294799772065, distance: 0.9726031186784128 entropy -3.659767378146778
epoch: 13, step: 44
	action: tensor([[-0.1873, -0.0757, -0.3860,  0.0439, -0.3140,  0.1079,  0.0884]],
       dtype=torch.float64)
	q_value: tensor([[1.7674]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03046275113033403, distance: 1.1267795147709225 entropy -3.645735294532467
epoch: 13, step: 45
	action: tensor([[-0.0038, -0.1089, -0.3607,  0.0647, -0.1215, -0.0287, -0.0070]],
       dtype=torch.float64)
	q_value: tensor([[1.8628]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21707447704974503, distance: 1.0125510277239862 entropy -3.6976293007046896
epoch: 13, step: 46
	action: tensor([[ 0.0720, -0.1290, -0.3806, -0.0845,  0.0208, -0.1505, -0.1324]],
       dtype=torch.float64)
	q_value: tensor([[1.8954]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2192466637297139, distance: 1.0111454166691318 entropy -3.6564303443309205
epoch: 13, step: 47
	action: tensor([[-0.0299, -0.3333, -0.3879, -0.2678, -0.4530, -0.0265, -0.0829]],
       dtype=torch.float64)
	q_value: tensor([[1.9359]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08553534734360935, distance: 1.1922811507051132 entropy -3.641852356209598
epoch: 13, step: 48
	action: tensor([[-0.0991, -0.1817, -0.3643, -0.0068, -0.4921, -0.0590,  0.1445]],
       dtype=torch.float64)
	q_value: tensor([[2.0690]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04465372082972541, distance: 1.1185028666673573 entropy -3.689830278080047
epoch: 13, step: 49
	action: tensor([[-0.0364,  0.0168, -0.3595,  0.0874, -0.0875,  0.1342,  0.0907]],
       dtype=torch.float64)
	q_value: tensor([[1.9458]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2907656123726392, distance: 0.9637215833832661 entropy -3.699914629339829
epoch: 13, step: 50
	action: tensor([[-0.0553,  0.0140, -0.3766,  0.1267, -0.2309, -0.1632,  0.3286]],
       dtype=torch.float64)
	q_value: tensor([[1.7615]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2657282040942266, distance: 0.9805847090385196 entropy -3.6646119656799656
epoch: 13, step: 51
	action: tensor([[-0.0630, -0.2362, -0.3666,  0.0403,  0.0479, -0.0863, -0.0324]],
       dtype=torch.float64)
	q_value: tensor([[1.7861]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.032684039456720826, distance: 1.1254880030636822 entropy -3.6853323062239904
epoch: 13, step: 52
	action: tensor([[-0.2222,  0.0034, -0.3870, -0.1321,  0.0820,  0.1688,  0.1444]],
       dtype=torch.float64)
	q_value: tensor([[1.9783]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.037293623920820806, distance: 1.1228031370668006 entropy -3.6436920841157123
epoch: 13, step: 53
	action: tensor([[-0.0989, -0.0153, -0.3691,  0.0993, -0.0783,  0.1785, -0.0592]],
       dtype=torch.float64)
	q_value: tensor([[1.7233]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20828422752148146, distance: 1.0182193400544943 entropy -3.68048378098742
epoch: 13, step: 54
	action: tensor([[-0.0890, -0.1972, -0.3771, -0.0849, -0.1654,  0.1255,  0.1567]],
       dtype=torch.float64)
	q_value: tensor([[1.8288]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.025900332028197637, distance: 1.1294275856281941 entropy -3.6637247005731415
epoch: 13, step: 55
	action: tensor([[-0.0307,  0.0409, -0.3699, -0.1161, -0.2685,  0.0537,  0.0570]],
       dtype=torch.float64)
	q_value: tensor([[1.8545]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2627139079070199, distance: 0.982595371509625 entropy -3.6784554251939516
epoch: 13, step: 56
	action: tensor([[-0.0403,  0.0279, -0.3667,  0.1141,  0.2236, -0.1376,  0.1949]],
       dtype=torch.float64)
	q_value: tensor([[1.7508]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26092726271877087, distance: 0.9837851993487587 entropy -3.684871028888609
epoch: 13, step: 57
	action: tensor([[-0.1653, -0.2733, -0.3862,  0.2166, -0.2355,  0.1111, -0.1420]],
       dtype=torch.float64)
	q_value: tensor([[1.7722]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.046808401732582716, distance: 1.170820433049055 entropy -3.6453738006244616
epoch: 13, step: 58
	action: tensor([[-0.0881, -0.1433, -0.3764, -0.1887,  0.1706, -0.2022, -0.1473]],
       dtype=torch.float64)
	q_value: tensor([[2.0602]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.007577448976041956, distance: 1.1400004044298087 entropy -3.6651119226589777
epoch: 13, step: 59
	action: tensor([[-0.1046, -0.2025, -0.3871, -0.2004, -0.1454, -0.1667,  0.1834]],
       dtype=torch.float64)
	q_value: tensor([[1.9406]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03350672278113764, distance: 1.1633579078062057 entropy -3.6437612608647134
epoch: 13, step: 60
	action: tensor([[ 0.0135, -0.0549, -0.3707, -0.0234,  0.1134, -0.0802, -0.1300]],
       dtype=torch.float64)
	q_value: tensor([[1.8992]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24315890190140443, distance: 0.9955407622651623 entropy -3.676849455476152
epoch: 13, step: 61
	action: tensor([[-0.1320, -0.1825, -0.3894,  0.1940, -0.0448, -0.0359,  0.0032]],
       dtype=torch.float64)
	q_value: tensor([[1.8762]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.046105865962109416, distance: 1.1176524702126036 entropy -3.638942374371981
epoch: 13, step: 62
	action: tensor([[ 0.0935, -0.1021, -0.3787, -0.0976, -0.0474,  0.0577,  0.0132]],
       dtype=torch.float64)
	q_value: tensor([[1.9729]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27516042163669274, distance: 0.9742662154199968 entropy -3.660510634129444
epoch: 13, step: 63
	action: tensor([[ 7.4546e-02,  3.3209e-04, -3.8191e-01,  1.9150e-01, -1.2258e-01,
         -2.9192e-02,  2.8283e-02]], dtype=torch.float64)
	q_value: tensor([[1.8272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.405120990148947, distance: 0.8826144144886784 entropy -3.653794014204855
epoch: 13, step: 64
	action: tensor([[-0.1189, -0.0585, -0.2060,  0.0067, -0.0583,  0.0435,  0.0849]],
       dtype=torch.float64)
	q_value: tensor([[2.3991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10331506085845255, distance: 1.0836190490656217 entropy -4.101923528241158
epoch: 13, step: 65
	action: tensor([[-0.0700, -0.1228, -0.3898, -0.0229,  0.1310, -0.1834,  0.2105]],
       dtype=torch.float64)
	q_value: tensor([[1.7556]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06318899602639694, distance: 1.1075993315539434 entropy -3.6379208173159396
epoch: 13, step: 66
	action: tensor([[-0.0348,  0.0216, -0.3821,  0.3869, -0.1256, -0.0764,  0.0828]],
       dtype=torch.float64)
	q_value: tensor([[1.8514]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3211563986990992, distance: 0.9428477253219233 entropy -3.6537197613508487
epoch: 13, step: 67
	action: tensor([[ 0.1038, -0.0234, -0.3782,  0.0979, -0.1325, -0.2101,  0.0960]],
       dtype=torch.float64)
	q_value: tensor([[1.8774]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3749818496044285, distance: 0.9046967036878858 entropy -3.6613182507126405
epoch: 13, step: 68
	action: tensor([[-0.1971,  0.0299, -0.3803,  0.1908, -0.1098,  0.0970,  0.0502]],
       dtype=torch.float64)
	q_value: tensor([[1.8610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1300549248918409, distance: 1.067339569160222 entropy -3.656986705739367
epoch: 13, step: 69
	action: tensor([[-0.0931,  0.0301, -0.3698, -0.0799, -0.0103,  0.1410,  0.0891]],
       dtype=torch.float64)
	q_value: tensor([[1.8214]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20774647687663306, distance: 1.01856507976352 entropy -3.678844675313466
epoch: 13, step: 70
	action: tensor([[-0.0289, -0.1641, -0.3740, -0.0134,  0.0231,  0.0342,  0.0064]],
       dtype=torch.float64)
	q_value: tensor([[1.7164]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1321131223720282, distance: 1.0660762155531127 entropy -3.6702067143089443
epoch: 13, step: 71
	action: tensor([[-0.1533, -0.0234, -0.3832,  0.1182, -0.0412, -0.0549, -0.1316]],
       dtype=torch.float64)
	q_value: tensor([[1.8862]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12247825499633569, distance: 1.071977418247854 entropy -3.651253154044323
epoch: 13, step: 72
	action: tensor([[-0.0572, -0.2110, -0.3778,  0.0228, -0.2088,  0.0335, -0.0409]],
       dtype=torch.float64)
	q_value: tensor([[1.9075]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07178282598070762, distance: 1.1025073490203794 entropy -3.6622852599620512
epoch: 13, step: 73
	action: tensor([[-0.0145, -0.0746, -0.3749,  0.0780, -0.0742,  0.0983,  0.0198]],
       dtype=torch.float64)
	q_value: tensor([[1.9670]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24687326584714608, distance: 0.9930948401074011 entropy -3.6681920392589156
epoch: 13, step: 74
	action: tensor([[ 0.0502, -0.3430, -0.3795, -0.0690, -0.1240, -0.0225, -0.2434]],
       dtype=torch.float64)
	q_value: tensor([[1.8421]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03227006890217665, distance: 1.1257288080715617 entropy -3.6587453894752566
epoch: 13, step: 75
	action: tensor([[-0.0042,  0.0020, -0.3861,  0.1393, -0.2824, -0.0341,  0.0976]],
       dtype=torch.float64)
	q_value: tensor([[2.0952]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3126378126202405, distance: 0.9487450258715174 entropy -3.6456080550513756
epoch: 13, step: 76
	action: tensor([[-0.2004,  0.0165, -0.3697,  0.1553,  0.0168,  0.1907, -0.1127]],
       dtype=torch.float64)
	q_value: tensor([[1.8378]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12968475164362692, distance: 1.0675666286294727 entropy -3.678752075704496
epoch: 13, step: 77
	action: tensor([[-0.0638, -0.2101, -0.3778,  0.1349,  0.1653,  0.0286, -0.0540]],
       dtype=torch.float64)
	q_value: tensor([[1.8353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09649858170849435, distance: 1.0877300147480515 entropy -3.662251137575732
epoch: 13, step: 78
	action: tensor([[-0.1246, -0.1548, -0.3905,  0.2456, -0.1867,  0.0729,  0.0458]],
       dtype=torch.float64)
	q_value: tensor([[1.9555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09650872622962203, distance: 1.0877239082102717 entropy -3.6368090111587144
epoch: 13, step: 79
	action: tensor([[-0.0783, -0.1561, -0.3732, -0.0129, -0.0472,  0.2582, -0.0537]],
       dtype=torch.float64)
	q_value: tensor([[1.9474]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11556304037083165, distance: 1.0761929302739275 entropy -3.6717082816057123
epoch: 13, step: 80
	action: tensor([[ 0.0141, -0.1515, -0.3779, -0.0613, -0.0352, -0.2218, -0.0018]],
       dtype=torch.float64)
	q_value: tensor([[1.8727]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14765649722907725, distance: 1.0564866674506002 entropy -3.6620550111438663
epoch: 13, step: 81
	action: tensor([[-0.0652, -0.1687, -0.3842,  0.1115, -0.2160, -0.1168, -0.0523]],
       dtype=torch.float64)
	q_value: tensor([[1.9385]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1149942439602003, distance: 1.076538933616681 entropy -3.6492312574459618
epoch: 13, step: 82
	action: tensor([[-0.3110, -0.1759, -0.3748,  0.2673,  0.0496, -0.0999,  0.0826]],
       dtype=torch.float64)
	q_value: tensor([[1.9936]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16022155682181527, distance: 1.2326141768833458 entropy -3.668291394354688
epoch: 13, step: 83
	action: tensor([[-0.0668, -0.1561, -0.3754,  0.0143, -0.1603, -0.1675,  0.2461]],
       dtype=torch.float64)
	q_value: tensor([[1.9611]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06982025846017004, distance: 1.1036722717662077 entropy -3.6676229005091847
epoch: 13, step: 84
	action: tensor([[-0.1599, -0.0628, -0.3721, -0.0199, -0.1492, -0.0660, -0.1592]],
       dtype=torch.float64)
	q_value: tensor([[1.8807]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.058834162735168394, distance: 1.1101707242470569 entropy -3.673982317609087
epoch: 13, step: 85
	action: tensor([[ 0.0120, -0.1740, -0.3726,  0.1993, -0.1280, -0.0212,  0.0589]],
       dtype=torch.float64)
	q_value: tensor([[1.9281]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22028316155005612, distance: 1.010474015062227 entropy -3.6730250243075417
epoch: 13, step: 86
	action: tensor([[-0.0632, -0.1697, -0.3811, -0.0934, -0.0225, -0.2774,  0.3412]],
       dtype=torch.float64)
	q_value: tensor([[1.9480]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.041579246355153754, distance: 1.12030119171271 entropy -3.6554054574562485
epoch: 13, step: 87
	action: tensor([[-0.0524, -0.0579, -0.3750, -0.0369, -0.2811, -0.0033,  0.1240]],
       dtype=torch.float64)
	q_value: tensor([[1.8650]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18301479157577483, distance: 1.0343411195192926 entropy -3.668206801524781
epoch: 13, step: 88
	action: tensor([[-0.2046, -0.1827, -0.3666,  0.0119,  0.1060,  0.0765,  0.1842]],
       dtype=torch.float64)
	q_value: tensor([[1.8217]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0703999056417266, distance: 1.183940083629014 entropy -3.6851264678971125
epoch: 13, step: 89
	action: tensor([[-0.1037,  0.1075, -0.3775,  0.2545,  0.2152, -0.0726,  0.0027]],
       dtype=torch.float64)
	q_value: tensor([[1.8455]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27615748771767223, distance: 0.9735959001422727 entropy -3.6631451243852906
epoch: 13, step: 90
	action: tensor([[-0.2489,  0.0635, -0.3863, -0.0353,  0.0131, -0.0273, -0.1374]],
       dtype=torch.float64)
	q_value: tensor([[1.8025]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04430607092603045, distance: 1.1187063593974071 entropy -3.6453327004810547
epoch: 13, step: 91
	action: tensor([[-0.0546, -0.1155, -0.3712, -0.0433, -0.0950, -0.1097,  0.0329]],
       dtype=torch.float64)
	q_value: tensor([[1.8295]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12434428485462423, distance: 1.0708370439872135 entropy -3.67603704704851
epoch: 13, step: 92
	action: tensor([[-0.2730, -0.0483, -0.3783, -0.0622, -0.1908,  0.0709,  0.0335]],
       dtype=torch.float64)
	q_value: tensor([[1.8865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06978366433426952, distance: 1.1835992307290002 entropy -3.6612767046244308
epoch: 13, step: 93
	action: tensor([[-0.1975, -0.0295, -0.3617, -0.0682, -0.2134, -0.1304,  0.0118]],
       dtype=torch.float64)
	q_value: tensor([[1.8433]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03311869701416903, distance: 1.1252351090488883 entropy -3.6957442252932657
epoch: 13, step: 94
	action: tensor([[ 0.0403, -0.0524, -0.3664, -0.0220, -0.0651,  0.1414, -0.0258]],
       dtype=torch.float64)
	q_value: tensor([[1.8680]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29681196659772824, distance: 0.9596048383001732 entropy -3.685844980228687
epoch: 13, step: 95
	action: tensor([[-0.0658, -0.1485, -0.3808,  0.1889, -0.1106,  0.0646, -0.0468]],
       dtype=torch.float64)
	q_value: tensor([[1.8078]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15854224312564735, distance: 1.0497185029697416 entropy -3.65598796006233
epoch: 13, step: 96
	action: tensor([[-0.1129,  0.0199, -0.2060,  0.1096,  0.0127, -0.0664,  0.0172]],
       dtype=torch.float64)
	q_value: tensor([[2.3991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1832600797411208, distance: 1.0341858347729365 entropy -4.101923528241158
epoch: 13, step: 97
	action: tensor([[ 0.0333, -0.2416, -0.3951, -0.1531, -0.1080,  0.0371, -0.3109]],
       dtype=torch.float64)
	q_value: tensor([[1.7662]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08721009428666515, distance: 1.0933069468660084 entropy -3.627281615002891
epoch: 13, step: 98
	action: tensor([[-0.0339, -0.0592, -0.3824, -0.0766,  0.0330, -0.2819, -0.1167]],
       dtype=torch.float64)
	q_value: tensor([[2.0344]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16427244833367316, distance: 1.0461381818577409 entropy -3.6530818163491867
epoch: 13, step: 99
	action: tensor([[ 0.0112, -0.0934, -0.3839, -0.1336,  0.1067, -0.0052,  0.1057]],
       dtype=torch.float64)
	q_value: tensor([[1.9297]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19297902879566475, distance: 1.0280141759832353 entropy -3.6500671162784153
epoch: 13, step: 100
	action: tensor([[-0.1780, -0.1996, -0.3831, -0.2419, -0.1375, -0.1084, -0.0450]],
       dtype=torch.float64)
	q_value: tensor([[1.7896]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11458564935632487, distance: 1.2081292991175838 entropy -3.65160418584613
epoch: 13, step: 101
	action: tensor([[-0.0716, -0.1867, -0.3708, -0.0470, -0.0720,  0.0142,  0.0484]],
       dtype=torch.float64)
	q_value: tensor([[1.9458]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05577528362222972, distance: 1.111973341356829 entropy -3.6766574760326756
epoch: 13, step: 102
	action: tensor([[-0.1979, -0.1721, -0.3779, -0.0973,  0.1711,  0.2869,  0.1493]],
       dtype=torch.float64)
	q_value: tensor([[1.8941]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03360620850782525, distance: 1.1634138990828977 entropy -3.6621716660699746
epoch: 13, step: 103
	action: tensor([[-0.0456, -0.2165, -0.3776,  0.1621, -0.2560, -0.0741,  0.1432]],
       dtype=torch.float64)
	q_value: tensor([[1.8013]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10991496037377968, distance: 1.0796237854882484 entropy -3.66313974598993
epoch: 13, step: 104
	action: tensor([[-0.0900, -0.1830, -0.3723,  0.1478, -0.3478, -0.0697,  0.0114]],
       dtype=torch.float64)
	q_value: tensor([[1.9623]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09068317092286615, distance: 1.0912250013858558 entropy -3.6733560232567974
epoch: 13, step: 105
	action: tensor([[-0.1585,  0.0457, -0.3692,  0.0346,  0.1360, -0.2077,  0.0640]],
       dtype=torch.float64)
	q_value: tensor([[1.9833]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12453232077008836, distance: 1.0707220635038985 entropy -3.6797570091198617
epoch: 13, step: 106
	action: tensor([[-0.0998,  0.0498, -0.3809, -0.1150, -0.2534,  0.1656,  0.2208]],
       dtype=torch.float64)
	q_value: tensor([[1.8206]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19685004404612994, distance: 1.0255456886611813 entropy -3.656170364607681
epoch: 13, step: 107
	action: tensor([[-0.0801, -0.1427, -0.3612,  0.0738,  0.0399, -0.2128, -0.0243]],
       dtype=torch.float64)
	q_value: tensor([[1.7087]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08613554385563404, distance: 1.093950286529858 entropy -3.6965465463693117
epoch: 13, step: 108
	action: tensor([[-0.1668, -0.0148, -0.3862,  0.0378,  0.0652, -0.1155,  0.0224]],
       dtype=torch.float64)
	q_value: tensor([[1.9542]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08663279302781601, distance: 1.093652627568437 entropy -3.645319572637787
epoch: 13, step: 109
	action: tensor([[-0.0669, -0.3629, -0.3779,  0.1069, -0.1174, -0.2100, -0.1493]],
       dtype=torch.float64)
	q_value: tensor([[1.8452]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.062053972221786324, distance: 1.1793154469493519 entropy -3.662286989096863
epoch: 13, step: 110
	action: tensor([[-0.0291, -0.1481, -0.3854,  0.1374, -0.0204, -0.0107,  0.3973]],
       dtype=torch.float64)
	q_value: tensor([[2.1324]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17140687027740553, distance: 1.041663285523796 entropy -3.647113757291137
epoch: 13, step: 111
	action: tensor([[-0.1543,  0.0298, -0.3750,  0.0217, -0.0381,  0.0713,  0.0356]],
       dtype=torch.float64)
	q_value: tensor([[1.8144]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15015586841793072, distance: 1.0549365348279782 entropy -3.668054918121662
epoch: 13, step: 112
	action: tensor([[-0.1057, -0.0619, -0.3728,  0.1500, -0.3876,  0.0016,  0.0570]],
       dtype=torch.float64)
	q_value: tensor([[1.7764]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16248346881259523, distance: 1.0472572782036367 entropy -3.672698169497004
epoch: 13, step: 113
	action: tensor([[-0.1479,  0.2325, -0.3644,  0.1945, -0.2061,  0.0725,  0.0213]],
       dtype=torch.float64)
	q_value: tensor([[1.8988]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3051610968811813, distance: 0.9538910111276916 entropy -3.689790572526646
epoch: 13, step: 114
	action: tensor([[-0.1384, -0.0397, -0.3669,  0.0473, -0.3864, -0.3027,  0.0773]],
       dtype=torch.float64)
	q_value: tensor([[1.7224]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14210398720664785, distance: 1.0599222718518762 entropy -3.6845308152782015
epoch: 13, step: 115
	action: tensor([[-0.1565, -0.1589, -0.3620,  0.1612,  0.0645,  0.0194,  0.2615]],
       dtype=torch.float64)
	q_value: tensor([[1.9302]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.031726185073286506, distance: 1.1260451048360247 entropy -3.69487827957812
epoch: 13, step: 116
	action: tensor([[-0.2622, -0.0984, -0.3784, -0.0684, -0.1328, -0.1213, -0.0372]],
       dtype=torch.float64)
	q_value: tensor([[1.8476]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09961866823808152, distance: 1.1999903273868402 entropy -3.661311590015605
epoch: 13, step: 117
	action: tensor([[-0.1701, -0.1076, -0.3672,  0.1176, -0.0958,  0.0290,  0.2207]],
       dtype=torch.float64)
	q_value: tensor([[1.9262]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04595829893962533, distance: 1.1177389170536163 entropy -3.684195791292255
epoch: 13, step: 118
	action: tensor([[ 0.0520, -0.1939, -0.3707, -0.0038, -0.1271,  0.2317,  0.0804]],
       dtype=torch.float64)
	q_value: tensor([[1.8395]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20945430791419184, distance: 1.01746664631918 entropy -3.676948487780431
epoch: 13, step: 119
	action: tensor([[-0.1300, -0.1085, -0.3785,  0.1780, -0.1320, -0.0304,  0.2920]],
       dtype=torch.float64)
	q_value: tensor([[1.8673]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10115970580582367, distance: 1.0849206110014102 entropy -3.660790586430963
epoch: 13, step: 120
	action: tensor([[ 0.0123, -0.1792, -0.3696,  0.0344, -0.0471, -0.0158,  0.1638]],
       dtype=torch.float64)
	q_value: tensor([[1.8481]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16089510023415554, distance: 1.048249881069451 entropy -3.6793711394179955
epoch: 13, step: 121
	action: tensor([[-0.0735, -0.0901, -0.3813,  0.0166, -0.1275, -0.2381, -0.0582]],
       dtype=torch.float64)
	q_value: tensor([[1.8696]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13747305227774187, distance: 1.0627791594409763 entropy -3.655187087340502
epoch: 13, step: 122
	action: tensor([[ 0.0009, -0.1684, -0.3770, -0.2244, -0.1506,  0.1177,  0.1202]],
       dtype=torch.float64)
	q_value: tensor([[1.9530]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10805847712732508, distance: 1.0807491046190607 entropy -3.6638884616712275
epoch: 13, step: 123
	action: tensor([[-0.1113, -0.2218, -0.3726,  0.0352, -0.1242,  0.0401, -0.0158]],
       dtype=torch.float64)
	q_value: tensor([[1.8183]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0073086883729905905, distance: 1.1401547572582897 entropy -3.6728803270154713
epoch: 13, step: 124
	action: tensor([[-0.0028,  0.1124, -0.3764, -0.0013, -0.1419, -0.0859,  0.0359]],
       dtype=torch.float64)
	q_value: tensor([[1.9575]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3527154130270135, distance: 0.9206707085989396 entropy -3.665159124498822
epoch: 13, step: 125
	action: tensor([[-0.1543, -0.0196, -0.3737,  0.0540, -0.0803,  0.0696,  0.0231]],
       dtype=torch.float64)
	q_value: tensor([[1.7525]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1204369338059893, distance: 1.073223529306776 entropy -3.6705698812279812
epoch: 13, step: 126
	action: tensor([[-0.1597, -0.2998, -0.3726,  0.0616, -0.4301, -0.2707,  0.2005]],
       dtype=torch.float64)
	q_value: tensor([[1.8181]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08422568283114917, distance: 1.1915617086474115 entropy -3.672937995651381
epoch: 13, step: 127
	action: tensor([[-0.0072, -0.1295, -0.3615,  0.1983, -0.2011, -0.1849,  0.0069]],
       dtype=torch.float64)
	q_value: tensor([[2.0572]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22801778352159052, distance: 1.0054496823707428 entropy -3.6960435970111942
LOSS epoch 13 actor 1.660426249318162 critic 0.7885278752847551 entropy 0.01
epoch: 14, step: 0
	action: tensor([[-0.0957, -0.1033, -0.2101,  0.0229, -0.1709, -0.1496,  0.1881]],
       dtype=torch.float64)
	q_value: tensor([[1.6930]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08488474644232702, distance: 1.094698670112017 entropy -4.076850207423306
epoch: 14, step: 1
	action: tensor([[-0.0909, -0.0458, -0.4936,  0.0491, -0.1166, -0.3048,  0.3276]],
       dtype=torch.float64)
	q_value: tensor([[1.2500]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15159368593895606, distance: 1.0540437544484413 entropy -3.4466026311105518
epoch: 14, step: 2
	action: tensor([[-0.0240, -0.1528, -0.4481,  0.0935, -0.2835, -0.0084, -0.0332]],
       dtype=torch.float64)
	q_value: tensor([[1.2916]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16644104182700425, distance: 1.0447800107247598 entropy -3.5238030670237444
epoch: 14, step: 3
	action: tensor([[ 0.0382, -0.1483, -0.4720,  0.1268, -0.0172,  0.0397,  0.4540]],
       dtype=torch.float64)
	q_value: tensor([[1.3542]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23681981732239832, distance: 0.9997012510136667 entropy -3.482358637132353
epoch: 14, step: 4
	action: tensor([[-0.2590, -0.2930, -0.4825,  0.0418,  0.0096, -0.0027, -0.1501]],
       dtype=torch.float64)
	q_value: tensor([[1.2482]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.22892128109228627, distance: 1.2685825282939502 entropy -3.464621597617743
epoch: 14, step: 5
	action: tensor([[-0.2052, -0.2884, -0.4952, -0.0365, -0.1341,  0.1747, -0.1873]],
       dtype=torch.float64)
	q_value: tensor([[1.4319]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1662522879024153, distance: 1.235813535596839 entropy -3.4443467893610955
epoch: 14, step: 6
	action: tensor([[-0.1835, -0.1928, -0.4873,  0.0288,  0.0589, -0.2061,  0.0970]],
       dtype=torch.float64)
	q_value: tensor([[1.4191]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07594354116546476, distance: 1.187001956242384 entropy -3.4569288266009357
epoch: 14, step: 7
	action: tensor([[-0.3298, -0.0880, -0.4851,  0.0431, -0.2114,  0.1410,  0.1958]],
       dtype=torch.float64)
	q_value: tensor([[1.3641]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16168574100258426, distance: 1.2333917030062338 entropy -3.4609687040278594
epoch: 14, step: 8
	action: tensor([[-9.4206e-02, -3.5078e-01, -4.4757e-01, -3.2197e-04, -1.3066e-01,
          4.6890e-02,  1.5668e-01]], dtype=torch.float64)
	q_value: tensor([[1.2879]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09434536521524506, distance: 1.197109548102591 entropy -3.5245681475972224
epoch: 14, step: 9
	action: tensor([[-0.1350, -0.1343, -0.4891, -0.0013,  0.0322,  0.0663,  0.0353]],
       dtype=torch.float64)
	q_value: tensor([[1.3720]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.043993964540974484, distance: 1.1188890156251956 entropy -3.4538632487521297
epoch: 14, step: 10
	action: tensor([[-0.0367, -0.3653, -0.4910,  0.2100,  0.1358,  0.0403,  0.2063]],
       dtype=torch.float64)
	q_value: tensor([[1.3184]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.00013219936303376922, distance: 1.1442686107091835 entropy -3.4511539840713827
epoch: 14, step: 11
	action: tensor([[-0.1466,  0.0417, -0.5146,  0.0166, -0.0695, -0.1581,  0.4051]],
       dtype=torch.float64)
	q_value: tensor([[1.3950]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15397922748268122, distance: 1.0525608357391838 entropy -3.4128019514418506
epoch: 14, step: 12
	action: tensor([[-0.2287, -0.1031, -0.4434, -0.0651, -0.0243,  0.1306,  0.0374]],
       dtype=torch.float64)
	q_value: tensor([[1.2351]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04745628577657235, distance: 1.1711826953989979 entropy -3.5319641556537964
epoch: 14, step: 13
	action: tensor([[ 0.0262, -0.1347, -0.4824,  0.1068, -0.0118,  0.0598,  0.0064]],
       dtype=torch.float64)
	q_value: tensor([[1.2907]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2311603088349058, distance: 1.0034011422051448 entropy -3.465257827852567
epoch: 14, step: 14
	action: tensor([[-0.1053, -0.2388, -0.4995, -0.2004, -0.1358,  0.3142,  0.0657]],
       dtype=torch.float64)
	q_value: tensor([[1.3360]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.027797398240118065, distance: 1.1601401318153894 entropy -3.4371902074370646
epoch: 14, step: 15
	action: tensor([[-0.0220, -0.3102, -0.4823,  0.1240, -0.2640,  0.0317, -0.0578]],
       dtype=torch.float64)
	q_value: tensor([[1.3157]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.043633721172770534, distance: 1.119099806335863 entropy -3.46492674243645
epoch: 14, step: 16
	action: tensor([[-0.0144, -0.1391, -0.4818,  0.0305,  0.0215, -0.3267,  0.0672]],
       dtype=torch.float64)
	q_value: tensor([[1.4167]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15088505431799248, distance: 1.0544838577449964 entropy -3.4658455068612186
epoch: 14, step: 17
	action: tensor([[ 0.0704, -0.2125, -0.4876, -0.0612,  0.1567,  0.2104, -0.1553]],
       dtype=torch.float64)
	q_value: tensor([[1.3693]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2163297542894277, distance: 1.0130324851172516 entropy -3.4569355725042
epoch: 14, step: 18
	action: tensor([[-0.2672, -0.0665, -0.5278,  0.1249, -0.2223,  0.0884,  0.1150]],
       dtype=torch.float64)
	q_value: tensor([[1.3762]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06196124762888533, distance: 1.179263964663861 entropy -3.3919447460246084
epoch: 14, step: 19
	action: tensor([[-0.0193, -0.2555, -0.4450,  0.0327, -0.1177,  0.0344, -0.0715]],
       dtype=torch.float64)
	q_value: tensor([[1.3131]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07616438491921629, distance: 1.099902131581272 entropy -3.5290496958225153
epoch: 14, step: 20
	action: tensor([[-0.0573, -0.0579, -0.4982,  0.1176, -0.3264,  0.2790,  0.2078]],
       dtype=torch.float64)
	q_value: tensor([[1.3905]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1893999916754645, distance: 1.0302912113626206 entropy -3.439082561892009
epoch: 14, step: 21
	action: tensor([[-0.2265,  0.1988, -0.4557,  0.2028,  0.1151, -0.1716,  0.1953]],
       dtype=torch.float64)
	q_value: tensor([[1.2651]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17036217597213354, distance: 1.0423197457762678 entropy -3.5101269333093215
epoch: 14, step: 22
	action: tensor([[-0.0349, -0.2215, -0.4629,  0.2850, -0.1094, -0.1909, -0.0064]],
       dtype=torch.float64)
	q_value: tensor([[1.2628]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13079845779150767, distance: 1.0668833497097991 entropy -3.498491131860825
epoch: 14, step: 23
	action: tensor([[ 0.0537, -0.1178, -0.4870,  0.1357,  0.2091, -0.0449, -0.0876]],
       dtype=torch.float64)
	q_value: tensor([[1.4065]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27068952585949113, distance: 0.9772662906340708 entropy -3.4577133382502496
epoch: 14, step: 24
	action: tensor([[-0.1938,  0.0933, -0.5215,  0.2260, -0.1890, -0.0536, -0.2582]],
       dtype=torch.float64)
	q_value: tensor([[1.3756]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1499031605402451, distance: 1.0550933700331824 entropy -3.402308026164168
epoch: 14, step: 25
	action: tensor([[-0.1516,  0.1697, -0.4564, -0.0293,  0.2154, -0.1063, -0.0559]],
       dtype=torch.float64)
	q_value: tensor([[1.3728]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.224139600146103, distance: 1.0079720418248197 entropy -3.5095226672591933
epoch: 14, step: 26
	action: tensor([[-0.0720, -0.1804, -0.4903,  0.0282, -0.1354,  0.2160,  0.3144]],
       dtype=torch.float64)
	q_value: tensor([[1.2706]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08479930849346162, distance: 1.094749771121264 entropy -3.452643825102141
epoch: 14, step: 27
	action: tensor([[ 0.1517,  0.1408, -0.4749, -0.0522,  0.0137, -0.0296, -0.1968]],
       dtype=torch.float64)
	q_value: tensor([[1.2785]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5045479709672978, distance: 0.8054850772900137 entropy -3.4773229218787174
epoch: 14, step: 28
	action: tensor([[-0.1304, -0.3577, -0.4976,  0.1076,  0.1970,  0.3621,  0.0293]],
       dtype=torch.float64)
	q_value: tensor([[1.2772]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04768544957269283, distance: 1.1713108047997705 entropy -3.440435962237333
epoch: 14, step: 29
	action: tensor([[ 0.0135, -0.2666, -0.5293, -0.0524, -0.0186,  0.1204,  0.1378]],
       dtype=torch.float64)
	q_value: tensor([[1.4009]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08382605944175714, distance: 1.0953317096382333 entropy -3.38977472555484
epoch: 14, step: 30
	action: tensor([[-0.1230, -0.0262, -0.4946, -0.0307, -0.4475, -0.0415,  0.3120]],
       dtype=torch.float64)
	q_value: tensor([[1.3416]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13830725286764411, distance: 1.0622650969693281 entropy -3.4447498695163787
epoch: 14, step: 31
	action: tensor([[ 0.0668,  0.0260, -0.4226,  0.0639, -0.1590, -0.1943,  0.1789]],
       dtype=torch.float64)
	q_value: tensor([[1.2636]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3801613441249053, distance: 0.9009403168156949 entropy -3.5690567278892855
epoch: 14, step: 32
	action: tensor([[-0.0844, -0.0529, -0.2101, -0.0065, -0.0437, -0.0194, -0.0199]],
       dtype=torch.float64)
	q_value: tensor([[1.6930]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13695107774109694, distance: 1.063100691282819 entropy -4.076850207423306
epoch: 14, step: 33
	action: tensor([[ 0.0107, -0.0420, -0.5171, -0.2247, -0.3512, -0.0584, -0.1292]],
       dtype=torch.float64)
	q_value: tensor([[1.2568]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2391556996499733, distance: 0.9981701747911144 entropy -3.4089306988732275
epoch: 14, step: 34
	action: tensor([[-0.1610, -0.1232, -0.4508,  0.1870, -0.2239, -0.1244,  0.0763]],
       dtype=torch.float64)
	q_value: tensor([[1.3259]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05169734717619012, distance: 1.1143719606144484 entropy -3.518617865220648
epoch: 14, step: 35
	action: tensor([[-0.1718,  0.0150, -0.4599,  0.0831,  0.1808, -0.0177,  0.2129]],
       dtype=torch.float64)
	q_value: tensor([[1.3395]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.112757981960521, distance: 1.0778981927068934 entropy -3.503377250939117
epoch: 14, step: 36
	action: tensor([[-0.3187, -0.1590, -0.4887,  0.0781, -0.2773, -0.1331,  0.1822]],
       dtype=torch.float64)
	q_value: tensor([[1.2564]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18205301474741797, distance: 1.244156952388822 entropy -3.455047827569065
epoch: 14, step: 37
	action: tensor([[-0.1453, -0.0965, -0.4339,  0.0132,  0.0700,  0.2507,  0.1494]],
       dtype=torch.float64)
	q_value: tensor([[1.3386]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0846102387029285, distance: 1.0948628465498618 entropy -3.5489013065201904
epoch: 14, step: 38
	action: tensor([[-0.2777, -0.0952, -0.4990, -0.0563, -0.0704, -0.2270,  0.0856]],
       dtype=torch.float64)
	q_value: tensor([[1.2590]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10085557770901366, distance: 1.2006650441224465 entropy -3.437984766100184
epoch: 14, step: 39
	action: tensor([[-0.0928,  0.1015, -0.4536, -0.0089,  0.2410, -0.1830,  0.0692]],
       dtype=torch.float64)
	q_value: tensor([[1.3489]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22888857541634733, distance: 1.0048824513807932 entropy -3.5143589263985384
epoch: 14, step: 40
	action: tensor([[-0.2539, -0.2300, -0.4956,  0.1056,  0.0297,  0.0088,  0.1193]],
       dtype=torch.float64)
	q_value: tensor([[1.2727]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16538217452588744, distance: 1.2353524438854502 entropy -3.4437683425753645
epoch: 14, step: 41
	action: tensor([[-0.0493, -0.1200, -0.4834,  0.1189, -0.0647,  0.1240, -0.0412]],
       dtype=torch.float64)
	q_value: tensor([[1.3584]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16550276243548778, distance: 1.0453678634708843 entropy -3.4638791369592306
epoch: 14, step: 42
	action: tensor([[ 0.0085,  0.1543, -0.4928,  0.1188, -0.3121,  0.2594, -0.0437]],
       dtype=torch.float64)
	q_value: tensor([[1.3382]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3921391991101877, distance: 0.8921928987764963 entropy -3.447999161472757
epoch: 14, step: 43
	action: tensor([[-0.2755, -0.0958, -0.4576,  0.1390,  0.0380, -0.0103,  0.0312]],
       dtype=torch.float64)
	q_value: tensor([[1.2303]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08218269097122088, distance: 1.1904385574689897 entropy -3.507024559925642
epoch: 14, step: 44
	action: tensor([[-0.0601, -0.3591, -0.4827, -0.0273, -0.3263,  0.0302,  0.1231]],
       dtype=torch.float64)
	q_value: tensor([[1.3347]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07355899661273302, distance: 1.185685888566974 entropy -3.4652309326087276
epoch: 14, step: 45
	action: tensor([[-0.0955, -0.2129, -0.4673,  0.0344,  0.0682, -0.0248,  0.1262]],
       dtype=torch.float64)
	q_value: tensor([[1.4002]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.020752239578594733, distance: 1.1324081510742514 entropy -3.4902045432422986
epoch: 14, step: 46
	action: tensor([[-0.1901, -0.3165, -0.4992,  0.0880, -0.0501, -0.1750, -0.2832]],
       dtype=torch.float64)
	q_value: tensor([[1.3378]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1663939353252728, distance: 1.235888581317825 entropy -3.437648742391292
epoch: 14, step: 47
	action: tensor([[-0.1263,  0.1510, -0.4952,  0.2972, -0.3134,  0.1870, -0.1491]],
       dtype=torch.float64)
	q_value: tensor([[1.4863]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22270054242416748, distance: 1.008906396746579 entropy -3.444525112758014
epoch: 14, step: 48
	action: tensor([[-0.1642,  0.0461, -0.4531,  0.4953, -0.2055,  0.1167,  0.0635]],
       dtype=torch.float64)
	q_value: tensor([[1.3012]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1912995369303211, distance: 1.0290833205063972 entropy -3.5150452472192817
epoch: 14, step: 49
	action: tensor([[-0.1635, -0.1065, -0.4671,  0.1563, -0.1027, -0.1218,  0.2372]],
       dtype=torch.float64)
	q_value: tensor([[1.3057]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04724022246950421, distance: 1.1169877248029008 entropy -3.491233235440075
epoch: 14, step: 50
	action: tensor([[-0.0336, -0.1329, -0.4635,  0.1016, -0.2021, -0.1350,  0.1993]],
       dtype=torch.float64)
	q_value: tensor([[1.3031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17029141255600688, distance: 1.0423641968192718 entropy -3.4972853645119684
epoch: 14, step: 51
	action: tensor([[ 0.0213, -0.1082, -0.4633, -0.1704, -0.3495,  0.0726,  0.1938]],
       dtype=torch.float64)
	q_value: tensor([[1.3127]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1941519683758688, distance: 1.0272668366882525 entropy -3.4972941616502387
epoch: 14, step: 52
	action: tensor([[ 0.0249, -0.0681, -0.4552,  0.0494, -0.1966,  0.2523,  0.2120]],
       dtype=torch.float64)
	q_value: tensor([[1.2660]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2817304397411573, distance: 0.9698407413698149 entropy -3.510811658677842
epoch: 14, step: 53
	action: tensor([[-0.2624, -0.1460, -0.4762, -0.0840, -0.1765,  0.0573,  0.2873]],
       dtype=torch.float64)
	q_value: tensor([[1.2411]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13306676733065426, distance: 1.218104210115306 entropy -3.475159356788408
epoch: 14, step: 54
	action: tensor([[-0.1549, -0.0516, -0.4531, -0.2549, -0.2711,  0.1750,  0.0314]],
       dtype=torch.float64)
	q_value: tensor([[1.2758]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04058721536678567, distance: 1.1208808359118343 entropy -3.514975950988247
epoch: 14, step: 55
	action: tensor([[-0.0554, -0.0644, -0.4566,  0.0114, -0.2033, -0.0644,  0.0153]],
       dtype=torch.float64)
	q_value: tensor([[1.2506]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18498013220512521, distance: 1.0330962642730224 entropy -3.5086615890768833
epoch: 14, step: 56
	action: tensor([[-0.1045, -0.0725, -0.4683,  0.1799,  0.0047,  0.0403,  0.0558]],
       dtype=torch.float64)
	q_value: tensor([[1.3086]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15169136227272628, distance: 1.0539830770988523 entropy -3.4888698236915254
epoch: 14, step: 57
	action: tensor([[ 0.1522,  0.1930, -0.4894,  0.0723,  0.2988, -0.1165,  0.1793]],
       dtype=torch.float64)
	q_value: tensor([[1.3123]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5320895525055936, distance: 0.78277701422775 entropy -3.453926687012234
epoch: 14, step: 58
	action: tensor([[-0.1546, -0.2444, -0.5029, -0.1103, -0.3237, -0.3891,  0.1207]],
       dtype=torch.float64)
	q_value: tensor([[1.2150]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03839946831088126, distance: 1.166108394719431 entropy -3.4318475707328417
epoch: 14, step: 59
	action: tensor([[-0.1004, -0.0454, -0.4412,  0.1071, -0.2172,  0.2882, -0.0847]],
       dtype=torch.float64)
	q_value: tensor([[1.4065]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16815531500549785, distance: 1.043705125661299 entropy -3.5356311621646355
epoch: 14, step: 60
	action: tensor([[-0.2062, -0.1753, -0.4803,  0.1988, -0.2044, -0.1763, -0.1673]],
       dtype=torch.float64)
	q_value: tensor([[1.2970]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04328436137980751, distance: 1.1688480105774737 entropy -3.4685352488385393
epoch: 14, step: 61
	action: tensor([[-0.1115, -0.3760, -0.4683,  0.0896, -0.0944, -0.1308,  0.0400]],
       dtype=torch.float64)
	q_value: tensor([[1.4202]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12465187590085458, distance: 1.2135725632764398 entropy -3.4891882664415936
epoch: 14, step: 62
	action: tensor([[-0.0410, -0.1233, -0.4916,  0.0997, -0.2832, -0.0067,  0.1637]],
       dtype=torch.float64)
	q_value: tensor([[1.4248]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16555911458606443, distance: 1.045332566935957 entropy -3.4500442002143816
epoch: 14, step: 63
	action: tensor([[-0.0731, -0.1680, -0.4562, -0.0042, -0.4562,  0.0269,  0.1377]],
       dtype=torch.float64)
	q_value: tensor([[1.3077]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07851667699040121, distance: 1.0985009406786665 entropy -3.5094192493011533
epoch: 14, step: 64
	action: tensor([[-0.0272, -0.0824, -0.2101,  0.0961,  0.1710,  0.0367, -0.0300]],
       dtype=torch.float64)
	q_value: tensor([[1.6930]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21273796383972132, distance: 1.0153513435651507 entropy -4.076850207423306
epoch: 14, step: 65
	action: tensor([[ 0.1648, -0.0432, -0.5445,  0.1809, -0.0056, -0.0230,  0.0232]],
       dtype=torch.float64)
	q_value: tensor([[1.2746]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4110441508601771, distance: 0.8782093627993598 entropy -3.3666091304272334
epoch: 14, step: 66
	action: tensor([[-0.1377, -0.3102, -0.4936, -0.0219, -0.1423, -0.0923, -0.0501]],
       dtype=torch.float64)
	q_value: tensor([[1.3251]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11239468751237092, distance: 1.2069412938369786 entropy -3.4468503431091464
epoch: 14, step: 67
	action: tensor([[-0.0707, -0.1434, -0.4815,  0.0401, -0.0340, -0.4206, -0.2035]],
       dtype=torch.float64)
	q_value: tensor([[1.4198]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09850765164527664, distance: 1.0865199768723293 entropy -3.4667160010598015
epoch: 14, step: 68
	action: tensor([[ 0.0059, -0.0893, -0.4870, -0.1116, -0.1866, -0.5110,  0.0889]],
       dtype=torch.float64)
	q_value: tensor([[1.4446]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22610020572099598, distance: 1.0066976594257677 entropy -3.4580883431851293
epoch: 14, step: 69
	action: tensor([[-0.0018, -0.2085, -0.4586,  0.0779, -0.2057, -0.0123,  0.1445]],
       dtype=torch.float64)
	q_value: tensor([[1.3697]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14016929066533734, distance: 1.061116748279586 entropy -3.5055735013861
epoch: 14, step: 70
	action: tensor([[-0.1259, -0.1666, -0.4760,  0.0215,  0.1130, -0.2846,  0.1821]],
       dtype=torch.float64)
	q_value: tensor([[1.3343]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0011682961279588788, distance: 1.1450125253527668 entropy -3.4756394126139867
epoch: 14, step: 71
	action: tensor([[ 0.1254, -0.3476, -0.4870,  0.0730,  0.0946,  0.0886,  0.0523]],
       dtype=torch.float64)
	q_value: tensor([[1.3531]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15019204637159012, distance: 1.0549140802096326 entropy -3.457741860832335
epoch: 14, step: 72
	action: tensor([[ 0.0544, -0.2453, -0.5243,  0.0980, -0.8873,  0.0411, -0.2409]],
       dtype=torch.float64)
	q_value: tensor([[1.4019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14736184136131636, distance: 1.0566692658368406 entropy -3.397431437330693
epoch: 14, step: 73
	action: tensor([[-0.0960, -0.0287, -0.4324, -0.3265, -0.0592,  0.1129,  0.1072]],
       dtype=torch.float64)
	q_value: tensor([[1.4608]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1187165207208456, distance: 1.074272621679324 entropy -3.551010777361012
epoch: 14, step: 74
	action: tensor([[-0.0646, -0.0944, -0.4784,  0.1270,  0.2575, -0.2125, -0.0205]],
       dtype=torch.float64)
	q_value: tensor([[1.2091]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13348740441774365, distance: 1.0652318256418238 entropy -3.471592045790735
epoch: 14, step: 75
	action: tensor([[-0.1427, -0.1627, -0.5120,  0.2601,  0.0546,  0.1990, -0.0703]],
       dtype=torch.float64)
	q_value: tensor([[1.3789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06552851502761114, distance: 1.1062154507655348 entropy -3.41751016150561
epoch: 14, step: 76
	action: tensor([[-0.1893, -0.0638, -0.5034,  0.0671,  0.0330, -0.2107, -0.0385]],
       dtype=torch.float64)
	q_value: tensor([[1.3852]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.028221354497458995, distance: 1.1280812191858662 entropy -3.431048779566839
epoch: 14, step: 77
	action: tensor([[-0.0349, -0.2462, -0.4764,  0.1110,  0.1861,  0.3331, -0.2806]],
       dtype=torch.float64)
	q_value: tensor([[1.3681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1480557606722367, distance: 1.0562391933321267 entropy -3.475709916851151
epoch: 14, step: 78
	action: tensor([[ 0.0382, -0.1332, -0.5393, -0.1779,  0.1636, -0.1341, -0.2139]],
       dtype=torch.float64)
	q_value: tensor([[1.4296]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18308164009086514, distance: 1.0342988019952515 entropy -3.3744898475351093
epoch: 14, step: 79
	action: tensor([[-0.0416, -0.1983, -0.5082,  0.0775,  0.0696, -0.2012,  0.0545]],
       dtype=torch.float64)
	q_value: tensor([[1.3994]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08339056090239272, distance: 1.0955920088376134 entropy -3.423391466900783
epoch: 14, step: 80
	action: tensor([[-0.0732, -0.0363, -0.4959, -0.0651, -0.0191,  0.1026,  0.3186]],
       dtype=torch.float64)
	q_value: tensor([[1.3877]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17963051451863032, distance: 1.0364812312746658 entropy -3.4431868615177033
epoch: 14, step: 81
	action: tensor([[-0.0346, -0.0293, -0.4735, -0.0262, -0.2641,  0.1191, -0.1100]],
       dtype=torch.float64)
	q_value: tensor([[1.2183]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22585757158207964, distance: 1.0068554576830018 entropy -3.479863165368605
epoch: 14, step: 82
	action: tensor([[-0.2547, -0.0474, -0.4688,  0.1908, -0.5410, -0.1581, -0.1300]],
       dtype=torch.float64)
	q_value: tensor([[1.3013]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0096773595427192, distance: 1.1387936772280998 entropy -3.4878497806093804
epoch: 14, step: 83
	action: tensor([[-0.0491, -0.1046, -0.4232, -0.1920, -0.0436, -0.0564,  0.0195]],
       dtype=torch.float64)
	q_value: tensor([[1.3671]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.114769848446025, distance: 1.076675404591657 entropy -3.568159090644849
epoch: 14, step: 84
	action: tensor([[-0.1925, -0.1124, -0.4907, -0.2225, -0.2321,  0.1376,  0.1143]],
       dtype=torch.float64)
	q_value: tensor([[1.2884]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03890484494798119, distance: 1.1663921257257586 entropy -3.4512936308906004
epoch: 14, step: 85
	action: tensor([[-0.1047, -0.0224, -0.4542,  0.1282,  0.0295,  0.2949, -0.2741]],
       dtype=torch.float64)
	q_value: tensor([[1.2709]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20271418305781275, distance: 1.0217948571486366 entropy -3.5128953206815603
epoch: 14, step: 86
	action: tensor([[-0.1167,  0.0590, -0.5101, -0.1311, -0.1764, -0.0457,  0.2983]],
       dtype=torch.float64)
	q_value: tensor([[1.3413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19637072037575853, distance: 1.025851668260187 entropy -3.4202712429648408
epoch: 14, step: 87
	action: tensor([[-0.0280, -0.0542, -0.4410,  0.0768, -0.4007,  0.1328, -0.1096]],
       dtype=torch.float64)
	q_value: tensor([[1.2055]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22611714417705564, distance: 1.0066866424972836 entropy -3.536148953621983
epoch: 14, step: 88
	action: tensor([[-0.1627, -0.5130, -0.4610,  0.3113, -0.0650, -0.1783,  0.1559]],
       dtype=torch.float64)
	q_value: tensor([[1.3158]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2516827471618981, distance: 1.2802766544730573 entropy -3.501049256852297
epoch: 14, step: 89
	action: tensor([[-0.2808, -0.1164, -0.4966, -0.3152,  0.0404, -0.1324, -0.0407]],
       dtype=torch.float64)
	q_value: tensor([[1.4567]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1423510880025285, distance: 1.2230845863325206 entropy -3.4420075265685948
epoch: 14, step: 90
	action: tensor([[-0.0777, -0.4687, -0.4690,  0.1428, -0.1174, -0.0182,  0.0522]],
       dtype=torch.float64)
	q_value: tensor([[1.3459]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15029724288763546, distance: 1.2273310816725678 entropy -3.48771808216108
epoch: 14, step: 91
	action: tensor([[ 0.1060,  0.0801, -0.4989, -0.0736, -0.2936, -0.0427, -0.3003]],
       dtype=torch.float64)
	q_value: tensor([[1.4514]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.422967069447739, distance: 0.8692746053983429 entropy -3.4377878272837505
epoch: 14, step: 92
	action: tensor([[-0.0112, -0.2345, -0.4671, -0.0704,  0.0735,  0.1052,  0.0395]],
       dtype=torch.float64)
	q_value: tensor([[1.3339]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0899111940097671, distance: 1.0916881082838812 entropy -3.490758408001187
epoch: 14, step: 93
	action: tensor([[-0.0365, -0.2595, -0.5105,  0.0043, -0.4936,  0.1050,  0.1713]],
       dtype=torch.float64)
	q_value: tensor([[1.3388]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03453653681188362, distance: 1.1244097810481493 entropy -3.419267576375075
epoch: 14, step: 94
	action: tensor([[-0.0560, -0.2608, -0.4438, -0.0600, -0.1481, -0.1107, -0.0896]],
       dtype=torch.float64)
	q_value: tensor([[1.3755]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.005728634660612797, distance: 1.1410617811475294 entropy -3.5308543183342422
epoch: 14, step: 95
	action: tensor([[-0.1859,  0.0276, -0.4895, -0.0996,  0.0798, -0.3545,  0.0979]],
       dtype=torch.float64)
	q_value: tensor([[1.4027]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08009971732879795, distance: 1.0975569633131956 entropy -3.4533645677112887
epoch: 14, step: 96
	action: tensor([[-0.0438, -0.0332, -0.2101, -0.1128, -0.1680,  0.0481,  0.0954]],
       dtype=torch.float64)
	q_value: tensor([[1.6930]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16910392521574968, distance: 1.043109851219564 entropy -4.076850207423306
epoch: 14, step: 97
	action: tensor([[-0.0231, -0.2068, -0.4990,  0.1224, -0.3485,  0.0329,  0.2010]],
       dtype=torch.float64)
	q_value: tensor([[1.1997]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1240319007715317, distance: 1.0710280337871516 entropy -3.4376180070846365
epoch: 14, step: 98
	action: tensor([[-0.1882, -0.2757, -0.4547,  0.0257, -0.0760,  0.2220, -0.2028]],
       dtype=torch.float64)
	q_value: tensor([[1.3419]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11264961786725314, distance: 1.207079584836359 entropy -3.5118572273321766
epoch: 14, step: 99
	action: tensor([[-0.1507, -0.1425, -0.5012, -0.0263, -0.0801, -0.4338, -0.1149]],
       dtype=torch.float64)
	q_value: tensor([[1.4105]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.019684763447551545, distance: 1.1330252009503974 entropy -3.4343033171740056
epoch: 14, step: 100
	action: tensor([[-0.1220, -0.4540, -0.4699,  0.0385, -0.1295,  0.1373, -0.0109]],
       dtype=torch.float64)
	q_value: tensor([[1.4271]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1978060855521213, distance: 1.2524198748314752 entropy -3.4865780185843334
epoch: 14, step: 101
	action: tensor([[ 0.0541,  0.2470, -0.4996,  0.0040, -0.4675,  0.2311, -0.0408]],
       dtype=torch.float64)
	q_value: tensor([[1.4423]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44850498261110605, distance: 0.8498210607088735 entropy -3.436537590236321
epoch: 14, step: 102
	action: tensor([[-0.1205, -0.2620, -0.4377, -0.0268, -0.0801, -0.0090, -0.0662]],
       dtype=torch.float64)
	q_value: tensor([[1.2002]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05488422142996652, distance: 1.1753280241339281 entropy -3.541667963896635
epoch: 14, step: 103
	action: tensor([[ 0.1390, -0.3668, -0.4960, -0.0967, -0.2082,  0.2569, -0.2303]],
       dtype=torch.float64)
	q_value: tensor([[1.3899]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11053623316307926, distance: 1.079246934999447 entropy -3.442754604325208
epoch: 14, step: 104
	action: tensor([[ 0.0901, -0.2099, -0.5067,  0.0329, -0.3771,  0.1106, -0.1608]],
       dtype=torch.float64)
	q_value: tensor([[1.4492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2166996034789439, distance: 1.012793409139906 entropy -3.424853841049763
epoch: 14, step: 105
	action: tensor([[-0.1393, -0.0321, -0.4737, -0.0179, -0.1746,  0.0514,  0.0675]],
       dtype=torch.float64)
	q_value: tensor([[1.3969]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11057547911500654, distance: 1.0792231248443067 entropy -3.4792439590933135
epoch: 14, step: 106
	action: tensor([[-0.0487, -0.1959, -0.4623, -0.1581, -0.1109,  0.1121,  0.1443]],
       dtype=torch.float64)
	q_value: tensor([[1.2731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05994017582056532, distance: 1.1095182227201756 entropy -3.499020348487911
epoch: 14, step: 107
	action: tensor([[-0.0723, -0.1391, -0.4839,  0.0033, -0.3795, -0.1350,  0.0390]],
       dtype=torch.float64)
	q_value: tensor([[1.2880]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11798122331421557, distance: 1.0747206870266302 entropy -3.4623930438176416
epoch: 14, step: 108
	action: tensor([[-0.0599, -0.1453, -0.4440,  0.1940, -0.0640,  0.2164, -0.2464]],
       dtype=torch.float64)
	q_value: tensor([[1.3476]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17419678188992327, distance: 1.0399081424966337 entropy -3.530735883283942
epoch: 14, step: 109
	action: tensor([[-0.1902, -0.0907, -0.5093,  0.2135, -0.2527,  0.0201, -0.0255]],
       dtype=torch.float64)
	q_value: tensor([[1.3928]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.030583318092109568, distance: 1.1267094521680705 entropy -3.421293441181298
epoch: 14, step: 110
	action: tensor([[-0.0854, -0.1121, -0.4554, -0.1268, -0.0471, -0.1199, -0.3168]],
       dtype=torch.float64)
	q_value: tensor([[1.3546]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08459655288218104, distance: 1.0948710310638736 entropy -3.5110205482504635
epoch: 14, step: 111
	action: tensor([[-0.0285, -0.0255, -0.4928, -0.1709, -0.0912, -0.0137, -0.0422]],
       dtype=torch.float64)
	q_value: tensor([[1.4050]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21810584252395648, distance: 1.011883879770376 entropy -3.44823274394548
epoch: 14, step: 112
	action: tensor([[-0.1384, -0.2997, -0.4770,  0.2473, -0.4289, -0.3354,  0.0525]],
       dtype=torch.float64)
	q_value: tensor([[1.2853]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.014180642866187032, distance: 1.1524294601378209 entropy -3.474203245241678
epoch: 14, step: 113
	action: tensor([[ 0.0258,  0.0337, -0.4456, -0.0401, -0.2767, -0.0796,  0.0689]],
       dtype=torch.float64)
	q_value: tensor([[1.4322]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33634018629594553, distance: 0.9322436925733397 entropy -3.5280295940183235
epoch: 14, step: 114
	action: tensor([[-0.0605, -0.1069, -0.4570,  0.1988,  0.0819, -0.0283, -0.2273]],
       dtype=torch.float64)
	q_value: tensor([[1.2486]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1763811456821539, distance: 1.0385318813204116 entropy -3.50795472906042
epoch: 14, step: 115
	action: tensor([[-1.7432e-01,  4.5052e-02, -5.1290e-01,  1.0719e-01,  9.9946e-02,
         -1.6603e-04, -2.3318e-02]], dtype=torch.float64)
	q_value: tensor([[1.4085]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1351992740853457, distance: 1.0641790771935424 entropy -3.415966296590113
epoch: 14, step: 116
	action: tensor([[-0.0400, -0.1650, -0.4829, -0.0679,  0.1059, -0.0954,  0.1624]],
       dtype=torch.float64)
	q_value: tensor([[1.3000]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09142137807510875, distance: 1.0907819689723917 entropy -3.464927577644193
epoch: 14, step: 117
	action: tensor([[-0.2305,  0.1360, -0.4977,  0.0299, -0.5153,  0.0376,  0.0213]],
       dtype=torch.float64)
	q_value: tensor([[1.3106]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1161934062313833, distance: 1.0758093437465328 entropy -3.439996313853879
epoch: 14, step: 118
	action: tensor([[-0.1315, -0.0821, -0.4126,  0.0301, -0.0844, -0.1116, -0.1306]],
       dtype=torch.float64)
	q_value: tensor([[1.2615]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08410339597670524, distance: 1.0951659122497148 entropy -3.587787762058198
epoch: 14, step: 119
	action: tensor([[-0.0609, -0.1568, -0.4870,  0.1217, -0.0575, -0.1847,  0.1398]],
       dtype=torch.float64)
	q_value: tensor([[1.3576]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11585703334817166, distance: 1.0760140484115688 entropy -3.4578119606695705
epoch: 14, step: 120
	action: tensor([[-0.0356, -0.0301, -0.4774,  0.2465, -0.1645,  0.0480,  0.1002]],
       dtype=torch.float64)
	q_value: tensor([[1.3501]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26512489855400634, distance: 0.980987469107864 entropy -3.473635901552052
epoch: 14, step: 121
	action: tensor([[-0.1836, -0.1585, -0.4712,  0.2433, -0.3214, -0.1250,  0.5286]],
       dtype=torch.float64)
	q_value: tensor([[1.2948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.008438669186445535, distance: 1.1395056532382661 entropy -3.483952658230198
epoch: 14, step: 122
	action: tensor([[-0.0194, -0.1683, -0.4359,  0.0346, -0.0543, -0.1636, -0.1831]],
       dtype=torch.float64)
	q_value: tensor([[1.3092]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13470771673812032, distance: 1.0644814767612572 entropy -3.5450544755084517
epoch: 14, step: 123
	action: tensor([[-0.0222,  0.0269, -0.5000, -0.1991, -0.1951, -0.0526,  0.1705]],
       dtype=torch.float64)
	q_value: tensor([[1.4079]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2642201267663644, distance: 0.9815911749351149 entropy -3.4364124206637956
epoch: 14, step: 124
	action: tensor([[-0.0636,  0.0233, -0.4529,  0.0328, -0.2443, -0.2279,  0.3420]],
       dtype=torch.float64)
	q_value: tensor([[1.2212]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25335304681454973, distance: 0.9888133970754852 entropy -3.5152696190117934
epoch: 14, step: 125
	action: tensor([[-0.1924,  0.0782, -0.4400, -0.1147,  0.0726, -0.0486,  0.1681]],
       dtype=torch.float64)
	q_value: tensor([[1.2428]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11131878806959827, distance: 1.078772066959922 entropy -3.537913956512575
epoch: 14, step: 126
	action: tensor([[-0.0560, -0.1878, -0.4731,  0.0368, -0.0377,  0.0227,  0.1732]],
       dtype=torch.float64)
	q_value: tensor([[1.2254]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09045791584065321, distance: 1.0913601516155913 entropy -3.4808612819852267
epoch: 14, step: 127
	action: tensor([[-0.1554, -0.1257, -0.4879, -0.1335, -0.3246,  0.0616,  0.0412]],
       dtype=torch.float64)
	q_value: tensor([[1.3105]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.001338202678159206, distance: 1.1435783153962233 entropy -3.455988206289517
LOSS epoch 14 actor 0.883860504124242 critic 0.5541690824835387 entropy 0.01
epoch: 15, step: 0
	action: tensor([[-0.0353, -0.1633, -0.8672,  0.1485, -0.1970,  0.0613, -0.2730]],
       dtype=torch.float64)
	q_value: tensor([[1.1116]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09201628110641902, distance: 1.090424809041849 entropy -3.028915596744333
epoch: 15, step: 1
	action: tensor([[ 0.4412,  0.5318, -1.6629,  0.9746, -0.1692,  0.3797,  0.7470]],
       dtype=torch.float64)
	q_value: tensor([[0.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32775644300916174, distance: 0.9382531205525884 entropy -2.48970043485336
epoch: 15, step: 2
	action: tensor([[-0.6653, -0.6205, -1.7840,  0.3135, -1.6956, -0.4733, -0.2507]],
       dtype=torch.float64)
	q_value: tensor([[0.9296]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1233051199446513, distance: 1.6674886893352194 entropy -2.4294711898165544
epoch: 15, step: 3
	action: tensor([[-0.1417,  0.1197, -1.5815,  0.8521, -1.2011,  0.0245,  0.0811]],
       dtype=torch.float64)
	q_value: tensor([[1.2060]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.33802323151543967, distance: 1.3236968039194505 entropy -2.531192189569208
epoch: 15, step: 4
	action: tensor([[-0.6934, -0.3956, -1.6028,  0.2165, -0.0379, -0.4804,  0.9691]],
       dtype=torch.float64)
	q_value: tensor([[1.0885]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7993833413121283, distance: 1.5350359148183794 entropy -2.5205316851951305
epoch: 15, step: 5
	action: tensor([[-0.0064,  0.8109, -1.5092,  0.3609, -0.5598, -0.9118,  0.0626]],
       dtype=torch.float64)
	q_value: tensor([[1.0961]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6608618843198275, distance: 0.6664153542349696 entropy -2.571252858436726
epoch: 15, step: 6
	action: tensor([[-0.2417, -0.3438, -1.3602, -0.3873, -0.7929, -0.2776,  1.0215]],
       dtype=torch.float64)
	q_value: tensor([[1.1253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1127461392764284, distance: 1.077905386440353 entropy -2.659005408629532
epoch: 15, step: 7
	action: tensor([[ 0.1552,  0.7703, -1.4516,  0.0088, -1.1994,  0.1378, -0.0313]],
       dtype=torch.float64)
	q_value: tensor([[1.0440]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7127145944073401, distance: 0.6133572106655159 entropy -2.603819245783982
epoch: 15, step: 8
	action: tensor([[-0.2771, -0.2794, -1.3914,  0.1163,  0.0122, -0.1191,  0.1415]],
       dtype=torch.float64)
	q_value: tensor([[1.0289]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2872650055410486, distance: 1.2983466891341735 entropy -2.6397568818892054
epoch: 15, step: 9
	action: tensor([[-0.2829,  0.3645, -1.6034,  0.3497, -0.5550,  1.5880,  0.1381]],
       dtype=torch.float64)
	q_value: tensor([[1.0182]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.19896449392837856, distance: 1.2530253413953327 entropy -2.52052515998781
epoch: 15, step: 10
	action: tensor([[ 0.2280, -0.4211, -1.8845,  0.2615, -0.6136,  0.2815,  0.5614]],
       dtype=torch.float64)
	q_value: tensor([[0.9624]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14447092972591835, distance: 1.22421888910976 entropy -2.3828689600676753
epoch: 15, step: 11
	action: tensor([[-0.4160, -1.1016, -1.7385, -0.6730, -0.1981,  0.3524, -0.4427]],
       dtype=torch.float64)
	q_value: tensor([[1.1037]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5285536073219228, distance: 1.414806447910458 entropy -2.451399846271535
epoch: 15, step: 12
	action: tensor([[-0.3662, -0.5845, -1.9271,  1.5541,  0.5215,  0.2549,  0.0290]],
       dtype=torch.float64)
	q_value: tensor([[1.2384]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9371091999041705, distance: 1.5926991384380322 entropy -2.3639549784563783
epoch: 15, step: 13
	action: tensor([[-0.0637, -0.8449, -2.3051,  1.4470, -1.8095, -0.5916,  0.3795]],
       dtype=torch.float64)
	q_value: tensor([[1.1557]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8850457334886845, distance: 1.571149960594151 entropy -2.21190106653036
epoch: 15, step: 14
	action: tensor([[-0.2337, -0.4539, -1.9843, -0.3404, -1.3695,  0.6145,  0.4948]],
       dtype=torch.float64)
	q_value: tensor([[1.3293]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17224322984183815, distance: 1.2389836058837445 entropy -2.339143101219219
epoch: 15, step: 15
	action: tensor([[-0.2811, -0.5230, -1.6033,  0.1662, -0.4163, -0.1151,  0.0927]],
       dtype=torch.float64)
	q_value: tensor([[1.2214]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.559638319345618, distance: 1.429119818062618 entropy -2.5194937081325155
epoch: 15, step: 16
	action: tensor([[-0.1564,  0.1669, -1.6248,  0.3704,  0.7089,  0.6266,  1.0192]],
       dtype=torch.float64)
	q_value: tensor([[1.0841]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08396487533400787, distance: 1.1914183865869306 entropy -2.5090990292965136
epoch: 15, step: 17
	action: tensor([[-8.5567e-01, -1.0981e-03, -1.8534e+00, -1.1712e-01, -4.4995e-01,
         -6.1652e-01,  3.2015e-01]], dtype=torch.float64)
	q_value: tensor([[0.8897]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4098356402863874, distance: 1.3587543137346396 entropy -2.3970681690447373
epoch: 15, step: 18
	action: tensor([[-0.3987,  0.1248, -1.2403,  0.2089, -0.5531,  0.5078, -0.1993]],
       dtype=torch.float64)
	q_value: tensor([[1.1843]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.39512265906234223, distance: 1.3516457695815038 entropy -2.7359730780944767
epoch: 15, step: 19
	action: tensor([[-0.1774, -0.3873, -1.5829,  0.3184, -0.0657, -0.9796,  0.4593]],
       dtype=torch.float64)
	q_value: tensor([[0.9531]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.01003066489919413, distance: 1.150069200437041 entropy -2.5311963982788237
epoch: 15, step: 20
	action: tensor([[-0.2178, -0.1825, -1.5959,  0.3744, -1.0193,  1.0783, -0.0336]],
       dtype=torch.float64)
	q_value: tensor([[1.1285]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5172520493948805, distance: 1.409566467495138 entropy -2.5243677490442082
epoch: 15, step: 21
	action: tensor([[-0.6658, -0.7941, -1.8080,  0.2145, -0.0442, -0.8267, -0.2798]],
       dtype=torch.float64)
	q_value: tensor([[1.0933]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8190166773388547, distance: 1.5433876959553428 entropy -2.4179765721829605
epoch: 15, step: 22
	action: tensor([[-0.2406,  0.0784, -1.7653, -0.1455, -1.0431,  0.6803,  0.0597]],
       dtype=torch.float64)
	q_value: tensor([[1.2654]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03890296823982009, distance: 1.166391072222822 entropy -2.4389371356875693
epoch: 15, step: 23
	action: tensor([[-0.3135, -0.5670, -1.5051,  0.1658, -0.9440,  0.3536,  0.0354]],
       dtype=torch.float64)
	q_value: tensor([[1.1122]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7779439908090857, distance: 1.5258636638473158 entropy -2.573331325345761
epoch: 15, step: 24
	action: tensor([[-0.0277,  0.1657, -1.6515,  0.6483, -0.8964, -0.5867,  0.3549]],
       dtype=torch.float64)
	q_value: tensor([[1.0654]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14737493415290137, distance: 1.0566611528973466 entropy -2.4951240511604142
epoch: 15, step: 25
	action: tensor([[-0.1588, -0.2416, -1.4764,  0.2316, -0.9888, -0.4408,  0.3674]],
       dtype=torch.float64)
	q_value: tensor([[1.1096]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04484088969451516, distance: 1.1697196171135602 entropy -2.5899770482943048
epoch: 15, step: 26
	action: tensor([[-0.5022, -0.2119, -1.4396, -0.1671, -0.1333, -0.0114,  1.0251]],
       dtype=torch.float64)
	q_value: tensor([[1.0539]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.35203095536443274, distance: 1.3306076341558066 entropy -2.6109651207297366
epoch: 15, step: 27
	action: tensor([[ 0.0441, -0.0448, -1.5078,  0.2509, -0.5692, -0.0140,  0.0743]],
       dtype=torch.float64)
	q_value: tensor([[0.9908]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14498658567010025, distance: 1.0581400621636026 entropy -2.5720304239452463
epoch: 15, step: 28
	action: tensor([[-0.0596, -0.0347, -1.5533,  0.5444, -1.0297,  0.0668, -0.4879]],
       dtype=torch.float64)
	q_value: tensor([[1.0268]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18476577912791337, distance: 1.2455837794466578 entropy -2.547164919043199
epoch: 15, step: 29
	action: tensor([[-0.4592,  0.2884, -1.6663,  0.5708, -0.3434, -0.6467,  0.0509]],
       dtype=torch.float64)
	q_value: tensor([[1.1027]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2198071588184045, distance: 1.2638696417420514 entropy -2.4875864109475962
epoch: 15, step: 30
	action: tensor([[-0.1802, -0.7534, -1.4376, -0.1194,  0.3897, -0.3016,  0.7159]],
       dtype=torch.float64)
	q_value: tensor([[1.1280]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.38286859051853095, distance: 1.3456965825808838 entropy -2.6125388985336917
epoch: 15, step: 31
	action: tensor([[-0.6317,  0.3776, -1.8143, -0.3992, -0.2726, -0.7873, -0.5106]],
       dtype=torch.float64)
	q_value: tensor([[1.0815]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.45828486718743966, distance: 0.8422522448100257 entropy -2.4154962957352955
epoch: 15, step: 32
	action: tensor([[-0.1627, -0.0590, -0.8672,  0.0668, -0.6641, -0.3597,  0.2558]],
       dtype=torch.float64)
	q_value: tensor([[1.1116]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17164437207348848, distance: 1.0415139875011021 entropy -3.028915596744333
epoch: 15, step: 33
	action: tensor([[-0.4649,  0.5940, -1.3904, -0.1541, -0.6057,  0.0558,  0.1374]],
       dtype=torch.float64)
	q_value: tensor([[0.8834]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17217734669372753, distance: 1.0411788718685902 entropy -2.640837804997072
epoch: 15, step: 34
	action: tensor([[-0.3590, -0.3449, -1.2494,  0.1318,  0.1216,  0.4382,  0.1501]],
       dtype=torch.float64)
	q_value: tensor([[0.9508]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5209777989270996, distance: 1.411296065277029 entropy -2.730551669926281
epoch: 15, step: 35
	action: tensor([[-0.2081, -0.8208, -1.7551, -0.0115, -0.5009, -0.3805,  0.1728]],
       dtype=torch.float64)
	q_value: tensor([[0.9389]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.44812805380161946, distance: 1.3770831895223434 entropy -2.443829988506533
epoch: 15, step: 36
	action: tensor([[-0.1222, -0.8804, -1.6701,  0.3234, -0.4732, -0.1547, -0.5439]],
       dtype=torch.float64)
	q_value: tensor([[1.1657]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7785919714219465, distance: 1.5261416928534755 entropy -2.485505444177123
epoch: 15, step: 37
	action: tensor([[ 0.0955, -0.3818, -1.9118, -0.1048,  0.7567, -0.1942,  0.5690]],
       dtype=torch.float64)
	q_value: tensor([[1.1968]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1525497417385443, distance: 1.0534496943353153 entropy -2.371408881769846
epoch: 15, step: 38
	action: tensor([[-0.1588, -1.0243, -1.9081,  0.1200, -1.6050, -0.1283,  0.1361]],
       dtype=torch.float64)
	q_value: tensor([[1.1008]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8553498715435193, distance: 1.5587253643723744 entropy -2.372607739874706
epoch: 15, step: 39
	action: tensor([[-0.0071,  0.2186, -1.7140,  0.3654,  0.4957, -0.8551, -0.3177]],
       dtype=torch.float64)
	q_value: tensor([[1.2331]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.40095754968605657, distance: 0.8856976509629291 entropy -2.4632395518508345
epoch: 15, step: 40
	action: tensor([[-0.1447, -0.0450, -1.7900,  0.5527, -1.0197,  0.1337, -0.0814]],
       dtype=torch.float64)
	q_value: tensor([[1.2400]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.35449611981434215, distance: 1.3318201333220365 entropy -2.42725511254881
epoch: 15, step: 41
	action: tensor([[-0.8186, -0.3036, -1.6203,  0.6525, -0.6315,  0.2887, -0.1607]],
       dtype=torch.float64)
	q_value: tensor([[1.1318]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.3788610508081507, distance: 1.7649858626958064 entropy -2.511239901199146
epoch: 15, step: 42
	action: tensor([[-0.4851, -0.7233, -1.7047,  0.2683, -0.8337,  0.7485, -0.7290]],
       dtype=torch.float64)
	q_value: tensor([[1.0612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.105321534040414, distance: 1.6604121764849684 entropy -2.468406175286725
epoch: 15, step: 43
	action: tensor([[-0.6405, -0.8071, -1.9393,  0.7971, -1.0055,  0.1741,  0.5712]],
       dtype=torch.float64)
	q_value: tensor([[1.1653]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.4256790798990888, distance: 1.7822694570555386 entropy -2.35880558588591
epoch: 15, step: 44
	action: tensor([[ 0.0780, -0.5130, -1.7929,  0.8374, -1.1930, -1.0702, -0.4866]],
       dtype=torch.float64)
	q_value: tensor([[1.1524]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10254562389150923, distance: 1.2015863280799854 entropy -2.4253639350900764
epoch: 15, step: 45
	action: tensor([[-0.8480, -0.7104, -1.8300, -0.0060, -1.2922,  0.6734,  1.2058]],
       dtype=torch.float64)
	q_value: tensor([[1.2531]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2043008856620343, distance: 1.6989951176127123 entropy -2.4082117569265007
epoch: 15, step: 46
	action: tensor([[-0.2319, -0.0675, -1.7312, -0.4725, -0.4724, -0.5793, -0.0656]],
       dtype=torch.float64)
	q_value: tensor([[1.2110]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.537666450335262, distance: 0.778098177432145 entropy -2.4544236728751945
epoch: 15, step: 47
	action: tensor([[-0.2207, -0.1272, -1.3855, -0.0981, -1.0179, -0.5187, -0.1382]],
       dtype=torch.float64)
	q_value: tensor([[1.1371]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20313875300211082, distance: 1.021522758268562 entropy -2.643264998996884
epoch: 15, step: 48
	action: tensor([[ 0.0901,  0.3056, -1.3898, -0.0699, -0.4743,  0.6521,  0.0261]],
       dtype=torch.float64)
	q_value: tensor([[1.0563]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3990218816481712, distance: 0.8871274610961546 entropy -2.6406171049784524
epoch: 15, step: 49
	action: tensor([[-0.0949, -0.6296, -1.5807,  0.0761, -0.1589, -1.3493, -0.5322]],
       dtype=torch.float64)
	q_value: tensor([[0.9236]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17560236702299015, distance: 1.0390227609085145 entropy -2.5322595453584533
epoch: 15, step: 50
	action: tensor([[-0.3149, -0.9263, -1.8469, -0.0060, -0.5424, -0.1265,  0.1752]],
       dtype=torch.float64)
	q_value: tensor([[1.2772]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7557975660486587, distance: 1.516330653574092 entropy -2.400674219645677
epoch: 15, step: 51
	action: tensor([[-0.5213, -0.7946, -1.7213,  0.9882,  0.3414, -0.3845, -0.4607]],
       dtype=torch.float64)
	q_value: tensor([[1.1746]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2958391491787307, distance: 1.7339134758584935 entropy -2.4599603342037666
epoch: 15, step: 52
	action: tensor([[-0.5415, -1.9275, -2.1490,  0.9041, -0.7176, -0.1980,  0.3083]],
       dtype=torch.float64)
	q_value: tensor([[1.2296]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -2.2721139614911228
epoch: 15, step: 53
	action: tensor([[-0.4119,  0.0305, -0.8672,  0.0659,  0.3112, -0.2350, -0.5586]],
       dtype=torch.float64)
	q_value: tensor([[1.1116]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.19934270566165835, distance: 1.2532229583894248 entropy -3.028915596744333
epoch: 15, step: 54
	action: tensor([[-0.1550, -0.4458, -1.7096,  0.5728, -0.7969,  0.2732, -0.1980]],
       dtype=torch.float64)
	q_value: tensor([[1.0331]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7122797242569958, distance: 1.4974214512457034 entropy -2.4665204804503444
epoch: 15, step: 55
	action: tensor([[-0.2427, -0.8257, -1.7813,  0.7306, -0.5005,  0.2821,  0.0704]],
       dtype=torch.float64)
	q_value: tensor([[1.0990]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0980234684434733, distance: 1.6575317811231256 entropy -2.431206311259907
epoch: 15, step: 56
	action: tensor([[-0.5254, -0.1462, -1.9206,  0.7585, -1.3199,  0.7436, -0.1158]],
       dtype=torch.float64)
	q_value: tensor([[1.1147]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9505580512530549, distance: 1.5982184256615783 entropy -2.3673293623308367
epoch: 15, step: 57
	action: tensor([[-0.6867, -0.0251, -1.7940,  0.4866, -0.2179, -0.9071, -0.1753]],
       dtype=torch.float64)
	q_value: tensor([[1.1819]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5467127628792317, distance: 1.423185557446613 entropy -2.424698232741844
epoch: 15, step: 58
	action: tensor([[ 0.3393,  0.1483, -1.5108,  0.4499, -1.3052,  0.8288,  0.5074]],
       dtype=torch.float64)
	q_value: tensor([[1.2158]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30658806375925085, distance: 0.9529110210623036 entropy -2.570532348713644
epoch: 15, step: 59
	action: tensor([[ 0.5581, -0.0925, -1.7028,  0.1508, -0.7699, -0.4266,  0.1207]],
       dtype=torch.float64)
	q_value: tensor([[1.0250]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6955084859612404, distance: 0.6314577388102531 entropy -2.468679190470905
epoch: 15, step: 60
	action: tensor([[-0.3469,  0.1869, -1.6235, -0.3234, -0.4251, -0.4018,  0.8320]],
       dtype=torch.float64)
	q_value: tensor([[1.1127]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4219464859315716, distance: 0.8700429977684053 entropy -2.509684712742844
epoch: 15, step: 61
	action: tensor([[-0.0253, -0.9084, -1.2769, -0.4397, -1.3429,  0.5905,  0.1644]],
       dtype=torch.float64)
	q_value: tensor([[1.0609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5519701666411756, distance: 1.4256022678601918 entropy -2.7119798287633654
epoch: 15, step: 62
	action: tensor([[-0.6258,  0.0826, -1.7398,  0.8078,  0.1642, -0.7906,  0.7954]],
       dtype=torch.float64)
	q_value: tensor([[1.1357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7340939319830673, distance: 1.5069297350002984 entropy -2.4501866654107434
epoch: 15, step: 63
	action: tensor([[-0.0145, -0.0754, -1.5142, -0.1050, -0.8240, -0.5572,  0.3922]],
       dtype=torch.float64)
	q_value: tensor([[1.1310]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.462985788622705, distance: 0.8385898136272156 entropy -2.5687807938231684
epoch: 15, step: 64
	action: tensor([[ 0.3434,  0.2080, -0.8672,  0.1863, -0.5534,  0.4732, -0.6207]],
       dtype=torch.float64)
	q_value: tensor([[1.1116]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.596409817962265, distance: 0.7269875771896914 entropy -3.028915596744333
epoch: 15, step: 65
	action: tensor([[ 0.1754,  0.3885, -1.7477,  0.4900, -0.4783,  0.2243, -0.4869]],
       dtype=torch.float64)
	q_value: tensor([[0.9494]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2838702583648497, distance: 0.9683950228424247 entropy -2.447149836384951
epoch: 15, step: 66
	action: tensor([[ 0.2079, -0.5135, -1.7301,  0.2560,  1.5887, -0.7450, -0.2208]],
       dtype=torch.float64)
	q_value: tensor([[1.1251]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24924022869252638, distance: 1.279026886343816 entropy -2.4558483510697924
epoch: 15, step: 67
	action: tensor([[-0.2437,  0.4037, -2.4006,  0.8208,  0.7245,  0.5464,  0.3699]],
       dtype=torch.float64)
	q_value: tensor([[1.3151]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.40544122391255, distance: 1.3566350623466752 entropy -2.1771957400203177
epoch: 15, step: 68
	action: tensor([[-0.4307, -0.5159, -1.9608, -0.0484,  0.0703,  0.1583, -0.6284]],
       dtype=torch.float64)
	q_value: tensor([[1.1360]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6791062399480525, distance: 1.4828450817126324 entropy -2.3493256834593166
epoch: 15, step: 69
	action: tensor([[-0.6006, -1.5931, -1.8623, -0.0167, -0.6839, -0.8470,  0.6079]],
       dtype=torch.float64)
	q_value: tensor([[1.2352]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -2.393519613198721
epoch: 15, step: 70
	action: tensor([[-0.1745, -0.3594, -0.8672,  0.2683, -0.6834, -1.0849,  0.6264]],
       dtype=torch.float64)
	q_value: tensor([[1.1116]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19424745447459746, distance: 1.0272059737186794 entropy -3.028915596744333
epoch: 15, step: 71
	action: tensor([[-0.2556,  0.5217, -1.4690,  0.5166, -0.6456,  0.6550,  0.3726]],
       dtype=torch.float64)
	q_value: tensor([[0.9438]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2221864891409504, distance: 1.2651016800776107 entropy -2.594539138666662
epoch: 15, step: 72
	action: tensor([[-0.2017,  0.2156, -1.5606,  0.5042, -0.2388,  0.2346,  0.4334]],
       dtype=torch.float64)
	q_value: tensor([[0.9584]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20925250297613074, distance: 1.258389803912128 entropy -2.542953678939122
epoch: 15, step: 73
	action: tensor([[-0.5222,  0.1849, -1.5650,  0.6363,  0.1061, -0.2583, -0.1627]],
       dtype=torch.float64)
	q_value: tensor([[0.9797]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5807705903777196, distance: 1.4387691495320187 entropy -2.540765758644528
epoch: 15, step: 74
	action: tensor([[-0.5908,  0.2446, -1.6226,  0.4598, -0.3086,  0.5974, -0.0625]],
       dtype=torch.float64)
	q_value: tensor([[1.1063]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7427324116284371, distance: 1.510678496793645 entropy -2.5105663825385447
epoch: 15, step: 75
	action: tensor([[ 0.0051,  0.1859, -1.6414,  0.3506, -0.2843,  0.2125, -0.7494]],
       dtype=torch.float64)
	q_value: tensor([[1.0283]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08583768291704053, distance: 1.094128550664179 entropy -2.5004684355028997
epoch: 15, step: 76
	action: tensor([[-0.3078, -0.6827, -1.7922,  0.4884, -0.4723, -1.3566,  0.1479]],
       dtype=torch.float64)
	q_value: tensor([[1.1476]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2441750004954668, distance: 1.276431251285164 entropy -2.4261289986291183
epoch: 15, step: 77
	action: tensor([[-0.5227, -1.0366, -1.6905,  0.0455, -0.5838, -0.8236,  0.2693]],
       dtype=torch.float64)
	q_value: tensor([[1.2438]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.646009395201953, distance: 1.4681581765590834 entropy -2.4754786711723016
epoch: 15, step: 78
	action: tensor([[-0.2334, -0.5954, -1.6474, -0.2706, -1.0759,  0.3950,  0.3528]],
       dtype=torch.float64)
	q_value: tensor([[1.2050]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.41843034838553295, distance: 1.362889672592329 entropy -2.4970960610091035
epoch: 15, step: 79
	action: tensor([[-0.4226,  0.3126, -1.5982, -0.3903, -0.8019,  0.6225,  0.3414]],
       dtype=torch.float64)
	q_value: tensor([[1.1319]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.049953220162547995, distance: 1.1153962714608892 entropy -2.522583262628861
epoch: 15, step: 80
	action: tensor([[-0.1076, -0.3289, -1.3608,  0.1050,  0.3846,  0.2512,  0.1513]],
       dtype=torch.float64)
	q_value: tensor([[1.0083]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1958425112219504, distance: 1.2513929021734551 entropy -2.6582676923659685
epoch: 15, step: 81
	action: tensor([[-0.2521, -0.4031, -1.8308,  1.1362, -0.0211, -0.1055, -0.2209]],
       dtype=torch.float64)
	q_value: tensor([[0.9668]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8980996890188411, distance: 1.576580687359823 entropy -2.407992551481752
epoch: 15, step: 82
	action: tensor([[-0.1073, -0.3062, -2.0123,  0.6760,  0.0986, -0.1033,  0.2135]],
       dtype=torch.float64)
	q_value: tensor([[1.1433]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.542392603852671, distance: 1.4211966014531494 entropy -2.327787676321639
epoch: 15, step: 83
	action: tensor([[-0.4474, -0.5944, -1.8588,  0.7097, -1.0064, -0.3881,  0.2350]],
       dtype=torch.float64)
	q_value: tensor([[1.1593]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0425264157809915, distance: 1.635462288074677 entropy -2.3950695999233185
epoch: 15, step: 84
	action: tensor([[-0.2702, -0.9040, -1.6598, -0.6717, -0.2565, -0.7977,  0.1528]],
       dtype=torch.float64)
	q_value: tensor([[1.1594]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24208855027177312, distance: 0.9962444781450689 entropy -2.4908617830378668
epoch: 15, step: 85
	action: tensor([[-0.1098,  0.1411, -1.6748,  0.3586, -1.1534,  0.1471, -0.9507]],
       dtype=torch.float64)
	q_value: tensor([[1.2286]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.022814240106881156, distance: 1.1573243112521228 entropy -2.48285358038732
epoch: 15, step: 86
	action: tensor([[-0.7698,  0.1468, -1.7019, -0.0468, -0.3039, -0.0580, -0.7880]],
       dtype=torch.float64)
	q_value: tensor([[1.1839]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5254075573000627, distance: 1.4133497294578372 entropy -2.4694708875615965
epoch: 15, step: 87
	action: tensor([[-0.5397, -0.4534, -1.5095,  0.0038, -0.2776, -0.9202, -0.1180]],
       dtype=torch.float64)
	q_value: tensor([[1.1645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24173007160618898, distance: 1.2751764767109202 entropy -2.571268985499402
epoch: 15, step: 88
	action: tensor([[-0.2205,  0.4665, -1.5261,  0.8676,  0.2584, -0.6415, -0.6780]],
       dtype=torch.float64)
	q_value: tensor([[1.1524]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.062218420900149285, distance: 1.1794067461511675 entropy -2.5620121059565126
epoch: 15, step: 89
	action: tensor([[-0.7604,  0.1921, -1.8362,  0.4346,  0.2873, -0.6389,  1.0610]],
       dtype=torch.float64)
	q_value: tensor([[1.2062]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6035286122949337, distance: 1.4490889677109904 entropy -2.40553465828383
epoch: 15, step: 90
	action: tensor([[-0.1732,  0.1840, -1.3906,  0.4414, -1.3353, -0.1088,  0.9883]],
       dtype=torch.float64)
	q_value: tensor([[1.1544]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0164322918321802, distance: 1.1537080429813935 entropy -2.640119941877736
epoch: 15, step: 91
	action: tensor([[-0.3346, -0.5576, -1.4512, -0.1180, -0.3812, -0.0638,  0.2009]],
       dtype=torch.float64)
	q_value: tensor([[1.0512]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.43986361602561797, distance: 1.3731480740029198 entropy -2.6041546537874765
epoch: 15, step: 92
	action: tensor([[-0.3001, -0.0978, -1.5719,  0.4300, -0.5590,  0.7251,  0.8666]],
       dtype=torch.float64)
	q_value: tensor([[1.0548]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5953305101117123, distance: 1.4453799592668546 entropy -2.5369115588038538
epoch: 15, step: 93
	action: tensor([[-0.6951,  0.1961, -1.6918,  0.0574, -0.7700, -0.6586,  0.4512]],
       dtype=torch.float64)
	q_value: tensor([[0.9915]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2041691902345535, distance: 1.2557420832782948 entropy -2.4744620604400107
epoch: 15, step: 94
	action: tensor([[-0.6431, -0.1690, -1.2195,  0.3062, -0.8047,  0.2588,  0.0808]],
       dtype=torch.float64)
	q_value: tensor([[1.1450]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9010719765450674, distance: 1.5778146104431223 entropy -2.750341719011876
epoch: 15, step: 95
	action: tensor([[-0.2008, -0.2944, -1.5339,  0.4804, -0.3763,  0.2788, -0.5041]],
       dtype=torch.float64)
	q_value: tensor([[0.9650]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5574912489304282, distance: 1.4281357829811578 entropy -2.5576723957633924
epoch: 15, step: 96
	action: tensor([[-0.1443,  0.0632, -0.8672, -0.0441, -0.2843,  0.4678, -0.3225]],
       dtype=torch.float64)
	q_value: tensor([[1.1116]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09946314047047278, distance: 1.085944024714209 entropy -3.028915596744333
epoch: 15, step: 97
	action: tensor([[-0.3229, -0.7904, -1.6358, -0.4350, -0.5286, -0.1498, -0.1633]],
       dtype=torch.float64)
	q_value: tensor([[0.8862]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2920260889722537, distance: 1.3007455082058532 entropy -2.5034413406091836
epoch: 15, step: 98
	action: tensor([[-0.1982, -1.4563, -1.6582,  0.1083, -1.0256,  0.1487, -0.3432]],
       dtype=torch.float64)
	q_value: tensor([[1.1671]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -2.4915107206866374
epoch: 15, step: 99
	action: tensor([[-0.0102,  0.0641, -0.8672,  0.1258,  0.2645, -0.3137, -0.0826]],
       dtype=torch.float64)
	q_value: tensor([[1.1116]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3066777828843461, distance: 0.9528493714826689 entropy -3.028915596744333
epoch: 15, step: 100
	action: tensor([[-0.7892, -0.0890, -1.6493,  0.0832, -0.3214, -1.3098,  0.3477]],
       dtype=torch.float64)
	q_value: tensor([[0.9448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15430280715324973, distance: 1.2294661303445085 entropy -2.4968315623579316
epoch: 15, step: 101
	action: tensor([[-0.3147, -0.2588, -1.2933,  0.3970, -0.1131, -0.1969,  0.5734]],
       dtype=torch.float64)
	q_value: tensor([[1.2109]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.41828424418619004, distance: 1.3628194791448522 entropy -2.7010114735509343
epoch: 15, step: 102
	action: tensor([[-0.6359, -0.6175, -1.5603,  0.7422,  0.2931, -0.5522,  0.5135]],
       dtype=torch.float64)
	q_value: tensor([[0.9781]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1783432442528037, distance: 1.6889618886138749 entropy -2.543481624464348
epoch: 15, step: 103
	action: tensor([[-0.2106, -0.3271, -1.7666,  0.8093, -1.1155, -0.7650, -1.0861]],
       dtype=torch.float64)
	q_value: tensor([[1.1104]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3792953996524695, distance: 1.3439568869536231 entropy -2.4384752446917095
epoch: 15, step: 104
	action: tensor([[-0.6551, -1.3182, -1.8918, -0.9654, -0.3448, -0.5793,  0.5070]],
       dtype=torch.float64)
	q_value: tensor([[1.2585]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07701391129051383, distance: 1.0993962998296642 entropy -2.38007779717327
epoch: 15, step: 105
	action: tensor([[-0.3713,  0.3970, -1.7265, -0.3020, -0.7495, -0.4616,  0.1475]],
       dtype=torch.float64)
	q_value: tensor([[1.3126]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.49249707918144925, distance: 0.8152221408917966 entropy -2.456605991811457
epoch: 15, step: 106
	action: tensor([[-0.2515,  0.0618, -1.2125,  0.7472, -0.3147, -0.8157,  0.1179]],
       dtype=torch.float64)
	q_value: tensor([[1.1041]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.009656229340513134, distance: 1.1388058262042684 entropy -2.7554562769981152
epoch: 15, step: 107
	action: tensor([[-0.1487, -0.2707, -1.5378, -0.0589, -0.5561,  0.2380,  0.2197]],
       dtype=torch.float64)
	q_value: tensor([[1.0174]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16153077671322968, distance: 1.2333094354718726 entropy -2.555936655549336
epoch: 15, step: 108
	action: tensor([[-0.3867, -0.7592, -1.5601,  0.0294, -0.1536, -0.1694, -0.3971]],
       dtype=torch.float64)
	q_value: tensor([[1.0322]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7202189888350854, distance: 1.5008889560341623 entropy -2.543333931940892
epoch: 15, step: 109
	action: tensor([[-0.7176,  0.0069, -1.7974, -0.6534,  0.7066,  0.0249,  0.7310]],
       dtype=torch.float64)
	q_value: tensor([[1.1522]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05294587847743304, distance: 1.174247698801486 entropy -2.4237026576912792
epoch: 15, step: 110
	action: tensor([[-0.3707, -1.1372, -1.5200,  0.3396, -0.4447, -0.3540, -0.2697]],
       dtype=torch.float64)
	q_value: tensor([[1.0596]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0559176438747535, distance: 1.6408147449468506 entropy -2.5649935108921715
epoch: 15, step: 111
	action: tensor([[-0.5370, -0.2874, -1.8819,  0.2456,  0.1358, -1.2036,  0.4715]],
       dtype=torch.float64)
	q_value: tensor([[1.1636]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.19276943659369206, distance: 1.2497839538625484 entropy -2.3847001632728206
epoch: 15, step: 112
	action: tensor([[ 0.1568, -0.4081, -1.5328,  0.6927, -0.8647, -1.3472,  0.7568]],
       dtype=torch.float64)
	q_value: tensor([[1.2590]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3403923976409188, distance: 0.9293932618965619 entropy -2.558177284416088
epoch: 15, step: 113
	action: tensor([[ 0.1249,  0.1818, -1.6326,  0.1557, -1.2893,  0.6102,  0.4494]],
       dtype=torch.float64)
	q_value: tensor([[1.1042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2863454001407978, distance: 0.9667200542412165 entropy -2.50518812362839
epoch: 15, step: 114
	action: tensor([[-0.1450, -1.1143, -1.5709, -0.1352,  0.6468, -0.4518, -0.5840]],
       dtype=torch.float64)
	q_value: tensor([[1.0708]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6101612916697587, distance: 1.4520828101912282 entropy -2.5370718547247555
epoch: 15, step: 115
	action: tensor([[-0.5821, -0.9910, -2.2124,  0.4393,  0.4568,  0.2566,  0.6079]],
       dtype=torch.float64)
	q_value: tensor([[1.3048]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.3630317207643783, distance: 1.7591038091978097 entropy -2.2469783555344636
epoch: 15, step: 116
	action: tensor([[-0.2726, -1.0671, -2.0610,  0.9584, -2.2217,  1.4920,  0.9625]],
       dtype=torch.float64)
	q_value: tensor([[1.2175]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7379925211962639, distance: 1.5086227233734413 entropy -2.3070050122473944
epoch: 15, step: 117
	action: tensor([[-0.1687, -0.8110, -2.2605,  0.0783, -0.2690,  0.6768,  0.9046]],
       dtype=torch.float64)
	q_value: tensor([[1.3394]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8153866835670309, distance: 1.5418469498919631 entropy -2.227228767651435
epoch: 15, step: 118
	action: tensor([[-0.3326, -0.3128, -1.9406,  0.2324, -1.2055,  0.5665, -0.6163]],
       dtype=torch.float64)
	q_value: tensor([[1.1813]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6094301939628337, distance: 1.4517531118706886 entropy -2.3578137898128806
epoch: 15, step: 119
	action: tensor([[-0.6571, -0.3269, -1.7469,  0.7808, -1.1031, -1.6527,  0.1642]],
       dtype=torch.float64)
	q_value: tensor([[1.2227]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.38204034587275393, distance: 1.345293531656743 entropy -2.447255174552861
epoch: 15, step: 120
	action: tensor([[-0.4808,  0.0618, -1.5346,  0.4329, -0.5541, -0.0035, -0.1261]],
       dtype=torch.float64)
	q_value: tensor([[1.2507]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5722465528606033, distance: 1.4348847465514012 entropy -2.556826656403542
epoch: 15, step: 121
	action: tensor([[-0.1183, -0.8503, -1.5181, -0.0696, -0.5190, -0.3170, -0.1189]],
       dtype=torch.float64)
	q_value: tensor([[1.0467]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3983849480366026, distance: 1.3532251591621416 entropy -2.5665108203824936
epoch: 15, step: 122
	action: tensor([[ 0.0037,  0.3285, -1.7099,  0.3875,  0.7656,  0.1201,  0.5582]],
       dtype=torch.float64)
	q_value: tensor([[1.1392]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19106080059791297, distance: 1.0292352070576094 entropy -2.4657643357057073
epoch: 15, step: 123
	action: tensor([[ 0.2544,  0.2534, -1.7769, -0.2130, -2.1907,  0.7330,  0.0109]],
       dtype=torch.float64)
	q_value: tensor([[0.9682]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7559096978179397, distance: 0.5653689531091782 entropy -2.4331875905152804
epoch: 15, step: 124
	action: tensor([[ 0.1917, -1.0227, -1.5905,  0.4254, -0.1984,  0.7457,  0.4334]],
       dtype=torch.float64)
	q_value: tensor([[1.2342]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7473269010749737, distance: 1.5126685406339355 entropy -2.525837575615301
epoch: 15, step: 125
	action: tensor([[-0.5650, -1.4347, -2.0993, -0.2202, -1.0530,  0.0520,  0.7895]],
       dtype=torch.float64)
	q_value: tensor([[1.0870]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -2.2910957016260665
epoch: 15, step: 126
	action: tensor([[-0.1963,  0.1747, -0.8672, -0.1483,  0.0730, -0.1883, -0.0257]],
       dtype=torch.float64)
	q_value: tensor([[1.1116]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24784923696655514, distance: 0.9924511595371441 entropy -3.028915596744333
epoch: 15, step: 127
	action: tensor([[ 0.0594,  0.0432, -1.4970,  0.0683,  0.0810,  0.6041,  0.1597]],
       dtype=torch.float64)
	q_value: tensor([[0.8794]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2202099331976688, distance: 1.010521464088808 entropy -2.578857368400206
LOSS epoch 15 actor 42.328874301038226 critic 544.2826042948166 entropy 0.1
epoch: 16, step: 0
	action: tensor([[-0.8723, -2.5466, -4.9898,  1.7125, -1.6224,  0.4112,  0.0214]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 16, step: 1
	action: tensor([[-0.0470,  0.8715, -4.9898,  0.9443, -1.1929, -0.3751,  0.2034]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.5835755886604405 entropy -1.8704687425011068
epoch: 16, step: 2
	action: tensor([[-1.3072, -1.9699, -4.9898, -0.1258, -2.5787, -0.2077,  0.2832]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 16, step: 3
	action: tensor([[-0.0226,  0.5527, -4.9898,  0.0165, -0.0733,  0.1195,  1.1629]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7575414390396603 entropy -1.8704687425011068
epoch: 16, step: 4
	action: tensor([[-1.6542e+00, -1.9930e+00, -4.9898e+00,  1.3478e+00, -5.1673e-01,
         -1.0318e+00, -1.5457e-03]], dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 16, step: 5
	action: tensor([[-0.4240,  0.4234, -4.9898,  0.9727, -2.1595, -0.4294,  0.2544]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.5776718914131804 entropy -1.8704687425011068
epoch: 16, step: 6
	action: tensor([[-1.1831,  0.3843, -4.9898, -0.2202, -0.9677, -0.5480, -0.9880]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.6988456421386235 entropy -1.8704687425011068
epoch: 16, step: 7
	action: tensor([[-1.0639, -0.7693, -4.9898, -0.3285, -3.1693,  0.0903,  0.8088]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.8664968608348749 entropy -1.8704687425011068
epoch: 16, step: 8
	action: tensor([[ 1.2466, -2.0102, -4.9898,  1.7582, -2.2637, -0.3708,  1.7974]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 16, step: 9
	action: tensor([[-1.1483, -2.5343, -4.9898,  1.1851, -1.5562, -0.6084,  0.1771]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 16, step: 10
	action: tensor([[-0.6213, -0.0630, -4.9898,  0.0239, -0.5097,  0.2150,  0.6722]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3700244693750538 entropy -1.8704687425011068
epoch: 16, step: 11
	action: tensor([[-0.5335, -0.0459, -4.9898,  0.1406, -1.8375, -0.2033, -1.1636]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2997036730669156 entropy -1.8704687425011068
epoch: 16, step: 12
	action: tensor([[ 1.8025, -0.4069, -4.9898,  0.3847, -0.5570,  0.3713,  1.1671]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 16, step: 13
	action: tensor([[-1.8835, -1.1405, -4.9898, -0.0590, -0.2844, -1.5055,  1.5702]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 16, step: 14
	action: tensor([[-1.1988, -1.4618, -4.9898,  1.0109, -1.0907,  0.1407,  0.5621]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 16, step: 15
	action: tensor([[ 1.3750, -1.6158, -4.9898, -0.7309, -1.3272,  0.2201,  0.6739]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 16, step: 16
	action: tensor([[-0.5771,  1.2045, -4.9898, -0.2373, -2.0710, -0.6092,  0.1857]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 16, step: 17
	action: tensor([[-1.8656, -1.7932, -4.9898,  2.0481, -1.1298, -0.7001,  0.9190]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 16, step: 18
	action: tensor([[-2.4570,  2.0198, -4.9898,  0.4530, -2.3450, -0.0978,  0.1060]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 16, step: 19
	action: tensor([[-0.3065, -1.1588, -4.9898,  1.1207, -0.8545,  0.2066, -1.0608]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9778218388982924 entropy -1.8704687425011068
epoch: 16, step: 20
	action: tensor([[-2.4632, -0.2814, -4.9898, -0.4449, -1.0778, -0.1814, -1.4248]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 16, step: 21
	action: tensor([[-0.3994, -1.8201, -4.9898,  0.5965, -1.5429, -1.5130,  0.0170]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 16, step: 22
	action: tensor([[-0.1948, -1.9504, -4.9898,  0.5604, -1.6135, -0.9524, -0.7120]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 16, step: 23
	action: tensor([[-0.2752, -0.6279, -4.9898,  0.7699, -1.7205, -0.1198,  0.7981]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0740553876504868 entropy -1.8704687425011068
epoch: 16, step: 24
	action: tensor([[-1.9423, -0.2283, -4.9898,  2.3997, -2.3788, -1.0264,  0.1243]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 16, step: 25
	action: tensor([[-2.1390, -0.4701, -4.9898,  1.5294, -2.6986, -0.2854, -0.0929]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 16, step: 26
	action: tensor([[ 0.6626, -2.0059, -4.9898, -0.1441, -1.4575,  1.4272,  1.7791]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 16, step: 27
	action: tensor([[ 0.6036, -2.7338, -4.9898,  0.5654, -0.7276, -0.1135,  1.6609]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 16, step: 28
	action: tensor([[-0.6454, -0.6752, -4.9898,  2.0448, -0.5249,  1.5181,  1.4527]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7575767754164195 entropy -1.8704687425011068
epoch: 16, step: 29
	action: tensor([[-0.1238, -2.1025, -4.9898,  0.1908, -0.2084, -0.4682, -1.3043]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 16, step: 30
	action: tensor([[-0.3861, -0.6533, -4.9898,  0.1446, -1.7654,  0.9123,  2.8776]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3790909283019996 entropy -1.8704687425011068
epoch: 16, step: 31
	action: tensor([[-1.5926, -0.0643, -4.9898,  0.9533, -0.9260, -2.1459,  1.0129]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.5707500443668747 entropy -1.8704687425011068
epoch: 16, step: 32
	action: tensor([[ 0.7248,  0.1663, -4.9898,  1.0088, -0.2891,  0.3978, -0.7957]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.4752988877958995 entropy -1.8704687425011068
epoch: 16, step: 33
	action: tensor([[-0.6564, -2.0549, -4.9898,  2.9965, -0.6540,  1.3801,  0.6678]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 16, step: 34
	action: tensor([[-0.1556, -0.3228, -4.9898,  1.0509,  0.1400, -0.7368,  0.7524]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8585757028045693 entropy -1.8704687425011068
epoch: 16, step: 35
	action: tensor([[-1.4447, -1.2527, -4.9898,  1.0942, -3.0706, -1.6922, -0.3062]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 16, step: 36
	action: tensor([[-0.2442, -0.0538, -4.9898, -0.3904, -0.2939,  0.8698, -0.0442]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1181540847887148 entropy -1.8704687425011068
epoch: 16, step: 37
	action: tensor([[ 0.6037, -1.6515, -4.9898, -2.1003, -0.7575, -0.5567, -0.0964]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 16, step: 38
	action: tensor([[-1.6260, -0.3645, -4.9898,  1.7896, -1.1689,  0.4416,  0.5946]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 16, step: 39
	action: tensor([[-0.6392, -2.6042, -4.9898,  0.9885, -0.6206, -0.5843, -1.4771]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 16, step: 40
	action: tensor([[ 1.2158, -0.8937, -4.9898,  0.5731, -0.3785, -3.2937,  0.7675]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 16, step: 41
	action: tensor([[-1.4788, -1.0007, -4.9898, -0.0902, -2.5175,  0.3022,  0.8241]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9230940492937996 entropy -1.8704687425011068
epoch: 16, step: 42
	action: tensor([[-0.3463,  0.1477, -4.9898,  0.8279, -3.0139,  1.1669,  1.9311]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9886443000878135 entropy -1.8704687425011068
epoch: 16, step: 43
	action: tensor([[-1.0570e+00,  5.7905e-01, -4.9898e+00,  3.6024e-01, -2.7581e-02,
          4.5246e-01,  4.7839e-03]], dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1584508699563645 entropy -1.8704687425011068
epoch: 16, step: 44
	action: tensor([[-1.1612, -1.5512, -4.9898,  0.1812, -2.7568,  0.4597,  0.8391]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 16, step: 45
	action: tensor([[ 0.8812, -0.9009, -4.9898,  0.6113, -1.4410,  0.5725,  0.5314]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0130306176663253 entropy -1.8704687425011068
epoch: 16, step: 46
	action: tensor([[ 0.3232, -2.0707, -4.9898,  0.6793, -1.8786, -1.0845,  0.5553]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 16, step: 47
	action: tensor([[-0.1128, -0.5260, -4.9898, -0.7251, -1.0002, -1.0570, -1.2515]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.5056229590529335 entropy -1.8704687425011068
epoch: 16, step: 48
	action: tensor([[-0.1963, -0.5697, -4.9898,  2.8129, -1.0857, -0.8183, -1.9250]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 16, step: 49
	action: tensor([[ 0.2960, -2.3600, -4.9898,  0.2784, -1.4759, -0.8198,  0.1940]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 16, step: 50
	action: tensor([[-1.9964,  0.0648, -4.9898,  1.0598, -1.0149, -0.6743,  0.5705]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 16, step: 51
	action: tensor([[-1.3952,  1.9331, -4.9898, -0.7701, -1.4482,  0.4080,  1.1281]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 16, step: 52
	action: tensor([[ 0.7073, -1.5903, -4.9898,  0.8191, -2.6965, -0.6264, -0.6839]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 16, step: 53
	action: tensor([[-1.0438,  0.0558, -4.9898, -0.0840, -1.1230,  0.9845, -0.9691]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4617507178068831 entropy -1.8704687425011068
epoch: 16, step: 54
	action: tensor([[-1.0105, -1.0287, -4.9898, -0.7634,  0.1812, -1.2576,  2.2068]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.6786033495292756 entropy -1.8704687425011068
epoch: 16, step: 55
	action: tensor([[-2.3141, -1.0600, -4.9898,  1.5431, -0.9726,  0.8960,  0.3842]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 16, step: 56
	action: tensor([[-0.8981, -2.4052, -4.9898,  2.1881, -0.7568,  1.5124,  1.1999]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 16, step: 57
	action: tensor([[-0.6696, -1.0060, -4.9898,  0.2321, -0.4387, -0.7736,  0.3510]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.6552624730851706 entropy -1.8704687425011068
epoch: 16, step: 58
	action: tensor([[-0.2173, -0.3169, -4.9898,  0.6547, -1.6808, -0.7622, -0.6131]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0015093847787255 entropy -1.8704687425011068
epoch: 16, step: 59
	action: tensor([[-0.9785,  0.6191, -4.9898,  1.8521, -2.8295,  1.5477,  1.2933]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8015616439402636 entropy -1.8704687425011068
epoch: 16, step: 60
	action: tensor([[-1.1675e-03, -2.2832e+00, -4.9898e+00, -6.5319e-01, -1.5724e+00,
         -1.4369e+00, -1.2623e+00]], dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 16, step: 61
	action: tensor([[-0.9270,  0.0773, -4.9898,  3.3984, -1.1990, -1.4634,  1.1397]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 16, step: 62
	action: tensor([[ 0.8868, -0.2624, -4.9898, -0.3406,  0.0332, -1.1346,  1.2449]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8698627260620945 entropy -1.8704687425011068
epoch: 16, step: 63
	action: tensor([[ 0.6475,  2.1082, -4.9898,  0.4976, -0.2593, -1.1759,  0.3970]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 16, step: 64
	action: tensor([[ 0.0632,  0.6552, -4.9898, -0.6589, -1.5530, -0.1353,  0.1278]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9950474795666477 entropy -1.8704687425011068
epoch: 16, step: 65
	action: tensor([[ 1.2893, -2.2799, -4.9898,  1.4554,  0.8837, -1.7025,  1.4025]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 16, step: 66
	action: tensor([[ 0.0991,  0.4603, -4.9898, -0.3336, -0.8287,  0.2482, -0.3692]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8503222614073068 entropy -1.8704687425011068
epoch: 16, step: 67
	action: tensor([[-2.3143, -0.3031, -4.9898,  0.7186, -1.8318,  0.8420,  0.2656]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 16, step: 68
	action: tensor([[ 0.9151, -0.6507, -4.9898,  0.6995, -0.4391,  0.8605,  0.4720]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8113120174343335 entropy -1.8704687425011068
epoch: 16, step: 69
	action: tensor([[ 0.3151, -1.1074, -4.9898,  0.5105, -0.5890, -2.5375,  0.5443]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 16, step: 70
	action: tensor([[ 0.2642, -1.0149, -4.9898,  0.4005, -2.3515, -1.4347,  1.8754]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1195045768328553 entropy -1.8704687425011068
epoch: 16, step: 71
	action: tensor([[ 0.0199, -0.0643, -4.9898,  1.1361, -0.9914, -0.2613, -2.1162]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.4976117745160633 entropy -1.8704687425011068
epoch: 16, step: 72
	action: tensor([[-0.1527,  1.4913, -4.9898, -0.3661, -2.3163,  0.0840, -0.9884]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 16, step: 73
	action: tensor([[-0.9224, -0.4766, -4.9898,  2.1351,  0.5814,  1.2717, -0.1760]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7899373927564786 entropy -1.8704687425011068
epoch: 16, step: 74
	action: tensor([[ 0.6265, -1.8357, -4.9898,  0.8605, -0.8088,  0.5998,  1.4672]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 16, step: 75
	action: tensor([[-0.8642, -1.3591, -4.9898,  0.0575, -1.1414, -0.3541,  1.7509]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 16, step: 76
	action: tensor([[ 0.2686, -0.9585, -4.9898, -0.0197, -1.8408, -1.5591,  0.4112]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2761910769900942 entropy -1.8704687425011068
epoch: 16, step: 77
	action: tensor([[-1.7689, -1.8808, -4.9898,  1.3075, -0.3931, -0.5217, -0.6933]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 16, step: 78
	action: tensor([[-1.0813, -0.7029, -4.9898,  0.8863, -1.3967,  0.0957, -2.7654]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4425030924215907 entropy -1.8704687425011068
epoch: 16, step: 79
	action: tensor([[ 0.4871, -1.4976, -4.9898,  2.7562, -1.6931,  0.6495,  0.9029]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 16, step: 80
	action: tensor([[-0.5287, -1.3280, -4.9898, -0.4769, -2.2773, -0.3147,  0.0934]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 16, step: 81
	action: tensor([[-1.6785, -0.6989, -4.9898,  2.5412, -0.9371, -0.4793,  0.3903]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 16, step: 82
	action: tensor([[-1.1156, -0.4146, -4.9898,  2.0943,  0.0696, -0.5471,  1.1904]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.843396759902565 entropy -1.8704687425011068
epoch: 16, step: 83
	action: tensor([[-0.6992,  0.2624, -4.9898,  0.8111, -1.1208,  2.2664, -0.3941]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 16, step: 84
	action: tensor([[-1.0655,  0.3051, -4.9898,  1.2445, -1.0452, -0.1934, -0.0152]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9928958365212974 entropy -1.8704687425011068
epoch: 16, step: 85
	action: tensor([[ 1.1177, -2.3373, -4.9898,  1.6818, -0.2974, -0.0470,  0.6801]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 16, step: 86
	action: tensor([[-0.9047, -1.9179, -4.9898,  0.0654, -1.6950, -0.3963,  1.5350]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 16, step: 87
	action: tensor([[-0.8490, -0.0931, -4.9898, -0.0795,  0.0522, -0.9441, -1.0601]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.6591332802885923 entropy -1.8704687425011068
epoch: 16, step: 88
	action: tensor([[-2.5940, -0.4983, -4.9898,  1.9188, -1.5750, -0.1914,  1.7717]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 16, step: 89
	action: tensor([[-2.6355, -0.2111, -4.9898,  0.9901,  0.7364, -2.3190, -0.7094]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 16, step: 90
	action: tensor([[-1.8747,  0.9548, -4.9898,  1.9573, -2.1406, -1.3613,  1.4502]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 16, step: 91
	action: tensor([[ 0.0169, -0.2216, -4.9898,  0.0377, -1.2346, -0.4261,  1.6728]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1206409642078685 entropy -1.8704687425011068
epoch: 16, step: 92
	action: tensor([[-0.1600, -0.5909, -4.9898,  2.7914, -0.4329,  0.4268,  1.5592]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 16, step: 93
	action: tensor([[-1.5368, -2.0784, -4.9898,  0.3405, -0.9293,  0.6845,  1.2475]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 16, step: 94
	action: tensor([[-1.4558, -2.6059, -4.9898,  0.8594, -1.4672,  0.0515, -0.2458]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 16, step: 95
	action: tensor([[-0.4830, -1.0844, -4.9898, -0.0112, -2.2729,  1.3659, -0.3043]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.571702346826569 entropy -1.8704687425011068
epoch: 16, step: 96
	action: tensor([[-0.5306, -0.9006, -4.9898,  0.8670,  1.8276, -1.4433,  1.1323]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0859039029636204 entropy -1.8704687425011068
epoch: 16, step: 97
	action: tensor([[-1.4335,  1.4386, -4.9898,  2.1512, -1.9057,  0.6220,  1.8441]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 16, step: 98
	action: tensor([[ 0.8602, -0.5993, -4.9898,  1.9897, -1.6291, -1.7092,  0.2837]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0785390170473623 entropy -1.8704687425011068
epoch: 16, step: 99
	action: tensor([[-0.7387, -0.2450, -4.9898, -1.4961, -2.2844, -1.3990, -0.1625]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.5859646293662475 entropy -1.8704687425011068
epoch: 16, step: 100
	action: tensor([[ 0.3034, -1.2788, -4.9898,  1.5076, -0.7455,  0.5802, -0.0457]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 16, step: 101
	action: tensor([[ 0.4920, -0.3560, -4.9898,  0.0288, -1.5586,  0.0535,  0.6523]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9015069440536968 entropy -1.8704687425011068
epoch: 16, step: 102
	action: tensor([[-2.6586, -1.1418, -4.9898,  1.5128, -1.5652,  0.3916,  0.2013]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 16, step: 103
	action: tensor([[ 0.2985, -1.4203, -4.9898, -0.7086,  0.3124, -0.5144, -0.4154]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 16, step: 104
	action: tensor([[ 1.0033, -1.7095, -4.9898,  0.3651, -0.5138, -2.2724,  0.5902]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 16, step: 105
	action: tensor([[ 0.2240, -0.7183, -4.9898,  2.0731, -0.1230,  0.0064, -0.2455]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.5551308253436683 entropy -1.8704687425011068
epoch: 16, step: 106
	action: tensor([[-0.7402, -1.0519, -4.9898,  1.0153, -1.4608,  0.6084, -0.1750]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2317061442473984 entropy -1.8704687425011068
epoch: 16, step: 107
	action: tensor([[-1.0775, -1.5015, -4.9898,  0.4165, -0.8846,  0.5860,  0.3906]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 16, step: 108
	action: tensor([[-0.3413, -1.5375, -4.9898, -0.5086, -2.4105, -0.1057, -0.4577]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 16, step: 109
	action: tensor([[ 0.5983, -1.5619, -4.9898,  1.3845, -0.1407,  0.1388, -0.8553]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 16, step: 110
	action: tensor([[ 1.4363, -1.3005, -4.9898, -0.2080, -0.4004, -1.6276,  0.7400]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 16, step: 111
	action: tensor([[ 0.1321, -1.9349, -4.9898, -0.0428, -0.3758, -1.6483, -0.3170]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 16, step: 112
	action: tensor([[-0.3465, -1.7024, -4.9898,  1.6614, -0.2280,  0.0211,  0.8621]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 16, step: 113
	action: tensor([[-0.9118, -0.5511, -4.9898,  0.2611, -0.3395, -0.4091,  0.6906]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.6793957041975514 entropy -1.8704687425011068
epoch: 16, step: 114
	action: tensor([[ 0.3783, -0.2509, -4.9898,  2.8110, -1.8045, -0.8768, -0.7897]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 16, step: 115
	action: tensor([[-2.2310, -1.6455, -4.9898, -0.8094, -1.7566,  1.0668,  0.0274]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 16, step: 116
	action: tensor([[-0.2711, -1.1974, -4.9898,  1.4236, -1.3133,  1.2684, -0.1539]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8366246880259995 entropy -1.8704687425011068
epoch: 16, step: 117
	action: tensor([[-0.4942,  0.9125, -4.9898,  0.9829, -2.2066, -2.4432,  0.3675]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 16, step: 118
	action: tensor([[-2.0636, -2.1390, -4.9898, -0.3576, -1.7365, -0.2717, -0.1833]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 16, step: 119
	action: tensor([[-1.6325, -0.1881, -4.9898,  0.6876, -0.6601, -0.3112,  1.4859]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 16, step: 120
	action: tensor([[-0.5628, -1.0398, -4.9898,  2.7899, -2.7216,  0.6283, -0.1181]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 16, step: 121
	action: tensor([[-1.0062, -1.2502, -4.9898,  0.2455,  0.8336,  0.6252,  0.4424]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 16, step: 122
	action: tensor([[-0.1196,  0.0753, -4.9898, -0.8180, -0.5318, -0.0072, -1.3279]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3362788643487593 entropy -1.8704687425011068
epoch: 16, step: 123
	action: tensor([[-0.1159,  1.0342, -4.9898,  1.1048, -1.0182,  0.3078, -2.0187]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.3824214820164674 entropy -1.8704687425011068
epoch: 16, step: 124
	action: tensor([[ 0.5391, -1.3833, -4.9898,  1.4524, -0.2062,  1.4136,  0.6210]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 16, step: 125
	action: tensor([[-1.8512,  0.3358, -4.9898,  0.4222, -1.0170,  0.0786,  0.4580]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 16, step: 126
	action: tensor([[-1.2024, -1.2485, -4.9898, -0.1702, -1.7000, -0.2063, -0.6455]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 16, step: 127
	action: tensor([[-0.4374, -2.4699, -4.9898,  0.6212, -1.0398, -0.9928, -0.0582]],
       dtype=torch.float64)
	q_value: tensor([[0.3284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
LOSS epoch 16 actor 1453.5198693217967 critic 2532.946090581372 entropy 100
epoch: 17, step: 0
	action: tensor([[-1.6086, -2.0411, -6.2800,  1.6601, -3.9526, -1.4168,  0.5171]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 17, step: 1
	action: tensor([[ 0.7078, -0.4651, -6.2800,  2.0159, -2.7999, -0.6551, -0.6024]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.559160097099134 entropy -1.8704687425011068
epoch: 17, step: 2
	action: tensor([[-1.8227, -1.6349, -6.2800,  2.6952, -2.5498, -0.9703,  0.8611]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 17, step: 3
	action: tensor([[-1.0571, -2.8538, -6.2800,  2.5427, -2.5689,  0.6751,  1.9801]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 17, step: 4
	action: tensor([[-1.4915, -1.9935, -6.2800, -0.7306, -2.1294, -1.7083, -0.0674]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 17, step: 5
	action: tensor([[-1.7342, -1.3464, -6.2800,  0.7989, -2.1610, -0.4820,  0.8992]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 17, step: 6
	action: tensor([[-1.5162, -3.7478, -6.2800,  1.7168, -3.1246, -1.0648,  0.6303]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 17, step: 7
	action: tensor([[-1.7942, -1.8161, -6.2800,  1.6435, -3.4216, -1.7391,  3.0709]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 17, step: 8
	action: tensor([[ 0.7634, -0.1666, -6.2800,  1.0060, -3.3225, -0.3435,  0.9269]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.13606358593624085 entropy -1.8704687425011068
epoch: 17, step: 9
	action: tensor([[-0.8781, -1.5893, -6.2800,  1.5270, -0.7288, -2.5946,  0.5979]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 17, step: 10
	action: tensor([[-1.1783, -1.5725, -6.2800, -0.7403, -3.1755, -1.1431, -0.8703]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 17, step: 11
	action: tensor([[-2.5195, -3.1076, -6.2800,  2.9569, -2.8707,  0.8122,  1.1784]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 17, step: 12
	action: tensor([[ 0.1201, -2.6887, -6.2800,  1.5595, -1.0978, -2.3580,  0.9523]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 17, step: 13
	action: tensor([[-1.4068, -1.3129, -6.2800,  0.5218, -3.3250, -1.7819,  1.8225]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 17, step: 14
	action: tensor([[-1.7208, -0.8971, -6.2800,  2.3734, -1.9058, -1.2713, -1.3351]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 17, step: 15
	action: tensor([[-0.9717, -1.3407, -6.2800,  0.8512, -0.9780, -1.3055,  3.0790]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 17, step: 16
	action: tensor([[-1.0015, -1.2685, -6.2800,  2.1360, -2.6230,  0.8170, -0.0767]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 17, step: 17
	action: tensor([[-1.1154, -0.3718, -6.2800,  2.7977, -2.1250, -1.2507,  2.1374]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 17, step: 18
	action: tensor([[-2.0544, -0.7555, -6.2800,  2.6319, -1.8757, -0.0170,  2.1622]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 17, step: 19
	action: tensor([[ 0.4620, -2.3394, -6.2800,  1.8304, -2.2760, -1.1593, -0.0468]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 17, step: 20
	action: tensor([[-2.1446, -0.5292, -6.2800,  1.3525, -1.8428, -1.3387,  0.9962]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 17, step: 21
	action: tensor([[-0.8328, -2.0366, -6.2800,  2.0913, -1.3340,  0.1065,  0.9201]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 17, step: 22
	action: tensor([[-0.3870, -0.5728, -6.2800,  2.0564, -2.7370, -0.4619,  0.7207]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7317364114078762 entropy -1.8704687425011068
epoch: 17, step: 23
	action: tensor([[-2.8130, -2.7804, -6.2800,  1.1523, -2.4175,  0.7932,  1.6946]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 17, step: 24
	action: tensor([[-2.3101, -1.9555, -6.2800,  1.8990, -3.3468, -2.0857,  1.3728]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 17, step: 25
	action: tensor([[-2.3091, -1.3286, -6.2800, -1.2388, -2.2832,  1.1308, -0.7437]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 17, step: 26
	action: tensor([[-1.7937, -2.3861, -6.2800,  1.4135, -3.7609, -0.1663,  2.6808]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 17, step: 27
	action: tensor([[-2.7886, -1.8533, -6.2800,  2.4617, -3.6932,  0.2681,  0.5204]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 17, step: 28
	action: tensor([[-1.3387, -1.2893, -6.2800,  2.7629, -3.3035, -1.6546,  0.1043]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 17, step: 29
	action: tensor([[-1.8991, -1.0438, -6.2800,  2.3956, -0.7584,  0.0500,  1.1125]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 17, step: 30
	action: tensor([[-1.7747, -2.2487, -6.2800,  2.7897, -1.9911,  0.8913,  0.4009]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 17, step: 31
	action: tensor([[-0.3473, -1.1142, -6.2800,  1.4659, -3.5044, -1.0881,  0.3729]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0416082734008025 entropy -1.8704687425011068
epoch: 17, step: 32
	action: tensor([[-3.0312, -0.7662, -6.2800,  0.5669, -2.1826, -0.1847,  1.4580]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 17, step: 33
	action: tensor([[-1.7446, -0.4751, -6.2800,  2.7741, -2.1243, -1.0884,  1.0831]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 17, step: 34
	action: tensor([[-1.5092, -0.4522, -6.2800,  0.4750, -1.4421, -1.6513,  0.9976]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.517786538796006 entropy -1.8704687425011068
epoch: 17, step: 35
	action: tensor([[-1.6354, -1.0079, -6.2800,  1.6868, -2.5072, -0.9115, -0.1569]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 17, step: 36
	action: tensor([[-0.2843, -2.5452, -6.2800,  2.2120, -0.9204, -1.5959,  1.8382]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 17, step: 37
	action: tensor([[-0.3251, -2.2830, -6.2800,  2.1403, -1.7692, -2.2617,  0.6432]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 17, step: 38
	action: tensor([[-1.2482,  0.8432, -6.2800,  3.1846, -0.5377, -1.3384, -0.0299]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 17, step: 39
	action: tensor([[-1.3420, -0.4980, -6.2800,  2.6048, -1.6720, -0.9692,  2.3399]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 17, step: 40
	action: tensor([[-0.7110, -1.3753, -6.2800,  2.6915, -0.1428,  1.3397,  0.7996]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 17, step: 41
	action: tensor([[-3.0778e-03, -2.6803e+00, -6.2800e+00,  1.6792e+00, -7.9596e-01,
         -8.5267e-01, -1.6846e+00]], dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 17, step: 42
	action: tensor([[-3.2871,  0.0103, -6.2800, -0.6516, -1.9973, -0.8855,  0.1155]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 17, step: 43
	action: tensor([[ 0.7108, -0.0782, -6.2800,  1.4341, -2.1385, -2.0563,  3.0430]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.5607152448350006 entropy -1.8704687425011068
epoch: 17, step: 44
	action: tensor([[-0.2230, -1.7342, -6.2800,  2.7417, -1.8140,  0.8754,  0.5664]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 17, step: 45
	action: tensor([[-0.9636, -1.9776, -6.2800,  2.7698, -2.2442, -0.2511, -0.4248]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 17, step: 46
	action: tensor([[-1.0876, -2.0554, -6.2800,  0.8455, -1.1415, -0.3125,  0.9524]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 17, step: 47
	action: tensor([[-0.4351, -3.9156, -6.2800,  3.3259, -3.0214, -2.2896, -0.0953]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 17, step: 48
	action: tensor([[ 0.5631, -1.0053, -6.2800,  0.4048, -1.2139, -1.8671,  0.7978]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0614553188806792 entropy -1.8704687425011068
epoch: 17, step: 49
	action: tensor([[-2.0158, -1.3382, -6.2800,  2.1698, -4.6413, -2.0586,  1.1460]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 17, step: 50
	action: tensor([[-2.1939, -1.9506, -6.2800,  2.1749, -2.6301, -1.8495,  1.3930]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 17, step: 51
	action: tensor([[-2.0161, -1.5713, -6.2800,  0.8848, -1.7937,  0.2454,  0.5630]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 17, step: 52
	action: tensor([[-1.5416, -2.3375, -6.2800,  0.5508, -1.0521,  0.0998,  0.9368]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 17, step: 53
	action: tensor([[-1.8550, -2.1873, -6.2800, -0.2972, -2.4077, -0.3888, -0.0081]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 17, step: 54
	action: tensor([[-0.3084, -3.5778, -6.2800,  1.5925, -3.4823, -1.5869, -1.1959]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 17, step: 55
	action: tensor([[ 0.3656, -1.4240, -6.2800,  1.5720, -3.8603, -0.6297,  2.2290]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 17, step: 56
	action: tensor([[ 0.5251, -1.7471, -6.2800,  1.1651, -1.9637, -1.4059,  1.4518]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 17, step: 57
	action: tensor([[-1.4426, -1.2250, -6.2800,  1.2687, -1.5302, -1.2086, -0.4569]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.504225048273554 entropy -1.8704687425011068
epoch: 17, step: 58
	action: tensor([[-0.0132, -1.4062, -6.2800,  2.2483, -2.6280, -0.4976,  0.0162]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 17, step: 59
	action: tensor([[-1.2892, -2.2399, -6.2800,  2.6301, -3.3450, -1.2166, -0.8119]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 17, step: 60
	action: tensor([[-1.2393, -2.4527, -6.2800,  1.6255, -0.8416, -0.3914,  1.5176]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 17, step: 61
	action: tensor([[-0.3221, -0.3463, -6.2800,  0.6126, -1.9807,  0.1551, -0.4058]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2039354860784839 entropy -1.8704687425011068
epoch: 17, step: 62
	action: tensor([[-1.5592, -1.7877, -6.2800,  2.0889, -0.8436, -1.0969,  1.4643]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 17, step: 63
	action: tensor([[ 0.6722, -1.7334, -6.2800,  2.3867, -1.8660, -0.0936, -0.5908]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 17, step: 64
	action: tensor([[-0.7179, -2.8616, -6.2800,  1.8322, -1.8237, -1.0623,  0.1799]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 17, step: 65
	action: tensor([[-2.3415, -2.8694, -6.2800,  2.6608, -1.8869, -0.6101,  0.9338]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 17, step: 66
	action: tensor([[-0.7259, -1.2591, -6.2800,  1.8216, -3.0105, -1.0334, -0.5283]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 17, step: 67
	action: tensor([[-3.4007, -1.2583, -6.2800,  0.4518, -0.6701, -0.7358,  0.5175]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 17, step: 68
	action: tensor([[ 0.2476, -1.3526, -6.2800, -0.8804, -2.2286,  0.8175,  1.5826]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 17, step: 69
	action: tensor([[-1.5114, -3.4351, -6.2800,  2.3104, -1.1210, -0.9594,  2.2856]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 17, step: 70
	action: tensor([[-3.1032, -0.9673, -6.2800,  0.9717, -0.4417, -0.6906,  0.3677]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 17, step: 71
	action: tensor([[-1.9941, -3.5108, -6.2800,  1.5839, -1.3939, -0.4751,  0.8958]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 17, step: 72
	action: tensor([[-3.4366, -2.8276, -6.2800,  1.4063, -2.0825, -1.4390,  2.5377]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 17, step: 73
	action: tensor([[-0.5455, -2.6483, -6.2800,  1.7923, -2.5504, -1.4954,  0.4098]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 17, step: 74
	action: tensor([[-1.5565, -1.3152, -6.2800,  2.7701, -0.5575, -2.5869,  0.1496]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 17, step: 75
	action: tensor([[-1.3960, -1.2736, -6.2800, -0.1688, -2.2872, -1.5017,  0.1147]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 17, step: 76
	action: tensor([[-1.3207,  0.3506, -6.2800,  0.9423, -1.3159, -1.0706,  1.6814]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.202446952450178 entropy -1.8704687425011068
epoch: 17, step: 77
	action: tensor([[-0.1424, -1.0157, -6.2800,  1.2634, -3.7879, -0.6239, -0.1815]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0572280349974459 entropy -1.8704687425011068
epoch: 17, step: 78
	action: tensor([[ 0.9971, -1.4285, -6.2800,  0.1710, -3.2731, -0.2534,  0.8195]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 17, step: 79
	action: tensor([[ 0.2126, -2.2376, -6.2800,  0.1469, -3.1648,  0.9926,  0.8515]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 17, step: 80
	action: tensor([[-2.8551, -0.1584, -6.2800,  0.6789, -2.5526, -0.9426,  0.4006]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 17, step: 81
	action: tensor([[-0.3820, -0.4583, -6.2800,  1.1832, -2.5533, -1.6330,  2.9578]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7164090446722419 entropy -1.8704687425011068
epoch: 17, step: 82
	action: tensor([[-1.1658, -2.5689, -6.2800,  2.6752, -3.1780,  0.3837,  1.3665]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 17, step: 83
	action: tensor([[ 0.1716, -2.4021, -6.2800,  1.0267, -2.2755, -0.7973, -0.1655]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 17, step: 84
	action: tensor([[-3.0134, -1.8094, -6.2800,  2.7529, -0.3604, -0.6007, -1.2573]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 17, step: 85
	action: tensor([[-1.0467, -1.4369, -6.2800,  1.1354, -4.4836, -1.1955, -0.4023]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 17, step: 86
	action: tensor([[-1.9107, -0.1316, -6.2800, -0.0073, -0.4983, -1.3693, -0.0953]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 17, step: 87
	action: tensor([[-0.6400, -1.0405, -6.2800,  2.7206, -2.8224, -1.0932, -0.4580]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 17, step: 88
	action: tensor([[-2.5300, -1.9128, -6.2800,  2.0157, -0.3756,  0.6960, -0.5591]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 17, step: 89
	action: tensor([[-1.0370, -1.9894, -6.2800,  1.7486, -3.3288, -0.2013,  0.5089]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 17, step: 90
	action: tensor([[-0.1592, -0.5555, -6.2800,  1.5228, -2.9787, -0.3785, -0.0347]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7481740819929723 entropy -1.8704687425011068
epoch: 17, step: 91
	action: tensor([[-1.8172, -1.8516, -6.2800,  3.3482, -0.8710, -1.2179,  0.1511]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 17, step: 92
	action: tensor([[-1.6080, -3.6872, -6.2800,  2.4675, -3.8344, -1.1445,  0.7216]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 17, step: 93
	action: tensor([[-2.1542, -3.0675, -6.2800,  0.4480, -2.2945,  0.4369,  0.4444]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 17, step: 94
	action: tensor([[ 1.2216e+00, -2.4857e+00, -6.2800e+00, -2.9935e-01, -2.1532e+00,
          5.1034e-03,  3.0113e+00]], dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 17, step: 95
	action: tensor([[-0.6950, -1.8946, -6.2800,  0.3876, -2.7494, -1.2487,  0.4189]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 17, step: 96
	action: tensor([[-2.9954, -1.7065, -6.2800,  0.2126, -3.0028,  1.6243, -0.0153]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 17, step: 97
	action: tensor([[ 0.2497, -0.9684, -6.2800,  2.2898, -0.3010, -0.5769,  1.0129]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 17, step: 98
	action: tensor([[-0.6092, -1.7045, -6.2800,  2.5679, -1.7643, -0.5666,  0.4324]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 17, step: 99
	action: tensor([[-0.9083, -2.9083, -6.2800,  2.3837, -2.8351, -0.3013, -1.0670]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 17, step: 100
	action: tensor([[-0.4738, -0.2903, -6.2800,  2.7740, -2.5800,  0.1329,  0.1468]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 17, step: 101
	action: tensor([[-1.4352, -1.4513, -6.2800,  3.6344, -2.6851, -0.8154, -1.1013]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 17, step: 102
	action: tensor([[-1.7238, -4.0530, -6.2800,  0.5291, -0.5149, -0.3162, -0.3585]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 17, step: 103
	action: tensor([[-0.5658, -2.0315, -6.2800,  0.3716, -4.8766, -1.1563,  1.4364]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 17, step: 104
	action: tensor([[-0.8810, -1.1994, -6.2800,  2.4839, -0.7593, -0.0698,  0.4918]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 17, step: 105
	action: tensor([[-2.4237, -1.9689, -6.2800,  0.6327, -3.4283, -0.4739,  1.7465]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 17, step: 106
	action: tensor([[-1.6602,  0.2789, -6.2800,  1.0028, -3.2160, -0.2868,  0.0974]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 17, step: 107
	action: tensor([[-1.5785, -2.8149, -6.2800,  0.7142, -3.5609,  0.8732, -0.1176]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 17, step: 108
	action: tensor([[-0.8017, -3.5003, -6.2800,  1.1002, -1.5687, -1.1413,  0.8640]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 17, step: 109
	action: tensor([[-0.0715, -0.2364, -6.2800,  1.4917, -3.2641, -0.3381, -0.7608]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7021171954143494 entropy -1.8704687425011068
epoch: 17, step: 110
	action: tensor([[-0.4482, -0.9344, -6.2800,  2.2573, -0.7076, -1.4886,  0.7555]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9046491295653505 entropy -1.8704687425011068
epoch: 17, step: 111
	action: tensor([[-1.3640, -2.9892, -6.2800,  3.5188, -1.9020, -0.2186,  0.6842]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 17, step: 112
	action: tensor([[-1.1634, -1.8909, -6.2800,  1.4526, -1.7874, -2.1253,  0.5445]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 17, step: 113
	action: tensor([[-1.5597, -1.1397, -6.2800,  1.0441, -1.3465, -0.2122,  0.8115]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.8047980856262502 entropy -1.8704687425011068
epoch: 17, step: 114
	action: tensor([[-0.8851, -0.8335, -6.2800,  1.3806, -3.8618, -3.5853,  0.1949]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 17, step: 115
	action: tensor([[-1.0571, -1.9628, -6.2800, -0.0250, -2.1799, -0.0857, -0.1198]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 17, step: 116
	action: tensor([[ 0.1881, -1.3095, -6.2800,  2.6777, -1.4608, -1.4757,  0.0813]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 17, step: 117
	action: tensor([[-1.5062, -0.6105, -6.2800,  1.2624, -1.8517, -0.8470,  2.8576]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4590109972905771 entropy -1.8704687425011068
epoch: 17, step: 118
	action: tensor([[-3.4003, -3.0183, -6.2800,  2.7163, -3.7091,  0.1688,  0.0862]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 17, step: 119
	action: tensor([[ 0.2794, -1.6376, -6.2800,  1.2773, -2.1333, -1.4807,  0.3518]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 17, step: 120
	action: tensor([[ 0.4258, -3.3935, -6.2800,  1.5012, -1.1355, -0.7010,  0.7202]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 17, step: 121
	action: tensor([[-2.5008, -0.8709, -6.2800,  3.0359, -2.3535, -0.3331, -0.1697]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 17, step: 122
	action: tensor([[-1.6173, -1.1067, -6.2800,  3.3301, -1.9097, -0.1886, -0.3407]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 17, step: 123
	action: tensor([[-1.7490, -2.0345, -6.2800,  0.9203, -1.9485,  0.0297,  1.2709]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 17, step: 124
	action: tensor([[-0.7578, -2.1152, -6.2800,  1.1020, -2.3503,  0.1791,  0.8979]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 17, step: 125
	action: tensor([[-2.9103, -0.3459, -6.2800,  0.9649, -0.6519, -0.7893,  0.5440]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 17, step: 126
	action: tensor([[-2.4123, -0.6936, -6.2800,  1.9123, -0.6807, -1.0628,  2.1260]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 17, step: 127
	action: tensor([[-1.0541, -0.1576, -6.2800,  2.1488, -2.2733, -1.1721,  2.2761]],
       dtype=torch.float64)
	q_value: tensor([[-0.0357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.726685172419309 entropy -1.8704687425011068
LOSS epoch 17 actor 1435.2615386794062 critic 2496.429328858591 entropy 100
epoch: 18, step: 0
	action: tensor([[-0.6369, -1.9833, -6.2800,  1.8803, -2.8366, -1.8350,  1.7540]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 18, step: 1
	action: tensor([[-1.4828, -2.1127, -6.2800,  1.8812, -3.5181, -0.6306,  1.4147]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 18, step: 2
	action: tensor([[-1.5839, -2.8591, -6.2800,  2.6153, -3.3104,  1.0475, -0.5399]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 18, step: 3
	action: tensor([[-1.3662, -1.4601, -6.2800,  2.0164, -1.3333, -1.4836,  0.2343]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 18, step: 4
	action: tensor([[-0.2353, -0.6163, -6.2800,  2.1127, -2.9301, -1.0289,  1.4743]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7263376557387514 entropy -1.8704687425011068
epoch: 18, step: 5
	action: tensor([[-0.8712, -2.2066, -6.2800,  1.0929, -3.8484, -1.9846,  2.4722]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 18, step: 6
	action: tensor([[-2.1597, -1.0065, -6.2800,  2.5466, -3.6942,  0.6882, -0.8839]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 18, step: 7
	action: tensor([[-0.9443, -1.8912, -6.2800,  2.2915, -3.8205, -2.7605,  1.6493]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 18, step: 8
	action: tensor([[-0.6707, -2.6065, -6.2800,  2.8187, -3.7535, -1.3913,  0.3061]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 18, step: 9
	action: tensor([[ 0.2239, -2.4682, -6.2800,  3.1070, -3.1548,  0.6328,  2.5898]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 18, step: 10
	action: tensor([[-2.5042e+00, -1.2083e+00, -6.2800e+00,  4.3159e+00, -3.1090e+00,
          5.6361e-01,  1.7932e-03]], dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 18, step: 11
	action: tensor([[-1.3537, -1.7670, -6.2800, -0.1521, -4.1502, -1.1320,  1.8313]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 18, step: 12
	action: tensor([[ 0.2686, -2.5373, -6.2800,  2.4696, -2.1469, -2.2033,  1.8917]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 18, step: 13
	action: tensor([[-2.2699, -3.2808, -6.2800,  3.6523, -2.5546, -1.9620,  2.4529]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 18, step: 14
	action: tensor([[-1.5718, -1.8094, -6.2800,  2.9474, -3.1602, -0.3585,  0.9489]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 18, step: 15
	action: tensor([[-0.1919, -2.1894, -6.2800,  1.7275, -4.7294, -0.4823, -0.2812]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 18, step: 16
	action: tensor([[-0.0695, -2.0853, -6.2800,  1.9997, -3.4610, -1.1260,  0.9504]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 18, step: 17
	action: tensor([[-0.5777, -1.4030, -6.2800,  1.8275, -2.6427, -2.2319,  2.5186]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 18, step: 18
	action: tensor([[-1.4207, -1.7785, -6.2800,  0.5427, -3.4500, -1.1263,  0.5168]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 18, step: 19
	action: tensor([[-1.7384, -2.1403, -6.2800,  0.4220, -1.6132, -0.1737,  0.0132]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 18, step: 20
	action: tensor([[-1.7971, -1.5369, -6.2800,  2.2168, -4.7538, -0.5056,  1.7508]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 18, step: 21
	action: tensor([[-0.3042, -2.8282, -6.2800,  2.0675, -3.1124, -1.8138, -0.1119]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 18, step: 22
	action: tensor([[-0.9540, -3.4859, -6.2800,  3.1017, -1.7351, -0.9718,  1.5812]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 18, step: 23
	action: tensor([[-1.5060, -1.5912, -6.2800,  1.2114, -3.1692, -1.6934,  2.3427]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 18, step: 24
	action: tensor([[-0.9054, -2.2433, -6.2800,  1.4523, -3.8485, -1.2158, -0.3303]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 18, step: 25
	action: tensor([[-1.5927, -1.6406, -6.2800,  3.6374, -3.3940, -3.1226,  0.6929]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 18, step: 26
	action: tensor([[-1.0069, -2.4343, -6.2800,  1.6689, -1.2797,  0.6626,  2.8136]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 18, step: 27
	action: tensor([[-4.0234, -1.8847, -6.2800, -0.6571, -3.0518, -0.3758, -0.5740]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 18, step: 28
	action: tensor([[-2.1847, -1.7728, -6.2800,  1.4283, -2.6807, -1.4502, -0.4323]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 18, step: 29
	action: tensor([[-4.1289, -0.6653, -6.2800,  1.7118, -1.1158, -0.8512,  1.2583]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 18, step: 30
	action: tensor([[ 0.0355, -0.9495, -6.2800,  1.8057, -3.1959, -0.9134,  0.8789]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.6936387481778548 entropy -1.8704687425011068
epoch: 18, step: 31
	action: tensor([[-1.9749,  0.2348, -6.2800,  2.2365, -3.3271, -0.9910,  2.2843]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 18, step: 32
	action: tensor([[ 0.1540, -1.9831, -6.2800,  2.1697, -2.0476, -1.6851, -0.7607]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 18, step: 33
	action: tensor([[-2.5246, -1.5668, -6.2800,  2.5769, -3.9025, -2.0888,  1.3725]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 18, step: 34
	action: tensor([[-3.3499, -1.9430, -6.2800,  2.2221, -2.6454, -1.3653,  2.1453]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 18, step: 35
	action: tensor([[-0.8378,  0.1679, -6.2800,  1.2647, -3.4684, -0.0552,  0.7025]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0757371509129496 entropy -1.8704687425011068
epoch: 18, step: 36
	action: tensor([[-0.0232, -2.7752, -6.2800,  3.4043, -2.0624, -0.8451,  3.1514]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 18, step: 37
	action: tensor([[-1.7916, -1.2862, -6.2800,  5.0905, -1.1176,  1.0660,  2.2113]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 18, step: 38
	action: tensor([[-1.3497, -2.4137, -6.2800,  1.6598, -2.6900, -1.8276,  1.5903]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 18, step: 39
	action: tensor([[-2.2639, -2.6379, -6.2800, -0.0692, -2.8802, -0.9264,  1.0587]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 18, step: 40
	action: tensor([[ 0.2102, -2.9416, -6.2800,  3.2074, -4.0658, -0.0649,  0.4151]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 18, step: 41
	action: tensor([[-1.4918, -3.3578, -6.2800,  1.9668, -1.0355, -0.5734,  0.1311]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 18, step: 42
	action: tensor([[-2.4067, -0.7339, -6.2800, -0.5142, -2.0624, -0.7795, -1.2960]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 18, step: 43
	action: tensor([[-0.0071, -3.3949, -6.2800,  1.5854, -4.1253, -1.8041,  1.5682]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 18, step: 44
	action: tensor([[-1.9317, -2.2107, -6.2800,  3.6812, -3.6125, -1.1165, -0.0649]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 18, step: 45
	action: tensor([[-0.2082, -2.7536, -6.2800,  2.7176, -2.7469, -1.5021,  1.1318]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 18, step: 46
	action: tensor([[-2.2528, -1.0818, -6.2800,  1.5335, -3.0925, -0.1578, -0.5052]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 18, step: 47
	action: tensor([[-0.1998, -2.9218, -6.2800,  0.5486, -2.5811, -1.3594, -0.0346]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 18, step: 48
	action: tensor([[-1.3991, -1.5261, -6.2800,  3.2347, -3.2826, -1.3304,  1.1634]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 18, step: 49
	action: tensor([[-1.6950, -1.6759, -6.2800,  1.7010, -2.1221, -0.1282,  1.1009]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 18, step: 50
	action: tensor([[-0.8955, -2.7412, -6.2800,  2.6575, -2.0533, -0.6027,  1.2877]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 18, step: 51
	action: tensor([[-2.8781, -1.0360, -6.2800,  2.0707, -2.7769, -0.3861,  0.2076]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 18, step: 52
	action: tensor([[-2.3016, -1.0585, -6.2800,  2.8239, -3.2270, -0.8491,  1.7897]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 18, step: 53
	action: tensor([[-1.9471, -1.1008, -6.2800,  1.4567, -2.2229,  1.1437, -0.8565]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 18, step: 54
	action: tensor([[-3.2815, -0.4311, -6.2800,  0.8607, -2.4790, -0.9273,  0.6879]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 18, step: 55
	action: tensor([[-1.8898, -1.5602, -6.2800,  0.7114, -1.8733, -1.7728,  3.6362]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 18, step: 56
	action: tensor([[-2.4319, -4.0015, -6.2800,  2.8836, -0.4957, -1.0750,  0.2799]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 18, step: 57
	action: tensor([[-0.0720, -0.8433, -6.2800,  1.7439, -1.4801, -0.6583,  0.3969]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.6475945355908964 entropy -1.8704687425011068
epoch: 18, step: 58
	action: tensor([[-1.9672, -2.1644, -6.2800,  2.9052, -0.9711,  1.0518,  0.4343]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 18, step: 59
	action: tensor([[-2.7380, -2.6407, -6.2800,  1.2452, -2.2945, -0.9115,  1.6116]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 18, step: 60
	action: tensor([[-0.6945, -1.8570, -6.2800,  1.9856, -3.4924, -0.5241,  1.2763]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 18, step: 61
	action: tensor([[-0.2210, -0.7312, -6.2800,  2.8770, -3.0458, -1.6462,  1.6280]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 18, step: 62
	action: tensor([[-0.6277, -2.1786, -6.2800,  2.9742, -3.1149, -0.7438,  1.1112]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 18, step: 63
	action: tensor([[ 0.9300, -4.1548, -6.2800,  0.9215, -4.0575, -1.3477,  1.8381]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 18, step: 64
	action: tensor([[-1.6365, -2.4615, -6.2800,  3.1811, -4.5130, -2.3102,  1.6367]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 18, step: 65
	action: tensor([[-1.7160, -2.2403, -6.2800,  1.2625, -2.1830, -1.3139,  0.7796]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 18, step: 66
	action: tensor([[-0.6998, -2.5430, -6.2800,  3.6347, -3.3014, -0.6023,  1.8996]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 18, step: 67
	action: tensor([[-0.4901, -2.4225, -6.2800,  3.7189, -4.0925, -0.9712, -1.0810]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 18, step: 68
	action: tensor([[-1.0408, -1.6704, -6.2800,  0.9338, -1.6172, -2.2982,  1.8052]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 18, step: 69
	action: tensor([[-1.7007, -1.4240, -6.2800,  2.9290, -2.8472, -1.5028,  1.5186]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 18, step: 70
	action: tensor([[-2.2719, -0.9126, -6.2800,  1.3349, -2.0110, -1.3937, -0.4917]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 18, step: 71
	action: tensor([[-0.1826, -1.2450, -6.2800,  2.3162, -2.3838,  0.2012,  2.0872]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 18, step: 72
	action: tensor([[-2.1143, -0.6738, -6.2800,  1.0002, -3.2622, -0.4577,  2.2405]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 18, step: 73
	action: tensor([[-0.8418, -2.7933, -6.2800,  1.0953, -1.9855,  0.1586,  1.1251]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 18, step: 74
	action: tensor([[-1.8420, -2.2315, -6.2800,  1.4057, -2.8055,  0.4064,  0.5219]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 18, step: 75
	action: tensor([[-1.7381, -2.5537, -6.2800,  2.1082, -2.6847, -0.3908,  0.8353]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 18, step: 76
	action: tensor([[-2.3728, -1.2419, -6.2800,  3.5828, -3.6830, -0.2322,  1.2317]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 18, step: 77
	action: tensor([[-1.3812, -1.7470, -6.2800,  2.6558, -4.8706, -2.3799,  1.0521]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 18, step: 78
	action: tensor([[-2.2163, -2.0434, -6.2800,  1.6905, -1.8371, -0.2613, -0.2898]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 18, step: 79
	action: tensor([[-3.2529, -2.4811, -6.2800,  2.4313, -4.0030,  0.5035,  1.3491]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 18, step: 80
	action: tensor([[-3.5223, -2.6333, -6.2800,  0.8851, -3.3288, -1.7975,  0.3160]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 18, step: 81
	action: tensor([[-1.9949, -2.6544, -6.2800,  2.4288, -4.2887, -0.4712,  0.6670]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 18, step: 82
	action: tensor([[-1.4382, -1.7427, -6.2800,  2.6690, -2.0477, -0.8018,  2.0347]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 18, step: 83
	action: tensor([[-1.6069, -2.9909, -6.2800,  3.7500, -4.0066, -0.5470,  1.4565]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 18, step: 84
	action: tensor([[-2.1867, -1.3549, -6.2800,  2.4419, -5.1854, -1.7280,  0.6467]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 18, step: 85
	action: tensor([[-1.4855, -3.5403, -6.2800,  1.4185, -4.2263, -0.3484,  2.1461]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 18, step: 86
	action: tensor([[-2.0783, -1.4525, -6.2800,  2.3030, -1.2252, -1.5800,  0.4983]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 18, step: 87
	action: tensor([[-0.4214, -1.4286, -6.2800,  1.9299, -3.4786, -1.7099,  0.8897]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 18, step: 88
	action: tensor([[-2.4778, -2.1371, -6.2800,  2.3494, -3.4943, -0.3834,  1.5235]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 18, step: 89
	action: tensor([[-1.1546, -2.2542, -6.2800,  2.9496, -4.2390, -2.5505, -0.1197]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 18, step: 90
	action: tensor([[-1.9889, -1.4951, -6.2800,  2.3529, -2.3441, -0.6657,  1.4344]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 18, step: 91
	action: tensor([[-0.6519, -1.3122, -6.2800,  2.8878, -4.3652,  0.2479,  1.3210]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 18, step: 92
	action: tensor([[-1.8543, -1.8021, -6.2800,  1.7394, -3.4814,  0.0283,  1.3058]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 18, step: 93
	action: tensor([[-1.6708, -4.2426, -6.2800,  3.2840, -2.1313, -1.8580, -0.3227]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 18, step: 94
	action: tensor([[-1.3422, -0.0796, -6.2800,  1.6449, -5.0235,  1.6082,  0.7849]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9801171433937199 entropy -1.8704687425011068
epoch: 18, step: 95
	action: tensor([[-1.1876,  0.3519, -6.2800,  2.5634, -4.4308, -1.7364,  0.4127]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 18, step: 96
	action: tensor([[-1.7330, -0.4202, -6.2800,  4.1244, -2.0482, -0.5156, -0.1483]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 18, step: 97
	action: tensor([[-2.0382, -1.4192, -6.2800,  0.6912, -3.3250, -0.9062,  1.0457]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 18, step: 98
	action: tensor([[-0.1689, -4.0172, -6.2800,  2.7673, -2.7950, -0.4763,  0.9461]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 18, step: 99
	action: tensor([[-3.3285, -2.9932, -6.2800,  0.9660, -2.9991, -2.6081,  1.6643]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 18, step: 100
	action: tensor([[-0.6252, -1.6700, -6.2800,  2.8996, -2.1618,  0.1475,  2.0807]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 18, step: 101
	action: tensor([[-2.8420, -1.4748, -6.2800,  0.9122, -2.4563,  0.5963,  2.0111]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 18, step: 102
	action: tensor([[-0.6646, -0.8500, -6.2800,  2.2349, -1.7937, -0.4345,  1.8806]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8412384498535336 entropy -1.8704687425011068
epoch: 18, step: 103
	action: tensor([[-1.6438, -0.9424, -6.2800,  0.7945, -2.0890, -0.0095,  2.6368]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 18, step: 104
	action: tensor([[-0.0626, -2.6131, -6.2800,  4.2745, -1.5522,  0.5198,  0.2754]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 18, step: 105
	action: tensor([[-2.3808, -2.6395, -6.2800,  1.0485, -3.1593, -1.1896,  0.9727]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 18, step: 106
	action: tensor([[-4.0034,  0.0344, -6.2800,  2.3577, -3.5404, -0.8733, -0.0960]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 18, step: 107
	action: tensor([[-0.9607, -3.2695, -6.2800,  6.0759, -2.7303, -0.1226, -0.8784]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 18, step: 108
	action: tensor([[-1.3176, -2.8942, -6.2800,  2.0322, -2.4315,  0.7102,  1.5091]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 18, step: 109
	action: tensor([[-1.6001, -1.3347, -6.2800,  1.4378, -2.0603, -0.8993, -1.2692]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 18, step: 110
	action: tensor([[-0.6872, -3.8159, -6.2800,  1.4804, -4.0613, -1.6337,  0.9969]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 18, step: 111
	action: tensor([[-2.1194, -2.5538, -6.2800,  0.6263, -4.2433, -2.5947,  1.4521]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 18, step: 112
	action: tensor([[-1.1392, -3.6564, -6.2800,  2.2158, -0.5594, -2.0826, -0.2487]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 18, step: 113
	action: tensor([[-2.6253, -2.2385, -6.2800,  1.6171, -2.9214, -1.8257, -2.8010]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 18, step: 114
	action: tensor([[-2.0998, -2.8771, -6.2800,  0.7000, -2.7055, -1.1884,  0.5571]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 18, step: 115
	action: tensor([[-1.0783,  0.0190, -6.2800,  1.4013, -3.7912, -2.0965,  1.9315]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1418962334302247 entropy -1.8704687425011068
epoch: 18, step: 116
	action: tensor([[-2.1191, -1.9825, -6.2800,  2.0489, -3.4851,  0.3598,  2.5730]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 18, step: 117
	action: tensor([[-0.9744, -2.7368, -6.2800,  2.2986, -3.2921, -2.5159,  1.9603]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 18, step: 118
	action: tensor([[-0.8265, -2.4840, -6.2800,  2.5516, -2.8392, -0.1354, -0.5048]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 18, step: 119
	action: tensor([[-1.3495, -3.0885, -6.2800,  2.1636, -2.9668, -1.0337,  1.1493]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 18, step: 120
	action: tensor([[-0.9960, -1.9178, -6.2800,  1.8978, -3.9714,  0.1671,  3.3461]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 18, step: 121
	action: tensor([[-3.6206, -2.1675, -6.2800,  4.3895, -2.8542,  0.2730,  2.3341]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 18, step: 122
	action: tensor([[-0.6265, -1.4261, -6.2800,  2.7638, -4.4414, -1.5651,  0.9475]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 18, step: 123
	action: tensor([[-1.9812, -2.2857, -6.2800,  0.3941, -1.6471, -1.2880,  2.5003]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 18, step: 124
	action: tensor([[-0.9767, -1.9495, -6.2800,  2.1753, -5.3748, -2.5556, -0.7777]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 18, step: 125
	action: tensor([[-1.4977, -0.2879, -6.2800,  3.1871, -4.3441,  0.9375,  0.4098]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 18, step: 126
	action: tensor([[-1.3493, -1.5826, -6.2800,  1.1867, -2.9952, -1.2403, -0.5227]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 18, step: 127
	action: tensor([[-3.1113, -1.0138, -6.2800,  0.4560, -3.1231, -0.9821,  2.8894]],
       dtype=torch.float64)
	q_value: tensor([[-0.1804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
LOSS epoch 18 actor 1428.0454551223227 critic 2481.997161744424 entropy 100
epoch: 19, step: 0
	action: tensor([[-2.3162, -2.1921, -6.2800,  1.7651, -2.5599, -0.3298,  1.7020]],
       dtype=torch.float64)
	q_value: tensor([[-0.4706]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 19, step: 1
	action: tensor([[-0.6225, -3.2083, -6.2800,  2.4824, -4.5148, -0.3310,  1.1922]],
       dtype=torch.float64)
	q_value: tensor([[-0.4706]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 19, step: 2
	action: tensor([[-0.9511, -0.9323, -6.2800,  2.4017, -3.9373, -1.2253,  1.6701]],
       dtype=torch.float64)
	q_value: tensor([[-0.4706]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 19, step: 3
	action: tensor([[ 0.7006, -1.3444, -6.2800,  1.0892, -3.1316, -1.8719,  0.7311]],
       dtype=torch.float64)
	q_value: tensor([[-0.4706]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 19, step: 4
	action: tensor([[-2.5439, -2.9134, -6.2800,  3.5812, -4.0721, -0.8548, -0.6521]],
       dtype=torch.float64)
	q_value: tensor([[-0.4706]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 19, step: 5
	action: tensor([[-1.7780, -2.7614, -6.2800,  4.0039, -5.6725,  0.1472,  0.9471]],
       dtype=torch.float64)
	q_value: tensor([[-0.4706]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 19, step: 6
	action: tensor([[-0.0931, -1.9997, -6.2800,  3.0839, -2.0682, -2.0760,  1.3012]],
       dtype=torch.float64)
	q_value: tensor([[-0.4706]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 19, step: 7
	action: tensor([[-1.8363, -1.4560, -6.2800,  3.2528, -1.4288, -2.0641,  1.9943]],
       dtype=torch.float64)
	q_value: tensor([[-0.4706]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 19, step: 8
	action: tensor([[-1.1309, -3.6135, -6.2800,  1.8539, -3.3338, -1.5862, -1.6874]],
       dtype=torch.float64)
	q_value: tensor([[-0.4706]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 19, step: 9
	action: tensor([[-1.7691, -1.6588, -6.2800,  3.4244, -3.5402, -0.8793,  0.9313]],
       dtype=torch.float64)
	q_value: tensor([[-0.4706]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 19, step: 10
	action: tensor([[-1.9132, -2.5940, -6.2800,  2.7206, -3.6813, -0.3886,  1.4980]],
       dtype=torch.float64)
	q_value: tensor([[-0.4706]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 19, step: 11
	action: tensor([[-1.1168, -2.1678, -6.2800,  2.9189, -4.5523, -1.2581,  0.8232]],
       dtype=torch.float64)
	q_value: tensor([[-0.4706]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 19, step: 12
	action: tensor([[-2.6866, -1.4043, -6.2800,  2.6262, -3.2450, -0.4172, -0.7140]],
       dtype=torch.float64)
	q_value: tensor([[-0.4706]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 19, step: 13
	action: tensor([[-2.2175, -2.2915, -6.2800,  3.3638, -5.1971, -1.0662, -0.1416]],
       dtype=torch.float64)
	q_value: tensor([[-0.4706]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 19, step: 14
	action: tensor([[-1.1045, -2.2781, -6.2800,  0.6196, -4.3084, -0.0955,  0.5387]],
       dtype=torch.float64)
	q_value: tensor([[-0.4706]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 19, step: 15
	action: tensor([[-1.2115, -1.3979, -6.2800,  3.4899, -3.0128, -1.1731,  1.1967]],
       dtype=torch.float64)
	q_value: tensor([[-0.4706]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 19, step: 16
	action: tensor([[-3.4246, -0.8032, -6.2800,  0.8418, -3.0385, -1.2212,  2.1160]],
       dtype=torch.float64)
	q_value: tensor([[-0.4706]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 19, step: 17
	action: tensor([[-1.7987, -3.2698, -6.2800,  3.0016, -2.8183, -1.7158,  1.6071]],
       dtype=torch.float64)
	q_value: tensor([[-0.4706]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 19, step: 18
	action: tensor([[-0.8803, -3.8156, -6.2800,  4.9976, -4.5864, -1.2866,  2.0510]],
       dtype=torch.float64)
	q_value: tensor([[-0.4706]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 19, step: 19
	action: tensor([[-1.5860, -0.8556, -6.2800,  2.4768, -2.8454, -0.7893,  1.4945]],
       dtype=torch.float64)
	q_value: tensor([[-0.4706]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 19, step: 20
	action: tensor([[-2.6201, -3.5645, -6.2800,  3.2988, -4.4198, -1.3976, -1.0006]],
       dtype=torch.float64)
	q_value: tensor([[-0.4706]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 19, step: 21
	action: tensor([[-1.1557, -2.9292, -6.2800,  1.8579, -5.2047,  0.6677,  0.8319]],
       dtype=torch.float64)
	q_value: tensor([[-0.4706]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 19, step: 22
	action: tensor([[-3.5661, -1.7461, -6.2800,  1.4518, -3.8895, -2.1325, -0.1880]],
       dtype=torch.float64)
	q_value: tensor([[-0.4706]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 19, step: 23
	action: tensor([[-0.0297, -1.9482, -6.2800,  2.1364, -3.4990, -2.3448,  3.0788]],
       dtype=torch.float64)
	q_value: tensor([[-0.4706]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 19, step: 24
	action: tensor([[-2.3860, -2.1354, -6.2800,  3.3574, -5.5805, -1.8237,  0.2600]],
       dtype=torch.float64)
	q_value: tensor([[-0.4706]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 19, step: 25
	action: tensor([[-1.6217, -2.3072, -6.2800,  1.6917, -4.1974, -1.3144,  1.8535]],
       dtype=torch.float64)
	q_value: tensor([[-0.4706]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 19, step: 26
	action: tensor([[-1.0664, -2.6070, -6.2800,  2.2403, -3.5562, -2.3058,  0.7469]],
       dtype=torch.float64)
	q_value: tensor([[-0.4706]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 19, step: 27
	action: tensor([[-1.1703, -2.6994, -6.2800,  4.2128, -3.0007, -0.2405,  1.0735]],
       dtype=torch.float64)
	q_value: tensor([[-0.4706]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 19, step: 28
	action: tensor([[-0.0293, -2.7650, -6.2800,  1.1631, -2.5238, -0.9086,  0.4532]],
       dtype=torch.float64)
	q_value: tensor([[-0.4706]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 19, step: 29
	action: tensor([[-1.0763, -3.1798, -6.2800,  3.4333, -3.8811, -2.1684,  1.5143]],
       dtype=torch.float64)
	q_value: tensor([[-0.4706]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 19, step: 30
	action: tensor([[-2.0886, -1.7388, -6.2800,  2.1560, -2.2921, -0.0381,  0.8948]],
       dtype=torch.float64)
	q_value: tensor([[-0.4706]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 19, step: 31
	action: tensor([[-2.6668, -1.5971, -6.2800,  3.0920, -4.7590, -0.4903,  1.0857]],
       dtype=torch.float64)
	q_value: tensor([[-0.4706]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 19, step: 32
	action: tensor([[-1.9777, -3.4409, -6.2800,  2.7515, -3.7053, -2.1177,  3.7124]],
       dtype=torch.float64)
	q_value: tensor([[-0.4706]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 19, step: 33
	action: tensor([[-1.8922, -2.0265, -6.2800,  2.2121, -4.1887, -0.1155,  1.5115]],
       dtype=torch.float64)
	q_value: tensor([[-0.4706]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 19, step: 34
	action: tensor([[-0.1960, -2.1473, -6.2800,  1.4543, -2.8627,  0.6312,  1.8517]],
       dtype=torch.float64)
	q_value: tensor([[-0.4706]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 19, step: 35
	action: tensor([[-1.4650, -2.0728, -6.2800,  2.5786, -3.0851,  0.3343,  3.3901]],
       dtype=torch.float64)
	q_value: tensor([[-0.4706]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 19, step: 36
	action: tensor([[-2.3209, -2.9095, -6.2800,  2.6266, -2.4879, -1.0843,  1.7779]],
       dtype=torch.float64)
	q_value: tensor([[-0.4706]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 19, step: 37
	action: tensor([[-0.8565, -2.7616, -6.2800,  1.6556, -3.9706,  0.6489,  1.4566]],
       dtype=torch.float64)
	q_value: tensor([[-0.4706]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 19, step: 38
	action: tensor([[-0.9019, -1.4615, -6.2800,  0.9857, -3.3629, -1.4676,  1.5605]],
       dtype=torch.float64)
	q_value: tensor([[-0.4706]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 19, step: 39
	action: tensor([[-3.4073, -2.4001, -6.2800,  1.7908, -3.0916, -0.4343,  1.0771]],
       dtype=torch.float64)
	q_value: tensor([[-0.4706]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 19, step: 40
	action: tensor([[-2.1369e+00, -1.6218e+00, -6.2800e+00,  1.7291e+00, -3.3973e+00,
         -7.6158e-01, -4.9238e-03]], dtype=torch.float64)
	q_value: tensor([[-0.4706]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 19, step: 41
	action: tensor([[-2.9463, -1.6716, -6.2800,  4.2486, -2.8498, -1.7785,  1.0343]],
       dtype=torch.float64)
	q_value: tensor([[-0.4706]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 19, step: 42
	action: tensor([[-2.0761, -2.1413, -6.2800,  1.8653, -3.1649, -0.3410,  1.9460]],
       dtype=torch.float64)
	q_value: tensor([[-0.4706]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 19, step: 43
	action: tensor([[-3.0367, -2.1849, -6.2800,  2.8041, -4.3614, -2.7018,  1.1843]],
       dtype=torch.float64)
	q_value: tensor([[-0.4706]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 19, step: 44
	action: tensor([[-1.0525, -2.0127, -6.2800,  3.4723, -3.4619, -1.6553,  0.6761]],
       dtype=torch.float64)
	q_value: tensor([[-0.4706]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 19, step: 45
	action: tensor([[-0.5745, -3.1449, -6.2800,  1.6857, -3.9016, -1.0028,  1.4436]],
       dtype=torch.float64)
	q_value: tensor([[-0.4706]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 19, step: 46
	action: tensor([[-2.4818, -3.0581, -6.2800,  3.2449, -4.2888, -0.7097,  0.9039]],
       dtype=torch.float64)
	q_value: tensor([[-0.4706]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 19, step: 47
	action: tensor([[-0.3666, -3.9933, -6.2800,  2.5145, -4.4634,  0.0365,  1.7910]],
       dtype=torch.float64)
	q_value: tensor([[-0.4706]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 19, step: 48
	action: tensor([[-1.4493, -1.7695, -6.2800,  3.4579, -4.9039, -1.8960,  0.8159]],
       dtype=torch.float64)
	q_value: tensor([[-0.4706]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 19, step: 49
	action: tensor([[-1.9635, -3.4865, -6.2800,  3.6546, -4.5108, -0.8507,  2.8578]],
       dtype=torch.float64)
	q_value: tensor([[-0.4706]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 19, step: 50
	action: tensor([[-2.2679, -1.2844, -6.2800,  2.0953, -4.3904, -1.6207,  1.0349]],
       dtype=torch.float64)
	q_value: tensor([[-0.4706]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 19, step: 51
	action: tensor([[-0.0607, -0.9248, -6.2800,  0.9077, -3.7779,  0.1356,  0.0476]],
       dtype=torch.float64)
	q_value: tensor([[-0.4706]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.152615064381069 entropy -1.8704687425011068
epoch: 19, step: 52
	action: tensor([[-3.2161, -1.2996, -6.2800,  2.2529, -3.6503, -0.9035,  2.2957]],
       dtype=torch.float64)
	q_value: tensor([[-0.4706]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 19, step: 53
	action: tensor([[-1.3071, -4.0891, -6.2800,  2.6007, -4.6888, -1.7255,  0.6953]],
       dtype=torch.float64)
	q_value: tensor([[-0.4706]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 19, step: 54
	action: tensor([[-2.3801, -1.6280, -6.2800,  2.5750, -3.5407, -1.2094,  1.1421]],
       dtype=torch.float64)
	q_value: tensor([[-0.4706]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 19, step: 55
	action: tensor([[-2.4330, -2.8232, -6.2800,  3.0471, -3.1912,  0.8896,  1.2543]],
       dtype=torch.float64)
	q_value: tensor([[-0.4706]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 19, step: 56
	action: tensor([[-1.4469, -1.5968, -6.2800,  6.1800, -4.4756, -2.3097,  1.1901]],
       dtype=torch.float64)
	q_value: tensor([[-0.4706]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 19, step: 57
	action: tensor([[-1.2282, -1.4357, -6.2800,  0.7829, -5.0009, -2.2266,  1.5339]],
       dtype=torch.float64)
	q_value: tensor([[-0.4706]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 19, step: 58
	action: tensor([[-2.0321, -2.5034, -6.2800,  4.8491, -2.5280, -2.5712,  1.9954]],
       dtype=torch.float64)
	q_value: tensor([[-0.4706]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 19, step: 59
	action: tensor([[-1.9557, -3.6887, -6.2800,  1.7212, -5.8521, -1.0838,  1.5554]],
       dtype=torch.float64)
	q_value: tensor([[-0.4706]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 19, step: 60
	action: tensor([[-0.9514, -1.3681, -6.2800,  3.0434, -3.0383, -1.2079,  1.6041]],
       dtype=torch.float64)
	q_value: tensor([[-0.4706]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
epoch: 19, step: 61
	action: tensor([[-0.6821, -3.6167, -6.2800,  2.5324, -3.2061, -0.1378, -0.1475]],
       dtype=torch.float64)
	q_value: tensor([[-0.4706]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy -1.8704687425011068
