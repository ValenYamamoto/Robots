epoch: 0, step: 0
	action: tensor([[ 1.5537, -0.9127,  0.5455, -0.7854,  1.0283, -0.4358,  1.4026]],
       dtype=torch.float64)
	q_value: tensor([[-27.1909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4060369959129282, distance: 1.3569225733199937 entropy 1.2378510236740112
epoch: 0, step: 1
	action: tensor([[ 0.8102, -0.0905,  0.7956,  0.1733, -0.7766, -0.5304,  1.0584]],
       dtype=torch.float64)
	q_value: tensor([[-27.3685]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5950296310418128, distance: 0.7282295825972792 entropy 1.2378510236740112
epoch: 0, step: 2
	action: tensor([[-1.1135, -0.7623,  0.4712, -0.0953,  1.2124,  1.4529, -1.2465]],
       dtype=torch.float64)
	q_value: tensor([[-25.5013]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5929968875928606, distance: 1.444322433699529 entropy 1.2378510236740112
epoch: 0, step: 3
	action: tensor([[ 1.6027, -1.0431,  1.3312,  0.7422,  0.2513, -1.0720, -0.3747]],
       dtype=torch.float64)
	q_value: tensor([[-33.1499]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.42083239582495025, distance: 1.364043180358087 entropy 1.2378510236740112
epoch: 0, step: 4
	action: tensor([[ 0.5015, -0.7582, -1.4684, -0.8849,  0.5874,  0.1360,  0.6261]],
       dtype=torch.float64)
	q_value: tensor([[-30.7668]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09632624415767677, distance: 1.0878337488345093 entropy 1.2378510236740112
epoch: 0, step: 5
	action: tensor([[ 1.3064, -1.0903, -0.6773, -0.8261, -0.1169,  0.0804,  0.0474]],
       dtype=torch.float64)
	q_value: tensor([[-27.9083]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9645959114922009, distance: 1.6039591789726495 entropy 1.2378510236740112
epoch: 0, step: 6
	action: tensor([[-0.6606, -0.0908, -0.7933,  1.8027, -1.9519, -0.4105, -0.2347]],
       dtype=torch.float64)
	q_value: tensor([[-27.6598]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03734727208948252, distance: 1.1655174440010554 entropy 1.2378510236740112
epoch: 0, step: 7
	action: tensor([[-0.4663, -2.7210,  0.7640, -0.1053,  0.4584,  1.0726, -0.5416]],
       dtype=torch.float64)
	q_value: tensor([[-38.3243]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4241949845847597 entropy 1.2378510236740112
epoch: 0, step: 8
	action: tensor([[ 0.6002,  0.3320, -1.4129, -0.6224, -0.5485, -0.8925, -0.7817]],
       dtype=torch.float64)
	q_value: tensor([[-28.2834]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8892198136409828, distance: 0.38087962332785646 entropy 1.2378510236740112
epoch: 0, step: 9
	action: tensor([[ 1.0367, -1.1156,  0.3172, -0.1266,  1.0202, -0.0962,  0.1400]],
       dtype=torch.float64)
	q_value: tensor([[-26.9375]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.619225850458514, distance: 1.45616439417614 entropy 1.2378510236740112
epoch: 0, step: 10
	action: tensor([[ 0.5309,  0.8240,  0.2469, -0.0898,  0.2436,  0.0592,  1.3479]],
       dtype=torch.float64)
	q_value: tensor([[-24.7589]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.2960031424846193 entropy 1.2378510236740112
epoch: 0, step: 11
	action: tensor([[ 0.5366, -1.4025,  0.2771,  0.6776,  0.0337, -1.9681, -1.2394]],
       dtype=torch.float64)
	q_value: tensor([[-23.8661]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3127452876016388 entropy 1.2378510236740112
epoch: 0, step: 12
	action: tensor([[-0.5700, -2.0153, -0.2388, -0.0483,  1.3393, -1.9691, -1.5070]],
       dtype=torch.float64)
	q_value: tensor([[-34.9330]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.5030890216104316 entropy 1.2378510236740112
epoch: 0, step: 13
	action: tensor([[-1.2757, -1.1768, -0.7033,  0.5749,  1.0010, -0.2727,  0.9500]],
       dtype=torch.float64)
	q_value: tensor([[-36.9985]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.7139048419248235, distance: 1.885185111476158 entropy 1.2378510236740112
epoch: 0, step: 14
	action: tensor([[-0.3601, -0.7479, -0.0115, -0.3908,  0.4539,  0.0981,  0.6267]],
       dtype=torch.float64)
	q_value: tensor([[-29.5169]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8048339969563172, distance: 1.5373591066889716 entropy 1.2378510236740112
epoch: 0, step: 15
	action: tensor([[ 0.5360, -0.0018,  1.6788,  0.1058, -0.5303,  0.0837,  0.8679]],
       dtype=torch.float64)
	q_value: tensor([[-22.3013]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6981025268996384, distance: 0.6287622107202843 entropy 1.2378510236740112
epoch: 0, step: 16
	action: tensor([[ 0.0811,  0.5208,  0.6493, -0.5699, -1.3891,  1.0275,  0.2901]],
       dtype=torch.float64)
	q_value: tensor([[-26.1796]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.5615760992391616 entropy 1.2378510236740112
epoch: 0, step: 17
	action: tensor([[ 0.3471, -1.0355, -0.2794, -0.6906,  2.0751, -1.0240,  0.2460]],
       dtype=torch.float64)
	q_value: tensor([[-26.4845]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6990193624690866, distance: 1.4916119614928918 entropy 1.2378510236740112
epoch: 0, step: 18
	action: tensor([[-0.1678,  0.2722,  0.1168, -0.3164,  0.2093, -0.2853,  1.2874]],
       dtype=torch.float64)
	q_value: tensor([[-31.2450]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.062034763703735285, distance: 1.108281450680868 entropy 1.2378510236740112
epoch: 0, step: 19
	action: tensor([[-0.3200, -1.2041,  0.4242,  1.6277, -1.1826,  0.9987,  0.7492]],
       dtype=torch.float64)
	q_value: tensor([[-23.2138]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28089712082413976, distance: 0.9704031711365967 entropy 1.2378510236740112
epoch: 0, step: 20
	action: tensor([[ 0.7441,  0.1532, -0.3204, -1.2341,  0.6704, -0.7175,  1.2344]],
       dtype=torch.float64)
	q_value: tensor([[-34.1034]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17941697476062457, distance: 1.0366161190016057 entropy 1.2378510236740112
epoch: 0, step: 21
	action: tensor([[-0.4687, -0.9742, -0.1994, -0.6701,  0.9227, -0.0625, -0.5871]],
       dtype=torch.float64)
	q_value: tensor([[-26.2256]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9411826894536319, distance: 1.5943728789025138 entropy 1.2378510236740112
epoch: 0, step: 22
	action: tensor([[-0.2427, -0.7094,  1.1759,  1.6214,  0.3446, -0.2218,  1.1730]],
       dtype=torch.float64)
	q_value: tensor([[-26.4335]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6934002278445572, distance: 0.6336400318982028 entropy 1.2378510236740112
epoch: 0, step: 23
	action: tensor([[ 1.4468, -0.1452, -0.6568, -0.3884,  0.4381, -0.4555,  0.2240]],
       dtype=torch.float64)
	q_value: tensor([[-29.2151]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.027853466923882686, distance: 1.128294728607188 entropy 1.2378510236740112
epoch: 0, step: 24
	action: tensor([[ 0.6136,  0.0793, -1.3878,  0.2817, -0.5447, -0.6164,  0.8702]],
       dtype=torch.float64)
	q_value: tensor([[-24.6279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8250299774768945, distance: 0.47867254341813353 entropy 1.2378510236740112
epoch: 0, step: 25
	action: tensor([[ 0.6889, -0.9587, -0.8471, -0.3199, -0.9926,  0.9714,  0.2637]],
       dtype=torch.float64)
	q_value: tensor([[-26.7981]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.41040169842049434, distance: 1.3590270606854182 entropy 1.2378510236740112
epoch: 0, step: 26
	action: tensor([[ 0.0824, -1.5558,  0.1807, -0.9723,  0.9821, -0.1933, -0.5958]],
       dtype=torch.float64)
	q_value: tensor([[-28.3546]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.6394258030413367 entropy 1.2378510236740112
epoch: 0, step: 27
	action: tensor([[-0.2372,  0.0172,  0.3189,  0.9429, -0.6595, -0.1595,  0.5460]],
       dtype=torch.float64)
	q_value: tensor([[-28.1493]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0806742075755529 entropy 1.2378510236740112
epoch: 0, step: 28
	action: tensor([[ 0.4550, -1.1682,  0.6977,  0.3596,  1.0782,  1.0701,  0.3625]],
       dtype=torch.float64)
	q_value: tensor([[-23.0560]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.052383599297554984, distance: 1.1739341294077041 entropy 1.2378510236740112
epoch: 0, step: 29
	action: tensor([[ 1.0574, -0.9373, -0.0293, -0.5414,  0.7419, -0.2921,  0.2835]],
       dtype=torch.float64)
	q_value: tensor([[-27.1006]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.670979063054385, distance: 1.479252110027421 entropy 1.2378510236740112
epoch: 0, step: 30
	action: tensor([[ 0.6941,  0.4561, -0.6116, -0.1374, -0.4027,  1.5020,  0.8315]],
       dtype=torch.float64)
	q_value: tensor([[-24.4407]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9084540944920366, distance: 0.3462391362354579 entropy 1.2378510236740112
epoch: 0, step: 31
	action: tensor([[ 0.8145, -0.1120, -1.3791,  0.5179, -0.3464,  0.5628, -2.2678]],
       dtype=torch.float64)
	q_value: tensor([[-27.9049]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.590888549190957, distance: 0.731943420008613 entropy 1.2378510236740112
epoch: 0, step: 32
	action: tensor([[ 0.6800, -1.5009,  2.2668,  0.4371,  0.9040, -1.1977,  1.2995]],
       dtype=torch.float64)
	q_value: tensor([[-31.6282]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.25391660829901 entropy 1.2378510236740112
epoch: 0, step: 33
	action: tensor([[ 0.8811,  0.9674, -0.4301, -0.8374,  0.8949,  0.9961,  0.5181]],
       dtype=torch.float64)
	q_value: tensor([[-30.7382]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0357407213787684 entropy 1.2378510236740112
epoch: 0, step: 34
	action: tensor([[-0.1363, -1.7105,  1.1267, -0.4937,  1.7614,  0.1568,  0.1122]],
       dtype=torch.float64)
	q_value: tensor([[-26.0117]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.686987680738327 entropy 1.2378510236740112
epoch: 0, step: 35
	action: tensor([[ 1.3402,  0.3704,  0.4370, -0.7987, -1.2401, -0.1924, -0.5519]],
       dtype=torch.float64)
	q_value: tensor([[-28.8210]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5849951655928974, distance: 0.7371965111224358 entropy 1.2378510236740112
epoch: 0, step: 36
	action: tensor([[ 0.7499, -1.3202,  1.5595, -1.4791, -0.6485, -1.5829, -0.0397]],
       dtype=torch.float64)
	q_value: tensor([[-28.3796]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1508280250989278 entropy 1.2378510236740112
epoch: 0, step: 37
	action: tensor([[ 1.8459, -0.2921,  0.1093,  0.0222, -0.9165,  0.4259,  1.0003]],
       dtype=torch.float64)
	q_value: tensor([[-32.6268]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0379941650507734 entropy 1.2378510236740112
epoch: 0, step: 38
	action: tensor([[ 0.6245, -1.4706,  0.7047, -0.8172, -0.6600, -0.6551,  1.6059]],
       dtype=torch.float64)
	q_value: tensor([[-23.6811]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.5860208556161797 entropy 1.2378510236740112
epoch: 0, step: 39
	action: tensor([[ 0.0883,  0.3785, -1.0424, -0.8056, -0.1083,  0.4600, -0.2827]],
       dtype=torch.float64)
	q_value: tensor([[-28.9565]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9980864610420654 entropy 1.2378510236740112
epoch: 0, step: 40
	action: tensor([[-0.0915, -0.3357,  0.3864,  0.1471,  0.4095,  0.6210, -0.0450]],
       dtype=torch.float64)
	q_value: tensor([[-24.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3085181358577863, distance: 0.9515839105238943 entropy 1.2378510236740112
epoch: 0, step: 41
	action: tensor([[ 0.7772, -0.3495, -1.1111,  0.0394,  0.3606,  1.1663,  0.3610]],
       dtype=torch.float64)
	q_value: tensor([[-21.9041]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7048873731095758, distance: 0.6216566578352312 entropy 1.2378510236740112
epoch: 0, step: 42
	action: tensor([[ 0.1876,  0.0939,  1.9889, -0.1328,  0.1329,  0.2515, -0.1182]],
       dtype=torch.float64)
	q_value: tensor([[-25.9113]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5472902346333084, distance: 0.7699572711596266 entropy 1.2378510236740112
epoch: 0, step: 43
	action: tensor([[ 1.5453, -0.3712,  1.0674,  0.8688,  1.6157,  0.3214,  0.3926]],
       dtype=torch.float64)
	q_value: tensor([[-27.2638]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0836533110893265, distance: 1.1912471494667234 entropy 1.2378510236740112
epoch: 0, step: 44
	action: tensor([[-0.9011, -0.5374,  1.3723,  0.7293,  1.0366, -0.0608,  0.0213]],
       dtype=torch.float64)
	q_value: tensor([[-29.0975]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3906928193115575, distance: 1.349498168197818 entropy 1.2378510236740112
epoch: 0, step: 45
	action: tensor([[-0.0998, -0.1741, -0.7550,  0.7464,  0.4097, -0.4429, -0.3217]],
       dtype=torch.float64)
	q_value: tensor([[-28.9693]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.008549468403886706, distance: 1.1492256104755003 entropy 1.2378510236740112
epoch: 0, step: 46
	action: tensor([[ 0.7384, -1.1543, -0.8519, -1.4324,  1.4787, -1.3231, -0.1988]],
       dtype=torch.float64)
	q_value: tensor([[-22.8362]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5506042780693794, distance: 1.42497479366265 entropy 1.2378510236740112
epoch: 0, step: 47
	action: tensor([[-0.0745, -2.2227,  0.8592,  0.2691,  0.8586, -0.5472, -0.5991]],
       dtype=torch.float64)
	q_value: tensor([[-33.9271]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.508049936244214 entropy 1.2378510236740112
epoch: 0, step: 48
	action: tensor([[ 1.0161, -0.4603,  0.7143, -2.2356,  0.5606,  0.7154,  0.6380]],
       dtype=torch.float64)
	q_value: tensor([[-27.8435]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2047597785453974, distance: 1.2560499867236288 entropy 1.2378510236740112
epoch: 0, step: 49
	action: tensor([[ 0.2850,  0.3802, -0.7582,  0.1214,  0.1505,  0.1799, -0.3440]],
       dtype=torch.float64)
	q_value: tensor([[-30.6595]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.5137678780995572 entropy 1.2378510236740112
epoch: 0, step: 50
	action: tensor([[ 1.2072,  0.9637,  1.2200, -0.0696,  0.3962, -0.2689,  0.0160]],
       dtype=torch.float64)
	q_value: tensor([[-20.3615]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0362579632912352 entropy 1.2378510236740112
epoch: 0, step: 51
	action: tensor([[-0.5442, -0.6606,  0.0723, -0.1584,  0.7587,  1.0679, -0.3954]],
       dtype=torch.float64)
	q_value: tensor([[-23.0741]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.361141087224246, distance: 1.33508299753503 entropy 1.2378510236740112
epoch: 0, step: 52
	action: tensor([[ 0.6435, -0.1018, -1.4300, -0.2345, -0.3161,  1.1288,  0.4934]],
       dtype=torch.float64)
	q_value: tensor([[-25.4010]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5949481572862233, distance: 0.7283028331604366 entropy 1.2378510236740112
epoch: 0, step: 53
	action: tensor([[ 0.2655,  0.4375, -0.0842, -0.3016, -0.4017,  1.2267, -0.2939]],
       dtype=torch.float64)
	q_value: tensor([[-26.8448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.6134877142184578 entropy 1.2378510236740112
epoch: 0, step: 54
	action: tensor([[ 0.3281, -0.6957,  0.5084, -0.5951, -0.5315,  0.1584,  1.3692]],
       dtype=torch.float64)
	q_value: tensor([[-24.4293]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5002476340013373, distance: 1.401645440225754 entropy 1.2378510236740112
epoch: 0, step: 55
	action: tensor([[ 0.7679, -0.9231,  0.8078, -0.6651, -0.6502, -1.7605,  0.9882]],
       dtype=torch.float64)
	q_value: tensor([[-26.0038]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.32675172567396604, distance: 1.3181096023717915 entropy 1.2378510236740112
epoch: 0, step: 56
	action: tensor([[-0.0978, -1.0381,  0.6216,  0.5411,  0.5971, -0.2641,  2.1806]],
       dtype=torch.float64)
	q_value: tensor([[-29.7304]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.35366588174598323, distance: 1.3314119013420573 entropy 1.2378510236740112
epoch: 0, step: 57
	action: tensor([[-0.3315, -0.0184, -0.9607,  1.1487, -0.3652,  0.6800,  1.4509]],
       dtype=torch.float64)
	q_value: tensor([[-28.4883]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.30245144458539897, distance: 1.305982820760348 entropy 1.2378510236740112
epoch: 0, step: 58
	action: tensor([[ 0.9042,  0.3960,  0.8384, -0.5562,  0.4910,  0.3620, -0.6680]],
       dtype=torch.float64)
	q_value: tensor([[-28.4393]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1674056491418427 entropy 1.2378510236740112
epoch: 0, step: 59
	action: tensor([[ 0.4673, -0.5703, -0.8353, -1.6420,  0.7355, -0.3381,  0.3713]],
       dtype=torch.float64)
	q_value: tensor([[-25.6535]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1645824921379282, distance: 1.2349285232421114 entropy 1.2378510236740112
epoch: 0, step: 60
	action: tensor([[ 2.0064, -0.4052,  1.7882, -0.4642, -0.7174, -1.3317,  1.4094]],
       dtype=torch.float64)
	q_value: tensor([[-28.1527]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8552726128997042 entropy 1.2378510236740112
epoch: 0, step: 61
	action: tensor([[ 1.4306, -0.5448,  1.0090, -1.6188,  0.2781, -0.0463,  0.3246]],
       dtype=torch.float64)
	q_value: tensor([[-31.1412]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16387977721684732, distance: 1.0463839198413498 entropy 1.2378510236740112
epoch: 0, step: 62
	action: tensor([[ 0.6395, -0.6663, -0.8698,  1.1792,  0.3955,  2.2626,  0.2746]],
       dtype=torch.float64)
	q_value: tensor([[-27.4467]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.6266400447255301 entropy 1.2378510236740112
epoch: 0, step: 63
	action: tensor([[ 1.2676, -0.0957,  0.1817, -1.4745,  0.2607,  0.4607,  1.0485]],
       dtype=torch.float64)
	q_value: tensor([[-30.6239]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.012454879851891754, distance: 1.1514485368013947 entropy 1.2378510236740112
epoch: 0, step: 64
	action: tensor([[ 0.5489, -0.2663, -0.3168,  1.9437,  0.1153,  0.1713,  0.5841]],
       dtype=torch.float64)
	q_value: tensor([[-27.4938]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.23104308091261827 entropy 1.2378510236740112
epoch: 0, step: 65
	action: tensor([[-0.2940, -0.9912,  0.3174,  1.0283, -0.7399, -0.8268,  0.7783]],
       dtype=torch.float64)
	q_value: tensor([[-24.1965]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0796542844532453, distance: 1.1890470767253638 entropy 1.2378510236740112
epoch: 0, step: 66
	action: tensor([[ 1.5460, -1.1535,  0.6478, -0.0510,  0.8816, -0.4052, -0.0978]],
       dtype=torch.float64)
	q_value: tensor([[-28.8582]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5739166773141045, distance: 1.4356466499662701 entropy 1.2378510236740112
epoch: 0, step: 67
	action: tensor([[-0.5496, -1.3782, -0.8379, -2.2773,  0.6146, -0.0627,  1.0254]],
       dtype=torch.float64)
	q_value: tensor([[-26.5425]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8062948880780993 entropy 1.2378510236740112
epoch: 0, step: 68
	action: tensor([[-0.2238, -0.2807, -0.2964, -1.4478, -0.5642, -0.5953,  1.2669]],
       dtype=torch.float64)
	q_value: tensor([[-33.5943]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07473786276929206, distance: 1.1863367058312229 entropy 1.2378510236740112
epoch: 0, step: 69
	action: tensor([[ 0.7264, -1.7115, -0.0552, -0.4172, -0.1760,  0.6575,  0.4431]],
       dtype=torch.float64)
	q_value: tensor([[-27.4513]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4780606542347992 entropy 1.2378510236740112
epoch: 0, step: 70
	action: tensor([[-0.1324,  1.1999, -2.3557,  0.1556,  0.4041, -0.9086,  0.9414]],
       dtype=torch.float64)
	q_value: tensor([[-25.6742]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9522286550797225 entropy 1.2378510236740112
epoch: 0, step: 71
	action: tensor([[ 0.9123, -2.1173,  1.8931, -2.0873,  0.0518, -0.1728,  1.0347]],
       dtype=torch.float64)
	q_value: tensor([[-26.2657]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0298084466439767 entropy 1.2378510236740112
epoch: 0, step: 72
	action: tensor([[-0.4027, -0.6316,  0.5978, -0.8477,  1.6224, -0.8096, -0.5247]],
       dtype=torch.float64)
	q_value: tensor([[-28.3226]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0370036220440757, distance: 1.6332497255563756 entropy 1.2378510236740112
epoch: 0, step: 73
	action: tensor([[-0.5128, -0.6014,  0.8681, -1.8129,  1.2066,  0.3006,  1.1398]],
       dtype=torch.float64)
	q_value: tensor([[-29.4559]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8866146676320297, distance: 1.571803663067333 entropy 1.2378510236740112
epoch: 0, step: 74
	action: tensor([[-0.8318, -0.6877,  0.6650, -0.0590, -1.7079, -0.9555, -0.6194]],
       dtype=torch.float64)
	q_value: tensor([[-30.1657]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.738793048600519, distance: 1.5089701226811743 entropy 1.2378510236740112
epoch: 0, step: 75
	action: tensor([[ 0.2885, -0.6083,  0.3638,  1.0203,  1.0961,  1.0838,  1.0521]],
       dtype=torch.float64)
	q_value: tensor([[-33.0951]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8059722047719513, distance: 0.5040674659353199 entropy 1.2378510236740112
epoch: 0, step: 76
	action: tensor([[ 0.1575, -0.2820,  0.5457,  1.1685,  0.7762, -0.6287, -0.3851]],
       dtype=torch.float64)
	q_value: tensor([[-27.2635]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.4796004684664066 entropy 1.2378510236740112
epoch: 0, step: 77
	action: tensor([[ 0.8039, -0.3537,  0.0183, -0.6717,  0.1795,  0.1153, -0.7204]],
       dtype=torch.float64)
	q_value: tensor([[-26.1274]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0006841072389709435, distance: 1.14473561417238 entropy 1.2378510236740112
epoch: 0, step: 78
	action: tensor([[ 0.6972, -0.5128,  0.7437, -0.3631,  1.5585, -0.9942,  1.4712]],
       dtype=torch.float64)
	q_value: tensor([[-23.1739]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3123071104537021, distance: 0.9489732274750267 entropy 1.2378510236740112
epoch: 0, step: 79
	action: tensor([[ 0.1133, -0.1140, -1.0379, -0.8571, -0.3623,  1.3390,  0.8319]],
       dtype=torch.float64)
	q_value: tensor([[-26.7451]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3301009945022404, distance: 0.9366155413576032 entropy 1.2378510236740112
epoch: 0, step: 80
	action: tensor([[-0.3292, -0.7394,  0.1368,  0.3075,  1.2608, -0.4057,  0.0383]],
       dtype=torch.float64)
	q_value: tensor([[-28.3849]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5879694089383263, distance: 1.4420415006781497 entropy 1.2378510236740112
epoch: 0, step: 81
	action: tensor([[ 0.7054, -0.1590, -0.0879,  0.6639,  0.3671, -0.1133, -0.1954]],
       dtype=torch.float64)
	q_value: tensor([[-24.6440]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8704563670529258, distance: 0.41187433376183574 entropy 1.2378510236740112
epoch: 0, step: 82
	action: tensor([[-0.2046, -0.3893,  0.9765, -0.5998,  1.2168, -0.1153, -0.2060]],
       dtype=torch.float64)
	q_value: tensor([[-22.0983]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6701573887467311, distance: 1.47888836731585 entropy 1.2378510236740112
epoch: 0, step: 83
	action: tensor([[-0.4158,  0.1242, -0.2286,  0.0080,  0.4991, -0.3663, -0.6266]],
       dtype=torch.float64)
	q_value: tensor([[-25.4205]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.22849148453481005, distance: 1.2683606751565883 entropy 1.2378510236740112
epoch: 0, step: 84
	action: tensor([[ 0.0419,  1.0721,  0.5346, -0.3192, -0.2416, -0.6410,  1.2822]],
       dtype=torch.float64)
	q_value: tensor([[-22.1213]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.119678325946503 entropy 1.2378510236740112
epoch: 0, step: 85
	action: tensor([[-1.2738, -0.7124,  0.3229, -0.4373,  0.3663, -0.0493, -0.4381]],
       dtype=torch.float64)
	q_value: tensor([[-20.6128]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.5849478301644524, distance: 1.8398507091904406 entropy 1.2378510236740112
epoch: 0, step: 86
	action: tensor([[ 1.1905,  0.2232,  1.5663, -0.0859,  1.2338,  0.7895,  1.4872]],
       dtype=torch.float64)
	q_value: tensor([[-25.6806]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6887490976305699, distance: 0.6384281131061438 entropy 1.2378510236740112
epoch: 0, step: 87
	action: tensor([[-0.3865, -0.5780,  0.7320, -0.7017,  0.2048, -0.6099,  0.1271]],
       dtype=torch.float64)
	q_value: tensor([[-28.8020]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0382938557806898, distance: 1.6337668921174384 entropy 1.2378510236740112
epoch: 0, step: 88
	action: tensor([[ 0.9308, -1.0100,  0.4029, -0.4982,  0.5383,  0.4454, -0.4798]],
       dtype=torch.float64)
	q_value: tensor([[-23.4763]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5800615884313784, distance: 1.438446457377611 entropy 1.2378510236740112
epoch: 0, step: 89
	action: tensor([[ 1.1530,  0.2457, -0.7203, -0.0954,  0.3038,  0.1614,  0.0722]],
       dtype=torch.float64)
	q_value: tensor([[-25.3556]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.826169755502057, distance: 0.477110927776776 entropy 1.2378510236740112
epoch: 0, step: 90
	action: tensor([[ 0.6868, -0.4198,  0.6070,  0.3372, -0.0712, -1.5286, -0.6534]],
       dtype=torch.float64)
	q_value: tensor([[-22.6389]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2586618758249791, distance: 0.9852917823909375 entropy 1.2378510236740112
epoch: 0, step: 91
	action: tensor([[ 0.3377, -1.4038, -0.5279,  0.5390, -0.4157, -1.1170, -0.1145]],
       dtype=torch.float64)
	q_value: tensor([[-28.2906]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.361887426134062 entropy 1.2378510236740112
epoch: 0, step: 92
	action: tensor([[-0.0831, -0.5518,  0.5527, -0.9897,  0.1288, -0.5994, -0.2410]],
       dtype=torch.float64)
	q_value: tensor([[-27.9656]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8020402160458144, distance: 1.5361687734114347 entropy 1.2378510236740112
epoch: 0, step: 93
	action: tensor([[ 0.0269, -0.4853,  1.7764, -0.1467, -0.4275,  0.6662,  0.7351]],
       dtype=torch.float64)
	q_value: tensor([[-23.8423]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.049986005559443325, distance: 1.1153770255533577 entropy 1.2378510236740112
epoch: 0, step: 94
	action: tensor([[ 0.9224, -0.6633,  0.4920,  0.5918, -1.2371, -1.4982,  2.2034]],
       dtype=torch.float64)
	q_value: tensor([[-27.0794]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1841762992658823, distance: 1.0336055967327473 entropy 1.2378510236740112
epoch: 0, step: 95
	action: tensor([[ 0.8935, -2.3797,  0.0711, -0.5448,  1.1161,  0.1686,  0.7314]],
       dtype=torch.float64)
	q_value: tensor([[-34.7839]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.5999917505071022 entropy 1.2378510236740112
epoch: 0, step: 96
	action: tensor([[ 1.5859, -1.2134, -0.4730,  0.5220,  1.0826, -0.2664, -1.0662]],
       dtype=torch.float64)
	q_value: tensor([[-26.4478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3447526540207939, distance: 1.327021313863079 entropy 1.2378510236740112
epoch: 0, step: 97
	action: tensor([[ 0.1814, -0.3510, -1.2804,  0.4678, -0.1435, -0.7284, -0.5039]],
       dtype=torch.float64)
	q_value: tensor([[-31.6327]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1971593582726614, distance: 1.0253481872997974 entropy 1.2378510236740112
epoch: 0, step: 98
	action: tensor([[-0.2764,  0.9820,  1.3431, -0.1542, -0.5192,  0.3196, -0.4200]],
       dtype=torch.float64)
	q_value: tensor([[-25.4579]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8632828441910019 entropy 1.2378510236740112
epoch: 0, step: 99
	action: tensor([[ 0.6566, -0.5967,  1.2077, -0.7482, -0.0820, -1.2929, -0.1756]],
       dtype=torch.float64)
	q_value: tensor([[-21.0085]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08187296571269975, distance: 1.1902681910057629 entropy 1.2378510236740112
epoch: 0, step: 100
	action: tensor([[ 1.8351,  0.4299, -0.5423,  0.2122,  0.8222,  0.5752, -0.5312]],
       dtype=torch.float64)
	q_value: tensor([[-27.1643]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.5857007070762758 entropy 1.2378510236740112
epoch: 0, step: 101
	action: tensor([[ 0.3341,  0.2090, -0.4323,  0.3186, -0.1159,  0.2287, -0.2002]],
       dtype=torch.float64)
	q_value: tensor([[-25.6268]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.109733502388916 entropy 1.2378510236740112
epoch: 0, step: 102
	action: tensor([[ 0.3374, -0.4130, -1.4688,  0.9649, -1.4104, -1.0814, -0.0920]],
       dtype=torch.float64)
	q_value: tensor([[-21.1497]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4084425630209457, distance: 0.8801468775312448 entropy 1.2378510236740112
epoch: 0, step: 103
	action: tensor([[-0.0339, -0.2266,  0.6786, -1.2183,  1.3830,  1.0052,  1.0699]],
       dtype=torch.float64)
	q_value: tensor([[-33.7792]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.28646237293131716, distance: 1.2979418549007944 entropy 1.2378510236740112
epoch: 0, step: 104
	action: tensor([[-0.6347,  0.5342, -0.6374, -0.9222,  0.9815,  0.3510,  1.8171]],
       dtype=torch.float64)
	q_value: tensor([[-28.8696]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13931223794780967, distance: 1.0616454607761543 entropy 1.2378510236740112
epoch: 0, step: 105
	action: tensor([[ 0.5814, -0.7766, -0.5324, -0.8771,  0.3787,  0.1819,  0.9347]],
       dtype=torch.float64)
	q_value: tensor([[-30.3536]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.42544219150969553, distance: 1.3662541619014696 entropy 1.2378510236740112
epoch: 0, step: 106
	action: tensor([[ 0.8297,  0.1687,  1.3128, -0.8217,  0.3004, -0.7842,  0.6043]],
       dtype=torch.float64)
	q_value: tensor([[-25.4963]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5740916105026032, distance: 0.7468180237497398 entropy 1.2378510236740112
epoch: 0, step: 107
	action: tensor([[ 0.7706,  0.2714,  0.3990,  1.4065, -0.4333, -0.5447,  1.3177]],
       dtype=torch.float64)
	q_value: tensor([[-25.0317]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9099213131172502, distance: 0.34345331821534686 entropy 1.2378510236740112
epoch: 0, step: 108
	action: tensor([[ 1.5026, -1.3093,  0.4600, -0.2795,  0.2980,  0.3364,  0.8378]],
       dtype=torch.float64)
	q_value: tensor([[-28.2452]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.5283725314574275 entropy 1.2378510236740112
epoch: 0, step: 109
	action: tensor([[ 0.1801,  0.6309, -0.8894, -0.3477, -0.6526, -0.2234,  0.1064]],
       dtype=torch.float64)
	q_value: tensor([[-26.1862]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9606589393934737 entropy 1.2378510236740112
epoch: 0, step: 110
	action: tensor([[ 0.8708, -2.8876,  0.6181, -0.3997,  0.5783, -0.3415,  1.0775]],
       dtype=torch.float64)
	q_value: tensor([[-23.9495]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.5418838890500386 entropy 1.2378510236740112
epoch: 0, step: 111
	action: tensor([[ 1.0304, -0.4074,  0.0465,  0.2861, -1.0357, -0.4253,  0.2791]],
       dtype=torch.float64)
	q_value: tensor([[-25.1089]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4399558862915023, distance: 0.8563825547505689 entropy 1.2378510236740112
epoch: 0, step: 112
	action: tensor([[-0.6538,  0.1075,  1.3655,  0.6071,  0.2794, -0.5756, -0.8980]],
       dtype=torch.float64)
	q_value: tensor([[-25.5827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06817691688734295, distance: 1.1827100511137427 entropy 1.2378510236740112
epoch: 0, step: 113
	action: tensor([[ 0.5808, -1.8231,  1.4688,  0.7242,  0.8226, -0.3819,  0.6800]],
       dtype=torch.float64)
	q_value: tensor([[-30.5703]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.291638703768934 entropy 1.2378510236740112
epoch: 0, step: 114
	action: tensor([[ 0.2126, -0.5824,  0.2138, -1.4771,  0.3454, -0.3459,  0.4884]],
       dtype=torch.float64)
	q_value: tensor([[-27.2421]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6375509107458612, distance: 1.464381044991296 entropy 1.2378510236740112
epoch: 0, step: 115
	action: tensor([[ 2.1582, -0.6058,  1.7836,  0.1583, -0.3199, -0.5157, -0.7796]],
       dtype=torch.float64)
	q_value: tensor([[-24.7830]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.226437314012576 entropy 1.2378510236740112
epoch: 0, step: 116
	action: tensor([[-1.1954,  0.3512, -0.2923, -1.8145,  0.4511,  0.1542, -0.4701]],
       dtype=torch.float64)
	q_value: tensor([[-30.8711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5213946048844431, distance: 1.4114894265192837 entropy 1.2378510236740112
epoch: 0, step: 117
	action: tensor([[-0.6629,  0.1139,  0.1362, -1.3389,  0.9636,  0.4522,  0.6818]],
       dtype=torch.float64)
	q_value: tensor([[-28.8310]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5981166060171452, distance: 1.4466415193366156 entropy 1.2378510236740112
epoch: 0, step: 118
	action: tensor([[ 0.6561, -1.4935, -0.3497, -0.3128, -0.2600,  1.3727,  0.2885]],
       dtype=torch.float64)
	q_value: tensor([[-27.1427]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2761650120119536 entropy 1.2378510236740112
epoch: 0, step: 119
	action: tensor([[-0.1184, -0.3262,  0.1007, -0.0996,  0.9982,  1.1370, -0.4205]],
       dtype=torch.float64)
	q_value: tensor([[-27.8410]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33700042139993935, distance: 0.9317798606855641 entropy 1.2378510236740112
epoch: 0, step: 120
	action: tensor([[ 0.4440,  1.3153,  0.2372, -0.0778,  0.2029, -0.0037,  0.1723]],
       dtype=torch.float64)
	q_value: tensor([[-25.1153]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.6785451742695329 entropy 1.2378510236740112
epoch: 0, step: 121
	action: tensor([[-0.3900, -0.3799,  0.9735, -0.7834, -0.1066,  0.8061,  0.6221]],
       dtype=torch.float64)
	q_value: tensor([[-22.3440]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6606810761311395, distance: 1.4746868638271362 entropy 1.2378510236740112
epoch: 0, step: 122
	action: tensor([[-0.1965, -0.1427,  0.5675, -1.5944, -0.5065,  0.6748, -0.7393]],
       dtype=torch.float64)
	q_value: tensor([[-25.0592]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6648483063336685, distance: 1.47653595735306 entropy 1.2378510236740112
epoch: 0, step: 123
	action: tensor([[ 2.3917,  1.0922,  1.1508, -1.7007,  1.3725, -0.0734,  0.8914]],
       dtype=torch.float64)
	q_value: tensor([[-27.2095]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.37528574764284794 entropy 1.2378510236740112
epoch: 0, step: 124
	action: tensor([[ 0.3995, -0.5071,  1.3192, -2.3482, -1.1431,  0.3238,  2.2744]],
       dtype=torch.float64)
	q_value: tensor([[-32.1810]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1914166637453878 entropy 1.2378510236740112
epoch: 0, step: 125
	action: tensor([[ 0.0393, -0.5182,  1.1862, -0.4746,  1.0977, -1.1869, -0.2358]],
       dtype=torch.float64)
	q_value: tensor([[-35.9400]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20614710376528045, distance: 1.2567729725383547 entropy 1.2378510236740112
epoch: 0, step: 126
	action: tensor([[ 0.0574, -0.7756,  0.6223,  1.3132, -0.5780, -1.1594,  1.2820]],
       dtype=torch.float64)
	q_value: tensor([[-27.4358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.414948701242841, distance: 0.8752934270087397 entropy 1.2378510236740112
epoch: 0, step: 127
	action: tensor([[ 1.0386, -1.8910,  0.8814,  1.2350,  0.2112, -1.3472,  0.2069]],
       dtype=torch.float64)
	q_value: tensor([[-30.5266]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1905460620667203 entropy 1.2378510236740112
LOSS epoch 0 actor 268.0564936927577 critic 411.83840218233365 
epoch: 1, step: 0
	action: tensor([[ 1.0018, -0.9603,  1.7328, -0.4452, -1.1512,  0.4078,  0.5317]],
       dtype=torch.float64)
	q_value: tensor([[-39.4231]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9815091522049912 entropy 1.2378510236740112
epoch: 1, step: 1
	action: tensor([[-0.2155,  0.1752,  0.5417, -1.0037,  0.4839,  0.5961,  1.0644]],
       dtype=torch.float64)
	q_value: tensor([[-36.4363]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1930051520189311, distance: 1.2499074390869918 entropy 1.2378510236740112
epoch: 1, step: 2
	action: tensor([[ 0.6492, -0.6845,  0.1760, -1.5525, -1.4478, -0.3231,  0.8251]],
       dtype=torch.float64)
	q_value: tensor([[-33.2948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6189089399723107, distance: 1.4560218889327154 entropy 1.2378510236740112
epoch: 1, step: 3
	action: tensor([[ 1.1243, -0.8184,  0.7945,  1.1525,  0.0256,  0.3426, -0.5616]],
       dtype=torch.float64)
	q_value: tensor([[-39.9433]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4252182855311597, distance: 0.8675772695351439 entropy 1.2378510236740112
epoch: 1, step: 4
	action: tensor([[ 0.7355, -0.4059,  0.3539, -0.4416, -0.2466, -1.1570,  0.8885]],
       dtype=torch.float64)
	q_value: tensor([[-36.5983]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09508705119429361, distance: 1.19751514628378 entropy 1.2378510236740112
epoch: 1, step: 5
	action: tensor([[-0.4001, -2.6887,  1.7418, -1.2549, -0.8641,  0.8262, -0.6057]],
       dtype=torch.float64)
	q_value: tensor([[-32.8421]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.604635310008148 entropy 1.2378510236740112
epoch: 1, step: 6
	action: tensor([[ 0.7258, -0.7147,  1.6041,  0.2715,  0.1714, -0.6905, -0.2975]],
       dtype=torch.float64)
	q_value: tensor([[-42.3968]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0222456139436287 entropy 1.2378510236740112
epoch: 1, step: 7
	action: tensor([[ 0.0019, -1.3469,  1.3717,  0.0530,  1.4485,  0.3531, -0.1976]],
       dtype=torch.float64)
	q_value: tensor([[-40.5212]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.5294213423308054 entropy 1.2378510236740112
epoch: 1, step: 8
	action: tensor([[ 0.7539,  1.6932,  0.2975, -0.2072,  0.9961,  1.0477,  1.7167]],
       dtype=torch.float64)
	q_value: tensor([[-36.7448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0549970229122227 entropy 1.2378510236740112
epoch: 1, step: 9
	action: tensor([[ 1.0913,  1.3152,  1.4868, -1.1078, -0.8271,  0.3682, -2.0050]],
       dtype=torch.float64)
	q_value: tensor([[-34.2982]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.057056552690526 entropy 1.2378510236740112
epoch: 1, step: 10
	action: tensor([[ 0.2500, -0.4321,  0.1719, -0.5497,  0.2617, -1.1763,  0.8339]],
       dtype=torch.float64)
	q_value: tensor([[-32.4169]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3011222672078009, distance: 1.3053162600490482 entropy 1.2378510236740112
epoch: 1, step: 11
	action: tensor([[-1.2570, -0.6413,  0.2363, -1.2351,  1.3361,  0.4507, -0.7148]],
       dtype=torch.float64)
	q_value: tensor([[-31.4584]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0438324282925104, distance: 1.6359850702694023 entropy 1.2378510236740112
epoch: 1, step: 12
	action: tensor([[-0.9438, -1.6959,  0.2622, -0.7663,  0.6357, -0.2857,  1.9279]],
       dtype=torch.float64)
	q_value: tensor([[-39.6066]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.6673046818761845 entropy 1.2378510236740112
epoch: 1, step: 13
	action: tensor([[ 0.3516, -0.7447, -0.7169,  0.1129,  1.2922,  0.1799,  0.3358]],
       dtype=torch.float64)
	q_value: tensor([[-37.5477]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.026818261737151516, distance: 1.159587393395725 entropy 1.2378510236740112
epoch: 1, step: 14
	action: tensor([[-1.7820, -0.0272,  0.8878,  0.2911, -0.0856,  1.6623,  0.3294]],
       dtype=torch.float64)
	q_value: tensor([[-32.6350]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3444561874069099 entropy 1.2378510236740112
epoch: 1, step: 15
	action: tensor([[-0.2274, -1.5373, -0.0216, -0.9600, -0.2054, -1.1333,  1.5457]],
       dtype=torch.float64)
	q_value: tensor([[-41.3571]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.385214887419738 entropy 1.2378510236740112
epoch: 1, step: 16
	action: tensor([[-0.5032,  0.2447,  1.2828,  0.0819, -1.9697, -0.5810,  0.0083]],
       dtype=torch.float64)
	q_value: tensor([[-38.1377]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06878411818657959, distance: 1.1042867987006935 entropy 1.2378510236740112
epoch: 1, step: 17
	action: tensor([[ 0.9821, -1.4349,  0.1301, -1.0865,  1.4537, -0.6390,  0.8826]],
       dtype=torch.float64)
	q_value: tensor([[-41.8952]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.539029706900228 entropy 1.2378510236740112
epoch: 1, step: 18
	action: tensor([[ 2.4701, -0.0122,  0.3561, -0.3003,  0.1308, -0.8362, -0.6009]],
       dtype=torch.float64)
	q_value: tensor([[-38.4596]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0014740628350371 entropy 1.2378510236740112
epoch: 1, step: 19
	action: tensor([[ 1.1633, -0.3606, -0.8928,  0.1830,  0.6995, -0.5043,  1.7771]],
       dtype=torch.float64)
	q_value: tensor([[-32.7955]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9994955631753657 entropy 1.2378510236740112
epoch: 1, step: 20
	action: tensor([[-0.6528, -0.7818,  0.2621, -0.6084, -0.6318, -0.7672,  0.4596]],
       dtype=torch.float64)
	q_value: tensor([[-31.1812]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8305081402458672, distance: 1.5482551223471717 entropy 1.2378510236740112
epoch: 1, step: 21
	action: tensor([[ 0.9387,  2.1165, -1.7872,  0.4954,  0.8459,  0.6019, -0.3125]],
       dtype=torch.float64)
	q_value: tensor([[-32.5343]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9597433609553628 entropy 1.2378510236740112
epoch: 1, step: 22
	action: tensor([[ 0.0135, -1.9181, -1.2556, -1.6378, -0.4900, -1.5564,  0.2526]],
       dtype=torch.float64)
	q_value: tensor([[-24.9837]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7185348138227737 entropy 1.2378510236740112
epoch: 1, step: 23
	action: tensor([[-0.3350, -1.4558,  0.2455, -0.3231,  0.1466, -0.5205, -0.3454]],
       dtype=torch.float64)
	q_value: tensor([[-44.6738]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.657606456720461 entropy 1.2378510236740112
epoch: 1, step: 24
	action: tensor([[ 0.4473,  0.1665, -1.6075,  1.2011,  1.0299, -0.5432,  1.5694]],
       dtype=torch.float64)
	q_value: tensor([[-32.5741]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.6921830233691417 entropy 1.2378510236740112
epoch: 1, step: 25
	action: tensor([[ 0.0198,  0.3666, -0.2306, -0.6273,  1.5694,  0.9188,  1.0869]],
       dtype=torch.float64)
	q_value: tensor([[-26.5968]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9984380585076699 entropy 1.2378510236740112
epoch: 1, step: 26
	action: tensor([[-0.4989, -0.2074,  0.8206,  1.2197,  2.3979,  0.4100, -2.1018]],
       dtype=torch.float64)
	q_value: tensor([[-33.6923]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0078063468545653 entropy 1.2378510236740112
epoch: 1, step: 27
	action: tensor([[-0.2670, -0.9914,  0.3343,  0.8961,  0.2329,  0.4134, -1.1032]],
       dtype=torch.float64)
	q_value: tensor([[-45.5013]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13787993165412848, distance: 1.0625284577490104 entropy 1.2378510236740112
epoch: 1, step: 28
	action: tensor([[ 1.6017, -1.2461, -0.3198, -1.3098,  1.5638, -0.9009,  0.5120]],
       dtype=torch.float64)
	q_value: tensor([[-36.3768]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.479998364597657 entropy 1.2378510236740112
epoch: 1, step: 29
	action: tensor([[ 0.5005,  0.0537,  0.1241, -0.3637, -0.4841, -0.5594,  0.4289]],
       dtype=torch.float64)
	q_value: tensor([[-42.8487]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.43621866550345434, distance: 0.8592351596291975 entropy 1.2378510236740112
epoch: 1, step: 30
	action: tensor([[ 0.6310, -0.1340, -0.7311, -0.1981,  0.7760,  1.4568,  0.1630]],
       dtype=torch.float64)
	q_value: tensor([[-28.2806]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8903419733808419, distance: 0.378945632791066 entropy 1.2378510236740112
epoch: 1, step: 31
	action: tensor([[ 1.0274,  0.3534, -1.0567,  0.2347, -0.3979,  0.8861, -0.2294]],
       dtype=torch.float64)
	q_value: tensor([[-34.1155]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9618906763731364, distance: 0.22339441624930859 entropy 1.2378510236740112
epoch: 1, step: 32
	action: tensor([[ 0.5809, -0.1474, -0.6107, -0.6913, -0.7067, -0.7420, -0.8291]],
       dtype=torch.float64)
	q_value: tensor([[-31.6384]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3626423646652689, distance: 0.9135835892361635 entropy 1.2378510236740112
epoch: 1, step: 33
	action: tensor([[-1.0940, -0.3477, -1.4813, -0.2366,  0.2239,  0.5351, -0.5030]],
       dtype=torch.float64)
	q_value: tensor([[-32.4152]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2628085681518897, distance: 1.7213952511602906 entropy 1.2378510236740112
epoch: 1, step: 34
	action: tensor([[ 0.6508, -0.0768,  0.2017, -0.8973, -0.0125, -0.2233, -0.0020]],
       dtype=torch.float64)
	q_value: tensor([[-33.2356]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07865563216077331, distance: 1.098418113276141 entropy 1.2378510236740112
epoch: 1, step: 35
	action: tensor([[-0.0969, -0.0312,  0.8300, -0.5817,  0.3068,  0.7859, -1.0190]],
       dtype=torch.float64)
	q_value: tensor([[-28.2532]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08275285195764603, distance: 1.0959730584514316 entropy 1.2378510236740112
epoch: 1, step: 36
	action: tensor([[ 0.2767, -0.6237,  1.3819,  0.2057,  0.0227,  1.0815, -0.6458]],
       dtype=torch.float64)
	q_value: tensor([[-32.0804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.892723647471289 entropy 1.2378510236740112
epoch: 1, step: 37
	action: tensor([[ 1.7466,  0.8032,  1.7929,  0.7531,  0.2922, -0.7211,  0.4694]],
       dtype=torch.float64)
	q_value: tensor([[-35.3022]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9433035706164787 entropy 1.2378510236740112
epoch: 1, step: 38
	action: tensor([[ 1.2338,  0.6485,  0.9983,  0.0451,  0.8572, -0.0696,  0.2271]],
       dtype=torch.float64)
	q_value: tensor([[-34.1580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.767733217483399 entropy 1.2378510236740112
epoch: 1, step: 39
	action: tensor([[ 0.3666, -0.6226, -0.1549, -1.1357, -0.7161, -0.3920, -0.6296]],
       dtype=torch.float64)
	q_value: tensor([[-25.2008]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.45302853513315733, distance: 1.3794112539997885 entropy 1.2378510236740112
epoch: 1, step: 40
	action: tensor([[ 0.7053, -0.4027,  0.2989,  0.1020, -0.1886, -0.1628,  1.4886]],
       dtype=torch.float64)
	q_value: tensor([[-33.3466]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3498326876644712, distance: 0.9227185654691782 entropy 1.2378510236740112
epoch: 1, step: 41
	action: tensor([[-1.7504,  0.7448,  0.8763, -0.0169,  0.7427,  0.8611,  0.0095]],
       dtype=torch.float64)
	q_value: tensor([[-32.2556]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4212918671993804 entropy 1.2378510236740112
epoch: 1, step: 42
	action: tensor([[ 0.7022,  0.9346, -0.2625,  0.4907, -0.2937, -0.3898,  0.3788]],
       dtype=torch.float64)
	q_value: tensor([[-38.4418]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0228884128831115 entropy 1.2378510236740112
epoch: 1, step: 43
	action: tensor([[ 1.7555, -0.1012,  1.5006, -0.9439,  0.0411, -0.5250, -0.4885]],
       dtype=torch.float64)
	q_value: tensor([[-29.6441]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.6083508334770557 entropy 1.2378510236740112
epoch: 1, step: 44
	action: tensor([[ 1.2753,  0.2235, -0.9396, -1.3101,  0.1395, -1.5485,  0.4479]],
       dtype=torch.float64)
	q_value: tensor([[-36.1810]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9765921071346 entropy 1.2378510236740112
epoch: 1, step: 45
	action: tensor([[-0.8332, -1.5325,  1.9000, -1.6746,  0.3386,  0.3408,  1.1721]],
       dtype=torch.float64)
	q_value: tensor([[-33.2071]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.6798852542133937 entropy 1.2378510236740112
epoch: 1, step: 46
	action: tensor([[ 0.8590, -2.5566,  0.5708, -1.3364,  1.7339,  0.2791, -1.2513]],
       dtype=torch.float64)
	q_value: tensor([[-42.1438]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.5633687741627507 entropy 1.2378510236740112
epoch: 1, step: 47
	action: tensor([[-1.3988, -0.2903, -0.7691, -0.3489, -1.9273, -0.1363,  0.6587]],
       dtype=torch.float64)
	q_value: tensor([[-44.6900]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.466104710277551, distance: 1.797059470779444 entropy 1.2378510236740112
epoch: 1, step: 48
	action: tensor([[-0.0822, -0.0334,  1.3764, -2.6647,  0.4876,  0.0728,  1.7596]],
       dtype=torch.float64)
	q_value: tensor([[-43.6874]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3374956429695068 entropy 1.2378510236740112
epoch: 1, step: 49
	action: tensor([[-0.6558,  0.2996,  0.6745, -0.2976,  0.8234, -0.7258,  0.8836]],
       dtype=torch.float64)
	q_value: tensor([[-42.8534]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.670434409414886, distance: 1.4790110101457428 entropy 1.2378510236740112
epoch: 1, step: 50
	action: tensor([[-2.0812, -0.5459,  1.3282, -0.7956,  0.5614,  0.2876,  0.7214]],
       dtype=torch.float64)
	q_value: tensor([[-31.3677]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9538608327393239 entropy 1.2378510236740112
epoch: 1, step: 51
	action: tensor([[ 0.3511, -0.9088, -0.0703, -1.2390,  0.6165,  0.4956, -0.3989]],
       dtype=torch.float64)
	q_value: tensor([[-37.1432]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6593885409916416, distance: 1.4741128657105713 entropy 1.2378510236740112
epoch: 1, step: 52
	action: tensor([[ 1.4510, -0.7008, -0.0857,  0.2877, -0.7251, -0.8245, -0.0874]],
       dtype=torch.float64)
	q_value: tensor([[-34.0134]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21349735454683838, distance: 1.2605965384011752 entropy 1.2378510236740112
epoch: 1, step: 53
	action: tensor([[ 0.2481,  0.0358,  0.2875, -1.3434, -0.6625,  0.6076,  0.9143]],
       dtype=torch.float64)
	q_value: tensor([[-34.4962]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0824442294537786, distance: 1.190582399487311 entropy 1.2378510236740112
epoch: 1, step: 54
	action: tensor([[-0.4650,  0.5410, -0.4671,  0.1516,  0.5315,  0.2480,  0.9001]],
       dtype=torch.float64)
	q_value: tensor([[-34.6707]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9685992055314793 entropy 1.2378510236740112
epoch: 1, step: 55
	action: tensor([[ 1.4939,  1.0040,  0.2300,  0.8432, -0.0598, -0.0592,  0.5007]],
       dtype=torch.float64)
	q_value: tensor([[-28.8606]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0455861592006066 entropy 1.2378510236740112
epoch: 1, step: 56
	action: tensor([[ 1.0229, -0.6182, -0.0983, -0.7516, -0.3918,  0.4574,  0.8962]],
       dtype=torch.float64)
	q_value: tensor([[-30.2047]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.22874853460966693, distance: 1.268493364376499 entropy 1.2378510236740112
epoch: 1, step: 57
	action: tensor([[ 0.5370,  0.5127, -0.4611, -0.3387,  0.3238,  0.7859,  0.2311]],
       dtype=torch.float64)
	q_value: tensor([[-33.2544]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.964426743794415, distance: 0.21583332605662506 entropy 1.2378510236740112
epoch: 1, step: 58
	action: tensor([[ 0.8200, -0.9412,  0.6601, -0.5684,  0.3808, -0.2969, -1.2911]],
       dtype=torch.float64)
	q_value: tensor([[-28.6408]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5917290318101949, distance: 1.4437475559117978 entropy 1.2378510236740112
epoch: 1, step: 59
	action: tensor([[-0.0098,  0.5569,  0.3335, -0.2076,  2.4203, -1.1728,  0.9386]],
       dtype=torch.float64)
	q_value: tensor([[-36.2098]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8854239985054313 entropy 1.2378510236740112
epoch: 1, step: 60
	action: tensor([[-0.0011,  0.2447, -0.5181, -0.5013,  0.0509, -0.5675,  0.6910]],
       dtype=torch.float64)
	q_value: tensor([[-36.0614]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38206181325758803, distance: 0.8995580831482785 entropy 1.2378510236740112
epoch: 1, step: 61
	action: tensor([[-0.7868, -0.7587,  2.3137, -0.6663, -0.5809,  0.1815, -0.4984]],
       dtype=torch.float64)
	q_value: tensor([[-28.3194]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.199536749984532, distance: 1.6971581130674647 entropy 1.2378510236740112
epoch: 1, step: 62
	action: tensor([[-0.2440, -0.2865,  0.4821, -0.5871,  1.3563, -0.9322,  1.9003]],
       dtype=torch.float64)
	q_value: tensor([[-41.2572]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6019114819636959, distance: 1.4483580930595412 entropy 1.2378510236740112
epoch: 1, step: 63
	action: tensor([[ 0.0129, -1.2805, -0.5331,  0.2653,  1.0010,  0.7298,  0.2143]],
       dtype=torch.float64)
	q_value: tensor([[-36.6408]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3395382548337298 entropy 1.2378510236740112
epoch: 1, step: 64
	action: tensor([[ 0.3360, -0.9045,  1.5487,  0.6504, -1.1382, -0.7535, -0.4805]],
       dtype=torch.float64)
	q_value: tensor([[-33.7152]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2728037232591629, distance: 1.291033203395178 entropy 1.2378510236740112
epoch: 1, step: 65
	action: tensor([[ 0.0525,  0.0504,  0.6490, -0.4299, -0.5295,  1.2110, -0.1547]],
       dtype=torch.float64)
	q_value: tensor([[-41.7298]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4761311465894188, distance: 0.8282624685296922 entropy 1.2378510236740112
epoch: 1, step: 66
	action: tensor([[-0.7423,  0.1049,  0.3285,  0.1752,  1.1138,  0.5741, -1.4284]],
       dtype=torch.float64)
	q_value: tensor([[-32.0680]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12389233598185645, distance: 1.2131626975628182 entropy 1.2378510236740112
epoch: 1, step: 67
	action: tensor([[-0.0697, -1.6974,  2.1735, -0.7164,  1.0195,  1.1860,  0.8021]],
       dtype=torch.float64)
	q_value: tensor([[-36.8939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.573797541616296 entropy 1.2378510236740112
epoch: 1, step: 68
	action: tensor([[ 0.0265,  0.1273,  0.3964, -0.3411,  1.4632, -0.8000,  0.3957]],
       dtype=torch.float64)
	q_value: tensor([[-40.8782]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09662538013700495, distance: 1.0876536854290764 entropy 1.2378510236740112
epoch: 1, step: 69
	action: tensor([[-0.4129, -1.9686,  1.2053,  0.1997,  1.6605, -0.0937,  0.4435]],
       dtype=torch.float64)
	q_value: tensor([[-32.3161]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.5657630772311601 entropy 1.2378510236740112
epoch: 1, step: 70
	action: tensor([[ 1.6097e+00, -1.6664e-01, -5.7485e-01, -1.4472e+00, -4.9163e-01,
         -1.5088e-03,  6.3218e-01]], dtype=torch.float64)
	q_value: tensor([[-36.9521]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0030370521070322 entropy 1.2378510236740112
epoch: 1, step: 71
	action: tensor([[ 0.8038, -0.7419,  1.5870,  0.6763,  0.1155, -0.3471,  1.5079]],
       dtype=torch.float64)
	q_value: tensor([[-32.9130]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.029731628893344575, distance: 1.1272042835514442 entropy 1.2378510236740112
epoch: 1, step: 72
	action: tensor([[-0.4635, -2.1221, -0.9674,  0.3578, -0.0287, -0.9468,  0.8223]],
       dtype=torch.float64)
	q_value: tensor([[-34.9736]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.575277952140684 entropy 1.2378510236740112
epoch: 1, step: 73
	action: tensor([[-0.1249, -0.5942, -0.4112, -1.1235, -0.7515, -0.1242,  0.6866]],
       dtype=torch.float64)
	q_value: tensor([[-37.5303]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.329099347070291, distance: 1.3192752517538082 entropy 1.2378510236740112
epoch: 1, step: 74
	action: tensor([[-1.7847,  0.0813,  0.7674, -0.3053,  0.8910,  0.1142,  1.1562]],
       dtype=torch.float64)
	q_value: tensor([[-33.5972]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9038090386691013 entropy 1.2378510236740112
epoch: 1, step: 75
	action: tensor([[-1.8404, -1.7169,  1.1961, -0.5945,  0.9665, -0.2077, -1.0443]],
       dtype=torch.float64)
	q_value: tensor([[-36.2147]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.8521393111188569 entropy 1.2378510236740112
epoch: 1, step: 76
	action: tensor([[ 0.4291, -0.4473,  1.3568, -0.5278, -0.7163, -1.1301, -0.0067]],
       dtype=torch.float64)
	q_value: tensor([[-43.4432]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16688934416467527, distance: 1.2361510162935903 entropy 1.2378510236740112
epoch: 1, step: 77
	action: tensor([[ 0.2523, -0.6973,  0.3552,  0.3916,  1.2128,  0.6133, -0.3775]],
       dtype=torch.float64)
	q_value: tensor([[-36.0545]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33195561850525857, distance: 0.9353181266783763 entropy 1.2378510236740112
epoch: 1, step: 78
	action: tensor([[ 1.1570, -0.4396, -0.1571, -1.2767,  1.3032, -0.9692,  1.5648]],
       dtype=torch.float64)
	q_value: tensor([[-32.5211]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2451559428556307, distance: 1.2769343391876795 entropy 1.2378510236740112
epoch: 1, step: 79
	action: tensor([[ 0.0537,  0.2111,  0.1529, -0.1693,  0.2293,  0.7198,  0.0369]],
       dtype=torch.float64)
	q_value: tensor([[-39.2667]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.6944510003469184 entropy 1.2378510236740112
epoch: 1, step: 80
	action: tensor([[ 1.2984, -1.0120, -1.5788,  0.1507,  1.1095, -1.0488, -0.7057]],
       dtype=torch.float64)
	q_value: tensor([[-27.1103]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5983645391202665, distance: 1.4467537315524266 entropy 1.2378510236740112
epoch: 1, step: 81
	action: tensor([[ 0.4148, -1.1725,  1.3179, -0.5562,  0.0249, -1.3645, -0.3039]],
       dtype=torch.float64)
	q_value: tensor([[-41.6897]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.479265922343175, distance: 1.3918095738710483 entropy 1.2378510236740112
epoch: 1, step: 82
	action: tensor([[ 0.3532, -2.3973, -0.0816, -0.8703, -0.7628, -0.1410,  2.3830]],
       dtype=torch.float64)
	q_value: tensor([[-37.4800]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.609320555450602 entropy 1.2378510236740112
epoch: 1, step: 83
	action: tensor([[ 0.6827, -1.4079,  0.9548, -1.2247, -0.7505, -0.0753,  1.4062]],
       dtype=torch.float64)
	q_value: tensor([[-42.9497]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.5354857960811443 entropy 1.2378510236740112
epoch: 1, step: 84
	action: tensor([[ 0.7541,  0.1256,  0.2875, -0.0512,  1.0115,  0.8201, -0.2039]],
       dtype=torch.float64)
	q_value: tensor([[-38.8141]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.108324747229188 entropy 1.2378510236740112
epoch: 1, step: 85
	action: tensor([[ 0.9438,  0.1258,  1.0611, -0.7898,  1.0145, -0.6251, -0.4107]],
       dtype=torch.float64)
	q_value: tensor([[-28.8474]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6564639132222363, distance: 0.6707225000396974 entropy 1.2378510236740112
epoch: 1, step: 86
	action: tensor([[-0.4937, -0.6580,  0.9083,  0.4313, -0.2774, -1.1506, -1.3084]],
       dtype=torch.float64)
	q_value: tensor([[-34.0358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6471863355460306, distance: 1.4686829687702148 entropy 1.2378510236740112
epoch: 1, step: 87
	action: tensor([[ 0.3530, -2.0509,  1.5708, -1.8904, -0.8055,  1.5850, -0.2167]],
       dtype=torch.float64)
	q_value: tensor([[-40.9735]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.139093313750605 entropy 1.2378510236740112
epoch: 1, step: 88
	action: tensor([[ 0.7964, -0.8765, -0.0895, -0.8959,  0.0385, -0.6266, -0.9135]],
       dtype=torch.float64)
	q_value: tensor([[-44.4165]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7194488078126604, distance: 1.5005529274197456 entropy 1.2378510236740112
epoch: 1, step: 89
	action: tensor([[ 0.5528, -1.1256,  0.3016,  0.8194,  1.2413,  0.1874,  0.2216]],
       dtype=torch.float64)
	q_value: tensor([[-35.3402]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1891320320193679, distance: 1.0304614887275318 entropy 1.2378510236740112
epoch: 1, step: 90
	action: tensor([[-0.9034, -2.1592,  0.3947,  0.3504, -0.2641, -0.5856,  1.4801]],
       dtype=torch.float64)
	q_value: tensor([[-33.3854]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.6891065912014243 entropy 1.2378510236740112
epoch: 1, step: 91
	action: tensor([[-0.4992, -1.2416, -0.8981, -0.1679,  0.1217,  0.5916,  0.0330]],
       dtype=torch.float64)
	q_value: tensor([[-36.0237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.6321891251003966 entropy 1.2378510236740112
epoch: 1, step: 92
	action: tensor([[-0.6413, -0.0852,  0.0256,  0.2774, -0.7631,  0.0169, -0.0110]],
       dtype=torch.float64)
	q_value: tensor([[-34.2144]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.36928096127115895, distance: 1.339069067998322 entropy 1.2378510236740112
epoch: 1, step: 93
	action: tensor([[ 0.6588, -1.2491,  0.3219,  0.4732, -1.2787,  0.8897,  1.2281]],
       dtype=torch.float64)
	q_value: tensor([[-29.6850]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1402545418010477 entropy 1.2378510236740112
epoch: 1, step: 94
	action: tensor([[-0.5759, -1.3749,  0.3655, -0.3553,  0.2411,  0.1528,  0.9199]],
       dtype=torch.float64)
	q_value: tensor([[-37.6269]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.6920589377198856 entropy 1.2378510236740112
epoch: 1, step: 95
	action: tensor([[ 0.1213, -0.5513, -0.7095, -0.5970,  1.2749,  0.2140,  0.3905]],
       dtype=torch.float64)
	q_value: tensor([[-32.1205]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05779411270035739, distance: 1.1769479751059917 entropy 1.2378510236740112
epoch: 1, step: 96
	action: tensor([[ 0.0873, -0.4533, -0.2257, -0.2679,  0.5275, -1.1180, -0.1771]],
       dtype=torch.float64)
	q_value: tensor([[-33.7035]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3128028165380079, distance: 1.3111622690431648 entropy 1.2378510236740112
epoch: 1, step: 97
	action: tensor([[ 1.0443,  1.1272,  0.7293,  0.2746,  0.9678,  1.0230, -0.3531]],
       dtype=torch.float64)
	q_value: tensor([[-31.0173]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6372164704078531, distance: 0.6892558711891086 entropy 1.2378510236740112
epoch: 1, step: 98
	action: tensor([[ 0.3731, -0.5784,  0.3743,  0.5482,  0.2041,  1.1321,  0.7584]],
       dtype=torch.float64)
	q_value: tensor([[-35.3554]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8169708903714764, distance: 0.4895722316112621 entropy 1.2378510236740112
epoch: 1, step: 99
	action: tensor([[ 0.0121,  1.9402, -0.7515, -0.3347,  0.2297, -0.6130, -0.5723]],
       dtype=torch.float64)
	q_value: tensor([[-31.4189]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.4242273665821244 entropy 1.2378510236740112
epoch: 1, step: 100
	action: tensor([[-0.2065, -0.8349, -1.2628, -0.6340,  0.2600, -2.3071, -0.6164]],
       dtype=torch.float64)
	q_value: tensor([[-27.4135]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.956012790193331 entropy 1.2378510236740112
epoch: 1, step: 101
	action: tensor([[ 0.2246, -1.1561,  0.9729, -0.3744, -0.0461, -0.4043,  0.7510]],
       dtype=torch.float64)
	q_value: tensor([[-44.5603]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9690099854272871, distance: 1.605760063819775 entropy 1.2378510236740112
epoch: 1, step: 102
	action: tensor([[ 0.7082, -1.8465, -0.4289, -0.5157, -0.2933, -0.8526, -0.5198]],
       dtype=torch.float64)
	q_value: tensor([[-32.3234]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.5260499199645126 entropy 1.2378510236740112
epoch: 1, step: 103
	action: tensor([[ 2.0730, -0.4302,  0.5846, -1.8727,  0.1243, -0.0798,  0.2677]],
       dtype=torch.float64)
	q_value: tensor([[-35.5678]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1749909319876137 entropy 1.2378510236740112
epoch: 1, step: 104
	action: tensor([[ 2.3646, -0.4898,  0.2980, -1.2653,  0.3362,  0.2184,  1.5986]],
       dtype=torch.float64)
	q_value: tensor([[-37.8629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.019835041690734 entropy 1.2378510236740112
epoch: 1, step: 105
	action: tensor([[ 0.4707, -0.6154,  0.5368, -0.2119,  0.0690,  1.0274,  1.1218]],
       dtype=torch.float64)
	q_value: tensor([[-32.6274]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35955164133406203, distance: 0.9157960199714122 entropy 1.2378510236740112
epoch: 1, step: 106
	action: tensor([[-0.7142,  0.2813,  0.9929, -1.6424,  1.5263,  0.3482,  0.0278]],
       dtype=torch.float64)
	q_value: tensor([[-32.2921]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9511155455074103, distance: 1.5984468049024394 entropy 1.2378510236740112
epoch: 1, step: 107
	action: tensor([[ 0.1954,  0.0213,  1.7053, -0.4009, -0.2266,  0.4466, -0.7539]],
       dtype=torch.float64)
	q_value: tensor([[-39.9762]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4426092508637002, distance: 0.8543514710579774 entropy 1.2378510236740112
epoch: 1, step: 108
	action: tensor([[-0.8491, -1.6878, -0.1927, -1.0611,  0.2047,  0.6477,  0.9271]],
       dtype=torch.float64)
	q_value: tensor([[-34.6536]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.5516453140159905 entropy 1.2378510236740112
epoch: 1, step: 109
	action: tensor([[ 0.0143, -1.1570,  0.1483,  0.2998,  0.7440,  0.8438, -0.1234]],
       dtype=torch.float64)
	q_value: tensor([[-37.3302]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09533956195048687, distance: 1.1976532029001128 entropy 1.2378510236740112
epoch: 1, step: 110
	action: tensor([[-1.7056, -1.2519, -0.8111,  0.0151, -0.8380, -1.9888,  0.3294]],
       dtype=torch.float64)
	q_value: tensor([[-32.5765]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4089118746531466 entropy 1.2378510236740112
epoch: 1, step: 111
	action: tensor([[ 0.1526, -1.3527,  1.9394, -0.2734,  0.2010,  0.9473,  2.5596]],
       dtype=torch.float64)
	q_value: tensor([[-47.1049]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4653539175605106 entropy 1.2378510236740112
epoch: 1, step: 112
	action: tensor([[ 0.6344,  0.1862, -1.3440,  1.2780,  1.1359, -1.0447,  1.2242]],
       dtype=torch.float64)
	q_value: tensor([[-43.5198]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0697199387823457 entropy 1.2378510236740112
epoch: 1, step: 113
	action: tensor([[ 1.4734, -0.8275,  0.3417, -1.9176,  0.4094, -1.9809,  1.5885]],
       dtype=torch.float64)
	q_value: tensor([[-31.0558]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4167005262973066, distance: 0.8739819945582901 entropy 1.2378510236740112
epoch: 1, step: 114
	action: tensor([[ 0.4487, -0.1001,  0.8190, -0.3299,  1.7661, -0.0801,  1.1061]],
       dtype=torch.float64)
	q_value: tensor([[-46.7349]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37220691251456917, distance: 0.9067028023270723 entropy 1.2378510236740112
epoch: 1, step: 115
	action: tensor([[-0.8836, -1.7461, -0.3590,  0.4887, -1.7323,  0.8280, -0.7052]],
       dtype=torch.float64)
	q_value: tensor([[-34.3171]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.7739169032005362 entropy 1.2378510236740112
epoch: 1, step: 116
	action: tensor([[ 0.9318, -0.3795,  1.0162,  0.2147,  1.0627, -0.3117, -0.2914]],
       dtype=torch.float64)
	q_value: tensor([[-45.6217]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5205733496792617, distance: 0.7923513080469314 entropy 1.2378510236740112
epoch: 1, step: 117
	action: tensor([[ 1.8555,  0.3854, -0.4674,  0.0982, -0.6148, -1.3354, -0.4263]],
       dtype=torch.float64)
	q_value: tensor([[-32.6086]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9002523893148423 entropy 1.2378510236740112
epoch: 1, step: 118
	action: tensor([[ 0.3774,  0.0690,  0.1269, -0.3788,  0.1949,  0.3719,  0.6225]],
       dtype=torch.float64)
	q_value: tensor([[-36.9739]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5780020855262397, distance: 0.7433816654959424 entropy 1.2378510236740112
epoch: 1, step: 119
	action: tensor([[-0.2596, -0.2745,  1.0795, -0.7890, -0.2934,  0.9010,  0.6012]],
       dtype=torch.float64)
	q_value: tensor([[-27.0360]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3290364903785863 entropy 1.2378510236740112
epoch: 1, step: 120
	action: tensor([[ 0.7955,  1.3459, -0.1706, -1.0800, -1.4261, -0.5644,  0.4918]],
       dtype=torch.float64)
	q_value: tensor([[-32.9436]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9801075428436642 entropy 1.2378510236740112
epoch: 1, step: 121
	action: tensor([[ 1.2115, -2.6656,  0.1156,  0.3699,  1.4280, -0.5669, -0.0147]],
       dtype=torch.float64)
	q_value: tensor([[-32.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3032184584406372 entropy 1.2378510236740112
epoch: 1, step: 122
	action: tensor([[-0.8565, -0.6452, -0.0292, -2.4406, -0.2294,  1.0215,  2.4976]],
       dtype=torch.float64)
	q_value: tensor([[-36.7363]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3709709404887365 entropy 1.2378510236740112
epoch: 1, step: 123
	action: tensor([[ 0.0304,  0.4398,  1.1631,  0.0947, -1.1884, -0.6443,  0.1565]],
       dtype=torch.float64)
	q_value: tensor([[-50.4050]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8080722752126641 entropy 1.2378510236740112
epoch: 1, step: 124
	action: tensor([[ 0.8889, -1.9282, -1.3651, -0.9577,  2.1185,  0.9857,  0.9424]],
       dtype=torch.float64)
	q_value: tensor([[-28.3294]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.144756660823969 entropy 1.2378510236740112
epoch: 1, step: 125
	action: tensor([[ 0.8424, -1.1015,  0.0985, -2.3168,  0.0825,  0.2443,  0.3966]],
       dtype=torch.float64)
	q_value: tensor([[-48.6070]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3742378080965443 entropy 1.2378510236740112
epoch: 1, step: 126
	action: tensor([[ 0.0773,  0.6714,  0.0830,  0.7119, -0.2637, -0.1323, -0.7300]],
       dtype=torch.float64)
	q_value: tensor([[-40.6258]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7876131123606076 entropy 1.2378510236740112
epoch: 1, step: 127
	action: tensor([[ 0.0394, -0.7099,  0.0580, -2.1029, -0.1283, -0.3538, -1.2876]],
       dtype=torch.float64)
	q_value: tensor([[-25.5314]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.29710693799563326, distance: 1.3033005676719582 entropy 1.2378510236740112
LOSS epoch 1 actor 296.61372834840364 critic 200.6288559106431 
epoch: 2, step: 0
	action: tensor([[-0.6061, -0.2192,  0.7101, -0.0278,  0.3541,  0.5523, -0.0024]],
       dtype=torch.float64)
	q_value: tensor([[-52.2443]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3588609400085192, distance: 1.3339642811279604 entropy 1.2378510236740112
epoch: 2, step: 1
	action: tensor([[-0.0150, -0.0471,  0.0681,  0.7377,  0.6240,  0.8621,  0.5293]],
       dtype=torch.float64)
	q_value: tensor([[-36.2891]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8729182980487419 entropy 1.2378510236740112
epoch: 2, step: 2
	action: tensor([[ 1.4958, -0.0861,  0.4448, -0.9477,  0.4517,  0.7819, -0.5493]],
       dtype=torch.float64)
	q_value: tensor([[-35.7883]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20065516885585888, distance: 1.0231134141135345 entropy 1.2378510236740112
epoch: 2, step: 3
	action: tensor([[ 0.6436, -0.2471,  0.8474,  0.4765,  0.5420,  0.1267,  1.9342]],
       dtype=torch.float64)
	q_value: tensor([[-42.0050]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7283943908760389, distance: 0.5963841058116708 entropy 1.2378510236740112
epoch: 2, step: 4
	action: tensor([[ 0.7032,  1.1318,  0.1907, -0.9739, -0.0403,  1.2617,  0.1884]],
       dtype=torch.float64)
	q_value: tensor([[-44.3925]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.17014050382080687 entropy 1.2378510236740112
epoch: 2, step: 5
	action: tensor([[ 1.2162,  1.8006,  0.8457, -0.5365, -0.7605, -0.1029,  0.5927]],
       dtype=torch.float64)
	q_value: tensor([[-42.1367]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9813429828289184 entropy 1.2378510236740112
epoch: 2, step: 6
	action: tensor([[ 0.1656, -2.5112,  0.8092, -2.7823,  1.4417,  0.0648, -0.7982]],
       dtype=torch.float64)
	q_value: tensor([[-41.0367]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3302732694230084 entropy 1.2378510236740112
epoch: 2, step: 7
	action: tensor([[-0.2788, -0.6639,  0.4390, -0.4415,  1.1047, -0.7913,  0.3128]],
       dtype=torch.float64)
	q_value: tensor([[-56.5552]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8938620472882663, distance: 1.5748197900629752 entropy 1.2378510236740112
epoch: 2, step: 8
	action: tensor([[ 0.1330, -0.5687, -1.7081,  0.2452,  1.2389, -0.8288,  0.5613]],
       dtype=torch.float64)
	q_value: tensor([[-40.7648]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.29661894674708655, distance: 1.3030553839164756 entropy 1.2378510236740112
epoch: 2, step: 9
	action: tensor([[-0.0263, -0.3231,  0.8904,  0.1857,  0.7676,  0.2102, -0.8821]],
       dtype=torch.float64)
	q_value: tensor([[-46.1116]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21486681572705368, distance: 1.013977597744026 entropy 1.2378510236740112
epoch: 2, step: 10
	action: tensor([[-1.5290,  0.1873,  0.1159, -0.5842,  1.3753,  0.3820, -1.5185]],
       dtype=torch.float64)
	q_value: tensor([[-41.1739]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.457211714339039, distance: 1.7938163651654293 entropy 1.2378510236740112
epoch: 2, step: 11
	action: tensor([[-0.3874, -1.3120,  1.2959, -1.0306,  0.4978,  0.1244, -0.1029]],
       dtype=torch.float64)
	q_value: tensor([[-50.4602]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.7485567321373008 entropy 1.2378510236740112
epoch: 2, step: 12
	action: tensor([[-0.0804, -1.1617,  0.8058, -0.1887,  0.5410,  0.0061, -0.1839]],
       dtype=torch.float64)
	q_value: tensor([[-44.7743]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.5992724708018824 entropy 1.2378510236740112
epoch: 2, step: 13
	action: tensor([[ 1.0242, -0.4856,  1.0764,  0.7037,  0.5685, -0.1383,  0.6716]],
       dtype=torch.float64)
	q_value: tensor([[-40.0367]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4305668813743114, distance: 0.8635312415960592 entropy 1.2378510236740112
epoch: 2, step: 14
	action: tensor([[-0.8089,  0.3550,  0.3546, -0.8589,  0.1180, -0.0647,  0.0828]],
       dtype=torch.float64)
	q_value: tensor([[-39.7750]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9101003592180508, distance: 1.5815567731376963 entropy 1.2378510236740112
epoch: 2, step: 15
	action: tensor([[ 0.7152, -1.3952,  0.6876,  0.1910, -0.0303,  1.3030,  0.9438]],
       dtype=torch.float64)
	q_value: tensor([[-37.1524]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0505289153340938 entropy 1.2378510236740112
epoch: 2, step: 16
	action: tensor([[ 0.6323, -0.7945,  0.9353,  0.3620,  0.2658,  0.8305, -0.6622]],
       dtype=torch.float64)
	q_value: tensor([[-44.0685]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34086004140983306, distance: 0.9290637462664594 entropy 1.2378510236740112
epoch: 2, step: 17
	action: tensor([[ 0.8354,  0.8097, -1.0497, -0.9601, -1.1133, -0.5111,  0.7761]],
       dtype=torch.float64)
	q_value: tensor([[-42.4413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.764732616185466, distance: 0.5550569626642405 entropy 1.2378510236740112
epoch: 2, step: 18
	action: tensor([[ 1.9510,  0.4760,  0.6402, -0.9917,  1.2822, -0.0036,  1.0799]],
       dtype=torch.float64)
	q_value: tensor([[-46.6993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.695476224134893 entropy 1.2378510236740112
epoch: 2, step: 19
	action: tensor([[ 1.9573, -0.3314, -0.6940, -1.5136,  1.0204,  0.7708,  1.6747]],
       dtype=torch.float64)
	q_value: tensor([[-45.6229]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.011831382243538 entropy 1.2378510236740112
epoch: 2, step: 20
	action: tensor([[ 0.4370, -0.0399, -0.2679, -0.3755,  0.0746, -1.5725, -0.1318]],
       dtype=torch.float64)
	q_value: tensor([[-42.8130]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3249586233672255, distance: 0.9402035575111438 entropy 1.2378510236740112
epoch: 2, step: 21
	action: tensor([[-0.7585, -1.2362, -0.2893, -0.9971,  0.9655, -0.4168,  0.8390]],
       dtype=torch.float64)
	q_value: tensor([[-40.5707]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.698599916764236, distance: 1.4914278291446919 entropy 1.2378510236740112
epoch: 2, step: 22
	action: tensor([[-0.0989, -2.0376,  0.3278, -0.9029,  0.2634, -0.5069,  0.9391]],
       dtype=torch.float64)
	q_value: tensor([[-46.4608]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.6307392873482072 entropy 1.2378510236740112
epoch: 2, step: 23
	action: tensor([[ 0.2840, -1.9409,  0.3697,  1.8806, -0.8414, -0.4001,  0.6542]],
       dtype=torch.float64)
	q_value: tensor([[-43.1032]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.5646752566497867 entropy 1.2378510236740112
epoch: 2, step: 24
	action: tensor([[-1.0380, -0.0379,  0.6385, -1.1475,  0.2435, -0.8587, -0.0576]],
       dtype=torch.float64)
	q_value: tensor([[-52.2680]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1256080664851784, distance: 1.668392727237978 entropy 1.2378510236740112
epoch: 2, step: 25
	action: tensor([[-0.9123, -0.8491, -0.3237,  0.9058,  0.6781, -0.2082,  1.8701]],
       dtype=torch.float64)
	q_value: tensor([[-40.5952]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2663714974795628, distance: 1.7227499387225478 entropy 1.2378510236740112
epoch: 2, step: 26
	action: tensor([[-0.3062, -1.5088,  0.8067,  0.1060, -1.4492,  0.2386, -1.0945]],
       dtype=torch.float64)
	q_value: tensor([[-49.0263]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.5948302963426346 entropy 1.2378510236740112
epoch: 2, step: 27
	action: tensor([[ 0.4208, -0.0603,  0.7528,  0.1795,  0.3754, -0.0446,  0.6758]],
       dtype=torch.float64)
	q_value: tensor([[-51.3942]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6886444543630488, distance: 0.6385354245855389 entropy 1.2378510236740112
epoch: 2, step: 28
	action: tensor([[-0.4322, -1.3376, -0.0372,  0.4264, -0.0521,  0.3088,  0.1144]],
       dtype=torch.float64)
	q_value: tensor([[-34.4792]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.5207263339094401 entropy 1.2378510236740112
epoch: 2, step: 29
	action: tensor([[ 0.0937, -1.4071,  0.2984,  0.9833, -0.1927, -0.2624, -0.1022]],
       dtype=torch.float64)
	q_value: tensor([[-39.7162]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1641663685639483 entropy 1.2378510236740112
epoch: 2, step: 30
	action: tensor([[ 0.4747, -0.0509,  0.2037,  1.2370, -0.0547,  1.0528,  0.6865]],
       dtype=torch.float64)
	q_value: tensor([[-42.7035]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.12780186121127 entropy 1.2378510236740112
epoch: 2, step: 31
	action: tensor([[ 0.6013, -0.6874, -0.3901,  0.0440, -0.8294,  0.9530, -0.2871]],
       dtype=torch.float64)
	q_value: tensor([[-33.3412]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20177111152190408, distance: 1.0223989948596652 entropy 1.2378510236740112
epoch: 2, step: 32
	action: tensor([[-0.1785,  0.5597, -0.2155, -1.5992, -1.9016, -0.5901, -0.7249]],
       dtype=torch.float64)
	q_value: tensor([[-40.1307]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3658422023999639, distance: 0.9112873920682675 entropy 1.2378510236740112
epoch: 2, step: 33
	action: tensor([[ 0.0973, -1.6990,  0.3899,  1.6653,  0.6520,  0.4776,  0.7400]],
       dtype=torch.float64)
	q_value: tensor([[-51.2661]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.5618329240013595 entropy 1.2378510236740112
epoch: 2, step: 34
	action: tensor([[ 0.6002, -1.3872,  0.1034, -0.2191,  0.4512, -0.6484,  1.1804]],
       dtype=torch.float64)
	q_value: tensor([[-45.1257]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.5688183264534203 entropy 1.2378510236740112
epoch: 2, step: 35
	action: tensor([[ 0.1508,  0.3713, -0.3217,  1.1010, -0.8450, -0.6464, -1.9614]],
       dtype=torch.float64)
	q_value: tensor([[-42.6272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1075274992341635 entropy 1.2378510236740112
epoch: 2, step: 36
	action: tensor([[-0.0519, -1.4713,  1.0833,  0.2076, -0.5732, -1.8042, -0.5225]],
       dtype=torch.float64)
	q_value: tensor([[-38.5579]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4500241005299872 entropy 1.2378510236740112
epoch: 2, step: 37
	action: tensor([[-0.0453,  1.1569,  0.8509, -0.3425,  0.8255, -0.3018, -0.4012]],
       dtype=torch.float64)
	q_value: tensor([[-52.9854]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1071693133490548 entropy 1.2378510236740112
epoch: 2, step: 38
	action: tensor([[-1.1930, -0.2690,  1.1074, -0.9663,  0.2940, -0.5110,  1.0266]],
       dtype=torch.float64)
	q_value: tensor([[-37.4895]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.4391543367980817, distance: 1.7872130781727005 entropy 1.2378510236740112
epoch: 2, step: 39
	action: tensor([[ 0.4455, -2.5012,  0.0594, -1.1589,  1.2799, -0.9603,  0.4713]],
       dtype=torch.float64)
	q_value: tensor([[-43.7567]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.5395355253064782 entropy 1.2378510236740112
epoch: 2, step: 40
	action: tensor([[-0.2434, -1.1625,  0.5930, -0.0866, -1.2797,  0.9981,  0.3652]],
       dtype=torch.float64)
	q_value: tensor([[-48.8506]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7800598967857852, distance: 1.5267713482459138 entropy 1.2378510236740112
epoch: 2, step: 41
	action: tensor([[-0.2375, -0.4343, -1.6961,  1.3108,  1.2262,  0.0984, -0.7824]],
       dtype=torch.float64)
	q_value: tensor([[-47.2101]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7692729465521135, distance: 1.5221382926889095 entropy 1.2378510236740112
epoch: 2, step: 42
	action: tensor([[-0.8455, -0.7877,  0.2151, -0.0816,  1.7389, -0.3845,  0.6339]],
       dtype=torch.float64)
	q_value: tensor([[-45.1680]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2903489734077982, distance: 1.7318390300720907 entropy 1.2378510236740112
epoch: 2, step: 43
	action: tensor([[ 0.9901,  0.1659, -0.0257, -1.3009,  0.1601, -0.5106,  0.8192]],
       dtype=torch.float64)
	q_value: tensor([[-45.7688]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1370083963672627, distance: 1.0630653882534622 entropy 1.2378510236740112
epoch: 2, step: 44
	action: tensor([[-0.9421, -1.2129,  0.0416, -0.7423,  0.5731, -0.6744, -0.2644]],
       dtype=torch.float64)
	q_value: tensor([[-41.7143]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9424003653461515, distance: 1.5948728640576104 entropy 1.2378510236740112
epoch: 2, step: 45
	action: tensor([[-0.1925, -2.1215,  0.7803,  0.9902,  0.9399, -0.2124,  0.7586]],
       dtype=torch.float64)
	q_value: tensor([[-45.1109]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1698308842150766 entropy 1.2378510236740112
epoch: 2, step: 46
	action: tensor([[ 0.6385, -0.3096, -0.0116, -0.6500, -0.5684, -3.0407, -0.2111]],
       dtype=torch.float64)
	q_value: tensor([[-42.9938]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9422305742409767 entropy 1.2378510236740112
epoch: 2, step: 47
	action: tensor([[ 0.0597, -1.0994, -0.1093, -0.2013,  1.0021,  0.2416, -0.9673]],
       dtype=torch.float64)
	q_value: tensor([[-49.3627]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6598612725920461, distance: 1.4743228256124161 entropy 1.2378510236740112
epoch: 2, step: 48
	action: tensor([[-0.0433, -0.6731,  0.3079, -0.1058,  1.8898,  0.1885,  0.5926]],
       dtype=torch.float64)
	q_value: tensor([[-43.4190]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.436764594148618, distance: 1.371669563094951 entropy 1.2378510236740112
epoch: 2, step: 49
	action: tensor([[ 0.8712, -0.4414,  0.0363, -1.2913,  0.3434,  0.8407,  0.4263]],
       dtype=torch.float64)
	q_value: tensor([[-44.2519]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17797959182461476, distance: 1.2420113840170686 entropy 1.2378510236740112
epoch: 2, step: 50
	action: tensor([[ 2.2938, -0.3047,  0.2227, -0.9648, -1.1697,  0.0916, -0.3740]],
       dtype=torch.float64)
	q_value: tensor([[-43.1549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2564333463708395 entropy 1.2378510236740112
epoch: 2, step: 51
	action: tensor([[-0.1604, -0.4926,  0.9804, -0.4259,  1.0990, -0.0105, -1.0298]],
       dtype=torch.float64)
	q_value: tensor([[-44.6987]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6195858592361292, distance: 1.4563262625245275 entropy 1.2378510236740112
epoch: 2, step: 52
	action: tensor([[-0.3845,  1.0138,  0.5144, -0.9254, -0.2660, -0.9127,  0.1292]],
       dtype=torch.float64)
	q_value: tensor([[-44.4389]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2945296674172562 entropy 1.2378510236740112
epoch: 2, step: 53
	action: tensor([[ 0.6662, -1.3622,  0.5681, -0.9016, -0.1130,  0.2817,  0.9926]],
       dtype=torch.float64)
	q_value: tensor([[-37.6665]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.6156405208824425 entropy 1.2378510236740112
epoch: 2, step: 54
	action: tensor([[-0.5499,  0.3280,  0.8748,  0.3300,  0.6795, -1.6259,  1.6021]],
       dtype=torch.float64)
	q_value: tensor([[-43.8346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2629954306026612 entropy 1.2378510236740112
epoch: 2, step: 55
	action: tensor([[ 1.0731, -0.0684, -0.9280,  1.1341,  1.1031,  0.2594,  1.7329]],
       dtype=torch.float64)
	q_value: tensor([[-48.2951]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1109173688461147 entropy 1.2378510236740112
epoch: 2, step: 56
	action: tensor([[-1.2885,  0.0481, -0.9731,  0.4503,  0.6728,  0.4683,  0.4980]],
       dtype=torch.float64)
	q_value: tensor([[-38.1183]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.12517834449514, distance: 1.6682240740198588 entropy 1.2378510236740112
epoch: 2, step: 57
	action: tensor([[-1.5808,  0.1469,  0.0031,  0.4530, -0.3395, -0.2660,  0.2994]],
       dtype=torch.float64)
	q_value: tensor([[-41.1004]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2335878516411949, distance: 1.7102445403571458 entropy 1.2378510236740112
epoch: 2, step: 58
	action: tensor([[-0.2801, -1.7818,  1.5260, -0.2791,  0.6984,  0.5897, -0.1969]],
       dtype=torch.float64)
	q_value: tensor([[-41.5782]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.603402202429649 entropy 1.2378510236740112
epoch: 2, step: 59
	action: tensor([[-0.0223, -0.3420,  0.4792,  1.2409,  0.9053, -0.7555,  2.1649]],
       dtype=torch.float64)
	q_value: tensor([[-44.6868]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0846560013720725 entropy 1.2378510236740112
epoch: 2, step: 60
	action: tensor([[-0.2503,  1.1972, -1.6591,  0.9635,  1.0919,  0.7357, -0.4007]],
       dtype=torch.float64)
	q_value: tensor([[-44.4052]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1252157433079515 entropy 1.2378510236740112
epoch: 2, step: 61
	action: tensor([[-0.2422, -2.2999,  0.4368, -0.0121,  1.1960,  0.2384,  2.6658]],
       dtype=torch.float64)
	q_value: tensor([[-37.4926]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.5583502834494187 entropy 1.2378510236740112
epoch: 2, step: 62
	action: tensor([[ 0.9062, -0.8660,  0.3107, -0.4621,  0.4924,  1.4869, -0.4629]],
       dtype=torch.float64)
	q_value: tensor([[-53.6461]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06786983508622413, distance: 1.1048287691939285 entropy 1.2378510236740112
epoch: 2, step: 63
	action: tensor([[ 0.2210,  0.1675, -0.1886, -0.3105,  0.0540, -0.1977,  0.7200]],
       dtype=torch.float64)
	q_value: tensor([[-45.4481]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.47550712480150115, distance: 0.8287556263098388 entropy 1.2378510236740112
epoch: 2, step: 64
	action: tensor([[ 1.8749,  0.2011,  0.1209,  0.3810, -0.6372, -0.3461, -0.4426]],
       dtype=torch.float64)
	q_value: tensor([[-33.6127]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8056203979679527 entropy 1.2378510236740112
epoch: 2, step: 65
	action: tensor([[-1.3489, -0.3280,  2.4293,  0.3114, -0.8156,  0.0538, -0.7461]],
       dtype=torch.float64)
	q_value: tensor([[-40.0216]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.4771798657832074, distance: 1.8010902034400207 entropy 1.2378510236740112
epoch: 2, step: 66
	action: tensor([[ 0.3428,  0.4860,  1.2382, -0.9786,  1.1171, -0.2939,  0.2493]],
       dtype=torch.float64)
	q_value: tensor([[-56.6645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0484998521231323 entropy 1.2378510236740112
epoch: 2, step: 67
	action: tensor([[-0.4478, -0.2734,  2.0276,  1.9002,  1.7791, -1.4003,  0.3639]],
       dtype=torch.float64)
	q_value: tensor([[-41.6963]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7546481773541472, distance: 0.5668280552121107 entropy 1.2378510236740112
epoch: 2, step: 68
	action: tensor([[-0.0256, -1.6606, -0.3391,  0.4507, -0.4091,  0.9153,  0.5029]],
       dtype=torch.float64)
	q_value: tensor([[-62.1561]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3495771369844425 entropy 1.2378510236740112
epoch: 2, step: 69
	action: tensor([[-1.3547, -0.0962,  0.1604, -0.9134,  0.6113,  1.1027,  0.0316]],
       dtype=torch.float64)
	q_value: tensor([[-41.3369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1862955303727567, distance: 1.6920419524655084 entropy 1.2378510236740112
epoch: 2, step: 70
	action: tensor([[ 0.9024, -0.4869,  0.5706, -0.5192,  0.2712, -0.1702, -0.3673]],
       dtype=torch.float64)
	q_value: tensor([[-46.0503]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03472017581630471, distance: 1.1640406639162482 entropy 1.2378510236740112
epoch: 2, step: 71
	action: tensor([[ 0.2367, -1.1386,  0.4612,  1.2534,  0.3978,  0.2016, -1.8835]],
       dtype=torch.float64)
	q_value: tensor([[-36.3296]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5341181652128661, distance: 0.7810783170973578 entropy 1.2378510236740112
epoch: 2, step: 72
	action: tensor([[ 1.1448, -0.2959, -0.1401,  0.1649,  0.2758, -0.1579,  2.3663]],
       dtype=torch.float64)
	q_value: tensor([[-53.8055]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.47951998087495107, distance: 0.8255791650228154 entropy 1.2378510236740112
epoch: 2, step: 73
	action: tensor([[-0.5263, -0.6695, -0.5657, -1.1448, -0.9316,  0.1384, -0.9156]],
       dtype=torch.float64)
	q_value: tensor([[-50.0089]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4493101838532858, distance: 1.3776451423602831 entropy 1.2378510236740112
epoch: 2, step: 74
	action: tensor([[-0.2250, -1.3240,  0.0648, -1.0523,  0.7034, -0.6513,  0.4054]],
       dtype=torch.float64)
	q_value: tensor([[-44.6260]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.5652248964977225 entropy 1.2378510236740112
epoch: 2, step: 75
	action: tensor([[-0.9178, -1.4388, -0.6233, -0.2766, -0.6046,  0.6290,  1.4498]],
       dtype=torch.float64)
	q_value: tensor([[-44.6034]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.7357501024800353 entropy 1.2378510236740112
epoch: 2, step: 76
	action: tensor([[-1.1234, -0.3930,  0.3699, -0.7554,  1.0166, -0.9947,  0.5261]],
       dtype=torch.float64)
	q_value: tensor([[-48.7881]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.3012967944707685, distance: 1.7359731731829586 entropy 1.2378510236740112
epoch: 2, step: 77
	action: tensor([[ 0.2884, -1.1631, -0.0780,  0.6829,  0.5775, -0.0144,  1.5830]],
       dtype=torch.float64)
	q_value: tensor([[-43.0129]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1615680902289176, distance: 1.233329244994923 entropy 1.2378510236740112
epoch: 2, step: 78
	action: tensor([[-0.1538, -0.9927, -0.3645, -0.6426, -1.0108, -0.8676, -0.5535]],
       dtype=torch.float64)
	q_value: tensor([[-44.0224]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.31051172954521156, distance: 1.3100176574697022 entropy 1.2378510236740112
epoch: 2, step: 79
	action: tensor([[ 0.1070, -0.6210,  0.5707, -0.5728, -0.3532, -0.4900,  0.3895]],
       dtype=torch.float64)
	q_value: tensor([[-44.0099]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4657384427359568 entropy 1.2378510236740112
epoch: 2, step: 80
	action: tensor([[-0.0351,  0.3890, -1.0064,  0.0100,  0.7098, -0.2083,  0.2313]],
       dtype=torch.float64)
	q_value: tensor([[-37.3350]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0496342943778871 entropy 1.2378510236740112
epoch: 2, step: 81
	action: tensor([[-0.2162, -0.2771,  1.3123, -0.5047, -0.1937,  0.8015,  0.6364]],
       dtype=torch.float64)
	q_value: tensor([[-35.6112]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1394659125971196, distance: 1.2215390700036546 entropy 1.2378510236740112
epoch: 2, step: 82
	action: tensor([[ 0.0987, -0.3703,  1.7787, -1.2619, -0.0095, -0.1088,  0.8944]],
       dtype=torch.float64)
	q_value: tensor([[-41.1028]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.264187832178199 entropy 1.2378510236740112
epoch: 2, step: 83
	action: tensor([[ 0.4951, -0.3672,  0.7554, -0.1284,  0.6821, -0.8854,  0.2269]],
       dtype=torch.float64)
	q_value: tensor([[-44.5331]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14991981146188982, distance: 1.0550830368792137 entropy 1.2378510236740112
epoch: 2, step: 84
	action: tensor([[ 1.7157, -1.1485,  0.0968, -0.2534,  0.1514,  0.5138, -0.2169]],
       dtype=torch.float64)
	q_value: tensor([[-37.9601]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4910891676866809 entropy 1.2378510236740112
epoch: 2, step: 85
	action: tensor([[ 1.0462, -2.2174, -0.1942,  0.2643,  0.0555, -0.4220, -0.7089]],
       dtype=torch.float64)
	q_value: tensor([[-43.2097]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4324830877506136 entropy 1.2378510236740112
epoch: 2, step: 86
	action: tensor([[ 2.1894, -0.1416,  0.9440, -1.5683,  0.3102, -0.8207,  0.3906]],
       dtype=torch.float64)
	q_value: tensor([[-41.7103]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9445007653688934 entropy 1.2378510236740112
epoch: 2, step: 87
	action: tensor([[-0.0439, -0.4672,  1.3304, -0.4022,  0.1752,  1.0035,  0.6542]],
       dtype=torch.float64)
	q_value: tensor([[-39.8720]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.006142120414631247, distance: 1.14082449087116 entropy 1.2378510236740112
epoch: 2, step: 88
	action: tensor([[-0.6510,  0.2637, -1.5846, -0.2959, -0.3810, -0.1474, -0.0556]],
       dtype=torch.float64)
	q_value: tensor([[-41.7750]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.008481875991137588, distance: 1.1394808262643674 entropy 1.2378510236740112
epoch: 2, step: 89
	action: tensor([[ 0.8497, -0.6468,  0.1549,  0.2052,  1.8854,  0.5365,  0.7980]],
       dtype=torch.float64)
	q_value: tensor([[-37.6256]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18543946593274163, distance: 1.0328051039859525 entropy 1.2378510236740112
epoch: 2, step: 90
	action: tensor([[ 0.4330,  0.5262, -0.2274,  0.4702,  1.5653, -2.3505, -0.1574]],
       dtype=torch.float64)
	q_value: tensor([[-45.0319]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.5671856748877024 entropy 1.2378510236740112
epoch: 2, step: 91
	action: tensor([[ 0.3764, -0.3279, -0.9323, -1.0492,  0.6364, -1.9855,  1.1034]],
       dtype=torch.float64)
	q_value: tensor([[-43.8784]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18910730259545228, distance: 1.030477201843417 entropy 1.2378510236740112
epoch: 2, step: 92
	action: tensor([[-0.5666, -0.3238,  1.4500, -0.7024,  1.2160, -0.2367,  1.6401]],
       dtype=torch.float64)
	q_value: tensor([[-51.2039]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9766412541215399, distance: 1.6088687672776667 entropy 1.2378510236740112
epoch: 2, step: 93
	action: tensor([[-0.5430, -1.0757,  0.0115, -0.8071,  0.0505,  0.0894, -0.7547]],
       dtype=torch.float64)
	q_value: tensor([[-47.0984]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0382766759878064, distance: 1.633760006987532 entropy 1.2378510236740112
epoch: 2, step: 94
	action: tensor([[ 0.7378, -0.6836, -0.0889,  0.0086,  0.5260, -0.4160, -0.4282]],
       dtype=torch.float64)
	q_value: tensor([[-42.0342]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08673389438017898, distance: 1.1929391720295517 entropy 1.2378510236740112
epoch: 2, step: 95
	action: tensor([[-0.7017, -0.3545, -0.9785, -0.8005, -0.8534, -0.5977, -0.5725]],
       dtype=torch.float64)
	q_value: tensor([[-36.7668]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0008950655405550734, distance: 1.1448562710075747 entropy 1.2378510236740112
epoch: 2, step: 96
	action: tensor([[ 0.1910, -2.4847,  1.3592, -0.3082,  0.6314, -1.0864,  1.7561]],
       dtype=torch.float64)
	q_value: tensor([[-42.6797]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.437367078721331 entropy 1.2378510236740112
epoch: 2, step: 97
	action: tensor([[ 0.6267, -0.0270,  0.5403, -0.9007, -0.3512,  0.1444,  0.4018]],
       dtype=torch.float64)
	q_value: tensor([[-47.6123]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21989200796499764, distance: 1.0107274410215559 entropy 1.2378510236740112
epoch: 2, step: 98
	action: tensor([[ 1.2075, -0.2100, -0.6531, -0.5493, -0.4336,  0.0951,  0.6118]],
       dtype=torch.float64)
	q_value: tensor([[-36.9273]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1431727577116616, distance: 1.0592618382557142 entropy 1.2378510236740112
epoch: 2, step: 99
	action: tensor([[ 1.1565, -0.6911,  1.7702, -0.6838,  0.1869, -0.2033, -0.0482]],
       dtype=torch.float64)
	q_value: tensor([[-40.3238]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2888339254217408, distance: 0.9650330980128929 entropy 1.2378510236740112
epoch: 2, step: 100
	action: tensor([[-0.4593, -0.3552,  0.8792, -1.0394, -0.5448, -0.1848,  0.3977]],
       dtype=torch.float64)
	q_value: tensor([[-43.4522]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0739738724069716, distance: 1.64800427418272 entropy 1.2378510236740112
epoch: 2, step: 101
	action: tensor([[ 0.0841,  0.2923,  1.4049,  0.1780,  1.2513, -0.0081,  0.0268]],
       dtype=torch.float64)
	q_value: tensor([[-40.2541]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7914334637390753 entropy 1.2378510236740112
epoch: 2, step: 102
	action: tensor([[-0.0477, -2.3277, -2.0651, -1.4681,  0.2817, -0.4855,  1.6773]],
       dtype=torch.float64)
	q_value: tensor([[-41.8885]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.6909355499578276 entropy 1.2378510236740112
epoch: 2, step: 103
	action: tensor([[ 0.3296,  0.3272, -0.4179,  0.3248, -0.9001, -0.3521, -1.0637]],
       dtype=torch.float64)
	q_value: tensor([[-58.8865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7672109750070912 entropy 1.2378510236740112
epoch: 2, step: 104
	action: tensor([[-0.8172,  0.0202, -0.6711,  0.0945, -0.7019, -1.6405,  1.0681]],
       dtype=torch.float64)
	q_value: tensor([[-31.2553]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0078026787798934905, distance: 1.1488000544079617 entropy 1.2378510236740112
epoch: 2, step: 105
	action: tensor([[-1.6612, -0.7315,  0.4550,  0.5491, -1.2239,  0.1550,  1.0419]],
       dtype=torch.float64)
	q_value: tensor([[-49.7673]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.8555878845034428 entropy 1.2378510236740112
epoch: 2, step: 106
	action: tensor([[ 0.8818,  0.9517, -0.4136, -0.6173,  0.0933, -1.6469,  1.1768]],
       dtype=torch.float64)
	q_value: tensor([[-50.4989]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9638958141626357 entropy 1.2378510236740112
epoch: 2, step: 107
	action: tensor([[-0.3234,  0.8404,  0.6912, -1.6683,  0.9312, -0.5304,  1.4116]],
       dtype=torch.float64)
	q_value: tensor([[-41.2884]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.403721111592505, distance: 1.3558046188206243 entropy 1.2378510236740112
epoch: 2, step: 108
	action: tensor([[-1.2743, -0.6714,  0.9839,  0.3194,  0.0840,  0.0071, -0.2988]],
       dtype=torch.float64)
	q_value: tensor([[-49.1785]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.3739984440103798, distance: 1.7631810446809284 entropy 1.2378510236740112
epoch: 2, step: 109
	action: tensor([[-0.0532, -0.9897, -0.1939, -0.8216, -0.1956, -0.2023,  0.7064]],
       dtype=torch.float64)
	q_value: tensor([[-42.4473]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.810387252800536, distance: 1.539722424158862 entropy 1.2378510236740112
epoch: 2, step: 110
	action: tensor([[-0.5656, -0.1864,  0.0909, -0.5572,  0.2992, -1.1445,  0.1252]],
       dtype=torch.float64)
	q_value: tensor([[-40.7758]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4710712744284145 entropy 1.2378510236740112
epoch: 2, step: 111
	action: tensor([[ 0.7582,  1.3520,  0.9127, -0.9656,  0.9289,  0.2572,  2.0016]],
       dtype=torch.float64)
	q_value: tensor([[-38.2778]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9996068912497311 entropy 1.2378510236740112
epoch: 2, step: 112
	action: tensor([[-0.2207, -0.5473, -0.2957, -1.5486,  0.2596,  0.1534, -0.5649]],
       dtype=torch.float64)
	q_value: tensor([[-41.1903]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3522381297788939, distance: 1.330709576088761 entropy 1.2378510236740112
epoch: 2, step: 113
	action: tensor([[-0.4984, -1.8976,  0.4781,  0.1055, -0.0162,  0.5395,  0.3656]],
       dtype=torch.float64)
	q_value: tensor([[-43.1230]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.526934569797516 entropy 1.2378510236740112
epoch: 2, step: 114
	action: tensor([[-0.7159,  0.3932, -1.4811, -0.6211,  0.0089,  0.5112, -2.3807]],
       dtype=torch.float64)
	q_value: tensor([[-40.0561]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.00839913553765026, distance: 1.149139956363361 entropy 1.2378510236740112
epoch: 2, step: 115
	action: tensor([[-1.6317, -0.6947,  1.2214, -1.4391,  1.8039,  0.4715,  0.3545]],
       dtype=torch.float64)
	q_value: tensor([[-49.3881]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.6709698694976147 entropy 1.2378510236740112
epoch: 2, step: 116
	action: tensor([[ 0.5866, -0.0668,  2.0343,  0.0668,  0.5491, -0.1941,  0.0244]],
       dtype=torch.float64)
	q_value: tensor([[-57.3312]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9533595126966262 entropy 1.2378510236740112
epoch: 2, step: 117
	action: tensor([[-0.1838, -0.7187,  0.0995,  0.4151,  2.4126,  0.4794,  0.5052]],
       dtype=torch.float64)
	q_value: tensor([[-44.3780]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20096621863781738, distance: 1.2540708960302942 entropy 1.2378510236740112
epoch: 2, step: 118
	action: tensor([[ 0.0276,  1.6779,  0.3628,  0.1949,  0.2061, -0.4073, -0.6521]],
       dtype=torch.float64)
	q_value: tensor([[-50.2986]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8651726047434273 entropy 1.2378510236740112
epoch: 2, step: 119
	action: tensor([[-0.0334,  0.0954, -1.3155,  0.1946,  0.8848, -0.3678,  0.3404]],
       dtype=torch.float64)
	q_value: tensor([[-41.6931]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2509830639508085, distance: 0.9903814836387359 entropy 1.2378510236740112
epoch: 2, step: 120
	action: tensor([[ 0.3284, -0.5642,  0.2400,  0.3357, -0.8723,  0.3571, -0.8516]],
       dtype=torch.float64)
	q_value: tensor([[-37.1299]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38514469820432906, distance: 0.8973113360670009 entropy 1.2378510236740112
epoch: 2, step: 121
	action: tensor([[-0.7582, -1.5639, -0.9464, -0.9549, -1.2510,  0.8562,  0.6178]],
       dtype=torch.float64)
	q_value: tensor([[-39.1074]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4559680251923706 entropy 1.2378510236740112
epoch: 2, step: 122
	action: tensor([[ 0.5248, -0.2102, -0.1399,  0.2264,  0.1782, -0.5595, -0.7255]],
       dtype=torch.float64)
	q_value: tensor([[-52.2930]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4951709877028846, distance: 0.8130717017755156 entropy 1.2378510236740112
epoch: 2, step: 123
	action: tensor([[ 0.9104, -0.7309,  0.8116, -0.5700, -0.0070, -0.3685,  0.1810]],
       dtype=torch.float64)
	q_value: tensor([[-34.6878]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.333769951333996, distance: 1.3215912595055346 entropy 1.2378510236740112
epoch: 2, step: 124
	action: tensor([[ 0.4478, -0.3762,  0.2253,  0.6354, -0.5076, -0.0585,  0.1098]],
       dtype=torch.float64)
	q_value: tensor([[-38.1657]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7123808948611212, distance: 0.6137133332132753 entropy 1.2378510236740112
epoch: 2, step: 125
	action: tensor([[-0.4991, -0.2959, -1.0979,  0.2642,  1.5460, -0.0447, -0.3543]],
       dtype=torch.float64)
	q_value: tensor([[-35.8858]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6290230103675505, distance: 1.460563027281613 entropy 1.2378510236740112
epoch: 2, step: 126
	action: tensor([[-0.3437, -0.5761, -0.4312,  0.9611,  0.6284, -2.1810, -0.2441]],
       dtype=torch.float64)
	q_value: tensor([[-42.3829]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.28326578941299 entropy 1.2378510236740112
epoch: 2, step: 127
	action: tensor([[-0.2538, -1.3737,  0.7518, -0.1551,  1.5328, -0.7055,  0.3965]],
       dtype=torch.float64)
	q_value: tensor([[-52.3783]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.6236629402618383 entropy 1.2378510236740112
LOSS epoch 2 actor 392.14539125794147 critic 64.73770254491066 
epoch: 3, step: 0
	action: tensor([[-0.3366, -0.4850, -0.5398, -0.4722,  0.2693,  2.2544,  0.3876]],
       dtype=torch.float64)
	q_value: tensor([[-49.7336]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0796942790288397 entropy 1.2378510236740112
epoch: 3, step: 1
	action: tensor([[-0.0298, -0.3708, -1.1118,  0.5539, -0.5251, -0.5090, -0.2631]],
       dtype=torch.float64)
	q_value: tensor([[-55.1030]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.011311815917661283, distance: 1.1507983591870767 entropy 1.2378510236740112
epoch: 3, step: 2
	action: tensor([[ 0.6480, -0.3037, -0.4693, -0.2948,  1.3144, -1.9264,  0.7591]],
       dtype=torch.float64)
	q_value: tensor([[-42.2511]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2524682271771056, distance: 0.9893991240064993 entropy 1.2378510236740112
epoch: 3, step: 3
	action: tensor([[ 0.7349,  0.1925, -0.1030, -0.8016,  0.1227,  0.0698,  0.3722]],
       dtype=torch.float64)
	q_value: tensor([[-52.2786]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5094827029044553, distance: 0.8014636990896143 entropy 1.2378510236740112
epoch: 3, step: 4
	action: tensor([[-0.1817,  1.4110,  0.0024,  0.5183, -0.4104, -0.7886,  0.6167]],
       dtype=torch.float64)
	q_value: tensor([[-38.5648]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.5169151589519853 entropy 1.2378510236740112
epoch: 3, step: 5
	action: tensor([[ 1.1207, -0.9138,  0.2965,  1.4305, -0.1949,  0.7343, -0.8520]],
       dtype=torch.float64)
	q_value: tensor([[-36.4577]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8135815435149594, distance: 0.49408440599137626 entropy 1.2378510236740112
epoch: 3, step: 6
	action: tensor([[ 0.7072,  0.9121, -0.0095,  1.2377,  0.4450,  0.3323, -0.6228]],
       dtype=torch.float64)
	q_value: tensor([[-51.7087]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7315847196866737 entropy 1.2378510236740112
epoch: 3, step: 7
	action: tensor([[ 1.2468, -1.1466, -0.2101, -1.3664,  0.6428, -0.8601,  0.3813]],
       dtype=torch.float64)
	q_value: tensor([[-42.6776]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5963991575495933, distance: 1.4458639790359555 entropy 1.2378510236740112
epoch: 3, step: 8
	action: tensor([[ 0.7173, -0.3521,  0.6315, -1.2596, -0.2553, -0.0574, -0.4705]],
       dtype=torch.float64)
	q_value: tensor([[-53.5193]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2791146563029516 entropy 1.2378510236740112
epoch: 3, step: 9
	action: tensor([[ 1.0087, -0.7214,  0.7275, -0.8979,  0.1690,  1.5117, -0.5543]],
       dtype=torch.float64)
	q_value: tensor([[-43.6508]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05736716281308751, distance: 1.1110356017006058 entropy 1.2378510236740112
epoch: 3, step: 10
	action: tensor([[-0.4367,  0.9880,  0.5757, -0.9697, -0.9132,  0.0664,  0.6533]],
       dtype=torch.float64)
	q_value: tensor([[-50.3433]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1648125295946927 entropy 1.2378510236740112
epoch: 3, step: 11
	action: tensor([[-0.1958, -0.7183,  1.1625, -0.1631,  0.6677,  0.3613, -1.0313]],
       dtype=torch.float64)
	q_value: tensor([[-45.0818]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5663690707331015, distance: 1.4322002420480147 entropy 1.2378510236740112
epoch: 3, step: 12
	action: tensor([[ 0.4401, -0.8933, -1.0774, -0.8039,  0.7178, -0.4688, -0.2601]],
       dtype=torch.float64)
	q_value: tensor([[-47.6429]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3491085899345969, distance: 1.3291688264001145 entropy 1.2378510236740112
epoch: 3, step: 13
	action: tensor([[ 0.8577, -0.9292, -0.9306, -0.4051,  1.1252, -0.7366,  0.9138]],
       dtype=torch.float64)
	q_value: tensor([[-48.3212]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5766853326091184, distance: 1.4369088083395323 entropy 1.2378510236740112
epoch: 3, step: 14
	action: tensor([[ 0.1387, -0.3717,  0.7366, -1.0958, -0.8080, -1.3245, -1.2770]],
       dtype=torch.float64)
	q_value: tensor([[-50.0646]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.27184648554822877, distance: 1.2905476383036831 entropy 1.2378510236740112
epoch: 3, step: 15
	action: tensor([[-1.0134, -1.0312,  1.0856,  1.4952,  1.0012, -0.9402,  0.4879]],
       dtype=torch.float64)
	q_value: tensor([[-52.8378]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3182740567810072, distance: 1.3138916298238246 entropy 1.2378510236740112
epoch: 3, step: 16
	action: tensor([[-1.0095, -0.4003, -0.0249,  0.1311, -1.5864, -1.1566,  0.8257]],
       dtype=torch.float64)
	q_value: tensor([[-57.1086]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.592436990405069, distance: 1.4440685904106598 entropy 1.2378510236740112
epoch: 3, step: 17
	action: tensor([[ 0.4740, -0.5731, -0.5971, -2.6397,  0.8966, -0.6063,  0.4680]],
       dtype=torch.float64)
	q_value: tensor([[-56.2459]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2096885641995971 entropy 1.2378510236740112
epoch: 3, step: 18
	action: tensor([[ 0.1921, -0.6103,  1.4472, -0.3978, -0.6582, -1.1906, -0.2773]],
       dtype=torch.float64)
	q_value: tensor([[-57.1089]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.41179769808988165, distance: 1.3596994691576312 entropy 1.2378510236740112
epoch: 3, step: 19
	action: tensor([[ 0.0569, -0.4168, -1.1228,  0.1777,  1.8320, -1.4272,  0.1160]],
       dtype=torch.float64)
	q_value: tensor([[-51.3265]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2325520138592463, distance: 1.2704551003319875 entropy 1.2378510236740112
epoch: 3, step: 20
	action: tensor([[-0.6865, -0.1211,  0.3977,  0.4200, -0.9801,  0.2574,  0.5272]],
       dtype=torch.float64)
	q_value: tensor([[-52.2848]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.27839682067672245, distance: 1.2938666957203646 entropy 1.2378510236740112
epoch: 3, step: 21
	action: tensor([[ 1.4591, -1.1251,  0.1904, -0.3915,  2.4853, -0.7544, -0.2489]],
       dtype=torch.float64)
	q_value: tensor([[-43.7660]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.48395459822872144, distance: 1.3940135660921147 entropy 1.2378510236740112
epoch: 3, step: 22
	action: tensor([[ 1.8893, -0.6259, -0.3962,  0.3500,  0.5246, -0.8654, -0.5981]],
       dtype=torch.float64)
	q_value: tensor([[-62.2220]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1876401290541119 entropy 1.2378510236740112
epoch: 3, step: 23
	action: tensor([[ 0.8979,  0.5434, -0.3579,  0.1669, -0.1484,  0.2702,  1.4137]],
       dtype=torch.float64)
	q_value: tensor([[-46.6011]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.045668251202402 entropy 1.2378510236740112
epoch: 3, step: 24
	action: tensor([[ 0.2889, -0.8672, -1.2575, -0.9108, -0.5878,  1.1419, -1.1692]],
       dtype=torch.float64)
	q_value: tensor([[-43.2004]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.272250532044199, distance: 1.290752615810847 entropy 1.2378510236740112
epoch: 3, step: 25
	action: tensor([[-0.3922,  0.4485,  0.3170, -1.0017, -0.2747, -1.3254, -0.7110]],
       dtype=torch.float64)
	q_value: tensor([[-53.0617]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13690199688422378, distance: 1.2201640016697821 entropy 1.2378510236740112
epoch: 3, step: 26
	action: tensor([[ 0.4974, -1.4387,  0.8078, -0.8160,  1.0271, -0.7652, -0.2153]],
       dtype=torch.float64)
	q_value: tensor([[-46.0885]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.5249635148883343 entropy 1.2378510236740112
epoch: 3, step: 27
	action: tensor([[ 1.7108,  0.3787, -0.1374, -0.1097, -0.4575, -0.7187,  0.7935]],
       dtype=torch.float64)
	q_value: tensor([[-50.0025]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9546869469313938 entropy 1.2378510236740112
epoch: 3, step: 28
	action: tensor([[-0.5925, -0.0462,  1.0205, -0.9997, -0.3566,  0.1358,  1.0907]],
       dtype=torch.float64)
	q_value: tensor([[-43.2978]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0454007365155396, distance: 1.6366126258353662 entropy 1.2378510236740112
epoch: 3, step: 29
	action: tensor([[ 0.0271,  0.6421,  0.6047,  0.6855, -1.3308,  0.7306,  0.6634]],
       dtype=torch.float64)
	q_value: tensor([[-47.4425]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1375710126484224 entropy 1.2378510236740112
epoch: 3, step: 30
	action: tensor([[ 1.7800,  0.3015,  0.4855, -1.0967, -0.1154, -1.7752,  0.7531]],
       dtype=torch.float64)
	q_value: tensor([[-45.1425]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8948465301214433 entropy 1.2378510236740112
epoch: 3, step: 31
	action: tensor([[-0.4448,  0.9115,  0.5463, -0.0316, -0.2419, -2.0107,  0.8689]],
       dtype=torch.float64)
	q_value: tensor([[-55.5129]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9459311666935983 entropy 1.2378510236740112
epoch: 3, step: 32
	action: tensor([[ 0.3082, -1.4935,  1.0236, -2.0760,  1.0810,  0.8278,  0.9546]],
       dtype=torch.float64)
	q_value: tensor([[-35.9346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4519217868258951 entropy 1.2378510236740112
epoch: 3, step: 33
	action: tensor([[-0.4838,  0.6451,  0.2341, -1.6497, -0.6117, -0.9854,  0.7743]],
       dtype=torch.float64)
	q_value: tensor([[-58.7195]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.00966239412579406, distance: 1.1388022817258112 entropy 1.2378510236740112
epoch: 3, step: 34
	action: tensor([[ 0.0035, -1.1289,  0.3720, -0.3276, -0.8847,  0.2412,  0.8540]],
       dtype=torch.float64)
	q_value: tensor([[-51.1514]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.920878453745029, distance: 1.5860126095507656 entropy 1.2378510236740112
epoch: 3, step: 35
	action: tensor([[-1.7286, -0.7111,  2.0040, -0.0207,  0.4559, -2.3997, -0.0376]],
       dtype=torch.float64)
	q_value: tensor([[-46.7935]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.5795268679910686 entropy 1.2378510236740112
epoch: 3, step: 36
	action: tensor([[-0.1377, -1.7705,  0.1681,  0.3476,  2.0872,  0.8080, -0.8817]],
       dtype=torch.float64)
	q_value: tensor([[-65.9522]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4021660686229696 entropy 1.2378510236740112
epoch: 3, step: 37
	action: tensor([[-0.3014, -1.5404,  0.8452,  0.1443,  0.5193,  0.7728,  1.2575]],
       dtype=torch.float64)
	q_value: tensor([[-56.4325]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3472889834886022 entropy 1.2378510236740112
epoch: 3, step: 38
	action: tensor([[-0.4555,  0.1389, -0.0730,  0.6239,  0.4687,  0.2894, -1.3094]],
       dtype=torch.float64)
	q_value: tensor([[-48.2354]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0308298200712014 entropy 1.2378510236740112
epoch: 3, step: 39
	action: tensor([[ 0.2131, -0.7853,  1.0649,  0.2162, -1.3868,  0.6821,  1.2156]],
       dtype=torch.float64)
	q_value: tensor([[-43.1102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12385455885466057, distance: 1.071136444442923 entropy 1.2378510236740112
epoch: 3, step: 40
	action: tensor([[-0.5359, -0.5780, -1.1840, -0.6618,  0.1278,  1.7539, -0.7208]],
       dtype=torch.float64)
	q_value: tensor([[-51.2986]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.34872966381558324, distance: 1.3289821504870956 entropy 1.2378510236740112
epoch: 3, step: 41
	action: tensor([[ 1.1615, -0.5442, -0.0706, -0.1737,  0.5357,  0.6653, -1.5569]],
       dtype=torch.float64)
	q_value: tensor([[-53.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2528955186290628, distance: 0.9891163118011259 entropy 1.2378510236740112
epoch: 3, step: 42
	action: tensor([[ 0.2998, -0.1433,  1.6933, -0.9144,  0.7501, -0.4056, -1.1080]],
       dtype=torch.float64)
	q_value: tensor([[-48.6924]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16353854632207243, distance: 1.0465974191019591 entropy 1.2378510236740112
epoch: 3, step: 43
	action: tensor([[ 0.0849,  0.1254,  0.2557,  1.1386, -0.6909,  0.1419, -0.5111]],
       dtype=torch.float64)
	q_value: tensor([[-51.8832]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.5307562867760723 entropy 1.2378510236740112
epoch: 3, step: 44
	action: tensor([[ 0.8195, -0.7515,  1.1436,  1.2950, -1.8902,  0.2412, -1.5632]],
       dtype=torch.float64)
	q_value: tensor([[-40.3092]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5194055944673976, distance: 0.7933156991848217 entropy 1.2378510236740112
epoch: 3, step: 45
	action: tensor([[ 0.6248, -1.5920,  0.9159, -0.1829,  0.9617,  1.3765, -0.0375]],
       dtype=torch.float64)
	q_value: tensor([[-63.9591]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3586897231122306 entropy 1.2378510236740112
epoch: 3, step: 46
	action: tensor([[-1.7618,  0.0492,  1.2371,  0.7067, -0.0989, -0.4526, -0.7698]],
       dtype=torch.float64)
	q_value: tensor([[-51.5598]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.6863574406063804 entropy 1.2378510236740112
epoch: 3, step: 47
	action: tensor([[ 0.2057, -0.0998,  0.6753, -0.8918,  0.1545, -0.3597,  0.7193]],
       dtype=torch.float64)
	q_value: tensor([[-54.6540]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23542219008685428, distance: 1.2719334597212522 entropy 1.2378510236740112
epoch: 3, step: 48
	action: tensor([[-0.0953, -0.6493,  0.6553,  0.2353,  0.6218,  0.5970, -0.1594]],
       dtype=torch.float64)
	q_value: tensor([[-41.0204]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11246632573670479, distance: 1.0780753427284329 entropy 1.2378510236740112
epoch: 3, step: 49
	action: tensor([[-0.7404, -0.7009, -1.2396, -0.8441,  0.6295,  0.7647, -0.1051]],
       dtype=torch.float64)
	q_value: tensor([[-40.7377]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.40180528377634617, distance: 1.3548790877224675 entropy 1.2378510236740112
epoch: 3, step: 50
	action: tensor([[ 0.6669, -0.7762, -0.5200, -0.2754,  0.8999,  0.4368,  1.0194]],
       dtype=torch.float64)
	q_value: tensor([[-49.3078]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03484823423500494, distance: 1.1641126933394503 entropy 1.2378510236740112
epoch: 3, step: 51
	action: tensor([[-1.1088,  0.0773, -0.1650, -0.5853,  0.5718, -1.1419,  1.7505]],
       dtype=torch.float64)
	q_value: tensor([[-46.1545]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.008115322827416, distance: 1.6216271925084544 entropy 1.2378510236740112
epoch: 3, step: 52
	action: tensor([[-0.4329,  0.9791,  0.3994, -0.4327, -0.1474,  0.7130, -0.1726]],
       dtype=torch.float64)
	q_value: tensor([[-52.4175]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.186098982425584 entropy 1.2378510236740112
epoch: 3, step: 53
	action: tensor([[1.0778, 0.3533, 0.7400, 1.1146, 0.6037, 0.2063, 0.2684]],
       dtype=torch.float64)
	q_value: tensor([[-40.7454]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0730332020267312 entropy 1.2378510236740112
epoch: 3, step: 54
	action: tensor([[-0.2238,  0.6567, -0.0709, -1.1941,  0.8708,  1.0337,  0.2664]],
       dtype=torch.float64)
	q_value: tensor([[-36.8983]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7845570835487039 entropy 1.2378510236740112
epoch: 3, step: 55
	action: tensor([[-0.6681, -0.9511,  0.1836, -0.1624,  2.3715, -1.4070, -0.3908]],
       dtype=torch.float64)
	q_value: tensor([[-48.4909]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8889508542454123, distance: 1.5727765407511491 entropy 1.2378510236740112
epoch: 3, step: 56
	action: tensor([[ 0.0836,  0.2081, -0.3086, -2.8039,  0.0387, -0.3231, -0.1326]],
       dtype=torch.float64)
	q_value: tensor([[-61.5421]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1357916982768554 entropy 1.2378510236740112
epoch: 3, step: 57
	action: tensor([[-0.5565, -0.5812,  0.4289, -0.6338,  0.0601, -1.8598,  1.0697]],
       dtype=torch.float64)
	q_value: tensor([[-51.2760]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.43547298152542835, distance: 1.3710528774949737 entropy 1.2378510236740112
epoch: 3, step: 58
	action: tensor([[ 2.3913,  1.3289,  1.1939,  0.4728,  1.1453, -1.5795, -0.1640]],
       dtype=torch.float64)
	q_value: tensor([[-51.0016]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7634288181696134 entropy 1.2378510236740112
epoch: 3, step: 59
	action: tensor([[-1.4402,  1.2437,  0.4759, -0.0681, -0.5623,  0.0476, -1.1777]],
       dtype=torch.float64)
	q_value: tensor([[-58.4445]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3339388506588572 entropy 1.2378510236740112
epoch: 3, step: 60
	action: tensor([[-0.0865, -1.2769,  0.5169, -1.1958,  2.1558,  1.0334, -1.0663]],
       dtype=torch.float64)
	q_value: tensor([[-53.4695]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.47790526235954 entropy 1.2378510236740112
epoch: 3, step: 61
	action: tensor([[-0.8717,  0.0439,  0.7172, -0.9752,  0.2108,  0.4217,  0.0329]],
       dtype=torch.float64)
	q_value: tensor([[-64.1521]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1701196396641045, distance: 1.6857708193857461 entropy 1.2378510236740112
epoch: 3, step: 62
	action: tensor([[-1.5362, -0.3308,  1.0185, -0.5055,  0.7386, -1.5723, -0.0363]],
       dtype=torch.float64)
	q_value: tensor([[-43.2508]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1600995481667558, distance: 1.6818744624621886 entropy 1.2378510236740112
epoch: 3, step: 63
	action: tensor([[-1.6201, -0.2527, -0.1575,  0.1194,  1.0667,  0.4079,  1.1765]],
       dtype=torch.float64)
	q_value: tensor([[-54.0568]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.6420954749038343, distance: 1.8600771036140757 entropy 1.2378510236740112
epoch: 3, step: 64
	action: tensor([[-0.4685,  1.2153,  0.1499, -0.3713,  0.5468, -0.0600,  0.5178]],
       dtype=torch.float64)
	q_value: tensor([[-49.9420]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.102984681480618 entropy 1.2378510236740112
epoch: 3, step: 65
	action: tensor([[-0.7326, -0.6318, -0.1408,  1.1383,  1.0083,  1.3904, -0.7424]],
       dtype=torch.float64)
	q_value: tensor([[-43.4791]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2418908988763505, distance: 0.9963743721289449 entropy 1.2378510236740112
epoch: 3, step: 66
	action: tensor([[ 0.5145, -0.2326,  0.3126, -0.6568, -0.1411, -1.1225,  0.8455]],
       dtype=torch.float64)
	q_value: tensor([[-50.5697]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0835212902832918, distance: 1.1911745828083882 entropy 1.2378510236740112
epoch: 3, step: 67
	action: tensor([[-0.5785,  0.3991, -0.2661, -0.4168,  0.4766, -0.1639, -0.7320]],
       dtype=torch.float64)
	q_value: tensor([[-44.2860]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.22100725270579358, distance: 1.264491211037445 entropy 1.2378510236740112
epoch: 3, step: 68
	action: tensor([[ 1.6914, -0.9821,  1.4302, -0.1632,  0.3666,  0.1270,  0.3829]],
       dtype=torch.float64)
	q_value: tensor([[-38.4642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4114498552081154 entropy 1.2378510236740112
epoch: 3, step: 69
	action: tensor([[ 1.3910,  0.4595,  0.3898, -0.1854,  0.3953,  0.3379,  1.0807]],
       dtype=torch.float64)
	q_value: tensor([[-47.9729]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0391807333534613 entropy 1.2378510236740112
epoch: 3, step: 70
	action: tensor([[ 1.0960, -1.6682,  1.2427, -1.7035, -0.2261, -0.8754,  0.3635]],
       dtype=torch.float64)
	q_value: tensor([[-43.0883]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.084949152129257 entropy 1.2378510236740112
epoch: 3, step: 71
	action: tensor([[ 0.6343, -0.3697,  0.0312,  0.0297,  1.5612,  0.4835, -0.8573]],
       dtype=torch.float64)
	q_value: tensor([[-55.5364]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4547532729473408, distance: 0.844993225118879 entropy 1.2378510236740112
epoch: 3, step: 72
	action: tensor([[ 0.6815, -1.0882, -0.6623, -2.0791,  0.0497,  1.4271,  0.2101]],
       dtype=torch.float64)
	q_value: tensor([[-47.4022]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.33848362492803163, distance: 1.3239245162865876 entropy 1.2378510236740112
epoch: 3, step: 73
	action: tensor([[ 0.4207,  0.7167,  0.2942,  0.9417,  1.1170, -0.5353, -1.0133]],
       dtype=torch.float64)
	q_value: tensor([[-62.0759]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.4448490496626432 entropy 1.2378510236740112
epoch: 3, step: 74
	action: tensor([[-0.3998, -0.7230, -1.5661, -0.0260,  0.9247, -0.6584,  1.1245]],
       dtype=torch.float64)
	q_value: tensor([[-39.6535]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5711986276671372, distance: 1.4344064810694737 entropy 1.2378510236740112
epoch: 3, step: 75
	action: tensor([[-0.1006, -1.2372,  1.4353,  0.5323,  0.1352, -0.4957, -0.6852]],
       dtype=torch.float64)
	q_value: tensor([[-51.9274]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.598027522014335, distance: 1.4466011986201297 entropy 1.2378510236740112
epoch: 3, step: 76
	action: tensor([[-0.2915, -1.8713,  1.3229,  0.8085,  0.7458, -0.3053,  0.2164]],
       dtype=torch.float64)
	q_value: tensor([[-52.0664]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2967402663949945 entropy 1.2378510236740112
epoch: 3, step: 77
	action: tensor([[-0.9033, -1.1658,  0.1455,  0.5797,  0.6505, -0.3491,  0.6992]],
       dtype=torch.float64)
	q_value: tensor([[-49.0869]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2431865299836318, distance: 1.7139154253601288 entropy 1.2378510236740112
epoch: 3, step: 78
	action: tensor([[-1.5873,  0.5923,  0.0873,  0.8587,  0.6937, -0.3211,  1.8100]],
       dtype=torch.float64)
	q_value: tensor([[-45.1205]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3691848217278355 entropy 1.2378510236740112
epoch: 3, step: 79
	action: tensor([[ 1.2488, -0.8146,  1.1562, -0.1906,  0.0445,  0.3260, -0.5052]],
       dtype=torch.float64)
	q_value: tensor([[-54.8762]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17450764831803944, distance: 1.240179698863138 entropy 1.2378510236740112
epoch: 3, step: 80
	action: tensor([[ 1.7436, -0.2138, -0.3706, -0.6123,  1.4870,  0.1850,  0.9869]],
       dtype=torch.float64)
	q_value: tensor([[-45.2226]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.269083160678811 entropy 1.2378510236740112
epoch: 3, step: 81
	action: tensor([[ 1.1278, -0.8363,  0.8677, -0.2046, -1.2876,  0.5613,  0.2164]],
       dtype=torch.float64)
	q_value: tensor([[-49.9441]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.005445217022841731, distance: 1.141224399725813 entropy 1.2378510236740112
epoch: 3, step: 82
	action: tensor([[ 0.9696, -0.8660,  0.7252,  0.7609,  0.5581,  0.9094,  0.0681]],
       dtype=torch.float64)
	q_value: tensor([[-49.1106]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3745591402380447, distance: 0.905002582110963 entropy 1.2378510236740112
epoch: 3, step: 83
	action: tensor([[ 0.6337, -0.3712, -0.1600, -0.1274,  0.6460, -0.5412, -0.3444]],
       dtype=torch.float64)
	q_value: tensor([[-45.6768]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14521198366695343, distance: 1.0580005799245236 entropy 1.2378510236740112
epoch: 3, step: 84
	action: tensor([[ 0.2164, -1.9309,  1.4400,  0.5211, -0.2984,  0.3470,  0.4718]],
       dtype=torch.float64)
	q_value: tensor([[-38.8719]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.253936598148737 entropy 1.2378510236740112
epoch: 3, step: 85
	action: tensor([[-0.7279,  0.0583,  0.7072, -0.5062,  1.1898,  0.4553, -1.4408]],
       dtype=torch.float64)
	q_value: tensor([[-47.9559]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7647426477976702, distance: 1.520188293652543 entropy 1.2378510236740112
epoch: 3, step: 86
	action: tensor([[ 0.3492,  0.5341,  0.8559, -0.3221,  0.5993,  0.0427, -0.4506]],
       dtype=torch.float64)
	q_value: tensor([[-50.9597]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8580818707132407 entropy 1.2378510236740112
epoch: 3, step: 87
	action: tensor([[-0.0039, -0.3423, -0.0025, -1.0693,  1.7984,  0.2108,  0.5419]],
       dtype=torch.float64)
	q_value: tensor([[-41.9273]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3579233743708128, distance: 1.333504007806188 entropy 1.2378510236740112
epoch: 3, step: 88
	action: tensor([[ 0.7021,  0.1022, -1.4895, -0.4122, -0.6783, -0.5432,  0.1768]],
       dtype=torch.float64)
	q_value: tensor([[-50.5393]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8316780730943607, distance: 0.46949075025502446 entropy 1.2378510236740112
epoch: 3, step: 89
	action: tensor([[-0.5722, -1.0188, -0.0225,  0.1412,  0.9471, -0.3687,  1.7015]],
       dtype=torch.float64)
	q_value: tensor([[-44.5880]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.114333956533426, distance: 1.6639623124681693 entropy 1.2378510236740112
epoch: 3, step: 90
	action: tensor([[-0.6982, -0.4942, -0.2631, -1.7794,  0.3323,  0.3419, -0.7534]],
       dtype=torch.float64)
	q_value: tensor([[-49.3615]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.36491006777446877, distance: 1.3369301327790366 entropy 1.2378510236740112
epoch: 3, step: 91
	action: tensor([[-0.1827, -1.1846, -0.5846, -0.1627,  1.1252, -0.9372,  1.2406]],
       dtype=torch.float64)
	q_value: tensor([[-51.0077]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9315598362694046, distance: 1.5904161474230332 entropy 1.2378510236740112
epoch: 3, step: 92
	action: tensor([[ 0.3172, -0.3348,  0.8780, -0.1510,  0.6263, -1.2654,  0.7667]],
       dtype=torch.float64)
	q_value: tensor([[-50.2128]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09160875732944984, distance: 1.0906694853379788 entropy 1.2378510236740112
epoch: 3, step: 93
	action: tensor([[-0.5995, -0.1661,  1.3394,  0.3609,  1.4688,  0.0892,  1.8282]],
       dtype=torch.float64)
	q_value: tensor([[-45.3229]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2081897441095839, distance: 1.2578367106229311 entropy 1.2378510236740112
epoch: 3, step: 94
	action: tensor([[-0.7527, -0.2137,  0.0918, -0.7587,  0.5391,  0.5114, -1.0570]],
       dtype=torch.float64)
	q_value: tensor([[-53.0526]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8408001962623828, distance: 1.5526015637912929 entropy 1.2378510236740112
epoch: 3, step: 95
	action: tensor([[ 0.6326, -1.1204,  0.3500, -0.9704,  0.1305, -2.2968,  0.5221]],
       dtype=torch.float64)
	q_value: tensor([[-44.2724]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.162205849260897 entropy 1.2378510236740112
epoch: 3, step: 96
	action: tensor([[-0.5524,  0.3213,  0.9826, -0.6271, -0.0489,  0.1330, -1.1271]],
       dtype=torch.float64)
	q_value: tensor([[-57.2674]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6312682718386751, distance: 1.4615692183171027 entropy 1.2378510236740112
epoch: 3, step: 97
	action: tensor([[ 1.2783, -1.2382,  0.7190,  1.0677,  1.7563,  0.5174,  0.4568]],
       dtype=torch.float64)
	q_value: tensor([[-45.4500]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2244552969576663, distance: 1.2662753723833204 entropy 1.2378510236740112
epoch: 3, step: 98
	action: tensor([[ 0.4944,  0.4849, -0.2736, -0.2909,  1.0234, -0.6308,  0.2673]],
       dtype=torch.float64)
	q_value: tensor([[-54.4921]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7252484759939174, distance: 0.5998280186741464 entropy 1.2378510236740112
epoch: 3, step: 99
	action: tensor([[ 0.2793, -0.3779, -0.3251,  0.0787, -1.2313,  0.0652,  0.1371]],
       dtype=torch.float64)
	q_value: tensor([[-39.7984]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17875985895794022, distance: 1.0370310925610193 entropy 1.2378510236740112
epoch: 3, step: 100
	action: tensor([[ 0.6101, -2.2553, -0.6779, -0.3030,  0.5088,  0.1899, -0.7171]],
       dtype=torch.float64)
	q_value: tensor([[-42.0962]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4822687555684002 entropy 1.2378510236740112
epoch: 3, step: 101
	action: tensor([[ 0.7291, -1.3274,  0.1307,  0.3516,  0.3821, -0.0164, -0.5416]],
       dtype=torch.float64)
	q_value: tensor([[-46.9903]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3268444520550067 entropy 1.2378510236740112
epoch: 3, step: 102
	action: tensor([[-0.4660,  0.1877,  0.0542, -1.4876,  0.6463,  0.0149,  0.0179]],
       dtype=torch.float64)
	q_value: tensor([[-43.8199]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4361018811329189, distance: 1.3713531827626484 entropy 1.2378510236740112
epoch: 3, step: 103
	action: tensor([[ 0.4130, -0.6770, -0.2493,  0.5356, -0.4939,  0.3420, -0.2851]],
       dtype=torch.float64)
	q_value: tensor([[-45.2665]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3439001348545693, distance: 0.926918750480704 entropy 1.2378510236740112
epoch: 3, step: 104
	action: tensor([[ 0.6283,  0.4352, -0.2695,  0.1473,  0.7238,  1.0679, -0.0735]],
       dtype=torch.float64)
	q_value: tensor([[-39.4319]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9759763794577317, distance: 0.1773682671210028 entropy 1.2378510236740112
epoch: 3, step: 105
	action: tensor([[-0.4041, -0.8234, -0.9192, -0.3044,  0.7310, -0.1456, -0.5720]],
       dtype=torch.float64)
	q_value: tensor([[-40.3512]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6425231875160866, distance: 1.4666025908744662 entropy 1.2378510236740112
epoch: 3, step: 106
	action: tensor([[ 1.1044, -0.5833,  1.5297, -0.3053, -0.0455, -1.0508,  0.8859]],
       dtype=torch.float64)
	q_value: tensor([[-43.8730]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3072611752384514, distance: 0.9524484020559907 entropy 1.2378510236740112
epoch: 3, step: 107
	action: tensor([[ 1.6231,  0.6195, -1.2617, -1.0907, -0.3948,  0.2712, -0.4981]],
       dtype=torch.float64)
	q_value: tensor([[-49.6438]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07239186246714346, distance: 1.1021455924183279 entropy 1.2378510236740112
epoch: 3, step: 108
	action: tensor([[ 0.0994, -1.4034, -0.2737, -0.5707, -0.4101,  0.2953,  0.1252]],
       dtype=torch.float64)
	q_value: tensor([[-49.3904]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.580847910895693 entropy 1.2378510236740112
epoch: 3, step: 109
	action: tensor([[-0.1944, -0.3673,  0.8007,  0.8758, -1.1033, -0.4663, -1.4360]],
       dtype=torch.float64)
	q_value: tensor([[-44.6816]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.48189860600922607, distance: 0.8236905314270409 entropy 1.2378510236740112
epoch: 3, step: 110
	action: tensor([[ 0.3615,  1.0518,  0.5392, -0.9683,  1.1034,  0.1521,  0.7102]],
       dtype=torch.float64)
	q_value: tensor([[-54.3329]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7912420075796613 entropy 1.2378510236740112
epoch: 3, step: 111
	action: tensor([[ 0.6305,  0.5897, -1.8534,  0.0573, -2.1948, -0.2550,  1.3053]],
       dtype=torch.float64)
	q_value: tensor([[-38.4701]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9618597000168502 entropy 1.2378510236740112
epoch: 3, step: 112
	action: tensor([[ 0.1603, -0.2433, -0.9626,  0.4461, -0.4046,  0.2663,  0.5555]],
       dtype=torch.float64)
	q_value: tensor([[-47.5106]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19901669378554576, distance: 1.0241614522100042 entropy 1.2378510236740112
epoch: 3, step: 113
	action: tensor([[ 1.1771, -1.0898,  0.8435, -0.3673, -0.3435, -0.0199,  0.7380]],
       dtype=torch.float64)
	q_value: tensor([[-39.4118]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5730201004346465, distance: 1.435237685828588 entropy 1.2378510236740112
epoch: 3, step: 114
	action: tensor([[ 0.3140,  0.0135, -0.6306,  1.2614, -0.1781, -0.5534, -0.8095]],
       dtype=torch.float64)
	q_value: tensor([[-46.6573]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.5336910033140821 entropy 1.2378510236740112
epoch: 3, step: 115
	action: tensor([[ 0.2857, -1.3396,  1.9789,  0.0390,  0.6898, -0.7314,  0.5967]],
       dtype=torch.float64)
	q_value: tensor([[-40.8243]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3904793502567239 entropy 1.2378510236740112
epoch: 3, step: 116
	action: tensor([[ 0.1887, -1.3248, -0.5967, -1.1846, -0.5701, -0.7887,  1.1567]],
       dtype=torch.float64)
	q_value: tensor([[-52.4750]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2416193946629266 entropy 1.2378510236740112
epoch: 3, step: 117
	action: tensor([[ 0.5210, -0.2188,  0.8721,  0.7752, -0.3338, -0.3918, -0.9354]],
       dtype=torch.float64)
	q_value: tensor([[-53.6288]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7600198373582032, distance: 0.5605887331252225 entropy 1.2378510236740112
epoch: 3, step: 118
	action: tensor([[-0.1765,  0.8055,  0.1850, -1.2860,  0.0277, -0.4404, -0.1280]],
       dtype=torch.float64)
	q_value: tensor([[-46.3006]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1063798210462844, distance: 1.0817656248825207 entropy 1.2378510236740112
epoch: 3, step: 119
	action: tensor([[ 0.5953, -0.1031,  0.5513, -0.0959,  0.9635, -0.0745,  0.2050]],
       dtype=torch.float64)
	q_value: tensor([[-42.2020]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5386707985724737, distance: 0.77725256888065 entropy 1.2378510236740112
epoch: 3, step: 120
	action: tensor([[ 0.1470, -0.9587,  1.3511, -0.2861, -0.4752, -1.1590, -1.0311]],
       dtype=torch.float64)
	q_value: tensor([[-38.6130]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6826038926172371, distance: 1.4843886942693483 entropy 1.2378510236740112
epoch: 3, step: 121
	action: tensor([[-0.1494, -0.0724,  1.4219, -0.0440,  1.1446, -0.5065,  0.4142]],
       dtype=torch.float64)
	q_value: tensor([[-54.4564]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06539209492398523, distance: 1.1062961940100067 entropy 1.2378510236740112
epoch: 3, step: 122
	action: tensor([[ 0.5387,  0.7488,  0.8999,  1.0575,  1.4482, -0.7213, -0.4219]],
       dtype=torch.float64)
	q_value: tensor([[-47.0259]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.806322069909037 entropy 1.2378510236740112
epoch: 3, step: 123
	action: tensor([[-0.1517, -1.0026, -0.0471, -2.0178,  0.7078,  0.5414, -0.8634]],
       dtype=torch.float64)
	q_value: tensor([[-46.9555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.31014795636106607, distance: 1.309835826831086 entropy 1.2378510236740112
epoch: 3, step: 124
	action: tensor([[ 0.8514, -1.0695,  0.1215, -1.5013,  0.8825,  0.0884,  0.3666]],
       dtype=torch.float64)
	q_value: tensor([[-56.9698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.84517702937811, distance: 1.554446262466739 entropy 1.2378510236740112
epoch: 3, step: 125
	action: tensor([[ 0.4677,  0.5176,  0.0697, -0.8940,  0.4392, -0.2741,  1.2640]],
       dtype=torch.float64)
	q_value: tensor([[-51.2507]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5538607177139983, distance: 0.7643493936846597 entropy 1.2378510236740112
epoch: 3, step: 126
	action: tensor([[-0.0312,  0.2110,  0.7037, -1.1104, -1.4037,  1.1159,  0.6480]],
       dtype=torch.float64)
	q_value: tensor([[-44.5966]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1080846051797788, distance: 1.204600818319715 entropy 1.2378510236740112
epoch: 3, step: 127
	action: tensor([[-0.5007,  0.0373, -0.1256,  0.4663,  1.0512, -1.3044,  0.0670]],
       dtype=torch.float64)
	q_value: tensor([[-52.8250]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3810680561581674, distance: 1.3448202295560838 entropy 1.2378510236740112
LOSS epoch 3 actor 613.5662155415946 critic 39.43742133807461 
epoch: 4, step: 0
	action: tensor([[-0.6609, -1.7279, -1.5256, -0.4254, -1.2297, -1.0168,  0.3081]],
       dtype=torch.float64)
	q_value: tensor([[-40.1365]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3443071410209468 entropy 1.2378510236740112
epoch: 4, step: 1
	action: tensor([[ 0.2933, -0.1773, -0.2924, -1.4724,  1.5427,  0.3109,  0.5970]],
       dtype=torch.float64)
	q_value: tensor([[-53.2670]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.011709671434233515, distance: 1.1510247020652102 entropy 1.2378510236740112
epoch: 4, step: 2
	action: tensor([[ 0.5467,  0.2595,  1.2342, -0.1981,  1.0673, -0.5336,  0.1214]],
       dtype=torch.float64)
	q_value: tensor([[-45.8948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7605180713851794, distance: 0.5600064985379771 entropy 1.2378510236740112
epoch: 4, step: 3
	action: tensor([[ 0.8337, -0.4435, -0.9316,  0.0926,  0.1763, -1.3743,  0.8582]],
       dtype=torch.float64)
	q_value: tensor([[-41.0177]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19165025960055537, distance: 1.0288601464098701 entropy 1.2378510236740112
epoch: 4, step: 4
	action: tensor([[ 0.6096, -1.3136, -0.8008, -0.6875,  1.1528, -1.4751,  0.0070]],
       dtype=torch.float64)
	q_value: tensor([[-44.5756]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.460798560458305 entropy 1.2378510236740112
epoch: 4, step: 5
	action: tensor([[ 0.9726, -1.5563,  2.5562, -0.1203,  1.1428, -0.7618,  1.6108]],
       dtype=torch.float64)
	q_value: tensor([[-50.1977]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0243513499087706 entropy 1.2378510236740112
epoch: 4, step: 6
	action: tensor([[-0.1700, -1.4255, -0.1750, -0.0819, -0.3223,  0.0326, -0.4886]],
       dtype=torch.float64)
	q_value: tensor([[-45.1390]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.5758693899604557 entropy 1.2378510236740112
epoch: 4, step: 7
	action: tensor([[ 0.4355, -0.1392,  0.3220,  0.8437,  0.2233, -0.5455,  1.1908]],
       dtype=torch.float64)
	q_value: tensor([[-38.8305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8389205139614714, distance: 0.4592792354272964 entropy 1.2378510236740112
epoch: 4, step: 8
	action: tensor([[-0.2793, -0.7142, -0.2198, -0.1628,  0.6617, -1.7031,  0.4415]],
       dtype=torch.float64)
	q_value: tensor([[-38.9746]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5547022985877514, distance: 1.4268565512550595 entropy 1.2378510236740112
epoch: 4, step: 9
	action: tensor([[-0.8167, -0.1753, -0.0406,  0.0935,  1.3450,  0.6540, -0.8087]],
       dtype=torch.float64)
	q_value: tensor([[-43.4801]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4702973500507861, distance: 1.3875839907206877 entropy 1.2378510236740112
epoch: 4, step: 10
	action: tensor([[ 1.0273, -0.2117,  0.9211, -1.1039,  1.1246,  0.6016, -1.8238]],
       dtype=torch.float64)
	q_value: tensor([[-41.6314]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13769505389730574, distance: 1.0626423788574884 entropy 1.2378510236740112
epoch: 4, step: 11
	action: tensor([[-1.5137, -1.4397,  0.2256,  0.1520,  0.7521,  0.3292,  0.0547]],
       dtype=torch.float64)
	q_value: tensor([[-49.9677]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.8209334615698973 entropy 1.2378510236740112
epoch: 4, step: 12
	action: tensor([[-0.3860, -0.4981, -1.2129, -0.4116,  0.0981,  0.9572,  1.0193]],
       dtype=torch.float64)
	q_value: tensor([[-43.2214]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4059590020382977, distance: 1.356884938066777 entropy 1.2378510236740112
epoch: 4, step: 13
	action: tensor([[-1.8700e+00, -2.0033e-01,  6.7899e-01,  3.2708e-04,  8.8307e-01,
         -6.2957e-01, -3.4026e-01]], dtype=torch.float64)
	q_value: tensor([[-43.3365]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.9178311235494339 entropy 1.2378510236740112
epoch: 4, step: 14
	action: tensor([[ 0.4003, -1.3062, -0.4298, -0.5697,  1.0638,  0.4183, -0.5786]],
       dtype=torch.float64)
	q_value: tensor([[-43.6454]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4881363220281847 entropy 1.2378510236740112
epoch: 4, step: 15
	action: tensor([[-0.1698,  0.1311, -0.1431, -0.0387,  0.9605, -1.0194, -0.3695]],
       dtype=torch.float64)
	q_value: tensor([[-43.6423]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07341424649307404, distance: 1.1856059516653121 entropy 1.2378510236740112
epoch: 4, step: 16
	action: tensor([[-0.4684, -1.9069,  0.9380,  0.0366,  1.1025, -0.5398, -0.1458]],
       dtype=torch.float64)
	q_value: tensor([[-37.6067]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.6562107089711713 entropy 1.2378510236740112
epoch: 4, step: 17
	action: tensor([[ 1.3845,  0.9216, -0.8922,  0.6963, -0.1707,  1.3079,  0.2962]],
       dtype=torch.float64)
	q_value: tensor([[-43.2333]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.095491136286322 entropy 1.2378510236740112
epoch: 4, step: 18
	action: tensor([[-0.9747, -0.8182, -0.5152, -0.6323, -0.5064,  0.5838,  0.4637]],
       dtype=torch.float64)
	q_value: tensor([[-40.1353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1855102193660385, distance: 1.6917380368933732 entropy 1.2378510236740112
epoch: 4, step: 19
	action: tensor([[-0.2952, -0.2496, -0.5159, -0.6932,  0.7090, -0.7615,  0.3578]],
       dtype=torch.float64)
	q_value: tensor([[-42.5148]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.35689058345342906, distance: 1.3329968021633414 entropy 1.2378510236740112
epoch: 4, step: 20
	action: tensor([[ 0.2479, -0.0359,  0.2076,  0.8205,  0.8468, -0.4259,  1.1878]],
       dtype=torch.float64)
	q_value: tensor([[-37.8209]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.7436203993215289 entropy 1.2378510236740112
epoch: 4, step: 21
	action: tensor([[-0.1930, -1.5744, -0.8426, -0.3038,  0.5376, -0.1903,  0.4565]],
       dtype=torch.float64)
	q_value: tensor([[-37.5643]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.5268353217224742 entropy 1.2378510236740112
epoch: 4, step: 22
	action: tensor([[ 0.2539,  0.0328,  0.9626, -0.0911, -0.6102, -1.2125, -0.3971]],
       dtype=torch.float64)
	q_value: tensor([[-41.7885]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2633909940760907, distance: 0.9821440849886922 entropy 1.2378510236740112
epoch: 4, step: 23
	action: tensor([[ 0.5907, -1.6452, -0.6346,  0.7052,  0.9419, -1.0248,  0.8476]],
       dtype=torch.float64)
	q_value: tensor([[-42.9659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4018681655536434 entropy 1.2378510236740112
epoch: 4, step: 24
	action: tensor([[ 1.4653e+00,  1.0371e-03, -6.8807e-01, -1.3157e-01,  1.3662e+00,
         -3.7745e-01,  1.0875e+00]], dtype=torch.float64)
	q_value: tensor([[-44.6437]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9706543188909648 entropy 1.2378510236740112
epoch: 4, step: 25
	action: tensor([[-1.4780,  0.0095, -0.1510, -1.0737,  1.4052,  0.6417,  0.6407]],
       dtype=torch.float64)
	q_value: tensor([[-40.3074]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1024631509784943, distance: 1.6592846275583486 entropy 1.2378510236740112
epoch: 4, step: 26
	action: tensor([[ 1.8478,  0.1173,  0.1537, -0.1903, -0.0878, -0.3411,  0.2191]],
       dtype=torch.float64)
	q_value: tensor([[-48.3570]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9837778205078055 entropy 1.2378510236740112
epoch: 4, step: 27
	action: tensor([[ 0.0308,  0.0982,  0.0682, -0.8607, -0.1378, -0.6172,  0.1519]],
       dtype=torch.float64)
	q_value: tensor([[-38.2125]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.025132115129908872, distance: 1.1586349182799554 entropy 1.2378510236740112
epoch: 4, step: 28
	action: tensor([[ 1.5207,  0.6827, -0.4941, -0.0069,  1.0395,  0.3886,  0.6397]],
       dtype=torch.float64)
	q_value: tensor([[-35.1939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6948460860550358, distance: 0.6321442116533853 entropy 1.2378510236740112
epoch: 4, step: 29
	action: tensor([[ 1.0169,  0.3112,  0.1994, -0.9586, -0.0773,  2.1404,  1.2345]],
       dtype=torch.float64)
	q_value: tensor([[-41.7095]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9238258421383961, distance: 0.31583519516698294 entropy 1.2378510236740112
epoch: 4, step: 30
	action: tensor([[ 0.2382,  0.5193,  1.2911,  0.1350,  0.2918, -0.6383, -0.8167]],
       dtype=torch.float64)
	q_value: tensor([[-55.9259]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.34602738236796726 entropy 1.2378510236740112
epoch: 4, step: 31
	action: tensor([[-0.6450,  0.8654,  0.0854, -1.8861, -0.9467, -1.5651,  0.2260]],
       dtype=torch.float64)
	q_value: tensor([[-35.5679]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3943125631185277, distance: 0.8905964837084243 entropy 1.2378510236740112
epoch: 4, step: 32
	action: tensor([[ 1.1916, -1.1962,  0.6957, -0.2617,  0.2250,  0.0220,  0.7313]],
       dtype=torch.float64)
	q_value: tensor([[-51.7526]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7532141604930986, distance: 1.5152147108517595 entropy 1.2378510236740112
epoch: 4, step: 33
	action: tensor([[ 0.5866,  0.4444,  0.4552, -0.7810,  0.1362, -0.8736,  0.0187]],
       dtype=torch.float64)
	q_value: tensor([[-41.1448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4828499836950616, distance: 0.8229339219324537 entropy 1.2378510236740112
epoch: 4, step: 34
	action: tensor([[-1.0705, -1.0335, -0.3966,  0.7264, -0.8866,  0.9651, -1.2141]],
       dtype=torch.float64)
	q_value: tensor([[-36.5481]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.3085391226766379, distance: 1.7387026367262115 entropy 1.2378510236740112
epoch: 4, step: 35
	action: tensor([[ 0.3654,  0.8550,  0.3631,  0.2824,  1.1732, -1.2583,  0.5998]],
       dtype=torch.float64)
	q_value: tensor([[-51.0057]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0820007421205584 entropy 1.2378510236740112
epoch: 4, step: 36
	action: tensor([[ 1.4067,  0.4770,  0.2537, -0.7423,  0.8478, -0.8317,  0.1014]],
       dtype=torch.float64)
	q_value: tensor([[-32.7848]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.006446364941159 entropy 1.2378510236740112
epoch: 4, step: 37
	action: tensor([[ 1.2938,  0.1910, -0.6121, -0.4562,  1.5807,  0.2161,  0.1177]],
       dtype=torch.float64)
	q_value: tensor([[-39.9320]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9864000120459814 entropy 1.2378510236740112
epoch: 4, step: 38
	action: tensor([[ 0.0179, -0.7255,  0.7618,  0.3424,  0.2388, -1.0895,  1.0179]],
       dtype=torch.float64)
	q_value: tensor([[-40.1831]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.35404674761095256, distance: 1.3315991904245301 entropy 1.2378510236740112
epoch: 4, step: 39
	action: tensor([[ 1.2075, -1.3578, -2.5896, -0.3324,  2.0184,  1.2426, -0.4652]],
       dtype=torch.float64)
	q_value: tensor([[-40.5281]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.072693170370866 entropy 1.2378510236740112
epoch: 4, step: 40
	action: tensor([[ 1.1186, -0.6908,  0.5226, -1.0519, -0.2972, -0.2657, -0.7686]],
       dtype=torch.float64)
	q_value: tensor([[-63.5122]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.457782531409507, distance: 1.3816659794594153 entropy 1.2378510236740112
epoch: 4, step: 41
	action: tensor([[ 0.8823, -0.6810,  0.1015, -1.1649,  1.0254, -1.4134,  0.9704]],
       dtype=torch.float64)
	q_value: tensor([[-41.4206]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1344724727426243, distance: 1.2188595781933056 entropy 1.2378510236740112
epoch: 4, step: 42
	action: tensor([[-0.2697, -1.8473, -1.5809,  0.2781, -0.8559, -0.1023, -0.2206]],
       dtype=torch.float64)
	q_value: tensor([[-47.2986]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.6495590972079743 entropy 1.2378510236740112
epoch: 4, step: 43
	action: tensor([[-0.8819, -0.1703, -0.7275,  0.7355, -1.6434, -1.1929, -0.3069]],
       dtype=torch.float64)
	q_value: tensor([[-46.5240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.250386531531408, distance: 1.2796135693276296 entropy 1.2378510236740112
epoch: 4, step: 44
	action: tensor([[ 0.5823, -0.0845, -0.0565, -0.8918, -1.2762,  0.0603, -0.0347]],
       dtype=torch.float64)
	q_value: tensor([[-53.2272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14318002750638392, distance: 1.0592573445654605 entropy 1.2378510236740112
epoch: 4, step: 45
	action: tensor([[0.1661, 0.1256, 1.0260, 1.0195, 1.2905, 0.0801, 0.4949]],
       dtype=torch.float64)
	q_value: tensor([[-40.7550]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9551226713378382, distance: 0.24242097411698074 entropy 1.2378510236740112
epoch: 4, step: 46
	action: tensor([[-0.8864, -1.0625,  0.1328,  0.3542, -0.5738, -0.2140,  0.1091]],
       dtype=torch.float64)
	q_value: tensor([[-43.4317]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2141429019778553, distance: 1.7027838277357743 entropy 1.2378510236740112
epoch: 4, step: 47
	action: tensor([[ 0.4795,  0.4313, -0.4639,  0.8770, -0.6479,  1.1994,  0.1530]],
       dtype=torch.float64)
	q_value: tensor([[-41.2135]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0211406717840688 entropy 1.2378510236740112
epoch: 4, step: 48
	action: tensor([[ 1.7888, -0.6776,  0.6573, -0.3438, -0.4631, -0.4038, -0.3429]],
       dtype=torch.float64)
	q_value: tensor([[-35.6956]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.3548936740878212 entropy 1.2378510236740112
epoch: 4, step: 49
	action: tensor([[ 0.3666,  0.2136,  0.9877,  1.1122,  0.6961, -0.2870, -0.2391]],
       dtype=torch.float64)
	q_value: tensor([[-41.0059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.169745128822755 entropy 1.2378510236740112
epoch: 4, step: 50
	action: tensor([[ 0.0024,  0.5584,  0.2371, -1.9395, -1.3002,  0.5229, -0.0389]],
       dtype=torch.float64)
	q_value: tensor([[-32.2170]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.116935471033911, distance: 1.209402145992933 entropy 1.2378510236740112
epoch: 4, step: 51
	action: tensor([[ 0.7615, -0.8073,  1.5850,  0.4304,  0.2002,  0.8771,  2.1778]],
       dtype=torch.float64)
	q_value: tensor([[-48.0125]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.008581246899837414, distance: 1.139423724896481 entropy 1.2378510236740112
epoch: 4, step: 52
	action: tensor([[ 0.7059,  0.0496, -0.2544,  0.5255, -0.1567, -0.1281, -0.6993]],
       dtype=torch.float64)
	q_value: tensor([[-50.3493]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9273820484447273, distance: 0.30837466467122154 entropy 1.2378510236740112
epoch: 4, step: 53
	action: tensor([[ 0.6774, -0.0790, -0.5058,  0.0316,  0.8412,  0.3625, -0.4441]],
       dtype=torch.float64)
	q_value: tensor([[-34.1548]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7753453237086225, distance: 0.542393435633926 entropy 1.2378510236740112
epoch: 4, step: 54
	action: tensor([[-9.5632e-01, -4.0279e-01,  3.7691e-01,  6.8049e-04, -1.1104e+00,
         -4.1826e-01, -1.0746e+00]], dtype=torch.float64)
	q_value: tensor([[-34.8648]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.083946683431475, distance: 1.6519617794705186 entropy 1.2378510236740112
epoch: 4, step: 55
	action: tensor([[ 0.6817,  0.1659,  0.6664, -1.7688,  1.8495,  0.7786,  0.8372]],
       dtype=torch.float64)
	q_value: tensor([[-45.9722]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06907616177046905, distance: 1.1041136244830787 entropy 1.2378510236740112
epoch: 4, step: 56
	action: tensor([[-0.0072, -1.0028,  0.1496,  1.5381,  0.2754,  0.9441, -0.3097]],
       dtype=torch.float64)
	q_value: tensor([[-50.5120]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.751516173845143, distance: 0.5704344635657245 entropy 1.2378510236740112
epoch: 4, step: 57
	action: tensor([[ 1.0966, -0.0878,  1.2392, -0.2064,  0.1537, -0.0518,  0.2625]],
       dtype=torch.float64)
	q_value: tensor([[-43.6383]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6518329242671963, distance: 0.6752281556499511 entropy 1.2378510236740112
epoch: 4, step: 58
	action: tensor([[-1.3879,  0.9698,  0.5628,  0.6250,  0.9678,  0.0411,  2.0226]],
       dtype=torch.float64)
	q_value: tensor([[-38.1818]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.589750447705479 entropy 1.2378510236740112
epoch: 4, step: 59
	action: tensor([[ 0.6246, -0.3679, -0.0631,  0.5848,  0.1005,  0.2134,  0.0230]],
       dtype=torch.float64)
	q_value: tensor([[-37.0509]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7780537969668253, distance: 0.5391139294270221 entropy 1.2378510236740112
epoch: 4, step: 60
	action: tensor([[ 0.0539, -0.0255,  0.3832, -0.8028, -0.4379,  0.0088,  0.3120]],
       dtype=torch.float64)
	q_value: tensor([[-33.4155]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15728622833597883, distance: 1.2310539495180406 entropy 1.2378510236740112
epoch: 4, step: 61
	action: tensor([[-1.1874,  0.0989, -0.3050, -0.5513,  0.5852, -0.6890,  1.9998]],
       dtype=torch.float64)
	q_value: tensor([[-35.5440]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.154454619072371, distance: 1.6796754265762264 entropy 1.2378510236740112
epoch: 4, step: 62
	action: tensor([[ 0.0463,  1.7738, -0.1941, -0.4476,  0.0532, -0.1160, -0.7672]],
       dtype=torch.float64)
	q_value: tensor([[-49.1459]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.253296071066444 entropy 1.2378510236740112
epoch: 4, step: 63
	action: tensor([[ 0.5864, -0.3936, -0.8373, -1.4668,  1.3066, -0.7355, -1.0447]],
       dtype=torch.float64)
	q_value: tensor([[-38.8787]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16671408618097328, distance: 1.236058182525339 entropy 1.2378510236740112
epoch: 4, step: 64
	action: tensor([[ 0.1737, -1.6484, -0.3720,  0.1034,  0.5403, -0.2245,  1.3673]],
       dtype=torch.float64)
	q_value: tensor([[-49.2234]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.5353522648181193 entropy 1.2378510236740112
epoch: 4, step: 65
	action: tensor([[-0.0410,  0.8129,  0.3076,  0.6811,  0.2294,  0.7910,  0.0592]],
       dtype=torch.float64)
	q_value: tensor([[-42.6374]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9381378803472102 entropy 1.2378510236740112
epoch: 4, step: 66
	action: tensor([[ 1.2807, -0.6503,  1.1883,  0.2471,  0.9263,  1.2010,  2.0827]],
       dtype=torch.float64)
	q_value: tensor([[-31.2273]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17592678507094628, distance: 1.2409287162385418 entropy 1.2378510236740112
epoch: 4, step: 67
	action: tensor([[ 1.3010,  0.6333,  0.4595, -0.9351,  0.1803, -0.8055, -1.0803]],
       dtype=torch.float64)
	q_value: tensor([[-50.8274]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7148713749385562, distance: 0.611050499140206 entropy 1.2378510236740112
epoch: 4, step: 68
	action: tensor([[ 0.6775, -0.9002,  0.4947,  0.1568,  0.3131,  0.1901,  0.6970]],
       dtype=torch.float64)
	q_value: tensor([[-42.8898]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.049998027831914316, distance: 1.1726028217793814 entropy 1.2378510236740112
epoch: 4, step: 69
	action: tensor([[ 0.3372, -1.6367, -0.9156, -1.8540,  1.3570, -1.0609, -1.0186]],
       dtype=torch.float64)
	q_value: tensor([[-37.2345]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.2983938989746582 entropy 1.2378510236740112
epoch: 4, step: 70
	action: tensor([[-0.3457, -1.8808,  0.5558, -0.0517,  0.2960,  0.4773, -0.8301]],
       dtype=torch.float64)
	q_value: tensor([[-56.8652]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.539345127756433 entropy 1.2378510236740112
epoch: 4, step: 71
	action: tensor([[ 0.0862,  1.0449,  0.5545, -0.5530, -0.2290,  0.0871, -0.2083]],
       dtype=torch.float64)
	q_value: tensor([[-41.6844]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.091524669110386 entropy 1.2378510236740112
epoch: 4, step: 72
	action: tensor([[ 0.2124, -1.2987,  0.9103,  0.1168,  0.0081,  1.0624, -0.0038]],
       dtype=torch.float64)
	q_value: tensor([[-37.8906]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.1935107827130556 entropy 1.2378510236740112
epoch: 4, step: 73
	action: tensor([[-0.2870, -0.1981,  0.4699,  0.8278, -0.7552, -0.8197,  0.2441]],
       dtype=torch.float64)
	q_value: tensor([[-41.7947]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33475047357145793, distance: 0.9333595603761751 entropy 1.2378510236740112
epoch: 4, step: 74
	action: tensor([[-0.2587, -0.7791, -0.2655, -1.4559,  1.8992,  0.5713,  0.3994]],
       dtype=torch.float64)
	q_value: tensor([[-41.9870]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.19089512857296098, distance: 1.2488016177214611 entropy 1.2378510236740112
epoch: 4, step: 75
	action: tensor([[ 0.4497, -0.2344, -0.6150,  0.1056, -0.5076,  1.8231,  0.0905]],
       dtype=torch.float64)
	q_value: tensor([[-50.6697]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5786392952984446, distance: 0.742820206544816 entropy 1.2378510236740112
epoch: 4, step: 76
	action: tensor([[-0.2317,  0.7372, -1.0898, -1.1446, -0.1139, -0.3151,  0.6458]],
       dtype=torch.float64)
	q_value: tensor([[-43.9590]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.5672312336817693 entropy 1.2378510236740112
epoch: 4, step: 77
	action: tensor([[-0.6646, -0.3637,  1.7846,  1.0820, -0.6454, -0.1341,  1.1362]],
       dtype=torch.float64)
	q_value: tensor([[-40.6030]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.010703312408330334, distance: 1.1382036415459995 entropy 1.2378510236740112
epoch: 4, step: 78
	action: tensor([[-0.1750, -1.9061,  0.2592,  0.1314,  0.0676,  0.2134, -1.1543]],
       dtype=torch.float64)
	q_value: tensor([[-48.5138]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4895966077292453 entropy 1.2378510236740112
epoch: 4, step: 79
	action: tensor([[ 0.2037, -1.2515,  0.7880, -0.3703,  0.6205,  0.2447,  0.2710]],
       dtype=torch.float64)
	q_value: tensor([[-41.8249]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.5835537451162054 entropy 1.2378510236740112
epoch: 4, step: 80
	action: tensor([[-0.8479,  1.2217,  0.6855, -1.1283, -0.1505,  1.3058,  0.5523]],
       dtype=torch.float64)
	q_value: tensor([[-39.4295]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0830443526151543, distance: 1.1909123920401157 entropy 1.2378510236740112
epoch: 4, step: 81
	action: tensor([[-0.3569,  0.0069,  0.9037, -0.2007, -1.0933,  0.7559, -0.6221]],
       dtype=torch.float64)
	q_value: tensor([[-50.8287]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06579958256810081, distance: 1.181393198389611 entropy 1.2378510236740112
epoch: 4, step: 82
	action: tensor([[ 1.4544,  0.6786, -0.0252,  0.7736, -0.2307, -2.0436,  1.7525]],
       dtype=torch.float64)
	q_value: tensor([[-42.5788]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7693193180966075, distance: 0.5496197200027315 entropy 1.2378510236740112
epoch: 4, step: 83
	action: tensor([[ 0.8175, -0.8062,  0.8036,  0.1766,  1.1473,  0.1083, -0.9034]],
       dtype=torch.float64)
	q_value: tensor([[-55.4394]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.004430612737543083, distance: 1.1418063666664202 entropy 1.2378510236740112
epoch: 4, step: 84
	action: tensor([[ 1.8778,  1.2586,  0.0534,  0.0921,  1.7278, -0.1589,  1.7185]],
       dtype=torch.float64)
	q_value: tensor([[-43.2014]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.5860735562906604 entropy 1.2378510236740112
epoch: 4, step: 85
	action: tensor([[-0.3953, -0.2344, -0.8170,  0.8744,  1.0284,  0.3064, -1.6297]],
       dtype=torch.float64)
	q_value: tensor([[-53.0895]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.22589482117860182, distance: 1.2670194986233307 entropy 1.2378510236740112
epoch: 4, step: 86
	action: tensor([[ 0.0674, -0.2311,  1.7998,  0.8431,  1.3828, -0.1374,  0.5796]],
       dtype=torch.float64)
	q_value: tensor([[-43.9540]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0425666766833535 entropy 1.2378510236740112
epoch: 4, step: 87
	action: tensor([[-0.8522,  0.1932,  0.4461,  1.4654, -0.1218,  0.7062,  1.1042]],
       dtype=torch.float64)
	q_value: tensor([[-41.1797]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.6905641852415509 entropy 1.2378510236740112
epoch: 4, step: 88
	action: tensor([[ 0.2174,  1.5915,  0.3816, -0.7968,  0.2524,  0.0068, -0.4967]],
       dtype=torch.float64)
	q_value: tensor([[-44.9799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9994252656451721 entropy 1.2378510236740112
epoch: 4, step: 89
	action: tensor([[-0.6198, -0.9252,  0.3714, -0.0531,  1.2469,  0.2835,  1.1039]],
       dtype=torch.float64)
	q_value: tensor([[-40.7901]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.907540760377431, distance: 1.5804967482187398 entropy 1.2378510236740112
epoch: 4, step: 90
	action: tensor([[ 0.4563, -0.8714, -0.0209,  0.8649,  0.3818, -0.4880, -0.0742]],
       dtype=torch.float64)
	q_value: tensor([[-41.9299]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2391371231717201, distance: 0.9981823601839446 entropy 1.2378510236740112
epoch: 4, step: 91
	action: tensor([[ 1.1511,  0.6047,  0.7914,  0.3602,  0.4339, -0.7468,  0.1548]],
       dtype=torch.float64)
	q_value: tensor([[-38.1444]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9603633029309089, distance: 0.22782712095396532 entropy 1.2378510236740112
epoch: 4, step: 92
	action: tensor([[ 2.1343, -0.4618,  0.5419,  0.5272, -0.7479, -0.0339,  0.4016]],
       dtype=torch.float64)
	q_value: tensor([[-39.7162]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.078231360919917 entropy 1.2378510236740112
epoch: 4, step: 93
	action: tensor([[ 1.4875, -0.8189,  0.4554, -1.1016, -0.2997, -0.6924, -0.3440]],
       dtype=torch.float64)
	q_value: tensor([[-42.0112]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4814973267961291, distance: 1.3928589185895486 entropy 1.2378510236740112
epoch: 4, step: 94
	action: tensor([[ 0.8505, -1.7616,  0.5737,  0.4907,  0.7731, -1.1527,  0.4024]],
       dtype=torch.float64)
	q_value: tensor([[-44.4364]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.292207534766581 entropy 1.2378510236740112
epoch: 4, step: 95
	action: tensor([[-0.6656,  0.2261,  0.8954,  0.0134, -0.0957,  0.0975, -0.2867]],
       dtype=torch.float64)
	q_value: tensor([[-43.7949]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3417235858806884, distance: 1.3255259072200127 entropy 1.2378510236740112
epoch: 4, step: 96
	action: tensor([[ 0.3783, -1.3362, -0.3346, -1.3005,  0.0869, -0.6374,  0.8091]],
       dtype=torch.float64)
	q_value: tensor([[-37.6702]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.4442659569535135 entropy 1.2378510236740112
epoch: 4, step: 97
	action: tensor([[-0.4363, -0.3480, -0.4094, -1.0669,  0.7162, -0.4494, -0.2616]],
       dtype=torch.float64)
	q_value: tensor([[-45.5266]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4283689389286689, distance: 1.367656053337931 entropy 1.2378510236740112
epoch: 4, step: 98
	action: tensor([[ 0.9350, -0.1749,  0.1405, -0.2705,  0.5264, -0.4253, -0.8648]],
       dtype=torch.float64)
	q_value: tensor([[-39.3281]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35103987346112564, distance: 0.9218615473941656 entropy 1.2378510236740112
epoch: 4, step: 99
	action: tensor([[ 1.1017, -0.0418, -0.8498, -0.1387,  0.0931, -0.9719,  0.5503]],
       dtype=torch.float64)
	q_value: tensor([[-37.0991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3993986902873905, distance: 0.8868493064554337 entropy 1.2378510236740112
epoch: 4, step: 100
	action: tensor([[ 0.2991, -0.2854,  0.5251, -0.0597, -1.0383, -0.4666,  0.4500]],
       dtype=torch.float64)
	q_value: tensor([[-40.4692]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.228985322058183, distance: 1.004819411170863 entropy 1.2378510236740112
epoch: 4, step: 101
	action: tensor([[-0.7942,  0.1599, -0.5740,  0.0501, -0.2985,  0.4871, -0.4658]],
       dtype=torch.float64)
	q_value: tensor([[-39.0430]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5535977078133019, distance: 1.4263495819687335 entropy 1.2378510236740112
epoch: 4, step: 102
	action: tensor([[-0.0325, -0.0593,  0.6545,  0.9631, -0.0167, -0.1794, -0.3254]],
       dtype=torch.float64)
	q_value: tensor([[-34.4334]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0859918746249428 entropy 1.2378510236740112
epoch: 4, step: 103
	action: tensor([[-0.4154, -0.1488,  1.0275, -0.6766, -0.7427, -1.1315, -1.3122]],
       dtype=torch.float64)
	q_value: tensor([[-33.7924]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7354075502236597, distance: 1.5075003948725902 entropy 1.2378510236740112
epoch: 4, step: 104
	action: tensor([[ 1.4279, -0.0118,  1.3060, -1.0565, -0.2910, -1.2382,  0.7814]],
       dtype=torch.float64)
	q_value: tensor([[-47.4637]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.538150656859945, distance: 0.7776906157134462 entropy 1.2378510236740112
epoch: 4, step: 105
	action: tensor([[ 0.5111,  0.2529, -0.1988,  0.9207,  0.3107,  0.7024,  0.4487]],
       dtype=torch.float64)
	q_value: tensor([[-47.6012]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.16426753968473434 entropy 1.2378510236740112
epoch: 4, step: 106
	action: tensor([[-1.0442, -0.4228,  0.0769, -0.7553,  0.5656,  0.1471,  1.2623]],
       dtype=torch.float64)
	q_value: tensor([[-34.7179]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2777906061840083, distance: 1.727084521865741 entropy 1.2378510236740112
epoch: 4, step: 107
	action: tensor([[-2.1780,  0.0588,  0.0306,  1.4032, -0.4719,  1.1633,  1.0572]],
       dtype=torch.float64)
	q_value: tensor([[-41.8378]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0894042100441026 entropy 1.2378510236740112
epoch: 4, step: 108
	action: tensor([[ 1.6143,  0.9265,  0.4744, -0.6507, -0.1480, -0.1980, -1.1627]],
       dtype=torch.float64)
	q_value: tensor([[-51.4519]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0225858817197084 entropy 1.2378510236740112
epoch: 4, step: 109
	action: tensor([[-0.0017, -0.2880,  0.0303,  0.6263, -0.2980,  0.4383, -0.2702]],
       dtype=torch.float64)
	q_value: tensor([[-40.0270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.46086880156220533, distance: 0.8402411084916966 entropy 1.2378510236740112
epoch: 4, step: 110
	action: tensor([[ 0.2301,  0.1250,  0.5741, -0.8459, -0.8766, -0.4362,  1.0110]],
       dtype=torch.float64)
	q_value: tensor([[-34.5638]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03174305688530876, distance: 1.1260352943341783 entropy 1.2378510236740112
epoch: 4, step: 111
	action: tensor([[-0.3983, -0.2370,  0.6267,  0.0973,  0.5231,  0.1937,  0.2374]],
       dtype=torch.float64)
	q_value: tensor([[-42.0595]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1904081908692925, distance: 1.2485462842543986 entropy 1.2378510236740112
epoch: 4, step: 112
	action: tensor([[-1.1352,  0.1303,  1.1081, -1.8103,  0.9470,  0.0770, -0.7857]],
       dtype=torch.float64)
	q_value: tensor([[-34.2919]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0638972675595668, distance: 1.6439959045384465 entropy 1.2378510236740112
epoch: 4, step: 113
	action: tensor([[-0.1600, -0.0763,  1.1308,  1.2438, -0.9127, -1.6722,  0.4119]],
       dtype=torch.float64)
	q_value: tensor([[-48.7262]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.47777881421955404, distance: 0.826958920893558 entropy 1.2378510236740112
epoch: 4, step: 114
	action: tensor([[ 0.3615, -0.8673,  0.6331, -1.4820,  0.3802,  0.7207, -1.6134]],
       dtype=torch.float64)
	q_value: tensor([[-53.5574]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7559663126250877, distance: 1.5164035177424422 entropy 1.2378510236740112
epoch: 4, step: 115
	action: tensor([[-0.2238, -0.1592, -0.0585, -0.5215,  0.9669,  0.0801, -0.5178]],
       dtype=torch.float64)
	q_value: tensor([[-48.4890]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.266649506097145, distance: 1.2879082415293321 entropy 1.2378510236740112
epoch: 4, step: 116
	action: tensor([[ 0.8626, -0.1073,  0.9698, -0.8554,  0.1218, -1.9181, -0.1301]],
       dtype=torch.float64)
	q_value: tensor([[-36.2719]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3972867460704349, distance: 0.8884071890282141 entropy 1.2378510236740112
epoch: 4, step: 117
	action: tensor([[-1.1915, -0.7976,  1.7734, -1.8244, -1.6556,  0.9578, -0.9135]],
       dtype=torch.float64)
	q_value: tensor([[-47.4201]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.694733600637002, distance: 1.489729484393013 entropy 1.2378510236740112
epoch: 4, step: 118
	action: tensor([[ 0.5305,  0.2660,  0.2352, -0.3934,  0.6601, -0.0205,  0.5221]],
       dtype=torch.float64)
	q_value: tensor([[-61.7942]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.691457876319975, distance: 0.6356439612851847 entropy 1.2378510236740112
epoch: 4, step: 119
	action: tensor([[ 0.7711, -1.0499, -0.4885, -1.6641,  0.7982, -0.0353, -0.3588]],
       dtype=torch.float64)
	q_value: tensor([[-34.3211]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6750107158100827, distance: 1.4810355668056232 entropy 1.2378510236740112
epoch: 4, step: 120
	action: tensor([[ 0.2681,  0.3227, -0.4464,  1.1076,  1.8263, -1.3136,  1.0686]],
       dtype=torch.float64)
	q_value: tensor([[-47.8177]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8803565247695775 entropy 1.2378510236740112
epoch: 4, step: 121
	action: tensor([[ 0.1494,  0.2597,  0.2513,  0.5732, -1.7368, -1.4579,  0.4110]],
       dtype=torch.float64)
	q_value: tensor([[-41.4667]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.9523960508392816 entropy 1.2378510236740112
epoch: 4, step: 122
	action: tensor([[0.4446, 0.2627, 1.6218, 0.2745, 0.1892, 0.8224, 0.7737]],
       dtype=torch.float64)
	q_value: tensor([[-42.5850]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0217784510215566 entropy 1.2378510236740112
epoch: 4, step: 123
	action: tensor([[ 1.8367,  0.1067,  0.6708, -0.5492,  0.5355, -0.0570,  0.7131]],
       dtype=torch.float64)
	q_value: tensor([[-46.8317]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 1.0343355127817317 entropy 1.2378510236740112
epoch: 4, step: 124
	action: tensor([[-1.0065,  0.5845,  0.0516, -0.8372,  1.7980, -1.1974,  0.9854]],
       dtype=torch.float64)
	q_value: tensor([[-39.4781]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8084196609522829, distance: 1.5388854847451858 entropy 1.2378510236740112
epoch: 4, step: 125
	action: tensor([[-0.6053, -0.2947, -0.6359, -0.0788,  0.7476, -0.2358, -0.7888]],
       dtype=torch.float64)
	q_value: tensor([[-49.4796]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7120635383721434, distance: 1.4973269189109892 entropy 1.2378510236740112
epoch: 4, step: 126
	action: tensor([[ 0.7631, -0.9842,  1.3332, -0.7385,  0.6064, -0.4776,  0.5984]],
       dtype=torch.float64)
	q_value: tensor([[-36.2686]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2577697512887207, distance: 1.2833859078979355 entropy 1.2378510236740112
epoch: 4, step: 127
	action: tensor([[ 0.0626,  2.5160, -0.7736, -0.1141,  0.9546,  0.5775,  0.0790]],
       dtype=torch.float64)
	q_value: tensor([[-41.3987]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0.8738502929494797 entropy 1.2378510236740112
LOSS epoch 4 actor 453.86783438969513 critic 73.7678968183008 
