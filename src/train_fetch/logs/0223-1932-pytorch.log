epoch: 0, step: 0
	action: tensor([[-50.8224, -14.5024, -18.0700,   9.9529, -26.9531, -27.7413,   7.5502]],
       dtype=torch.float64)
	q_value: tensor([[-35.8682]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.374130671920644
epoch: 0, step: 1
	action: tensor([[ -3.1067,  -7.7342,   5.8081,  12.0927,  19.7273, -30.1874,  29.8333]],
       dtype=torch.float64)
	q_value: tensor([[-35.9253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.376282187494627
epoch: 0, step: 2
	action: tensor([[-67.7153,  27.9278,  -5.7793,   7.5833,  -4.7876,  13.5356,  17.8729]],
       dtype=torch.float64)
	q_value: tensor([[-35.9253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.376282187494627
epoch: 0, step: 3
	action: tensor([[-14.9363,  -4.4554, -20.7160, -10.7900, -38.7282,  19.8832,  -6.6329]],
       dtype=torch.float64)
	q_value: tensor([[-35.9253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.376282187494627
epoch: 0, step: 4
	action: tensor([[-35.5226, -41.7555,   3.3548,   0.2988,  -9.1254,  21.6317,   4.0414]],
       dtype=torch.float64)
	q_value: tensor([[-35.9253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0747521105238147, distance: 1.1863445694130006 entropy 4.376282187494627
epoch: 0, step: 5
	action: tensor([[-101.8573,  -17.2185,   26.6152,    4.6373,   -1.9059,   17.2544,
            1.2481]], dtype=torch.float64)
	q_value: tensor([[-52.9825]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.9534733181031205
epoch: 0, step: 6
	action: tensor([[-26.0207, -50.9173,  13.9135, -22.6896,  -0.4625,   2.3673,  14.6962]],
       dtype=torch.float64)
	q_value: tensor([[-35.9253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.49691544903226803, distance: 1.4000879846199217 entropy 4.376282187494627
epoch: 0, step: 7
	action: tensor([[-21.1583,   2.4472, -30.3585,  35.6309,  -0.8393, -12.0468,  18.5733]],
       dtype=torch.float64)
	q_value: tensor([[-32.1249]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.515852900732821
epoch: 0, step: 8
	action: tensor([[-29.4782, -25.5969,   0.1314,   0.9454,  -4.8299,  20.2223,  11.5706]],
       dtype=torch.float64)
	q_value: tensor([[-35.9253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.33152697592154134, distance: 1.3204795445517363 entropy 4.376282187494627
epoch: 0, step: 9
	action: tensor([[-24.0956,   1.3274,  27.1730,  13.2327,   0.6535,  11.2423,  15.0932]],
       dtype=torch.float64)
	q_value: tensor([[-23.5583]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.3558428033736725
epoch: 0, step: 10
	action: tensor([[-59.3402, -19.7893, -29.9008,  -0.8203, -18.8868,  24.3053,  55.0579]],
       dtype=torch.float64)
	q_value: tensor([[-35.9253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.372035846619311, distance: 1.7624520783227902 entropy 4.376282187494627
epoch: 0, step: 11
	action: tensor([[-22.2218, -65.5279,   8.1028,  28.3293, -21.7269,  18.5212,  -9.3601]],
       dtype=torch.float64)
	q_value: tensor([[-29.4913]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.3726912326570302, distance: 1.7626955414876193 entropy 4.438036533020435
epoch: 0, step: 12
	action: tensor([[-21.5774, -36.0739,  15.4004,  59.8828,  11.1929,  44.1891,   8.0747]],
       dtype=torch.float64)
	q_value: tensor([[-46.1752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21262505506799556, distance: 1.0154241515741886 entropy 5.063384917264777
epoch: 0, step: 13
	action: tensor([[-40.8085, -48.3772, -29.1465, -53.4443, -38.7828,  21.9295,  54.5406]],
       dtype=torch.float64)
	q_value: tensor([[-36.7699]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03183737593838121, distance: 1.1259804487841432 entropy 4.9056781469839885
epoch: 0, step: 14
	action: tensor([[-20.3612,  43.0874,  -9.5852,  -9.5806, -57.7637,  31.5510, -26.7705]],
       dtype=torch.float64)
	q_value: tensor([[-40.2230]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2196164132395959, distance: 1.010905958968608 entropy 5.008932377415776
epoch: 0, step: 15
	action: tensor([[-52.6204, -62.6726,   1.9257,  32.7041,  36.9306,  10.7014, -17.3131]],
       dtype=torch.float64)
	q_value: tensor([[-33.5328]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.801013788076836
epoch: 0, step: 16
	action: tensor([[-47.6147,   7.7050,  13.4973,  18.6195, -20.8061,  11.7851,   7.8465]],
       dtype=torch.float64)
	q_value: tensor([[-35.9253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1276462075478646, distance: 1.215187028736462 entropy 4.376282187494627
epoch: 0, step: 17
	action: tensor([[-31.6941, -21.0592, -23.9261,  14.7393,  28.7662, -16.5204,  14.0517]],
       dtype=torch.float64)
	q_value: tensor([[-39.8772]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.558058510666583
epoch: 0, step: 18
	action: tensor([[-21.2213, -25.6075,   0.9987,  -1.3022, -10.5081,  -8.7655,   2.6196]],
       dtype=torch.float64)
	q_value: tensor([[-35.9253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.376282187494627
epoch: 0, step: 19
	action: tensor([[-31.2760,  10.8988,  -0.3852,   1.8250,  -8.0975,  -5.3895,  10.2001]],
       dtype=torch.float64)
	q_value: tensor([[-35.9253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.376282187494627
epoch: 0, step: 20
	action: tensor([[-23.7807, -26.6851,   2.5995,  24.6242,  -9.9582, -20.0902, -20.1490]],
       dtype=torch.float64)
	q_value: tensor([[-35.9253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7498573134553042, distance: 1.5137634389123529 entropy 4.376282187494627
epoch: 0, step: 21
	action: tensor([[-56.5745, -38.5689,  -7.9647,   3.1676, -30.3441, -12.7390,  14.1105]],
       dtype=torch.float64)
	q_value: tensor([[-33.6106]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.622167985621718
epoch: 0, step: 22
	action: tensor([[-74.8992, -26.9233, -45.2097, -18.8003,  18.8705,  30.9619,  24.5308]],
       dtype=torch.float64)
	q_value: tensor([[-35.9253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7281710335162708, distance: 1.5043540307682413 entropy 4.376282187494627
epoch: 0, step: 23
	action: tensor([[-26.7057, -27.6368, -24.6110, -11.1374, -12.7949,  12.4595,  -5.4281]],
       dtype=torch.float64)
	q_value: tensor([[-31.4779]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.431073313096669
epoch: 0, step: 24
	action: tensor([[-40.7080, -12.1318,  -4.2496,  -8.3813, -11.2712,  17.8164, -23.9879]],
       dtype=torch.float64)
	q_value: tensor([[-35.9253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18065437099095338, distance: 1.2434206709437732 entropy 4.376282187494627
epoch: 0, step: 25
	action: tensor([[-42.2295, -42.6573,  19.2839,  28.9033,  29.5297,  24.6332,  10.9146]],
       dtype=torch.float64)
	q_value: tensor([[-33.0317]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.40227429566945494, distance: 1.355105724499341 entropy 4.63799138293713
epoch: 0, step: 26
	action: tensor([[-31.1703, -19.2972, -11.8881,   9.7014, -24.0799, -25.5088,   2.8266]],
       dtype=torch.float64)
	q_value: tensor([[-44.3759]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9188502026063823, distance: 1.585175054867149 entropy 4.641262188069724
epoch: 0, step: 27
	action: tensor([[-33.6333,  -6.5302, -25.1018, -60.8576, -25.9695,  20.3994, -11.4233]],
       dtype=torch.float64)
	q_value: tensor([[-29.9802]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5245990380880883, distance: 1.4129751174845986 entropy 4.67772270341906
epoch: 0, step: 28
	action: tensor([[-17.7870,  -1.6430,   3.4163,   6.3676,  18.4038, -53.3018, -34.3674]],
       dtype=torch.float64)
	q_value: tensor([[-33.5934]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.60555412377134
epoch: 0, step: 29
	action: tensor([[-47.1245, -20.7543,   4.6419,  58.7700,  21.1439,   7.4464,   3.8285]],
       dtype=torch.float64)
	q_value: tensor([[-35.9253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.520603511341482, distance: 1.816807699520903 entropy 4.376282187494627
epoch: 0, step: 30
	action: tensor([[  2.3971, -52.4410,   4.6876,   5.1095, -54.8800,   9.7154,   4.0571]],
       dtype=torch.float64)
	q_value: tensor([[-32.4202]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.736520520577727
epoch: 0, step: 31
	action: tensor([[-29.6772, -11.2077, -16.7665,  12.2495, -17.8289,  34.6010,  18.0120]],
       dtype=torch.float64)
	q_value: tensor([[-35.9253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.376282187494627
epoch: 0, step: 32
	action: tensor([[ -5.1941, -22.7506, -20.8949, -11.2237,  -9.0173,  -1.9430,  33.5576]],
       dtype=torch.float64)
	q_value: tensor([[-35.9253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.376282187494627
epoch: 0, step: 33
	action: tensor([[ -2.9248, -12.5894,   6.1691,  20.1694,  -8.1302,  -1.5601,   9.8310]],
       dtype=torch.float64)
	q_value: tensor([[-35.9253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.376282187494627
epoch: 0, step: 34
	action: tensor([[-33.5972, -39.5794,   3.9464,  -4.3535,  13.7789, -25.2451,  -6.4190]],
       dtype=torch.float64)
	q_value: tensor([[-35.9253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.376282187494627
epoch: 0, step: 35
	action: tensor([[-31.1475,   3.3205,  -4.8374,  12.5678, -14.5881,   9.0081,  36.6926]],
       dtype=torch.float64)
	q_value: tensor([[-35.9253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.376282187494627
epoch: 0, step: 36
	action: tensor([[-30.6532, -17.6366,  -2.8757,   4.9224, -35.9690, -45.8677,  -5.0709]],
       dtype=torch.float64)
	q_value: tensor([[-35.9253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.376282187494627
epoch: 0, step: 37
	action: tensor([[ -4.7469, -30.6048,  14.6828, -20.7354,   2.7367,  24.4549,  14.2642]],
       dtype=torch.float64)
	q_value: tensor([[-35.9253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.376282187494627
epoch: 0, step: 38
	action: tensor([[-41.2691, -41.2851,  23.3210, -28.7198, -10.9518,   2.2055,  15.6492]],
       dtype=torch.float64)
	q_value: tensor([[-35.9253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.376282187494627
epoch: 0, step: 39
	action: tensor([[-34.0489, -32.3599,  10.1347,  -5.1193,   5.6430,   2.5799,  -8.3726]],
       dtype=torch.float64)
	q_value: tensor([[-35.9253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.376282187494627
epoch: 0, step: 40
	action: tensor([[-41.4793, -27.2290,  31.1957,  24.8907, -37.6501,  39.8917,  32.7681]],
       dtype=torch.float64)
	q_value: tensor([[-35.9253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.376282187494627
epoch: 0, step: 41
	action: tensor([[-70.0200, -19.3759, -43.7927,  -6.4173,  12.0422,   5.8265,  14.3434]],
       dtype=torch.float64)
	q_value: tensor([[-35.9253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.3671858295554125, distance: 1.7606493449140226 entropy 4.376282187494627
epoch: 0, step: 42
	action: tensor([[-33.4526, -10.4763,  14.4962,  -2.5323, -21.9923, -11.1584,  -0.6312]],
       dtype=torch.float64)
	q_value: tensor([[-18.5868]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.187600544081385
epoch: 0, step: 43
	action: tensor([[ -6.2886, -29.0317, -17.2111,  16.0435,  -7.5502,   9.1640,  13.0138]],
       dtype=torch.float64)
	q_value: tensor([[-35.9253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.376282187494627
epoch: 0, step: 44
	action: tensor([[-24.4934, -20.5265, -43.0325,  -7.2502,  -8.9914,  -1.1679, -25.1677]],
       dtype=torch.float64)
	q_value: tensor([[-35.9253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.376282187494627
epoch: 0, step: 45
	action: tensor([[ -6.2137, -33.6576, -26.4764, -17.9327,  44.7114, -12.9077,  -5.0574]],
       dtype=torch.float64)
	q_value: tensor([[-35.9253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.376282187494627
epoch: 0, step: 46
	action: tensor([[-27.2953,   5.9976, -29.6628,  30.4037,  -5.9857,  14.8599,  11.2361]],
       dtype=torch.float64)
	q_value: tensor([[-35.9253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.376282187494627
epoch: 0, step: 47
	action: tensor([[-28.8576, -25.1612,  34.7356,   5.5313,  -0.4967,   8.6819, -17.6794]],
       dtype=torch.float64)
	q_value: tensor([[-35.9253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0740291737329892, distance: 1.6480262455815256 entropy 4.376282187494627
epoch: 0, step: 48
	action: tensor([[-21.3153, -23.5983, -72.7907,  -4.6464,  13.3249,  -3.6836,  27.9188]],
       dtype=torch.float64)
	q_value: tensor([[-41.6263]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.703415847965628
epoch: 0, step: 49
	action: tensor([[-25.4878, -29.7765, -26.9765,  18.6990,  -4.0454,  -4.6133, -19.1625]],
       dtype=torch.float64)
	q_value: tensor([[-35.9253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.376282187494627
epoch: 0, step: 50
	action: tensor([[-15.3942, -19.9477,   1.6066,   3.6928,   3.5637,  17.5602,  17.6874]],
       dtype=torch.float64)
	q_value: tensor([[-35.9253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.376282187494627
epoch: 0, step: 51
	action: tensor([[-71.7579, -16.5687, -15.0405,   5.4308, -51.4788,  37.2213, -10.4732]],
       dtype=torch.float64)
	q_value: tensor([[-35.9253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.40092242530424493, distance: 1.354452368373076 entropy 4.376282187494627
epoch: 0, step: 52
	action: tensor([[-53.4293, -18.8505,   3.2012,   6.2411,  -6.2774,  29.4749,  17.9298]],
       dtype=torch.float64)
	q_value: tensor([[-44.1762]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.412867474198947, distance: 1.7775565575630887 entropy 4.695542335345881
epoch: 0, step: 53
	action: tensor([[-48.3567, -34.3966,   4.6412,  11.8177, -13.4655, -18.4229,  49.3048]],
       dtype=torch.float64)
	q_value: tensor([[-39.4857]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.529851655467065, distance: 1.8201375996376348 entropy 4.6730361930669115
epoch: 0, step: 54
	action: tensor([[-37.9762,  -5.5857,  26.8361,  18.2040,   5.6400, -25.8323,  48.7012]],
       dtype=torch.float64)
	q_value: tensor([[-31.9492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9754245379655271, distance: 1.6083735236542205 entropy 4.598009551844906
epoch: 0, step: 55
	action: tensor([[-10.8632, -43.3907,  10.1744,  55.9574,   1.2053,   5.3734,   7.7454]],
       dtype=torch.float64)
	q_value: tensor([[-31.8075]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.6148325517312605
epoch: 0, step: 56
	action: tensor([[-10.2696, -16.8823,  53.5550, -20.3642, -10.1269, -37.4139, -23.7118]],
       dtype=torch.float64)
	q_value: tensor([[-35.9253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.376282187494627
epoch: 0, step: 57
	action: tensor([[-15.1032,  17.7696,  -6.1288,  24.2376,   2.8985,  41.2303,  -9.6850]],
       dtype=torch.float64)
	q_value: tensor([[-35.9253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1048983010774331, distance: 1.6602452716974003 entropy 4.376282187494627
epoch: 0, step: 58
	action: tensor([[-39.5732,  11.4077, -34.6884,  -3.9380,  40.2977,  33.0423,  56.6483]],
       dtype=torch.float64)
	q_value: tensor([[-34.6651]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.690453355026881
epoch: 0, step: 59
	action: tensor([[-55.7532, -46.8024, -11.6799,   6.7683, -14.7171,  21.1911,  14.2987]],
       dtype=torch.float64)
	q_value: tensor([[-35.9253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5065986446720778, distance: 1.4046091056066334 entropy 4.376282187494627
epoch: 0, step: 60
	action: tensor([[ 15.7621, -15.6177,  15.3188,  31.1894,  -1.9659,  38.5568,  20.0215]],
       dtype=torch.float64)
	q_value: tensor([[-35.2634]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.620894447020441
epoch: 0, step: 61
	action: tensor([[-62.0730, -37.9537,   5.0813,  19.1778, -37.5803,  11.6186,  -7.5349]],
       dtype=torch.float64)
	q_value: tensor([[-35.9253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7264961731310666, distance: 1.5036248802144812 entropy 4.376282187494627
epoch: 0, step: 62
	action: tensor([[-32.2730, -10.3062,  16.4971,  -0.2117,   6.7145,  -1.2830,  31.0337]],
       dtype=torch.float64)
	q_value: tensor([[-19.1566]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.0584036994517545
epoch: 0, step: 63
	action: tensor([[-33.5630,  27.2020,  24.6851,  -0.5853, -23.6201, -34.9839,   9.1137]],
       dtype=torch.float64)
	q_value: tensor([[-35.9253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.376282187494627
epoch: 0, step: 64
	action: tensor([[-38.1301, -10.9625,   3.2083,  -6.5763,  -5.8264,  21.3881,  25.0372]],
       dtype=torch.float64)
	q_value: tensor([[-35.9253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.376282187494627
epoch: 0, step: 65
	action: tensor([[-10.4080, -27.9113, -30.7981,   2.7509,   9.0807, -22.0371,  29.2819]],
       dtype=torch.float64)
	q_value: tensor([[-35.9253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.376282187494627
epoch: 0, step: 66
	action: tensor([[-36.1610,  -6.8900, -23.3496, -23.5501,  11.9102, -22.0446,   6.6256]],
       dtype=torch.float64)
	q_value: tensor([[-35.9253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.376282187494627
epoch: 0, step: 67
	action: tensor([[  3.2145,  -1.3555, -13.6024, -10.3673,  -1.0431,  17.5613,   5.9335]],
       dtype=torch.float64)
	q_value: tensor([[-35.9253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.376282187494627
epoch: 0, step: 68
	action: tensor([[-50.4845, -28.1816, -23.9679,   0.6234,  14.0405,  17.6896,  31.9734]],
       dtype=torch.float64)
	q_value: tensor([[-35.9253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5950554234310939, distance: 1.4452553387214029 entropy 4.376282187494627
epoch: 0, step: 69
	action: tensor([[-28.4024, -40.8450,  -6.3790, -55.7199, -15.4315,  -1.5779,  22.6395]],
       dtype=torch.float64)
	q_value: tensor([[-20.7726]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7621701772813838, distance: 1.51907989811769 entropy 4.231164563392502
epoch: 0, step: 70
	action: tensor([[-69.4071, -56.0472, -54.5655, -49.7399, -31.8041,  49.3311,  10.5827]],
       dtype=torch.float64)
	q_value: tensor([[-39.4882]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2856862583741027, distance: 1.7300752844613225 entropy 4.979568605627977
epoch: 0, step: 71
	action: tensor([[-46.1564,  12.9017, -15.7369,   7.6682, -45.8593,  15.9038,   9.9279]],
       dtype=torch.float64)
	q_value: tensor([[-32.0493]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.619959468890777
epoch: 0, step: 72
	action: tensor([[-35.4285, -10.5109,   9.0746,  24.3222, -47.0835,  15.2000, -26.1572]],
       dtype=torch.float64)
	q_value: tensor([[-35.9253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2214266811111859, distance: 1.0097327711168518 entropy 4.376282187494627
epoch: 0, step: 73
	action: tensor([[ -7.4715, -18.1570,  77.9506,  25.5128,   3.4321, -13.4797,   3.9255]],
       dtype=torch.float64)
	q_value: tensor([[-48.5010]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4868605587434911, distance: 1.3953778150818004 entropy 4.8898898257796635
epoch: 0, step: 74
	action: tensor([[-39.6421, -14.1695,  34.2679,  13.6269,  19.3477, -16.8805,  63.6230]],
       dtype=torch.float64)
	q_value: tensor([[-46.5380]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.9079699621185116
epoch: 0, step: 75
	action: tensor([[-27.3052, -17.7170, -10.4503, -34.8461, -19.5788,   2.6761, -13.8136]],
       dtype=torch.float64)
	q_value: tensor([[-35.9253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.376282187494627
epoch: 0, step: 76
	action: tensor([[-52.1050,  -4.5693, -11.8509,   4.0318,  -7.6242,  16.4050, -22.6893]],
       dtype=torch.float64)
	q_value: tensor([[-35.9253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.376282187494627
epoch: 0, step: 77
	action: tensor([[-30.7630, -38.8512, -18.9200, -17.7451, -13.0429, -15.3590,  -4.2568]],
       dtype=torch.float64)
	q_value: tensor([[-35.9253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.376282187494627
epoch: 0, step: 78
	action: tensor([[-59.1703,   9.3639, -11.0570,  -0.9954, -14.5318,  38.7375, -22.8658]],
       dtype=torch.float64)
	q_value: tensor([[-35.9253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.376282187494627
epoch: 0, step: 79
	action: tensor([[-30.9865, -31.8792, -41.7617,   9.3797, -25.6409, -45.5603,   1.3551]],
       dtype=torch.float64)
	q_value: tensor([[-35.9253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.029896558333774026, distance: 1.1613242551035423 entropy 4.376282187494627
epoch: 0, step: 80
	action: tensor([[-39.0065, -19.6523,   8.4063,  14.1630, -37.9554,  27.1552,  13.8626]],
       dtype=torch.float64)
	q_value: tensor([[-30.1836]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.48899870477571716, distance: 0.8180271043472137 entropy 4.619076878402771
epoch: 0, step: 81
	action: tensor([[-19.6414,  13.6780, -12.8952, -28.5185, -24.9093, -23.5072,   7.8272]],
       dtype=torch.float64)
	q_value: tensor([[-27.4171]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2892041207602839, distance: 0.9647818927946258 entropy 4.57182928816303
epoch: 0, step: 82
	action: tensor([[-36.8981,  -9.2757, -18.4905,   9.6784,   0.1683,  23.3503,   3.5771]],
       dtype=torch.float64)
	q_value: tensor([[-20.2653]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.190812332880161
epoch: 0, step: 83
	action: tensor([[-38.4867, -48.1988,  -7.8941,  30.1745,  20.3320, -41.0041, -18.6239]],
       dtype=torch.float64)
	q_value: tensor([[-35.9253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3295710001832337, distance: 1.3195093144246657 entropy 4.376282187494627
epoch: 0, step: 84
	action: tensor([[-40.3197, -31.7705,  27.7359,  26.0929,   0.4998,  -1.3471,  34.4375]],
       dtype=torch.float64)
	q_value: tensor([[-33.0347]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2514810460263526, distance: 1.7170812302333784 entropy 4.492655834635579
epoch: 0, step: 85
	action: tensor([[-21.6120, -61.9514, -27.1079, -14.1258, -27.3968, -40.8001,   7.5438]],
       dtype=torch.float64)
	q_value: tensor([[-31.4818]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7763911515766182, distance: 1.5251971809445055 entropy 4.697031504975586
epoch: 0, step: 86
	action: tensor([[-36.8851, -67.0889,   5.4394,  34.2710, -31.0028,  85.3972,  58.3722]],
       dtype=torch.float64)
	q_value: tensor([[-44.2557]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.530612427251226, distance: 1.4157589338188912 entropy 4.8876756641380865
epoch: 0, step: 87
	action: tensor([[-46.9293,   1.8825,   3.8766,  10.8782, -54.6939,   9.9716, -16.9794]],
       dtype=torch.float64)
	q_value: tensor([[-26.4990]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.397608678881501
epoch: 0, step: 88
	action: tensor([[-36.2177,  -7.0975, -19.2540,   4.3411,   3.2964,  42.5369,   3.7452]],
       dtype=torch.float64)
	q_value: tensor([[-35.9253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.376282187494627
epoch: 0, step: 89
	action: tensor([[-35.8342, -29.9751, -22.9167, -14.2446, -11.9022,  22.9976,  -6.2965]],
       dtype=torch.float64)
	q_value: tensor([[-35.9253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04928767498726949, distance: 1.1722061054731956 entropy 4.376282187494627
epoch: 0, step: 90
	action: tensor([[-9.0668e+01, -5.9131e+01,  2.9199e+01, -1.4824e+01, -2.6767e+01,
          1.8839e+01, -4.3632e-02]], dtype=torch.float64)
	q_value: tensor([[-46.4386]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.7896388823604905
epoch: 0, step: 91
	action: tensor([[-45.3075,   1.4871, -14.1145, -25.6534, -42.5325,   9.8261, -26.0817]],
       dtype=torch.float64)
	q_value: tensor([[-35.9253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.376282187494627
epoch: 0, step: 92
	action: tensor([[-16.6111, -36.1933, -17.5459,  -6.3068, -47.7237,  -4.6959,  11.3220]],
       dtype=torch.float64)
	q_value: tensor([[-35.9253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.376282187494627
epoch: 0, step: 93
	action: tensor([[-18.2542, -16.0546,   6.1399,   2.2854, -19.3474,  15.9676,  14.8655]],
       dtype=torch.float64)
	q_value: tensor([[-35.9253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.376282187494627
epoch: 0, step: 94
	action: tensor([[-23.7512, -23.9123,  -6.3694,   2.5092,  -6.0169,   6.2871,  -0.4114]],
       dtype=torch.float64)
	q_value: tensor([[-35.9253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33992266779282576, distance: 0.9297241298692148 entropy 4.376282187494627
epoch: 0, step: 95
	action: tensor([[-16.1982, -18.7758,  15.4166,  30.2586,  -6.3044, -21.9890,   0.1790]],
       dtype=torch.float64)
	q_value: tensor([[-22.7439]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.39519815868077
epoch: 0, step: 96
	action: tensor([[-36.4780, -14.3202,  -6.6863,  -8.1142, -30.5048,  -9.2113,  39.3657]],
       dtype=torch.float64)
	q_value: tensor([[-35.9253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23524282740865887, distance: 1.2718411246172332 entropy 4.376282187494627
epoch: 0, step: 97
	action: tensor([[-35.1071,  -9.4311,  15.4453,  -5.2070, -34.6137, -18.6315,  17.9423]],
       dtype=torch.float64)
	q_value: tensor([[-33.9575]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.634566138374007
epoch: 0, step: 98
	action: tensor([[-41.2102, -38.0919,  12.4783, -15.9760, -30.1456,   3.1240,  22.2073]],
       dtype=torch.float64)
	q_value: tensor([[-35.9253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.376282187494627
epoch: 0, step: 99
	action: tensor([[-3.3953e+01, -1.0138e+01, -1.5428e+01,  2.9593e-02, -1.1748e+01,
          2.0392e+01,  1.7201e+01]], dtype=torch.float64)
	q_value: tensor([[-35.9253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.31636314912788044, distance: 1.3129390069321136 entropy 4.376282187494627
epoch: 0, step: 100
	action: tensor([[-48.9202,  -9.7371, -32.4563,  29.2381,   2.4126,  24.3148,  39.5489]],
       dtype=torch.float64)
	q_value: tensor([[-44.2606]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.761970817362575
epoch: 0, step: 101
	action: tensor([[-25.4859, -26.0843,  -6.9856,  -3.7914,   9.4039,  -9.1524, -12.0713]],
       dtype=torch.float64)
	q_value: tensor([[-35.9253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.376282187494627
epoch: 0, step: 102
	action: tensor([[-10.5567, -17.1268,  27.5612,  30.4388,  -9.1518,   0.2565,  -0.9801]],
       dtype=torch.float64)
	q_value: tensor([[-35.9253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.376282187494627
epoch: 0, step: 103
	action: tensor([[-14.9165,   4.4913,  47.6316,  12.7012, -25.6166,  30.7219,  10.5754]],
       dtype=torch.float64)
	q_value: tensor([[-35.9253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.376282187494627
epoch: 0, step: 104
	action: tensor([[-18.1080,  10.5258,  -8.3981,  26.5309,  -2.3687, -36.1985,  -5.4389]],
       dtype=torch.float64)
	q_value: tensor([[-35.9253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.376282187494627
epoch: 0, step: 105
	action: tensor([[ -5.7711, -25.4608,   3.3913, -15.0323, -20.3467,  -2.0560, -23.4866]],
       dtype=torch.float64)
	q_value: tensor([[-35.9253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3654836290447453, distance: 0.9115449913608915 entropy 4.376282187494627
epoch: 0, step: 106
	action: tensor([[-57.9580,  -7.0553,  55.8608,  21.0436, -17.8344, -21.1341,   0.9362]],
       dtype=torch.float64)
	q_value: tensor([[-44.1297]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9008383082096014, distance: 1.5777176397213173 entropy 4.9526499637579
epoch: 0, step: 107
	action: tensor([[-29.7816, -29.1919, -16.4020,   0.3419,  -7.6007,   6.3004, -22.5824]],
       dtype=torch.float64)
	q_value: tensor([[-29.5305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.587192704308044
epoch: 0, step: 108
	action: tensor([[-13.9252, -14.1718, -13.9799,   7.5309,   6.7940,  15.1630,  33.7725]],
       dtype=torch.float64)
	q_value: tensor([[-35.9253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4891600403961842, distance: 1.396456398427521 entropy 4.376282187494627
epoch: 0, step: 109
	action: tensor([[ -8.1258,  -2.9716,   2.2577,   3.7466,  10.1091,  10.1730, -19.1635]],
       dtype=torch.float64)
	q_value: tensor([[-27.0603]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.430328774660619
epoch: 0, step: 110
	action: tensor([[  2.2882, -19.0581, -11.5290,  15.0731, -37.1291, -43.2793,   7.6793]],
       dtype=torch.float64)
	q_value: tensor([[-35.9253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.376282187494627
epoch: 0, step: 111
	action: tensor([[-30.0992, -24.6554,   4.0530, -18.6885, -31.6634, -37.6874,  -8.7355]],
       dtype=torch.float64)
	q_value: tensor([[-35.9253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.376282187494627
epoch: 0, step: 112
	action: tensor([[-30.3201,  13.5432,   2.7343,  21.7233,  16.0533,  13.0568,  13.8868]],
       dtype=torch.float64)
	q_value: tensor([[-35.9253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.376282187494627
epoch: 0, step: 113
	action: tensor([[-11.7118, -18.4637, -27.8122,  41.0742, -14.7779,  63.6497, -13.4275]],
       dtype=torch.float64)
	q_value: tensor([[-35.9253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.376282187494627
epoch: 0, step: 114
	action: tensor([[-53.3817,   3.9484,   3.3164,   2.6379,  -8.8959,  -6.6047,  18.3573]],
       dtype=torch.float64)
	q_value: tensor([[-35.9253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.376282187494627
epoch: 0, step: 115
	action: tensor([[-37.1934, -14.5191, -19.2040,  -6.1970, -29.7763, -20.2418,  -7.2934]],
       dtype=torch.float64)
	q_value: tensor([[-35.9253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9777585010844998, distance: 1.6093233894337 entropy 4.376282187494627
epoch: 0, step: 116
	action: tensor([[-38.2701,   6.0574, -49.8034,  -0.4905, -15.7966, -16.4226,  -8.8735]],
       dtype=torch.float64)
	q_value: tensor([[-25.6432]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.392079854615771
epoch: 0, step: 117
	action: tensor([[-33.1728, -13.6138,  -6.2417,  -3.3215,  14.1640,  19.0484,  28.2750]],
       dtype=torch.float64)
	q_value: tensor([[-35.9253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.376282187494627
epoch: 0, step: 118
	action: tensor([[-17.4805, -26.6013,  -5.8995,   1.3513,   7.6144,  13.7326,  -9.9526]],
       dtype=torch.float64)
	q_value: tensor([[-35.9253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5029674475570678, distance: 0.8067688285464835 entropy 4.376282187494627
epoch: 0, step: 119
	action: tensor([[-51.4599,   2.9387,  -6.0956,  -9.7454,  15.3536,  -6.6251, -31.6514]],
       dtype=torch.float64)
	q_value: tensor([[-30.2740]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.594958265869489
epoch: 0, step: 120
	action: tensor([[-31.9680, -44.5283,  -2.3641, -21.3182,  30.2818, -24.2437,  14.7515]],
       dtype=torch.float64)
	q_value: tensor([[-35.9253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.376282187494627
epoch: 0, step: 121
	action: tensor([[-38.1601, -13.4489, -16.3847,   7.2495,  -0.4295,  39.3760,  30.8791]],
       dtype=torch.float64)
	q_value: tensor([[-35.9253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.376282187494627
epoch: 0, step: 122
	action: tensor([[-24.4141, -19.9028,   2.8437,  -2.0015, -11.8774,  29.9320,  35.7736]],
       dtype=torch.float64)
	q_value: tensor([[-35.9253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5059116859069195, distance: 1.4042888416276544 entropy 4.376282187494627
epoch: 0, step: 123
	action: tensor([[-53.3755, -36.4314,   5.0129, -39.8918, -50.1119, -27.2558, -36.7465]],
       dtype=torch.float64)
	q_value: tensor([[-40.0084]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.47495001124263403, distance: 0.8291956594128224 entropy 4.7428636715794426
epoch: 0, step: 124
	action: tensor([[-34.5492, -53.4366,  -9.7639,  25.2042, -20.8717, -16.1630, -13.7080]],
       dtype=torch.float64)
	q_value: tensor([[-36.7805]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.004902014116504, distance: 1.6203292404138798 entropy 4.611865819357474
epoch: 0, step: 125
	action: tensor([[-64.3417, -88.5348,   0.9562,  50.9669, -46.2440, -41.0413, -16.5108]],
       dtype=torch.float64)
	q_value: tensor([[-52.7565]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6182176907754156, distance: 1.4557110062769036 entropy 5.06046188469393
epoch: 0, step: 126
	action: tensor([[  8.1087, -93.9501, -30.2256, -39.6641,  11.0032, -12.3017, -72.0187]],
       dtype=torch.float64)
	q_value: tensor([[-41.0749]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20115464320471899, distance: 1.0227937155682394 entropy 4.964886196150245
epoch: 0, step: 127
	action: tensor([[-55.1642,  -4.4708, -54.1534,  -5.9343,  18.2436, -30.2607,   9.2948]],
       dtype=torch.float64)
	q_value: tensor([[-41.3426]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.698939484211138
LOSS epoch 0 actor 231.96933307405305 critic 223.47183775962088
epoch: 1, step: 0
	action: tensor([[-14.6839, -31.2666, -39.2906,   1.9312, -25.7139,  -8.4323,  20.2099]],
       dtype=torch.float64)
	q_value: tensor([[-47.9678]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.623370731055019
epoch: 1, step: 1
	action: tensor([[-37.6805, -87.5523,  -9.2660,  -0.9672,  -5.2983,  18.0658, -16.9204]],
       dtype=torch.float64)
	q_value: tensor([[-47.9678]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.623370731055019
epoch: 1, step: 2
	action: tensor([[  3.4214, -46.8089,   4.6011,  -8.7514,  22.9999,  -7.7791,  -2.6326]],
       dtype=torch.float64)
	q_value: tensor([[-47.9678]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.623370731055019
epoch: 1, step: 3
	action: tensor([[-37.1232, -31.3515, -24.6681,   8.6040,  21.0343, -22.9465,  -3.2479]],
       dtype=torch.float64)
	q_value: tensor([[-47.9678]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.623370731055019
epoch: 1, step: 4
	action: tensor([[-31.8615, -31.5942,  49.0368,  -6.7614, -39.1187,  15.8498, -23.1962]],
       dtype=torch.float64)
	q_value: tensor([[-47.9678]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.623370731055019
epoch: 1, step: 5
	action: tensor([[-20.1315,  32.2228, -37.3557,  -5.5219,   6.7544,  -8.4442,  14.5810]],
       dtype=torch.float64)
	q_value: tensor([[-47.9678]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.36578988377141086, distance: 1.3373609534808686 entropy 4.623370731055019
epoch: 1, step: 6
	action: tensor([[-26.0343, -26.2245,  26.0066,  29.1333, -12.7249,  -8.9338, -19.5653]],
       dtype=torch.float64)
	q_value: tensor([[-43.4210]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20079042764653976, distance: 1.253979110586896 entropy 4.849764317386429
epoch: 1, step: 7
	action: tensor([[-28.9160,  -1.0746, -24.0731, -55.8681, -15.4756,  10.6984,  -8.1080]],
       dtype=torch.float64)
	q_value: tensor([[-35.7578]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4067155254310926, distance: 1.3572499476953996 entropy 4.765525782964441
epoch: 1, step: 8
	action: tensor([[-40.8344, -72.8060,   5.6346,  47.8493, -47.6274,   7.0760, -94.1253]],
       dtype=torch.float64)
	q_value: tensor([[-46.1094]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.72816975573833, distance: 1.5043534746222598 entropy 5.100673649534438
epoch: 1, step: 9
	action: tensor([[-36.5108, -45.6449,  15.8740,  54.5846,   1.5802,  27.6475,  -6.9368]],
       dtype=torch.float64)
	q_value: tensor([[-39.8399]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5215815501554779, distance: 1.411576144050386 entropy 5.003370201034893
epoch: 1, step: 10
	action: tensor([[-58.1900, -30.4016, -22.1005, -72.4580, -86.7858,  10.7635,  13.2328]],
       dtype=torch.float64)
	q_value: tensor([[-44.6181]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.074254151638032
epoch: 1, step: 11
	action: tensor([[-58.9421, -48.4783,   4.2896,  27.8163, -15.7539,  -8.4724,  21.9239]],
       dtype=torch.float64)
	q_value: tensor([[-47.9678]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5470469012136916, distance: 1.423339275442305 entropy 4.623370731055019
epoch: 1, step: 12
	action: tensor([[-33.0908,   4.9585,  44.5056,  42.5913,  -9.7941, -15.7845,  32.8317]],
       dtype=torch.float64)
	q_value: tensor([[-54.2588]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0540619934829909, distance: 1.1748698809963056 entropy 5.057447785809345
epoch: 1, step: 13
	action: tensor([[ -40.3752, -124.7614,   37.8213,  -37.4422,    5.7801,  -40.8764,
           24.5237]], dtype=torch.float64)
	q_value: tensor([[-51.9995]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7855895321662645, distance: 1.5291409152867534 entropy 5.122200617446908
epoch: 1, step: 14
	action: tensor([[-50.1367, -44.1134,   9.1095,   0.3909,  -4.3586,   0.4196, -57.9593]],
       dtype=torch.float64)
	q_value: tensor([[-37.6938]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.095629121093955, distance: 1.6565856907088714 entropy 4.841307112966698
epoch: 1, step: 15
	action: tensor([[-17.1296, -17.7808,   4.9533, -28.6705, -20.0251,   7.5095,  44.0230]],
       dtype=torch.float64)
	q_value: tensor([[-32.3124]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.68337025632767
epoch: 1, step: 16
	action: tensor([[-21.5722, -41.5987,   5.4056,  31.0080, -27.7419,  21.3959,  22.3362]],
       dtype=torch.float64)
	q_value: tensor([[-47.9678]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6806052528537798, distance: 1.483506834966871 entropy 4.623370731055019
epoch: 1, step: 17
	action: tensor([[-36.9299, -32.5983,  71.4687,  62.5669, -50.9619,   8.8725,  28.1434]],
       dtype=torch.float64)
	q_value: tensor([[-59.3693]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7380370321214065, distance: 1.5086420415702566 entropy 4.962616700379146
epoch: 1, step: 18
	action: tensor([[-99.0425, -18.9825, -14.6508, -29.7874, -26.5984,  27.5786,   3.3460]],
       dtype=torch.float64)
	q_value: tensor([[-60.3586]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5122182089495138, distance: 1.4072262407163805 entropy 5.141065998135465
epoch: 1, step: 19
	action: tensor([[-104.7310,   24.9007,  -12.9379,  -93.2493,  -65.5921,   71.2562,
            9.0644]], dtype=torch.float64)
	q_value: tensor([[-53.7246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5665937777215835, distance: 1.4323029681130552 entropy 5.128761004174637
epoch: 1, step: 20
	action: tensor([[-54.6065, -48.1710, -43.0307,  35.9618, -57.2763,  73.7861,  38.6158]],
       dtype=torch.float64)
	q_value: tensor([[-62.9213]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.009841133649868494, distance: 1.1386995096197252 entropy 5.15087720229753
epoch: 1, step: 21
	action: tensor([[ 11.4215,  -4.3927,  -3.0452, -46.9799, -27.8413,  -3.3711,  12.0982]],
       dtype=torch.float64)
	q_value: tensor([[-40.8732]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.728578781620615
epoch: 1, step: 22
	action: tensor([[-62.9073,  -3.6880,  10.2471, -29.0026,  43.3791,  38.3342, -13.0478]],
       dtype=torch.float64)
	q_value: tensor([[-47.9678]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.623370731055019
epoch: 1, step: 23
	action: tensor([[ -9.7133,  14.8116, -28.6935,  -1.5945, -16.1013,  17.1182,  51.1301]],
       dtype=torch.float64)
	q_value: tensor([[-47.9678]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.623370731055019
epoch: 1, step: 24
	action: tensor([[-41.9691,  -8.9858,  -4.0409,  28.9412,  19.5311,  25.8807,  27.0652]],
       dtype=torch.float64)
	q_value: tensor([[-47.9678]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.623370731055019
epoch: 1, step: 25
	action: tensor([[  1.7591, -46.2432,  66.8785,  21.8744, -66.5747, -18.2145, -18.6552]],
       dtype=torch.float64)
	q_value: tensor([[-47.9678]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.623370731055019
epoch: 1, step: 26
	action: tensor([[-36.6939, -10.8969,  21.4338,  37.3033,   9.9348, -25.9766,   0.2158]],
       dtype=torch.float64)
	q_value: tensor([[-47.9678]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8445032030023287, distance: 1.5541624082571792 entropy 4.623370731055019
epoch: 1, step: 27
	action: tensor([[ 10.6884, -68.3868,  28.9152,   4.6405, -38.3609, -65.5616,  35.2166]],
       dtype=torch.float64)
	q_value: tensor([[-52.9198]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.053556816113728
epoch: 1, step: 28
	action: tensor([[-28.2659, -25.0429, -46.7172,  -4.1499,  -9.6983, -29.0134,  37.7793]],
       dtype=torch.float64)
	q_value: tensor([[-47.9678]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.623370731055019
epoch: 1, step: 29
	action: tensor([[-45.3566, -25.8291,  -2.0007,  13.9654, -15.5704, -47.0182,  28.2773]],
       dtype=torch.float64)
	q_value: tensor([[-47.9678]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.179256011203057, distance: 1.2426841028783926 entropy 4.623370731055019
epoch: 1, step: 30
	action: tensor([[-126.1870,  -36.2907,    9.7013,   19.9150,  -92.0731,   21.3303,
           73.6949]], dtype=torch.float64)
	q_value: tensor([[-64.7700]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07484372960168795, distance: 1.1006880250958953 entropy 5.397122133551572
epoch: 1, step: 31
	action: tensor([[ -26.8756,  -50.1309,   52.7935,  -12.4832,    9.5730, -102.1373,
          -58.6200]], dtype=torch.float64)
	q_value: tensor([[-51.1110]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23908072594543772, distance: 1.2738153970001973 entropy 5.20042960203857
epoch: 1, step: 32
	action: tensor([[-22.8259, -53.1870,  30.8202, -60.0560, -49.6842,  35.3101, -37.1036]],
       dtype=torch.float64)
	q_value: tensor([[-62.7039]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.264326093044246
epoch: 1, step: 33
	action: tensor([[-53.1441, -43.0659,  -8.2802,  -6.7357, -10.5783,  13.2098,  35.0244]],
       dtype=torch.float64)
	q_value: tensor([[-47.9678]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.623370731055019
epoch: 1, step: 34
	action: tensor([[-49.8111, -37.0038, -19.1705,  26.5014, -22.6584,  52.0203,  32.0959]],
       dtype=torch.float64)
	q_value: tensor([[-47.9678]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1615436930131584, distance: 1.2333162926945171 entropy 4.623370731055019
epoch: 1, step: 35
	action: tensor([[  2.7225, -80.3251,  18.4425,  34.4108, -29.8218,  16.2753,  15.3161]],
       dtype=torch.float64)
	q_value: tensor([[-39.1100]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.676566085271736
epoch: 1, step: 36
	action: tensor([[ -0.1783,  -3.8021, -22.8025,  -1.7930,  -7.9993,  -1.7807, -23.5293]],
       dtype=torch.float64)
	q_value: tensor([[-47.9678]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.623370731055019
epoch: 1, step: 37
	action: tensor([[-41.2798, -17.0023,  40.6999, -25.6637,  13.2873, -38.0596,  20.1971]],
       dtype=torch.float64)
	q_value: tensor([[-47.9678]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.623370731055019
epoch: 1, step: 38
	action: tensor([[-43.1666, -39.8362,   8.5446,  37.9860, -31.2944,  21.1840, -11.7966]],
       dtype=torch.float64)
	q_value: tensor([[-47.9678]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.3069314227252224, distance: 1.7380971024815042 entropy 4.623370731055019
epoch: 1, step: 39
	action: tensor([[-4.0684,  2.9598,  7.4832,  6.9984, -7.5138, -6.1046,  8.3099]],
       dtype=torch.float64)
	q_value: tensor([[-37.1696]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.799128171100855
epoch: 1, step: 40
	action: tensor([[ -8.5289, -18.0689,  30.1395,  31.6576, -23.0078, -13.2737,  -2.1892]],
       dtype=torch.float64)
	q_value: tensor([[-47.9678]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.085720734881423, distance: 1.6526647825186034 entropy 4.623370731055019
epoch: 1, step: 41
	action: tensor([[-128.9012,  -67.5858,  -32.0873,   18.9852,  -38.7752,  -29.7644,
           46.3781]], dtype=torch.float64)
	q_value: tensor([[-44.5929]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2085984948306533, distance: 1.0180172316562195 entropy 4.87243275359117
epoch: 1, step: 42
	action: tensor([[-84.3506,   6.7025, -17.0557,   1.2610, -46.9632,  80.8192,  26.2031]],
       dtype=torch.float64)
	q_value: tensor([[-59.9933]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37113220105395706, distance: 0.9074785572965985 entropy 5.147175912577696
epoch: 1, step: 43
	action: tensor([[-94.3755, -19.7776, -45.1754,  18.8267, -76.5468, -71.9760,   2.2699]],
       dtype=torch.float64)
	q_value: tensor([[-51.0472]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3106286979644399, distance: 1.3100761183259235 entropy 5.127792446855186
epoch: 1, step: 44
	action: tensor([[  6.9296, -76.8213,  -2.3246,  -5.0355,  14.1894,  83.3792,  47.8016]],
       dtype=torch.float64)
	q_value: tensor([[-44.1921]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.799195608924778, distance: 1.5349558364195215 entropy 5.034785882598729
epoch: 1, step: 45
	action: tensor([[-96.5873, -29.2399, -23.4824, -42.8293, -72.2973, -56.8768,  -0.6935]],
       dtype=torch.float64)
	q_value: tensor([[-41.2716]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.0611617664394615
epoch: 1, step: 46
	action: tensor([[-53.5037, -46.9961,  -2.8763,  33.8010,  -3.4370, -75.2149, -30.8023]],
       dtype=torch.float64)
	q_value: tensor([[-47.9678]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8407761173502915, distance: 1.552591409218597 entropy 4.623370731055019
epoch: 1, step: 47
	action: tensor([[-81.3010, -49.1744, -24.0782,  26.4064,  -0.4606,  -4.4694,  35.4951]],
       dtype=torch.float64)
	q_value: tensor([[-54.2816]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.042861582252578
epoch: 1, step: 48
	action: tensor([[-32.9696,   0.9448, -36.0179,  19.4563, -37.8119,  -2.7416,  10.2949]],
       dtype=torch.float64)
	q_value: tensor([[-47.9678]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.623370731055019
epoch: 1, step: 49
	action: tensor([[ 17.2736, -33.2319, -18.7432,   1.5028,   8.1744,  -1.2970,  30.7640]],
       dtype=torch.float64)
	q_value: tensor([[-47.9678]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.623370731055019
epoch: 1, step: 50
	action: tensor([[-21.8370,  20.4794,  18.1499,  39.1668, -49.2657,   8.5572,  -6.7728]],
       dtype=torch.float64)
	q_value: tensor([[-47.9678]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.623370731055019
epoch: 1, step: 51
	action: tensor([[-59.1622, -26.9965,  43.5766,  24.0150, -36.1261,   6.6374,  42.5665]],
       dtype=torch.float64)
	q_value: tensor([[-47.9678]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0759881716238473, distance: 1.6488043730025266 entropy 4.623370731055019
epoch: 1, step: 52
	action: tensor([[-26.3792,  -1.4307,  47.9866,  11.2810,  -4.4851,  65.6959,   0.3810]],
       dtype=torch.float64)
	q_value: tensor([[-34.7355]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.599777310280147
epoch: 1, step: 53
	action: tensor([[-29.4364, -40.9615, -46.9991, -19.8376,  -2.4894,  18.0430,  -0.5737]],
       dtype=torch.float64)
	q_value: tensor([[-47.9678]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.623370731055019
epoch: 1, step: 54
	action: tensor([[ 9.0226,  7.6161, -8.3518, -0.3021,  5.6693, 13.0087, 17.2017]],
       dtype=torch.float64)
	q_value: tensor([[-47.9678]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.623370731055019
epoch: 1, step: 55
	action: tensor([[-23.1707, -29.7871,  48.4860,  34.7813,   7.2165, -16.5963,  15.8311]],
       dtype=torch.float64)
	q_value: tensor([[-47.9678]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.623370731055019
epoch: 1, step: 56
	action: tensor([[-25.5281, -42.1324,   8.5544,  18.6276,  27.1484, -12.3094, -55.0443]],
       dtype=torch.float64)
	q_value: tensor([[-47.9678]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.623370731055019
epoch: 1, step: 57
	action: tensor([[-59.2401,  36.7912, -28.3857,  17.4814, -12.5782,  19.3599, -42.7522]],
       dtype=torch.float64)
	q_value: tensor([[-47.9678]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5424066771466303, distance: 1.421203085169381 entropy 4.623370731055019
epoch: 1, step: 58
	action: tensor([[-13.9587, -18.1516,  -3.7839,   9.3957,  -4.1868,  -6.4938,  32.8441]],
       dtype=torch.float64)
	q_value: tensor([[-45.4562]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4156758239940379, distance: 1.3615656959869904 entropy 4.643696444848329
epoch: 1, step: 59
	action: tensor([[ -5.9449, -25.7081,   9.3839,   4.5340,  10.1482,  23.8605,  28.9650]],
       dtype=torch.float64)
	q_value: tensor([[-46.5255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.0011552063089795
epoch: 1, step: 60
	action: tensor([[-40.0602,  -9.8518,  -4.7545,  -9.9808,  -5.3219,  60.7619,  -0.5322]],
       dtype=torch.float64)
	q_value: tensor([[-47.9678]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.623370731055019
epoch: 1, step: 61
	action: tensor([[-52.6058,  -4.1660, -13.6164,  29.8179,  -2.4891, -13.1772,  19.5755]],
       dtype=torch.float64)
	q_value: tensor([[-47.9678]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.623370731055019
epoch: 1, step: 62
	action: tensor([[-60.2398, -22.9658,  57.4646, -26.5661,  14.1767, -31.4670,   3.2426]],
       dtype=torch.float64)
	q_value: tensor([[-47.9678]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7895056511129466, distance: 1.5308168373967375 entropy 4.623370731055019
epoch: 1, step: 63
	action: tensor([[-105.0101,  -18.0607,  -44.1658,  -30.5208,  -56.2528,   42.3562,
            2.7946]], dtype=torch.float64)
	q_value: tensor([[-54.8006]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.38129893285032956, distance: 1.3449326333835412 entropy 4.918968205687881
epoch: 1, step: 64
	action: tensor([[ -27.7181,  -24.7368, -115.1985,  -49.6938,  -39.1733,   42.9122,
           41.8403]], dtype=torch.float64)
	q_value: tensor([[-53.9935]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.235750220698002, distance: 1.272102311074705 entropy 4.975857555193779
epoch: 1, step: 65
	action: tensor([[-67.6697, -37.9480,  87.1142,  14.8092, -13.8055,   6.0007,  12.1351]],
       dtype=torch.float64)
	q_value: tensor([[-67.8766]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.3430784681733967, distance: 1.7516611981043482 entropy 5.117194120639695
epoch: 1, step: 66
	action: tensor([[ -3.1999, -10.4765,   5.7708,  21.5759, -28.9331,   2.5140, -23.8419]],
       dtype=torch.float64)
	q_value: tensor([[-29.9746]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.489556278714687
epoch: 1, step: 67
	action: tensor([[-29.3053,  -8.9156, -15.5956, -12.5561,  25.6510,  64.0280,  -1.4932]],
       dtype=torch.float64)
	q_value: tensor([[-47.9678]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.623370731055019
epoch: 1, step: 68
	action: tensor([[-3.4378e+01, -1.9442e+01,  1.4005e-02,  1.2270e+01,  2.7662e+01,
         -3.9403e+01,  1.9801e+00]], dtype=torch.float64)
	q_value: tensor([[-47.9678]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.623370731055019
epoch: 1, step: 69
	action: tensor([[-29.4437, -18.0118, -10.2052, -33.3911,  -2.0376, -11.2175,  12.1913]],
       dtype=torch.float64)
	q_value: tensor([[-47.9678]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.623370731055019
epoch: 1, step: 70
	action: tensor([[-64.3665,  -7.8643, -23.9503,  11.1788, -51.3017, -52.6790, -36.7422]],
       dtype=torch.float64)
	q_value: tensor([[-47.9678]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.3727822731142947, distance: 1.76272935858665 entropy 4.623370731055019
epoch: 1, step: 71
	action: tensor([[-30.5782,  -4.0347,  29.3461,  56.8527, -21.3834,  23.2790,  23.7163]],
       dtype=torch.float64)
	q_value: tensor([[-31.6683]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.631289618708018
epoch: 1, step: 72
	action: tensor([[ -6.7436, -72.2846,  31.1886,  19.6100,  -8.9388,   0.6385,  20.2599]],
       dtype=torch.float64)
	q_value: tensor([[-47.9678]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.623370731055019
epoch: 1, step: 73
	action: tensor([[-11.1210,   0.4387, -12.1277,  -1.8162,  -7.4221, -20.9531,  24.7776]],
       dtype=torch.float64)
	q_value: tensor([[-47.9678]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6022937218405344, distance: 0.7216687781727601 entropy 4.623370731055019
epoch: 1, step: 74
	action: tensor([[-37.2722, -14.0663,  20.7473, -13.5211, -26.2680, -49.2659,  41.6270]],
       dtype=torch.float64)
	q_value: tensor([[-44.4280]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04969207745564175, distance: 1.172431971738356 entropy 4.739130241151644
epoch: 1, step: 75
	action: tensor([[-80.5607, -79.1100,  29.0601,   5.5104, -39.3128,  32.0911,  46.8183]],
       dtype=torch.float64)
	q_value: tensor([[-52.0250]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4939286210629623, distance: 1.3986904741397936 entropy 4.95164057181916
epoch: 1, step: 76
	action: tensor([[-42.2911,  -9.7432,  -7.7559,  13.2035, -38.8651,  35.1046, -54.3077]],
       dtype=torch.float64)
	q_value: tensor([[-49.1129]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.133835998825207
epoch: 1, step: 77
	action: tensor([[-52.3379,   0.3977, -30.5584,   1.7975, -26.6495,  31.2168,  -1.7043]],
       dtype=torch.float64)
	q_value: tensor([[-47.9678]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.35239381775533984, distance: 1.3307861785427622 entropy 4.623370731055019
epoch: 1, step: 78
	action: tensor([[-57.9856, -25.7947,  11.0310,  19.2382, -55.4041, -48.9694,  -4.4364]],
       dtype=torch.float64)
	q_value: tensor([[-41.0182]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.4723316626418148, distance: 1.7993268418117123 entropy 4.899746155823057
epoch: 1, step: 79
	action: tensor([[-52.8228, -18.1796,  24.6908,  51.5316, -33.7205, -39.0155,  11.7022]],
       dtype=torch.float64)
	q_value: tensor([[-41.8096]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10299925132480259, distance: 1.2018334908530874 entropy 4.868153489156825
epoch: 1, step: 80
	action: tensor([[-78.3839, -31.2429,  55.2480,   0.7045,  38.2139, -13.2734,  33.4569]],
       dtype=torch.float64)
	q_value: tensor([[-43.8261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.3027294126716642, distance: 1.7365134337518668 entropy 4.965731155835308
epoch: 1, step: 81
	action: tensor([[ 14.0103, -15.5707,   4.5795,  17.4695, -36.0955, -25.5199,  -7.8280]],
       dtype=torch.float64)
	q_value: tensor([[-35.0451]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.691055689034146
epoch: 1, step: 82
	action: tensor([[-30.2214, -16.3714, -33.9837,   5.5224,  -6.4603,  -4.5107,  15.1688]],
       dtype=torch.float64)
	q_value: tensor([[-47.9678]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.623370731055019
epoch: 1, step: 83
	action: tensor([[-45.5998, -44.7987,  20.3902,  19.3164, -17.2627,  30.6905,  10.5727]],
       dtype=torch.float64)
	q_value: tensor([[-47.9678]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08245475023875226, distance: 1.0961511374526096 entropy 4.623370731055019
epoch: 1, step: 84
	action: tensor([[-63.7431,   5.3812, -15.7981,  -3.1533,  17.1559,  42.0260, -15.0078]],
       dtype=torch.float64)
	q_value: tensor([[-37.0905]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.714384251152063
epoch: 1, step: 85
	action: tensor([[ 17.1178, -21.4455, -24.5308,  31.3425,  -1.4308,   4.4918,  21.4505]],
       dtype=torch.float64)
	q_value: tensor([[-47.9678]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.623370731055019
epoch: 1, step: 86
	action: tensor([[-23.4792, -34.4161,  -2.6518, -42.8016,   2.6392,   8.5010,   6.0482]],
       dtype=torch.float64)
	q_value: tensor([[-47.9678]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.623370731055019
epoch: 1, step: 87
	action: tensor([[-48.3319, -55.4795,   5.3117,  -2.9315,   7.6138, -24.6195, -17.4913]],
       dtype=torch.float64)
	q_value: tensor([[-47.9678]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.623370731055019
epoch: 1, step: 88
	action: tensor([[ -8.8155,  -4.1797, -21.8370,   4.5236,   8.3965, -15.7776,   3.4332]],
       dtype=torch.float64)
	q_value: tensor([[-47.9678]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.623370731055019
epoch: 1, step: 89
	action: tensor([[-52.8777,  12.8805,   3.8430,  38.7630, -34.4843,  40.6225,  35.2053]],
       dtype=torch.float64)
	q_value: tensor([[-47.9678]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.28081206094110556, distance: 1.2950883525207157 entropy 4.623370731055019
epoch: 1, step: 90
	action: tensor([[-41.3889,  -2.4318, -12.6432, -14.8422,   1.9559,  31.5729,  25.0957]],
       dtype=torch.float64)
	q_value: tensor([[-62.5296]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.020187652369962
epoch: 1, step: 91
	action: tensor([[ 11.0013, -56.4830,  32.2928,  29.8315,  -4.6487, -28.5402,   1.0407]],
       dtype=torch.float64)
	q_value: tensor([[-47.9678]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.623370731055019
epoch: 1, step: 92
	action: tensor([[-65.5466, -10.2545, -12.4902,   9.3807, -26.3280, -37.8005, -15.0874]],
       dtype=torch.float64)
	q_value: tensor([[-47.9678]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7356079651664971, distance: 0.5884111272339907 entropy 4.623370731055019
epoch: 1, step: 93
	action: tensor([[-73.9825, -33.1470,  -1.2773,  12.8065,  -2.6861, -26.4940, -27.9859]],
       dtype=torch.float64)
	q_value: tensor([[-45.6569]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.36276840572319924, distance: 1.335880841454644 entropy 5.031085295644796
epoch: 1, step: 94
	action: tensor([[-45.0744,  48.8835,  22.1466,  63.7534, -41.2739,  12.2297,   3.9481]],
       dtype=torch.float64)
	q_value: tensor([[-55.9452]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.3107033941724868, distance: 1.7395174687052806 entropy 5.234042492441658
epoch: 1, step: 95
	action: tensor([[-36.9884,  32.1762,  38.8341,  39.6236,  40.8477,   0.4152,  45.5130]],
       dtype=torch.float64)
	q_value: tensor([[-52.5814]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4939040024134629, distance: 1.3986789494886434 entropy 5.164297092601864
epoch: 1, step: 96
	action: tensor([[ -93.4336,  -41.5337, -110.8802,    0.5216,  -56.9169,  -23.4444,
           50.6957]], dtype=torch.float64)
	q_value: tensor([[-50.0099]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.178997778814067
epoch: 1, step: 97
	action: tensor([[-27.8576, -26.0952, -39.5251,  14.9343,   1.7086, -12.0062,  31.5474]],
       dtype=torch.float64)
	q_value: tensor([[-47.9678]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.7634241179409083, distance: 1.9023063826918023 entropy 4.623370731055019
epoch: 1, step: 98
	action: tensor([[-84.2297, -10.4100, -13.9039,  44.5348, -20.7231, -48.6676,  32.0817]],
       dtype=torch.float64)
	q_value: tensor([[-38.6494]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.799357304432768
epoch: 1, step: 99
	action: tensor([[-16.7235, -60.0743,  19.8299,  -2.1200, -33.5851,  12.8260, -28.6626]],
       dtype=torch.float64)
	q_value: tensor([[-47.9678]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.623370731055019
epoch: 1, step: 100
	action: tensor([[  0.8894,   8.3185, -13.2777, -10.0562, -42.8302, -25.1428,   9.4144]],
       dtype=torch.float64)
	q_value: tensor([[-47.9678]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.623370731055019
epoch: 1, step: 101
	action: tensor([[-29.9274, -11.8047,  -2.3550,   0.1269, -25.3477, -25.5257, -18.2074]],
       dtype=torch.float64)
	q_value: tensor([[-47.9678]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5053096456596518, distance: 1.4040081070651425 entropy 4.623370731055019
epoch: 1, step: 102
	action: tensor([[-5.8048e+01, -7.1415e+01, -2.1214e+01, -3.5998e+01, -1.9552e+01,
          7.2769e+00, -3.2768e-02]], dtype=torch.float64)
	q_value: tensor([[-46.3423]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3991164887159837, distance: 1.3535790709445057 entropy 4.784405873462789
epoch: 1, step: 103
	action: tensor([[-61.9476, -64.7201, -34.5950, -24.5818,   2.1197,  38.8344, -10.5896]],
       dtype=torch.float64)
	q_value: tensor([[-48.8178]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7772472507107191, distance: 1.525564657078838 entropy 4.7329517343443825
epoch: 1, step: 104
	action: tensor([[-57.4386,   3.0995, -31.7748, -46.8853, -29.2594, -42.3611,  -3.5825]],
       dtype=torch.float64)
	q_value: tensor([[-43.5049]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.963376254913696
epoch: 1, step: 105
	action: tensor([[ -3.4175, -26.9084,  12.9575,  10.9677,  -2.3173, -31.1091,  45.7869]],
       dtype=torch.float64)
	q_value: tensor([[-47.9678]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.623370731055019
epoch: 1, step: 106
	action: tensor([[-48.4951, -12.7521,   3.9103,  17.3682,   5.8992,  43.9805, -20.4813]],
       dtype=torch.float64)
	q_value: tensor([[-47.9678]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.478827187090956, distance: 1.8016889652045047 entropy 4.623370731055019
epoch: 1, step: 107
	action: tensor([[-113.7423,  -22.8282,  -24.6428,   15.2717,  -41.7938,   35.1487,
          -29.4423]], dtype=torch.float64)
	q_value: tensor([[-52.3652]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.927723817523285
epoch: 1, step: 108
	action: tensor([[-39.6567, -49.1838,  -1.6265,  -6.7106, -14.0091,  21.4839,  13.9640]],
       dtype=torch.float64)
	q_value: tensor([[-47.9678]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3137162788292083, distance: 1.3116183501107572 entropy 4.623370731055019
epoch: 1, step: 109
	action: tensor([[-79.6829, -55.8271,  -3.5074, -32.9833, -22.3103,  88.0088,   3.1989]],
       dtype=torch.float64)
	q_value: tensor([[-59.0164]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29859793622747677, distance: 0.9583854527294735 entropy 4.940870867490344
epoch: 1, step: 110
	action: tensor([[-78.8875, -51.4403, -30.7790, -58.4838, -17.1011, -37.8474,  39.8589]],
       dtype=torch.float64)
	q_value: tensor([[-54.3837]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6758582764800736, distance: 1.4814102237356581 entropy 5.192040332309669
epoch: 1, step: 111
	action: tensor([[-103.3773,  -37.2795,  -11.8978,    8.8830,  -87.3085,   28.0827,
            8.1932]], dtype=torch.float64)
	q_value: tensor([[-42.3519]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1996554830484616, distance: 1.0237529831302243 entropy 4.977466908764177
epoch: 1, step: 112
	action: tensor([[-56.0404, -53.3854, -73.4253,  15.2736, -21.0978, -36.4898,  -2.4683]],
       dtype=torch.float64)
	q_value: tensor([[-43.2117]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06986741520031192, distance: 1.1036442953204497 entropy 4.891791507702217
epoch: 1, step: 113
	action: tensor([[-25.5320,  -4.9000,  -8.3968,  55.3853,  24.9836, -22.9021,  26.0239]],
       dtype=torch.float64)
	q_value: tensor([[-46.4176]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3970792343351752, distance: 0.888560113367258 entropy 5.168715401529909
epoch: 1, step: 114
	action: tensor([[-41.8530, -49.4330,  -3.1357, -77.4264,  20.5291, -76.7326, -14.5058]],
       dtype=torch.float64)
	q_value: tensor([[-45.7467]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.3527577415443544, distance: 1.755275531509527 entropy 4.953806376315121
epoch: 1, step: 115
	action: tensor([[-63.0708,   0.7028, -29.9491,  45.3485,  13.7861, -41.8686,  36.0157]],
       dtype=torch.float64)
	q_value: tensor([[-52.3698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12237176128026617, distance: 1.0720424624454297 entropy 5.0111560446527035
epoch: 1, step: 116
	action: tensor([[-63.7318, -14.7774, -42.2514, -39.1996,   9.9562, -50.5600,  12.7852]],
       dtype=torch.float64)
	q_value: tensor([[-44.7670]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2240038153756556, distance: 1.7065713769143154 entropy 4.8653336314572675
epoch: 1, step: 117
	action: tensor([[-93.5353, -62.0030,  -1.4138, -22.0286, -36.4177, -34.3274,  33.1642]],
       dtype=torch.float64)
	q_value: tensor([[-44.3451]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6419887553608075, distance: 0.6847074085896945 entropy 4.9154924375773374
epoch: 1, step: 118
	action: tensor([[-95.3569,  19.8481, -64.7791,  53.0203,   1.7096,  79.9038,  -4.4614]],
       dtype=torch.float64)
	q_value: tensor([[-54.5051]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.074843432597697
epoch: 1, step: 119
	action: tensor([[-48.1988,   4.7196,   7.9225,  27.5570, -31.2025,  16.2101,  44.0285]],
       dtype=torch.float64)
	q_value: tensor([[-47.9678]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.623370731055019
epoch: 1, step: 120
	action: tensor([[-17.1329, -59.2068,  12.3556,  23.2872,  -5.4591, -20.6485, -11.1924]],
       dtype=torch.float64)
	q_value: tensor([[-47.9678]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.623370731055019
epoch: 1, step: 121
	action: tensor([[-30.4332,  -7.6716, -20.7111,  14.5349, -29.1414,   0.9855, -48.2735]],
       dtype=torch.float64)
	q_value: tensor([[-47.9678]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.4059004365628454, distance: 1.7749883982530685 entropy 4.623370731055019
epoch: 1, step: 122
	action: tensor([[-43.2488,  23.4661,  -9.7440,  -6.4349, -16.6233,  69.0207,  16.0208]],
       dtype=torch.float64)
	q_value: tensor([[-43.8332]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.44800303493010896, distance: 1.377023745503643 entropy 4.894663740115844
epoch: 1, step: 123
	action: tensor([[-48.8078, -32.4605, -61.8119, -52.1304,   1.3966, -15.7236,  56.2400]],
       dtype=torch.float64)
	q_value: tensor([[-48.8372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.01809253862, distance: 1.625650685952628 entropy 5.046607299210477
epoch: 1, step: 124
	action: tensor([[-47.6352, -31.3410, -12.6448, -31.4846, -30.5667,  16.3700,  15.1492]],
       dtype=torch.float64)
	q_value: tensor([[-37.7777]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17870274309813317, distance: 1.2423925554393729 entropy 4.777709482929483
epoch: 1, step: 125
	action: tensor([[-55.4603, -14.9651, -36.2437,  70.3340, -74.3989,  15.2305,  -7.3766]],
       dtype=torch.float64)
	q_value: tensor([[-43.2769]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7458670062953485, distance: 1.512036489938505 entropy 4.9990296565540495
epoch: 1, step: 126
	action: tensor([[-44.8911, -26.2110,  32.7228,  40.0742, -43.4874,  24.0898,  -4.3622]],
       dtype=torch.float64)
	q_value: tensor([[-41.0062]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.887544267799518
epoch: 1, step: 127
	action: tensor([[-43.2292,  -9.1532,  -6.3335,   5.4036, -26.7343, -17.1449,  56.7051]],
       dtype=torch.float64)
	q_value: tensor([[-47.9678]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.623370731055019
LOSS epoch 1 actor 510.36094798288343 critic 42.99191977370759
epoch: 2, step: 0
	action: tensor([[-69.6836,  -1.2647,  15.8230,  60.6835, -11.5317, -14.2214,  33.0598]],
       dtype=torch.float64)
	q_value: tensor([[-47.4300]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9920859462202791, distance: 1.6151420686029048 entropy 4.855890214097324
epoch: 2, step: 1
	action: tensor([[-119.1141, -153.3401,   31.6533,   72.2410,  -52.5492,   61.3832,
           99.5796]], dtype=torch.float64)
	q_value: tensor([[-49.5069]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04256903178120175, distance: 1.168447230608627 entropy 5.3223700043525355
epoch: 2, step: 2
	action: tensor([[-78.0999, -89.4320,  43.8966,  36.1864, -68.1323,  23.4219, 167.8890]],
       dtype=torch.float64)
	q_value: tensor([[-44.9652]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6880117122237679, distance: 1.4867721627748742 entropy 5.352342099590413
epoch: 2, step: 3
	action: tensor([[-79.2202, -47.8770,   4.0316,  14.6028, -25.7419, -33.8472,  42.0642]],
       dtype=torch.float64)
	q_value: tensor([[-32.1881]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.041499896457772, distance: 1.6350512665536303 entropy 5.125898952213445
epoch: 2, step: 4
	action: tensor([[-56.5251, -33.6579, -44.3739,  43.7415, -18.7869,  -6.1569, -14.5170]],
       dtype=torch.float64)
	q_value: tensor([[-42.8501]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0464319144247054, distance: 1.6370251186309261 entropy 5.37276687285839
epoch: 2, step: 5
	action: tensor([[-64.4972, -36.8992,  -6.7632, -16.1437,  39.0185, -15.4148, -56.1410]],
       dtype=torch.float64)
	q_value: tensor([[-29.7374]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0690935863339965, distance: 1.6460641655474546 entropy 5.02385393485364
epoch: 2, step: 6
	action: tensor([[-12.0209, -19.6806,   5.7847,  39.3789,  -1.1179,  -3.5799, -35.4980]],
       dtype=torch.float64)
	q_value: tensor([[-34.0619]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.991209880809471
epoch: 2, step: 7
	action: tensor([[ 29.7959,  -8.0348, -61.8179,  -6.5101, -12.9461,  87.1882,  -7.9265]],
       dtype=torch.float64)
	q_value: tensor([[-47.4300]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.855890214097324
epoch: 2, step: 8
	action: tensor([[-30.8410, -43.4696,  46.8078, -21.7391, -24.1549,  27.4006,  12.1841]],
       dtype=torch.float64)
	q_value: tensor([[-47.4300]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16162737529295124, distance: 1.0477923847619954 entropy 4.855890214097324
epoch: 2, step: 9
	action: tensor([[-26.8825, -77.7523, -94.0536, -40.2537, -11.2140, -32.3138,  23.6518]],
       dtype=torch.float64)
	q_value: tensor([[-59.2061]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.243579653732233
epoch: 2, step: 10
	action: tensor([[-77.0241, -41.0378,   2.5077,  73.9585, -13.9560,   4.2678, -12.4395]],
       dtype=torch.float64)
	q_value: tensor([[-47.4300]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.855890214097324
epoch: 2, step: 11
	action: tensor([[  2.9751, -12.3393, -18.4241, -40.6803,  13.9461, -30.2953,  21.1272]],
       dtype=torch.float64)
	q_value: tensor([[-47.4300]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.855890214097324
epoch: 2, step: 12
	action: tensor([[-52.1702,   5.5336,  10.2902,  42.6664, -20.9182,  -9.9134,  46.3714]],
       dtype=torch.float64)
	q_value: tensor([[-47.4300]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.855890214097324
epoch: 2, step: 13
	action: tensor([[  0.7819, -41.5819,  30.3083,  59.1196, -19.7768,  -3.7217,  -6.2341]],
       dtype=torch.float64)
	q_value: tensor([[-47.4300]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.855890214097324
epoch: 2, step: 14
	action: tensor([[-80.4799,   3.0150, -21.8460,  19.8251, -23.1629,   4.4747,  32.2415]],
       dtype=torch.float64)
	q_value: tensor([[-47.4300]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.855890214097324
epoch: 2, step: 15
	action: tensor([[-22.9933, -37.8742, -52.3783, -39.4757, -24.2133,  28.6901, -19.4776]],
       dtype=torch.float64)
	q_value: tensor([[-47.4300]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.855890214097324
epoch: 2, step: 16
	action: tensor([[-58.5507, -32.9443, -36.9590,  -4.6805,  23.8598,  38.7881, -29.3095]],
       dtype=torch.float64)
	q_value: tensor([[-47.4300]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.855890214097324
epoch: 2, step: 17
	action: tensor([[ -6.3847, -54.9965, -93.5758,  -6.2264, -31.8589, -47.8071,  -2.1346]],
       dtype=torch.float64)
	q_value: tensor([[-47.4300]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.855890214097324
epoch: 2, step: 18
	action: tensor([[-66.1557, -33.7601, -49.8047,   4.0862,  64.3428,  -9.5787,  14.8935]],
       dtype=torch.float64)
	q_value: tensor([[-47.4300]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.855890214097324
epoch: 2, step: 19
	action: tensor([[-70.5040, -15.6038, -21.6955,  11.2385, -32.9001, -15.1472, -19.2545]],
       dtype=torch.float64)
	q_value: tensor([[-47.4300]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.855890214097324
epoch: 2, step: 20
	action: tensor([[-108.0665,  -14.1236,    6.3290,  -34.1496,  -32.8392,   76.8172,
           11.2781]], dtype=torch.float64)
	q_value: tensor([[-47.4300]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.570725358282993, distance: 1.434190432064436 entropy 4.855890214097324
epoch: 2, step: 21
	action: tensor([[-60.3142,   5.0322, -15.7049,  50.6311,  15.2700,  78.8152,  -9.3923]],
       dtype=torch.float64)
	q_value: tensor([[-52.5864]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0276134575969942, distance: 1.6294809058306574 entropy 5.165320512660694
epoch: 2, step: 22
	action: tensor([[-26.9891, -62.9466,  33.0012,  67.5175, -10.8943, -11.3985,   0.2591]],
       dtype=torch.float64)
	q_value: tensor([[-60.8464]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.017988137829157758, distance: 1.1545906918722522 entropy 5.556598394711139
epoch: 2, step: 23
	action: tensor([[-81.9757, -76.2274, -21.5053, -23.2143, -27.1011, -35.5432, -67.3255]],
       dtype=torch.float64)
	q_value: tensor([[-38.3306]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08265305469431783, distance: 1.1906972375820701 entropy 5.118489122943317
epoch: 2, step: 24
	action: tensor([[-66.2238, -75.1597, -36.8287,  26.7661,  13.9955, -87.8578, -76.7114]],
       dtype=torch.float64)
	q_value: tensor([[-42.7577]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.465712151313354, distance: 1.3854186748271744 entropy 5.360426412949755
epoch: 2, step: 25
	action: tensor([[-38.5451, -16.8029,  33.3086, -47.2924,  17.6679,  21.3383,   0.3522]],
       dtype=torch.float64)
	q_value: tensor([[-38.4290]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.118296944270384
epoch: 2, step: 26
	action: tensor([[-30.9372, -50.1252, -32.4368,   2.5595, -38.0891, -24.2408,   1.8984]],
       dtype=torch.float64)
	q_value: tensor([[-47.4300]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11984766996934382, distance: 1.2109777638198285 entropy 4.855890214097324
epoch: 2, step: 27
	action: tensor([[ -91.5653,   -7.6221,   10.8541, -107.6297,  -87.5784,   14.3811,
          -50.7374]], dtype=torch.float64)
	q_value: tensor([[-35.6027]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0906980635204095, distance: 1.6546355530899828 entropy 5.052195050498626
epoch: 2, step: 28
	action: tensor([[ 19.7221, -10.0339,  10.5768,  64.7986,  34.2510, -86.9735, -27.2729]],
       dtype=torch.float64)
	q_value: tensor([[-33.7653]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.027757470782958
epoch: 2, step: 29
	action: tensor([[-27.5185, -92.1111, -49.0184,  48.4315,  -1.6823,  60.6972,   5.9194]],
       dtype=torch.float64)
	q_value: tensor([[-47.4300]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2437424487026314, distance: 1.2762093489714832 entropy 4.855890214097324
epoch: 2, step: 30
	action: tensor([[-57.4837, -51.0941,  53.7670,  42.2792,  32.4747,   5.3956,  31.8119]],
       dtype=torch.float64)
	q_value: tensor([[-50.9710]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2677900279990457, distance: 1.2884879429555474 entropy 5.0510108221830325
epoch: 2, step: 31
	action: tensor([[ 10.6287, -18.4373,  23.7023,  24.5071, -16.6480,  31.1396,  33.7159]],
       dtype=torch.float64)
	q_value: tensor([[-38.5634]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6610616914151104, distance: 1.474855847568342 entropy 4.955818100095304
epoch: 2, step: 32
	action: tensor([[-131.5249,  -21.2253,   38.8147,    8.3558,   24.9984,   36.6775,
           19.1532]], dtype=torch.float64)
	q_value: tensor([[-39.8422]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2212834692662433, distance: 1.7055273397415232 entropy 5.022390079055282
epoch: 2, step: 33
	action: tensor([[-77.4364, -79.4577, -10.5388, -33.5078,  -5.1349,  28.2317, -41.1785]],
       dtype=torch.float64)
	q_value: tensor([[-26.0031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4232536950289931, distance: 1.3652049467927543 entropy 4.821445885143824
epoch: 2, step: 34
	action: tensor([[-14.8581, -69.8548, -39.6272,  -4.6454, -69.1861,  13.2928, -89.5278]],
       dtype=torch.float64)
	q_value: tensor([[-40.0924]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.187361238068272
epoch: 2, step: 35
	action: tensor([[-59.8677, -23.9587,  -9.2314,   1.6694,  -8.7949, -33.2376,   8.9023]],
       dtype=torch.float64)
	q_value: tensor([[-47.4300]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.19248041870468158, distance: 1.2496325282002743 entropy 4.855890214097324
epoch: 2, step: 36
	action: tensor([[-85.8542, -90.0229,  -7.7270,  32.8742,   8.9548,  68.4178, -56.7701]],
       dtype=torch.float64)
	q_value: tensor([[-65.0211]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2586853989010576, distance: 1.2838529709066493 entropy 5.581813625168274
epoch: 2, step: 37
	action: tensor([[-45.7162, -31.4378, -94.0104,  20.9422,  47.8498,  29.5733,  23.2698]],
       dtype=torch.float64)
	q_value: tensor([[-47.3087]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.142633488159982, distance: 1.0595951246972275 entropy 5.268086946522925
epoch: 2, step: 38
	action: tensor([[-147.5487,  -31.6839,  -20.1989,   51.7991,   31.9400,    0.2078,
          -19.6181]], dtype=torch.float64)
	q_value: tensor([[-38.2912]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7475752055027498, distance: 1.5127760159304424 entropy 5.2421634224989315
epoch: 2, step: 39
	action: tensor([[-104.3286,  -69.8109,  -19.2330,   20.6920,  -29.2347,   -9.0938,
            3.2762]], dtype=torch.float64)
	q_value: tensor([[-32.0933]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3051406059499533, distance: 1.3073303518445047 entropy 4.984429549598128
epoch: 2, step: 40
	action: tensor([[   7.8650,   54.2432,  -38.7920,   28.5234,    4.5230, -119.7736,
           30.1605]], dtype=torch.float64)
	q_value: tensor([[-48.2882]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.45069719511675244, distance: 0.848130344248851 entropy 5.370664063545807
epoch: 2, step: 41
	action: tensor([[-95.6925,  18.2926, -26.8174,  24.4320,  67.3192, 102.9336,  26.5000]],
       dtype=torch.float64)
	q_value: tensor([[-30.6033]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.44910674101675774, distance: 1.377548447437794 entropy 5.159391218571053
epoch: 2, step: 42
	action: tensor([[-10.3397, -93.6720,  -2.8500,  33.0362, -34.0169,  -7.8122,  74.1745]],
       dtype=torch.float64)
	q_value: tensor([[-41.9174]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.201754783052197
epoch: 2, step: 43
	action: tensor([[-20.7005, -11.3076,  14.1600, -25.8914,  -3.5251,  26.0134,  -5.2851]],
       dtype=torch.float64)
	q_value: tensor([[-47.4300]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2817965645435412, distance: 0.9697960979608725 entropy 4.855890214097324
epoch: 2, step: 44
	action: tensor([[-109.0354,  -59.9302,  -53.8399,   12.5865,  -29.5233,  116.7537,
           60.1026]], dtype=torch.float64)
	q_value: tensor([[-59.3543]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.47125796372337914, distance: 1.388037203299679 entropy 5.318470433786958
epoch: 2, step: 45
	action: tensor([[ -98.7353,   28.2489,    9.6046,   34.9545, -131.6899,  -50.1799,
          -38.8164]], dtype=torch.float64)
	q_value: tensor([[-50.7821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6697680991833694, distance: 1.4787160035127969 entropy 5.319067469057989
epoch: 2, step: 46
	action: tensor([[-51.4044,  18.2040,  -7.9168,  33.9120, -21.0703,  28.6188, -48.6845]],
       dtype=torch.float64)
	q_value: tensor([[-56.6440]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3909494965058722, distance: 1.3496226994448726 entropy 5.319175822123642
epoch: 2, step: 47
	action: tensor([[-150.2777,  -93.2582,  -22.6984,   41.2072,  -10.1453,  -26.5514,
           57.5820]], dtype=torch.float64)
	q_value: tensor([[-59.4927]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9108648022224839, distance: 1.5818732196178393 entropy 5.45553427805058
epoch: 2, step: 48
	action: tensor([[-71.2914, -48.9546, -46.8985,  44.2706, -17.6639,  32.6868,  20.5799]],
       dtype=torch.float64)
	q_value: tensor([[-56.0692]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.45964748231800656, distance: 1.3825494842294832 entropy 5.371928264615692
epoch: 2, step: 49
	action: tensor([[-120.0704, -104.3958,  -50.4877,   19.9534,  -31.3766,  108.4801,
          149.4379]], dtype=torch.float64)
	q_value: tensor([[-58.2970]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10451424254555652, distance: 1.2026585786305088 entropy 5.392740440991939
epoch: 2, step: 50
	action: tensor([[-83.3132, -43.3440, -41.5885, -34.9974, -14.8645, -53.6038, -10.2137]],
       dtype=torch.float64)
	q_value: tensor([[-41.5045]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.3520795754912927, distance: 1.7550225402964015 entropy 5.266953245976517
epoch: 2, step: 51
	action: tensor([[-71.5336, -57.3777,  38.4874,   9.2119,  81.2825, -77.1250, -27.5625]],
       dtype=torch.float64)
	q_value: tensor([[-53.0967]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18393521198755725, distance: 1.245147101432743 entropy 5.4927081615651465
epoch: 2, step: 52
	action: tensor([[-184.0802,  -25.5467,  -30.4787,  -56.4757,  -34.5169,    5.8778,
           -3.4869]], dtype=torch.float64)
	q_value: tensor([[-44.9261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.004039342634532872, distance: 1.146653124037356 entropy 5.372069436614325
epoch: 2, step: 53
	action: tensor([[-39.9279, -76.7247, -80.1581,  23.3387,  53.1994,  28.0071, -32.7834]],
       dtype=torch.float64)
	q_value: tensor([[-46.1921]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.414528443301342
epoch: 2, step: 54
	action: tensor([[-43.8203,   8.7840, -34.7205,   8.5715, -26.6883, -30.4011,  20.4532]],
       dtype=torch.float64)
	q_value: tensor([[-47.4300]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.855890214097324
epoch: 2, step: 55
	action: tensor([[-61.1663,  13.3625, -26.6174,  16.8898, -10.4570,  20.3862,  -3.3338]],
       dtype=torch.float64)
	q_value: tensor([[-47.4300]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11585527564432718, distance: 1.2088171942819192 entropy 4.855890214097324
epoch: 2, step: 56
	action: tensor([[-109.4459, -115.0379,  -83.8923,   17.3533,  -43.9210,   13.8324,
           10.0528]], dtype=torch.float64)
	q_value: tensor([[-57.7125]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.3778710152478744, distance: 1.7646185481330199 entropy 5.271798466994157
epoch: 2, step: 57
	action: tensor([[-55.0294, -62.1502,   3.6593,  12.4310, -43.9941, -35.2174, -55.4633]],
       dtype=torch.float64)
	q_value: tensor([[-53.6978]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8612322064434954, distance: 1.5611943565495592 entropy 5.211960672424825
epoch: 2, step: 58
	action: tensor([[-139.7315,  -15.3883,  -30.3367,   37.1937,  -16.1586,  -41.1483,
           10.5329]], dtype=torch.float64)
	q_value: tensor([[-51.3587]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1085827269718953, distance: 1.661697687754094 entropy 5.200690198991809
epoch: 2, step: 59
	action: tensor([[  19.2937, -106.3899,  -13.5230,   41.1741,  -46.4475,  -15.6803,
          -50.0827]], dtype=torch.float64)
	q_value: tensor([[-53.1770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5985091209102562, distance: 1.446819163908127 entropy 5.3605007024263065
epoch: 2, step: 60
	action: tensor([[-60.6042,  17.2651,  17.8362,  47.5891, -56.3859, -60.7157,  47.4687]],
       dtype=torch.float64)
	q_value: tensor([[-44.6257]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6587703284589941, distance: 1.4738382464999307 entropy 5.3176774138132
epoch: 2, step: 61
	action: tensor([[-73.2118,  23.6351,  43.0962,  33.5547, -24.3678, -18.1334,  26.6066]],
       dtype=torch.float64)
	q_value: tensor([[-43.9605]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2064291807977079, distance: 1.2569199223030463 entropy 5.33966881853518
epoch: 2, step: 62
	action: tensor([[-11.3709, -61.2303,   5.3509, -22.3669,   7.4002, -14.8231,  35.0619]],
       dtype=torch.float64)
	q_value: tensor([[-37.7367]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.434204424480356, distance: 1.3704469297572288 entropy 5.112427425459643
epoch: 2, step: 63
	action: tensor([[-76.7555, -25.6807,  32.4816, -34.2328, -64.8485,  41.7409, -40.4696]],
       dtype=torch.float64)
	q_value: tensor([[-43.8871]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4667779268163752, distance: 1.3859222788152106 entropy 5.243114189882937
epoch: 2, step: 64
	action: tensor([[-77.6525, -60.4271,   9.5076,  -3.8326,  56.5804,  49.6546,  38.5987]],
       dtype=torch.float64)
	q_value: tensor([[-34.7247]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.129897029122025
epoch: 2, step: 65
	action: tensor([[-41.2988, -52.9084, -19.1616,  55.9184, -32.7788, 110.2927,   6.7168]],
       dtype=torch.float64)
	q_value: tensor([[-47.4300]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5633008861797391, distance: 1.4307968663349508 entropy 4.855890214097324
epoch: 2, step: 66
	action: tensor([[-64.7963, -43.5763,  35.4484, -35.9774, -36.8426, -50.4855,  25.2684]],
       dtype=torch.float64)
	q_value: tensor([[-47.2584]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28817589425413526, distance: 0.965479460018097 entropy 4.9808354090700515
epoch: 2, step: 67
	action: tensor([[ 37.8426, -65.8557,  45.7646,  37.2681,  27.2386,  30.3561,  42.9759]],
       dtype=torch.float64)
	q_value: tensor([[-51.3565]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.05931503330636
epoch: 2, step: 68
	action: tensor([[-62.0757,  21.7693,  19.1529, -13.2237, -53.3113,  -1.8964, -13.0465]],
       dtype=torch.float64)
	q_value: tensor([[-47.4300]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.855890214097324
epoch: 2, step: 69
	action: tensor([[-33.5100, -80.5815, -41.6366,  -6.9223, -41.9026,  25.1050, -12.5539]],
       dtype=torch.float64)
	q_value: tensor([[-47.4300]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.041863250007625585, distance: 1.1201351930096506 entropy 4.855890214097324
epoch: 2, step: 70
	action: tensor([[-100.7925,  -30.7899,   63.3208,  -69.7543,  -15.2293, -104.2602,
         -117.7279]], dtype=torch.float64)
	q_value: tensor([[-55.4280]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0903602482369128, distance: 1.6545018695683675 entropy 5.242582041125564
epoch: 2, step: 71
	action: tensor([[-108.6450,  -83.4862,    7.1229,  -15.5528,  -67.7840,   21.4024,
           51.4686]], dtype=torch.float64)
	q_value: tensor([[-55.1878]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.33836338161459567, distance: 1.323865047256459 entropy 5.289839441917166
epoch: 2, step: 72
	action: tensor([[-35.8546,  -0.1218,   0.3284,   1.2062, -28.6388, -20.0717,  75.8797]],
       dtype=torch.float64)
	q_value: tensor([[-36.8427]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3322211785679847, distance: 1.3208237211859233 entropy 5.024121654068419
epoch: 2, step: 73
	action: tensor([[ 18.9978,  17.8980,  23.2643, -43.3487, -49.5876,  51.4215,  35.9870]],
       dtype=torch.float64)
	q_value: tensor([[-39.4709]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.38485915539426396, distance: 1.3466647631811264 entropy 5.093356223408223
epoch: 2, step: 74
	action: tensor([[-21.2211,  -4.2488, -20.6877,  20.8230, -42.0520,  -5.9631,  43.0067]],
       dtype=torch.float64)
	q_value: tensor([[-28.6243]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.598710608663751
epoch: 2, step: 75
	action: tensor([[-42.1700, -66.6941, -20.8324,  55.5119, -35.5442, -13.3247,   5.0674]],
       dtype=torch.float64)
	q_value: tensor([[-47.4300]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.40429023778621387, distance: 1.3560794404071894 entropy 4.855890214097324
epoch: 2, step: 76
	action: tensor([[-12.3856, -26.4416,  40.4558, -13.7942,  -0.5184,  21.4974, -79.0360]],
       dtype=torch.float64)
	q_value: tensor([[-49.7664]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.176100928072556
epoch: 2, step: 77
	action: tensor([[ -4.9814,  -6.6365,  18.8392, -21.0381, -47.3207, -49.5426,  -7.3790]],
       dtype=torch.float64)
	q_value: tensor([[-47.4300]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12042430184870434, distance: 1.2112895019852163 entropy 4.855890214097324
epoch: 2, step: 78
	action: tensor([[-76.7663, -45.5361, -32.3207,  70.2965, -60.9633, -55.7388, -39.8196]],
       dtype=torch.float64)
	q_value: tensor([[-41.5332]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.36538919200158637, distance: 1.3371647634070989 entropy 5.105870295532958
epoch: 2, step: 79
	action: tensor([[-37.3738, -60.7869,  20.3281,  57.0770, -30.0269, -44.2839, -58.6001]],
       dtype=torch.float64)
	q_value: tensor([[-37.0347]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.260859637602112, distance: 1.7206537826747559 entropy 5.030417544275258
epoch: 2, step: 80
	action: tensor([[-16.7247,   3.8372,  58.9750,  31.9149, -25.5442,   2.4296,  -2.8666]],
       dtype=torch.float64)
	q_value: tensor([[-32.9924]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.91211343852512
epoch: 2, step: 81
	action: tensor([[-50.4410, -35.3598,  16.7383,  -8.0363, -87.5084, -57.8916,  -3.1433]],
       dtype=torch.float64)
	q_value: tensor([[-47.4300]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.855890214097324
epoch: 2, step: 82
	action: tensor([[-31.3585, -76.3142, -11.0772,  31.0110,  26.1901,  54.1664,  16.2234]],
       dtype=torch.float64)
	q_value: tensor([[-47.4300]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1031518870617526, distance: 1.0837176400998432 entropy 4.855890214097324
epoch: 2, step: 83
	action: tensor([[-43.4991, -27.2528, -13.4798,  47.8095, -22.6187, -17.1965, -26.7187]],
       dtype=torch.float64)
	q_value: tensor([[-35.0663]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2955505833627745, distance: 1.7338045039646193 entropy 4.9450550829706215
epoch: 2, step: 84
	action: tensor([[-83.5631, -42.6741,  18.0572,  45.2638, -53.3350, -56.5416,  30.6735]],
       dtype=torch.float64)
	q_value: tensor([[-38.8009]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9019040903040398, distance: 1.578159883432564 entropy 5.158232653526049
epoch: 2, step: 85
	action: tensor([[ -86.6387,   -6.4866,    1.3970,   69.9725, -153.0007,   62.9159,
          -40.0702]], dtype=torch.float64)
	q_value: tensor([[-45.0142]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11967238039246775, distance: 1.210882983032724 entropy 5.378843932229498
epoch: 2, step: 86
	action: tensor([[ -87.0538, -112.2550,  -66.0920,   -5.6252,  -22.5403,  -93.2738,
          -24.5140]], dtype=torch.float64)
	q_value: tensor([[-40.5859]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7982148651848928, distance: 1.5345374262353668 entropy 5.218414203098983
epoch: 2, step: 87
	action: tensor([[-76.6058, -31.8424,   1.1430,  31.7230, -58.9497,  81.5248,  32.5974]],
       dtype=torch.float64)
	q_value: tensor([[-49.4961]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6332839422942429, distance: 1.4624719307766119 entropy 5.275752289118835
epoch: 2, step: 88
	action: tensor([[-58.3858, -62.7061,   9.5607,  18.2196, -70.9283, -80.1386,  34.0373]],
       dtype=torch.float64)
	q_value: tensor([[-36.7710]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.6490078382912494, distance: 1.8625087209362325 entropy 5.143862136362702
epoch: 2, step: 89
	action: tensor([[ -25.7520, -123.3391,   78.8125,   -9.4830,    2.8836,   10.9267,
           83.1414]], dtype=torch.float64)
	q_value: tensor([[-60.1266]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.518337441405692
epoch: 2, step: 90
	action: tensor([[-32.8791, -81.7788, -25.7217,   8.4800,   9.4325,  20.9223,  69.3531]],
       dtype=torch.float64)
	q_value: tensor([[-47.4300]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6846114409351389, distance: 1.4852739571943336 entropy 4.855890214097324
epoch: 2, step: 91
	action: tensor([[-23.9550, -51.2755,  24.8507,  -0.4818,  43.8692,  -8.2683, -22.8310]],
       dtype=torch.float64)
	q_value: tensor([[-49.4376]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8202091242362728, distance: 1.543893492953758 entropy 5.252402074525189
epoch: 2, step: 92
	action: tensor([[ -8.4185, -40.6781,  66.0921,  36.8822,  -8.9653,  18.7431, -63.0652]],
       dtype=torch.float64)
	q_value: tensor([[-34.6237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.6937498569076208, distance: 1.8781718431852394 entropy 4.9611822525515015
epoch: 2, step: 93
	action: tensor([[  40.5138, -132.1701,  -59.9254,   95.5123,    2.0286,   17.4468,
           14.1792]], dtype=torch.float64)
	q_value: tensor([[-46.8267]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.48784776924295126, distance: 1.3958409731911214 entropy 5.310043825465714
epoch: 2, step: 94
	action: tensor([[ -36.1162, -111.3999,    4.0276,   68.1116,    3.5260,   12.3686,
            4.7095]], dtype=torch.float64)
	q_value: tensor([[-46.5805]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23955322584491445, distance: 1.2740582465041554 entropy 5.251481040642801
epoch: 2, step: 95
	action: tensor([[-45.0045, -28.9974, 107.6822, -24.0691, -31.1040,  27.7176, 116.2836]],
       dtype=torch.float64)
	q_value: tensor([[-42.4418]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.006146901732905219, distance: 1.1408217466902977 entropy 5.394259789769005
epoch: 2, step: 96
	action: tensor([[-75.9687, -14.4547,  -0.2271, 112.1375,  -9.6397, -29.3040,  43.5240]],
       dtype=torch.float64)
	q_value: tensor([[-34.3101]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6046592830557276, distance: 1.4495997642731897 entropy 5.150201465862394
epoch: 2, step: 97
	action: tensor([[ -27.9868,  -45.2112,   52.3531,   49.1364, -129.8741,   73.5206,
          -86.4598]], dtype=torch.float64)
	q_value: tensor([[-40.3587]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.158836177560163, distance: 1.2318780467148143 entropy 5.357809650446476
epoch: 2, step: 98
	action: tensor([[-73.6082, -54.9378,   2.4687,   2.3208, -10.1329,  46.3071,  -1.2687]],
       dtype=torch.float64)
	q_value: tensor([[-42.9727]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.249550489577416
epoch: 2, step: 99
	action: tensor([[-28.3584, -65.7099,  17.5081,  60.8735, -22.4094, -66.0063,   6.0876]],
       dtype=torch.float64)
	q_value: tensor([[-47.4300]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5953982221761387, distance: 1.4454106327297613 entropy 4.855890214097324
epoch: 2, step: 100
	action: tensor([[-63.0435, -39.4445,  19.8337,  10.9922,   4.6792,  18.4276,  21.5227]],
       dtype=torch.float64)
	q_value: tensor([[-45.5054]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8060940639829637, distance: 1.5378956761252678 entropy 5.109352101796422
epoch: 2, step: 101
	action: tensor([[-54.5918, -13.9135,  14.6020,  22.2558, -64.6713, -29.6360,  21.1881]],
       dtype=torch.float64)
	q_value: tensor([[-29.0608]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.048309965732645344, distance: 1.171659856868849 entropy 4.926645418487325
epoch: 2, step: 102
	action: tensor([[-43.0219,  23.6131,  40.9683,  70.1627, -59.6021,  -6.0205,  60.4291]],
       dtype=torch.float64)
	q_value: tensor([[-46.4373]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7408415343772574, distance: 1.5098587258299998 entropy 5.421301531068697
epoch: 2, step: 103
	action: tensor([[ -99.6929, -126.2846,  -41.4806,  -25.1971,  -47.1772,   35.5823,
           75.5184]], dtype=torch.float64)
	q_value: tensor([[-54.3754]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2100320442031811, distance: 1.701202368332655 entropy 5.581236503242623
epoch: 2, step: 104
	action: tensor([[ -83.3793,  -79.9821,   14.2304,   -9.9409, -112.3962,   50.1695,
          -27.4345]], dtype=torch.float64)
	q_value: tensor([[-44.4371]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.279822655228736
epoch: 2, step: 105
	action: tensor([[-44.5928, -64.5265, -25.6580,  46.8598, -27.0217,  -8.5658,  24.8174]],
       dtype=torch.float64)
	q_value: tensor([[-47.4300]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1119383687522576, distance: 1.0783959462185384 entropy 4.855890214097324
epoch: 2, step: 106
	action: tensor([[-79.5855, -82.5113, -28.5628,   3.3468,  -6.7883,  66.8576, -18.6877]],
       dtype=torch.float64)
	q_value: tensor([[-40.4269]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.045283062018288
epoch: 2, step: 107
	action: tensor([[-41.4228,  15.8124,  24.7418,  12.6654, -61.2949,  18.8225,  28.9010]],
       dtype=torch.float64)
	q_value: tensor([[-47.4300]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.855890214097324
epoch: 2, step: 108
	action: tensor([[-57.4344, -22.8365,   1.5938, -33.3023,   8.7903, -64.3646, -25.2478]],
       dtype=torch.float64)
	q_value: tensor([[-47.4300]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.855890214097324
epoch: 2, step: 109
	action: tensor([[-41.1389, -25.1288, -31.1353, -70.0060,  15.2296,  38.7787,  49.1369]],
       dtype=torch.float64)
	q_value: tensor([[-47.4300]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.014737741800417647, distance: 1.1358804287575963 entropy 4.855890214097324
epoch: 2, step: 110
	action: tensor([[-57.8559,  13.2689,  32.3758,  73.0319,   8.9690, -16.8874,  -1.9116]],
       dtype=torch.float64)
	q_value: tensor([[-44.1579]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.3600614170704475, distance: 1.7579978749241942 entropy 5.085702257941053
epoch: 2, step: 111
	action: tensor([[-44.1383,   0.2103,  18.5614,   2.4390, -40.7984, -52.7745, -80.7064]],
       dtype=torch.float64)
	q_value: tensor([[-44.3468]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2852387450235834, distance: 0.9674693041410487 entropy 5.095149324946699
epoch: 2, step: 112
	action: tensor([[-51.3391,  10.5585, -50.3802,  26.4627, -33.1644, 110.0266, -16.6411]],
       dtype=torch.float64)
	q_value: tensor([[-45.0867]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.382917019647095
epoch: 2, step: 113
	action: tensor([[-16.7887,   6.5909, -25.3036,  20.6184, -38.2892,  10.7410,  52.6837]],
       dtype=torch.float64)
	q_value: tensor([[-47.4300]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.855890214097324
epoch: 2, step: 114
	action: tensor([[-41.6459, -17.0442,  48.8796,   3.2014,  -6.5090, -35.1059, -20.2345]],
       dtype=torch.float64)
	q_value: tensor([[-47.4300]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.855890214097324
epoch: 2, step: 115
	action: tensor([[-84.2745, -20.3581, -32.3274,  64.8559,  -9.5510, -14.4874, -63.8672]],
       dtype=torch.float64)
	q_value: tensor([[-47.4300]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.179745788386942, distance: 1.6895055270920565 entropy 4.855890214097324
epoch: 2, step: 116
	action: tensor([[-50.5118,  -0.3868,  51.9085,   1.7450, -29.9164, -16.0663,  59.5500]],
       dtype=torch.float64)
	q_value: tensor([[-41.4962]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2948831514585277, distance: 1.3021828859114242 entropy 5.105337369985203
epoch: 2, step: 117
	action: tensor([[-24.7233, -36.0379,  -6.6166,  49.2093,   8.3667, -33.2638,  13.7856]],
       dtype=torch.float64)
	q_value: tensor([[-47.2009]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2557329413013303, distance: 1.282346340695118 entropy 5.296415619358025
epoch: 2, step: 118
	action: tensor([[-117.9891,  -32.5280,   81.4198,  -24.6082,   84.1732,  -30.0149,
          -16.2955]], dtype=torch.float64)
	q_value: tensor([[-50.6640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5615374569333225, distance: 1.4299896573738171 entropy 5.31963079949796
epoch: 2, step: 119
	action: tensor([[-164.1978,   17.8798,  -11.4464,   -9.8762,   23.5374,   -7.7248,
            5.3766]], dtype=torch.float64)
	q_value: tensor([[-50.9900]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.341479006364625
epoch: 2, step: 120
	action: tensor([[ 20.2195, -14.7364,  48.8737,  14.4951, -24.1823,  40.1640, -55.1633]],
       dtype=torch.float64)
	q_value: tensor([[-47.4300]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3472908765519749, distance: 0.9245204774024146 entropy 4.855890214097324
epoch: 2, step: 121
	action: tensor([[-15.9726, -50.8943,   5.4207,   1.9863, -65.2658, -11.4430,   4.0841]],
       dtype=torch.float64)
	q_value: tensor([[-36.4219]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.954659457004202
epoch: 2, step: 122
	action: tensor([[-29.1669,  15.9395,  -3.1267,   1.8068,  -2.8095,  50.9445,  27.3795]],
       dtype=torch.float64)
	q_value: tensor([[-47.4300]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.855890214097324
epoch: 2, step: 123
	action: tensor([[-75.7198,  16.1208,  25.2746, -21.9858, -11.0459,  42.7206,  39.9445]],
       dtype=torch.float64)
	q_value: tensor([[-47.4300]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.855890214097324
epoch: 2, step: 124
	action: tensor([[-34.5175, -78.8104, -29.5983, -52.3797, -34.5104,  29.8880,   5.2532]],
       dtype=torch.float64)
	q_value: tensor([[-47.4300]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15365303344066716, distance: 1.0527637302757733 entropy 4.855890214097324
epoch: 2, step: 125
	action: tensor([[-109.6241,  -37.8057,   23.8807,   33.4796,   40.9040,   34.3525,
          114.1252]], dtype=torch.float64)
	q_value: tensor([[-62.8244]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06479457996564797, distance: 1.1808360658460342 entropy 5.389026989883762
epoch: 2, step: 126
	action: tensor([[-132.6850,  -38.6197,  -99.2309,    8.2401,    2.4104,  -30.5694,
          -46.4740]], dtype=torch.float64)
	q_value: tensor([[-59.5342]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0258618834147595, distance: 1.6287769320681185 entropy 5.327130857392037
epoch: 2, step: 127
	action: tensor([[  5.0640, -36.5470, -11.9283, -24.5580, -54.5169,  34.8290,  -5.9528]],
       dtype=torch.float64)
	q_value: tensor([[-33.7941]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5436602795700272, distance: 1.421780514519952 entropy 5.083150427014644
LOSS epoch 2 actor 617.1013532401712 critic 80.40008368320427
epoch: 3, step: 0
	action: tensor([[-102.4910,  -28.6560,   -5.2330,  -43.4290,   27.8304,  -43.2043,
           -8.6522]], dtype=torch.float64)
	q_value: tensor([[-29.8183]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8957927126901706, distance: 1.5756222971933704 entropy 5.21863317384237
epoch: 3, step: 1
	action: tensor([[-72.2609, -74.4366,  48.4011, -57.0642,   8.3863,  -0.6186,  25.3631]],
       dtype=torch.float64)
	q_value: tensor([[-32.6935]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9423352575574482, distance: 1.594846134368547 entropy 5.308893010796064
epoch: 3, step: 2
	action: tensor([[-33.0625, -30.3190, -10.1921,  47.9226,   5.4549,  12.5946, -73.9872]],
       dtype=torch.float64)
	q_value: tensor([[-31.4670]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.148990887825417
epoch: 3, step: 3
	action: tensor([[-90.4483, -77.2445,  15.3574, -24.5958,   0.9497, -20.5578, -50.9673]],
       dtype=torch.float64)
	q_value: tensor([[-38.9889]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18277927465083765, distance: 1.2445391021520276 entropy 5.073594036763349
epoch: 3, step: 4
	action: tensor([[-37.8909,  47.8250,  98.0782, -63.1241,  39.8261, 110.7712, -38.7369]],
       dtype=torch.float64)
	q_value: tensor([[-34.0778]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.001580952306707406, distance: 1.1434393193498804 entropy 5.312885001594645
epoch: 3, step: 5
	action: tensor([[-50.3084, -36.5662, -69.9972,  -7.7510, -66.4760, -21.3757,  42.3705]],
       dtype=torch.float64)
	q_value: tensor([[-42.3760]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2800930994256958, distance: 1.2947248138656653 entropy 5.421159713506496
epoch: 3, step: 6
	action: tensor([[-67.3273, -23.4914, -39.1113, -18.6904,   2.3905, -78.5110,  11.3671]],
       dtype=torch.float64)
	q_value: tensor([[-45.0913]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4769403354516506, distance: 1.3907150961291788 entropy 5.486069286101714
epoch: 3, step: 7
	action: tensor([[-29.4896, -30.6615, -62.8418, -53.9901,  61.2516,  18.7343,   0.6929]],
       dtype=torch.float64)
	q_value: tensor([[-46.1334]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7792386210432978, distance: 1.526419100285622 entropy 5.509159250540938
epoch: 3, step: 8
	action: tensor([[-44.0658, -53.9020, -61.8034,  68.5643, -37.6980,  76.3904,  39.5173]],
       dtype=torch.float64)
	q_value: tensor([[-42.3698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.209669323709861, distance: 1.701062758075716 entropy 5.270572861199654
epoch: 3, step: 9
	action: tensor([[-83.7862, -17.3111,   8.6570,  70.8914, -92.6995,   5.2273, -45.4336]],
       dtype=torch.float64)
	q_value: tensor([[-29.6713]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.173085264615774
epoch: 3, step: 10
	action: tensor([[-65.8437, -25.7954,   5.2897,  42.3076, -34.8188, -17.9606, -10.2858]],
       dtype=torch.float64)
	q_value: tensor([[-38.9889]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.455570014290195, distance: 1.7932170272154488 entropy 5.073594036763349
epoch: 3, step: 11
	action: tensor([[ 22.4906, -71.7911, -67.8664, -38.3359, -18.7746, -14.8269, -49.7442]],
       dtype=torch.float64)
	q_value: tensor([[-36.2137]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7631298785362837, distance: 1.5194934973445058 entropy 5.308727191410476
epoch: 3, step: 12
	action: tensor([[-22.0967, -24.1262, -21.3317,  20.9438, -34.2434,  72.8765, -37.0057]],
       dtype=torch.float64)
	q_value: tensor([[-22.2240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.5162388442383183, distance: 1.8152340294605125 entropy 4.881644850245453
epoch: 3, step: 13
	action: tensor([[-71.3600, -37.3325, -59.5887, -54.7734,  31.3626,  51.2717, -81.6045]],
       dtype=torch.float64)
	q_value: tensor([[-34.2230]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6418143309952751, distance: 1.4662860891023266 entropy 5.391753299205999
epoch: 3, step: 14
	action: tensor([[  7.7145, -79.2822,  52.6149,  60.4534,  -1.1204, -83.2152,  -7.1892]],
       dtype=torch.float64)
	q_value: tensor([[-38.1353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11138326750882244, distance: 1.2063924768627736 entropy 5.349131083952665
epoch: 3, step: 15
	action: tensor([[  9.7500, -73.7731, -82.1941,  18.4887,  13.9108, -76.6515,  18.8190]],
       dtype=torch.float64)
	q_value: tensor([[-33.6240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.260843375390244
epoch: 3, step: 16
	action: tensor([[-85.7344,  -0.1808, -26.8543,  -4.7955,   3.1162,   2.6086, -22.9500]],
       dtype=torch.float64)
	q_value: tensor([[-38.9889]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.073594036763349
epoch: 3, step: 17
	action: tensor([[-61.9728, -25.0439, -10.1668,  -3.3828,   2.1981, -64.3267, -38.3454]],
       dtype=torch.float64)
	q_value: tensor([[-38.9889]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.073594036763349
epoch: 3, step: 18
	action: tensor([[ 13.4626, -75.5888, -61.1020,  28.2729, -30.0744,  28.1944, -29.2196]],
       dtype=torch.float64)
	q_value: tensor([[-38.9889]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.073594036763349
epoch: 3, step: 19
	action: tensor([[-85.3571, -35.3933,  -9.0311,   0.7761,  18.0882,   4.4195,  83.4965]],
       dtype=torch.float64)
	q_value: tensor([[-38.9889]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.073594036763349
epoch: 3, step: 20
	action: tensor([[-15.3057, -74.1531,  31.1429,  17.0327, -34.4958, -35.5158, -30.1552]],
       dtype=torch.float64)
	q_value: tensor([[-38.9889]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16227493750354038, distance: 1.0473876471759787 entropy 5.073594036763349
epoch: 3, step: 21
	action: tensor([[ 29.7663, -47.1095, -15.8109,  -6.2323, -16.9473, 116.8306,  30.5690]],
       dtype=torch.float64)
	q_value: tensor([[-42.6908]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.487219973208
epoch: 3, step: 22
	action: tensor([[-12.1742, -36.5880,  46.4870, -52.8892, -49.8260, -10.3273,  13.6123]],
       dtype=torch.float64)
	q_value: tensor([[-38.9889]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.073594036763349
epoch: 3, step: 23
	action: tensor([[ -97.6086, -112.3178,  -50.9214,  -21.7261,  -63.9302,   32.1722,
         -102.2757]], dtype=torch.float64)
	q_value: tensor([[-38.9889]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.073594036763349
epoch: 3, step: 24
	action: tensor([[-33.0830, -44.7651,  -7.2143, -42.2048,  37.4787, -24.8595, -10.7735]],
       dtype=torch.float64)
	q_value: tensor([[-38.9889]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4357782742067845, distance: 0.8595706849438475 entropy 5.073594036763349
epoch: 3, step: 25
	action: tensor([[-57.9211, -18.6188, -12.6139, -21.5703, -59.2701, -30.3148, -16.3672]],
       dtype=torch.float64)
	q_value: tensor([[-35.1150]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.241381800742517
epoch: 3, step: 26
	action: tensor([[-77.9778, -56.8442,  63.1611, -10.2616, -12.9323,  -7.3779,  33.6843]],
       dtype=torch.float64)
	q_value: tensor([[-38.9889]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.073594036763349
epoch: 3, step: 27
	action: tensor([[-60.0238, -51.3478, -21.8049, -13.6333, -47.1018, -52.0992,   3.4110]],
       dtype=torch.float64)
	q_value: tensor([[-38.9889]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.073594036763349
epoch: 3, step: 28
	action: tensor([[-42.5409,  15.3946,  37.9551,  50.9391, -38.7626,  22.4289,  17.8223]],
       dtype=torch.float64)
	q_value: tensor([[-38.9889]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.073594036763349
epoch: 3, step: 29
	action: tensor([[-51.3432, -53.2414,  19.6865, -48.9598,   2.5589,  73.2340,  44.4598]],
       dtype=torch.float64)
	q_value: tensor([[-38.9889]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25357961726078115, distance: 0.9886633576959677 entropy 5.073594036763349
epoch: 3, step: 30
	action: tensor([[ -96.0390, -108.7291,    1.2352,   28.1395,   53.0168, -115.7841,
          -69.7118]], dtype=torch.float64)
	q_value: tensor([[-37.7381]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14412052981344958, distance: 1.224031466686281 entropy 5.418200668699417
epoch: 3, step: 31
	action: tensor([[-149.8689, -109.3815,  -18.1580,   37.7741,   90.4075, -176.4703,
         -100.1187]], dtype=torch.float64)
	q_value: tensor([[-36.4690]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.44623730848614684, distance: 1.3761839030188732 entropy 5.44344457287722
epoch: 3, step: 32
	action: tensor([[-97.2273, -71.2885,  16.3841, -14.7468,   8.3748,   6.4251, -53.2693]],
       dtype=torch.float64)
	q_value: tensor([[-30.5628]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4024052147698769, distance: 0.8846268010187016 entropy 5.284979530617771
epoch: 3, step: 33
	action: tensor([[-74.8382,  66.2084, -15.9698, 154.9621, -30.2995, 167.8399, -63.4163]],
       dtype=torch.float64)
	q_value: tensor([[-37.6385]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7146612976654672, distance: 1.4984624551997097 entropy 5.481362866944542
epoch: 3, step: 34
	action: tensor([[-28.9475, -54.8295,   4.7506,   7.6717, -43.5640,  94.0062, 186.9235]],
       dtype=torch.float64)
	q_value: tensor([[-46.2450]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0156150520690383, distance: 1.6246525244512413 entropy 5.482277973478422
epoch: 3, step: 35
	action: tensor([[-68.3391,  -3.2682,  37.1011,  76.3653, -16.7230, -67.0683, -50.5889]],
       dtype=torch.float64)
	q_value: tensor([[-31.6782]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.115867189185958
epoch: 3, step: 36
	action: tensor([[-53.9695, -69.2431, -10.1439,  13.6023,  10.0341,  40.6111,   0.1752]],
       dtype=torch.float64)
	q_value: tensor([[-38.9889]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7112406464381982, distance: 1.4969670355577192 entropy 5.073594036763349
epoch: 3, step: 37
	action: tensor([[ -15.4463,  -29.4070,  -44.2566,   60.8292, -116.1030,   63.7555,
          -28.7614]], dtype=torch.float64)
	q_value: tensor([[-40.6486]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6020639755613273, distance: 1.4484270294779076 entropy 5.571675069033124
epoch: 3, step: 38
	action: tensor([[-120.8314, -106.5198,  -24.0191,  -72.6272,  -13.8820,   85.3860,
           92.8318]], dtype=torch.float64)
	q_value: tensor([[-40.5703]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.021472171256389117, distance: 1.131991807820781 entropy 5.676425228901253
epoch: 3, step: 39
	action: tensor([[-100.4639,  -28.9811,   28.0244,   96.7505,   21.5942,   14.2743,
           34.2521]], dtype=torch.float64)
	q_value: tensor([[-37.3742]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.007573885622923049, distance: 1.1486696456901302 entropy 5.58294222006541
epoch: 3, step: 40
	action: tensor([[  -6.3093,  127.3797,   -6.8046,   88.7377,  -70.6371, -139.0327,
          143.7388]], dtype=torch.float64)
	q_value: tensor([[-48.4757]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.709218692956087
epoch: 3, step: 41
	action: tensor([[-98.0091, -40.0819,  24.7726,  36.9561,  46.3551, -28.5325, -18.6053]],
       dtype=torch.float64)
	q_value: tensor([[-38.9889]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.073594036763349
epoch: 3, step: 42
	action: tensor([[-43.9944, -40.7430, -21.0042,  65.4985, -25.8159,   6.0521,  48.6635]],
       dtype=torch.float64)
	q_value: tensor([[-38.9889]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2019078624418389, distance: 1.2545624405409364 entropy 5.073594036763349
epoch: 3, step: 43
	action: tensor([[ 71.9240, -10.3697,  26.3164, -14.1133,  28.9003, -72.3328,  12.2481]],
       dtype=torch.float64)
	q_value: tensor([[-29.7818]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.465482946212239
epoch: 3, step: 44
	action: tensor([[-19.3023, -38.7913,  43.3474, -51.1533, -14.0487, -20.9240,   3.0128]],
       dtype=torch.float64)
	q_value: tensor([[-38.9889]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24022926078481566, distance: 0.9974657106969212 entropy 5.073594036763349
epoch: 3, step: 45
	action: tensor([[-140.3813,  -56.3481,  -28.9439,  116.6713,  -15.4026,   31.0440,
          -46.4903]], dtype=torch.float64)
	q_value: tensor([[-35.3241]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2506883548527492, distance: 1.7167789325607277 entropy 5.5164650032988645
epoch: 3, step: 46
	action: tensor([[-135.1288,  -69.4411,   50.8073,    3.2552,   11.2686,  -27.1919,
           83.4719]], dtype=torch.float64)
	q_value: tensor([[-41.7480]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.614837097104193
epoch: 3, step: 47
	action: tensor([[-74.3136,  -0.6900, -31.5980, -13.8451,  72.9356, -72.2680,  11.0161]],
       dtype=torch.float64)
	q_value: tensor([[-38.9889]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7952013490039982, distance: 1.5332510692779222 entropy 5.073594036763349
epoch: 3, step: 48
	action: tensor([[-105.2293,    7.1865,  -19.5295,  -76.2797,  -31.6930,  -33.3749,
           13.7416]], dtype=torch.float64)
	q_value: tensor([[-43.3871]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21354738961813902, distance: 1.014829241094935 entropy 5.484468549469531
epoch: 3, step: 49
	action: tensor([[-132.7751, -136.0285,   32.8495,   21.2227,  -58.1149,   -3.9523,
          -25.9928]], dtype=torch.float64)
	q_value: tensor([[-41.7242]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.310406093446555
epoch: 3, step: 50
	action: tensor([[-59.7101,  -6.0355,  -3.1081, 102.5004, -71.7260,  16.9963, -28.5534]],
       dtype=torch.float64)
	q_value: tensor([[-38.9889]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.073594036763349
epoch: 3, step: 51
	action: tensor([[-41.6022, -29.4486,  13.6627,  42.9768,   0.1519,  62.1927,  23.0980]],
       dtype=torch.float64)
	q_value: tensor([[-38.9889]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.012188578252263937, distance: 1.1512970965945013 entropy 5.073594036763349
epoch: 3, step: 52
	action: tensor([[ -79.0449, -145.8095,   -7.7515,   14.8190,   36.8207,   12.9487,
           26.5929]], dtype=torch.float64)
	q_value: tensor([[-37.1781]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.4096914674710521, distance: 1.776386292174762 entropy 5.21921994162027
epoch: 3, step: 53
	action: tensor([[-14.3116,  22.4470,  51.9330, -13.6056, -88.7237,  38.5856,   1.7955]],
       dtype=torch.float64)
	q_value: tensor([[-37.2570]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16433945745376355, distance: 1.0460962410337507 entropy 5.445830206407886
epoch: 3, step: 54
	action: tensor([[-64.1541, -60.1676,  -7.4757, 119.4798, -10.3749, -74.8441,   3.5268]],
       dtype=torch.float64)
	q_value: tensor([[-36.8893]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.481866035353965, distance: 1.393032232116597 entropy 5.41525210384846
epoch: 3, step: 55
	action: tensor([[  -3.7463,  -73.0780, -156.1816,   44.8803,  -51.6284,    1.9344,
           66.5204]], dtype=torch.float64)
	q_value: tensor([[-43.1298]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.51339189431337
epoch: 3, step: 56
	action: tensor([[-98.5943, -76.0389,  29.3956,  16.7247,  18.6489, -13.5872, -49.4709]],
       dtype=torch.float64)
	q_value: tensor([[-38.9889]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.19746996674341077, distance: 1.2522441404545597 entropy 5.073594036763349
epoch: 3, step: 57
	action: tensor([[  7.6397, -30.9900,   0.0714,  67.3638,   8.6784,  29.7014,   3.6330]],
       dtype=torch.float64)
	q_value: tensor([[-37.8974]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.275776663054593
epoch: 3, step: 58
	action: tensor([[-27.8137, -39.6365, -39.6497, -51.6557, -13.9394,  14.9888, -16.2268]],
       dtype=torch.float64)
	q_value: tensor([[-38.9889]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.47829306016117745, distance: 1.3913518260185782 entropy 5.073594036763349
epoch: 3, step: 59
	action: tensor([[  -8.5675,   36.0552,  -40.1243,   46.1165,   15.7823, -131.4192,
           24.6592]], dtype=torch.float64)
	q_value: tensor([[-44.7455]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11740112453779439, distance: 1.2096542212818524 entropy 5.415589755229874
epoch: 3, step: 60
	action: tensor([[-92.4741, -32.0655, -16.0562,  99.4113,  23.0224,   1.6982, 140.3720]],
       dtype=torch.float64)
	q_value: tensor([[-52.8873]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6237633831216363, distance: 1.45820326078137 entropy 5.568953178290975
epoch: 3, step: 61
	action: tensor([[ -22.8996,  -15.4648,  -27.2080,   13.8080,  -44.7706, -103.4890,
           68.0606]], dtype=torch.float64)
	q_value: tensor([[-38.4577]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.50662308452914
epoch: 3, step: 62
	action: tensor([[-65.5276, -48.2233, -41.6816,  50.6328,  42.2661,  44.8315, -40.5272]],
       dtype=torch.float64)
	q_value: tensor([[-38.9889]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.075118139469742, distance: 1.18654656924266 entropy 5.073594036763349
epoch: 3, step: 63
	action: tensor([[  16.0262,  -80.2592,   71.2132,   -2.7376, -141.9112,  -24.2091,
           18.5321]], dtype=torch.float64)
	q_value: tensor([[-49.9043]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.508084581202672
epoch: 3, step: 64
	action: tensor([[ -2.3220, -41.9379, -13.3480, -15.2137,   4.7915,  20.6999,  61.9403]],
       dtype=torch.float64)
	q_value: tensor([[-38.9889]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.073594036763349
epoch: 3, step: 65
	action: tensor([[-15.3967, -75.6112,   0.2196,  20.6075, -36.2655, -23.0332,  18.8716]],
       dtype=torch.float64)
	q_value: tensor([[-38.9889]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.68585269540515, distance: 1.4858210458042642 entropy 5.073594036763349
epoch: 3, step: 66
	action: tensor([[-58.2679,  56.1690,  39.6555,  14.3345, -93.5273,  -5.1711, -30.0441]],
       dtype=torch.float64)
	q_value: tensor([[-23.7451]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.9362045479704735
epoch: 3, step: 67
	action: tensor([[-69.1523,   8.5312,  18.2468, -18.5027, -25.7406,  26.2288,   3.8453]],
       dtype=torch.float64)
	q_value: tensor([[-38.9889]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.073594036763349
epoch: 3, step: 68
	action: tensor([[-68.5475, -60.3581,  44.9405,  44.6418, -29.4355,  -4.4374, -84.4110]],
       dtype=torch.float64)
	q_value: tensor([[-38.9889]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.073594036763349
epoch: 3, step: 69
	action: tensor([[ -55.9827,  -71.4651,   17.6888,   87.7790,  -47.3449, -121.6845,
          -46.1642]], dtype=torch.float64)
	q_value: tensor([[-38.9889]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4960106290128985, distance: 1.3996647746344955 entropy 5.073594036763349
epoch: 3, step: 70
	action: tensor([[-82.3077,   9.3863, -21.2786,  74.7613, -52.5099, 128.2935, -47.1740]],
       dtype=torch.float64)
	q_value: tensor([[-39.3640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.385241901219865
epoch: 3, step: 71
	action: tensor([[-69.8449, -20.3042, -20.2182, -24.9209, -67.8671,  69.9746,  19.5046]],
       dtype=torch.float64)
	q_value: tensor([[-38.9889]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08786726127845546, distance: 1.1935610747078935 entropy 5.073594036763349
epoch: 3, step: 72
	action: tensor([[  6.1559, -68.4392,  26.8856, -60.9667,  -8.4515,  74.8756,  26.9709]],
       dtype=torch.float64)
	q_value: tensor([[-35.3795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5338922816198503, distance: 0.7812676477202442 entropy 5.239066268225192
epoch: 3, step: 73
	action: tensor([[ -99.2016, -222.5414,   33.7680,  122.7594,  -11.4576,  118.4322,
          -30.8788]], dtype=torch.float64)
	q_value: tensor([[-50.8415]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6825711445282263, distance: 1.4843742490603995 entropy 5.684121746417712
epoch: 3, step: 74
	action: tensor([[-136.1745,  -91.0585,  -22.5719,   -0.7138,   61.8861,    3.0595,
         -210.2580]], dtype=torch.float64)
	q_value: tensor([[-34.0557]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.403665143284136
epoch: 3, step: 75
	action: tensor([[  3.8189, -42.6857,  30.6575,   9.2447,  21.9725,   8.3576,  18.5026]],
       dtype=torch.float64)
	q_value: tensor([[-38.9889]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.073594036763349
epoch: 3, step: 76
	action: tensor([[-61.7142,  -7.9462,  30.4746, -23.9571,  14.5143, -38.9642,  -7.0026]],
       dtype=torch.float64)
	q_value: tensor([[-38.9889]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.073594036763349
epoch: 3, step: 77
	action: tensor([[-31.3256,  21.4706,  -3.0565, -59.4472,  11.5326,  40.8194,  21.5807]],
       dtype=torch.float64)
	q_value: tensor([[-38.9889]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.073594036763349
epoch: 3, step: 78
	action: tensor([[-43.4681, -41.2549,  -3.5750,  26.0816, -13.1305,  46.6501,  -5.6865]],
       dtype=torch.float64)
	q_value: tensor([[-38.9889]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1194391972761012, distance: 1.0738320648785997 entropy 5.073594036763349
epoch: 3, step: 79
	action: tensor([[-114.3053,   -5.1452,  -65.6758,  -17.6773,  -80.7488,    4.9343,
          -58.9873]], dtype=torch.float64)
	q_value: tensor([[-33.9901]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.330811861916944
epoch: 3, step: 80
	action: tensor([[-48.7191, -70.4089,  33.9553,  11.2281,  -2.7293,  58.6586,  22.9774]],
       dtype=torch.float64)
	q_value: tensor([[-38.9889]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04674547506433657, distance: 1.1707852418258595 entropy 5.073594036763349
epoch: 3, step: 81
	action: tensor([[ -45.9971, -107.8717,  179.3683,  -19.5434,  -21.3572,   56.0546,
           27.9523]], dtype=torch.float64)
	q_value: tensor([[-54.5913]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15449617279501326, distance: 1.052239212706322 entropy 5.597270696853012
epoch: 3, step: 82
	action: tensor([[-144.8869,  -30.7404,  -30.3389,   -4.6311,  -88.3020,   53.4627,
           17.8774]], dtype=torch.float64)
	q_value: tensor([[-54.3486]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.721854210784628
epoch: 3, step: 83
	action: tensor([[-18.1164, -32.5919,  38.0546, -28.5767, -13.8696, -25.1885,  42.2486]],
       dtype=torch.float64)
	q_value: tensor([[-38.9889]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2061262734289161, distance: 1.0196060595364178 entropy 5.073594036763349
epoch: 3, step: 84
	action: tensor([[ -36.6257, -112.1795,  -42.2722,  -33.3348,    2.7120,   27.0702,
          102.2211]], dtype=torch.float64)
	q_value: tensor([[-39.2985]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14537088345532245, distance: 1.2247001262575203 entropy 5.306531988451583
epoch: 3, step: 85
	action: tensor([[-114.8794,  -92.0321,  -19.3152,   47.5855,   10.0297,   27.1826,
          117.2415]], dtype=torch.float64)
	q_value: tensor([[-41.3469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.006649635501302598, distance: 1.1405331717670326 entropy 5.581458604679374
epoch: 3, step: 86
	action: tensor([[-162.1965,   53.3059,   -7.6260,   35.8073,    8.2929,  -51.7173,
          -40.9463]], dtype=torch.float64)
	q_value: tensor([[-48.1338]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09067074207799242, distance: 1.1950980143102303 entropy 5.631293713702626
epoch: 3, step: 87
	action: tensor([[-104.2785,  -72.6657,  -12.2509,   85.9953,  -62.3096,  -46.2918,
          -22.8193]], dtype=torch.float64)
	q_value: tensor([[-44.6708]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7259376955169403, distance: 1.5033816683358798 entropy 5.523566793494787
epoch: 3, step: 88
	action: tensor([[-42.6073, -38.1750,  69.6079,  82.1179, -51.3150, -45.2146,  26.6813]],
       dtype=torch.float64)
	q_value: tensor([[-30.2109]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6989537800164638, distance: 1.4915831729702365 entropy 5.286068096512534
epoch: 3, step: 89
	action: tensor([[-55.0737, -29.1691,  26.0677,  52.0833,  64.0419, -87.2898,  25.8050]],
       dtype=torch.float64)
	q_value: tensor([[-29.4959]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9887299776450478, distance: 1.6137810201959408 entropy 5.263757264907655
epoch: 3, step: 90
	action: tensor([[-36.7007, -41.3260, -31.6266,  56.4106, -41.6761,  48.9667,  35.4094]],
       dtype=torch.float64)
	q_value: tensor([[-31.1513]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7211315223128634, distance: 1.5012869954232289 entropy 5.287504615874444
epoch: 3, step: 91
	action: tensor([[  16.3967,  -60.0303,  -40.1453,   29.0463, -140.5448,   28.4508,
           80.2099]], dtype=torch.float64)
	q_value: tensor([[-28.1063]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7960446388730995, distance: 1.5336111467943503 entropy 5.393994510096824
epoch: 3, step: 92
	action: tensor([[-84.1006,  67.9373, -41.8519, -19.4918, -54.9491,  17.9310, -49.9542]],
       dtype=torch.float64)
	q_value: tensor([[-30.9524]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.7515177413435725, distance: 1.8982038590313184 entropy 5.566936466748133
epoch: 3, step: 93
	action: tensor([[-43.9599, -47.6784,  -5.1448, -76.9342, -57.3288,  11.7972, -81.1649]],
       dtype=torch.float64)
	q_value: tensor([[-31.3800]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.3028980715410898, distance: 1.7365770263364404 entropy 5.349255822099183
epoch: 3, step: 94
	action: tensor([[ -19.7594,  -78.9055,    4.5314,  -18.0102,    1.2084, -102.1342,
           54.2312]], dtype=torch.float64)
	q_value: tensor([[-23.4159]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07757822649071477, distance: 1.1879033224197368 entropy 5.024635622711402
epoch: 3, step: 95
	action: tensor([[-65.8693,   0.4357,   6.7887, -16.3056, -88.9112, 143.5011,  -6.1092]],
       dtype=torch.float64)
	q_value: tensor([[-34.6250]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9428852279304474, distance: 1.5950719079676157 entropy 5.340569790025789
epoch: 3, step: 96
	action: tensor([[ -45.8210,  -11.0031, -108.2483,   71.5038,  -80.9443,  -52.7280,
           43.6069]], dtype=torch.float64)
	q_value: tensor([[-40.4124]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21967700761536024, distance: 1.263802213648514 entropy 5.19900818068374
epoch: 3, step: 97
	action: tensor([[-95.5262,  36.9949,   7.5099, -44.4023, -12.0640,  15.3528,  73.5334]],
       dtype=torch.float64)
	q_value: tensor([[-46.2241]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2552502929915836, distance: 1.718517928251553 entropy 5.450895869170507
epoch: 3, step: 98
	action: tensor([[-47.9382, -77.2623, -85.5503,  90.5324, -32.1246,   5.8426, -31.1978]],
       dtype=torch.float64)
	q_value: tensor([[-35.5165]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.577700350290001, distance: 1.837269682144765 entropy 5.183371134225686
epoch: 3, step: 99
	action: tensor([[-73.3609, -28.7190,   1.5258,   8.2635, -88.9290,   0.3456,   8.7614]],
       dtype=torch.float64)
	q_value: tensor([[-26.2607]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20129439946341243, distance: 1.022704243886221 entropy 5.1916234300243
epoch: 3, step: 100
	action: tensor([[-28.0448, -20.2344,  45.3845,  17.7578,  34.1825,  20.4716,  36.4926]],
       dtype=torch.float64)
	q_value: tensor([[-28.7691]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.412127202239761
epoch: 3, step: 101
	action: tensor([[-47.3123, -47.8890, -43.4610,   7.6230,  28.7765,  84.0697,  -2.1092]],
       dtype=torch.float64)
	q_value: tensor([[-38.9889]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4259402637743124, distance: 0.867032220244452 entropy 5.073594036763349
epoch: 3, step: 102
	action: tensor([[-155.9136,  -21.7696,   66.0521,   49.3108,   11.4278,  -55.7772,
         -109.7098]], dtype=torch.float64)
	q_value: tensor([[-44.6780]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.606229716376428
epoch: 3, step: 103
	action: tensor([[  1.6028, -36.2899,   4.5901, -29.9139,  67.0979, -36.4915,  -9.0501]],
       dtype=torch.float64)
	q_value: tensor([[-38.9889]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.073594036763349
epoch: 3, step: 104
	action: tensor([[ -14.8928,  -22.8323,  -43.1809,    3.7864,  -10.2539,  -20.5052,
         -127.3425]], dtype=torch.float64)
	q_value: tensor([[-38.9889]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.073594036763349
epoch: 3, step: 105
	action: tensor([[ -5.6231, -45.8833,  14.2629,  -5.2731, -62.1147, -27.4986, -23.8589]],
       dtype=torch.float64)
	q_value: tensor([[-38.9889]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.073594036763349
epoch: 3, step: 106
	action: tensor([[ 10.6793, -25.9431,  46.8823,  32.4877,  -3.9374,  52.5329,  54.2715]],
       dtype=torch.float64)
	q_value: tensor([[-38.9889]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.073594036763349
epoch: 3, step: 107
	action: tensor([[-64.0383, -46.7417, -49.5045,  51.7270,  47.1936,  -7.4594,  18.4398]],
       dtype=torch.float64)
	q_value: tensor([[-38.9889]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09478767792893938, distance: 1.1973514476424545 entropy 5.073594036763349
epoch: 3, step: 108
	action: tensor([[-92.2911, -24.0374, -18.9147,   2.8534,  47.4138,  69.1964, 103.1971]],
       dtype=torch.float64)
	q_value: tensor([[-36.3203]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.293226100505526
epoch: 3, step: 109
	action: tensor([[-67.7326, -64.3098, -29.2166,   0.3593, -28.4825,  42.6676, -19.5647]],
       dtype=torch.float64)
	q_value: tensor([[-38.9889]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5298010881931998, distance: 1.4153836550224252 entropy 5.073594036763349
epoch: 3, step: 110
	action: tensor([[-19.4647, -20.7595,  -5.9762,  -0.4830, -89.8652,  13.0528,  56.2599]],
       dtype=torch.float64)
	q_value: tensor([[-37.7661]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2956438480194796, distance: 1.7338397244996466 entropy 5.46012517313838
epoch: 3, step: 111
	action: tensor([[-92.0795,  -2.2881, -51.9890,  -3.4052,  -7.3102,  36.0016, -27.7838]],
       dtype=torch.float64)
	q_value: tensor([[-25.9920]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.082296773269464
epoch: 3, step: 112
	action: tensor([[-143.0757,  -68.3168,   57.5522,  -56.6304,   26.9072,   39.1128,
           17.9930]], dtype=torch.float64)
	q_value: tensor([[-38.9889]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.22904751483670793, distance: 1.2686476804778295 entropy 5.073594036763349
epoch: 3, step: 113
	action: tensor([[-12.0174, -19.5125, -27.6650,  32.1199,  12.2976, -17.7698,  12.6241]],
       dtype=torch.float64)
	q_value: tensor([[-36.1203]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9068008028821335, distance: 1.580190171846382 entropy 5.219446976817204
epoch: 3, step: 114
	action: tensor([[-37.5959, -40.4792,   3.2516,  -2.2966,  -0.6139, -64.4859,  -4.4820]],
       dtype=torch.float64)
	q_value: tensor([[-30.5344]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08752021646240316, distance: 1.1933706782141955 entropy 5.226337400867235
epoch: 3, step: 115
	action: tensor([[-185.7320,   21.0358,   21.0362,  -28.7257,    6.5181,  -40.1528,
          115.4922]], dtype=torch.float64)
	q_value: tensor([[-42.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3287821456291862, distance: 0.9375370586078671 entropy 5.534902437209724
epoch: 3, step: 116
	action: tensor([[-52.4972, -74.8162,  37.0228, 115.5021, -72.4447,  85.7838, -63.0354]],
       dtype=torch.float64)
	q_value: tensor([[-39.4324]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.4919286968896444, distance: 1.8064439835947679 entropy 5.540094453421211
epoch: 3, step: 117
	action: tensor([[-71.4459, -31.8124,  65.2112,  33.6394, -26.5304,  67.3857,   7.5414]],
       dtype=torch.float64)
	q_value: tensor([[-35.6233]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -2.123534474883097, distance: 2.022459553728516 entropy 5.380797921422583
epoch: 3, step: 118
	action: tensor([[-99.2052, -65.7096, -30.5972,   6.1480, -72.4079,  88.3556,  39.8460]],
       dtype=torch.float64)
	q_value: tensor([[-28.3992]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.5107935679252709, distance: 1.8132688336085856 entropy 5.214181981914251
epoch: 3, step: 119
	action: tensor([[-73.3616, -81.9481, -85.0333, -45.3438, -22.2634, -61.1996,  22.6357]],
       dtype=torch.float64)
	q_value: tensor([[-36.2603]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1630096160926113, distance: 1.6830069847176918 entropy 5.403794052156827
epoch: 3, step: 120
	action: tensor([[  -5.4152, -149.0622,   14.9979,  -24.2050,  -87.6241, -125.7392,
            1.4024]], dtype=torch.float64)
	q_value: tensor([[-46.0923]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.701815022381516
epoch: 3, step: 121
	action: tensor([[-64.7781, -15.3447, -59.9010, -30.4262, -14.0743,  44.6770,  18.2144]],
       dtype=torch.float64)
	q_value: tensor([[-38.9889]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.073594036763349
epoch: 3, step: 122
	action: tensor([[-72.8201, -13.7290,  28.9400,   1.7192,  -8.6342, -11.5014, -18.7014]],
       dtype=torch.float64)
	q_value: tensor([[-38.9889]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.073594036763349
epoch: 3, step: 123
	action: tensor([[-45.8971,  15.7040,  65.4760,  43.1242, -29.8685,  11.7736,   6.5256]],
       dtype=torch.float64)
	q_value: tensor([[-38.9889]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.073594036763349
epoch: 3, step: 124
	action: tensor([[-95.4459,  19.1496,  21.6369, -24.1751,  16.5184,   1.9204,  10.6817]],
       dtype=torch.float64)
	q_value: tensor([[-38.9889]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.073594036763349
epoch: 3, step: 125
	action: tensor([[-26.6866, -63.2280,  20.9080,  -6.3970, -50.9485,  17.9920, -30.9935]],
       dtype=torch.float64)
	q_value: tensor([[-38.9889]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.073594036763349
epoch: 3, step: 126
	action: tensor([[-9.0243e+01, -7.2842e+01, -2.7986e+01, -3.8715e-02, -1.0112e+01,
          2.9736e+01, -5.9262e+01]], dtype=torch.float64)
	q_value: tensor([[-38.9889]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11728212637779967, distance: 1.0751465198659786 entropy 5.073594036763349
epoch: 3, step: 127
	action: tensor([[-105.2142, -134.2995,   -4.6161,  -37.2255,  -33.2769,  -95.0808,
          -18.0821]], dtype=torch.float64)
	q_value: tensor([[-49.4543]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.4893598361069023, distance: 1.8055126367889651 entropy 5.574416208608737
LOSS epoch 3 actor 342.52745877307495 critic 149.95372898609733
epoch: 4, step: 0
	action: tensor([[-110.3022, -167.0307,  -30.8518,    8.5897,   -4.6369,  100.9910,
           13.1149]], dtype=torch.float64)
	q_value: tensor([[-28.8677]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.27568912590115713, distance: 1.292495739110704 entropy 5.446037654992649
epoch: 4, step: 1
	action: tensor([[-117.5487,   61.2878,  -61.7448,  -52.0677,  -71.4309,  -64.2336,
           47.6494]], dtype=torch.float64)
	q_value: tensor([[-25.1155]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08232813182235854, distance: 1.1905185497760145 entropy 5.543542089111361
epoch: 4, step: 2
	action: tensor([[-78.2578, -36.2870, -93.4898,  30.8427, -72.2053, -26.2684,  21.7893]],
       dtype=torch.float64)
	q_value: tensor([[-28.1082]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08738291553999666, distance: 1.0932034424232238 entropy 5.641109758304331
epoch: 4, step: 3
	action: tensor([[ -54.5969, -102.7909,  -37.3126,  -13.2430,    7.9735,  -11.9896,
         -119.5302]], dtype=torch.float64)
	q_value: tensor([[-33.0888]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.7956812346790407, distance: 1.9133768669041278 entropy 5.843114713729148
epoch: 4, step: 4
	action: tensor([[ 14.9649, -55.5872,  68.5950, 105.1845,  32.9766, 147.8037, -25.0555]],
       dtype=torch.float64)
	q_value: tensor([[-21.4963]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7727422982962874, distance: 0.5455266837419313 entropy 5.373939606931314
epoch: 4, step: 5
	action: tensor([[-73.4539, -19.6366, -77.5685,  19.3408,  29.4284,  37.5215, -66.6379]],
       dtype=torch.float64)
	q_value: tensor([[-27.9818]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.462621092227285
epoch: 4, step: 6
	action: tensor([[-69.0207, -44.6057,  24.8284,  -7.1613, -43.1134,  38.5942, -22.9783]],
       dtype=torch.float64)
	q_value: tensor([[-32.7659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.654580683670591, distance: 1.471975793755031 entropy 5.279468472831874
epoch: 4, step: 7
	action: tensor([[-26.6416, -63.0197,  88.0735, -65.6741, -30.1109, -49.3475,   3.0980]],
       dtype=torch.float64)
	q_value: tensor([[-23.9974]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2022916726457431, distance: 1.2547627368557968 entropy 5.191370724173602
epoch: 4, step: 8
	action: tensor([[-193.7962,  -67.6593,   -6.6439,  -45.0450,   19.7919,   23.4624,
          132.2604]], dtype=torch.float64)
	q_value: tensor([[-32.4847]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.671592226635902
epoch: 4, step: 9
	action: tensor([[-94.7647,   9.6454,   0.9011, -31.6555, -80.8698,  43.5559,  -3.2857]],
       dtype=torch.float64)
	q_value: tensor([[-32.7659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.279468472831874
epoch: 4, step: 10
	action: tensor([[-28.6135, -62.2447, -80.6912, -24.8190, -43.9823, -32.0604, -15.0264]],
       dtype=torch.float64)
	q_value: tensor([[-32.7659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5445758580437408, distance: 1.422202096484452 entropy 5.279468472831874
epoch: 4, step: 11
	action: tensor([[  -8.4780, -139.9352,  -38.4291,  150.1989,  -80.0867,  -21.6901,
          -89.3731]], dtype=torch.float64)
	q_value: tensor([[-31.4853]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9130443105893891, distance: 1.5827750948764354 entropy 5.350444106273486
epoch: 4, step: 12
	action: tensor([[-179.2086,  -35.1702,  -42.8644,   15.2126,    9.5857,  -52.5182,
          145.6924]], dtype=torch.float64)
	q_value: tensor([[-31.1197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2678411402055201, distance: 1.2885139160254415 entropy 5.546918269321064
epoch: 4, step: 13
	action: tensor([[-120.7708,  -67.8114,   26.6466,   75.8703,  -26.8361,  -10.1875,
          -88.4164]], dtype=torch.float64)
	q_value: tensor([[-29.0359]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.765089580690821
epoch: 4, step: 14
	action: tensor([[-59.0423, -92.0105,  14.9213,  14.6336, -28.1301, -65.5529, -24.2860]],
       dtype=torch.float64)
	q_value: tensor([[-32.7659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8071577486401127, distance: 1.53834847514758 entropy 5.279468472831874
epoch: 4, step: 15
	action: tensor([[ -84.9862, -109.9204, -129.2166,   23.8457,  -92.2091,  -73.5725,
          154.8931]], dtype=torch.float64)
	q_value: tensor([[-42.8602]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.893734664722946
epoch: 4, step: 16
	action: tensor([[ -80.7737,   68.6245,   -8.1421,   43.2584,   29.4193,  -28.6733,
         -112.2305]], dtype=torch.float64)
	q_value: tensor([[-32.7659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.279468472831874
epoch: 4, step: 17
	action: tensor([[-65.1837,  63.4672,  39.7506,  37.3713,  -6.0445, -38.0683,  -2.8496]],
       dtype=torch.float64)
	q_value: tensor([[-32.7659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.279468472831874
epoch: 4, step: 18
	action: tensor([[-90.1757,  18.1171,  -7.6290,   4.7906,  12.4864,  30.0927,  -9.0551]],
       dtype=torch.float64)
	q_value: tensor([[-32.7659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.279468472831874
epoch: 4, step: 19
	action: tensor([[ -5.2214, -70.8048,  29.6075,  49.5966,  40.2498,  33.2640, -44.2958]],
       dtype=torch.float64)
	q_value: tensor([[-32.7659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.279468472831874
epoch: 4, step: 20
	action: tensor([[ 22.9933, -24.5670,  32.7385, -30.9729,  53.0751,  41.7075,  46.9154]],
       dtype=torch.float64)
	q_value: tensor([[-32.7659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04804802893473614, distance: 1.1165141000413907 entropy 5.279468472831874
epoch: 4, step: 21
	action: tensor([[-139.4392,  -36.9944,  -51.4814,    9.4811,  -17.8562,   44.6207,
          -71.4228]], dtype=torch.float64)
	q_value: tensor([[-31.8653]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05606148571879477, distance: 1.1118048045584796 entropy 5.553797004118288
epoch: 4, step: 22
	action: tensor([[-161.2889,  -70.6278,   47.5214,  -96.0539,   18.6734,   -9.7471,
           -3.6859]], dtype=torch.float64)
	q_value: tensor([[-26.7060]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.5824600374602635
epoch: 4, step: 23
	action: tensor([[-26.0242, -65.7374,   6.0627,  36.8008, -36.4525,  12.2654,  30.1516]],
       dtype=torch.float64)
	q_value: tensor([[-32.7659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.279468472831874
epoch: 4, step: 24
	action: tensor([[-28.6323,  35.9152, -83.8165,  88.8769, -17.0924, 140.8201, -14.6457]],
       dtype=torch.float64)
	q_value: tensor([[-32.7659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.279468472831874
epoch: 4, step: 25
	action: tensor([[ -27.2732,    0.9739,  -39.5224,  -27.0196, -115.2185,  -20.0103,
           87.9459]], dtype=torch.float64)
	q_value: tensor([[-32.7659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.279468472831874
epoch: 4, step: 26
	action: tensor([[-92.9534,   8.5344,  69.6970,  60.1604, -34.7907, -13.1573,  12.1788]],
       dtype=torch.float64)
	q_value: tensor([[-32.7659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.279468472831874
epoch: 4, step: 27
	action: tensor([[-98.7692, -48.3993, -34.2712,  77.2302, -54.5383, -40.9119,  45.0504]],
       dtype=torch.float64)
	q_value: tensor([[-32.7659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3496766559640676, distance: 1.3294486319686531 entropy 5.279468472831874
epoch: 4, step: 28
	action: tensor([[ -47.7176, -161.5559,  -83.0796,   10.8744,  -31.9001,  -61.2468,
           77.4051]], dtype=torch.float64)
	q_value: tensor([[-38.6369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.730224088068504
epoch: 4, step: 29
	action: tensor([[ 18.2643, -28.4934,   1.9541,  51.8176, -74.1671,  -2.8174, -80.7925]],
       dtype=torch.float64)
	q_value: tensor([[-32.7659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.279468472831874
epoch: 4, step: 30
	action: tensor([[-40.2643, -46.4983, -86.5290,  30.7455,  15.5342,  90.4819,  57.5409]],
       dtype=torch.float64)
	q_value: tensor([[-32.7659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9326138911896877, distance: 1.5908500344025742 entropy 5.279468472831874
epoch: 4, step: 31
	action: tensor([[-45.2147, -95.9306,  79.5189,  20.6663,  12.3359, -72.2259, -88.0626]],
       dtype=torch.float64)
	q_value: tensor([[-30.4084]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9564925938455007, distance: 1.6006478565639999 entropy 5.475137567281215
epoch: 4, step: 32
	action: tensor([[-95.2399, -14.1112,  12.3099,  35.8624,  32.1469,  36.6680, 110.6567]],
       dtype=torch.float64)
	q_value: tensor([[-22.8030]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07950274656120837, distance: 1.1889636277856455 entropy 5.25877546321712
epoch: 4, step: 33
	action: tensor([[-120.1421,  -51.7732,  -46.3098,   87.9770,   15.7094,   -0.3899,
           38.6404]], dtype=torch.float64)
	q_value: tensor([[-20.6603]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7540203739189633, distance: 1.5155630556422863 entropy 5.408755681256258
epoch: 4, step: 34
	action: tensor([[  27.0219, -189.8091,  -52.4346,   57.3492,   25.9499,   90.0058,
           97.8114]], dtype=torch.float64)
	q_value: tensor([[-31.9838]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07636463652267178, distance: 1.0997829171350078 entropy 5.865696300876037
epoch: 4, step: 35
	action: tensor([[ -87.2099, -222.8721,  -41.9551,  149.0315,  -67.1948,  121.5596,
         -142.6816]], dtype=torch.float64)
	q_value: tensor([[-33.4374]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2703793382552071, distance: 1.2898030634459168 entropy 5.705859355955806
epoch: 4, step: 36
	action: tensor([[-159.3521,  -67.0209,   29.7690,  115.1481,   57.0347,   31.1341,
          -64.5587]], dtype=torch.float64)
	q_value: tensor([[-34.8039]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14067041331806918, distance: 1.2221845285686042 entropy 5.73797856479449
epoch: 4, step: 37
	action: tensor([[ 89.0609, -32.0826,  44.1859, -46.4686, -25.0294,  93.1202,  29.7527]],
       dtype=torch.float64)
	q_value: tensor([[-31.8978]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35009909504353975, distance: 0.9225295031952427 entropy 5.878989561449131
epoch: 4, step: 38
	action: tensor([[  7.8126, -83.1067,  42.0219,  16.2661, -33.9537, -26.6843, -54.7807]],
       dtype=torch.float64)
	q_value: tensor([[-19.9598]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13012626524689652, distance: 1.0672958043599274 entropy 5.378485167591209
epoch: 4, step: 39
	action: tensor([[-201.4648,  -37.5988,  114.1486,  -88.6346,   -4.3557,   97.3564,
          242.5340]], dtype=torch.float64)
	q_value: tensor([[-27.4270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.775878847316922, distance: 1.5249772346348882 entropy 5.952093154829258
epoch: 4, step: 40
	action: tensor([[-38.7836, -32.4025, -35.0655,  10.4280,  52.5427, -49.8039,  -5.1197]],
       dtype=torch.float64)
	q_value: tensor([[-23.8321]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.556455477516051
epoch: 4, step: 41
	action: tensor([[ -67.6499, -100.3931,   25.5199,   24.1274,  -56.5554,   84.6952,
           57.1191]], dtype=torch.float64)
	q_value: tensor([[-32.7659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14838883091458266, distance: 1.226312551185348 entropy 5.279468472831874
epoch: 4, step: 42
	action: tensor([[ 30.8693, -52.2957, -33.6312,   0.6058,  11.2820,   7.6189,  21.6311]],
       dtype=torch.float64)
	q_value: tensor([[-28.1329]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9275289358769023, distance: 1.5887557905791363 entropy 5.271225650202149
epoch: 4, step: 43
	action: tensor([[-109.1687,  -31.3137,  -72.9384,   80.6314,   37.1269,    1.8041,
           58.9937]], dtype=torch.float64)
	q_value: tensor([[-27.1812]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.043998389397692295, distance: 1.1692479243060352 entropy 5.463397234547185
epoch: 4, step: 44
	action: tensor([[-143.5969, -172.6759,   72.0991,  -41.1949,  -74.0864,  -43.8403,
           33.9070]], dtype=torch.float64)
	q_value: tensor([[-35.0307]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9760096009215482, distance: 1.608611682613527 entropy 5.784206270760518
epoch: 4, step: 45
	action: tensor([[-63.8598, -52.9200, -39.6221, -38.6306, -74.5459, -61.9560,  98.5317]],
       dtype=torch.float64)
	q_value: tensor([[-32.8342]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7825372561618171, distance: 1.5278334041440875 entropy 5.7579514158695835
epoch: 4, step: 46
	action: tensor([[ -42.9718,   30.9043,  -10.7128,  -18.8533,   74.0600, -169.7147,
           52.7138]], dtype=torch.float64)
	q_value: tensor([[-29.4479]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0167648654233141, distance: 1.6251158522167553 entropy 5.683182634412209
epoch: 4, step: 47
	action: tensor([[-181.2304,  -35.2388,   33.5908,  -58.8046,   61.4231,   76.6769,
           70.3841]], dtype=torch.float64)
	q_value: tensor([[-32.5465]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.3085459549462088, distance: 1.7387052096252826 entropy 5.77064277197333
epoch: 4, step: 48
	action: tensor([[-153.4185,  -69.3562,  -55.0536, -104.8580,  -19.6919,   84.4975,
           49.1428]], dtype=torch.float64)
	q_value: tensor([[-29.0515]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8706740945689841, distance: 1.5651492567394012 entropy 5.611530194210211
epoch: 4, step: 49
	action: tensor([[  -4.7170,  -95.2818,   56.5622,   11.5943,  -45.6574,  -68.7380,
         -106.2896]], dtype=torch.float64)
	q_value: tensor([[-33.1685]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.593096779832483
epoch: 4, step: 50
	action: tensor([[-48.0592,  30.7524, -97.5831,  37.3515, -81.6571,  45.2966, -42.0716]],
       dtype=torch.float64)
	q_value: tensor([[-32.7659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.279468472831874
epoch: 4, step: 51
	action: tensor([[-64.2126,   5.3564,  21.4750, -24.4073,  -5.8583, -32.1603, -24.0751]],
       dtype=torch.float64)
	q_value: tensor([[-32.7659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.279468472831874
epoch: 4, step: 52
	action: tensor([[-73.5159, -46.2469, -32.7672,  47.8311,  45.9173, -86.5917,  24.1954]],
       dtype=torch.float64)
	q_value: tensor([[-32.7659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.279468472831874
epoch: 4, step: 53
	action: tensor([[ -40.6351, -123.4461,   15.2712,   -6.1636,  -12.6352,   44.7832,
           -8.4385]], dtype=torch.float64)
	q_value: tensor([[-32.7659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.44050293767245896, distance: 1.3734528896297118 entropy 5.279468472831874
epoch: 4, step: 54
	action: tensor([[-80.0090,  79.5312, -42.7175,  97.2375,  14.9856, 208.1533, -32.3447]],
       dtype=torch.float64)
	q_value: tensor([[-33.4287]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.015675705125183237, distance: 1.135339624589846 entropy 5.619596245244606
epoch: 4, step: 55
	action: tensor([[-18.0946, -91.8486,  64.2115,  46.2068,  -2.5915, -20.8634,  27.7884]],
       dtype=torch.float64)
	q_value: tensor([[-29.8128]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.5576808953763805
epoch: 4, step: 56
	action: tensor([[-104.9623,  -83.3156,   22.6610,   40.8831,  -24.1631,  -25.5732,
            1.2323]], dtype=torch.float64)
	q_value: tensor([[-32.7659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.546776153541003, distance: 1.826215735932076 entropy 5.279468472831874
epoch: 4, step: 57
	action: tensor([[ -28.5162, -124.0108,  -54.3037,   15.5196,  -64.0342,  -81.4276,
           52.1435]], dtype=torch.float64)
	q_value: tensor([[-31.0714]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8813358314709814, distance: 1.5696031325759643 entropy 5.523235668792531
epoch: 4, step: 58
	action: tensor([[ -3.3115, -90.3899, -17.5395, 114.5890,   3.0632,  38.3151, 102.1544]],
       dtype=torch.float64)
	q_value: tensor([[-30.1277]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.716611208830415
epoch: 4, step: 59
	action: tensor([[-52.4543, -18.0130, 112.9531, -15.7815,  98.4642,  17.3641,  65.0290]],
       dtype=torch.float64)
	q_value: tensor([[-32.7659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.279468472831874
epoch: 4, step: 60
	action: tensor([[ -67.8327, -109.8288,  -59.0125,  -32.3665,   -8.0635,  140.3679,
          -61.5304]], dtype=torch.float64)
	q_value: tensor([[-32.7659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1508540351226213, distance: 1.227628085662407 entropy 5.279468472831874
epoch: 4, step: 61
	action: tensor([[-117.9638,   73.2088,  -58.7263,   92.7203, -170.5197, -105.1601,
          -44.7871]], dtype=torch.float64)
	q_value: tensor([[-41.0422]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.030487854371584966, distance: 1.1616575836748972 entropy 5.725727171353329
epoch: 4, step: 62
	action: tensor([[ -80.7931, -118.7542,   11.6674,   13.4841,  -60.9918,   -2.3694,
          -84.4265]], dtype=torch.float64)
	q_value: tensor([[-38.3040]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.27171421618088965, distance: 1.2904805294348385 entropy 5.483866710305706
epoch: 4, step: 63
	action: tensor([[-138.8395,   44.4534,   34.7190,   -6.8822,   25.4357,   60.6623,
           46.9568]], dtype=torch.float64)
	q_value: tensor([[-31.3606]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30714752583908556, distance: 0.9525265272784348 entropy 5.591239238462196
epoch: 4, step: 64
	action: tensor([[-19.6446,  18.1613,  25.5375,  25.3177,  -4.8319, -82.8498,  89.7405]],
       dtype=torch.float64)
	q_value: tensor([[-37.4801]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.35836480893539946, distance: 1.3337207383448475 entropy 5.749865987084873
epoch: 4, step: 65
	action: tensor([[-96.7760, -68.3509, -34.3150, -45.9077, -39.9979, -18.6846,  64.6980]],
       dtype=torch.float64)
	q_value: tensor([[-24.7112]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07540359484549874, distance: 1.186704079196591 entropy 5.305201676475135
epoch: 4, step: 66
	action: tensor([[-156.5959,  -47.9839, -158.5905,   40.6666,  -25.3363,   -4.9952,
           73.7662]], dtype=torch.float64)
	q_value: tensor([[-40.5113]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.8070449859550495
epoch: 4, step: 67
	action: tensor([[-22.0309, -49.4779,  19.9065,  -2.9708, -26.1739,  41.4476,   7.5782]],
       dtype=torch.float64)
	q_value: tensor([[-32.7659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.279468472831874
epoch: 4, step: 68
	action: tensor([[-117.6947,  -21.7681,  -22.4759,   -6.9416,  -15.9874,    0.2735,
          -40.4968]], dtype=torch.float64)
	q_value: tensor([[-32.7659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.279468472831874
epoch: 4, step: 69
	action: tensor([[ 65.7329, -21.1871, -53.5837, -51.7117,  20.3016,  47.9530,  74.5056]],
       dtype=torch.float64)
	q_value: tensor([[-32.7659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.279468472831874
epoch: 4, step: 70
	action: tensor([[ -58.4818, -115.8802,  -54.5616,  -54.8386,  -23.5421,  -71.0529,
            5.3659]], dtype=torch.float64)
	q_value: tensor([[-32.7659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.279468472831874
epoch: 4, step: 71
	action: tensor([[-116.7938,  -41.0688,   21.9192,  -39.2549,  -63.1027,   58.5732,
           57.5658]], dtype=torch.float64)
	q_value: tensor([[-32.7659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5168285812757154, distance: 1.4093697473403257 entropy 5.279468472831874
epoch: 4, step: 72
	action: tensor([[ -11.0452,  -61.8753,  -32.4920,  -54.1665, -133.3658,  -36.0543,
          -19.5263]], dtype=torch.float64)
	q_value: tensor([[-31.3480]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6627418449858713, distance: 0.6645657011749436 entropy 5.619278055162027
epoch: 4, step: 73
	action: tensor([[-102.7622, -102.0691,    4.9129,  -21.9288,  -35.5657,    8.0395,
           23.9244]], dtype=torch.float64)
	q_value: tensor([[-31.6896]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5946658503159171, distance: 0.728556589756788 entropy 5.429327915745915
epoch: 4, step: 74
	action: tensor([[-95.3309, -58.1266, 102.2555, -80.1941,  14.3209, -97.9640,  18.0797]],
       dtype=torch.float64)
	q_value: tensor([[-30.8678]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20962198035118895, distance: 1.0173587397422301 entropy 5.582504377264724
epoch: 4, step: 75
	action: tensor([[-136.8837,   -0.4437,   22.7853,  -59.8581,   90.7236,  -38.9149,
           43.0390]], dtype=torch.float64)
	q_value: tensor([[-26.8288]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.02426372773358576, distance: 1.1581440756309644 entropy 5.475534232397267
epoch: 4, step: 76
	action: tensor([[-81.9358,  -3.1073, 102.4048, -90.5350, -53.1449,   5.7469, -36.0222]],
       dtype=torch.float64)
	q_value: tensor([[-35.2117]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.716810008111379
epoch: 4, step: 77
	action: tensor([[-27.6365, -31.9284,  -0.6849,  25.0841,   2.8311,  75.2824,  62.1185]],
       dtype=torch.float64)
	q_value: tensor([[-32.7659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7640175158953826, distance: 1.5198759392445798 entropy 5.279468472831874
epoch: 4, step: 78
	action: tensor([[ -1.4643, 115.9025,  31.5849,  41.4970, -95.1880,  76.3384,  52.0380]],
       dtype=torch.float64)
	q_value: tensor([[-28.6820]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6125330917629292, distance: 0.7123181361366137 entropy 5.4935180139859074
epoch: 4, step: 79
	action: tensor([[-51.2420, -49.0738, -84.1447,  23.6041,  -6.9642,  82.4422,  93.5572]],
       dtype=torch.float64)
	q_value: tensor([[-26.3838]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.372183879069633
epoch: 4, step: 80
	action: tensor([[ 12.6990,  30.3356,  89.7298,  18.6505, -28.1392, 108.7469, -67.7113]],
       dtype=torch.float64)
	q_value: tensor([[-32.7659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.279468472831874
epoch: 4, step: 81
	action: tensor([[-61.1067, -21.0837, -78.9898, -31.5346, -54.7170, -44.6642,  23.7330]],
       dtype=torch.float64)
	q_value: tensor([[-32.7659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.279468472831874
epoch: 4, step: 82
	action: tensor([[-68.1917, -22.3312, -18.5721,  67.6594,   5.1305,  57.6797,   6.5456]],
       dtype=torch.float64)
	q_value: tensor([[-32.7659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0088986666017608, distance: 1.6219434511649107 entropy 5.279468472831874
epoch: 4, step: 83
	action: tensor([[-23.5549,  29.8899, -41.0345,  62.3952, -45.8195,  25.2206,  67.5446]],
       dtype=torch.float64)
	q_value: tensor([[-26.6200]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.245353257522315
epoch: 4, step: 84
	action: tensor([[ -44.4276, -112.7240,   42.1635,   15.0662,  -19.7332,   37.7086,
           58.4035]], dtype=torch.float64)
	q_value: tensor([[-32.7659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.842057139591472, distance: 1.5531315504670695 entropy 5.279468472831874
epoch: 4, step: 85
	action: tensor([[-94.4077, -82.6159,  -7.8610, -44.8351, -31.7793,  23.6615,  63.4878]],
       dtype=torch.float64)
	q_value: tensor([[-28.2922]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.422807893765461
epoch: 4, step: 86
	action: tensor([[-94.7302, -11.1142, -49.0825,  -3.9719, -34.6483,   1.2965,  -5.5201]],
       dtype=torch.float64)
	q_value: tensor([[-32.7659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.279468472831874
epoch: 4, step: 87
	action: tensor([[-176.1800,  -79.0942,  -61.7476,   85.0475,  -66.0787, -118.6757,
           47.8330]], dtype=torch.float64)
	q_value: tensor([[-32.7659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.40212918811561427, distance: 0.8848310802328041 entropy 5.279468472831874
epoch: 4, step: 88
	action: tensor([[ -38.1542,  -21.1315,  -70.4863,   77.1173,  -98.5243, -122.6166,
            0.6178]], dtype=torch.float64)
	q_value: tensor([[-31.5354]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24826109879700664, distance: 1.2785255500445143 entropy 5.5488135550063955
epoch: 4, step: 89
	action: tensor([[ -63.8133, -121.8533,   54.6113,   -7.9258,  -68.6024,  -36.4649,
          -39.2477]], dtype=torch.float64)
	q_value: tensor([[-29.1192]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.047187550059523686, distance: 1.1170186001733995 entropy 5.588076862772923
epoch: 4, step: 90
	action: tensor([[-102.8464,    3.3542,   70.0935,  165.5315, -164.5812,  -44.5019,
          121.9566]], dtype=torch.float64)
	q_value: tensor([[-31.0118]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.6187434925229764
epoch: 4, step: 91
	action: tensor([[ -98.4304, -119.4354,  -25.9029,   -4.4048,   14.4867,  -18.2293,
           -9.5855]], dtype=torch.float64)
	q_value: tensor([[-32.7659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6803589085520383, distance: 1.4833981042438604 entropy 5.279468472831874
epoch: 4, step: 92
	action: tensor([[-133.5210,  -75.6421,  -72.7119,    8.0999,  -33.1191,   21.1851,
            4.5818]], dtype=torch.float64)
	q_value: tensor([[-32.8863]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1432965010318537, distance: 1.6753201836732436 entropy 5.6379546004534955
epoch: 4, step: 93
	action: tensor([[ -73.0452,  -76.8737,   18.5903,  -54.8523, -226.8430,  168.2307,
           69.8162]], dtype=torch.float64)
	q_value: tensor([[-33.7470]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.42868362863897125, distance: 1.367806701945938 entropy 5.877846456098865
epoch: 4, step: 94
	action: tensor([[ -1.9780, -15.7495,  34.1575,  51.8246,  40.1539,  20.0315,  67.8875]],
       dtype=torch.float64)
	q_value: tensor([[-25.7069]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.52571237696509
epoch: 4, step: 95
	action: tensor([[-63.3514, -78.5873, -14.2862, -32.1444, -64.1633,  34.2982,   8.3436]],
       dtype=torch.float64)
	q_value: tensor([[-32.7659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.278655830628187, distance: 1.2939977611970102 entropy 5.279468472831874
epoch: 4, step: 96
	action: tensor([[-101.9124,  -41.1055,   40.5682,  -54.2939,  -14.7248,  -70.2074,
          -60.1043]], dtype=torch.float64)
	q_value: tensor([[-34.0972]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.010199228384643488, distance: 1.1384935834867527 entropy 5.539092406971677
epoch: 4, step: 97
	action: tensor([[-229.2274,    1.1344,   97.4652, -115.1030,  -44.9112,  -36.5564,
           55.0101]], dtype=torch.float64)
	q_value: tensor([[-43.6894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7396025593913609, distance: 0.5839491746352324 entropy 5.978092366540551
epoch: 4, step: 98
	action: tensor([[-226.8105,  121.7567,   -6.7612,   37.6919,   66.2194,   66.4179,
           28.8452]], dtype=torch.float64)
	q_value: tensor([[-39.3385]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.687206695061116
epoch: 4, step: 99
	action: tensor([[-79.4849, -62.5770, 106.0375,   9.8581,   5.3692, -12.8928,  74.5203]],
       dtype=torch.float64)
	q_value: tensor([[-32.7659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.279468472831874
epoch: 4, step: 100
	action: tensor([[  7.8184, -75.3163,  -9.2075,  64.5576,  19.9144, -86.3690, -12.8890]],
       dtype=torch.float64)
	q_value: tensor([[-32.7659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.279468472831874
epoch: 4, step: 101
	action: tensor([[-114.8417,  -75.0933,  -47.2969,   19.6483,   10.5436,   87.8341,
           20.5032]], dtype=torch.float64)
	q_value: tensor([[-32.7659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1299172755744149, distance: 1.0674240071070404 entropy 5.279468472831874
epoch: 4, step: 102
	action: tensor([[  15.0501, -123.9196,  -70.9313,  183.0873,   24.5652,  -32.7925,
           77.8218]], dtype=torch.float64)
	q_value: tensor([[-42.8083]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.765130087226121
epoch: 4, step: 103
	action: tensor([[-57.0485, -39.3163, -41.9256, -30.9410, -30.0389,  55.5833,  14.4061]],
       dtype=torch.float64)
	q_value: tensor([[-32.7659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.279468472831874
epoch: 4, step: 104
	action: tensor([[-21.7654, -37.9776,  69.7684,   7.1539,  -2.5834,  27.5173, -20.1256]],
       dtype=torch.float64)
	q_value: tensor([[-32.7659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8630649631872387, distance: 1.5619628220434054 entropy 5.279468472831874
epoch: 4, step: 105
	action: tensor([[-121.8590,  -53.1364,  101.9994,  174.9691,   45.7004,  149.0155,
           50.7979]], dtype=torch.float64)
	q_value: tensor([[-32.3902]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.41646605837219797, distance: 1.361945657943783 entropy 5.643714240340818
epoch: 4, step: 106
	action: tensor([[   5.7740, -147.5701,  -26.8984,  -52.8813,  -24.0192,   50.7914,
          -74.9509]], dtype=torch.float64)
	q_value: tensor([[-29.8433]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3958028112590155, distance: 1.3519752075750013 entropy 5.658171101658506
epoch: 4, step: 107
	action: tensor([[-109.6631, -125.3079,  -58.4689,   43.8757,   -4.7085,  105.7177,
          -94.9305]], dtype=torch.float64)
	q_value: tensor([[-30.6862]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6706767792161445, distance: 1.479118303973219 entropy 5.713784259350676
epoch: 4, step: 108
	action: tensor([[-40.8460, -22.6296,  36.9274,  33.9750,  -9.1934,  55.5382, -19.8697]],
       dtype=torch.float64)
	q_value: tensor([[-32.7625]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.729859721622826
epoch: 4, step: 109
	action: tensor([[-69.8225, -68.1041, -72.8750,   0.9176, -87.1078,  79.1821,  18.8031]],
       dtype=torch.float64)
	q_value: tensor([[-32.7659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.287145900291887, distance: 1.2982866224492045 entropy 5.279468472831874
epoch: 4, step: 110
	action: tensor([[-79.6860,  16.9509, -35.4320, -28.9632,  21.2956, -31.1331, -19.2929]],
       dtype=torch.float64)
	q_value: tensor([[-37.6552]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.657525076359578
epoch: 4, step: 111
	action: tensor([[-39.2402,   6.0617, -26.8230, -56.3100,  26.1152,  37.1229,  49.6038]],
       dtype=torch.float64)
	q_value: tensor([[-32.7659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.279468472831874
epoch: 4, step: 112
	action: tensor([[  2.8288,   0.4292,  12.5645,  15.8375, -43.2695, 108.2504,  29.1565]],
       dtype=torch.float64)
	q_value: tensor([[-32.7659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.279468472831874
epoch: 4, step: 113
	action: tensor([[-65.5637,   3.2068,  19.1296, -33.9213,   2.3027,   9.5238, -14.5018]],
       dtype=torch.float64)
	q_value: tensor([[-32.7659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.279468472831874
epoch: 4, step: 114
	action: tensor([[-114.0974,  -65.8886,   65.7662,  -15.8458,  -39.4123,   36.0165,
           -0.2719]], dtype=torch.float64)
	q_value: tensor([[-32.7659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.279468472831874
epoch: 4, step: 115
	action: tensor([[-36.5784, -93.9434, -14.4344,   2.7374, -96.5900,  50.6124,  69.7482]],
       dtype=torch.float64)
	q_value: tensor([[-32.7659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.279468472831874
epoch: 4, step: 116
	action: tensor([[ -73.1002, -111.6336,  -32.5373,   37.1940,   18.5808,   59.7142,
          109.7735]], dtype=torch.float64)
	q_value: tensor([[-32.7659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16944609130191024, distance: 1.2375045277646526 entropy 5.279468472831874
epoch: 4, step: 117
	action: tensor([[-75.3071, -30.2658,  70.3716, 105.4478,  -2.8816, -50.2260,  30.6625]],
       dtype=torch.float64)
	q_value: tensor([[-33.1418]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.538257499855632, distance: 1.4192902321229304 entropy 5.387899128537922
epoch: 4, step: 118
	action: tensor([[-136.3086,  -84.3335,  144.2951,  -39.3126,   93.8829,   44.4611,
          147.3910]], dtype=torch.float64)
	q_value: tensor([[-30.0814]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5174165198390395, distance: 1.4096428640769563 entropy 5.528746008091665
epoch: 4, step: 119
	action: tensor([[-25.3030, -19.2333, -59.1390,  42.0073,  29.2727, -24.7327,  -6.2665]],
       dtype=torch.float64)
	q_value: tensor([[-28.1529]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1115234181741114, distance: 1.662856010158945 entropy 5.266061629549737
epoch: 4, step: 120
	action: tensor([[-146.9928,  -60.3985,  -33.4429,  -28.9638,  -35.6492,  -95.4422,
           94.3478]], dtype=torch.float64)
	q_value: tensor([[-28.1628]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.551872854622379, distance: 1.4255575729258496 entropy 5.464276881428175
epoch: 4, step: 121
	action: tensor([[-123.8650,   70.0344,   -1.5729,   11.6109,   22.9181,   -1.4127,
            5.0749]], dtype=torch.float64)
	q_value: tensor([[-36.7292]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09444625187467826, distance: 1.0889647192120007 entropy 5.669848699369085
epoch: 4, step: 122
	action: tensor([[-141.7708,  -39.9153,   29.5349,  -50.2016,   30.0368,   57.1172,
         -126.3561]], dtype=torch.float64)
	q_value: tensor([[-40.2511]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.38999793063512134, distance: 1.3491609736161263 entropy 5.672415898209566
epoch: 4, step: 123
	action: tensor([[-140.0750,  -44.5648,   46.0350,   37.6091,  -94.7532,  -13.8967,
          -13.0767]], dtype=torch.float64)
	q_value: tensor([[-34.1846]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2880333202897223, distance: 1.7309633223370142 entropy 5.4738242272022815
epoch: 4, step: 124
	action: tensor([[-39.2365, -87.0636, 137.7434,   2.5469,  -9.3868, 103.9165,  22.9324]],
       dtype=torch.float64)
	q_value: tensor([[-27.1578]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3650383960608101, distance: 1.3369929801139995 entropy 5.479567550519108
epoch: 4, step: 125
	action: tensor([[-1.7395e+02, -1.5627e+02, -1.4435e+02,  7.4740e+01, -1.4311e-01,
         -1.3214e+02,  2.6541e+00]], dtype=torch.float64)
	q_value: tensor([[-32.4354]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6086652307249403, distance: 1.4514080612569844 entropy 5.84491905448845
epoch: 4, step: 126
	action: tensor([[ 37.7028, -30.5409,  12.7911,  18.9489,   1.4363,  -1.0562,  64.2745]],
       dtype=torch.float64)
	q_value: tensor([[-28.5805]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5793011715460117, distance: 1.4381002839699455 entropy 5.655907261353695
epoch: 4, step: 127
	action: tensor([[-125.5070,   31.7559,   14.8118,   71.4058,   17.7231,  -38.8815,
          -56.3427]], dtype=torch.float64)
	q_value: tensor([[-24.7718]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.34286299822227684, distance: 1.3260886162917145 entropy 5.352745210363052
LOSS epoch 4 actor 220.94028557486794 critic 263.5819823874901
epoch: 5, step: 0
	action: tensor([[-187.8283, -132.5946,   12.3954,   65.3864,    6.2084,   49.2258,
          -10.9646]], dtype=torch.float64)
	q_value: tensor([[-21.0192]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31683380248048876, distance: 0.9458447946932388 entropy 5.666395890512892
epoch: 5, step: 1
	action: tensor([[-131.2421, -193.6106,  -60.4130,  -43.2932,   20.9225, -117.4728,
          -74.2407]], dtype=torch.float64)
	q_value: tensor([[-22.5061]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12061853610304096, distance: 1.2113944906564988 entropy 5.644561321757072
epoch: 5, step: 2
	action: tensor([[-195.2052,   86.8143,  128.5412,  -35.3459,  -32.6382,   97.4897,
           37.4394]], dtype=torch.float64)
	q_value: tensor([[-31.1825]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.7185958742476046, distance: 1.886813695856508 entropy 5.855506023276436
epoch: 5, step: 3
	action: tensor([[ -82.2319, -128.7136,  -58.1804,  -11.5053, -124.7175,   89.9139,
          154.0718]], dtype=torch.float64)
	q_value: tensor([[-27.5126]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2521878466998593, distance: 1.7173507281576343 entropy 5.773423594567644
epoch: 5, step: 4
	action: tensor([[ -99.9929,  -54.2640,   98.0932,  -27.0851, -163.9179,  179.0186,
         -209.1823]], dtype=torch.float64)
	q_value: tensor([[-28.9701]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.133953009671651, distance: 1.6716644979709412 entropy 5.7039966144763445
epoch: 5, step: 5
	action: tensor([[-231.2825, -108.8055,   73.6360,   37.5725,  -24.5378,  -33.1575,
          223.6795]], dtype=torch.float64)
	q_value: tensor([[-35.8995]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -2.2371004989329886, distance: 2.058897776630572 entropy 5.920543737442671
epoch: 5, step: 6
	action: tensor([[ -55.7562,  -34.0237,  136.2428,   51.8492,  -86.2830,  -42.7605,
         -151.4623]], dtype=torch.float64)
	q_value: tensor([[-29.6191]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.3814742459732403, distance: 1.7659550211442927 entropy 5.838239212266765
epoch: 5, step: 7
	action: tensor([[-144.5458,  -74.8775,   68.9127,   73.2820,  -11.0812,  -70.7082,
          -14.2982]], dtype=torch.float64)
	q_value: tensor([[-25.3619]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6812768673783707, distance: 1.4838032297372752 entropy 5.754117687088111
epoch: 5, step: 8
	action: tensor([[-177.9186, -139.5449,  163.9995,  -73.1821, -126.1559,   37.9412,
           85.3010]], dtype=torch.float64)
	q_value: tensor([[-24.7902]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4801800088023338, distance: 1.392239529630851 entropy 5.853269559782739
epoch: 5, step: 9
	action: tensor([[-136.8377, -105.3239,  -81.3357,   42.8930,   87.9134,    1.6616,
            6.4769]], dtype=torch.float64)
	q_value: tensor([[-22.9162]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11653424928694933, distance: 1.0756018789536748 entropy 5.860039017492291
epoch: 5, step: 10
	action: tensor([[-282.2088,  -39.2567,   77.0637,   55.1643, -131.6398,   13.6034,
          -35.5518]], dtype=torch.float64)
	q_value: tensor([[-22.4861]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32717676586448785, distance: 0.9386575622811069 entropy 5.648737491269146
epoch: 5, step: 11
	action: tensor([[-138.2122, -134.7568,  -35.9745,   89.8231,   56.6460,    0.8072,
         -168.7169]], dtype=torch.float64)
	q_value: tensor([[-24.2040]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5994478187227055, distance: 0.7242462310388719 entropy 5.768858522406559
epoch: 5, step: 12
	action: tensor([[ -21.5796, -102.6316,   17.4029,   20.9790,  -54.9724,  -45.3787,
           30.0052]], dtype=torch.float64)
	q_value: tensor([[-21.3442]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23172438946753293, distance: 1.0030329880821613 entropy 5.575936250703555
epoch: 5, step: 13
	action: tensor([[ -78.3324, -142.4212,    4.4712,   61.9233,  -96.3749,  102.1070,
           98.5901]], dtype=torch.float64)
	q_value: tensor([[-29.4825]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.19633039570008948, distance: 1.2516481502131702 entropy 5.968679370409193
epoch: 5, step: 14
	action: tensor([[-272.8380,  -87.0572, -122.6122,   -4.8478,  -46.4312,   99.9034,
           68.5291]], dtype=torch.float64)
	q_value: tensor([[-28.6668]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0892170767807259, distance: 1.654049402589977 entropy 6.088376534004695
epoch: 5, step: 15
	action: tensor([[ -28.0032, -197.9314,   12.3700,  -36.1548,   41.2710,  118.2823,
          157.5434]], dtype=torch.float64)
	q_value: tensor([[-33.9111]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1195988660503051, distance: 1.6660327432203799 entropy 6.060944071837075
epoch: 5, step: 16
	action: tensor([[-220.0048,  -96.4826,  -15.5304,   39.8887, -155.1525,  107.8098,
           48.7882]], dtype=torch.float64)
	q_value: tensor([[-32.1657]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7665214752757201, distance: 1.520954261319657 entropy 6.026938181489064
epoch: 5, step: 17
	action: tensor([[  30.6131, -108.5371,    7.5920,   -4.2670,  -81.7615, -170.3833,
          193.5053]], dtype=torch.float64)
	q_value: tensor([[-32.3709]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16990483243160015, distance: 1.2377472232456772 entropy 6.021372097459798
epoch: 5, step: 18
	action: tensor([[   1.6115, -107.5656,   60.7577,   47.6319,  -74.2364,  120.2454,
          -16.7040]], dtype=torch.float64)
	q_value: tensor([[-23.5987]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33958506354136675, distance: 0.9299618586016142 entropy 5.7311163966725855
epoch: 5, step: 19
	action: tensor([[-303.6064,  -27.9893,  -25.9138,   49.9815,  -10.5347,  -61.6520,
           73.3241]], dtype=torch.float64)
	q_value: tensor([[-26.9032]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8383805183959803, distance: 1.5515808036326526 entropy 5.938640280577375
epoch: 5, step: 20
	action: tensor([[-75.4569, -16.9587, -94.3128, -65.2649,  47.2002, -19.7709,  28.2175]],
       dtype=torch.float64)
	q_value: tensor([[-23.9765]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24994573446886104, distance: 0.9910670465463579 entropy 5.801020451109649
epoch: 5, step: 21
	action: tensor([[-124.3198,  -22.9152,   29.4235, -139.2452,   57.2586,   72.8582,
          133.1487]], dtype=torch.float64)
	q_value: tensor([[-30.5090]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0595017003151272, distance: 1.6422443284550106 entropy 6.0718302569552645
epoch: 5, step: 22
	action: tensor([[-38.7071, -47.7105,  12.6933, -72.8852, -56.1817, -37.5214, -40.6416]],
       dtype=torch.float64)
	q_value: tensor([[-28.1929]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.7663563099891157, distance: 1.9033153571650123 entropy 5.666878197529542
epoch: 5, step: 23
	action: tensor([[-169.9841,  -15.2699,    5.9908,  -27.6549, -113.5917,   71.3791,
           84.9169]], dtype=torch.float64)
	q_value: tensor([[-26.7983]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5102101125319212, distance: 1.4062915922973185 entropy 5.660435364387512
epoch: 5, step: 24
	action: tensor([[-127.4361,  -24.4120, -126.7680, -296.8032,   94.9039,  -80.4560,
         -123.5362]], dtype=torch.float64)
	q_value: tensor([[-35.0063]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13506052693595338, distance: 1.0642644413052986 entropy 5.981624272150384
epoch: 5, step: 25
	action: tensor([[-125.0944,  -43.9634,  -23.0426,  -46.9404, -129.8605, -105.3307,
          158.8675]], dtype=torch.float64)
	q_value: tensor([[-28.7289]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7053760259298887, distance: 1.494399694050776 entropy 5.7959138331391085
epoch: 5, step: 26
	action: tensor([[-125.6178, -103.5140,  -28.0846,  -87.1389,   -2.3716,  -65.0377,
           32.2761]], dtype=torch.float64)
	q_value: tensor([[-40.7106]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2855689370926133, distance: 1.7300308826425306 entropy 6.103981810076588
epoch: 5, step: 27
	action: tensor([[  22.2326,  -92.0786,   77.1136,  -25.8079,   18.8696, -106.5703,
          -26.4183]], dtype=torch.float64)
	q_value: tensor([[-41.2384]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2933123194382268, distance: 1.3013928025345507 entropy 6.1684416003188165
epoch: 5, step: 28
	action: tensor([[ -73.7235,  -44.3399, -133.1170,   76.0523,  -73.1117,   10.2698,
           82.7559]], dtype=torch.float64)
	q_value: tensor([[-29.9328]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.6255382518439796, distance: 1.8542396701073263 entropy 5.745189231419153
epoch: 5, step: 29
	action: tensor([[ -12.3996,  -59.3884,   42.1007,  -84.9346, -114.4091,   68.4246,
           26.9088]], dtype=torch.float64)
	q_value: tensor([[-27.2452]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3207766891523276, distance: 1.3151381948309018 entropy 5.733376077016786
epoch: 5, step: 30
	action: tensor([[-166.9716, -147.3877,   23.0501,  -41.0414, -242.1365,  114.3029,
           62.7604]], dtype=torch.float64)
	q_value: tensor([[-33.1059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11614242549965259, distance: 1.2089727204338356 entropy 5.979140291143933
epoch: 5, step: 31
	action: tensor([[-102.6923, -199.3876,  -11.7533, -114.3352,   15.7165, -199.3470,
         -101.4729]], dtype=torch.float64)
	q_value: tensor([[-34.1980]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7699675058520707, distance: 1.5224370344891218 entropy 6.0064430101583595
epoch: 5, step: 32
	action: tensor([[-180.9147, -339.7487,   -0.5430,  -53.1700,   15.8125,  -86.2694,
           51.9584]], dtype=torch.float64)
	q_value: tensor([[-35.9383]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22139331250637706, distance: 1.009754408778957 entropy 5.915221021742599
epoch: 5, step: 33
	action: tensor([[-154.5889,  -87.4617,   -1.9594,  -68.2132,  -27.9331,   78.2480,
            0.6326]], dtype=torch.float64)
	q_value: tensor([[-35.7692]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.01463321943376128, distance: 1.1526865664035844 entropy 5.939185631987938
epoch: 5, step: 34
	action: tensor([[  49.5370,  -63.8223,  -60.4930,   61.2310,   94.3713,   68.0838,
         -183.8617]], dtype=torch.float64)
	q_value: tensor([[-41.9305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5192580614189815, distance: 1.4104979782752343 entropy 6.043185282513968
epoch: 5, step: 35
	action: tensor([[-109.4098, -129.4860,  -81.4240,   57.0848,  -82.8427,  123.7406,
           -1.2430]], dtype=torch.float64)
	q_value: tensor([[-32.7574]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2858893090748498, distance: 1.7301521290405093 entropy 5.8847162647322495
epoch: 5, step: 36
	action: tensor([[  13.0693, -120.4245,  -96.1923,   93.4169, -201.1615,  -68.1097,
           30.4381]], dtype=torch.float64)
	q_value: tensor([[-28.2559]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2804023786299714, distance: 1.7280743976381279 entropy 5.769091463727696
epoch: 5, step: 37
	action: tensor([[  14.6547, -140.8150,  -38.1485,  -12.1927,  -42.7730,  -49.4990,
           64.6955]], dtype=torch.float64)
	q_value: tensor([[-16.2974]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7487458314047377, distance: 1.5132826030919537 entropy 5.30077543249
epoch: 5, step: 38
	action: tensor([[ -98.7224,   19.2226,  -25.7005,  -32.0173,  -36.8312, -122.6285,
          -22.1110]], dtype=torch.float64)
	q_value: tensor([[-16.3664]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3810600682459113, distance: 1.344816340420614 entropy 5.236225025524822
epoch: 5, step: 39
	action: tensor([[ -99.7834,   19.2093,   53.4095,  -24.6793, -100.9373,  -34.7583,
          -92.9245]], dtype=torch.float64)
	q_value: tensor([[-19.8654]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.041502527806504985, distance: 1.1203460290981984 entropy 5.3628011477188124
epoch: 5, step: 40
	action: tensor([[-2.1096e+02,  8.7931e-02,  9.2481e+01,  1.0027e+02, -3.0952e+01,
         -1.7172e+02,  1.8702e+02]], dtype=torch.float64)
	q_value: tensor([[-33.4462]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0301427191637864, distance: 1.6304969029756442 entropy 5.824856795206778
epoch: 5, step: 41
	action: tensor([[-144.9469,   44.1201, -103.6634,  -55.4764,    3.6361,  138.5997,
           75.8789]], dtype=torch.float64)
	q_value: tensor([[-29.1285]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.3746557810974256, distance: 1.76342513162981 entropy 5.673732543755792
epoch: 5, step: 42
	action: tensor([[ -65.1717,  -84.8452,   -3.1422, -137.9573,   57.5511,  188.0594,
          -82.9364]], dtype=torch.float64)
	q_value: tensor([[-36.1610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0649876982086637, distance: 1.6444301380707929 entropy 5.882033432657082
epoch: 5, step: 43
	action: tensor([[  15.6383,  -97.6530, -107.9353,  145.4275,   58.2487,   96.5749,
          -38.0219]], dtype=torch.float64)
	q_value: tensor([[-38.0809]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.354171144423459, distance: 1.7558026871448786 entropy 6.002925418451745
epoch: 5, step: 44
	action: tensor([[-181.4529,  -58.0811,   11.8099,  118.7472,    6.4405,   69.7643,
           49.8932]], dtype=torch.float64)
	q_value: tensor([[-26.2200]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4399312696515836, distance: 1.3731803330842491 entropy 5.7331740226533014
epoch: 5, step: 45
	action: tensor([[-75.3046, -45.9780,  -3.5737,  13.5618, -31.5757, -47.9246,   5.7758]],
       dtype=torch.float64)
	q_value: tensor([[-24.8539]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0786139483570556, distance: 1.1884740661119424 entropy 5.722818422176959
epoch: 5, step: 46
	action: tensor([[  -7.3360,   61.8654, -124.6010,  -92.9080,   96.4172,   68.0710,
          -97.7792]], dtype=torch.float64)
	q_value: tensor([[-28.4554]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.404002037710061, distance: 1.774287974232812 entropy 5.828750524801213
epoch: 5, step: 47
	action: tensor([[-115.4228,  -70.6852,  118.6255,   25.4848,  -21.9166,   95.6942,
           45.9965]], dtype=torch.float64)
	q_value: tensor([[-26.6275]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -2.2385601336958514, distance: 2.059361911135152 entropy 5.778997244925202
epoch: 5, step: 48
	action: tensor([[ -80.6649,  -93.8826,  -13.4142, -242.6915,   53.8311,   21.0847,
           -8.6422]], dtype=torch.float64)
	q_value: tensor([[-35.1601]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.41063494290027935, distance: 1.3591394302521487 entropy 6.038469338291628
epoch: 5, step: 49
	action: tensor([[ -61.8056,  -10.8222,  118.5481,  -48.2539,  -33.3980,  -47.6723,
         -159.1193]], dtype=torch.float64)
	q_value: tensor([[-37.6298]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.38208181481506376, distance: 1.3453137146714935 entropy 6.028571045466579
epoch: 5, step: 50
	action: tensor([[-111.9975, -149.1157, -135.8923,  -46.3708, -108.8967,   62.3513,
            7.1291]], dtype=torch.float64)
	q_value: tensor([[-37.1040]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42476175889989776, distance: 0.8679217424918507 entropy 5.8621018020023
epoch: 5, step: 51
	action: tensor([[ -57.9418,  -64.7973, -162.7409,   84.7960,   49.9413,  -22.7442,
          -84.9908]], dtype=torch.float64)
	q_value: tensor([[-42.5754]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7974496981808654, distance: 1.5342109073188073 entropy 5.935228226880426
epoch: 5, step: 52
	action: tensor([[-145.8796,  -53.1822,  -48.1931,  103.5946,   33.6551,   62.7981,
          -92.6400]], dtype=torch.float64)
	q_value: tensor([[-26.9386]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -2.278121385554348, distance: 2.071901995939882 entropy 5.687049283566377
epoch: 5, step: 53
	action: tensor([[ -82.0608,  -26.7133,   90.0335,   34.3907,   26.2088, -172.4507,
           48.3428]], dtype=torch.float64)
	q_value: tensor([[-28.0482]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.4193154938063928, distance: 1.7799300970239356 entropy 5.770345586405476
epoch: 5, step: 54
	action: tensor([[-104.3911, -247.7692,    7.9694,  -56.8728, -166.9346,   79.8256,
            4.9608]], dtype=torch.float64)
	q_value: tensor([[-34.9299]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.399289196018442, distance: 1.7725479506292394 entropy 5.994362306535995
epoch: 5, step: 55
	action: tensor([[-108.3238, -120.5878,  -13.0200,  -85.9773,  -86.2381, -166.7764,
          155.6264]], dtype=torch.float64)
	q_value: tensor([[-28.5011]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2463624359541723, distance: 1.7151282779000465 entropy 5.868951529455764
epoch: 5, step: 56
	action: tensor([[-7.4012e+01, -2.3093e+00, -6.1703e-02,  3.5533e+01,  1.3640e+02,
         -3.5038e-01,  7.9860e+01]], dtype=torch.float64)
	q_value: tensor([[-27.1194]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.73386635796795
epoch: 5, step: 57
	action: tensor([[-101.8360,  -46.0773,   15.6304,   55.0951, -121.2233,   49.5731,
          -61.6053]], dtype=torch.float64)
	q_value: tensor([[-31.7652]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.01724866049825957, distance: 1.1541712622547091 entropy 5.475063414723296
epoch: 5, step: 58
	action: tensor([[-273.6170, -129.6727,  114.6351,  181.9521,   33.2298,  -22.3640,
            2.5901]], dtype=torch.float64)
	q_value: tensor([[-39.8248]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5021699526526269, distance: 1.4025431408966436 entropy 5.933278293540931
epoch: 5, step: 59
	action: tensor([[-136.2027,  -85.1632,   59.7010,  120.7883,    4.6815,  -54.0504,
            2.5585]], dtype=torch.float64)
	q_value: tensor([[-34.3444]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1257501397585915, distance: 1.0699770905767185 entropy 5.897470184846122
epoch: 5, step: 60
	action: tensor([[-113.5143, -161.1811,   48.1910,  103.1779, -236.6460,  172.5889,
          -29.7111]], dtype=torch.float64)
	q_value: tensor([[-45.0350]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6722476138799387, distance: 1.4798135026016632 entropy 6.140269757739653
epoch: 5, step: 61
	action: tensor([[-128.1490, -139.0951, -124.4493,    2.5011,  -18.1978,  -99.3497,
          168.4228]], dtype=torch.float64)
	q_value: tensor([[-38.8375]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 6.017856200907161
epoch: 5, step: 62
	action: tensor([[-169.4733,  -99.3741,   -6.2186,   48.7566,   36.5410,   96.0039,
          -17.6709]], dtype=torch.float64)
	q_value: tensor([[-31.7652]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18454736973556285, distance: 1.245468963615466 entropy 5.475063414723296
epoch: 5, step: 63
	action: tensor([[ -93.0129,  -44.6414, -111.1413,   24.4428,   -7.4623,  -21.7719,
           22.7317]], dtype=torch.float64)
	q_value: tensor([[-28.3226]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.4096817670484691, distance: 1.7763827166724042 entropy 5.533970845119284
epoch: 5, step: 64
	action: tensor([[-59.2680,  79.2248,  56.7206, 121.7348,  -6.6891,  57.2593,   2.4589]],
       dtype=torch.float64)
	q_value: tensor([[-27.4730]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24641733345324213, distance: 0.9933953974908646 entropy 5.807215002635433
epoch: 5, step: 65
	action: tensor([[-112.9026,  -92.0516,  119.4307,   28.3376,  -50.8912,  189.3978,
           17.3963]], dtype=torch.float64)
	q_value: tensor([[-30.6784]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7126811369082262, distance: 0.6133929256731054 entropy 5.815611536777274
epoch: 5, step: 66
	action: tensor([[-29.9670, -94.2611, -35.1842,  23.0532, -91.0807,  -6.8319,   0.3183]],
       dtype=torch.float64)
	q_value: tensor([[-22.4757]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3069031323589868, distance: 0.9526945073007105 entropy 5.699200425847061
epoch: 5, step: 67
	action: tensor([[ -92.5006,   -4.8907,  -92.5747,  -76.2917,  -61.8413, -181.6532,
          163.7356]], dtype=torch.float64)
	q_value: tensor([[-28.6200]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8432780511870368, distance: 1.553646171370346 entropy 6.012581917832071
epoch: 5, step: 68
	action: tensor([[   4.9816, -125.6159,  -29.6511,    9.9914,    5.6362,  -35.1632,
          -23.7421]], dtype=torch.float64)
	q_value: tensor([[-22.0124]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.4336041947852305
epoch: 5, step: 69
	action: tensor([[ 20.5024, -46.8359,  38.6627, -58.3403,   8.1579, -56.2937,   3.5135]],
       dtype=torch.float64)
	q_value: tensor([[-31.7652]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11124770217224977, distance: 1.2063188973900072 entropy 5.475063414723296
epoch: 5, step: 70
	action: tensor([[ -90.9891,  -85.0861,  -18.4645,   50.8743,   68.2794, -102.2262,
           72.3376]], dtype=torch.float64)
	q_value: tensor([[-28.5159]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23197287523711396, distance: 1.270156591199207 entropy 5.738820327272903
epoch: 5, step: 71
	action: tensor([[-238.8746, -140.9999,   52.1691,  103.0475,   80.6711,   90.5421,
          -86.3596]], dtype=torch.float64)
	q_value: tensor([[-28.2572]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1186294436348296, distance: 1.6656517102277344 entropy 5.883516493455772
epoch: 5, step: 72
	action: tensor([[-88.0985, -72.9636, 115.0436, -29.2782,  -3.4929, 114.4764,  42.0161]],
       dtype=torch.float64)
	q_value: tensor([[-23.7783]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4083643598347342, distance: 1.3580451422416961 entropy 5.719283350915077
epoch: 5, step: 73
	action: tensor([[ -88.6085, -329.5431,  -79.2809,   72.4228,   -5.3550,   22.3029,
           -5.8930]], dtype=torch.float64)
	q_value: tensor([[-36.7546]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7040676824531833, distance: 1.493826341545364 entropy 6.0792625298939855
epoch: 5, step: 74
	action: tensor([[ -40.5224,  -28.5284,   77.9454, -182.7958,   43.6812,   35.7122,
         -228.2169]], dtype=torch.float64)
	q_value: tensor([[-28.4266]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0175722084177172, distance: 1.6254410995078372 entropy 5.819543249680964
epoch: 5, step: 75
	action: tensor([[-238.7137, -103.5347, -107.7445,  139.6200,  154.9146,   90.0457,
         -204.2158]], dtype=torch.float64)
	q_value: tensor([[-32.7867]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03177773144780427, distance: 1.1260151317466385 entropy 6.023393765434281
epoch: 5, step: 76
	action: tensor([[-102.3945, -310.0054, -159.4400,  -83.5390,  -51.9950,  -52.2969,
           46.1298]], dtype=torch.float64)
	q_value: tensor([[-37.5854]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.41358458806541676, distance: 1.3605596720937503 entropy 6.107207300861153
epoch: 5, step: 77
	action: tensor([[-103.5262, -222.7557,  -16.8274,  118.4491,   57.5878,  -26.4766,
          108.8078]], dtype=torch.float64)
	q_value: tensor([[-35.8529]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6765219513753633, distance: 1.4817035294014018 entropy 6.0370080910638375
epoch: 5, step: 78
	action: tensor([[-104.7529, -187.3597,   11.2559,   12.2596, -137.0367,   90.5973,
          219.9145]], dtype=torch.float64)
	q_value: tensor([[-29.7547]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.3957304920111793, distance: 1.7712329123483717 entropy 5.911336016096491
epoch: 5, step: 79
	action: tensor([[  13.8679,  -57.2590,   63.7121,  -11.6320,  -53.7356,   92.0601,
         -109.6157]], dtype=torch.float64)
	q_value: tensor([[-27.9246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.717779564001904
epoch: 5, step: 80
	action: tensor([[-2.5729e+01, -1.1334e+01,  6.2857e+00,  4.3396e+01, -9.6335e-04,
         -1.6110e+02, -1.0262e+02]], dtype=torch.float64)
	q_value: tensor([[-31.7652]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21549801728708817, distance: 1.2616352674392322 entropy 5.475063414723296
epoch: 5, step: 81
	action: tensor([[-188.3025,  -64.5707,  -15.2571,  -25.6293,   62.3340,   80.6338,
           50.4191]], dtype=torch.float64)
	q_value: tensor([[-30.7680]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19254949820836598, distance: 1.0282877158350257 entropy 5.643037056693421
epoch: 5, step: 82
	action: tensor([[122.0831,   8.2585,  65.2778, 103.0016,   6.5295, -37.1139, -13.3160]],
       dtype=torch.float64)
	q_value: tensor([[-31.2534]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.668213617633654
epoch: 5, step: 83
	action: tensor([[-153.7184,  -82.4447,   -3.5747,  -29.3188,  -88.5342,  -56.8114,
           53.8930]], dtype=torch.float64)
	q_value: tensor([[-31.7652]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.475063414723296
epoch: 5, step: 84
	action: tensor([[-100.2398,  -54.1315,   49.4683,   15.8529, -120.3472,  -78.3099,
           57.4863]], dtype=torch.float64)
	q_value: tensor([[-31.7652]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5298649918951843, distance: 0.784635564452789 entropy 5.475063414723296
epoch: 5, step: 85
	action: tensor([[ -65.2014, -130.6512,   47.5644,   -7.6044,    1.1739,  -81.1798,
          -11.2168]], dtype=torch.float64)
	q_value: tensor([[-28.0880]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2540117372692101, distance: 1.2814671981358778 entropy 5.771040501734224
epoch: 5, step: 86
	action: tensor([[-111.6134,  -13.1027,   23.8181,   98.9284,  -91.0778,   70.3910,
           89.8934]], dtype=torch.float64)
	q_value: tensor([[-33.3506]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.8281299228182109, distance: 1.92444884662632 entropy 5.7753036407175555
epoch: 5, step: 87
	action: tensor([[ -94.7457,   51.5616,   -8.2118,  102.4666, -218.1562,  -67.0283,
          -89.8643]], dtype=torch.float64)
	q_value: tensor([[-34.2903]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.022941377064838, distance: 1.6276024766344876 entropy 5.95006335252243
epoch: 5, step: 88
	action: tensor([[  8.8928, -22.7303, -77.9043, 105.6465, -84.3457, -95.4495, -29.7828]],
       dtype=torch.float64)
	q_value: tensor([[-32.6949]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.43952688136309204, distance: 0.8567104949940936 entropy 5.933914318088031
epoch: 5, step: 89
	action: tensor([[ -95.3365,    0.9424,   82.6285,  149.3723, -150.1674,  212.3443,
          -70.5411]], dtype=torch.float64)
	q_value: tensor([[-31.5335]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6407759374222017, distance: 0.6858662036424582 entropy 6.080052255345092
epoch: 5, step: 90
	action: tensor([[-87.6910, -79.2807, 123.1376, -49.6401, -40.5704, 107.9362,  52.4681]],
       dtype=torch.float64)
	q_value: tensor([[-27.2201]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.49826434036247, distance: 1.4007186619515581 entropy 5.701471295530849
epoch: 5, step: 91
	action: tensor([[-303.3451,   64.6544, -121.2180,  155.6212,  219.8297,  -23.1285,
           71.3406]], dtype=torch.float64)
	q_value: tensor([[-39.9755]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5916476912657112, distance: 1.4437106662433052 entropy 6.101391680616609
epoch: 5, step: 92
	action: tensor([[-175.8858,  -62.4768,   48.9866,  -67.6249,   43.4123,  -22.6571,
         -203.6867]], dtype=torch.float64)
	q_value: tensor([[-32.4139]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7715962783690118, distance: 1.523137367512726 entropy 5.836448538863775
epoch: 5, step: 93
	action: tensor([[-153.6387, -117.5187, -159.5538,   38.4530,   30.2179, -175.8601,
           92.4664]], dtype=torch.float64)
	q_value: tensor([[-26.8159]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0038853009987796, distance: 1.6199183428011426 entropy 5.779190984388328
epoch: 5, step: 94
	action: tensor([[-237.4075,  -70.2573,  -10.3319,  141.2053,   59.9455,   13.1395,
          -35.8054]], dtype=torch.float64)
	q_value: tensor([[-31.0312]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42257249548475206, distance: 0.8695717587264753 entropy 5.998121601707315
epoch: 5, step: 95
	action: tensor([[  -1.1080, -123.6415,   66.5221,  -79.3692,  -76.5190,   96.5477,
         -245.2645]], dtype=torch.float64)
	q_value: tensor([[-33.4633]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21420235045435065, distance: 1.0144065761797854 entropy 6.181232393668203
epoch: 5, step: 96
	action: tensor([[ 133.0230, -273.4742,  -55.4762, -116.9684,  -93.7054,   61.5823,
          -29.7936]], dtype=torch.float64)
	q_value: tensor([[-29.7462]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13812868428973235, distance: 1.0623751578594558 entropy 6.013313150843738
epoch: 5, step: 97
	action: tensor([[-86.9955, -47.3450,  -2.4142,  59.1428,  85.0797,  69.8192,  48.8443]],
       dtype=torch.float64)
	q_value: tensor([[-26.8811]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7363496719550402, distance: 1.5079095368569224 entropy 5.699190615121942
epoch: 5, step: 98
	action: tensor([[ -98.2443, -232.4467,   74.9321,   10.6330,  -31.6241,  167.7009,
         -148.9409]], dtype=torch.float64)
	q_value: tensor([[-34.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 6.023104721269765
epoch: 5, step: 99
	action: tensor([[-96.7857, -81.0451,  41.3844, -45.8178, -44.4838, -59.9708, -10.0604]],
       dtype=torch.float64)
	q_value: tensor([[-31.7652]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.475063414723296
epoch: 5, step: 100
	action: tensor([[ -59.8077,  -47.0019, -102.7001,  -56.7056,  -91.1317,   13.5217,
           88.5302]], dtype=torch.float64)
	q_value: tensor([[-31.7652]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05172232587832182, distance: 1.1143572839980287 entropy 5.475063414723296
epoch: 5, step: 101
	action: tensor([[-118.0812,  -99.7173,   42.5937,   27.9001,  -56.4362,   47.5591,
            0.4062]], dtype=torch.float64)
	q_value: tensor([[-31.5722]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08618541268111946, distance: 1.1926380919370771 entropy 5.796028869188215
epoch: 5, step: 102
	action: tensor([[-35.9026, -56.3115, -91.5004,  16.1967, -70.9388, 130.1210,  46.6770]],
       dtype=torch.float64)
	q_value: tensor([[-25.6074]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20364277042952184, distance: 1.2554675704563414 entropy 5.517419763989387
epoch: 5, step: 103
	action: tensor([[-140.6599, -177.1604, -143.5773,  -93.2587,   85.7854, -271.8441,
          -70.0707]], dtype=torch.float64)
	q_value: tensor([[-35.6237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16137260935907172, distance: 1.233225461833228 entropy 6.071476005030633
epoch: 5, step: 104
	action: tensor([[ -94.2356,  -29.5458,  156.5071,   58.7762, -133.9685,   68.2072,
           84.6873]], dtype=torch.float64)
	q_value: tensor([[-22.9580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4456580805986299, distance: 1.3759082899090134 entropy 5.776144314511916
epoch: 5, step: 105
	action: tensor([[ -75.4678,  -87.0223, -120.2442,  -16.9546, -114.7705,  -17.5953,
           76.6026]], dtype=torch.float64)
	q_value: tensor([[-26.0479]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.907756953157694
epoch: 5, step: 106
	action: tensor([[-87.4192, -78.7788,  30.7651,  91.5916,  90.3580,  69.2641, -65.6242]],
       dtype=torch.float64)
	q_value: tensor([[-31.7652]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3700362826612992, distance: 0.9082689376721543 entropy 5.475063414723296
epoch: 5, step: 107
	action: tensor([[-182.3325,  -17.8778,   20.0727,   -3.3497,  -33.8839,  -26.5806,
          -61.0264]], dtype=torch.float64)
	q_value: tensor([[-30.1335]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.743082278194284
epoch: 5, step: 108
	action: tensor([[-74.4811, -77.1965, -26.8007, 172.9950, -49.7951,  -4.3592, -66.8429]],
       dtype=torch.float64)
	q_value: tensor([[-31.7652]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.475063414723296
epoch: 5, step: 109
	action: tensor([[-11.8567,  76.9154, -47.1950,  52.9565, -70.3257,  23.2521, 135.6632]],
       dtype=torch.float64)
	q_value: tensor([[-31.7652]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.475063414723296
epoch: 5, step: 110
	action: tensor([[ -46.5382, -113.1339,   73.3425,   28.1353,    5.2246,  -61.8885,
           36.0218]], dtype=torch.float64)
	q_value: tensor([[-31.7652]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.468257005971352, distance: 1.3866208754486424 entropy 5.475063414723296
epoch: 5, step: 111
	action: tensor([[ -91.5362, -141.7050,  -72.7121,  -15.0222,  -41.1541,   43.7238,
          -12.6439]], dtype=torch.float64)
	q_value: tensor([[-32.0094]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.49171171862915997, distance: 1.3976523015445865 entropy 5.753357889954096
epoch: 5, step: 112
	action: tensor([[ 134.2309,  -81.8439,  -43.5822,   31.4635, -159.1730,  -69.0266,
           52.5886]], dtype=torch.float64)
	q_value: tensor([[-32.6470]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.8430660489530775
epoch: 5, step: 113
	action: tensor([[-122.6418,  -58.7138,  -59.5714,  -47.4758,   48.4903,   21.0946,
          100.7213]], dtype=torch.float64)
	q_value: tensor([[-31.7652]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30268627860136943, distance: 0.9555882452595204 entropy 5.475063414723296
epoch: 5, step: 114
	action: tensor([[ -19.6143, -178.6470, -144.3491,  -25.8728,  -40.4445,  150.1058,
          -26.7948]], dtype=torch.float64)
	q_value: tensor([[-43.0810]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13721731249825164, distance: 1.2203331939527673 entropy 5.970929243401518
epoch: 5, step: 115
	action: tensor([[-136.6761,  -28.0247,  119.3063,  167.7264,  -61.7257,   43.8850,
          -44.2012]], dtype=torch.float64)
	q_value: tensor([[-35.1105]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4986434222890166, distance: 1.400895851494173 entropy 5.9661959305347425
epoch: 5, step: 116
	action: tensor([[-108.1286,  -19.0240,  -74.1596,   49.0177,  -52.3009,  153.9985,
           46.8199]], dtype=torch.float64)
	q_value: tensor([[-23.7442]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.307528229546866, distance: 1.738321912182733 entropy 5.640916891404692
epoch: 5, step: 117
	action: tensor([[-122.8391,  -34.6443,    1.4436,  -47.6100,   87.0247,   68.6437,
           68.4097]], dtype=torch.float64)
	q_value: tensor([[-31.0723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9526800449020154, distance: 1.5990875327044498 entropy 5.894586470038036
epoch: 5, step: 118
	action: tensor([[-234.1588, -208.4402,   49.7991,  -59.7592,    8.4892,   58.3937,
           88.5251]], dtype=torch.float64)
	q_value: tensor([[-28.7630]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3119749635835126, distance: 0.9492023707758873 entropy 5.754282198419633
epoch: 5, step: 119
	action: tensor([[  -3.0004,  -65.8390,  -27.8581,  107.4233, -122.5792,   54.7308,
           78.4736]], dtype=torch.float64)
	q_value: tensor([[-30.8522]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.903482238587704
epoch: 5, step: 120
	action: tensor([[-49.1753, -64.4433, -38.2430, -18.2607, -12.2303,  35.4574,  87.8326]],
       dtype=torch.float64)
	q_value: tensor([[-31.7652]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4302198558678991, distance: 1.3685418882749556 entropy 5.475063414723296
epoch: 5, step: 121
	action: tensor([[-91.9631, -28.0408, 126.9309,  59.9500,  -4.0209, -17.5267, -69.5111]],
       dtype=torch.float64)
	q_value: tensor([[-23.0358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.46418584858392165, distance: 1.3846971419241605 entropy 5.469000100133444
epoch: 5, step: 122
	action: tensor([[ -86.8671,   -5.5543, -100.6261,   -4.1614, -104.0193,  -48.6175,
           81.4197]], dtype=torch.float64)
	q_value: tensor([[-25.7175]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.755534419796845
epoch: 5, step: 123
	action: tensor([[-57.3881, -67.1055, -53.5724,  90.0844,  11.5638,   1.4455,  49.3534]],
       dtype=torch.float64)
	q_value: tensor([[-31.7652]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09800980119581326, distance: 1.1991121449575435 entropy 5.475063414723296
epoch: 5, step: 124
	action: tensor([[ -25.6771,  -17.3177,  -39.6786, -128.7559, -122.3740,  -81.1188,
          194.0279]], dtype=torch.float64)
	q_value: tensor([[-37.1446]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.01978598837988188, distance: 1.1556097930495899 entropy 5.8215410633901366
epoch: 5, step: 125
	action: tensor([[-128.5103, -218.6261,   79.9492,  153.8650,   51.4103, -158.1433,
          -93.6057]], dtype=torch.float64)
	q_value: tensor([[-41.2208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9721138986472933, distance: 1.60702521159603 entropy 5.979658514158613
epoch: 5, step: 126
	action: tensor([[ -6.8464, -66.3566, -55.7804, -68.0634,  31.5697,  44.9104,  68.5536]],
       dtype=torch.float64)
	q_value: tensor([[-28.4674]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5401680552258159, distance: 1.4201713560954168 entropy 5.754675148597443
epoch: 5, step: 127
	action: tensor([[-21.5384,  32.9599,   0.9447,  33.1328,  10.1835, -61.3372,  10.0426]],
       dtype=torch.float64)
	q_value: tensor([[-15.4891]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 4.871663843074382
LOSS epoch 5 actor 293.73751267657155 critic 236.78558802005287
epoch: 6, step: 0
	action: tensor([[-194.0536,  -27.4984,   36.7571,  -89.7778,  -66.2578,   96.0206,
          124.2748]], dtype=torch.float64)
	q_value: tensor([[-32.4792]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5656710032164832, distance: 0.7541645351052236 entropy 5.657581256270132
epoch: 6, step: 1
	action: tensor([[-256.9749,  -30.5783,    2.8273,  111.0084,   39.7165,   97.3655,
            3.4380]], dtype=torch.float64)
	q_value: tensor([[-30.8967]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13361836187242915, distance: 1.218400670085751 entropy 6.0691195786413115
epoch: 6, step: 2
	action: tensor([[ -93.9670, -105.9713,   17.2675, -112.0139,  -49.6819, -123.8355,
          -81.7594]], dtype=torch.float64)
	q_value: tensor([[-39.4488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35069482745648506, distance: 0.9221065873907199 entropy 6.219808844769578
epoch: 6, step: 3
	action: tensor([[-113.5387,  -48.0042,  -68.7220,   40.7702, -101.3117,  -10.1023,
           71.7363]], dtype=torch.float64)
	q_value: tensor([[-30.5916]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.897406551505489
epoch: 6, step: 4
	action: tensor([[ 50.2962,  23.2797, -99.3019,  72.0018, -15.0862,  28.1546,  -7.1000]],
       dtype=torch.float64)
	q_value: tensor([[-32.4792]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.657581256270132
epoch: 6, step: 5
	action: tensor([[-116.3440, -175.6631,  -30.1100,  -45.8450,   84.1995,    5.6799,
           60.6915]], dtype=torch.float64)
	q_value: tensor([[-32.4792]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0822640722819452, distance: 1.6512947349073177 entropy 5.657581256270132
epoch: 6, step: 6
	action: tensor([[-262.0632, -314.7186,   84.5488,  -39.5374,  -52.5778,  135.0239,
           25.5539]], dtype=torch.float64)
	q_value: tensor([[-35.6344]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6228886647166179, distance: 1.4578104421386675 entropy 6.025994394075714
epoch: 6, step: 7
	action: tensor([[ -29.0817, -210.7867, -183.6551,   93.2102,  -81.7248,   -9.2591,
           -0.4763]], dtype=torch.float64)
	q_value: tensor([[-34.7270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.19495591683414593, distance: 1.2509289265115944 entropy 6.084267117544149
epoch: 6, step: 8
	action: tensor([[-125.7565,  -61.0196,   23.1037,  -84.8904,   -5.8425,   76.3747,
          -86.7658]], dtype=torch.float64)
	q_value: tensor([[-32.9875]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3078780236109159, distance: 0.9520242544030882 entropy 6.062691901164998
epoch: 6, step: 9
	action: tensor([[-150.3441,   17.4181,  101.5572,  117.5961, -122.4794,  114.2484,
           53.1768]], dtype=torch.float64)
	q_value: tensor([[-31.4828]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09493760283646413, distance: 1.197433430050226 entropy 6.008350388858756
epoch: 6, step: 10
	action: tensor([[-102.3204, -233.1298,   56.9456,  -85.2904,  -92.1899,  -62.8503,
          124.4478]], dtype=torch.float64)
	q_value: tensor([[-35.6973]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32906063539278285, distance: 0.9373425453362803 entropy 6.181482346087001
epoch: 6, step: 11
	action: tensor([[ -89.7047,  -87.6962, -166.5253,  129.2778, -164.1185,  132.8543,
           74.7224]], dtype=torch.float64)
	q_value: tensor([[-31.0090]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0708067441233653, distance: 1.6467454745494903 entropy 5.989802555658346
epoch: 6, step: 12
	action: tensor([[-43.9313, -80.7300, -25.6290,   2.8810,  29.6146, 179.1548, -27.5188]],
       dtype=torch.float64)
	q_value: tensor([[-31.2604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.949061513482568
epoch: 6, step: 13
	action: tensor([[-208.0732,  -50.3166,   10.3080,   14.8651, -115.1530,  -65.6766,
           13.3760]], dtype=torch.float64)
	q_value: tensor([[-32.4792]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.42114004729828936, distance: 1.364190849844379 entropy 5.657581256270132
epoch: 6, step: 14
	action: tensor([[   6.7043,  -74.9794,  221.3535,  210.9344,  131.6449, -180.9977,
          205.8812]], dtype=torch.float64)
	q_value: tensor([[-34.2042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.46159889851522407, distance: 0.8396719842198849 entropy 6.13511404381038
epoch: 6, step: 15
	action: tensor([[-227.5565, -218.8975,   58.1210,  -46.1421, -182.9223, -133.9411,
          -70.9974]], dtype=torch.float64)
	q_value: tensor([[-27.4877]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5437014043720516, distance: 1.4217994532895544 entropy 5.957777784298355
epoch: 6, step: 16
	action: tensor([[-156.0966,  -30.1074,   65.5917,   14.2534,  -35.2037,  -65.2003,
          125.7235]], dtype=torch.float64)
	q_value: tensor([[-26.5496]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.052639012321029965, distance: 1.1138185371556606 entropy 5.986496674825119
epoch: 6, step: 17
	action: tensor([[ -72.5173, -162.2324, -154.5078,  -65.7464,   97.7589, -180.9260,
          113.8439]], dtype=torch.float64)
	q_value: tensor([[-33.9628]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0909574258850343, distance: 1.0910604292417103 entropy 6.328271195058983
epoch: 6, step: 18
	action: tensor([[-238.8386,   22.2155,   81.8905,   -5.7416,  -24.6426, -163.4182,
           98.2799]], dtype=torch.float64)
	q_value: tensor([[-32.8211]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15885553001665587, distance: 1.2318883327963308 entropy 6.280697843747128
epoch: 6, step: 19
	action: tensor([[-58.2926, -46.3702,  76.4386,  16.8822,  71.3772,  48.9599,  70.9370]],
       dtype=torch.float64)
	q_value: tensor([[-21.7057]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.520980901995888
epoch: 6, step: 20
	action: tensor([[ -84.8496,  -48.1710,   60.5424,   52.4489,  -44.5487, -110.3280,
           12.6387]], dtype=torch.float64)
	q_value: tensor([[-32.4792]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17152793282706502, distance: 1.0415871860394559 entropy 5.657581256270132
epoch: 6, step: 21
	action: tensor([[-240.4171,  -29.5554,  -21.6246,   40.4441,  -44.9964, -148.5241,
           98.5166]], dtype=torch.float64)
	q_value: tensor([[-34.7921]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.40580827967337396, distance: 1.3568122053669374 entropy 6.015420058475915
epoch: 6, step: 22
	action: tensor([[-367.8127,  -11.0857,  -60.4386,   48.3676,  -20.7569,  278.6794,
          -78.0179]], dtype=torch.float64)
	q_value: tensor([[-36.3910]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.031103013098505494, distance: 1.126407402603509 entropy 6.215851445027758
epoch: 6, step: 23
	action: tensor([[-156.3707,   11.2998,  -86.4793,  -68.3912, -178.8699,    2.1310,
          -10.3643]], dtype=torch.float64)
	q_value: tensor([[-32.8079]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08908555399992846, distance: 1.1942292168329638 entropy 6.264074480745799
epoch: 6, step: 24
	action: tensor([[-143.8222, -109.9149,   -8.3326,  101.4925, -138.2318,  -77.2335,
          117.3285]], dtype=torch.float64)
	q_value: tensor([[-30.5548]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.44310022343089805, distance: 1.374690527891526 entropy 6.124862145913946
epoch: 6, step: 25
	action: tensor([[ -91.0333, -335.2734,   82.2772, -205.4510,   51.1495,   42.6560,
           21.4197]], dtype=torch.float64)
	q_value: tensor([[-30.5978]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.4848131627667098, distance: 1.8038630504513566 entropy 6.259016411441289
epoch: 6, step: 26
	action: tensor([[-287.4618, -205.2386,  -17.6826,  167.6575,  -30.7348, -114.6871,
           32.8075]], dtype=torch.float64)
	q_value: tensor([[-27.8689]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.606420259051208, distance: 1.450394951903893 entropy 6.019754011592404
epoch: 6, step: 27
	action: tensor([[-39.2388, -91.9081,  70.0664,  26.3544,  65.8893,  31.3167, -38.5419]],
       dtype=torch.float64)
	q_value: tensor([[-28.4930]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2170171408673629, distance: 1.2624234141075354 entropy 6.027642304865721
epoch: 6, step: 28
	action: tensor([[-109.1809, -161.9456,  192.7483,   62.0243,  -42.0494,   82.8632,
          105.7141]], dtype=torch.float64)
	q_value: tensor([[-28.9006]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6984074872711235, distance: 1.4913433469699888 entropy 6.035505171132175
epoch: 6, step: 29
	action: tensor([[-190.6212, -131.3226,   96.1034,  235.6750, -101.2850,  -48.1151,
           34.9782]], dtype=torch.float64)
	q_value: tensor([[-26.6533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06580641802464693, distance: 1.181396986788976 entropy 5.994267371912116
epoch: 6, step: 30
	action: tensor([[-187.3182,   73.9610,  -14.9426, -300.7294,  128.0952,  -54.5350,
           -6.0985]], dtype=torch.float64)
	q_value: tensor([[-23.3729]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7532967438734384, distance: 1.515250396759981 entropy 6.039628950985217
epoch: 6, step: 31
	action: tensor([[-158.2284, -109.5184,   29.7322,   36.3065,   62.4325,  -99.0832,
           62.5858]], dtype=torch.float64)
	q_value: tensor([[-28.8083]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1750224989560496, distance: 1.240451488217735 entropy 5.915363823621098
epoch: 6, step: 32
	action: tensor([[-168.5769,  -29.1276,  146.2537,  107.9845,  106.9589, -190.3167,
          -43.4867]], dtype=torch.float64)
	q_value: tensor([[-24.8773]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2230294462554433, distance: 1.0086929215626126 entropy 5.969850381436315
epoch: 6, step: 33
	action: tensor([[-72.4096, -50.8284, -49.6321, -85.1756, 107.0587, -61.3303,  14.6415]],
       dtype=torch.float64)
	q_value: tensor([[-25.6347]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9032408288488167, distance: 1.5787143847373937 entropy 5.986369647567396
epoch: 6, step: 34
	action: tensor([[-176.3806,  -86.2950, -113.5639,  119.0644,   13.8455,   20.9839,
          -95.2625]], dtype=torch.float64)
	q_value: tensor([[-30.0277]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.42258217556602307, distance: 1.36488284334133 entropy 6.0474575523884635
epoch: 6, step: 35
	action: tensor([[ -21.7387, -110.8065,   71.4500, -207.6063,  100.5318,   92.2274,
           25.6139]], dtype=torch.float64)
	q_value: tensor([[-26.3155]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.057760751475337546, distance: 1.1769294153809222 entropy 6.070989759972467
epoch: 6, step: 36
	action: tensor([[-179.4529, -121.1511,   -7.1939,  -35.2484,  -75.0996,  -11.1573,
          -10.7268]], dtype=torch.float64)
	q_value: tensor([[-25.3492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9047608852593656, distance: 1.5793446927699633 entropy 5.921031456372553
epoch: 6, step: 37
	action: tensor([[-31.6453, -65.8279,  62.7207,  30.7440,   0.7156,  62.3736,  80.5214]],
       dtype=torch.float64)
	q_value: tensor([[-23.6823]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15726533441080848, distance: 1.0505146728120272 entropy 5.82646423384859
epoch: 6, step: 38
	action: tensor([[-211.1710,  -73.6368, -131.9487,  285.8417,  -52.4945,   47.6584,
          102.0964]], dtype=torch.float64)
	q_value: tensor([[-26.4074]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.776254947387174, distance: 1.5251387078351888 entropy 5.907017772595919
epoch: 6, step: 39
	action: tensor([[ -34.8683, -132.9549,  124.1532,  -84.4667, -139.1663,  -58.1491,
         -112.4198]], dtype=torch.float64)
	q_value: tensor([[-31.3708]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9924861802498881, distance: 1.615304311189341 entropy 6.162122246272993
epoch: 6, step: 40
	action: tensor([[ -61.2617, -122.9653,    1.6567,  143.9582,  -25.4849, -156.4870,
          133.8330]], dtype=torch.float64)
	q_value: tensor([[-23.8461]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0582769613147232, distance: 1.6417559530903296 entropy 5.97482437459434
epoch: 6, step: 41
	action: tensor([[ -81.5399, -126.3903,   10.1738,  -33.2112,  116.2976,  -40.1710,
         -115.7366]], dtype=torch.float64)
	q_value: tensor([[-22.3323]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9374981422838555, distance: 1.5928590254235477 entropy 5.862983377749144
epoch: 6, step: 42
	action: tensor([[-135.6761, -158.4839,  240.0708,  146.3191,  -67.4153, -114.5553,
          209.8053]], dtype=torch.float64)
	q_value: tensor([[-43.5231]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1295098166502155, distance: 1.6699232697620519 entropy 6.4395727097225075
epoch: 6, step: 43
	action: tensor([[-203.1160, -133.9650,  141.4897,   45.3290,   78.2293,  141.5723,
          -34.0393]], dtype=torch.float64)
	q_value: tensor([[-32.9044]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5359995914467812, distance: 1.4182482075659162 entropy 6.101611649778619
epoch: 6, step: 44
	action: tensor([[-203.1314, -186.2027,   60.5133,  141.4621,  131.4828,  158.6891,
          -32.7074]], dtype=torch.float64)
	q_value: tensor([[-34.8998]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23387065151702358, distance: 1.001630966976862 entropy 6.267020971582044
epoch: 6, step: 45
	action: tensor([[-102.1893, -172.3473,    6.0525, -202.0909,  -28.8818,   17.9181,
           57.9785]], dtype=torch.float64)
	q_value: tensor([[-30.7961]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.4838919690332082, distance: 1.8035286467620486 entropy 6.21922952250803
epoch: 6, step: 46
	action: tensor([[-253.6538, -111.9968,  107.3861,  -37.9034,  127.1477,  -97.9715,
           66.3521]], dtype=torch.float64)
	q_value: tensor([[-27.6380]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0644887196320223, distance: 1.644231448029098 entropy 5.911555248906969
epoch: 6, step: 47
	action: tensor([[-200.2752, -309.5917, -171.3422,  -60.0695,  -80.5518,  -98.2922,
         -188.1515]], dtype=torch.float64)
	q_value: tensor([[-32.6918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15470487124866916, distance: 1.2296802341076254 entropy 6.141565551205517
epoch: 6, step: 48
	action: tensor([[-94.2402,  25.1790,  44.1180,  -5.2410,  -2.8202, 237.0204, 184.3110]],
       dtype=torch.float64)
	q_value: tensor([[-33.5104]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 6.107211565386625
epoch: 6, step: 49
	action: tensor([[-97.4858,   7.2177, -36.9641,   6.5392,  45.0506,  28.2863,  29.1724]],
       dtype=torch.float64)
	q_value: tensor([[-32.4792]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6733868496236206, distance: 1.4803174858016377 entropy 5.657581256270132
epoch: 6, step: 50
	action: tensor([[-139.0024,  -10.2860,  -90.7137,  -16.7010,  -91.5379,  -87.7815,
           15.6295]], dtype=torch.float64)
	q_value: tensor([[-25.7516]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.68709488125874
epoch: 6, step: 51
	action: tensor([[ 30.5876, -64.7863, -46.9947,  47.6774,  27.0786, 127.5291, -36.8677]],
       dtype=torch.float64)
	q_value: tensor([[-32.4792]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.657581256270132
epoch: 6, step: 52
	action: tensor([[  20.0784,  -40.0730,   49.8419,   34.2713, -113.5933,  -72.9568,
          -66.9657]], dtype=torch.float64)
	q_value: tensor([[-32.4792]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.046016730735194855, distance: 1.1177046876840375 entropy 5.657581256270132
epoch: 6, step: 53
	action: tensor([[ -64.8153,  -27.3567,   28.4182,   90.7442, -146.2088,  -90.7110,
           65.6344]], dtype=torch.float64)
	q_value: tensor([[-21.5438]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.30737855959945537, distance: 1.3084507259953577 entropy 5.62078914278163
epoch: 6, step: 54
	action: tensor([[ -47.6041, -238.1250,   58.0896,  -26.7110,   92.3399,  175.2747,
           97.9327]], dtype=torch.float64)
	q_value: tensor([[-29.7627]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.49711537914237147, distance: 1.400181480347789 entropy 6.189575100080154
epoch: 6, step: 55
	action: tensor([[-162.1324,  -82.4194,  161.0306, -128.6520,   28.4781,  118.8988,
           -4.6110]], dtype=torch.float64)
	q_value: tensor([[-32.3442]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7582125969305034, distance: 1.517373121983353 entropy 6.2140326670720984
epoch: 6, step: 56
	action: tensor([[-169.1426,  -95.6933,  144.0636,  227.7036, -249.2245,  -74.8600,
           56.4658]], dtype=torch.float64)
	q_value: tensor([[-41.0625]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.9025555256380913, distance: 1.949606488344338 entropy 6.305830358118534
epoch: 6, step: 57
	action: tensor([[-132.8358,   89.8390,    1.3082,  219.4737,  101.4172,  -69.1960,
           17.2232]], dtype=torch.float64)
	q_value: tensor([[-25.0720]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7718216623028749, distance: 0.546630549267868 entropy 5.848375818231887
epoch: 6, step: 58
	action: tensor([[-61.2648, -84.1181, -97.1447, 125.1643, -62.4433,  96.1590, -88.0455]],
       dtype=torch.float64)
	q_value: tensor([[-25.3918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.750111904991845
epoch: 6, step: 59
	action: tensor([[-90.3774, -54.6542, -20.3295, -19.9526,  38.7112, -51.9244,  94.8296]],
       dtype=torch.float64)
	q_value: tensor([[-32.4792]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6179253177532338, distance: 1.4555794943502658 entropy 5.657581256270132
epoch: 6, step: 60
	action: tensor([[-129.1389,  -39.1982,  -81.9994,  -11.6986,   19.9342,   48.2806,
          152.4900]], dtype=torch.float64)
	q_value: tensor([[-27.1592]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7800973658337267, distance: 1.526787416909645 entropy 5.6270191581753375
epoch: 6, step: 61
	action: tensor([[ -38.8730,  -99.2901,   24.5323,  -81.3588,   47.3772,  -36.8120,
         -114.2598]], dtype=torch.float64)
	q_value: tensor([[-26.7018]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24034571135329685, distance: 0.9973892666499197 entropy 5.872691606121502
epoch: 6, step: 62
	action: tensor([[-137.0244, -110.5335,  -67.8460,  -92.5610,   99.3666,   -2.9245,
          -34.7939]], dtype=torch.float64)
	q_value: tensor([[-33.0490]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 6.079961731886805
epoch: 6, step: 63
	action: tensor([[ -71.5217,  -78.2134,   34.6166, -120.4925,   59.6968,   12.7661,
           46.4163]], dtype=torch.float64)
	q_value: tensor([[-32.4792]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15056454716851786, distance: 1.054682851845044 entropy 5.657581256270132
epoch: 6, step: 64
	action: tensor([[ -95.8536,  -97.7947,  -40.8401,  -39.8967,   39.7169, -205.8348,
           53.2260]], dtype=torch.float64)
	q_value: tensor([[-39.5327]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3354312396993906, distance: 1.3224140629195897 entropy 6.147560894979163
epoch: 6, step: 65
	action: tensor([[-211.0633,   33.7372,  -52.0170,  -29.7796,   75.9963,   77.7445,
          -68.5022]], dtype=torch.float64)
	q_value: tensor([[-45.3270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 6.423867552065701
epoch: 6, step: 66
	action: tensor([[-45.1485, -80.1562, -62.6251,  21.6995,  37.6061,  -2.1184, 213.9621]],
       dtype=torch.float64)
	q_value: tensor([[-32.4792]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.657581256270132
epoch: 6, step: 67
	action: tensor([[ 11.5502,  60.3387,  58.6884,  53.4192, -69.5372,   7.0650, -82.2710]],
       dtype=torch.float64)
	q_value: tensor([[-32.4792]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.657581256270132
epoch: 6, step: 68
	action: tensor([[-15.9902,   8.5792, -15.1227, -22.0269,   0.1018,  19.9539,  51.9394]],
       dtype=torch.float64)
	q_value: tensor([[-32.4792]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.657581256270132
epoch: 6, step: 69
	action: tensor([[ -54.6895,   16.8980,  133.5616,   36.4521,  -30.1687, -123.4808,
           43.3493]], dtype=torch.float64)
	q_value: tensor([[-32.4792]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.657581256270132
epoch: 6, step: 70
	action: tensor([[ -89.8926, -131.3309,  -22.3193,   28.3271,    1.1032,   76.6296,
          -67.5804]], dtype=torch.float64)
	q_value: tensor([[-32.4792]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0059250703360156, distance: 1.620742596398151 entropy 5.657581256270132
epoch: 6, step: 71
	action: tensor([[ -25.1688, -360.4311,    8.9655,   28.0669,  222.9003,   86.9895,
          -22.2196]], dtype=torch.float64)
	q_value: tensor([[-38.1769]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5961863825369047, distance: 1.4457676203099012 entropy 6.066971801895526
epoch: 6, step: 72
	action: tensor([[-103.2013, -303.0129, -125.5536,  -84.2208, -134.9191,  -89.0033,
          168.2255]], dtype=torch.float64)
	q_value: tensor([[-43.5179]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7633749670468848, distance: 1.519599104271594 entropy 6.33391190279749
epoch: 6, step: 73
	action: tensor([[-288.4778,  118.7609,   53.5784, -114.2345,   37.8748,  154.9764,
          -78.8976]], dtype=torch.float64)
	q_value: tensor([[-29.9371]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.6294379326207102, distance: 1.8556161992391507 entropy 6.214113846987251
epoch: 6, step: 74
	action: tensor([[-157.6292, -327.2991,   77.0429, -162.5067,   82.0565,  128.7931,
           46.2299]], dtype=torch.float64)
	q_value: tensor([[-31.0078]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0234740927232844, distance: 1.6278167666432826 entropy 6.077605946396846
epoch: 6, step: 75
	action: tensor([[-193.1773,   67.9847, -147.4358,  200.5680,  -79.1611, -259.0585,
           64.7561]], dtype=torch.float64)
	q_value: tensor([[-28.1345]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8291201983139314, distance: 1.5476680461005512 entropy 5.989743142757959
epoch: 6, step: 76
	action: tensor([[-213.6113,  -21.6761, -102.6673,   86.6235,  163.9696,  165.8938,
          -66.2488]], dtype=torch.float64)
	q_value: tensor([[-36.1284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2277922636743464, distance: 1.708024276059352 entropy 6.154919521219038
epoch: 6, step: 77
	action: tensor([[ 113.9070,   71.3537,  -16.5406,  184.4275, -108.6637,  201.1657,
         -106.3188]], dtype=torch.float64)
	q_value: tensor([[-33.8355]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 6.087118805823034
epoch: 6, step: 78
	action: tensor([[ -51.3512, -130.4968,  -51.3554,   64.4201,   44.9913,  -18.7138,
           78.5508]], dtype=torch.float64)
	q_value: tensor([[-32.4792]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.657581256270132
epoch: 6, step: 79
	action: tensor([[-120.2705,  -92.0657,   34.8552,  -81.2098,  -83.9115,  -70.8806,
           21.2633]], dtype=torch.float64)
	q_value: tensor([[-32.4792]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.657581256270132
epoch: 6, step: 80
	action: tensor([[-96.8384,  21.6778, -72.3344, -89.7794, -60.2687, -10.2095,  29.6540]],
       dtype=torch.float64)
	q_value: tensor([[-32.4792]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.657581256270132
epoch: 6, step: 81
	action: tensor([[ 64.5724, -94.7851,  66.4673,  83.6081,  -3.5099,   4.1493, -29.7723]],
       dtype=torch.float64)
	q_value: tensor([[-32.4792]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.657581256270132
epoch: 6, step: 82
	action: tensor([[-11.2540,  16.3454, -61.6715, -20.4424,   6.2913,  33.2610, -39.9397]],
       dtype=torch.float64)
	q_value: tensor([[-32.4792]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.657581256270132
epoch: 6, step: 83
	action: tensor([[ -72.0234, -127.1134,  -94.7476,  -92.0332,   41.3006,   30.6830,
          114.8110]], dtype=torch.float64)
	q_value: tensor([[-32.4792]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08988848052663911, distance: 1.1946693574839444 entropy 5.657581256270132
epoch: 6, step: 84
	action: tensor([[ -44.2589, -153.4984,   72.0120,  141.7074, -185.6162,  168.3267,
           68.6854]], dtype=torch.float64)
	q_value: tensor([[-36.1989]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03721010910126521, distance: 1.1228518376208603 entropy 6.069726923099319
epoch: 6, step: 85
	action: tensor([[-307.1085,  -43.7279,   43.1765, -127.8960,   19.6100,   -4.7463,
          -69.6082]], dtype=torch.float64)
	q_value: tensor([[-35.4105]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09455190127986046, distance: 1.0889011935345296 entropy 6.182501550115816
epoch: 6, step: 86
	action: tensor([[-104.9020,  -83.9315,  -79.1515,  164.4339,   39.8083,  -23.6303,
         -215.2653]], dtype=torch.float64)
	q_value: tensor([[-26.7126]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8783755727273175, distance: 1.5683677708360306 entropy 5.825994885280862
epoch: 6, step: 87
	action: tensor([[ -22.0454, -224.0708,  -48.5965,  -23.6986, -178.5979,  -88.3911,
          115.5526]], dtype=torch.float64)
	q_value: tensor([[-32.9429]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 6.104573150620804
epoch: 6, step: 88
	action: tensor([[ -54.9395,   10.9936,  -76.9136,  -46.2464,  -65.3821, -181.3420,
          128.6629]], dtype=torch.float64)
	q_value: tensor([[-32.4792]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.657581256270132
epoch: 6, step: 89
	action: tensor([[-46.5040,  49.7451,  10.7741,  23.4232, -41.3830,  55.3633, -16.5952]],
       dtype=torch.float64)
	q_value: tensor([[-32.4792]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.657581256270132
epoch: 6, step: 90
	action: tensor([[-121.6900,  -17.2927,   14.3456,  168.3026,   33.2583,    6.3284,
          -67.7301]], dtype=torch.float64)
	q_value: tensor([[-32.4792]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7117984091632434, distance: 1.49721097681451 entropy 5.657581256270132
epoch: 6, step: 91
	action: tensor([[ 100.9628,   55.0203,  -44.7815,  -14.6678,  107.6109, -168.2092,
           30.7929]], dtype=torch.float64)
	q_value: tensor([[-27.7338]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20352741088010573, distance: 1.2554074057433706 entropy 5.8546200400254245
epoch: 6, step: 92
	action: tensor([[ -11.6173,  -51.0329,  -16.3664,   45.9304, -117.4069,   58.7759,
          -93.9870]], dtype=torch.float64)
	q_value: tensor([[-23.7097]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.451154774125143
epoch: 6, step: 93
	action: tensor([[  55.1000, -115.4990,  -66.4824,    1.7254,  -53.9248,  115.9422,
           -0.8476]], dtype=torch.float64)
	q_value: tensor([[-32.4792]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.657581256270132
epoch: 6, step: 94
	action: tensor([[-200.2073, -159.1285,   68.7357,  127.9888,  -29.2045,  -22.0082,
           22.8292]], dtype=torch.float64)
	q_value: tensor([[-32.4792]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7468659424568644, distance: 1.5124690005270962 entropy 5.657581256270132
epoch: 6, step: 95
	action: tensor([[ -0.3673,  12.4047,  36.1587, -58.3902,  87.5325, 135.9154, -65.6076]],
       dtype=torch.float64)
	q_value: tensor([[-29.7942]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.935792333044861
epoch: 6, step: 96
	action: tensor([[ -85.1780, -107.6164,  -69.6099,   16.7357,   -5.4613,  -58.9270,
          -25.6477]], dtype=torch.float64)
	q_value: tensor([[-32.4792]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.5844625381574606, distance: 1.83967799646819 entropy 5.657581256270132
epoch: 6, step: 97
	action: tensor([[-39.3782, -67.9587,  10.5586, -34.7854, -58.8659,  32.2064,  33.9564]],
       dtype=torch.float64)
	q_value: tensor([[-22.3847]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20128324600336778, distance: 1.2542364079955903 entropy 5.545819137329514
epoch: 6, step: 98
	action: tensor([[   5.1781, -125.4177,  -74.2284,  295.2664,    0.4536,  -48.3027,
           44.2946]], dtype=torch.float64)
	q_value: tensor([[-39.2774]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.43288664785848097, distance: 1.3698171876089626 entropy 6.075996179560931
epoch: 6, step: 99
	action: tensor([[ -52.2077, -205.5900,   81.8235,  -91.1551,  -57.0751,  -24.0514,
           30.3728]], dtype=torch.float64)
	q_value: tensor([[-28.5183]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.873647026245991
epoch: 6, step: 100
	action: tensor([[-110.7029,  -72.6288,    8.3906,  -24.5833,  -19.6380,  -20.1993,
          -98.3168]], dtype=torch.float64)
	q_value: tensor([[-32.4792]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.657581256270132
epoch: 6, step: 101
	action: tensor([[-51.7494, -24.2969, -69.1185, -82.8478,  -8.1839,  93.2774, -33.4261]],
       dtype=torch.float64)
	q_value: tensor([[-32.4792]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8618408875964327, distance: 1.5614496153771105 entropy 5.657581256270132
epoch: 6, step: 102
	action: tensor([[-102.4398,  -43.9330,    8.3472,  -29.9221,   68.8677,   -8.1704,
          -21.5703]], dtype=torch.float64)
	q_value: tensor([[-34.3722]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06195372940955601, distance: 1.1083293239185892 entropy 5.873822890497869
epoch: 6, step: 103
	action: tensor([[  -3.3777, -129.3879,  -97.8837,  109.0683,  -78.3497,  -84.3703,
           68.5744]], dtype=torch.float64)
	q_value: tensor([[-35.9621]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.956496501337209
epoch: 6, step: 104
	action: tensor([[-149.9696,   64.2429,  -65.8476,   10.5941,  -75.2619,  -13.8706,
         -116.5186]], dtype=torch.float64)
	q_value: tensor([[-32.4792]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7083771660588591, distance: 0.617970091028362 entropy 5.657581256270132
epoch: 6, step: 105
	action: tensor([[-200.1294, -111.2099,  -99.1286,  -61.9609,  -33.2020,   52.9289,
          -13.0624]], dtype=torch.float64)
	q_value: tensor([[-40.0389]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20622970330188883, distance: 1.0195396376630108 entropy 6.141769989029482
epoch: 6, step: 106
	action: tensor([[-85.1000, -27.9081, -78.8285,  56.1557,  45.4317, 202.6439,  80.8857]],
       dtype=torch.float64)
	q_value: tensor([[-28.6075]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.819215663504629, distance: 1.5434721109047145 entropy 5.844098953712753
epoch: 6, step: 107
	action: tensor([[ -52.9849, -207.3574,  -19.1514,  129.8003,  -17.6257,  -81.0262,
          -13.1997]], dtype=torch.float64)
	q_value: tensor([[-31.7254]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.5610706014302216, distance: 1.8313336215664422 entropy 5.991979168268026
epoch: 6, step: 108
	action: tensor([[-95.9145, -90.1300,  86.4220,  79.4154,  34.7727, -43.8618,  27.5137]],
       dtype=torch.float64)
	q_value: tensor([[-26.4965]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10442221138873087, distance: 1.0829498619786946 entropy 5.842253846005795
epoch: 6, step: 109
	action: tensor([[-291.7074, -254.5224,  -35.8082, -114.6182,   21.2347,    9.3123,
           71.1949]], dtype=torch.float64)
	q_value: tensor([[-30.5072]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9385244006452822, distance: 1.5932808241466445 entropy 6.267401562324002
epoch: 6, step: 110
	action: tensor([[-1.4468e+01, -8.5854e+01,  6.0000e+01,  1.9628e+01, -7.2962e+01,
         -1.3172e-02, -3.2653e+01]], dtype=torch.float64)
	q_value: tensor([[-35.3821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.34236609958631425, distance: 1.3258432477795308 entropy 6.16413601619167
epoch: 6, step: 111
	action: tensor([[-308.4709, -368.4689,  -58.5706,   76.2558,   -6.6892,  139.9506,
          -50.0861]], dtype=torch.float64)
	q_value: tensor([[-35.0525]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.044768735523174175, distance: 1.1184355360359104 entropy 6.337672428564562
epoch: 6, step: 112
	action: tensor([[-222.5841, -271.1891,   75.2039,   51.0755,   84.8196,  202.4129,
          -77.8013]], dtype=torch.float64)
	q_value: tensor([[-29.1154]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.142866876149085, distance: 1.6751522658630371 entropy 6.11308152985806
epoch: 6, step: 113
	action: tensor([[-193.2601, -310.2441,  139.5809,    5.1839,  -11.9366,  -44.9803,
         -116.1483]], dtype=torch.float64)
	q_value: tensor([[-29.5180]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 6.243628443882017
epoch: 6, step: 114
	action: tensor([[-132.5961, -144.7244,   79.1199,  -22.1972,   97.1539, -139.6705,
           -5.9903]], dtype=torch.float64)
	q_value: tensor([[-32.4792]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.657581256270132
epoch: 6, step: 115
	action: tensor([[-103.7666,   30.9591,  122.4934,  136.0547, -117.4981,  -12.3895,
          -14.7786]], dtype=torch.float64)
	q_value: tensor([[-32.4792]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.36290766192383184, distance: 1.335949094041628 entropy 5.657581256270132
epoch: 6, step: 116
	action: tensor([[ -52.9800, -164.1783,    0.4170,  -93.8796,   76.8200,   90.5894,
           68.1636]], dtype=torch.float64)
	q_value: tensor([[-37.6920]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5038433787172538, distance: 1.4033241440550972 entropy 6.068611432277159
epoch: 6, step: 117
	action: tensor([[-132.5665,  -41.6498,  -11.6292,   15.5454,    1.6642,   23.3640,
          151.3155]], dtype=torch.float64)
	q_value: tensor([[-25.3721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.847158532962324
epoch: 6, step: 118
	action: tensor([[ -31.2324, -103.1504,   63.9021,  -79.2243,  -37.5064,  144.0670,
           92.1511]], dtype=torch.float64)
	q_value: tensor([[-32.4792]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7663735838525949, distance: 1.5208905935971253 entropy 5.657581256270132
epoch: 6, step: 119
	action: tensor([[-116.0431,  -49.0399, -213.2702,  -19.0692,  -79.8837,   58.6322,
          -21.4554]], dtype=torch.float64)
	q_value: tensor([[-31.0228]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21733462254355462, distance: 1.2625880667458276 entropy 5.861064254716081
epoch: 6, step: 120
	action: tensor([[-278.8563, -240.5398,  -25.0053,  130.3145,   53.1565,    6.4011,
          243.0410]], dtype=torch.float64)
	q_value: tensor([[-34.7492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13611304705176575, distance: 1.2197405635669265 entropy 6.1578430854849575
epoch: 6, step: 121
	action: tensor([[-154.8702,   15.6951,   -1.4158,  -70.9923,  -87.8131,  -40.7146,
           51.0614]], dtype=torch.float64)
	q_value: tensor([[-30.2748]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20439699345337958, distance: 1.0207159496545584 entropy 6.056006882882865
epoch: 6, step: 122
	action: tensor([[ 10.5095, -79.3204, -39.5386,  90.8041,  19.7813, -32.8707, 216.3140]],
       dtype=torch.float64)
	q_value: tensor([[-40.6405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 6.055864168122025
epoch: 6, step: 123
	action: tensor([[-31.5136, -20.4638,  59.3535, 128.7910,  -9.6148, -62.0816,  42.4232]],
       dtype=torch.float64)
	q_value: tensor([[-32.4792]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8484069051685781, distance: 1.5558061518145345 entropy 5.657581256270132
epoch: 6, step: 124
	action: tensor([[  -6.2872, -141.4254,  137.9504,   54.0120,  -73.5798, -133.6934,
          151.0635]], dtype=torch.float64)
	q_value: tensor([[-33.7231]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1698216340596208, distance: 1.0426592467911648 entropy 6.029951956320427
epoch: 6, step: 125
	action: tensor([[ -65.9586, -115.1931,  -80.8231,   82.5199,    9.7612,   60.8454,
          -11.2984]], dtype=torch.float64)
	q_value: tensor([[-28.2184]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.8898624467070855
epoch: 6, step: 126
	action: tensor([[-58.0257,  34.9713,  81.5792,  29.1783,  83.3240, 101.4341,  81.7763]],
       dtype=torch.float64)
	q_value: tensor([[-32.4792]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.31335898761541126, distance: 1.3114399776617272 entropy 5.657581256270132
epoch: 6, step: 127
	action: tensor([[-106.7536,  -78.1674,   -9.4791,    6.8765,   92.2684,  -29.3393,
          106.9894]], dtype=torch.float64)
	q_value: tensor([[-28.2218]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04743775082248303, distance: 1.171172333193999 entropy 5.667794207796841
LOSS epoch 6 actor 261.04384462928874 critic 256.3011471956984
epoch: 7, step: 0
	action: tensor([[  6.9068,  -8.6846,  72.2371, 184.8306,  16.2289,  12.7947,   4.2913]],
       dtype=torch.float64)
	q_value: tensor([[-27.3419]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 6.042188215086393
epoch: 7, step: 1
	action: tensor([[ -74.3281, -228.1438,   13.1443,  -22.5962,  -23.2209,  -17.0576,
           -8.6834]], dtype=torch.float64)
	q_value: tensor([[-34.3979]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.829836709810637
epoch: 7, step: 2
	action: tensor([[-156.7863,  -31.6374,  -52.2076,  -35.5600,  -10.2363,   48.4841,
          -19.0456]], dtype=torch.float64)
	q_value: tensor([[-34.3979]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05855218202232104, distance: 1.1103370197475444 entropy 5.829836709810637
epoch: 7, step: 3
	action: tensor([[ -87.0044,  -69.9985, -194.9841,   76.1984,  119.8314,  214.5921,
          -14.5569]], dtype=torch.float64)
	q_value: tensor([[-39.4013]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.155966519806901, distance: 1.6802646840888757 entropy 6.24364725155981
epoch: 7, step: 4
	action: tensor([[-96.8224, -66.3302, -46.5232,  35.2865,   4.9283,  12.4441, 172.1515]],
       dtype=torch.float64)
	q_value: tensor([[-27.8471]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0786448634415708, distance: 1.649859042925045 entropy 5.9636506432762095
epoch: 7, step: 5
	action: tensor([[-183.3518, -199.4699,   39.0977, -122.8050, -102.8057,  112.2914,
          148.8842]], dtype=torch.float64)
	q_value: tensor([[-33.1952]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09997681171287653, distance: 1.0856342662746727 entropy 6.296576998894634
epoch: 7, step: 6
	action: tensor([[  25.9226, -239.7809,  161.5830,  425.5132,  -71.3694,  -38.9676,
          268.7886]], dtype=torch.float64)
	q_value: tensor([[-44.5106]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4832167744552889, distance: 1.3936669705055964 entropy 6.45842862113942
epoch: 7, step: 7
	action: tensor([[-236.8723,  -20.8711, -147.5744,  195.7135,  -74.0826,   31.6828,
          -51.2473]], dtype=torch.float64)
	q_value: tensor([[-31.2933]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9213091535528315, distance: 1.5861904076405193 entropy 6.282703751312677
epoch: 7, step: 8
	action: tensor([[-262.5529, -321.0479, -116.7963,  429.2784,   48.9633,   90.3111,
          142.5468]], dtype=torch.float64)
	q_value: tensor([[-37.2108]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10802025581662011, distance: 1.0807722603998744 entropy 6.381340556871373
epoch: 7, step: 9
	action: tensor([[ -55.5702, -180.5575,   82.2080,  194.5504, -187.9230,  -20.6760,
            5.2711]], dtype=torch.float64)
	q_value: tensor([[-35.2181]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4649959500405616, distance: 0.8370188297510192 entropy 6.4173361539985425
epoch: 7, step: 10
	action: tensor([[ -48.3685, -155.7845,   41.4297,   73.3594,    7.0949,  -28.9857,
         -158.2197]], dtype=torch.float64)
	q_value: tensor([[-23.5558]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.011027092271631234, distance: 1.1380173690242148 entropy 6.052368795612693
epoch: 7, step: 11
	action: tensor([[ -46.8683, -200.2898,    4.8516,   45.0923,    8.1050,  162.9174,
          103.8704]], dtype=torch.float64)
	q_value: tensor([[-33.6771]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1098443195589378, distance: 1.6621947211353394 entropy 6.304356900850924
epoch: 7, step: 12
	action: tensor([[-203.3630,   39.1320, -106.8092,  222.6649, -241.1166,   19.5839,
           -6.4258]], dtype=torch.float64)
	q_value: tensor([[-31.5423]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5635058464564118, distance: 0.7560419765179497 entropy 6.334501035530372
epoch: 7, step: 13
	action: tensor([[-219.1722, -124.2610,   48.3847,  107.2130, -308.2159, -172.6946,
           29.1505]], dtype=torch.float64)
	q_value: tensor([[-29.9655]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6109459586663, distance: 1.4524365830347514 entropy 6.252304521576037
epoch: 7, step: 14
	action: tensor([[  24.0378, -125.1112,  152.5829,   47.1263, -252.1473,   90.5448,
         -209.8668]], dtype=torch.float64)
	q_value: tensor([[-30.3187]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5357485934013706, distance: 1.4181323246992248 entropy 6.222857399579092
epoch: 7, step: 15
	action: tensor([[ -82.4281, -335.2509,  226.0808,  -19.8420,  135.6996,   67.6173,
           14.3032]], dtype=torch.float64)
	q_value: tensor([[-33.4267]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1026398247045828, distance: 1.659354342416227 entropy 6.341216226988455
epoch: 7, step: 16
	action: tensor([[-183.9169, -116.5500,  113.3640,   75.0282,  149.4967, -106.8340,
         -240.7151]], dtype=torch.float64)
	q_value: tensor([[-34.4625]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.57878133743246, distance: 1.4378635858889468 entropy 6.392466204119162
epoch: 7, step: 17
	action: tensor([[-200.5466,   39.3458, -158.7908,    2.8265,  -84.9770,  537.1215,
           43.2600]], dtype=torch.float64)
	q_value: tensor([[-26.2859]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 6.321198851173579
epoch: 7, step: 18
	action: tensor([[   8.0656,    2.9887,  -71.4696,   72.7878, -148.1197,   17.0513,
          -47.8174]], dtype=torch.float64)
	q_value: tensor([[-34.3979]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.829836709810637
epoch: 7, step: 19
	action: tensor([[-170.5686,   76.0747, -183.8343,  -24.5966,  -44.7029,   45.1471,
          -62.1824]], dtype=torch.float64)
	q_value: tensor([[-34.3979]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.829836709810637
epoch: 7, step: 20
	action: tensor([[-66.2583, -54.9316,  38.8112, 173.6084,  21.1442,  23.2739, -54.5513]],
       dtype=torch.float64)
	q_value: tensor([[-34.3979]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05720930118293699, distance: 1.111128629729044 entropy 5.829836709810637
epoch: 7, step: 21
	action: tensor([[-224.7299, -103.3214,  -43.9405,   23.2265,  -46.1545,   91.7132,
          -71.8076]], dtype=torch.float64)
	q_value: tensor([[-37.4871]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06322503255827483, distance: 1.1075780282055074 entropy 6.15137632039838
epoch: 7, step: 22
	action: tensor([[-231.3253, -202.2065,   15.3866,  -84.5660,  -61.7320, -448.4214,
           21.7779]], dtype=torch.float64)
	q_value: tensor([[-32.5573]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4539867829693802, distance: 1.379866028273016 entropy 6.352673667433132
epoch: 7, step: 23
	action: tensor([[-194.5747, -306.2950,  -70.8645,   95.9826, -111.5619,   10.4265,
         -256.4931]], dtype=torch.float64)
	q_value: tensor([[-32.3865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 6.198355221144744
epoch: 7, step: 24
	action: tensor([[-229.8044,  -73.1338, -179.4259,   68.1694,  -82.5023,  188.7869,
          211.5445]], dtype=torch.float64)
	q_value: tensor([[-34.3979]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.001256534892225325, distance: 1.145062982535943 entropy 5.829836709810637
epoch: 7, step: 25
	action: tensor([[ -25.0414,  -31.4392,  175.4877,  -24.1054, -158.8443,  -19.5652,
           87.2964]], dtype=torch.float64)
	q_value: tensor([[-46.4347]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 6.371220177302748
epoch: 7, step: 26
	action: tensor([[  -7.4792,  -93.3915, -161.3540,   60.8132,   70.3576, -199.1188,
          -85.2722]], dtype=torch.float64)
	q_value: tensor([[-34.3979]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.829836709810637
epoch: 7, step: 27
	action: tensor([[-185.8957, -108.4201,  -83.2106,  -57.7619,   71.9737,  141.0239,
           91.3114]], dtype=torch.float64)
	q_value: tensor([[-34.3979]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0627158447412961, distance: 1.1796828646765443 entropy 5.829836709810637
epoch: 7, step: 28
	action: tensor([[-39.7110, -23.7965,  13.1132,  -0.2448, -22.9138, -40.2223,  -3.1399]],
       dtype=torch.float64)
	q_value: tensor([[-36.2640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.9091497382742242, distance: 1.951819852751979 entropy 6.1638650668804065
epoch: 7, step: 29
	action: tensor([[-125.4543,  -79.8511,   33.0258,   15.3264, -183.8749,  143.3845,
          111.6679]], dtype=torch.float64)
	q_value: tensor([[-33.1316]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3335119003281357, distance: 1.3214634059537744 entropy 6.17145272697536
epoch: 7, step: 30
	action: tensor([[-267.0609, -160.1992, -320.2536,   57.0202, -110.6245,    7.1512,
          138.6924]], dtype=torch.float64)
	q_value: tensor([[-27.1992]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1433502708664847, distance: 1.6753411982935291 entropy 6.115119404441367
epoch: 7, step: 31
	action: tensor([[-63.5953, -61.3747, -71.2286, 177.0639, 122.9074,  33.9906,  71.7838]],
       dtype=torch.float64)
	q_value: tensor([[-27.8639]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9225599620496907, distance: 1.5867066435706483 entropy 6.059111595505706
epoch: 7, step: 32
	action: tensor([[-142.2482, -308.5890,   -3.6758,  101.8789, -241.1958,  110.2458,
         -115.6639]], dtype=torch.float64)
	q_value: tensor([[-30.8745]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7612467677519983, distance: 1.5186818330827365 entropy 6.391933997361968
epoch: 7, step: 33
	action: tensor([[-201.8088,   -1.6425, -144.4208,    0.7472, -250.3016,   61.3271,
          160.8807]], dtype=torch.float64)
	q_value: tensor([[-36.9292]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 6.470845585450097
epoch: 7, step: 34
	action: tensor([[ -97.7146, -180.2888,   49.4572,  157.3072,  -54.1287,   43.2897,
          -39.8624]], dtype=torch.float64)
	q_value: tensor([[-34.3979]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5637068569378676, distance: 1.4309826347906127 entropy 5.829836709810637
epoch: 7, step: 35
	action: tensor([[ -86.3814,   -7.1230,  150.7240,  207.0410,   97.4525,  175.2896,
         -330.5969]], dtype=torch.float64)
	q_value: tensor([[-32.7373]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0329247866891333, distance: 1.6316137208824826 entropy 6.132106771707976
epoch: 7, step: 36
	action: tensor([[-208.6904, -266.4545,  -73.3917,   71.1185,   34.6886, -161.8707,
         -470.2538]], dtype=torch.float64)
	q_value: tensor([[-36.7984]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.43445822676656143, distance: 0.8605756189420717 entropy 6.493334818306923
epoch: 7, step: 37
	action: tensor([[-339.9297, -142.9823, -126.5879, -198.0468,    3.1422,    0.4021,
           30.8066]], dtype=torch.float64)
	q_value: tensor([[-30.3539]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.125892956988161, distance: 1.668504528967948 entropy 6.4323596914280925
epoch: 7, step: 38
	action: tensor([[  42.0026,   58.9546,   46.9296,  -76.2908,  107.5342, -169.4199,
           68.6432]], dtype=torch.float64)
	q_value: tensor([[-27.6589]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 6.263743323350618
epoch: 7, step: 39
	action: tensor([[-119.9797, -141.8650,  -48.6163,   34.0563,   46.5466,   28.1184,
          -25.8600]], dtype=torch.float64)
	q_value: tensor([[-34.3979]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9388292926183752, distance: 1.5934061151759036 entropy 5.829836709810637
epoch: 7, step: 40
	action: tensor([[-142.7236, -270.5016,   53.7961,   64.2405,  -13.5336,  104.3347,
          104.1129]], dtype=torch.float64)
	q_value: tensor([[-41.6148]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09530732558258759, distance: 1.1976355790167652 entropy 6.390968886032951
epoch: 7, step: 41
	action: tensor([[-231.0743, -143.1130,  -25.9671,  -15.5574,  -40.1403, -139.5431,
         -304.3048]], dtype=torch.float64)
	q_value: tensor([[-36.2455]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.266667446924949, distance: 1.722862415923711 entropy 6.498983552189585
epoch: 7, step: 42
	action: tensor([[-290.3915,  -76.2584, -206.3865,  294.3490,  -25.6379,  308.2737,
          226.8310]], dtype=torch.float64)
	q_value: tensor([[-29.0070]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11799251772679786, distance: 1.074713806006617 entropy 6.3941634230489415
epoch: 7, step: 43
	action: tensor([[-224.3876,  -30.3789,  -63.5360, -179.1660,   60.3508,  -47.2757,
          146.6301]], dtype=torch.float64)
	q_value: tensor([[-29.9319]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0335361138716848, distance: 1.6318590262794748 entropy 6.22173614616923
epoch: 7, step: 44
	action: tensor([[  39.3332, -180.2510,  -41.0378,   75.6885,  -41.0747,  109.5876,
          -56.7823]], dtype=torch.float64)
	q_value: tensor([[-27.8564]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6678221067050378, distance: 1.4778540846900297 entropy 6.287792337156527
epoch: 7, step: 45
	action: tensor([[-111.5284, -120.4484,  -42.7721,  -11.0759,  -24.0889,  130.7026,
          234.5778]], dtype=torch.float64)
	q_value: tensor([[-34.6930]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09032025699919877, distance: 1.1949059775839945 entropy 6.2985962335874826
epoch: 7, step: 46
	action: tensor([[-171.5036, -158.2272,    3.8297, -115.0086,  -79.6810, -169.1649,
           36.3431]], dtype=torch.float64)
	q_value: tensor([[-32.2747]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.158477086576069, distance: 1.681242711645425 entropy 6.193058866697217
epoch: 7, step: 47
	action: tensor([[-139.3219, -138.7393,  -35.7446, -234.5328,  102.8805,   47.2927,
           41.8261]], dtype=torch.float64)
	q_value: tensor([[-32.8941]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4099823467851287, distance: 1.358825007401634 entropy 6.253580994429319
epoch: 7, step: 48
	action: tensor([[ -36.5215,    8.1086,  -76.0192,   -8.0146, -221.7133, -125.8406,
         -145.3435]], dtype=torch.float64)
	q_value: tensor([[-37.7194]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1952820828472919, distance: 1.6955158722329877 entropy 6.305647359757012
epoch: 7, step: 49
	action: tensor([[-106.9141, -100.8590, -114.3059,  -38.8643,  -77.4653,   50.8699,
          -35.3442]], dtype=torch.float64)
	q_value: tensor([[-33.7524]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6553763067945491, distance: 1.472329659008084 entropy 5.946967518184336
epoch: 7, step: 50
	action: tensor([[-207.5927, -252.9427,  191.4224,  189.5196, -147.0500,  137.1155,
          -84.1255]], dtype=torch.float64)
	q_value: tensor([[-34.0491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3576918163464422, distance: 1.3333903059861103 entropy 6.060226563157014
epoch: 7, step: 51
	action: tensor([[-256.2366,  -34.9666, -253.0024,   60.9335, -179.7275, -255.9411,
           81.3450]], dtype=torch.float64)
	q_value: tensor([[-39.2607]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.101583633116828, distance: 1.65893752917462 entropy 6.4499605150281765
epoch: 7, step: 52
	action: tensor([[-166.2655, -401.9425, -172.4352,  140.0046,   64.6214,  290.0882,
          -96.1728]], dtype=torch.float64)
	q_value: tensor([[-32.0750]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7849683652820783, distance: 1.5288749150937755 entropy 6.227736085520173
epoch: 7, step: 53
	action: tensor([[-119.7777, -298.4282,   74.1804, -101.5674,  148.3912,  -19.0984,
          -77.7027]], dtype=torch.float64)
	q_value: tensor([[-37.8922]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.5634546758564314, distance: 1.8321858081576214 entropy 6.3922076790313
epoch: 7, step: 54
	action: tensor([[   6.3413, -142.6018,  -87.5971, -140.4542,  -75.5853,   12.0497,
          158.1232]], dtype=torch.float64)
	q_value: tensor([[-29.1645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3053782891911576, distance: 1.3074493874366073 entropy 6.397153788482405
epoch: 7, step: 55
	action: tensor([[  12.8648, -167.9867, -116.7802,  104.5630,   60.6251,  153.7647,
           93.2650]], dtype=torch.float64)
	q_value: tensor([[-18.2634]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20773836770067644, distance: 1.018570292553415 entropy 5.712790354548379
epoch: 7, step: 56
	action: tensor([[-257.7248, -101.5955,    7.6808,   82.4821,  -98.6235,    5.5085,
           24.4027]], dtype=torch.float64)
	q_value: tensor([[-29.0178]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.029949734034301567, distance: 1.1270775851687937 entropy 6.1755592281296545
epoch: 7, step: 57
	action: tensor([[-99.4884, -88.2129,   2.9524, -12.2318,  37.3178,  95.0018,  99.2996]],
       dtype=torch.float64)
	q_value: tensor([[-24.2657]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.6136656807063554, distance: 1.8500425238529652 entropy 5.829115057128661
epoch: 7, step: 58
	action: tensor([[ -44.8877,  -49.8536,   73.9429, -144.4132,   31.7487,  -92.7959,
          273.6830]], dtype=torch.float64)
	q_value: tensor([[-29.3665]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03163624827291878, distance: 1.1260973992080803 entropy 6.0845937505333
epoch: 7, step: 59
	action: tensor([[-57.2702,  34.1937, -79.0958, -99.7728, -38.5551, 113.8108, -10.0103]],
       dtype=torch.float64)
	q_value: tensor([[-39.2570]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.001660012236762709, distance: 1.1433940467649482 entropy 6.328634277493363
epoch: 7, step: 60
	action: tensor([[-225.0300, -147.1295,   -4.3548,  129.3641, -125.0956,   -9.7079,
         -192.4317]], dtype=torch.float64)
	q_value: tensor([[-35.6054]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 6.312178889400942
epoch: 7, step: 61
	action: tensor([[ -62.6946, -199.4484,   -9.9795,   44.0007,   96.9459,  -49.6210,
           35.0050]], dtype=torch.float64)
	q_value: tensor([[-34.3979]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.3173092985203771, distance: 1.7420021850467695 entropy 5.829836709810637
epoch: 7, step: 62
	action: tensor([[ 119.5557, -200.9514, -100.9437,  156.1401, -137.3544,  401.0947,
          -31.5861]], dtype=torch.float64)
	q_value: tensor([[-38.6671]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13514309123271584, distance: 1.0642136444894785 entropy 6.266460876400651
epoch: 7, step: 63
	action: tensor([[-168.9073,  -78.5660,  -50.7864,  149.3223,  -74.0830,  -88.2684,
           79.1196]], dtype=torch.float64)
	q_value: tensor([[-26.3112]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.44433831849816663, distance: 1.3752801032505522 entropy 6.012495040202023
epoch: 7, step: 64
	action: tensor([[-59.5308,  69.4364, -18.8601, -92.7445,  -1.8078, -18.0133, -23.3527]],
       dtype=torch.float64)
	q_value: tensor([[-24.6534]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.02550588881920146, distance: 1.1588461241204329 entropy 6.126917070490549
epoch: 7, step: 65
	action: tensor([[-114.7428,  -31.2418,  -20.0232,  196.7808,   -7.6846,  179.5921,
          -20.8419]], dtype=torch.float64)
	q_value: tensor([[-30.5296]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5643276707865561, distance: 1.4312666668252005 entropy 6.24186228419254
epoch: 7, step: 66
	action: tensor([[-117.5181, -275.0480,  175.2893, -140.9940,   62.7803,  132.4074,
           73.7953]], dtype=torch.float64)
	q_value: tensor([[-31.1199]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6828889625620775, distance: 1.4845144329187292 entropy 6.3503783947281685
epoch: 7, step: 67
	action: tensor([[-145.2713, -294.7334,   72.7930,  166.7128,   34.6840,  -70.8377,
           -0.7572]], dtype=torch.float64)
	q_value: tensor([[-26.2273]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.012873064171005755, distance: 1.136954787191093 entropy 6.093842060593867
epoch: 7, step: 68
	action: tensor([[-244.8356,  -73.1804, -171.1345,   -8.1528,  -39.1847, -155.4438,
          -54.6228]], dtype=torch.float64)
	q_value: tensor([[-32.0723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1411964200747471, distance: 1.0604827688846463 entropy 6.4663008617966256
epoch: 7, step: 69
	action: tensor([[ -73.6603, -197.4475,   44.4095,  154.4180,   10.1963,   29.7609,
         -112.9146]], dtype=torch.float64)
	q_value: tensor([[-29.2496]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7365727694682278, distance: 1.5080064067639025 entropy 6.25952765615608
epoch: 7, step: 70
	action: tensor([[-110.6323,  -44.7119,  -13.2985,  122.1946,   74.1969, -193.5298,
           60.5597]], dtype=torch.float64)
	q_value: tensor([[-22.7470]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4636584730405944, distance: 1.3844477469526821 entropy 6.082905200629382
epoch: 7, step: 71
	action: tensor([[-262.6573,  -67.0300,  155.3713,   18.1096,  -57.9873,   78.9148,
           51.1253]], dtype=torch.float64)
	q_value: tensor([[-27.1918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.45326014615240795, distance: 1.3795211878752558 entropy 6.269075071815358
epoch: 7, step: 72
	action: tensor([[ -44.3949, -205.9094,  -42.6207,  -64.6122,  -98.0499,   44.8341,
           52.9749]], dtype=torch.float64)
	q_value: tensor([[-27.7817]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7302404220391272, distance: 1.5052544513643336 entropy 6.160747406356476
epoch: 7, step: 73
	action: tensor([[-216.6732, -348.6033,  230.0820,  133.9880,  -65.2179,  -89.0505,
         -194.7115]], dtype=torch.float64)
	q_value: tensor([[-41.1334]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0940724433483129, distance: 1.6559703029187003 entropy 6.394210364109419
epoch: 7, step: 74
	action: tensor([[-167.0548,   -8.2300,  159.7631,  233.7379, -140.1924,  106.6076,
           65.4957]], dtype=torch.float64)
	q_value: tensor([[-35.4061]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 6.381300199218701
epoch: 7, step: 75
	action: tensor([[-11.6767, -72.2457,  75.4740, -13.0590, -63.7130,  14.8112,  11.3237]],
       dtype=torch.float64)
	q_value: tensor([[-34.3979]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4128399516929546, distance: 1.3602012727155013 entropy 5.829836709810637
epoch: 7, step: 76
	action: tensor([[-289.0559, -142.4643,   -6.6810,  -73.5210, -215.6599,  127.4244,
           48.8965]], dtype=torch.float64)
	q_value: tensor([[-22.1188]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.007051764324112586, distance: 1.140302292661129 entropy 5.906605477147155
epoch: 7, step: 77
	action: tensor([[-54.2473,  -0.9878, 119.9888, 127.6608,  99.9862,  87.4073, -31.1677]],
       dtype=torch.float64)
	q_value: tensor([[-34.1606]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0524534854350764, distance: 1.6394318007802076 entropy 6.228948215710524
epoch: 7, step: 78
	action: tensor([[  -6.0127,  -35.4298, -239.3868,  101.2062,  -69.4662,   38.0712,
           53.8664]], dtype=torch.float64)
	q_value: tensor([[-25.9687]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5990804148127538, distance: 1.4470776813971453 entropy 5.955151424327993
epoch: 7, step: 79
	action: tensor([[-147.4866, -143.0091,   32.5348,    1.0628, -240.5507,   42.8810,
          140.8514]], dtype=torch.float64)
	q_value: tensor([[-19.6483]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5282747011800921, distance: 1.4146773663370098 entropy 5.929832591362816
epoch: 7, step: 80
	action: tensor([[-384.8632, -211.8715,   73.2014,  -49.2220,  -74.0660,  113.8663,
           72.0852]], dtype=torch.float64)
	q_value: tensor([[-32.2207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0500781168335831, distance: 1.1726475412912873 entropy 6.298448010725204
epoch: 7, step: 81
	action: tensor([[-241.7539, -224.6900,  -68.4350,   20.7382, -154.6900, -112.9366,
          -52.9470]], dtype=torch.float64)
	q_value: tensor([[-36.1628]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03690813191521647, distance: 1.1652707186630322 entropy 6.326928474662731
epoch: 7, step: 82
	action: tensor([[  -1.2187, -114.7480,   80.3339,   36.2228,   86.1678,   22.9391,
          -57.6062]], dtype=torch.float64)
	q_value: tensor([[-31.1920]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6509697825657206, distance: 1.4703687212963972 entropy 6.314668030590866
epoch: 7, step: 83
	action: tensor([[-267.5898, -153.2435,  -73.9612, -310.0180,  -54.0077,  232.2768,
          -47.4218]], dtype=torch.float64)
	q_value: tensor([[-27.4081]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8011661376287544, distance: 1.5357961693984505 entropy 6.27933400465288
epoch: 7, step: 84
	action: tensor([[-180.3729, -238.9804,   59.8888,  -64.0879, -256.6643,  163.4236,
          -13.6439]], dtype=torch.float64)
	q_value: tensor([[-33.3147]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8720851665481593, distance: 1.565739450921563 entropy 6.329410911415691
epoch: 7, step: 85
	action: tensor([[-277.1527,  -81.0380,   46.3384,   81.9709,  140.3643,  -65.7471,
            0.7615]], dtype=torch.float64)
	q_value: tensor([[-33.8151]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.6855105461099353, distance: 1.8752972828813443 entropy 6.198042283783628
epoch: 7, step: 86
	action: tensor([[-108.5731,   14.1092,  154.4562,   54.2526,  -14.0317,   46.7212,
           75.4385]], dtype=torch.float64)
	q_value: tensor([[-31.4057]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.3877308520385698, distance: 1.7682732569026818 entropy 6.129176275369104
epoch: 7, step: 87
	action: tensor([[-277.0440, -472.0548,   90.5286,  146.8616,  -95.3398,  134.5573,
         -121.3949]], dtype=torch.float64)
	q_value: tensor([[-35.6151]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16183632303313056, distance: 1.0476618058955998 entropy 6.199890152465826
epoch: 7, step: 88
	action: tensor([[-166.3149,  -52.6118, -101.0983,   69.7777,    5.2128,  -37.8584,
         -288.0861]], dtype=torch.float64)
	q_value: tensor([[-33.1490]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.025865196019967396, distance: 1.1294479548062188 entropy 6.348963107944679
epoch: 7, step: 89
	action: tensor([[ -20.9673,   37.0417, -146.7122,   52.7882, -202.6255,  130.5658,
          157.7633]], dtype=torch.float64)
	q_value: tensor([[-22.5223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5844054550941151, distance: 1.4404223711807544 entropy 6.092242864129219
epoch: 7, step: 90
	action: tensor([[-258.4992, -127.9566,  -19.8855,  136.4733,   42.4729,   93.8733,
          305.5729]], dtype=torch.float64)
	q_value: tensor([[-30.7398]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3752643302945362, distance: 1.3419915546830532 entropy 6.2701090086804925
epoch: 7, step: 91
	action: tensor([[-340.4641,  -59.0039,   35.9753,  358.1276,  -62.5445,  163.4016,
           -6.1572]], dtype=torch.float64)
	q_value: tensor([[-34.8599]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.28304602269153634, distance: 1.2962172915263845 entropy 6.4083233061350375
epoch: 7, step: 92
	action: tensor([[ -37.5734, -163.3972, -264.4819,  -26.0801,  -21.4315, -228.1698,
           50.1647]], dtype=torch.float64)
	q_value: tensor([[-28.5257]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7997683786253307, distance: 1.5352001417720476 entropy 6.13699456349584
epoch: 7, step: 93
	action: tensor([[-160.5103, -238.2250,   59.9174, -177.1565,   38.5218, -234.5597,
           85.6407]], dtype=torch.float64)
	q_value: tensor([[-25.0480]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.26774330648654066, distance: 1.288464200593305 entropy 6.124099025842746
epoch: 7, step: 94
	action: tensor([[-322.7096,  -61.9131,   27.5504,   30.5735,   -8.0124, -178.1523,
          135.2109]], dtype=torch.float64)
	q_value: tensor([[-38.6995]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.033228080070821, distance: 1.6317354271036368 entropy 6.325235840583401
epoch: 7, step: 95
	action: tensor([[-222.9434,   11.5292,  111.5674,  356.8790,   82.7273,  230.0887,
          119.8922]], dtype=torch.float64)
	q_value: tensor([[-40.7034]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.846020119882775, distance: 1.9305260945185778 entropy 6.394784998863267
epoch: 7, step: 96
	action: tensor([[-165.5132, -180.3439,   16.2562,  266.8364, -121.1584,  156.9574,
          158.7920]], dtype=torch.float64)
	q_value: tensor([[-33.0828]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11317756988603633, distance: 1.0776432867503152 entropy 6.143368330847496
epoch: 7, step: 97
	action: tensor([[ -38.7889, -247.0009,  -16.1948,   -1.9954,    1.5312,   39.2987,
          130.4463]], dtype=torch.float64)
	q_value: tensor([[-35.4207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0813984553653468, distance: 1.6509514697904324 entropy 6.4411629179060785
epoch: 7, step: 98
	action: tensor([[ -33.4780,  -30.4468,   21.8372,  234.2834,   93.2527, -454.0369,
         -141.7419]], dtype=torch.float64)
	q_value: tensor([[-35.8466]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.021688320096184, distance: 1.6270983111229074 entropy 6.248164323790818
epoch: 7, step: 99
	action: tensor([[ 190.3873, -171.5081,  -62.4660,  -36.7467,  -98.0507,   97.5620,
          374.1801]], dtype=torch.float64)
	q_value: tensor([[-36.9180]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21053070832011955, distance: 1.2590547005086887 entropy 6.42109630223752
epoch: 7, step: 100
	action: tensor([[-160.7984, -247.7956,   68.3933,   -0.5363,  157.1958,  -18.5695,
         -243.4757]], dtype=torch.float64)
	q_value: tensor([[-36.2751]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.3878243249196882, distance: 1.7683078680017934 entropy 6.337214572705228
epoch: 7, step: 101
	action: tensor([[ -68.0796,  -31.0292, -114.0508, -100.6596,    3.7718,   67.1762,
          -29.0009]], dtype=torch.float64)
	q_value: tensor([[-24.7194]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.31916910868243087, distance: 1.3143375923494285 entropy 6.023053502444875
epoch: 7, step: 102
	action: tensor([[-326.4817, -142.4207,   15.6378, -199.1635,   34.1992,  386.5889,
           49.8552]], dtype=torch.float64)
	q_value: tensor([[-37.6764]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1349722143960692, distance: 1.2191280059650993 entropy 6.340798679326468
epoch: 7, step: 103
	action: tensor([[-125.0297, -235.4411, -304.4496,  138.6528,  -86.0943,   -6.9438,
         -111.7264]], dtype=torch.float64)
	q_value: tensor([[-44.0656]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.252714934653787, distance: 1.7175516754115383 entropy 6.513520910838407
epoch: 7, step: 104
	action: tensor([[ -25.8702, -428.5747,  -81.7190,  -66.6130,    5.7538,  -71.0271,
          274.8859]], dtype=torch.float64)
	q_value: tensor([[-41.3083]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1908476473499564, distance: 1.029370798445403 entropy 6.434417102101684
epoch: 7, step: 105
	action: tensor([[-295.4593,    7.0718, -104.7457,  142.8391,  -11.8686,  -88.3439,
          -73.5539]], dtype=torch.float64)
	q_value: tensor([[-39.9218]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4052536775582305, distance: 0.8825159756891292 entropy 6.314185159343239
epoch: 7, step: 106
	action: tensor([[-315.1787, -273.3821,  124.1075,  110.7997,   -0.9649,  101.3206,
         -129.0985]], dtype=torch.float64)
	q_value: tensor([[-34.8340]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8671457928150896, distance: 1.5636725363232469 entropy 6.275842946032786
epoch: 7, step: 107
	action: tensor([[-371.3180,  -61.2414,  128.0654,  203.9991, -129.3587,  220.7915,
           28.2930]], dtype=torch.float64)
	q_value: tensor([[-31.7592]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1372728155043603, distance: 1.6729643027965888 entropy 6.263378117937781
epoch: 7, step: 108
	action: tensor([[-344.9982,  -95.6654,  163.1732, -110.2309,  -29.6208,  108.0914,
          -91.2007]], dtype=torch.float64)
	q_value: tensor([[-44.6832]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7080063924679476, distance: 1.4955517286569622 entropy 6.580060309389478
epoch: 7, step: 109
	action: tensor([[ -11.2749,  -91.6139, -190.2206,  -61.4257,  -43.3189,  177.2290,
           26.8948]], dtype=torch.float64)
	q_value: tensor([[-38.9694]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44951884477190684, distance: 0.8490395506827682 entropy 6.359778813140581
epoch: 7, step: 110
	action: tensor([[ -52.0118,  -39.4092,  -81.2542,  184.0775, -160.5849,  122.3626,
          -96.7550]], dtype=torch.float64)
	q_value: tensor([[-27.4316]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.030831006497949565, distance: 1.161850983377891 entropy 6.1390379857035215
epoch: 7, step: 111
	action: tensor([[-172.3980, -187.2341,    2.8371, -114.7244,  -61.5269,  146.5760,
           31.0458]], dtype=torch.float64)
	q_value: tensor([[-33.7816]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5722385676969772, distance: 1.4348811027831854 entropy 6.293016198420207
epoch: 7, step: 112
	action: tensor([[-251.6389, -144.3798, -148.7695,  -97.1627,   -1.7439, -190.0145,
          169.9323]], dtype=torch.float64)
	q_value: tensor([[-36.8023]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7052210014377276, distance: 1.4943317694947726 entropy 6.187751162573833
epoch: 7, step: 113
	action: tensor([[-291.6023, -125.1709,  121.8368,   47.3360, -129.0346,   60.0855,
           39.6495]], dtype=torch.float64)
	q_value: tensor([[-33.8713]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5038927689729278, distance: 1.4033471883329032 entropy 6.1067715899957955
epoch: 7, step: 114
	action: tensor([[   6.7394,   79.4190, -173.6834,  -97.2439, -274.7361,   63.7132,
          130.4503]], dtype=torch.float64)
	q_value: tensor([[-40.2334]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.27610539240720344, distance: 1.2927065972231826 entropy 6.3804804931164245
epoch: 7, step: 115
	action: tensor([[-266.1402, -293.5180, -110.3801,  105.8430,  198.9104,  172.5331,
           35.0981]], dtype=torch.float64)
	q_value: tensor([[-42.2147]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3898431056095464, distance: 1.3490858333246907 entropy 6.317254784111021
epoch: 7, step: 116
	action: tensor([[  32.5193, -264.1127,  -10.8563,   -8.9437, -389.0875,  108.1152,
          440.1076]], dtype=torch.float64)
	q_value: tensor([[-42.6226]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6624228838170321, distance: 1.4754600248446774 entropy 6.426789572744089
epoch: 7, step: 117
	action: tensor([[-155.7049,  -15.1832,  -88.1381,  -72.0531, -243.4362,   69.4650,
           49.0806]], dtype=torch.float64)
	q_value: tensor([[-29.6082]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24630304034478, distance: 0.9934707269459434 entropy 6.058922183760589
epoch: 7, step: 118
	action: tensor([[ -62.8133,   48.7846,   18.9382,  127.2943, -253.8807,  221.4302,
         -127.3186]], dtype=torch.float64)
	q_value: tensor([[-32.7205]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5841621129779488, distance: 1.440311752750848 entropy 6.1848445140196935
epoch: 7, step: 119
	action: tensor([[-191.5124,  142.9795, -156.6859,  121.3991,  -14.4283,  -45.6980,
          138.6947]], dtype=torch.float64)
	q_value: tensor([[-35.1772]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.947419220912401, distance: 1.5969319845183987 entropy 6.252385247066276
epoch: 7, step: 120
	action: tensor([[-208.8345, -344.0187, -127.8122,  -17.1753,   16.9811,   74.2416,
         -165.8981]], dtype=torch.float64)
	q_value: tensor([[-28.4085]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.7729174767582898, distance: 1.9055711355586429 entropy 5.89718257196767
epoch: 7, step: 121
	action: tensor([[-67.0298, -99.9031, 118.0530,  93.9793, -32.7569, -36.6617,  88.1355]],
       dtype=torch.float64)
	q_value: tensor([[-33.4640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24835619997395497, distance: 1.2785742525831476 entropy 6.234621750755847
epoch: 7, step: 122
	action: tensor([[-208.3576,  -31.4141,  101.0467,  -76.8148,  154.7421,  195.4057,
          154.7538]], dtype=torch.float64)
	q_value: tensor([[-32.9273]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.8662306889114295, distance: 1.937368633533383 entropy 6.16962203443402
epoch: 7, step: 123
	action: tensor([[ -15.0692,  -46.7682,  149.1752,  115.4080, -128.2855,  -55.5457,
          306.6191]], dtype=torch.float64)
	q_value: tensor([[-33.0612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5063260940917775, distance: 1.4044820497568429 entropy 6.210639023277735
epoch: 7, step: 124
	action: tensor([[-387.5784,  -26.5629,  306.6372,  -34.6967,  149.8427,  174.0973,
          -62.1916]], dtype=torch.float64)
	q_value: tensor([[-28.7863]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.056171992525512, distance: 1.6409162388249612 entropy 6.286000726826361
epoch: 7, step: 125
	action: tensor([[-65.7784,  22.6221,  63.5572, -72.5900,  -7.1291, -40.7737, -75.4113]],
       dtype=torch.float64)
	q_value: tensor([[-25.5723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0012997232986391, distance: 1.6188729295042847 entropy 6.043274102303298
epoch: 7, step: 126
	action: tensor([[ -33.4304,   69.3559,    9.0554,   59.9557,   77.7308,   59.2599,
         -100.5905]], dtype=torch.float64)
	q_value: tensor([[-22.9373]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7582149016494848, distance: 1.5173741164926338 entropy 5.835090184602227
epoch: 7, step: 127
	action: tensor([[-276.1006,  243.6225,  -76.1815,  166.2076, -104.3531,  -93.9026,
           47.3808]], dtype=torch.float64)
	q_value: tensor([[-38.7401]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6530665994661897, distance: 0.6740308117459037 entropy 6.340178354872508
LOSS epoch 7 actor 342.2645567873939 critic 149.45015084513923
epoch: 8, step: 0
	action: tensor([[-199.3604,   90.9547,  106.6313,  -32.7561,   93.7289,  118.0168,
            2.4157]], dtype=torch.float64)
	q_value: tensor([[-34.5162]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08798647555609018, distance: 1.1936264713001072 entropy 6.373404678539737
epoch: 8, step: 1
	action: tensor([[-193.1183, -116.3541,  194.9350,   31.9509, -267.8483,   -5.6922,
          101.7900]], dtype=torch.float64)
	q_value: tensor([[-30.9097]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.5884950545164647, distance: 1.8411126545436998 entropy 6.2731454842050045
epoch: 8, step: 2
	action: tensor([[-179.4746, -112.3401,  -13.2768, -238.8551,   67.8124,  241.2139,
           97.8042]], dtype=torch.float64)
	q_value: tensor([[-31.2876]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2422946366285, distance: 1.713574664185094 entropy 6.241956627709954
epoch: 8, step: 3
	action: tensor([[-208.6079, -187.0242, -201.8428,  247.1899,  -21.9812,   39.6138,
          336.9325]], dtype=torch.float64)
	q_value: tensor([[-34.5296]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.6253815710546284, distance: 1.8541843427721816 entropy 6.518181296372014
epoch: 8, step: 4
	action: tensor([[-160.6444,   49.2214, -295.6867, -231.8952, -124.1974,  191.1524,
         -136.2413]], dtype=torch.float64)
	q_value: tensor([[-32.7226]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7578531759929581, distance: 1.5172180202778238 entropy 6.700556686310479
epoch: 8, step: 5
	action: tensor([[-406.8934,    0.8039,  110.3641,  194.5018, -114.8853, -158.1027,
           -4.8615]], dtype=torch.float64)
	q_value: tensor([[-32.3997]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14044327406296309, distance: 1.222062837017157 entropy 6.29216860765962
epoch: 8, step: 6
	action: tensor([[-153.7958, -346.8623,   20.4594,   20.4665,   70.0111,  -19.4945,
         -189.5740]], dtype=torch.float64)
	q_value: tensor([[-35.4000]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.37695645564339353, distance: 1.342816894148119 entropy 6.556705813426852
epoch: 8, step: 7
	action: tensor([[-104.8224,  -83.2166,  183.8730, -128.6347,   56.9972,  114.0383,
           98.4442]], dtype=torch.float64)
	q_value: tensor([[-33.9740]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6268870161140843, distance: 1.459605159421883 entropy 6.396482756609214
epoch: 8, step: 8
	action: tensor([[-145.3020, -152.0717,   52.7366,    7.5230,   58.2502,   43.9518,
          315.7806]], dtype=torch.float64)
	q_value: tensor([[-24.4153]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8373498502136587, distance: 1.5511458041447623 entropy 6.181440071405069
epoch: 8, step: 9
	action: tensor([[-200.4565,  103.6548,  -49.4211,  234.1297,  -57.2145,   24.2051,
          172.6763]], dtype=torch.float64)
	q_value: tensor([[-33.0992]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3500454928696092, distance: 0.92256754635454 entropy 6.37755668875264
epoch: 8, step: 10
	action: tensor([[-277.5693, -193.1798,  -92.7292,   49.1574,  -68.0461,   73.0562,
           45.2575]], dtype=torch.float64)
	q_value: tensor([[-27.9746]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.45166924436875067, distance: 1.3787658916917973 entropy 6.256976396639909
epoch: 8, step: 11
	action: tensor([[ -47.7258,  -75.7185, -107.0131,  177.8519,  221.2338, -145.9601,
           66.3241]], dtype=torch.float64)
	q_value: tensor([[-33.8369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.23113038224073, distance: 1.7093034467944486 entropy 6.39286048001277
epoch: 8, step: 12
	action: tensor([[-373.6520,  -80.9208,  200.7735,  -36.0843, -222.0558,   50.2382,
          104.5826]], dtype=torch.float64)
	q_value: tensor([[-35.2179]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.9692556215384758, distance: 1.97188002184415 entropy 6.442203510885789
epoch: 8, step: 13
	action: tensor([[-260.7346,   -1.8103, -187.0927,  296.1952,  -43.6007, -192.8219,
           35.8561]], dtype=torch.float64)
	q_value: tensor([[-27.7805]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 6.52129466900505
epoch: 8, step: 14
	action: tensor([[-215.9755, -111.4224,  -88.7954,  339.6335,  115.5193,   30.9266,
          -55.9369]], dtype=torch.float64)
	q_value: tensor([[-35.0120]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.013167821060440232, distance: 1.1518538737020703 entropy 5.991488759240534
epoch: 8, step: 15
	action: tensor([[ -33.8907, -231.1788,  -44.5378,  144.2733,  231.9266,   33.8485,
          265.6422]], dtype=torch.float64)
	q_value: tensor([[-33.6865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14857732111373334, distance: 1.0559158289060748 entropy 6.21947925854957
epoch: 8, step: 16
	action: tensor([[ -39.0327, -258.9760,  -91.9720, -247.0543,   97.8965,  -20.1953,
         -160.6595]], dtype=torch.float64)
	q_value: tensor([[-28.7999]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.5095883931694187, distance: 1.8128335990775448 entropy 6.29650003677466
epoch: 8, step: 17
	action: tensor([[-305.3806,  -39.1872,    9.0053,  -69.4025,   73.4545,   96.5691,
          121.8812]], dtype=torch.float64)
	q_value: tensor([[-29.2166]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8777956018186239, distance: 1.5681256259960081 entropy 6.433209136285988
epoch: 8, step: 18
	action: tensor([[224.3343, -42.8290, -51.9644, 299.4486,  82.0047, -98.3695,  89.6536]],
       dtype=torch.float64)
	q_value: tensor([[-36.1380]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1574443356179207, distance: 1.231138039323025 entropy 6.600439745947802
epoch: 8, step: 19
	action: tensor([[  6.5697, -78.2301, -39.0918,  53.1673, -66.5297, -71.7406, 243.0049]],
       dtype=torch.float64)
	q_value: tensor([[-28.2174]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31255662737428214, distance: 0.9488010529744563 entropy 6.275247845845849
epoch: 8, step: 20
	action: tensor([[-316.0509,  -54.6495,   28.0929,   -0.9237,  -35.3609, -326.9436,
          -19.7931]], dtype=torch.float64)
	q_value: tensor([[-39.0116]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8052386067207609, distance: 1.5375314204998538 entropy 6.476175937098467
epoch: 8, step: 21
	action: tensor([[-252.6005,  -97.5660,  150.9069,   56.5309,  -10.7836,  163.3805,
           39.2826]], dtype=torch.float64)
	q_value: tensor([[-40.2861]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8337378390110854, distance: 1.5496203700205478 entropy 6.510510178999017
epoch: 8, step: 22
	action: tensor([[-220.3820, -161.1318,  261.4390, -129.7096,   63.6120,  -80.9499,
          372.2614]], dtype=torch.float64)
	q_value: tensor([[-31.6192]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18590930161517671, distance: 1.2461847461356292 entropy 6.3809973350911
epoch: 8, step: 23
	action: tensor([[-415.8086, -213.0266,  -59.9048,  285.2240, -146.6408, -284.5147,
         -149.5860]], dtype=torch.float64)
	q_value: tensor([[-35.5038]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0218643620237824, distance: 1.6271691507486299 entropy 6.550097887230051
epoch: 8, step: 24
	action: tensor([[  61.7895, -174.4496, -133.7425, -197.6767, -181.3098, -183.9852,
         -211.4133]], dtype=torch.float64)
	q_value: tensor([[-35.1835]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2943495865571025, distance: 0.961283512231001 entropy 6.505810759699074
epoch: 8, step: 25
	action: tensor([[-337.1457, -269.3070, -170.0836,  109.0022, -335.8600,    0.9098,
          -17.7874]], dtype=torch.float64)
	q_value: tensor([[-36.5823]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7241907085630965, distance: 1.5026206173693777 entropy 6.506052570288864
epoch: 8, step: 26
	action: tensor([[ -34.1732, -108.9231, -113.5123,   75.6664, -192.7724,   28.8048,
          -74.9011]], dtype=torch.float64)
	q_value: tensor([[-33.4725]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6119400279799034, distance: 1.452884642748525 entropy 6.410940375772002
epoch: 8, step: 27
	action: tensor([[-101.6722, -131.6748,  -60.9957,  253.3360,  -75.2843, -132.7349,
           89.9759]], dtype=torch.float64)
	q_value: tensor([[-30.3341]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5959812791715671, distance: 0.7273734385593386 entropy 6.28789338529243
epoch: 8, step: 28
	action: tensor([[-100.2794,  -80.5138,   74.2123,   59.4704,   58.3118,  285.6883,
          179.2674]], dtype=torch.float64)
	q_value: tensor([[-25.8647]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.002543856914382303, distance: 1.1457988535357806 entropy 6.23428571827174
epoch: 8, step: 29
	action: tensor([[-308.0208, -189.1066,  -59.4086,  -77.5946,  -44.6704,  110.6883,
          -20.4459]], dtype=torch.float64)
	q_value: tensor([[-40.8872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06909524107879983, distance: 1.1041023100063485 entropy 6.572507694725824
epoch: 8, step: 30
	action: tensor([[ -85.4613, -118.5289,  -82.0333,  -59.1158,  -47.9961,  318.2778,
          104.3094]], dtype=torch.float64)
	q_value: tensor([[-33.9807]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5145915949118993, distance: 1.4083301097036274 entropy 6.592517887583918
epoch: 8, step: 31
	action: tensor([[-212.8786,  -96.6343,  -28.5893, -111.3792, -243.4435,   73.9222,
          -19.0162]], dtype=torch.float64)
	q_value: tensor([[-34.6755]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6079648552019878, distance: 1.451092072159917 entropy 6.542607947078921
epoch: 8, step: 32
	action: tensor([[ -80.3155, -152.4162,  -20.8312, -130.4185, -159.6509, -169.7360,
          228.9043]], dtype=torch.float64)
	q_value: tensor([[-37.3468]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.7279454420645104, distance: 1.8900553971200702 entropy 6.583032347137263
epoch: 8, step: 33
	action: tensor([[-415.4806, -218.8837, -182.8536, -106.5295, -185.5443,   53.5959,
         -139.5444]], dtype=torch.float64)
	q_value: tensor([[-33.8154]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.040901157730353854, distance: 1.1206974316586915 entropy 6.590685753345304
epoch: 8, step: 34
	action: tensor([[-103.0951,  -73.9501,  -99.0592, -153.8236,  -35.6814,  234.8765,
           74.2624]], dtype=torch.float64)
	q_value: tensor([[-34.2076]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15884441814477512, distance: 1.0495300043859084 entropy 6.454618676583897
epoch: 8, step: 35
	action: tensor([[-214.9484,  -25.8251, -163.2127, -230.4969, -155.1059, -162.7699,
         -112.6540]], dtype=torch.float64)
	q_value: tensor([[-34.6182]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.49872177767664416, distance: 1.4009324733820399 entropy 6.408420555243454
epoch: 8, step: 36
	action: tensor([[-110.9744, -203.2247,   55.0798,  -62.0229,  -47.0569,    4.1807,
          141.4708]], dtype=torch.float64)
	q_value: tensor([[-30.9872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 6.251329808668918
epoch: 8, step: 37
	action: tensor([[-181.9199, -235.5426,   27.7358,   32.5846,   80.5069,  -31.5112,
          -68.4313]], dtype=torch.float64)
	q_value: tensor([[-35.0120]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.328130356431441, distance: 1.7460647311244786 entropy 5.991488759240534
epoch: 8, step: 38
	action: tensor([[ 138.9430, -323.1141, -277.9955, -223.7134,   65.1741, -184.9495,
            5.6206]], dtype=torch.float64)
	q_value: tensor([[-37.5082]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.027283966818749228, distance: 1.1286251674175996 entropy 6.4872344954328565
epoch: 8, step: 39
	action: tensor([[-263.9702, -172.6207, -194.5947,   32.3090,    6.8063,  274.1732,
          153.9447]], dtype=torch.float64)
	q_value: tensor([[-39.6752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.588938442025642, distance: 1.4424814250247584 entropy 6.49983955841617
epoch: 8, step: 40
	action: tensor([[  -8.0174, -198.1392,   34.3797,  263.6902,  -79.9872,  133.6588,
          -39.7835]], dtype=torch.float64)
	q_value: tensor([[-26.7626]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5229041541846313, distance: 1.4121895028178157 entropy 6.191537890328859
epoch: 8, step: 41
	action: tensor([[-142.8132, -135.3350, -192.8242,   85.3367,   -7.7407, -144.6114,
          105.7728]], dtype=torch.float64)
	q_value: tensor([[-30.2222]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.560279387036347, distance: 0.7588310718243214 entropy 6.375997007283821
epoch: 8, step: 42
	action: tensor([[ -49.9344,  -73.3972,   45.4051,   16.8142, -231.0882,  -25.1986,
           71.3449]], dtype=torch.float64)
	q_value: tensor([[-29.0136]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 6.531105008415457
epoch: 8, step: 43
	action: tensor([[-1.2319e+02, -5.5339e+01,  1.9887e+02, -2.1022e+02, -6.3226e+00,
         -1.2454e-01,  9.1338e+00]], dtype=torch.float64)
	q_value: tensor([[-35.0120]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.991488759240534
epoch: 8, step: 44
	action: tensor([[-175.7609,   -6.4901,  -47.3477,  -17.1747,  -21.1238,  -52.9196,
         -105.1843]], dtype=torch.float64)
	q_value: tensor([[-35.0120]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.991488759240534
epoch: 8, step: 45
	action: tensor([[-107.2103, -141.1866,   49.9550,   37.6856, -137.8342,   27.3959,
           -5.4294]], dtype=torch.float64)
	q_value: tensor([[-35.0120]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10354497765857895, distance: 1.0834801160993275 entropy 5.991488759240534
epoch: 8, step: 46
	action: tensor([[-210.9277,   89.6334, -192.6823,   34.3872,   34.1368,   49.2534,
         -187.9363]], dtype=torch.float64)
	q_value: tensor([[-28.2390]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6397116033771875, distance: 1.4653468269641077 entropy 6.23681156610199
epoch: 8, step: 47
	action: tensor([[  25.7928, -175.0714,   40.1128,   35.1225, -284.7786, -230.6996,
          292.2320]], dtype=torch.float64)
	q_value: tensor([[-44.8949]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.094008090015012, distance: 1.6559448577561118 entropy 6.436501855352217
epoch: 8, step: 48
	action: tensor([[-257.8692, -210.9098,  200.3322,  -46.7035,  162.4217,  -53.6175,
         -162.6012]], dtype=torch.float64)
	q_value: tensor([[-38.1269]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.552328614612704, distance: 0.7656607088377266 entropy 6.436852051756186
epoch: 8, step: 49
	action: tensor([[-283.5149, -139.5602,  -14.8665,   -6.1388,  -37.7408,  145.8180,
         -139.3213]], dtype=torch.float64)
	q_value: tensor([[-34.2355]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5136478148965684, distance: 1.4078912584189642 entropy 6.1520189667388445
epoch: 8, step: 50
	action: tensor([[-120.2456, -154.4213,    6.7732,  -34.5141,  -24.6145,  104.3050,
          171.7590]], dtype=torch.float64)
	q_value: tensor([[-28.1525]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.387724732634266, distance: 0.8954267236444506 entropy 6.089291485384065
epoch: 8, step: 51
	action: tensor([[-527.2274, -109.9408,   16.5755,  -16.8227, -111.0821,  134.7715,
          167.7657]], dtype=torch.float64)
	q_value: tensor([[-36.5039]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8008225864112735, distance: 1.535649694902089 entropy 6.284137007881649
epoch: 8, step: 52
	action: tensor([[-136.1901,  -63.9280,    8.9373,  -70.8722,  -18.4929,  167.2799,
          -23.9436]], dtype=torch.float64)
	q_value: tensor([[-27.3397]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -2.9813166281558985, distance: 2.2833371996020646 entropy 6.276888813837699
epoch: 8, step: 53
	action: tensor([[-255.2706, -289.9434, -184.8591,   -0.9037,  104.1545,  -57.9873,
         -119.9040]], dtype=torch.float64)
	q_value: tensor([[-35.7272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.593816980983576, distance: 1.8430043396423874 entropy 6.356414923473503
epoch: 8, step: 54
	action: tensor([[-216.8890, -115.3869,   40.7754,  228.9052, -177.3315,  109.8468,
          -47.7119]], dtype=torch.float64)
	q_value: tensor([[-39.5146]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.4400829970300864, distance: 1.7875532689800175 entropy 6.437532540458897
epoch: 8, step: 55
	action: tensor([[-201.8217, -150.0053,   -9.8686,  150.4470, -240.1345, -150.8801,
         -122.9403]], dtype=torch.float64)
	q_value: tensor([[-45.0597]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -5.973037850132403, distance: 3.0218138303762134 entropy 6.572601791295831
epoch: 8, step: 56
	action: tensor([[-250.9570,  121.0568,  -20.3209,  106.5553,   24.2809, -102.5767,
         -256.1942]], dtype=torch.float64)
	q_value: tensor([[-29.2911]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -2.371466084630497, distance: 2.101193704344889 entropy 6.334267711901502
epoch: 8, step: 57
	action: tensor([[ -34.3040,  -54.7335, -192.9660,  -83.2945,  -61.2256,  -82.2880,
         -113.0794]], dtype=torch.float64)
	q_value: tensor([[-32.5466]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.424795638499482, distance: 1.3659442735253047 entropy 6.388271431201866
epoch: 8, step: 58
	action: tensor([[ -24.7719,   30.6584, -209.2749,  -42.6615, -136.5879,  304.0171,
          184.6098]], dtype=torch.float64)
	q_value: tensor([[-39.4394]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.116138464732682, distance: 1.6646722271058358 entropy 6.465810581748399
epoch: 8, step: 59
	action: tensor([[-232.8922, -131.0104,   52.2705,   21.0442, -166.7854,   95.5503,
           67.0755]], dtype=torch.float64)
	q_value: tensor([[-27.0005]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -5.44013285692083, distance: 2.9040501147198876 entropy 6.109912613311873
epoch: 8, step: 60
	action: tensor([[-201.1214,    9.5450,  -17.7420,  135.7157,  -56.6562,  108.7468,
           73.3778]], dtype=torch.float64)
	q_value: tensor([[-21.7549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.910466655809982
epoch: 8, step: 61
	action: tensor([[-219.6973,   33.9711,  -14.9855,   19.9760,  -80.6050,  126.6345,
          -91.4182]], dtype=torch.float64)
	q_value: tensor([[-35.0120]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41859667508512843, distance: 0.8725602987185611 entropy 5.991488759240534
epoch: 8, step: 62
	action: tensor([[-201.6307, -162.7579,   -6.3956,   46.6693,  -74.9226,   21.3206,
         -119.8421]], dtype=torch.float64)
	q_value: tensor([[-39.5045]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3601117477342619, distance: 0.9153954760374103 entropy 6.31741774605117
epoch: 8, step: 63
	action: tensor([[ -1.8791, -70.8541,  60.3644, -30.0420, -36.7460, 242.5862, 125.0402]],
       dtype=torch.float64)
	q_value: tensor([[-23.7132]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.92516188811044
epoch: 8, step: 64
	action: tensor([[-109.9664, -166.7709,   -5.2746,  -24.5426,   -4.3857,   29.8563,
           32.8440]], dtype=torch.float64)
	q_value: tensor([[-35.0120]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11365857065305529, distance: 1.2076267518510055 entropy 5.991488759240534
epoch: 8, step: 65
	action: tensor([[ 32.0284,   8.1363,  91.0255,  41.8363, -47.0944, -27.9203, 202.8126]],
       dtype=torch.float64)
	q_value: tensor([[-31.1865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 6.356444925753337
epoch: 8, step: 66
	action: tensor([[ 126.0803, -194.0752,  -68.6115,   21.4017,   47.2163,  -46.0040,
          116.8847]], dtype=torch.float64)
	q_value: tensor([[-35.0120]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.991488759240534
epoch: 8, step: 67
	action: tensor([[-125.4335,  -41.7413, -222.9463,   43.4415,   68.6160,  -61.6010,
          166.3633]], dtype=torch.float64)
	q_value: tensor([[-35.0120]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3320577430942785, distance: 0.9352466324537899 entropy 5.991488759240534
epoch: 8, step: 68
	action: tensor([[ -44.6070,  -19.3478, -158.1163,  -68.0562,  295.1578,  111.0648,
          183.6606]], dtype=torch.float64)
	q_value: tensor([[-40.6359]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9487925273126561, distance: 1.5974949579121624 entropy 6.436793254243574
epoch: 8, step: 69
	action: tensor([[ -36.3546, -165.2470,    6.8350,  128.5342,    4.9090, -113.4987,
          -20.1942]], dtype=torch.float64)
	q_value: tensor([[-25.6791]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6330946934924269, distance: 1.4623871999250861 entropy 5.925082175722416
epoch: 8, step: 70
	action: tensor([[-84.8042,  -4.1009, 130.3882, 106.5391,  22.3840, -59.0732, -57.8742]],
       dtype=torch.float64)
	q_value: tensor([[-22.8808]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 6.128395168393202
epoch: 8, step: 71
	action: tensor([[ -70.5982,   15.4602,   48.7249,    4.7713, -220.7088,  145.8026,
           -0.5677]], dtype=torch.float64)
	q_value: tensor([[-35.0120]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.991488759240534
epoch: 8, step: 72
	action: tensor([[ -65.2796, -146.6666,   -3.1372,  -77.0699, -104.6248,  205.7365,
           31.7931]], dtype=torch.float64)
	q_value: tensor([[-35.0120]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2888305725046989, distance: 1.2991359714597621 entropy 5.991488759240534
epoch: 8, step: 73
	action: tensor([[ -70.5120,  -56.3255,   89.7022,  246.3472, -138.6764,   39.7632,
          273.9744]], dtype=torch.float64)
	q_value: tensor([[-39.2376]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.855136300674578, distance: 1.5586356487030502 entropy 6.522950148386359
epoch: 8, step: 74
	action: tensor([[  34.7780,  -82.5883,  102.8599,   40.7865,   46.0984, -183.3076,
          -43.9384]], dtype=torch.float64)
	q_value: tensor([[-25.7196]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12056884726720951, distance: 1.0731430473354528 entropy 6.073655765341089
epoch: 8, step: 75
	action: tensor([[-107.3991, -247.2216,  -41.6525,  198.1400,   84.6734, -176.3437,
          -39.6004]], dtype=torch.float64)
	q_value: tensor([[-30.6298]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26963990949455496, distance: 0.9779692737456972 entropy 6.199537182545435
epoch: 8, step: 76
	action: tensor([[-529.7214, -158.0326,   65.3453,  279.4399,  198.2849,   46.4751,
           41.5621]], dtype=torch.float64)
	q_value: tensor([[-41.6539]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4111955175462213, distance: 0.8780965018919303 entropy 6.612964591124578
epoch: 8, step: 77
	action: tensor([[  98.5302, -399.0060,   -7.9110,  112.4099,  303.2069,   -5.9773,
          -98.1782]], dtype=torch.float64)
	q_value: tensor([[-35.9182]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.43220333968902547, distance: 1.3694905327082967 entropy 6.701469947497821
epoch: 8, step: 78
	action: tensor([[-482.4808, -290.7203,  362.6656,  -51.3989, -366.1269,   -6.9290,
          148.2649]], dtype=torch.float64)
	q_value: tensor([[-34.2266]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9934310232650683, distance: 1.6156872569082605 entropy 6.4575464847745305
epoch: 8, step: 79
	action: tensor([[-291.2342,  -91.3422,   92.4404,  172.4264, -254.3828,  120.5617,
          -54.6286]], dtype=torch.float64)
	q_value: tensor([[-32.5655]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7121468726089639, distance: 1.4973633594620865 entropy 6.631315022605258
epoch: 8, step: 80
	action: tensor([[-166.7361, -215.4890,  123.9861,  393.0714, -206.7468,  196.2276,
           68.5112]], dtype=torch.float64)
	q_value: tensor([[-36.6963]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15577724866983234, distance: 1.2302511052139689 entropy 6.713763929657157
epoch: 8, step: 81
	action: tensor([[-321.9405,   38.7062,  109.5347, -137.9745, -351.9605,  -10.5286,
           63.8171]], dtype=torch.float64)
	q_value: tensor([[-32.0300]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7747254091462068, distance: 1.5244819157507168 entropy 6.419809714347445
epoch: 8, step: 82
	action: tensor([[-216.8017,  -70.3785,  276.5976,   47.9705,  175.1401,  188.6015,
          -54.5587]], dtype=torch.float64)
	q_value: tensor([[-28.7864]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04441899257334725, distance: 1.1186402661153871 entropy 6.289476769489545
epoch: 8, step: 83
	action: tensor([[-188.2202,  -54.1272, -128.5191, -103.6241,  -54.5153,   64.7318,
          300.2670]], dtype=torch.float64)
	q_value: tensor([[-30.4530]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16702744209576803, distance: 1.236224161552936 entropy 6.396690110723286
epoch: 8, step: 84
	action: tensor([[-198.2560,  -50.7302,  237.2907,   93.6701,   26.7259,  -90.6910,
          153.1005]], dtype=torch.float64)
	q_value: tensor([[-34.3432]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5533776962142121, distance: 1.426248582668299 entropy 6.447709694695368
epoch: 8, step: 85
	action: tensor([[-269.8828,  -40.2955,  100.8875, -115.9763, -194.9970,  349.3180,
          -31.5257]], dtype=torch.float64)
	q_value: tensor([[-35.4758]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1369716430780863, distance: 1.2202013744940976 entropy 6.4756072256053985
epoch: 8, step: 86
	action: tensor([[-323.8319, -308.8574,  166.9668,    9.2368, -195.2058,  149.3796,
          -56.6042]], dtype=torch.float64)
	q_value: tensor([[-27.7159]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.183181014296238, distance: 1.0342358913345575 entropy 6.3058151510385985
epoch: 8, step: 87
	action: tensor([[ -63.8545,   21.1343,   54.5306,   13.0641, -226.1397, -241.2848,
           85.6847]], dtype=torch.float64)
	q_value: tensor([[-30.6419]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.3433880405944638, distance: 1.7517769108504655 entropy 6.457601211974719
epoch: 8, step: 88
	action: tensor([[ 10.6490, -28.8209, 153.8637, 238.5698,  23.0043, 123.0605, 302.2353]],
       dtype=torch.float64)
	q_value: tensor([[-26.3752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.41562196497100223, distance: 1.3615397955320858 entropy 6.173714972253597
epoch: 8, step: 89
	action: tensor([[-169.8310, -147.3535,  -48.2419,  -92.5086,  -38.5272,   45.0269,
           47.5818]], dtype=torch.float64)
	q_value: tensor([[-32.3790]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21005660918753544, distance: 1.0170789785148706 entropy 6.525072882648345
epoch: 8, step: 90
	action: tensor([[-189.2119,  -16.8711,   68.2197,   94.2824, -235.2986,  101.2770,
            2.7450]], dtype=torch.float64)
	q_value: tensor([[-26.3621]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 6.340151805665122
epoch: 8, step: 91
	action: tensor([[  19.1120, -158.1135,  120.9693,   89.8000, -224.3039,   60.2278,
          218.3954]], dtype=torch.float64)
	q_value: tensor([[-35.0120]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3012761934742232, distance: 1.3053934689791418 entropy 5.991488759240534
epoch: 8, step: 92
	action: tensor([[-266.8340, -243.6319,  -53.0036,  187.7747, -122.3681,   40.9898,
           91.2644]], dtype=torch.float64)
	q_value: tensor([[-34.7927]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7504000066446737, distance: 1.5139981567425873 entropy 6.40449439892151
epoch: 8, step: 93
	action: tensor([[ -27.0948,  -81.5632, -122.5093,  110.9227,   29.8329, -311.6459,
            7.8876]], dtype=torch.float64)
	q_value: tensor([[-39.5195]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13461536704650823, distance: 1.2189363374849953 entropy 6.849518090692159
epoch: 8, step: 94
	action: tensor([[-146.2741,  -39.0554, -167.7642,  -55.3518,   42.8861,  154.9797,
          116.2102]], dtype=torch.float64)
	q_value: tensor([[-36.0992]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1416912593661901, distance: 1.0601772018327638 entropy 6.664227978775108
epoch: 8, step: 95
	action: tensor([[-107.8505, -290.5390, -256.2257, -310.8362,    9.2688,   92.8263,
         -150.2601]], dtype=torch.float64)
	q_value: tensor([[-35.3856]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.695811227787164, distance: 1.4902030449036126 entropy 6.547999065296941
epoch: 8, step: 96
	action: tensor([[-282.4007, -128.8405,    6.1794,  -74.4383,   28.7360, -106.4779,
         -103.8428]], dtype=torch.float64)
	q_value: tensor([[-36.4534]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3413260999454413, distance: 0.9287352313201555 entropy 6.463569940807296
epoch: 8, step: 97
	action: tensor([[-188.0274,    9.4034,   86.6235,   65.3811, -211.7705, -187.5405,
         -299.0574]], dtype=torch.float64)
	q_value: tensor([[-36.8678]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 6.568976404604263
epoch: 8, step: 98
	action: tensor([[ -76.5154, -143.4145, -108.9383,  -19.7415,   49.4321,   24.0373,
          -47.3443]], dtype=torch.float64)
	q_value: tensor([[-35.0120]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.030593554511605836, distance: 1.1617171594477833 entropy 5.991488759240534
epoch: 8, step: 99
	action: tensor([[-64.6095, 158.3574, 142.9214, -45.9497, 291.9356, 162.8167, -64.5201]],
       dtype=torch.float64)
	q_value: tensor([[-35.6213]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 6.294746566192307
epoch: 8, step: 100
	action: tensor([[-193.1451,  -92.0469,  -54.3927,   13.8217,  -18.6610, -146.0474,
          151.1552]], dtype=torch.float64)
	q_value: tensor([[-35.0120]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.991488759240534
epoch: 8, step: 101
	action: tensor([[ -43.2420, -186.2468,  -32.5590, -122.9070, -209.9457,  -54.7193,
           74.1391]], dtype=torch.float64)
	q_value: tensor([[-35.0120]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04156448744868135, distance: 1.1678841792035053 entropy 5.991488759240534
epoch: 8, step: 102
	action: tensor([[-667.4119,  -40.6273, -208.5327,   76.1425, -230.4550,  271.7980,
          190.0888]], dtype=torch.float64)
	q_value: tensor([[-43.4968]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.012172624996196646, distance: 1.137358092188542 entropy 6.527041684484287
epoch: 8, step: 103
	action: tensor([[-261.4104, -162.7214, -114.5980,   67.6023,   -7.3330,   22.8050,
          -87.6858]], dtype=torch.float64)
	q_value: tensor([[-28.5362]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0128188873174047, distance: 1.6235252326323855 entropy 6.248261219289843
epoch: 8, step: 104
	action: tensor([[-321.3768, -198.7764,  -22.7079,  123.4903,  -89.0781,  237.5206,
           27.7105]], dtype=torch.float64)
	q_value: tensor([[-30.1149]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12899630933316097, distance: 1.2159142672044607 entropy 6.228286000858183
epoch: 8, step: 105
	action: tensor([[-402.1689, -133.3068, -359.3512, -155.6183, -102.6119,    4.0536,
          -82.9024]], dtype=torch.float64)
	q_value: tensor([[-38.3025]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 6.594593194198311
epoch: 8, step: 106
	action: tensor([[-173.5786,   40.8919,   56.7551,  -12.3181,  138.7385,  -37.2629,
           -0.4915]], dtype=torch.float64)
	q_value: tensor([[-35.0120]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 5.991488759240534
epoch: 8, step: 107
	action: tensor([[ -14.8814, -165.2249,   39.7235,   48.0570,   44.3833,  196.4903,
          156.6802]], dtype=torch.float64)
	q_value: tensor([[-35.0120]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.3287003841254337, distance: 1.7462784743827622 entropy 5.991488759240534
epoch: 8, step: 108
	action: tensor([[-202.7979,  -65.9860, -310.5808,   14.4144,   70.1922,  167.7202,
          -87.0030]], dtype=torch.float64)
	q_value: tensor([[-31.5533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.662014235254661, distance: 1.475278668559935 entropy 6.297751180935235
epoch: 8, step: 109
	action: tensor([[ -30.1578, -226.0311,   94.8525,  170.7759,   51.8257, -232.2686,
         -146.5218]], dtype=torch.float64)
	q_value: tensor([[-35.9527]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15755451642554108, distance: 1.0503344167791184 entropy 6.408541666411218
epoch: 8, step: 110
	action: tensor([[-179.6653, -354.3591,  -29.0810, -111.7083, -228.1104,  101.6442,
         -111.6017]], dtype=torch.float64)
	q_value: tensor([[-28.7642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11560022581956075, distance: 1.0761703061980998 entropy 6.286394603663828
epoch: 8, step: 111
	action: tensor([[-317.9681, -113.9074, -102.2025, -168.5333,  -82.7496,   16.2914,
          -12.3027]], dtype=torch.float64)
	q_value: tensor([[-29.6739]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.34123668877857893, distance: 1.3252853758273617 entropy 6.487335359110484
epoch: 8, step: 112
	action: tensor([[-225.2585, -278.9344,  146.2064,   34.4663, -103.1938,  130.8745,
           98.8772]], dtype=torch.float64)
	q_value: tensor([[-34.3052]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2981332101875598, distance: 1.303816052027882 entropy 6.451054451718676
epoch: 8, step: 113
	action: tensor([[-296.4895, -588.3589, -268.8909,  182.7972,  204.3988,   -4.6624,
          -98.1709]], dtype=torch.float64)
	q_value: tensor([[-35.5224]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3006211365713043, distance: 1.3050648628582542 entropy 6.660334014299954
epoch: 8, step: 114
	action: tensor([[  59.5610, -124.6298,  341.2042, -288.7212,  170.0170, -161.2214,
          -16.2601]], dtype=torch.float64)
	q_value: tensor([[-34.4026]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07801820706793461, distance: 1.0987980136880682 entropy 6.652123410125425
epoch: 8, step: 115
	action: tensor([[-266.3919, -214.7228, -181.5500, -151.3142, -230.1224,  118.4165,
          -80.3390]], dtype=torch.float64)
	q_value: tensor([[-34.5590]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.443969858399634, distance: 1.7889764187383042 entropy 6.371265972838742
epoch: 8, step: 116
	action: tensor([[-207.6774,  -88.4486,   87.3642,  -78.3678,   25.1170, -145.4128,
          -35.8365]], dtype=torch.float64)
	q_value: tensor([[-32.6126]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.179539537705937, distance: 1.0365387011991107 entropy 6.392918230921633
epoch: 8, step: 117
	action: tensor([[-242.5238, -186.9695, -267.7529,  -48.7700,  204.3105,  137.9956,
          185.4145]], dtype=torch.float64)
	q_value: tensor([[-34.2652]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0256854973642211, distance: 1.1589476006482116 entropy 6.429882359152218
epoch: 8, step: 118
	action: tensor([[-138.8215, -260.4741,  -62.8008,   61.7573, -260.3737,  134.6415,
         -107.9893]], dtype=torch.float64)
	q_value: tensor([[-43.5439]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.025133089314614998, distance: 1.1298722909028274 entropy 6.611977584128577
epoch: 8, step: 119
	action: tensor([[-371.6556,    9.8710,   41.8516, -131.2754,   52.5161, -241.5787,
          140.8852]], dtype=torch.float64)
	q_value: tensor([[-37.6925]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 6.542341231269812
epoch: 8, step: 120
	action: tensor([[-42.4303, -67.7619,  13.1095, -24.8887, 103.9398,  39.8449,  73.7466]],
       dtype=torch.float64)
	q_value: tensor([[-35.0120]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6883268899554111, distance: 1.4869109579037216 entropy 5.991488759240534
epoch: 8, step: 121
	action: tensor([[-204.8361,  -97.4002,   16.8753,  142.3221,  -81.4812,   54.7168,
          -64.8902]], dtype=torch.float64)
	q_value: tensor([[-38.8281]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8054631108759733, distance: 1.537627023225199 entropy 6.2808594700608245
epoch: 8, step: 122
	action: tensor([[-285.5558, -161.9986,   16.3783,  -68.5832, -333.0635,  -67.8652,
          266.7082]], dtype=torch.float64)
	q_value: tensor([[-35.8259]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22134578089006107, distance: 1.0097852295562717 entropy 6.320342309601837
epoch: 8, step: 123
	action: tensor([[-153.4085, -440.4268,   10.2028,  278.1671, -200.0292,  353.5567,
         -168.7645]], dtype=torch.float64)
	q_value: tensor([[-40.6748]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1125224231593638, distance: 1.6632493292642334 entropy 6.560458287410181
epoch: 8, step: 124
	action: tensor([[-246.2468,   -6.8628,    8.3344, -107.3645,   84.6768,  101.9877,
          116.0187]], dtype=torch.float64)
	q_value: tensor([[-32.4085]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7247473190914306, distance: 1.5028631389657348 entropy 6.346889971883473
epoch: 8, step: 125
	action: tensor([[-281.1527, -156.7370,  144.3133,  234.7239,  374.4658,  182.9331,
         -307.2738]], dtype=torch.float64)
	q_value: tensor([[-42.4626]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.078772384880101, distance: 1.6499096502162467 entropy 6.619485618395182
epoch: 8, step: 126
	action: tensor([[-230.7294,  -92.7125,   38.0131,   38.6330, -461.8469,  112.8085,
         -102.7157]], dtype=torch.float64)
	q_value: tensor([[-31.9295]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2096516574458267, distance: 1.7010559580799958 entropy 6.496393867951802
epoch: 8, step: 127
	action: tensor([[-129.2429, -179.4706, -241.7588,  -22.7913,  -34.2861,  121.1938,
          -39.4094]], dtype=torch.float64)
	q_value: tensor([[-42.9457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5606527309210558, distance: 1.4295845027320406 entropy 6.566962151424682
LOSS epoch 8 actor 360.35469085797354 critic 202.7927026813607
epoch: 9, step: 0
	action: tensor([[-423.9044, -171.5890, -233.5257,    5.6779, -171.4510,   94.3353,
         -118.6603]], dtype=torch.float64)
	q_value: tensor([[-32.7536]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.5462782031065379, distance: 1.826037194647166 entropy 6.830950756957051
epoch: 9, step: 1
	action: tensor([[-161.4392, -141.9593,  -25.1208,   62.5495,   43.0076,   -2.4521,
           58.8034]], dtype=torch.float64)
	q_value: tensor([[-24.3141]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 6.3906049641905325
epoch: 9, step: 2
	action: tensor([[-101.5887, -163.8955,  -80.0476,   72.0840, -125.7156,  -37.9218,
          119.9406]], dtype=torch.float64)
	q_value: tensor([[-34.1281]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.658731168334469, distance: 1.4738208492662952 entropy 6.143527473874401
epoch: 9, step: 3
	action: tensor([[  21.9484, -157.0903,  -53.7071,  149.3459, -259.0669,  161.0096,
         -104.8687]], dtype=torch.float64)
	q_value: tensor([[-31.0328]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5901789405846776, distance: 0.7325779272080235 entropy 6.485834523039555
epoch: 9, step: 4
	action: tensor([[-190.0154, -224.4901,   51.7114, -183.9108, -228.5315,    5.8041,
          254.5902]], dtype=torch.float64)
	q_value: tensor([[-34.2786]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.9776269808765816, distance: 1.974657771514463 entropy 6.682563205931144
epoch: 9, step: 5
	action: tensor([[ -17.9436, -116.4768,  -41.6546,   96.1129, -360.7894, -372.3214,
          -62.2597]], dtype=torch.float64)
	q_value: tensor([[-35.5067]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03949991487657489, distance: 1.1667261243206775 entropy 6.651817746374388
epoch: 9, step: 6
	action: tensor([[-186.6664,  488.1732,    1.2742,   64.2628,  178.6322, -105.8103,
          263.6079]], dtype=torch.float64)
	q_value: tensor([[-31.9961]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 6.615081255391063
epoch: 9, step: 7
	action: tensor([[-300.0397,  -51.3692,   68.4372,   16.5382, -115.9803,  195.1968,
          100.4262]], dtype=torch.float64)
	q_value: tensor([[-34.1281]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.413253751291637, distance: 1.360400449392769 entropy 6.143527473874401
epoch: 9, step: 8
	action: tensor([[-304.3604, -162.9404, -157.0662,  263.5324,   33.8079,  112.6291,
          -33.3322]], dtype=torch.float64)
	q_value: tensor([[-28.5613]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6820543400423247, distance: 1.4841462675605914 entropy 6.668051403533299
epoch: 9, step: 9
	action: tensor([[ -18.5488, -214.6487,   53.2937,  100.8952,  -38.5271,   11.6977,
         -187.0882]], dtype=torch.float64)
	q_value: tensor([[-28.2929]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2803522356636581, distance: 0.9707707528672462 entropy 6.587024142783752
epoch: 9, step: 10
	action: tensor([[-197.7292,  186.5465,   61.5300,    9.3653,   62.8786,  179.8666,
          -65.4780]], dtype=torch.float64)
	q_value: tensor([[-19.3402]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 6.229863051813065
epoch: 9, step: 11
	action: tensor([[-174.0077, -198.9659,  -43.8621,  -57.1098, -180.2911,   42.9070,
          196.0357]], dtype=torch.float64)
	q_value: tensor([[-34.1281]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21878610979526347, distance: 1.2633405650691918 entropy 6.143527473874401
epoch: 9, step: 12
	action: tensor([[ -82.1924, -219.9199, -201.0233,  -52.1274,   26.4718,   18.6339,
         -129.2909]], dtype=torch.float64)
	q_value: tensor([[-30.5721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0339514965339649, distance: 1.1636082087444946 entropy 6.453675246294675
epoch: 9, step: 13
	action: tensor([[-177.0097, -147.7623, -121.1495, -262.0396, -142.6186, -120.1259,
          286.5753]], dtype=torch.float64)
	q_value: tensor([[-26.1665]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5476731424222864, distance: 0.769631582355842 entropy 6.417873306669681
epoch: 9, step: 14
	action: tensor([[-698.9450,  -36.4490,  -63.3707, -162.6901, -141.9408,   28.0320,
           58.7158]], dtype=torch.float64)
	q_value: tensor([[-39.6082]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2604211728381588, distance: 0.9841219717021547 entropy 6.720069072633723
epoch: 9, step: 15
	action: tensor([[-311.9873,  -48.8089,  -93.0189, -276.2607,   71.1877,  199.4534,
            8.5152]], dtype=torch.float64)
	q_value: tensor([[-34.5536]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.01918423576859385, distance: 1.155268793157235 entropy 6.667907499632038
epoch: 9, step: 16
	action: tensor([[-450.5301, -159.8561,  140.2181, -211.5600, -140.7282, -377.6332,
         -132.4681]], dtype=torch.float64)
	q_value: tensor([[-32.9076]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.812664719558098, distance: 1.5406906049936964 entropy 6.66598965469701
epoch: 9, step: 17
	action: tensor([[   6.4427, -190.3960,  168.5929,  268.8581, -127.3061,   72.4595,
          157.0856]], dtype=torch.float64)
	q_value: tensor([[-37.6876]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6451235552661676, distance: 1.4677630609003258 entropy 6.692534052780464
epoch: 9, step: 18
	action: tensor([[-191.7947,   10.8375,   46.5420,   10.1080, -192.6864,   -5.1131,
          144.1405]], dtype=torch.float64)
	q_value: tensor([[-21.3356]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 6.282715480036136
epoch: 9, step: 19
	action: tensor([[-289.1091,   17.9127,  -14.5066,  125.3648,   69.0144, -114.9108,
          205.4861]], dtype=torch.float64)
	q_value: tensor([[-34.1281]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 6.143527473874401
epoch: 9, step: 20
	action: tensor([[-268.5213,  -93.3418, -216.4666,   55.0700, -113.5810,   63.7663,
          -75.7954]], dtype=torch.float64)
	q_value: tensor([[-34.1281]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6522531621507279, distance: 1.4709401050489568 entropy 6.143527473874401
epoch: 9, step: 21
	action: tensor([[-194.3110,   30.1115, -252.7915,  156.6237,  -31.8890, -100.0985,
          184.9346]], dtype=torch.float64)
	q_value: tensor([[-39.8124]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3204437608458879, distance: 1.3149724308788915 entropy 6.437366296631078
epoch: 9, step: 22
	action: tensor([[ 103.4414, -394.1467,  -56.3520,  133.7056, -216.9492,   72.8036,
          151.5633]], dtype=torch.float64)
	q_value: tensor([[-32.6459]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05965121085218028, distance: 1.109688737008453 entropy 6.453532819277931
epoch: 9, step: 23
	action: tensor([[-211.7049, -165.0272,  -73.6477,  -25.8401,  121.2541,  -91.8616,
           28.9732]], dtype=torch.float64)
	q_value: tensor([[-26.2900]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5612914241435762, distance: 1.4298769997483372 entropy 6.617449604107316
epoch: 9, step: 24
	action: tensor([[-306.3053, -190.2122,  206.0615,   35.9287,  135.7891,  296.2101,
         -210.4772]], dtype=torch.float64)
	q_value: tensor([[-32.7928]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7340937120455286, distance: 1.506929639437301 entropy 6.6531045135485885
epoch: 9, step: 25
	action: tensor([[-213.9435, -271.5216,   25.4252,   35.5504, -235.8690, -164.9943,
           98.3237]], dtype=torch.float64)
	q_value: tensor([[-29.7694]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11381141732916855, distance: 1.0772581005421276 entropy 6.850276902572254
epoch: 9, step: 26
	action: tensor([[-243.1625, -288.6067, -171.2667,    3.2336, -243.5205, -114.3358,
          288.4384]], dtype=torch.float64)
	q_value: tensor([[-33.0124]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 6.704836364756096
epoch: 9, step: 27
	action: tensor([[ -80.8896, -113.6904,  -14.5275,   60.7202,   -3.9259,   53.8070,
          -10.2938]], dtype=torch.float64)
	q_value: tensor([[-34.1281]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07484337283317466, distance: 1.1006882373253342 entropy 6.143527473874401
epoch: 9, step: 28
	action: tensor([[-252.3509,  207.7004,   -9.4878,  214.3480,   52.1426,  120.8944,
           94.2979]], dtype=torch.float64)
	q_value: tensor([[-37.3322]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.38893203031880863, distance: 1.3486435818726827 entropy 6.5290999366873095
epoch: 9, step: 29
	action: tensor([[ -98.6972, -334.7179, -111.2255,    1.3704,  120.1555, -214.4333,
         -278.9304]], dtype=torch.float64)
	q_value: tensor([[-39.3888]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8856546378336929, distance: 1.5714036952322148 entropy 6.590170486387441
epoch: 9, step: 30
	action: tensor([[ 16.2298,  69.8382, -13.2623, 191.0890, 167.1875, 331.9333, 234.8202]],
       dtype=torch.float64)
	q_value: tensor([[-27.2617]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 6.392483228147624
epoch: 9, step: 31
	action: tensor([[ -88.1862,   71.6037,   23.3709,   29.5847, -103.3610, -138.3342,
            9.7821]], dtype=torch.float64)
	q_value: tensor([[-34.1281]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19219743418883306, distance: 1.028511868059839 entropy 6.143527473874401
epoch: 9, step: 32
	action: tensor([[ -92.5230, -338.0470,  -51.8693,  271.1657,   19.9451,  322.2249,
          186.2768]], dtype=torch.float64)
	q_value: tensor([[-38.3308]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5080268281802873, distance: 1.405274699056154 entropy 6.758412438550932
epoch: 9, step: 33
	action: tensor([[-167.0526,  107.5584,  222.5805,  288.3163,  100.0587,   23.5828,
          501.1806]], dtype=torch.float64)
	q_value: tensor([[-32.1312]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 6.62801806967609
epoch: 9, step: 34
	action: tensor([[-254.7880, -205.8946,  209.2642, -107.4411, -167.6573, -105.1362,
           36.0394]], dtype=torch.float64)
	q_value: tensor([[-34.1281]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.34908931231071505, distance: 1.3291593300154512 entropy 6.143527473874401
epoch: 9, step: 35
	action: tensor([[-143.8881, -208.7339,  166.0903,   31.6808, -134.8928,  -57.8849,
           48.1360]], dtype=torch.float64)
	q_value: tensor([[-44.3270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.3985458688145043, distance: 1.7722733515561975 entropy 6.719967393786287
epoch: 9, step: 36
	action: tensor([[-434.1294,  -21.5985,  292.0303,  346.9988,   -6.4596,  -72.6152,
         -149.1576]], dtype=torch.float64)
	q_value: tensor([[-41.8393]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.783830780329283, distance: 1.5283876509135461 entropy 6.8219497274291125
epoch: 9, step: 37
	action: tensor([[ -86.6289, -279.8342, -210.9338,  -44.1170,   29.8097, -159.1986,
          141.2406]], dtype=torch.float64)
	q_value: tensor([[-35.2970]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7653036575312251, distance: 1.5204299075394612 entropy 6.728447831043771
epoch: 9, step: 38
	action: tensor([[  10.4883,   14.1504,  -56.6330,  -48.1990, -294.9144,   35.8964,
           31.7121]], dtype=torch.float64)
	q_value: tensor([[-39.1034]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7512230017567219, distance: 1.514354037298789 entropy 6.79196060018088
epoch: 9, step: 39
	action: tensor([[ -93.5320,   -6.2278, -111.3767,  141.2464,  -62.9210,  -66.3181,
         -271.9106]], dtype=torch.float64)
	q_value: tensor([[-23.6702]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7588032217878744, distance: 1.5176279611974637 entropy 6.182887283029676
epoch: 9, step: 40
	action: tensor([[  63.5566, -224.7045,   38.1604,  200.2543,  202.0796,   66.7274,
         -156.7461]], dtype=torch.float64)
	q_value: tensor([[-34.9186]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.817088662745118, distance: 0.4894146955720705 entropy 6.663706172710192
epoch: 9, step: 41
	action: tensor([[-124.0216,  -91.4204,  215.7149,   -3.8560, -113.9149,  161.3701,
          133.4781]], dtype=torch.float64)
	q_value: tensor([[-25.5945]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3709350145046669, distance: 0.9076208197407458 entropy 6.314876083112119
epoch: 9, step: 42
	action: tensor([[-689.4789, -105.7152,   18.7383,  260.8246, -205.3155,  311.9552,
         -109.3325]], dtype=torch.float64)
	q_value: tensor([[-29.3765]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07123732332406352, distance: 1.1028312669399274 entropy 6.568780560072672
epoch: 9, step: 43
	action: tensor([[ 17.0765, -71.7696,  66.4435, 140.3690, 167.1578, -20.9100, 111.3098]],
       dtype=torch.float64)
	q_value: tensor([[-37.3526]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.30107598425727566, distance: 1.305293043773887 entropy 6.7275979112145246
epoch: 9, step: 44
	action: tensor([[-634.7464, -260.6874, -153.4010,  -97.0616,   39.3872, -333.8793,
         -127.9032]], dtype=torch.float64)
	q_value: tensor([[-28.5808]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1400021154747667, distance: 1.0612198986852475 entropy 6.790782430091168
epoch: 9, step: 45
	action: tensor([[-525.3947, -210.5172,   12.2060,  238.4763, -330.8447,  100.5346,
           82.3442]], dtype=torch.float64)
	q_value: tensor([[-34.4523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7330433800620142, distance: 1.5064732003969021 entropy 6.776755473736694
epoch: 9, step: 46
	action: tensor([[-460.6314, -270.1668,   -9.6523,   34.0481,  -67.4518,  221.3438,
          231.9722]], dtype=torch.float64)
	q_value: tensor([[-27.2386]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5962335465482322, distance: 1.445788979875886 entropy 6.487655039757004
epoch: 9, step: 47
	action: tensor([[-301.2410, -247.5110,  191.2328,  411.9939,  -30.8451, -279.7302,
          301.7270]], dtype=torch.float64)
	q_value: tensor([[-33.9203]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03964009769678567, distance: 1.1668047916887356 entropy 6.637061557589326
epoch: 9, step: 48
	action: tensor([[-278.4380, -219.4808, -132.4602, -289.2491,  -75.5969,   19.5781,
          -36.7113]], dtype=torch.float64)
	q_value: tensor([[-34.3977]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.7652936032442899, distance: 1.9029497389777594 entropy 6.639265775332534
epoch: 9, step: 49
	action: tensor([[-271.9882,  -38.7660,  -92.5770, -100.1976,  108.4257, -275.3277,
          254.2160]], dtype=torch.float64)
	q_value: tensor([[-24.8690]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4668669950863147, distance: 1.3859643573819396 entropy 6.362907226062682
epoch: 9, step: 50
	action: tensor([[ -79.1289,  -15.0732, -131.1181, -192.6865,  -41.2354,  486.7918,
           12.5396]], dtype=torch.float64)
	q_value: tensor([[-39.4739]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26313205010529517, distance: 0.9823166989131158 entropy 6.796440772724127
epoch: 9, step: 51
	action: tensor([[-281.2169,  -46.2400,  219.6605,  164.8868,  142.1397,  -19.6127,
         -102.4788]], dtype=torch.float64)
	q_value: tensor([[-33.6826]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11811342822569726, distance: 1.210039715743643 entropy 6.688251613766191
epoch: 9, step: 52
	action: tensor([[-277.3731, -315.4091,  102.0179,  -47.9768, -204.1571,  -25.8193,
         -185.5073]], dtype=torch.float64)
	q_value: tensor([[-30.5008]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3977997745254247, distance: 1.3529419916485588 entropy 6.51552894538103
epoch: 9, step: 53
	action: tensor([[ 213.6783, -112.5652,  262.2150,  171.6986,  -83.0526, -175.1893,
          188.1374]], dtype=torch.float64)
	q_value: tensor([[-40.3878]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19100441268135193, distance: 1.0292710783676433 entropy 6.758000492374646
epoch: 9, step: 54
	action: tensor([[-331.6948,   84.3409,   98.8640,   30.7013, -152.5705, -152.0019,
           43.4751]], dtype=torch.float64)
	q_value: tensor([[-28.0903]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 6.402404896668021
epoch: 9, step: 55
	action: tensor([[-178.8569,  -40.7925,   55.6678,  -72.3035,   51.4640,  -14.1673,
          194.2547]], dtype=torch.float64)
	q_value: tensor([[-34.1281]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 6.143527473874401
epoch: 9, step: 56
	action: tensor([[-186.8620, -193.9054,  136.4614, -127.0199,    0.4184,  -52.6049,
          -66.2844]], dtype=torch.float64)
	q_value: tensor([[-34.1281]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1590820874902763, distance: 1.0493817210379432 entropy 6.143527473874401
epoch: 9, step: 57
	action: tensor([[-343.7012, -413.2413, -157.4506,    8.8524,  -59.4575,  184.0610,
         -327.1390]], dtype=torch.float64)
	q_value: tensor([[-37.7505]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8503691804345754, distance: 1.5566317573952162 entropy 6.579432623851287
epoch: 9, step: 58
	action: tensor([[-145.9111, -332.7412,  -97.9779,  110.3660,  214.9551,   55.9188,
           65.7402]], dtype=torch.float64)
	q_value: tensor([[-31.7261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5057980795790478, distance: 0.8044682484676773 entropy 6.641966167856159
epoch: 9, step: 59
	action: tensor([[  55.2153, -213.1574,  -90.8909,  -59.2592, -102.7998,  508.8592,
          106.8746]], dtype=torch.float64)
	q_value: tensor([[-25.4419]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.583836932270817, distance: 1.4401639188808144 entropy 6.470110077144982
epoch: 9, step: 60
	action: tensor([[-404.4708, -421.8033,  111.8059,  331.9413,  -67.3542,  -13.0823,
           25.1831]], dtype=torch.float64)
	q_value: tensor([[-34.6243]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.44289049587091833, distance: 1.3745906315151855 entropy 6.826552726393918
epoch: 9, step: 61
	action: tensor([[ -29.8861,  -50.6205,    8.0245,  213.6580, -176.7333, -438.6894,
         -109.5641]], dtype=torch.float64)
	q_value: tensor([[-26.4672]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07490986775931074, distance: 1.1864316348722972 entropy 6.474083467321045
epoch: 9, step: 62
	action: tensor([[-158.2163, -343.5356, -127.9608,   36.4067,  260.3091,   -2.3941,
         -205.3661]], dtype=torch.float64)
	q_value: tensor([[-23.4383]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 6.36261377978535
epoch: 9, step: 63
	action: tensor([[-178.6032, -227.6800,  171.5261,  -36.6686,   38.7189,  -13.5188,
         -298.1670]], dtype=torch.float64)
	q_value: tensor([[-34.1281]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24393302324436483, distance: 0.9950314965018656 entropy 6.143527473874401
epoch: 9, step: 64
	action: tensor([[ 100.1571, -277.8237,  -68.9446,  -95.0118, -202.1816, -506.8698,
          762.5218]], dtype=torch.float64)
	q_value: tensor([[-39.4353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10189388154142676, distance: 1.2012311316846187 entropy 6.6703168787458775
epoch: 9, step: 65
	action: tensor([[-260.8481, -581.0874,  166.2391,   -9.9040,  -93.7566, -116.0368,
         -134.9639]], dtype=torch.float64)
	q_value: tensor([[-33.0477]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 6.639970698105427
epoch: 9, step: 66
	action: tensor([[  33.6207, -222.5411,  -55.8842, -122.0494,  345.6181,  182.2272,
          -25.3759]], dtype=torch.float64)
	q_value: tensor([[-34.1281]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 6.143527473874401
epoch: 9, step: 67
	action: tensor([[-189.8004, -141.3235, -163.0595,   78.3547,  -64.2144,   32.4991,
          -19.7099]], dtype=torch.float64)
	q_value: tensor([[-34.1281]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0617491410317648, distance: 1.64314013747656 entropy 6.143527473874401
epoch: 9, step: 68
	action: tensor([[-1.1385e+02, -2.4970e+02, -6.0798e+00,  7.4256e+01,  9.2891e+01,
          9.8901e+00, -3.5418e-02]], dtype=torch.float64)
	q_value: tensor([[-24.1414]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 6.144682071205518
epoch: 9, step: 69
	action: tensor([[-256.9923,  -27.2822,  -73.9412,  241.4660,  168.3609,  188.7223,
           72.2395]], dtype=torch.float64)
	q_value: tensor([[-34.1281]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.404164671187333, distance: 1.3560188111707427 entropy 6.143527473874401
epoch: 9, step: 70
	action: tensor([[-244.2866, -224.0011,    1.8470,   17.9585, -106.0029,   18.2366,
         -154.1522]], dtype=torch.float64)
	q_value: tensor([[-25.8599]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 6.365567857700445
epoch: 9, step: 71
	action: tensor([[ -84.0052, -140.0744,  101.6211,  -97.7094,  -12.2718,  -11.2380,
         -104.3352]], dtype=torch.float64)
	q_value: tensor([[-34.1281]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 6.143527473874401
epoch: 9, step: 72
	action: tensor([[ -85.2170, -102.3961,  -90.1372,  105.9360,  -31.2398, -128.5447,
         -168.6095]], dtype=torch.float64)
	q_value: tensor([[-34.1281]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12681878316387274, distance: 1.214741117649872 entropy 6.143527473874401
epoch: 9, step: 73
	action: tensor([[ -53.5095, -176.2511,   -2.9037, -166.4715,   -6.5819,  107.8152,
          148.6820]], dtype=torch.float64)
	q_value: tensor([[-35.5692]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17096583704218538, distance: 1.2383083620051145 entropy 6.502830607613485
epoch: 9, step: 74
	action: tensor([[-373.0417, -668.8567,  -62.1227,  258.3774, -314.8032,   70.2942,
          196.2308]], dtype=torch.float64)
	q_value: tensor([[-43.7840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.4421849803612177, distance: 1.7883230375356973 entropy 6.772472095931154
epoch: 9, step: 75
	action: tensor([[-383.0800, -206.3713,    1.2182, -208.8366,   93.2296,  168.8719,
          153.3606]], dtype=torch.float64)
	q_value: tensor([[-24.9684]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9254925144548332, distance: 1.5879163138646235 entropy 6.466592211084221
epoch: 9, step: 76
	action: tensor([[-111.3549, -236.5992,   17.7054, -471.7777,  -89.9148,  446.9185,
         -156.0939]], dtype=torch.float64)
	q_value: tensor([[-30.6519]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11404586914596115, distance: 1.2078367225620354 entropy 6.659106756257848
epoch: 9, step: 77
	action: tensor([[-153.2816,  155.1302, -187.8467,   21.3860,  321.7485,   43.3322,
          248.7582]], dtype=torch.float64)
	q_value: tensor([[-32.8774]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.589653142291739, distance: 1.442805800698599 entropy 6.599137395553186
epoch: 9, step: 78
	action: tensor([[ -78.1408,   88.4346, -165.2234,   90.4857, -268.2967,   93.7508,
          -36.2826]], dtype=torch.float64)
	q_value: tensor([[-33.0658]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38084516273870395, distance: 0.9004432118316456 entropy 6.531213886439389
epoch: 9, step: 79
	action: tensor([[-400.9243, -291.7573,   27.8202, -123.5167,  -96.3897,   35.4164,
          105.0569]], dtype=torch.float64)
	q_value: tensor([[-36.7757]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.32249336387496697, distance: 1.3159925903517058 entropy 6.467437930383121
epoch: 9, step: 80
	action: tensor([[-415.6908,   22.6662,   -7.4013, -146.1277, -147.4362,  -30.6474,
         -181.3310]], dtype=torch.float64)
	q_value: tensor([[-37.5753]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 6.722646062054207
epoch: 9, step: 81
	action: tensor([[-294.9913,  -21.8188,  186.0673,   88.3482,  312.9634,   72.3611,
           66.5414]], dtype=torch.float64)
	q_value: tensor([[-34.1281]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 6.143527473874401
epoch: 9, step: 82
	action: tensor([[-267.2300, -345.9905,  -67.6491,   68.1680,  -89.3206,   66.3970,
           64.5592]], dtype=torch.float64)
	q_value: tensor([[-34.1281]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0142143332442464, distance: 1.624087913447825 entropy 6.143527473874401
epoch: 9, step: 83
	action: tensor([[-203.2068,  -70.7979, -143.6569,   49.0061,  -46.5697,  151.1120,
           81.2085]], dtype=torch.float64)
	q_value: tensor([[-27.3575]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.280657973680491, distance: 1.7281712390752697 entropy 6.397180353224497
epoch: 9, step: 84
	action: tensor([[-284.1084, -244.1142,  195.9271,  -34.1440,   14.5356, -101.3159,
         -146.7025]], dtype=torch.float64)
	q_value: tensor([[-27.5349]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0009224230774696, distance: 1.6187203212022483 entropy 6.546816964771257
epoch: 9, step: 85
	action: tensor([[-172.3016, -107.1514,   47.2766,    5.9847, -153.2559,   69.3803,
          177.9063]], dtype=torch.float64)
	q_value: tensor([[-26.4639]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.850917863153291, distance: 1.9321865108879615 entropy 6.357261102562553
epoch: 9, step: 86
	action: tensor([[-147.2387, -591.5056, -171.8826,   84.6503, -113.8378,   -1.8375,
         -349.8828]], dtype=torch.float64)
	q_value: tensor([[-30.0565]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2048232432272048, distance: 1.6991964128142256 entropy 6.550749762865398
epoch: 9, step: 87
	action: tensor([[-106.0774, -223.1827,   -0.4667,   33.8729,  219.8452, -306.7598,
         -244.7762]], dtype=torch.float64)
	q_value: tensor([[-29.3926]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8171767864186403, distance: 1.542606949008416 entropy 6.5789297768573745
epoch: 9, step: 88
	action: tensor([[-231.9767,  -87.2428,   81.9442, -121.5710,   96.3239,  157.7513,
           88.0182]], dtype=torch.float64)
	q_value: tensor([[-26.2114]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.36911199253991533, distance: 1.3389864451515794 entropy 6.510905717348699
epoch: 9, step: 89
	action: tensor([[ -28.2970, -191.4760,  252.9185,  215.8898,   36.7878,  189.1256,
          176.7376]], dtype=torch.float64)
	q_value: tensor([[-25.4313]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1378297584304229, distance: 1.0625593755436136 entropy 6.396299124834914
epoch: 9, step: 90
	action: tensor([[-148.0776, -303.2521,   80.8260, -180.6067, -185.6145,  170.7826,
           57.3934]], dtype=torch.float64)
	q_value: tensor([[-27.9729]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1871438667360485, distance: 1.6923701975446706 entropy 6.557873867368825
epoch: 9, step: 91
	action: tensor([[-249.5018, -281.7748,  102.0166,  238.4227, -216.7828,  150.4275,
         -109.2636]], dtype=torch.float64)
	q_value: tensor([[-29.5525]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5133812615324935, distance: 1.4077672881431298 entropy 6.640851422939044
epoch: 9, step: 92
	action: tensor([[ -73.6204, -259.1776,  -45.4035,  159.3634,   63.9672,   92.2172,
          226.2594]], dtype=torch.float64)
	q_value: tensor([[-40.7875]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05705237659242135, distance: 1.1765352587323727 entropy 6.795643124342624
epoch: 9, step: 93
	action: tensor([[-2.1483e+02, -3.5625e+01,  2.6756e+01, -5.1068e+01, -3.6033e+02,
          9.3910e-02, -2.1782e+02]], dtype=torch.float64)
	q_value: tensor([[-31.1114]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4924716111033747, distance: 1.3980082450657332 entropy 6.5770109324082835
epoch: 9, step: 94
	action: tensor([[-460.8183, -197.8858,  192.4348, -386.4793, -264.1642,   69.9002,
           57.8393]], dtype=torch.float64)
	q_value: tensor([[-27.3321]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0756098698372654, distance: 1.648654137546703 entropy 6.628506616996519
epoch: 9, step: 95
	action: tensor([[-301.6359, -267.4117,   44.8889,  398.7111,  -71.8798,  -67.0092,
           -1.0213]], dtype=torch.float64)
	q_value: tensor([[-29.1848]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3831953201791034, distance: 1.3458555467163986 entropy 6.570445210269185
epoch: 9, step: 96
	action: tensor([[-205.3493,  -64.9404,  325.7313, -296.5464, -317.8678,  146.1148,
         -386.0185]], dtype=torch.float64)
	q_value: tensor([[-24.9849]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0745481802813837, distance: 1.6482324343181425 entropy 6.566811246766064
epoch: 9, step: 97
	action: tensor([[ -40.2508,   65.0087,   21.7143,  206.4567, -147.0869,   89.3103,
          190.1500]], dtype=torch.float64)
	q_value: tensor([[-28.6276]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0012552274979831, distance: 1.6188549328377408 entropy 6.557577891041286
epoch: 9, step: 98
	action: tensor([[-263.6627, -691.0802,  227.1063,   83.7439,   44.4971,  237.9198,
          -23.2643]], dtype=torch.float64)
	q_value: tensor([[-40.7369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14390853329401154, distance: 1.0588069353612954 entropy 6.823314488020505
epoch: 9, step: 99
	action: tensor([[-379.7031, -203.7263, -125.2686,   83.5237, -429.3306,  -31.4519,
          -79.3151]], dtype=torch.float64)
	q_value: tensor([[-32.5967]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6264822608485068, distance: 1.4594235796262172 entropy 6.659979613840726
epoch: 9, step: 100
	action: tensor([[-300.4861, -258.1963,    4.3116,  -19.4222,  -81.9583,   -6.1523,
          140.2517]], dtype=torch.float64)
	q_value: tensor([[-27.2339]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0674350574278892, distance: 1.6454043132527947 entropy 6.643625426954438
epoch: 9, step: 101
	action: tensor([[-328.1815,  -19.8842,  115.8589,  -16.8161, -136.3056,   83.4242,
            8.3931]], dtype=torch.float64)
	q_value: tensor([[-22.6790]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 6.242913082806767
epoch: 9, step: 102
	action: tensor([[-146.9487,   25.5309,  -69.4423,  177.2309,   77.7458,  -30.0267,
          -91.5936]], dtype=torch.float64)
	q_value: tensor([[-34.1281]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6220313020371568, distance: 1.4574253148613985 entropy 6.143527473874401
epoch: 9, step: 103
	action: tensor([[-116.3679,   94.1139,   54.2892,  -20.9287,  168.7256,   84.6396,
          113.0679]], dtype=torch.float64)
	q_value: tensor([[-30.6808]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.28374114047768595, distance: 1.2965683707838414 entropy 6.351926181490534
epoch: 9, step: 104
	action: tensor([[-247.4506,  164.6096,   66.9273,   63.2766,  -57.8223,  -87.2052,
          148.6387]], dtype=torch.float64)
	q_value: tensor([[-39.1054]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20763557361205243, distance: 1.0186363689634241 entropy 6.5667343605475486
epoch: 9, step: 105
	action: tensor([[-354.6964,  -54.8349, -134.5372,  240.9991, -332.0380,   74.6116,
          438.2383]], dtype=torch.float64)
	q_value: tensor([[-39.4746]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9015170324123691, distance: 1.577999289025737 entropy 6.580016565017149
epoch: 9, step: 106
	action: tensor([[ 266.9710,   55.0227,   -4.2596, -250.6000, -209.4324,  -62.8543,
          148.3703]], dtype=torch.float64)
	q_value: tensor([[-34.5148]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 6.599971319585578
epoch: 9, step: 107
	action: tensor([[-158.0319,  187.9598,  -73.7531,   22.7302, -117.0797, -156.2162,
         -140.1940]], dtype=torch.float64)
	q_value: tensor([[-34.1281]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21633337814483866, distance: 1.2620687275055997 entropy 6.143527473874401
epoch: 9, step: 108
	action: tensor([[-102.0820, -348.4286,  -11.5995,  211.1776,  207.0248,  -78.4735,
         -157.1548]], dtype=torch.float64)
	q_value: tensor([[-39.0498]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4229839276968326, distance: 1.3650755583531355 entropy 6.6647531359242675
epoch: 9, step: 109
	action: tensor([[ -86.1755,  219.7996,   33.5507, -146.0401,   12.7915,  -93.6933,
          -19.3890]], dtype=torch.float64)
	q_value: tensor([[-27.7288]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04502058985273916, distance: 1.1698202016930892 entropy 6.490447272836133
epoch: 9, step: 110
	action: tensor([[-217.5620, -135.8357,   74.6100,    7.4954,  -51.3879,  225.0307,
          235.8419]], dtype=torch.float64)
	q_value: tensor([[-32.2149]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.049244940606625986, distance: 1.1721822349879376 entropy 6.418189074122752
epoch: 9, step: 111
	action: tensor([[ -74.4484, -221.1671, -184.7833,  -46.3384,  -84.6869,   50.0867,
           15.3345]], dtype=torch.float64)
	q_value: tensor([[-25.2914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.6841219884505034, distance: 1.8748124039270708 entropy 6.401609645401601
epoch: 9, step: 112
	action: tensor([[-407.5516, -339.2783,   68.6965,   82.3434,  387.3857,  231.6530,
           49.6762]], dtype=torch.float64)
	q_value: tensor([[-34.8898]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9720971854838636, distance: 1.6070184020167455 entropy 6.7471618069555825
epoch: 9, step: 113
	action: tensor([[-169.1092, -105.1030,  354.2578,  -56.1047,  105.7425,  203.7603,
         -300.3631]], dtype=torch.float64)
	q_value: tensor([[-28.6205]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9493618116780118, distance: 1.5977282722553454 entropy 6.633322715214198
epoch: 9, step: 114
	action: tensor([[-270.4008,  -33.4421,   76.7331,   49.0534, -259.8751,   24.0625,
           94.7510]], dtype=torch.float64)
	q_value: tensor([[-22.7663]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 6.332895882711662
epoch: 9, step: 115
	action: tensor([[-114.1715,  -20.8660,   -1.1735,   84.7524,  105.9296,  143.8318,
         -145.1172]], dtype=torch.float64)
	q_value: tensor([[-34.1281]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9260068529756393, distance: 1.5881283822024876 entropy 6.143527473874401
epoch: 9, step: 116
	action: tensor([[  42.4905,   32.9790, -163.4762, -149.4893,  182.3378,   89.8848,
          -25.1174]], dtype=torch.float64)
	q_value: tensor([[-26.4450]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 6.302855581067692
epoch: 9, step: 117
	action: tensor([[ -93.5080, -205.3326,   34.5378,  -67.2874,  -45.3984,   63.7423,
          -45.1530]], dtype=torch.float64)
	q_value: tensor([[-34.1281]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20368397105698066, distance: 1.021173231952845 entropy 6.143527473874401
epoch: 9, step: 118
	action: tensor([[-290.9901, -140.0862,  -60.9136,   79.9197,  189.7436,  -46.8494,
          112.3436]], dtype=torch.float64)
	q_value: tensor([[-43.7390]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6334810503453114, distance: 1.462560175168164 entropy 6.674187741952901
epoch: 9, step: 119
	action: tensor([[-186.5521, -372.0678,  -10.3183,   49.1272, -233.3099,  208.6363,
          112.1552]], dtype=torch.float64)
	q_value: tensor([[-29.6518]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.4856837639392748, distance: 1.8041790315029118 entropy 6.425849185441601
epoch: 9, step: 120
	action: tensor([[ 101.9785,  -34.8399,  161.6943,  -78.6619,  314.8098, -109.7069,
           94.5251]], dtype=torch.float64)
	q_value: tensor([[-29.7190]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6800089261184511, distance: 0.6473298370751969 entropy 6.457242340261618
epoch: 9, step: 121
	action: tensor([[-127.0197, -223.1713,  227.7533, -158.9748,  -21.7860,  494.4546,
           74.3299]], dtype=torch.float64)
	q_value: tensor([[-32.1854]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 6.518268279643856
epoch: 9, step: 122
	action: tensor([[-201.9519, -289.3873,    9.9797,  222.8586,  -64.9002,  -72.7546,
           19.9608]], dtype=torch.float64)
	q_value: tensor([[-34.1281]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 6.143527473874401
epoch: 9, step: 123
	action: tensor([[-323.6923,   41.3712,  -40.3210,   81.7431,   57.0456,  -19.1318,
          166.6292]], dtype=torch.float64)
	q_value: tensor([[-34.1281]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6154205422644157, distance: 0.7096590322104998 entropy 6.143527473874401
epoch: 9, step: 124
	action: tensor([[-297.2913,  -26.0485, -317.2538,  162.9229,   42.0618,   27.7756,
          126.9340]], dtype=torch.float64)
	q_value: tensor([[-40.9330]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0453265350170713, distance: 1.6365829396696017 entropy 6.464036342704438
epoch: 9, step: 125
	action: tensor([[-247.2574, -404.8911,  102.9813,  -44.6436,   98.5439,   -6.5959,
          119.4848]], dtype=torch.float64)
	q_value: tensor([[-39.0776]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.6354109439260487, distance: 1.8577226055500058 entropy 6.641134643132253
epoch: 9, step: 126
	action: tensor([[-146.5556, -207.9922,  146.5746,  154.9188, -104.7868,   72.2981,
          -51.6634]], dtype=torch.float64)
	q_value: tensor([[-30.7655]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.873383436500258, distance: 1.5662822681549025 entropy 6.552266223368731
epoch: 9, step: 127
	action: tensor([[-211.7366, -393.9107, -307.2591,  -77.0531,   46.0141,   14.9603,
         -107.1686]], dtype=torch.float64)
	q_value: tensor([[-29.2322]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10175183635146823, distance: 1.2011537038095537 entropy 6.55935233584504
LOSS epoch 9 actor 319.0189489647291 critic 234.19565652773008
epoch: 10, step: 0
	action: tensor([[-201.7165, -175.4382, -157.7771,  108.5043,   28.3814, -364.5902,
         -115.1184]], dtype=torch.float64)
	q_value: tensor([[-25.6264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9970473863936256, distance: 1.6171521343459156 entropy 6.624047083277754
epoch: 10, step: 1
	action: tensor([[-373.5985, -233.9233, -322.6401,  190.8359, -303.1487,   72.1498,
           98.9692]], dtype=torch.float64)
	q_value: tensor([[-33.2969]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4219892868470234, distance: 1.3645983929992123 entropy 6.786952528780069
epoch: 10, step: 2
	action: tensor([[-158.1441,   35.7310,   22.7422, -300.0139, -475.6582, -560.9237,
           49.7555]], dtype=torch.float64)
	q_value: tensor([[-30.2647]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6416575433956444, distance: 1.4662160748014523 entropy 6.8245485072913175
epoch: 10, step: 3
	action: tensor([[-109.6458,  -65.2110, -203.7356, -111.6697,  333.2210,  127.1795,
         -309.8779]], dtype=torch.float64)
	q_value: tensor([[-34.3883]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.49067807540578423, distance: 1.397167984045029 entropy 6.659952698867535
epoch: 10, step: 4
	action: tensor([[-384.0772, -352.1179,  446.6145,  137.7064, -220.2575,  114.7109,
          112.3057]], dtype=torch.float64)
	q_value: tensor([[-37.8800]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2951413055512893, distance: 1.7336499355246564 entropy 6.803872905813622
epoch: 10, step: 5
	action: tensor([[-102.9263,  108.1537, -380.7454,    6.8137, -168.3069,  244.5086,
           -4.7041]], dtype=torch.float64)
	q_value: tensor([[-26.1877]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8630247769493666, distance: 1.5619459762142902 entropy 6.555461883005084
epoch: 10, step: 6
	action: tensor([[  38.5832, -291.9216,   -6.4029,  315.8854,  -65.1352, -166.1479,
           -1.9878]], dtype=torch.float64)
	q_value: tensor([[-28.7442]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3373497226489014, distance: 0.9315343742934139 entropy 6.6045918962366255
epoch: 10, step: 7
	action: tensor([[ -32.4425, -212.3501, -145.4020,  348.3699, -421.5580,  124.3382,
         -190.4064]], dtype=torch.float64)
	q_value: tensor([[-29.2167]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 6.809210112679297
epoch: 10, step: 8
	action: tensor([[-180.1031,  -95.6851,   32.4628,  114.5528, -212.5773,   55.9893,
          157.9557]], dtype=torch.float64)
	q_value: tensor([[-33.3368]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04019476754531959, distance: 1.1211100606389006 entropy 6.289442541935271
epoch: 10, step: 9
	action: tensor([[   8.5445, -273.4455,  175.5652,   50.7534,   43.6804,  156.0249,
         -177.9600]], dtype=torch.float64)
	q_value: tensor([[-23.7770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4623270894840297, distance: 0.8391039611989669 entropy 6.2697553545736096
epoch: 10, step: 10
	action: tensor([[-298.1730,  -99.8635,  166.5168,  265.5726, -294.8793,   67.4160,
          140.5766]], dtype=torch.float64)
	q_value: tensor([[-19.8497]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2989617013552821, distance: 0.9581368988413477 entropy 6.548540977609865
epoch: 10, step: 11
	action: tensor([[ -27.4708,   29.8481,  -21.3763,  101.5220,  -49.5948, -261.0609,
          286.2355]], dtype=torch.float64)
	q_value: tensor([[-30.6382]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.945263732438224, distance: 1.596047962869973 entropy 6.771472200699508
epoch: 10, step: 12
	action: tensor([[-484.1808,   11.4413,  185.4528, -385.8502,  -63.4178,  -85.9480,
         -108.3432]], dtype=torch.float64)
	q_value: tensor([[-31.1649]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.013985613874758496, distance: 1.1523186475507154 entropy 6.853092506695924
epoch: 10, step: 13
	action: tensor([[ 112.9642, -177.7949,  171.2868,  370.2485,   29.6211,  358.8797,
          -89.2178]], dtype=torch.float64)
	q_value: tensor([[-41.3894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.49021531650971006, distance: 0.8170527286902233 entropy 6.8073031992058635
epoch: 10, step: 14
	action: tensor([[-444.7620, -166.8862,   88.7065, -308.1594,  146.5700,   37.6362,
           62.2974]], dtype=torch.float64)
	q_value: tensor([[-22.5746]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12162825662264609, distance: 1.072496470068576 entropy 6.57060011294549
epoch: 10, step: 15
	action: tensor([[-252.6875, -482.4899,  162.5424,  -47.2128,  -21.3924,   31.1634,
          373.3135]], dtype=torch.float64)
	q_value: tensor([[-26.9784]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21135785739102775, distance: 1.0162409325949955 entropy 6.669876120110603
epoch: 10, step: 16
	action: tensor([[-617.6462, -350.4329,  -16.8171, -292.8303,  270.6348,   36.5341,
         -135.7601]], dtype=torch.float64)
	q_value: tensor([[-33.7800]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6759792428675595, distance: 1.4814636881640124 entropy 6.775192802247498
epoch: 10, step: 17
	action: tensor([[-121.4950, -306.3977,  234.7682,  196.9410,  -36.6903,  159.2853,
         -252.2858]], dtype=torch.float64)
	q_value: tensor([[-28.9194]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8323478146149061, distance: 1.5490329309580966 entropy 6.7838115798765575
epoch: 10, step: 18
	action: tensor([[-133.7022,   12.1736,  -44.5914,  326.8565,  197.1638,  401.5216,
         -160.7503]], dtype=torch.float64)
	q_value: tensor([[-30.2158]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.39162693616621125, distance: 1.349951315491024 entropy 6.755687545579058
epoch: 10, step: 19
	action: tensor([[-756.7917, -537.3468,  202.0887, -204.4213,  191.4661,  430.7250,
         -229.0052]], dtype=torch.float64)
	q_value: tensor([[-34.9654]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5764181649050393, distance: 1.4367870618237841 entropy 6.819340449329499
epoch: 10, step: 20
	action: tensor([[-429.9480,   58.3518,  136.0863, -140.6192,   78.3795,  -87.9360,
         -184.4519]], dtype=torch.float64)
	q_value: tensor([[-32.1355]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17558645813936857, distance: 1.0390327861753137 entropy 6.913445310103798
epoch: 10, step: 21
	action: tensor([[-329.9539,  299.0382,  210.2701,  227.4815,   -2.7758,   88.5918,
          -79.0153]], dtype=torch.float64)
	q_value: tensor([[-39.9492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5923500840647968, distance: 0.730634830427278 entropy 6.8885880225880385
epoch: 10, step: 22
	action: tensor([[-571.4966, -346.0103, -238.1735,  150.8389, -188.5186, -268.8633,
          544.6797]], dtype=torch.float64)
	q_value: tensor([[-39.2778]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2137125231384767, distance: 1.7026183285391674 entropy 6.9815830769784535
epoch: 10, step: 23
	action: tensor([[-309.2202, -266.0497,  -22.5738,  -47.1625, -102.8592,  -53.3029,
          198.3271]], dtype=torch.float64)
	q_value: tensor([[-24.2244]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7492046059463506, distance: 1.5134810910575776 entropy 6.4424967937553665
epoch: 10, step: 24
	action: tensor([[-278.0603, -370.6336,   32.5535,  134.9616, -422.2857,  133.0360,
          481.2098]], dtype=torch.float64)
	q_value: tensor([[-39.1005]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7724358908257549, distance: 1.5234982549332594 entropy 6.932712264157969
epoch: 10, step: 25
	action: tensor([[-263.6588,  184.7647,  597.3867,  128.9291, -233.6483,  315.1941,
          126.4162]], dtype=torch.float64)
	q_value: tensor([[-26.5964]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1714831726756143, distance: 1.04161532275785 entropy 6.737074627737755
epoch: 10, step: 26
	action: tensor([[ -95.8622,  -19.1276,  173.0331, -216.0899,   96.6122,  -28.2803,
         -374.8135]], dtype=torch.float64)
	q_value: tensor([[-31.4318]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.4484879503771793, distance: 1.7906292680644094 entropy 6.750855423097754
epoch: 10, step: 27
	action: tensor([[-427.2356,  -75.8381, -409.1285, -349.4605,  -29.9625,  131.1418,
          445.1739]], dtype=torch.float64)
	q_value: tensor([[-34.2373]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08492040620530683, distance: 1.0946773410634316 entropy 6.798341632174469
epoch: 10, step: 28
	action: tensor([[ -39.8951, -350.0283,   42.5445,   80.7726, -230.5128,    9.4410,
           79.2434]], dtype=torch.float64)
	q_value: tensor([[-34.9550]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 6.680930733575269
epoch: 10, step: 29
	action: tensor([[-381.7446,  -30.6259, -201.7815,  129.6029,   -5.2629,  242.9420,
           38.7094]], dtype=torch.float64)
	q_value: tensor([[-33.3368]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22553063143146135, distance: 1.0070680456479555 entropy 6.289442541935271
epoch: 10, step: 30
	action: tensor([[ -80.4413, -255.1710,   58.4390,  132.1274,  -50.3729, -282.5244,
          328.7601]], dtype=torch.float64)
	q_value: tensor([[-28.5238]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7431930155083075, distance: 1.5108781196392769 entropy 6.447816486712773
epoch: 10, step: 31
	action: tensor([[-149.7935,  -23.6292, -191.7151,   39.9286, -194.9144,  284.3412,
          -44.7680]], dtype=torch.float64)
	q_value: tensor([[-24.8825]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3358681407267131, distance: 0.9325751751129736 entropy 6.6012054104574895
epoch: 10, step: 32
	action: tensor([[-182.7553,  -53.1804,  188.0651, -220.1462,    5.2617,  389.1546,
         -230.0715]], dtype=torch.float64)
	q_value: tensor([[-27.4106]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7905378911699787, distance: 1.5312582840755822 entropy 6.694670364440496
epoch: 10, step: 33
	action: tensor([[ -61.0981, -100.7060,  105.5616,  116.2401, -110.8352,  -32.7778,
          167.1476]], dtype=torch.float64)
	q_value: tensor([[-24.3666]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.42555164137710233, distance: 1.3663066135082058 entropy 6.582587502848761
epoch: 10, step: 34
	action: tensor([[-221.9536, -473.9970, -126.2177,  452.4654,  -19.3800, -777.8899,
          208.1929]], dtype=torch.float64)
	q_value: tensor([[-31.2945]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6656351667132567, distance: 1.476884845114394 entropy 6.904041216137752
epoch: 10, step: 35
	action: tensor([[-194.4541,   57.0143, -495.4660, -185.9866,  407.9747,  519.1297,
         -125.1203]], dtype=torch.float64)
	q_value: tensor([[-30.1571]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09112843042801078, distance: 1.090957801999241 entropy 6.854063261096036
epoch: 10, step: 36
	action: tensor([[ -80.1974, -254.9411,  215.2783,   66.1602,   99.2620,   48.1891,
          109.4287]], dtype=torch.float64)
	q_value: tensor([[-21.6628]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3111693080339839, distance: 1.3103462815398026 entropy 6.34188773984445
epoch: 10, step: 37
	action: tensor([[-388.1717, -287.1171, -382.5218,  -55.8800, -471.9023,  282.2952,
          -78.4823]], dtype=torch.float64)
	q_value: tensor([[-33.1868]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4106196119692487, distance: 1.3591320445959285 entropy 6.9147673038959665
epoch: 10, step: 38
	action: tensor([[ -89.6966, -182.0768,  191.8745, -291.2856, -182.0320,  -72.3383,
          129.7823]], dtype=torch.float64)
	q_value: tensor([[-30.8789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -2.0087261337725697, distance: 1.9849429197330433 entropy 6.839624674724627
epoch: 10, step: 39
	action: tensor([[-267.9410, -182.6994,    2.9579,  221.7477,  -81.6543,   20.4812,
          482.6979]], dtype=torch.float64)
	q_value: tensor([[-29.0010]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.4101737870856796, distance: 1.77656406246082 entropy 6.6348552809450805
epoch: 10, step: 40
	action: tensor([[-217.8414, -186.3204, -102.5698,  176.9894, -214.3270, -225.9961,
          -96.9705]], dtype=torch.float64)
	q_value: tensor([[-27.4732]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0709333005870718, distance: 1.6467957938514741 entropy 6.633621091454744
epoch: 10, step: 41
	action: tensor([[-525.8497,  -68.3345,  -77.7800, -176.9286, -367.8316,  907.9338,
          -84.8494]], dtype=torch.float64)
	q_value: tensor([[-29.4636]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2408979163511975, distance: 1.7130408901516527 entropy 6.889513644930642
epoch: 10, step: 42
	action: tensor([[ -16.7889, -518.9742, -187.7190,  571.3398,  283.0389,  168.8024,
          178.0412]], dtype=torch.float64)
	q_value: tensor([[-39.9411]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 7.0569160947430545
epoch: 10, step: 43
	action: tensor([[  23.8879, -384.6334,  344.1132, -184.3543,   63.0331,  169.7613,
           53.4026]], dtype=torch.float64)
	q_value: tensor([[-33.3368]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 6.289442541935271
epoch: 10, step: 44
	action: tensor([[-128.1912, -104.2810,   -7.5955,  -10.6065,  -50.9960, -125.4520,
           60.5542]], dtype=torch.float64)
	q_value: tensor([[-33.3368]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 6.289442541935271
epoch: 10, step: 45
	action: tensor([[-181.8138,  -83.4642,  171.6942,    0.6870,   18.3515,   95.6866,
          225.9899]], dtype=torch.float64)
	q_value: tensor([[-33.3368]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7469890725442949, distance: 1.5125223037469333 entropy 6.289442541935271
epoch: 10, step: 46
	action: tensor([[-484.2311, -109.2248,   97.2524,  -84.8388, -104.1032,  174.0167,
         -193.4333]], dtype=torch.float64)
	q_value: tensor([[-34.8676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.010493216152748897, distance: 1.1503325117791103 entropy 6.642440808654984
epoch: 10, step: 47
	action: tensor([[-292.0864, -203.8130,  -18.6423,  164.3953, -126.1518,  297.4737,
         -263.2869]], dtype=torch.float64)
	q_value: tensor([[-37.6722]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4919934423967194, distance: 1.397784275195404 entropy 6.928080938128194
epoch: 10, step: 48
	action: tensor([[-235.3099,   53.1313, -164.2289,  212.7378,   55.6535,   -4.2744,
           10.7893]], dtype=torch.float64)
	q_value: tensor([[-24.2814]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.19672351471320093, distance: 1.2518537816473667 entropy 6.577357563819248
epoch: 10, step: 49
	action: tensor([[-104.3587, -227.3045,   15.2863,  -36.6508,  -85.2591,  288.7227,
         -143.0558]], dtype=torch.float64)
	q_value: tensor([[-29.3393]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0473945126481463, distance: 1.6374100843358559 entropy 6.5958800581044335
epoch: 10, step: 50
	action: tensor([[-115.8447, -142.7655, -267.3261,  317.3409,   -5.8704,  -10.4328,
          195.5242]], dtype=torch.float64)
	q_value: tensor([[-42.0654]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2318392184859569, distance: 1.002958026964103 entropy 6.983510348131787
epoch: 10, step: 51
	action: tensor([[-253.5841,  -39.3073,   84.0987,  101.0315,  136.8278,  179.9701,
         -138.6834]], dtype=torch.float64)
	q_value: tensor([[-28.5560]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1137450591944702, distance: 1.2076736440724245 entropy 6.743196286110548
epoch: 10, step: 52
	action: tensor([[-585.5863,   18.1410,  254.8761,   25.4701, -401.7956, -215.6940,
          156.3308]], dtype=torch.float64)
	q_value: tensor([[-30.3535]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6281413854538427, distance: 1.4601677464866287 entropy 6.848731163200953
epoch: 10, step: 53
	action: tensor([[-215.3955,  -40.9010,  115.1051,  275.6083,  295.3606, -374.7493,
          206.4927]], dtype=torch.float64)
	q_value: tensor([[-27.1829]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3025202145286223, distance: 1.3060172985058744 entropy 6.700563228493574
epoch: 10, step: 54
	action: tensor([[ -79.7372, -255.7758,  -71.5523,  140.9971,   14.9629, -304.7426,
           76.5503]], dtype=torch.float64)
	q_value: tensor([[-30.5653]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19186248150593266, distance: 1.0287250804955521 entropy 6.763994327356386
epoch: 10, step: 55
	action: tensor([[-154.7992,  -22.8553,  307.6055, -351.3728,  -26.8536,   80.8209,
          279.4302]], dtype=torch.float64)
	q_value: tensor([[-35.5504]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 6.922132938555633
epoch: 10, step: 56
	action: tensor([[  51.3190,  -98.4636,  -40.5022,   25.8184, -188.7696,   -4.3593,
          135.6783]], dtype=torch.float64)
	q_value: tensor([[-33.3368]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 6.289442541935271
epoch: 10, step: 57
	action: tensor([[-161.4916,  -80.3206, -103.9190,   42.0325,  -32.4173,  -83.9124,
          115.4330]], dtype=torch.float64)
	q_value: tensor([[-33.3368]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 6.289442541935271
epoch: 10, step: 58
	action: tensor([[-183.9021, -192.9624,  146.5783,  271.6682, -151.8403,   -5.5880,
          -26.1501]], dtype=torch.float64)
	q_value: tensor([[-33.3368]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9461676697163817, distance: 1.5964187505586467 entropy 6.289442541935271
epoch: 10, step: 59
	action: tensor([[-412.5445, -154.6784, -227.5375,  261.4838,   62.3781, -113.0048,
           54.3198]], dtype=torch.float64)
	q_value: tensor([[-29.3401]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9797718623892899, distance: 1.610142327953632 entropy 6.669590959009092
epoch: 10, step: 60
	action: tensor([[-201.5603, -175.3731, -117.5542,  322.7366, -162.3624,  117.0595,
          188.6819]], dtype=torch.float64)
	q_value: tensor([[-31.4007]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12092528858727691, distance: 1.0729255480519564 entropy 6.856226884211059
epoch: 10, step: 61
	action: tensor([[ 232.7361, -282.5911,  234.1984,  -66.8106, -287.3395,  213.8084,
          -29.5405]], dtype=torch.float64)
	q_value: tensor([[-27.9419]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06313720294232605, distance: 1.1799167088367264 entropy 6.732205936446809
epoch: 10, step: 62
	action: tensor([[ 232.2244,  -84.3862,  -96.3106, -129.3124,  110.1056, -267.2216,
          222.0217]], dtype=torch.float64)
	q_value: tensor([[-28.2184]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4559281670134925, distance: 1.38078692944916 entropy 6.637171362911703
epoch: 10, step: 63
	action: tensor([[-249.5002, -169.2876,  -41.9335, -105.9240, -252.4981, -328.5681,
         -164.4106]], dtype=torch.float64)
	q_value: tensor([[-30.4324]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.38870921856400065, distance: 1.3485354031825982 entropy 6.982338836981922
epoch: 10, step: 64
	action: tensor([[ -61.2098, -335.0339,   56.3598, -124.4992,  172.4379,   40.5402,
         -105.6896]], dtype=torch.float64)
	q_value: tensor([[-29.7187]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.00011510757420385431, distance: 1.14427839075904 entropy 6.671571393843572
epoch: 10, step: 65
	action: tensor([[-237.6444, -121.3156,   51.2140,  289.9081, -211.3079,  146.9337,
          -58.7031]], dtype=torch.float64)
	q_value: tensor([[-31.4810]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2105399521421556, distance: 1.016767770657544 entropy 6.620576091091929
epoch: 10, step: 66
	action: tensor([[-394.0030,   36.1265, -109.1928,   27.0955,   56.1637,  440.8896,
          300.2354]], dtype=torch.float64)
	q_value: tensor([[-26.3646]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2586227623405154, distance: 0.9853177743676446 entropy 6.739576156485943
epoch: 10, step: 67
	action: tensor([[-577.4709,  295.4869, -289.8412,  344.9206,  306.9726,  -36.2056,
           85.6206]], dtype=torch.float64)
	q_value: tensor([[-28.1225]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4206547052871492, distance: 1.363957883476669 entropy 6.778454553536835
epoch: 10, step: 68
	action: tensor([[-152.0302, -326.6041, -168.9367,  166.9790, -355.6053,  375.0005,
          189.9343]], dtype=torch.float64)
	q_value: tensor([[-33.0364]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.44654476402658205, distance: 1.3763301766898772 entropy 6.827997893051119
epoch: 10, step: 69
	action: tensor([[  71.0120,   48.0406, -134.4304,  -82.9446, -192.5764,  -16.3543,
          -72.8089]], dtype=torch.float64)
	q_value: tensor([[-35.1658]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 6.835454174783043
epoch: 10, step: 70
	action: tensor([[-108.0222, -173.5020,   43.4954,   56.4939,   -9.2626,   -4.7291,
         -175.1607]], dtype=torch.float64)
	q_value: tensor([[-33.3368]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 6.289442541935271
epoch: 10, step: 71
	action: tensor([[ -82.6911, -375.9418,  183.8722,  110.9704,  -95.7566,  370.3958,
          259.9052]], dtype=torch.float64)
	q_value: tensor([[-33.3368]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12857046980625753, distance: 1.2156849337751814 entropy 6.289442541935271
epoch: 10, step: 72
	action: tensor([[-378.7024, -211.5862,  186.5662,  -56.2839,  -53.9823,   80.6073,
         -173.0804]], dtype=torch.float64)
	q_value: tensor([[-29.7947]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12997223948340753, distance: 1.2164396856904132 entropy 6.781716238387838
epoch: 10, step: 73
	action: tensor([[-593.6551, -206.4218, -337.7305, -137.2360, -130.7806,   84.1620,
          371.5289]], dtype=torch.float64)
	q_value: tensor([[-38.2498]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7700216039863257, distance: 1.5224603005585535 entropy 6.947372254141809
epoch: 10, step: 74
	action: tensor([[-244.5152, -104.2978,  -58.9660, -166.8244,   41.0896,  180.4435,
           28.2727]], dtype=torch.float64)
	q_value: tensor([[-30.4313]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18978483957630743, distance: 1.0300466070700043 entropy 6.827821655384027
epoch: 10, step: 75
	action: tensor([[-79.1816, -98.9290, 277.4413, -17.1733, 422.9425, 189.1661, 477.0179]],
       dtype=torch.float64)
	q_value: tensor([[-44.0716]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 7.085360284331898
epoch: 10, step: 76
	action: tensor([[-344.1760, -219.5282,    3.7708,   99.1324,   19.9432,   91.3354,
         -155.6745]], dtype=torch.float64)
	q_value: tensor([[-33.3368]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9857277593328115, distance: 1.6125624654248458 entropy 6.289442541935271
epoch: 10, step: 77
	action: tensor([[-527.1002, -136.0258,  -88.0796,  203.9733,  -47.8692, -310.0588,
         -172.7589]], dtype=torch.float64)
	q_value: tensor([[-34.5775]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4436094369198367, distance: 1.3749330436946008 entropy 6.709565070442426
epoch: 10, step: 78
	action: tensor([[-706.4232, -380.5199,   92.2264,  176.4682,  242.9061,  -85.6114,
          258.1157]], dtype=torch.float64)
	q_value: tensor([[-33.1790]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.35676594674766093, distance: 1.3329355797819598 entropy 6.775435092267509
epoch: 10, step: 79
	action: tensor([[-111.8538,   44.6910, -316.5576, -215.0499, -236.2634, -381.2758,
          111.3107]], dtype=torch.float64)
	q_value: tensor([[-33.4948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9225016443007019, distance: 1.5866825782974432 entropy 6.993441628477997
epoch: 10, step: 80
	action: tensor([[-178.1378, -620.2936,  -55.4171,  -16.2585,   66.7663,  201.5364,
          231.6434]], dtype=torch.float64)
	q_value: tensor([[-42.3264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3341655080618826, distance: 1.3217872174217884 entropy 6.972415568616782
epoch: 10, step: 81
	action: tensor([[-285.7986, -144.7234,  125.8696,   88.7639, -125.5864,  -14.0420,
          149.3721]], dtype=torch.float64)
	q_value: tensor([[-35.0788]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04218552256349195, distance: 1.1682323040730074 entropy 6.77032283621799
epoch: 10, step: 82
	action: tensor([[ -38.6795, -129.7246,   19.4906,   88.5654,   62.6437,  201.9047,
          -64.7384]], dtype=torch.float64)
	q_value: tensor([[-28.8376]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.4898495562413343, distance: 1.805690223089814 entropy 6.475790666201328
epoch: 10, step: 83
	action: tensor([[-154.5988, -125.0824,  292.7229,  332.2069,  -97.6996, -257.9961,
          -45.2626]], dtype=torch.float64)
	q_value: tensor([[-22.3368]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17605827397596996, distance: 1.2409980929204822 entropy 6.421006429403976
epoch: 10, step: 84
	action: tensor([[-456.7288, -390.8721,  241.5286,   98.9760, -170.8724,  -33.0220,
          106.7961]], dtype=torch.float64)
	q_value: tensor([[-35.1483]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21146342665318918, distance: 1.0161729122660124 entropy 6.801241095865355
epoch: 10, step: 85
	action: tensor([[-339.1692, -449.6530,  -50.9321,  280.3322,   77.0527,  181.9991,
          407.9832]], dtype=torch.float64)
	q_value: tensor([[-29.5014]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7797644121477443, distance: 1.526644623239793 entropy 6.841164011967794
epoch: 10, step: 86
	action: tensor([[-334.2175, -109.8907, -102.4298,  -82.8900,  -57.5688,  262.6911,
          -72.5809]], dtype=torch.float64)
	q_value: tensor([[-33.1019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8164938799500074, distance: 1.5423170610722547 entropy 6.870527818259327
epoch: 10, step: 87
	action: tensor([[-261.9137, -485.9994,  -91.6505, -139.7882,    9.8018,  180.2078,
           19.9501]], dtype=torch.float64)
	q_value: tensor([[-32.2108]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.75905381436703, distance: 1.5177360724275406 entropy 6.967946363806228
epoch: 10, step: 88
	action: tensor([[-548.8452,  127.4666,    1.4768,   29.6993,   49.6753,  184.4826,
          -95.3296]], dtype=torch.float64)
	q_value: tensor([[-34.8785]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05743840736927197, distance: 1.1109936146559538 entropy 6.96462674259947
epoch: 10, step: 89
	action: tensor([[-207.3869, -654.7588,  137.3974,  539.3063,  -59.6990,   33.5475,
          -67.7795]], dtype=torch.float64)
	q_value: tensor([[-31.0016]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7993670103202799, distance: 1.535028948899746 entropy 6.639023093209429
epoch: 10, step: 90
	action: tensor([[ -48.9880, -517.0123,  286.5260,  912.7579, -151.7056, -117.3883,
           58.2803]], dtype=torch.float64)
	q_value: tensor([[-33.9525]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9859595386362918, distance: 1.6126565739191787 entropy 7.011615035359472
epoch: 10, step: 91
	action: tensor([[ -40.0397, -235.8577, -218.8962,   45.7460, -419.3311,  280.6421,
          400.2039]], dtype=torch.float64)
	q_value: tensor([[-32.7571]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9804273334753908, distance: 1.6104088521977356 entropy 6.9325701344953305
epoch: 10, step: 92
	action: tensor([[ -34.3552, -102.4654,   -7.5445,  115.5644,  489.2597,  -93.7274,
          -85.0381]], dtype=torch.float64)
	q_value: tensor([[-25.1110]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0924070712929863, distance: 1.6553116926589804 entropy 6.705142695036884
epoch: 10, step: 93
	action: tensor([[-295.1262, -803.7445,  157.9903, -355.8044, -482.9850, -284.4703,
           -3.0100]], dtype=torch.float64)
	q_value: tensor([[-31.4410]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9755308509746821, distance: 1.6084168026368721 entropy 6.887290792762222
epoch: 10, step: 94
	action: tensor([[-222.4484,  -49.3621,  -45.2800,  -78.3827,   -3.0458, -264.0600,
          101.6353]], dtype=torch.float64)
	q_value: tensor([[-30.9418]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2265039605109194, distance: 1.7075303405768396 entropy 6.6603662451591035
epoch: 10, step: 95
	action: tensor([[-265.4216, -503.6561, -178.0634,  -28.7921,    0.7560, -320.0736,
           18.6196]], dtype=torch.float64)
	q_value: tensor([[-39.3885]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15954952926953514, distance: 1.049090020125417 entropy 6.961854045512175
epoch: 10, step: 96
	action: tensor([[-304.6095, -416.3378, -204.5978,  -16.9599,   29.0899,  296.8248,
          175.9844]], dtype=torch.float64)
	q_value: tensor([[-37.2150]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 6.803767947306021
epoch: 10, step: 97
	action: tensor([[-116.2426, -149.3819, -146.8246,   20.5129,   24.3048, -141.3335,
         -218.6771]], dtype=torch.float64)
	q_value: tensor([[-33.3368]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.34258902410242187, distance: 1.3259533335060112 entropy 6.289442541935271
epoch: 10, step: 98
	action: tensor([[ 152.3372, -189.9324,  375.4545,   16.6586, -251.8372,   16.6497,
         -169.8366]], dtype=torch.float64)
	q_value: tensor([[-32.6708]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 6.656268835205196
epoch: 10, step: 99
	action: tensor([[ -22.2330, -221.3047,  157.0974,  230.9370,    6.4674, -100.4496,
          141.8949]], dtype=torch.float64)
	q_value: tensor([[-33.3368]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.3716763124509064, distance: 1.7623185040860436 entropy 6.289442541935271
epoch: 10, step: 100
	action: tensor([[-141.2735, -457.2306, -132.7540, -246.9471,  179.2088, -103.0012,
         -146.7795]], dtype=torch.float64)
	q_value: tensor([[-26.7984]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6985384447184084, distance: 1.491400841639143 entropy 6.495700514215031
epoch: 10, step: 101
	action: tensor([[-366.2441, -129.8826, -101.7881,  344.3108,  -10.2876, -118.7583,
          539.0715]], dtype=torch.float64)
	q_value: tensor([[-37.7282]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5427119735726647, distance: 1.421343731200255 entropy 6.842331395217788
epoch: 10, step: 102
	action: tensor([[-351.5618,   71.3731,  265.1928,  -45.8204, -152.2214,   80.7188,
          122.6643]], dtype=torch.float64)
	q_value: tensor([[-31.4895]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.42266551531027696, distance: 1.3649228225155217 entropy 6.756501720183571
epoch: 10, step: 103
	action: tensor([[-234.7538,   -5.3310,  329.0645,  192.4616, -133.3007,  129.3088,
           93.8272]], dtype=torch.float64)
	q_value: tensor([[-35.0891]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2656758493433521, distance: 0.9806196670427265 entropy 6.707108428474241
epoch: 10, step: 104
	action: tensor([[-284.4585, -227.4730,  315.0994,  -12.8769, -249.1169,  129.3286,
         -428.4415]], dtype=torch.float64)
	q_value: tensor([[-27.5701]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9651918272588924, distance: 1.6042024229108391 entropy 6.664278307660956
epoch: 10, step: 105
	action: tensor([[ -61.4936,   11.9970, -193.4399, -421.9741,  -88.2326,  445.7550,
         -103.6345]], dtype=torch.float64)
	q_value: tensor([[-26.9353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2982014243597053, distance: 1.7348052908809042 entropy 6.709723576640846
epoch: 10, step: 106
	action: tensor([[-522.2879,    8.1074,   31.4732,  104.7221,   42.0294, -256.3347,
         -400.9021]], dtype=torch.float64)
	q_value: tensor([[-34.8931]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5729401995385976, distance: 1.4352012342176437 entropy 6.8031334262129
epoch: 10, step: 107
	action: tensor([[ -24.0046,   74.7162,  -53.7214, -168.6398,  -52.8582, -179.5954,
          259.7249]], dtype=torch.float64)
	q_value: tensor([[-30.8146]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4519940878604012, distance: 1.3789201479299167 entropy 6.560124796830354
epoch: 10, step: 108
	action: tensor([[ -69.4663, -263.2970,   41.7262,   33.6918,   56.8567, -180.9595,
         -174.1309]], dtype=torch.float64)
	q_value: tensor([[-38.0364]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9240390260196987, distance: 1.5873168688638597 entropy 6.8289159586767525
epoch: 10, step: 109
	action: tensor([[ -43.8994, -167.3776,   28.5282,  137.6173, -159.3654,  -29.3707,
          476.5696]], dtype=torch.float64)
	q_value: tensor([[-33.3392]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3242607753980635, distance: 1.3168716586153708 entropy 6.776754801032469
epoch: 10, step: 110
	action: tensor([[-490.2445, -285.8728,  476.8191, -387.6432, -121.8474,  -37.0687,
         -106.6290]], dtype=torch.float64)
	q_value: tensor([[-33.7475]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2800327362613095, distance: 1.7279343356457737 entropy 6.9712792379549935
epoch: 10, step: 111
	action: tensor([[-146.5331, -259.8216, -205.8332, -193.1677,  -93.6984,   46.5018,
         -328.8860]], dtype=torch.float64)
	q_value: tensor([[-31.5326]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.46262638327724603, distance: 0.838870386509662 entropy 6.775092451246566
epoch: 10, step: 112
	action: tensor([[-311.0058, -585.3110,   36.6257, -200.4827,    2.6215,  147.6509,
         -329.3496]], dtype=torch.float64)
	q_value: tensor([[-30.8067]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1106500844738092, distance: 1.2059944807575895 entropy 6.618774030890627
epoch: 10, step: 113
	action: tensor([[-484.5794, -288.1478,  191.1304,  129.2730,  -25.3400,  -54.2322,
           40.8724]], dtype=torch.float64)
	q_value: tensor([[-36.1516]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.3322452811725403, distance: 1.7476071174734529 entropy 6.917870058316302
epoch: 10, step: 114
	action: tensor([[-498.6816, -106.7655, -187.0327,  306.2214,  111.4992,   92.7171,
           99.5769]], dtype=torch.float64)
	q_value: tensor([[-35.4414]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.044814889653165, distance: 1.6363782284823085 entropy 6.855727770548236
epoch: 10, step: 115
	action: tensor([[-9.1676e+01,  1.1550e+02, -8.9274e+01, -7.3083e+01, -2.4960e+02,
         -5.1584e+02, -3.7955e-01]], dtype=torch.float64)
	q_value: tensor([[-30.9389]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0024250409006608, distance: 1.6193280063138236 entropy 6.763928992202528
epoch: 10, step: 116
	action: tensor([[ 117.2355, -351.6960, -232.8736, -159.0572, -254.2449,   30.4374,
          232.7992]], dtype=torch.float64)
	q_value: tensor([[-34.2726]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1934265380582696, distance: 1.2501281619580302 entropy 6.562740534804718
epoch: 10, step: 117
	action: tensor([[-169.9729, -352.6880,  153.7378,   13.0867,  312.1830,  -52.4530,
          209.0800]], dtype=torch.float64)
	q_value: tensor([[-29.0197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2564307412293503, distance: 1.7189676246952945 entropy 6.616783063407391
epoch: 10, step: 118
	action: tensor([[-301.6008, -113.7376,  231.5170,  368.0146,  150.3194,  147.7405,
           12.7935]], dtype=torch.float64)
	q_value: tensor([[-34.4126]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.410060410558271, distance: 1.7765222764630628 entropy 6.845193969262823
epoch: 10, step: 119
	action: tensor([[-316.3553, -319.3355, -323.0178,  342.6449,   86.1160,  171.8126,
         -118.2783]], dtype=torch.float64)
	q_value: tensor([[-21.4903]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08476799267147994, distance: 1.191859670071212 entropy 6.491641413674195
epoch: 10, step: 120
	action: tensor([[-506.2474, -616.7041,  -84.5862,  160.9020, -104.5035, -164.9122,
          207.0950]], dtype=torch.float64)
	q_value: tensor([[-34.2667]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.386440914650248, distance: 0.896364996854069 entropy 6.9614008206455065
epoch: 10, step: 121
	action: tensor([[-670.5852, -466.7524,  101.9537,  531.4095,  247.3328,  590.4546,
          342.2889]], dtype=torch.float64)
	q_value: tensor([[-33.3982]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.26758874586173453, distance: 1.288385654762656 entropy 6.882325830866414
epoch: 10, step: 122
	action: tensor([[ -96.4523, -104.8925,  149.8818,   92.1275, -512.5049, -238.1283,
          -32.8738]], dtype=torch.float64)
	q_value: tensor([[-32.3720]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1002995008158869, distance: 1.2003617589156754 entropy 6.963387412510211
epoch: 10, step: 123
	action: tensor([[-336.7314,   82.9521,  -45.0790,  109.7438,  152.2928,  228.0589,
          268.9258]], dtype=torch.float64)
	q_value: tensor([[-29.9944]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.010283658455440547, distance: 1.1384450256625287 entropy 6.804268288726951
epoch: 10, step: 124
	action: tensor([[-234.0119, -130.1198,   82.8029,   74.1778, -119.4947,  349.8886,
         -117.2085]], dtype=torch.float64)
	q_value: tensor([[-30.8936]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10162818477301117, distance: 1.084637841784813 entropy 6.720372976317482
epoch: 10, step: 125
	action: tensor([[-165.2714, -157.1636,  -54.0115,  148.6262, -170.3147,  389.8533,
          745.7836]], dtype=torch.float64)
	q_value: tensor([[-28.1963]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10722560070907905, distance: 1.2041338151227963 entropy 6.840468006333177
epoch: 10, step: 126
	action: tensor([[-241.1300, -306.5192,   55.8853,  339.3565,  -83.8966, -137.6934,
          -92.1116]], dtype=torch.float64)
	q_value: tensor([[-29.6125]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10221333462552817, distance: 1.0842845476118164 entropy 6.856063793886927
epoch: 10, step: 127
	action: tensor([[-103.2310, -375.8546,  231.1230,   19.6691,   82.1544,  197.8994,
          369.1083]], dtype=torch.float64)
	q_value: tensor([[-29.3874]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.32320606597958346, distance: 1.3163471419837176 entropy 6.891937420600207
LOSS epoch 10 actor 280.28204136085145 critic 172.95090620856894
epoch: 11, step: 0
	action: tensor([[-380.2962, -153.7181,  -85.5950,  -65.4260,   56.8005,  276.1078,
          546.2440]], dtype=torch.float64)
	q_value: tensor([[-27.6549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.3273189382732933, distance: 1.7457604285554031 entropy 6.931717470975815
epoch: 11, step: 1
	action: tensor([[-526.6781, -502.9699,  -85.9473,  177.9707,  177.3801,   -8.2607,
          166.8237]], dtype=torch.float64)
	q_value: tensor([[-34.1207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.320898126907069, distance: 0.943027065463096 entropy 6.839742100356306
epoch: 11, step: 2
	action: tensor([[-448.3098,   -4.1475,  341.4733, -289.7102, -157.4967,  586.4308,
           43.4284]], dtype=torch.float64)
	q_value: tensor([[-29.3529]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 6.896504717789308
epoch: 11, step: 3
	action: tensor([[-146.1024, -393.7476, -207.4823,   50.0426,   23.2213,  139.1290,
          -61.5545]], dtype=torch.float64)
	q_value: tensor([[-31.8660]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8084909863152598, distance: 1.538915831817246 entropy 6.428776352822188
epoch: 11, step: 4
	action: tensor([[  90.8176, -362.7503,  -13.0379,  301.8884,  -62.0339,   84.6649,
          277.7026]], dtype=torch.float64)
	q_value: tensor([[-25.9317]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5995949352082339, distance: 0.7241132167271965 entropy 6.770745565334187
epoch: 11, step: 5
	action: tensor([[-389.2023, -411.4389,  240.6274,    1.5130,  128.8988,  -15.6296,
          166.4722]], dtype=torch.float64)
	q_value: tensor([[-22.8281]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3720372494267963, distance: 0.9068253137150386 entropy 6.678287468795369
epoch: 11, step: 6
	action: tensor([[  91.0712, -264.0251,  154.8639, -154.1278,  -93.9569, -351.4986,
         -565.2455]], dtype=torch.float64)
	q_value: tensor([[-31.1414]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26463341878039226, distance: 0.9813154533550827 entropy 7.116271966209662
epoch: 11, step: 7
	action: tensor([[-900.1679, -279.2162,  298.2240,  265.5547,   21.9469,  480.3571,
         -295.0672]], dtype=torch.float64)
	q_value: tensor([[-31.5320]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10009971668720041, distance: 1.2002527776270375 entropy 6.791111718015949
epoch: 11, step: 8
	action: tensor([[-614.3415, -196.7516,  304.6519,   36.6638, -284.7733,  447.3742,
          379.3740]], dtype=torch.float64)
	q_value: tensor([[-33.4763]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07863392563372162, distance: 1.0984310523589051 entropy 7.076424056161791
epoch: 11, step: 9
	action: tensor([[-374.5261, -687.6275,  216.8114,  248.5170, -583.2737,  228.7528,
         -348.7779]], dtype=torch.float64)
	q_value: tensor([[-35.0589]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.058073044757078196, distance: 1.110619529389153 entropy 7.177565194906483
epoch: 11, step: 10
	action: tensor([[-627.3852, -628.0121, -248.6358, -272.1820,  266.3443, -347.0539,
         -221.4193]], dtype=torch.float64)
	q_value: tensor([[-37.9624]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.41569956420011467, distance: 1.3615771123416918 entropy 7.188206201865975
epoch: 11, step: 11
	action: tensor([[-334.3104,  109.2849,   -3.8321, -424.0932,  197.2038,  109.7962,
         -343.1370]], dtype=torch.float64)
	q_value: tensor([[-33.6346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0678517422547078, distance: 1.1825300169949575 entropy 6.9935527041413
epoch: 11, step: 12
	action: tensor([[ 102.0720, -196.3670,   53.1483,   47.7315, -134.3533, -227.1919,
           76.9721]], dtype=torch.float64)
	q_value: tensor([[-34.4553]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 7.0848023549251975
epoch: 11, step: 13
	action: tensor([[ -62.9305, -209.4145, -118.3192, -294.2638,  108.8249,  159.9648,
           75.1341]], dtype=torch.float64)
	q_value: tensor([[-31.8660]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1664450287416045, distance: 1.0447775121290583 entropy 6.428776352822188
epoch: 11, step: 14
	action: tensor([[-577.9756,  228.0235,  -99.9504, -293.7448,   13.6018, -359.1138,
         -110.6991]], dtype=torch.float64)
	q_value: tensor([[-27.0270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17237790613080262, distance: 1.239054775807173 entropy 6.812030890940889
epoch: 11, step: 15
	action: tensor([[-1.1889e+02, -1.3024e+02, -7.0283e+00, -1.5757e+02, -1.5795e-01,
         -1.9402e+02, -2.2903e+02]], dtype=torch.float64)
	q_value: tensor([[-34.4693]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.247649601262375, distance: 0.9925828588923539 entropy 6.797814762957587
epoch: 11, step: 16
	action: tensor([[-483.6540, -197.9713,   74.0624, -261.5236, -247.8498, -431.9916,
         -243.2003]], dtype=torch.float64)
	q_value: tensor([[-40.6447]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.39214547812797607, distance: 1.3502027985534182 entropy 6.898687960488522
epoch: 11, step: 17
	action: tensor([[-949.9091, -501.4243,  192.5580,   -6.9635,  232.0280,  668.5313,
         -363.0479]], dtype=torch.float64)
	q_value: tensor([[-37.9595]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9051154915283441, distance: 1.5794916979583835 entropy 7.091343606885514
epoch: 11, step: 18
	action: tensor([[-122.0986,  109.5667, -294.1735,  217.9372, -334.1606,  165.9454,
          126.1357]], dtype=torch.float64)
	q_value: tensor([[-29.6132]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18670026755927727, distance: 1.0320054908372023 entropy 6.83338828965256
epoch: 11, step: 19
	action: tensor([[ -51.1422,  -22.6089,  -46.4218,  182.4174,  283.4392, -323.9765,
          -19.8779]], dtype=torch.float64)
	q_value: tensor([[-29.6973]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2323444978924982, distance: 1.709768459918161 entropy 6.811067603687121
epoch: 11, step: 20
	action: tensor([[-295.9766, -329.9591,  149.6116,  -25.7960, -117.5637, -167.1614,
         -325.8779]], dtype=torch.float64)
	q_value: tensor([[-28.1499]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7297121512908209, distance: 1.50502464439934 entropy 6.639896808877554
epoch: 11, step: 21
	action: tensor([[ 131.9683,  -20.0462, -277.7903,  147.1335,  208.3108,   90.4981,
          111.1867]], dtype=torch.float64)
	q_value: tensor([[-27.1215]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8660107671839092, distance: 1.563197190953879 entropy 6.759364356747821
epoch: 11, step: 22
	action: tensor([[-237.8530, -236.4660,  -44.4400, -267.8895,    0.9833,   23.2114,
           97.4838]], dtype=torch.float64)
	q_value: tensor([[-17.8585]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.899839385814099, distance: 1.5773030267088335 entropy 6.538622029748568
epoch: 11, step: 23
	action: tensor([[-687.1505,   -2.3890,  -49.8994,  325.7756,  222.5131, -168.8196,
          338.7487]], dtype=torch.float64)
	q_value: tensor([[-26.2484]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 6.879564789560854
epoch: 11, step: 24
	action: tensor([[-162.2833,  -14.7616,  124.8109, -199.5264,  -45.0922,  258.0562,
           74.6596]], dtype=torch.float64)
	q_value: tensor([[-31.8660]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3985756164985892, distance: 1.3533174115014117 entropy 6.428776352822188
epoch: 11, step: 25
	action: tensor([[-192.2894, -149.9935,   12.1600,  -45.3050,  112.5722,  115.6081,
          137.3369]], dtype=torch.float64)
	q_value: tensor([[-29.9100]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6357456829684327, distance: 1.4635736593154414 entropy 6.740872101842394
epoch: 11, step: 26
	action: tensor([[   7.4459, -115.4904,  210.0158,   75.2707,   15.2479, -316.2822,
          -19.5633]], dtype=torch.float64)
	q_value: tensor([[-24.9720]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7836300873980848, distance: 1.5283016715625284 entropy 6.641316205320037
epoch: 11, step: 27
	action: tensor([[-399.2755,  101.8214, -144.1813,   58.6658,  -54.3360, -122.0815,
           43.4789]], dtype=torch.float64)
	q_value: tensor([[-27.2199]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 6.789461572516464
epoch: 11, step: 28
	action: tensor([[   3.0281, -157.6033,  -93.6136,   -3.1661,   47.6204, -418.2414,
          -46.5852]], dtype=torch.float64)
	q_value: tensor([[-31.8660]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 6.428776352822188
epoch: 11, step: 29
	action: tensor([[-217.2188,  -97.9640,  -93.8650,  219.4935,  -72.9875,  -31.4566,
         -183.4419]], dtype=torch.float64)
	q_value: tensor([[-31.8660]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11970896563687194, distance: 1.210902765644487 entropy 6.428776352822188
epoch: 11, step: 30
	action: tensor([[-606.7790,  -81.9938,  142.6757,   27.5648,  -23.6284, -331.7418,
         -307.7562]], dtype=torch.float64)
	q_value: tensor([[-27.6714]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.631687844609198, distance: 1.4617571687694946 entropy 6.664870711725785
epoch: 11, step: 31
	action: tensor([[ 150.8202,   -2.3260,  127.5107, -237.0985,  -38.5452,  108.9960,
           25.6743]], dtype=torch.float64)
	q_value: tensor([[-29.6317]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 6.803874653119112
epoch: 11, step: 32
	action: tensor([[  15.7419, -234.8517, -260.9110, -108.4726,   88.6576,  168.9449,
          -99.2188]], dtype=torch.float64)
	q_value: tensor([[-31.8660]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 6.428776352822188
epoch: 11, step: 33
	action: tensor([[-84.3956, -35.0283,  70.6381,  30.9484,  63.5074,  12.7862, 211.0263]],
       dtype=torch.float64)
	q_value: tensor([[-31.8660]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15843125080983134, distance: 1.23166280319435 entropy 6.428776352822188
epoch: 11, step: 34
	action: tensor([[-140.2021,  -84.1643, -120.4470,   34.8068,  112.1219, -154.9142,
         -159.3994]], dtype=torch.float64)
	q_value: tensor([[-27.9634]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4314404063539692, distance: 1.369125720976421 entropy 6.54154763884742
epoch: 11, step: 35
	action: tensor([[-335.2990, -309.0674, -249.8949,  -45.7171, -179.4811,  -30.1553,
           88.5653]], dtype=torch.float64)
	q_value: tensor([[-28.6245]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8406568976392408, distance: 1.5525411308347734 entropy 6.792430196558922
epoch: 11, step: 36
	action: tensor([[-705.5808, -414.8971,  -10.5450,  -88.6966, -515.6430,   92.4961,
         -224.7348]], dtype=torch.float64)
	q_value: tensor([[-31.8844]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2006472532630084, distance: 1.697586490047677 entropy 6.9416082282125755
epoch: 11, step: 37
	action: tensor([[-520.6214,  -65.2882, -136.5393,  468.4724, -352.1496,  -13.5637,
          293.3676]], dtype=torch.float64)
	q_value: tensor([[-25.9486]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.3105743672492873, distance: 1.7394689017363065 entropy 6.8037209191326635
epoch: 11, step: 38
	action: tensor([[ 104.1124, -332.4159, -282.6925,  471.4585,   41.4294,  -76.5881,
           -5.9842]], dtype=torch.float64)
	q_value: tensor([[-28.4899]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 6.90854175840492
epoch: 11, step: 39
	action: tensor([[-135.0611,  -29.8085,  166.4245,  -99.3862,  -79.0644,  263.0929,
          -94.6397]], dtype=torch.float64)
	q_value: tensor([[-31.8660]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06963128447747524, distance: 1.1037843763109472 entropy 6.428776352822188
epoch: 11, step: 40
	action: tensor([[  33.3695, -162.8939,  -13.0980,  -45.2049,   78.1256,  138.3803,
         -148.4253]], dtype=torch.float64)
	q_value: tensor([[-39.7977]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 6.953893850457257
epoch: 11, step: 41
	action: tensor([[-112.2583,  116.2320,  -56.1517,  265.7753,  -43.2841,   82.2511,
          193.4447]], dtype=torch.float64)
	q_value: tensor([[-31.8660]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05527466397023473, distance: 1.1122680815446966 entropy 6.428776352822188
epoch: 11, step: 42
	action: tensor([[-200.3972, -159.4437,   38.8837,  -79.3267,   -6.2557,  -40.2345,
         -104.2678]], dtype=torch.float64)
	q_value: tensor([[-26.0027]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7687529061728486, distance: 1.521914576062348 entropy 6.375182875709867
epoch: 11, step: 43
	action: tensor([[-323.6414,  110.8978,  -84.1507,  230.9044, -109.5554,  -37.5631,
          -49.3584]], dtype=torch.float64)
	q_value: tensor([[-31.3528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6755670349025151, distance: 1.4812814935789294 entropy 6.732926194176946
epoch: 11, step: 44
	action: tensor([[-395.8300,   72.6192, -112.4715, -146.0045,   34.0478, -522.5846,
          268.5904]], dtype=torch.float64)
	q_value: tensor([[-34.1561]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.620417056510463, distance: 0.7050339621729524 entropy 7.024331743292185
epoch: 11, step: 45
	action: tensor([[ -11.7805, -207.6267,  -22.7372,   -7.9232,   19.4242, -220.1349,
          243.4369]], dtype=torch.float64)
	q_value: tensor([[-37.8148]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0935637651852077, distance: 1.0894952028285894 entropy 7.044605879856299
epoch: 11, step: 46
	action: tensor([[-358.8913,   64.7462, -176.8311,  262.5935, -134.7006,  -21.0356,
          298.2613]], dtype=torch.float64)
	q_value: tensor([[-25.1380]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 6.606612812618664
epoch: 11, step: 47
	action: tensor([[ -53.3728,  -76.6888,  -45.1682,   26.5902,  -91.3317, -113.3725,
          173.1211]], dtype=torch.float64)
	q_value: tensor([[-31.8660]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 6.428776352822188
epoch: 11, step: 48
	action: tensor([[-107.5666, -149.3341,   -8.6643,  185.8198,   15.8181,  -23.6821,
           66.3008]], dtype=torch.float64)
	q_value: tensor([[-31.8660]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8350515396740472, distance: 1.5501753492523407 entropy 6.428776352822188
epoch: 11, step: 49
	action: tensor([[-430.0637,  -75.6093, -625.4858, -281.9066, -384.0650, -288.2974,
          491.1466]], dtype=torch.float64)
	q_value: tensor([[-31.9037]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.30220713663591603, distance: 1.3058603298414808 entropy 6.958133573182246
epoch: 11, step: 50
	action: tensor([[-231.7570, -275.0213,  432.1528, -200.9941,   46.4465,  146.8073,
          242.0380]], dtype=torch.float64)
	q_value: tensor([[-31.9333]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35529095954943357, distance: 0.9188372077119761 entropy 6.833106178029629
epoch: 11, step: 51
	action: tensor([[-298.5085, -143.4040, -211.6946, -194.8882,   -5.1215, -451.7087,
           84.4653]], dtype=torch.float64)
	q_value: tensor([[-30.8185]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0386618451645353, distance: 1.166255708222739 entropy 6.951985185699209
epoch: 11, step: 52
	action: tensor([[-430.9934,  388.0017, -108.0785,  133.4725,  127.1720,  463.1360,
         -294.9652]], dtype=torch.float64)
	q_value: tensor([[-42.6808]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11107261707717897, distance: 1.2062238615224719 entropy 7.122656979501865
epoch: 11, step: 53
	action: tensor([[-289.9901, -475.6505,  -64.0762,  241.5305, -363.5791, -295.4743,
         -292.8215]], dtype=torch.float64)
	q_value: tensor([[-29.6323]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.716756097268354, distance: 1.4993775106756777 entropy 6.601900769464914
epoch: 11, step: 54
	action: tensor([[-254.7231, -347.3570,   20.5726,  267.8864,   93.5650,  -23.7641,
          522.9662]], dtype=torch.float64)
	q_value: tensor([[-29.4829]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4808345706170922, distance: 1.3925473320943824 entropy 6.988554670625098
epoch: 11, step: 55
	action: tensor([[-230.6991, -695.0797,   92.3871,   36.1464,  -97.1324,  108.5837,
          -47.7120]], dtype=torch.float64)
	q_value: tensor([[-24.2861]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7157946505715236, distance: 1.498957598538166 entropy 6.844622534553183
epoch: 11, step: 56
	action: tensor([[-182.3469, -554.9095, -112.5844, -153.0399, -820.7713,  499.9002,
           12.7288]], dtype=torch.float64)
	q_value: tensor([[-35.6010]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7190820905980853, distance: 1.5003929029017713 entropy 7.203313670766116
epoch: 11, step: 57
	action: tensor([[ 197.3382,  106.5395, -108.0262, -205.6601,   59.5223, -465.8303,
          400.2516]], dtype=torch.float64)
	q_value: tensor([[-26.8108]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 6.835044470124968
epoch: 11, step: 58
	action: tensor([[  73.1866, -297.8815,  -62.8732,   65.8396,  -31.1415,  -64.0385,
          204.9968]], dtype=torch.float64)
	q_value: tensor([[-31.8660]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04322048278265189, distance: 1.1688122267025798 entropy 6.428776352822188
epoch: 11, step: 59
	action: tensor([[-68.9134, -91.9916, 178.5440, 196.4732,  -5.2878,  12.3407, -73.7326]],
       dtype=torch.float64)
	q_value: tensor([[-16.2208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41704123615901556, distance: 0.8737267073570196 entropy 6.439043483952623
epoch: 11, step: 60
	action: tensor([[-31.9349,  82.0335, 358.4382, 282.6973, -23.9100, -18.3503, -92.8607]],
       dtype=torch.float64)
	q_value: tensor([[-22.9262]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 6.712749495233311
epoch: 11, step: 61
	action: tensor([[-216.6530,  -15.0622, -158.1958,   62.1341, -259.8101,  -24.6653,
          -74.2157]], dtype=torch.float64)
	q_value: tensor([[-31.8660]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.9942953009412356, distance: 1.980176980969331 entropy 6.428776352822188
epoch: 11, step: 62
	action: tensor([[  27.9021, -468.6980, -187.4693,  -99.5712,   11.9893, -158.6231,
          248.2971]], dtype=torch.float64)
	q_value: tensor([[-25.0874]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24130983644159465, distance: 1.2749606812836036 entropy 6.750564577978619
epoch: 11, step: 63
	action: tensor([[-186.1288,  185.8710,  -14.0943,   94.5214, -178.5578,  -84.8775,
          150.8633]], dtype=torch.float64)
	q_value: tensor([[-26.7748]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06190086532755834, distance: 1.1083605537143046 entropy 6.885251445739638
epoch: 11, step: 64
	action: tensor([[-676.8467,  -90.6414,   42.9056,  -72.3124,  219.3543,  324.5171,
          -23.0078]], dtype=torch.float64)
	q_value: tensor([[-34.9129]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7415786251405507, distance: 1.5101783370984356 entropy 6.902379383363511
epoch: 11, step: 65
	action: tensor([[-131.1952,  -87.8378,  122.0319,  268.3855,  144.1621,  443.8227,
         -116.1626]], dtype=torch.float64)
	q_value: tensor([[-31.8990]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5207275876613586, distance: 1.4111799765678361 entropy 6.702979438704086
epoch: 11, step: 66
	action: tensor([[-425.3655, -329.1164,   34.2889, -162.3226,  127.4149,  -90.3662,
          -32.6764]], dtype=torch.float64)
	q_value: tensor([[-36.4680]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.8029527311951425, distance: 1.9158635735216782 entropy 7.0327269283609075
epoch: 11, step: 67
	action: tensor([[  25.0356, -495.6392, -487.0562,  170.8024, -196.8732,  236.7361,
          -93.0603]], dtype=torch.float64)
	q_value: tensor([[-29.0247]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.29975903513982827, distance: 1.3046322677096145 entropy 6.863280938224384
epoch: 11, step: 68
	action: tensor([[-636.1070, -104.9077, -650.8054,  344.7716, -220.0508, -265.1872,
          365.3670]], dtype=torch.float64)
	q_value: tensor([[-30.4404]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38900618208915694, distance: 0.8944891999869374 entropy 6.9910558217492795
epoch: 11, step: 69
	action: tensor([[-251.9429,  -81.8310,  104.0851, -107.9043,  149.2773,  437.3452,
           18.2403]], dtype=torch.float64)
	q_value: tensor([[-26.3922]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2740455021838908, distance: 1.2916628318914714 entropy 6.882840383819063
epoch: 11, step: 70
	action: tensor([[ 108.0294,  481.2563, -223.5807,  -13.1595,    3.2234,  363.4675,
          -83.9698]], dtype=torch.float64)
	q_value: tensor([[-31.2662]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 7.010644246974197
epoch: 11, step: 71
	action: tensor([[ -85.5214,  -47.2019,  146.5667,  195.3448,  -16.8484, -157.1486,
          134.4896]], dtype=torch.float64)
	q_value: tensor([[-31.8660]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2940869449381751, distance: 1.3017824768351471 entropy 6.428776352822188
epoch: 11, step: 72
	action: tensor([[-935.5482,  -71.1178,  110.3380,  761.7826, -504.0204,  107.0838,
         -191.4889]], dtype=torch.float64)
	q_value: tensor([[-35.5636]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6681994830778282, distance: 1.4780212714863201 entropy 6.953524150204927
epoch: 11, step: 73
	action: tensor([[-608.0590, -326.4010,  249.2680,  -74.7212,  -63.6332,  337.4870,
         -497.2206]], dtype=torch.float64)
	q_value: tensor([[-32.0988]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12413437623242962, distance: 1.2132933232403937 entropy 7.02282381438821
epoch: 11, step: 74
	action: tensor([[ -24.6348, -104.1825, -338.3492,  330.3085, -430.7038, -564.7465,
          122.8308]], dtype=torch.float64)
	q_value: tensor([[-28.5483]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19432057571956873, distance: 1.0271593636991472 entropy 6.887669860739307
epoch: 11, step: 75
	action: tensor([[-416.8350, -295.4779,   90.1076, -233.3401, -687.1848,   67.2000,
         -237.9003]], dtype=torch.float64)
	q_value: tensor([[-29.8446]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.493144381744631, distance: 1.8068845657878065 entropy 7.118042625222229
epoch: 11, step: 76
	action: tensor([[  29.8237, -490.5423,   34.2049, -255.4543, -409.2959,   11.8540,
          -56.5621]], dtype=torch.float64)
	q_value: tensor([[-24.5326]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10188243809315534, distance: 1.2012248941229817 entropy 6.755420033067069
epoch: 11, step: 77
	action: tensor([[ 163.1885, -251.2998,  212.3908, -266.8527,  187.5264, -229.6553,
          302.3871]], dtype=torch.float64)
	q_value: tensor([[-30.1924]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42665589839598606, distance: 0.8664916216520027 entropy 6.854206893901801
epoch: 11, step: 78
	action: tensor([[-151.2303, -350.0896,   49.5447,  -18.7889, -150.1576,   30.6818,
          468.8740]], dtype=torch.float64)
	q_value: tensor([[-29.7755]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17435757973759536, distance: 1.0398068937196177 entropy 6.862781376743941
epoch: 11, step: 79
	action: tensor([[-109.4698, -280.1015,  258.6202,   60.7095,   41.8961, -173.5514,
          365.0133]], dtype=torch.float64)
	q_value: tensor([[-31.0998]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.25706907510361665, distance: 1.2830283849043935 entropy 6.85142603130408
epoch: 11, step: 80
	action: tensor([[-141.5071, -278.2840,   -3.4898, -203.4611,  -91.4181,   88.9016,
           79.8178]], dtype=torch.float64)
	q_value: tensor([[-31.4523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.36321721831039966, distance: 1.3361008020795166 entropy 6.966829368410722
epoch: 11, step: 81
	action: tensor([[  23.7702,  -51.7535, -100.8871, -149.8363, -493.1668,   85.4614,
          163.1215]], dtype=torch.float64)
	q_value: tensor([[-35.3400]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19432255504731133, distance: 1.0271581019775418 entropy 7.112520805052286
epoch: 11, step: 82
	action: tensor([[ 136.3909, -245.8022, -386.2224,  -14.7104,  165.0501, -179.1464,
         -293.8244]], dtype=torch.float64)
	q_value: tensor([[-37.7941]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.797532555205826, distance: 0.5149135188998244 entropy 7.06243333085982
epoch: 11, step: 83
	action: tensor([[-117.0224, -339.1338,  203.9890,   12.4614,  109.7441,  227.1865,
          370.6824]], dtype=torch.float64)
	q_value: tensor([[-31.5529]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5850669247913325, distance: 1.440723019074356 entropy 6.893935228961387
epoch: 11, step: 84
	action: tensor([[-168.2739, -540.7700, -444.7889,  139.6625, -174.1230,  231.6674,
          169.6362]], dtype=torch.float64)
	q_value: tensor([[-31.4134]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5832095270709452, distance: 1.4398786446164302 entropy 7.029106913280935
epoch: 11, step: 85
	action: tensor([[-365.9450, -551.8890, -204.1564,   54.1283,   27.3298,  -86.7351,
          -38.2730]], dtype=torch.float64)
	q_value: tensor([[-23.4440]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41898193598469835, distance: 0.8722711542826265 entropy 6.749964458464613
epoch: 11, step: 86
	action: tensor([[ -11.6765,  232.1023,  352.8505,  302.6832, -209.2152, -124.9046,
          150.3816]], dtype=torch.float64)
	q_value: tensor([[-30.6749]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 6.937701944485606
epoch: 11, step: 87
	action: tensor([[ -30.1154, -154.0032,   48.3034,  191.2207,  -41.6549,   60.2374,
          105.5648]], dtype=torch.float64)
	q_value: tensor([[-31.8660]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6225525451741176, distance: 1.457659469381485 entropy 6.428776352822188
epoch: 11, step: 88
	action: tensor([[ -90.3062, -286.4528,  -98.2969,  149.5789, -118.1439,   57.7344,
         -130.3927]], dtype=torch.float64)
	q_value: tensor([[-32.6388]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1547627958312796, distance: 1.0520732919111466 entropy 6.892547482787522
epoch: 11, step: 89
	action: tensor([[ -63.8010,  161.4614,   47.3090,  277.7526,   67.7280,   44.5955,
         -175.1660]], dtype=torch.float64)
	q_value: tensor([[-28.1714]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09427462063729841, distance: 1.0890679110476904 entropy 6.949280165867287
epoch: 11, step: 90
	action: tensor([[-402.0338,   63.5801, -153.4006,  -68.2447,    1.6425,  820.2880,
         -126.3147]], dtype=torch.float64)
	q_value: tensor([[-32.1038]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.36754535992020143, distance: 1.3382201454329874 entropy 6.974838353943864
epoch: 11, step: 91
	action: tensor([[  -5.9209, -266.9810,  133.8164, -266.5904, -295.1209, -256.7829,
         -218.1364]], dtype=torch.float64)
	q_value: tensor([[-33.9305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09851923213719105, distance: 1.199390281815071 entropy 6.820085986712887
epoch: 11, step: 92
	action: tensor([[-409.1090,   83.8202,   79.7442, -150.7570,  300.6430,  205.5768,
           39.0798]], dtype=torch.float64)
	q_value: tensor([[-23.1529]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0524650495736827, distance: 1.1139207970338874 entropy 6.742341369152174
epoch: 11, step: 93
	action: tensor([[-655.3136, -149.4904, -305.7755, -478.4865,  236.4540, -616.0050,
          176.5383]], dtype=torch.float64)
	q_value: tensor([[-32.2510]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9895432790662564, distance: 1.6141109685177322 entropy 6.961601588855214
epoch: 11, step: 94
	action: tensor([[-512.8014, -161.6119,  109.9718,  106.5098,  118.6747,  159.0712,
          384.4210]], dtype=torch.float64)
	q_value: tensor([[-33.1105]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0862950003525396, distance: 1.6528922825347496 entropy 6.9056886039658005
epoch: 11, step: 95
	action: tensor([[-330.9968,  -60.8988, -220.4866,  149.4656,  261.4580,  291.8506,
         -377.4054]], dtype=torch.float64)
	q_value: tensor([[-34.0271]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.238406667785781, distance: 1.2734688729808894 entropy 6.951035654419205
epoch: 11, step: 96
	action: tensor([[ 231.7603, -140.1245, -174.0801, -261.4389,  -96.7458,  162.0710,
           79.3356]], dtype=torch.float64)
	q_value: tensor([[-30.0768]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38185497756427345, distance: 0.8997086201735699 entropy 6.889642169030978
epoch: 11, step: 97
	action: tensor([[-385.1493,  182.2853,  120.4027, -298.0250,   53.7510,  318.9180,
          347.0488]], dtype=torch.float64)
	q_value: tensor([[-36.0030]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23598124150639554, distance: 1.0002503317888414 entropy 7.136670230893216
epoch: 11, step: 98
	action: tensor([[-799.5213, -128.1053,   73.5069,  198.3529,  -77.5025,   68.9534,
          472.0082]], dtype=torch.float64)
	q_value: tensor([[-38.3255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.30085539087318947, distance: 1.3051823848965196 entropy 7.0899280516016825
epoch: 11, step: 99
	action: tensor([[-345.5078, -303.4000,  -84.8568,  -77.0980,  -30.5367,  149.1152,
          381.8900]], dtype=torch.float64)
	q_value: tensor([[-35.1608]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.049515312975375, distance: 1.6382579231391527 entropy 6.935309119513534
epoch: 11, step: 100
	action: tensor([[-298.7750, -168.6259, -157.7617,   -2.7222,   16.3263,   -3.8247,
           -8.3129]], dtype=torch.float64)
	q_value: tensor([[-29.9793]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 6.953429110285463
epoch: 11, step: 101
	action: tensor([[-406.1101,   57.9552, -202.2732,   95.2004,   12.7659,  105.1643,
          -97.4495]], dtype=torch.float64)
	q_value: tensor([[-31.8660]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.049294626340540715, distance: 1.1722099883003003 entropy 6.428776352822188
epoch: 11, step: 102
	action: tensor([[-276.9431, -374.9748,   56.9498, -196.4772, -448.2760, -149.4288,
            8.2617]], dtype=torch.float64)
	q_value: tensor([[-29.9893]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5561541120272813, distance: 1.4275226100032763 entropy 6.503816411682702
epoch: 11, step: 103
	action: tensor([[-223.2539, -433.7129,   46.6163,   29.0542, -295.8986,    1.5942,
          558.6252]], dtype=torch.float64)
	q_value: tensor([[-33.7768]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2497472364293092, distance: 1.7164199621121012 entropy 6.943218794034286
epoch: 11, step: 104
	action: tensor([[ -84.3082,   46.9331, -142.3500,  -16.0535, -325.0555,   65.0739,
           49.1143]], dtype=torch.float64)
	q_value: tensor([[-25.5875]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5787264824582166, distance: 1.4378386062881519 entropy 6.721412032235916
epoch: 11, step: 105
	action: tensor([[-367.8929, -248.1962,   57.2398,  118.8748,  227.4984,  287.9104,
          172.4346]], dtype=torch.float64)
	q_value: tensor([[-32.0310]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.33999868032675984, distance: 1.324673592204772 entropy 6.872410935799349
epoch: 11, step: 106
	action: tensor([[  17.0458, -215.3725, -112.4516,  252.7074,  -16.8799, -332.0333,
          128.2323]], dtype=torch.float64)
	q_value: tensor([[-27.8562]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8143149534232161, distance: 1.5413917609715633 entropy 6.903813027753658
epoch: 11, step: 107
	action: tensor([[  98.3549, -486.4533, -265.3620,  213.7381,   22.6392, -147.0046,
         -220.5794]], dtype=torch.float64)
	q_value: tensor([[-28.4378]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4010221018235859, distance: 0.8856499287882567 entropy 6.823900521593564
epoch: 11, step: 108
	action: tensor([[-264.6460,  -84.2476, -330.1742, -321.6224,   51.8284, -152.3522,
          328.0514]], dtype=torch.float64)
	q_value: tensor([[-27.0954]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5513280283588939, distance: 1.4253073109974492 entropy 7.039880934128509
epoch: 11, step: 109
	action: tensor([[-126.8884, -119.2406, -888.3302, -180.6698, -149.3088,   -5.9918,
          -94.5266]], dtype=torch.float64)
	q_value: tensor([[-32.6129]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 6.987267781447197
epoch: 11, step: 110
	action: tensor([[-176.0900, -118.1221,  239.6438,   83.3949,  -56.0469,   49.3874,
         -107.4738]], dtype=torch.float64)
	q_value: tensor([[-31.8660]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 6.428776352822188
epoch: 11, step: 111
	action: tensor([[-319.4903,  232.6884, -254.1458,  252.3162,  -37.2744,   35.3093,
          114.6743]], dtype=torch.float64)
	q_value: tensor([[-31.8660]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14440128802202257, distance: 1.05850217398891 entropy 6.428776352822188
epoch: 11, step: 112
	action: tensor([[-454.2430,   54.4068,  322.0549,  292.3465, -132.1156,  134.0390,
         -199.8572]], dtype=torch.float64)
	q_value: tensor([[-34.3563]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.556664148251298, distance: 1.4277565291849748 entropy 6.805145522857491
epoch: 11, step: 113
	action: tensor([[ 415.2058, -138.6265,  124.8163,  -89.3955,  472.8083, -139.6635,
         -271.5829]], dtype=torch.float64)
	q_value: tensor([[-30.4665]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 6.862192881210385
epoch: 11, step: 114
	action: tensor([[-251.2416, -229.0124,  -39.4940,  105.4193,  312.4146,   31.6447,
         -201.5633]], dtype=torch.float64)
	q_value: tensor([[-31.8660]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9477802738258712, distance: 1.5970800138223784 entropy 6.428776352822188
epoch: 11, step: 115
	action: tensor([[ -98.8148, -207.4239, -176.1994, -236.3409,   50.6692, -302.1879,
         -467.0615]], dtype=torch.float64)
	q_value: tensor([[-24.5753]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9485841189364543, distance: 1.5974095357271196 entropy 6.818361557516695
epoch: 11, step: 116
	action: tensor([[-197.3542,  -55.8353,   89.1624,   94.4759, -351.7195,   -6.9400,
         -290.2438]], dtype=torch.float64)
	q_value: tensor([[-26.7459]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08541206648117972, distance: 1.1922134469577021 entropy 6.749997965705316
epoch: 11, step: 117
	action: tensor([[-233.5377, -113.0327,   13.5747, -133.5341,    2.1977, -110.5047,
           89.4364]], dtype=torch.float64)
	q_value: tensor([[-21.7034]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4097305488816756, distance: 1.3587036709304292 entropy 6.670130510891871
epoch: 11, step: 118
	action: tensor([[  -8.6902, -527.8390,  110.7709,  213.4506, -367.1132,  148.2748,
           -7.3352]], dtype=torch.float64)
	q_value: tensor([[-26.5174]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 6.719466185185302
epoch: 11, step: 119
	action: tensor([[-143.9357, -142.3719, -202.3605,   18.8189,  -19.4799, -126.5421,
         -392.1769]], dtype=torch.float64)
	q_value: tensor([[-31.8660]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13586031402631948, distance: 1.2196048878656796 entropy 6.428776352822188
epoch: 11, step: 120
	action: tensor([[ -79.0936,   14.7624,  -65.7870,  -71.2681, -196.1661,  639.4662,
           78.2203]], dtype=torch.float64)
	q_value: tensor([[-31.4426]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25327133144037495, distance: 0.9888675050081044 entropy 6.735177821703084
epoch: 11, step: 121
	action: tensor([[-407.8367, -510.4088, -321.3902,  248.2008, -494.8464,  281.7163,
         -404.4645]], dtype=torch.float64)
	q_value: tensor([[-43.7379]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6783089464016954, distance: 1.4824929886874325 entropy 7.0318183574034405
epoch: 11, step: 122
	action: tensor([[-191.9242, -167.7364,   -4.8632,  278.3056, -319.3391,  138.4553,
          -64.0967]], dtype=torch.float64)
	q_value: tensor([[-28.0673]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04232580986329548, distance: 1.1198647764314897 entropy 6.892618556604197
epoch: 11, step: 123
	action: tensor([[ 1.8415e-01, -5.3922e+01, -6.9258e+01,  4.2970e+01, -2.4053e+02,
         -2.8721e+01,  3.6416e+02]], dtype=torch.float64)
	q_value: tensor([[-21.9333]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5991493084972626, distance: 0.7245160518651832 entropy 6.6794499120661825
epoch: 11, step: 124
	action: tensor([[-282.9097, -294.6750,  -62.3720,  178.3671,  -39.6179,  118.4137,
         -256.6558]], dtype=torch.float64)
	q_value: tensor([[-20.7834]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.625919212302404, distance: 1.4591709493110394 entropy 6.724032335774702
epoch: 11, step: 125
	action: tensor([[ -12.5164, -268.6746, -315.8314,  157.7801,  266.7007, -111.2007,
          165.8838]], dtype=torch.float64)
	q_value: tensor([[-24.6462]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.006895340359268198, distance: 1.148282797823334 entropy 6.851845344902743
epoch: 11, step: 126
	action: tensor([[-194.3976,  -52.1807,   50.7197, -125.4160, -448.9282, -157.1195,
          294.0894]], dtype=torch.float64)
	q_value: tensor([[-35.6449]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4076744079560004, distance: 1.3577124511396967 entropy 7.046573320963486
epoch: 11, step: 127
	action: tensor([[-358.9207,   57.9558, -138.6218,  499.7667, -283.8560,  -44.3913,
         -243.0789]], dtype=torch.float64)
	q_value: tensor([[-31.3153]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08086672009448304, distance: 1.1897145304496137 entropy 7.033191133327945
LOSS epoch 11 actor 258.6896692140433 critic 256.94006561643124
epoch: 12, step: 0
	action: tensor([[-1117.8957,    60.8023,  -598.1997,   115.8807,  -183.0485,  -234.8589,
           294.6770]], dtype=torch.float64)
	q_value: tensor([[-30.1079]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41137135193685226, distance: 0.87796537934046 entropy 7.1398365306734
epoch: 12, step: 1
	action: tensor([[-232.1374, -455.1699,   41.5852, -118.7777,  -56.0957, -103.5066,
          213.8759]], dtype=torch.float64)
	q_value: tensor([[-31.2050]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.148654292448653, distance: 1.677412851103407 entropy 7.018698335532005
epoch: 12, step: 2
	action: tensor([[-760.3830,  -98.3549,   59.5972,  237.8618,    1.0615, -533.4951,
          189.9918]], dtype=torch.float64)
	q_value: tensor([[-27.2997]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17441047444604996, distance: 1.24012839414994 entropy 6.9405063881606335
epoch: 12, step: 3
	action: tensor([[-708.3347, -183.6689,  773.0139, -206.3038,  171.2682,  357.8122,
         -206.0348]], dtype=torch.float64)
	q_value: tensor([[-36.1006]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5434172737973344, distance: 1.421668600494724 entropy 7.194722804362386
epoch: 12, step: 4
	action: tensor([[-449.0933, -138.8308,   38.4864, -297.2248, -166.3646,  -61.1747,
         -140.7392]], dtype=torch.float64)
	q_value: tensor([[-24.8932]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.6696786100677548, distance: 1.8697613758401022 entropy 6.933608672436658
epoch: 12, step: 5
	action: tensor([[  41.6789, -348.8725, -341.5582,  326.5752,  215.9558, -239.5169,
         -305.8957]], dtype=torch.float64)
	q_value: tensor([[-32.5427]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7598525057645094, distance: 1.518080594342408 entropy 7.167249715748226
epoch: 12, step: 6
	action: tensor([[-375.5253, -258.0737,   27.8285,  274.1440,  304.1673,   67.3074,
         -144.3314]], dtype=torch.float64)
	q_value: tensor([[-28.5135]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23077326101880113, distance: 1.2695380430814527 entropy 6.971068660779807
epoch: 12, step: 7
	action: tensor([[-78.9854,  60.6315,  58.2087, 225.5543, 243.2482, 590.9279,  44.0757]],
       dtype=torch.float64)
	q_value: tensor([[-29.6811]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2139970565849465, distance: 1.2608560601141894 entropy 7.099229247971863
epoch: 12, step: 8
	action: tensor([[-227.7570, -133.2146,   18.1032, -213.3640, -250.7723,  394.1076,
         -426.4109]], dtype=torch.float64)
	q_value: tensor([[-30.8019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6655454505063103, distance: 1.4768450698102589 entropy 7.123084153555642
epoch: 12, step: 9
	action: tensor([[-309.4851, -250.7593,  275.1249,  -48.1994,  -37.5888, -178.6100,
          233.2210]], dtype=torch.float64)
	q_value: tensor([[-21.7834]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.8524627461873293, distance: 1.9327099559724181 entropy 6.761027581371457
epoch: 12, step: 10
	action: tensor([[-410.4613,  -85.0791,  -40.5918,  121.1812,  -42.1383, -351.4668,
          -13.4426]], dtype=torch.float64)
	q_value: tensor([[-18.2919]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04424078224602945, distance: 1.118744571220206 entropy 6.5615123384295675
epoch: 12, step: 11
	action: tensor([[-297.7147, -468.5189, -170.5677, -119.2555,   47.4809,  347.8071,
         -120.4973]], dtype=torch.float64)
	q_value: tensor([[-31.3705]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.7125699080765213, distance: 1.8847214056521524 entropy 7.185133096271685
epoch: 12, step: 12
	action: tensor([[ -70.7982,  147.9069,  125.5726, -336.8176, -154.7630, -508.7887,
           88.1422]], dtype=torch.float64)
	q_value: tensor([[-27.1024]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7702303702327487, distance: 1.5225500816522433 entropy 7.085315208021697
epoch: 12, step: 13
	action: tensor([[-280.6340, -466.7116,   -5.4501,  360.3413, -164.3221,  159.9496,
          -13.3994]], dtype=torch.float64)
	q_value: tensor([[-31.9908]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.4056526806321088, distance: 1.7748970031109195 entropy 6.866081437834514
epoch: 12, step: 14
	action: tensor([[-253.4087,   48.8368,  149.9606,  525.0464,  180.7725, -472.3792,
          204.5577]], dtype=torch.float64)
	q_value: tensor([[-22.4075]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.19639109279177114, distance: 1.2516799016587583 entropy 6.842601718625454
epoch: 12, step: 15
	action: tensor([[  93.6759, -301.8952,  700.5954,  202.9908,   24.6330, -515.6099,
         -153.1151]], dtype=torch.float64)
	q_value: tensor([[-28.9977]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8529220074083343, distance: 1.557705176128782 entropy 7.1454788186565334
epoch: 12, step: 16
	action: tensor([[-332.4400,  -58.2990,  -51.2226,  -99.3269,  -92.1000,  190.7092,
          305.4528]], dtype=torch.float64)
	q_value: tensor([[-26.0472]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.4662805287634346, distance: 1.7971235294227432 entropy 7.086374754947471
epoch: 12, step: 17
	action: tensor([[-196.3157,   41.0369,  349.4960,  110.9832, -607.9056,  -22.5579,
           34.5708]], dtype=torch.float64)
	q_value: tensor([[-25.9666]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6467036107544872, distance: 1.4684677467230918 entropy 7.048567964914328
epoch: 12, step: 18
	action: tensor([[-197.9495, -422.9608,  -59.8742,  397.2648, -396.4657,  856.4191,
          293.0662]], dtype=torch.float64)
	q_value: tensor([[-34.4261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10007993771995372, distance: 1.085572067691502 entropy 7.189996543638485
epoch: 12, step: 19
	action: tensor([[  78.0181, -430.0949, -129.4939,  -14.7315,    8.1959,  137.4463,
         -501.0336]], dtype=torch.float64)
	q_value: tensor([[-33.6049]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7510170017206172, distance: 1.5142649663579024 entropy 7.273570335456193
epoch: 12, step: 20
	action: tensor([[ -97.4790, -510.0117, -114.6636,  -73.3585,   62.9066,  178.1070,
          333.6384]], dtype=torch.float64)
	q_value: tensor([[-26.5863]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23006784749592568, distance: 1.2691741752234114 entropy 6.948677286671196
epoch: 12, step: 21
	action: tensor([[-549.0502,  372.7710, -427.6910, -177.0263,  274.5473,  -62.0116,
          184.9271]], dtype=torch.float64)
	q_value: tensor([[-28.2869]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12740901848546615, distance: 1.0689614741569835 entropy 6.964597330183077
epoch: 12, step: 22
	action: tensor([[-710.2113, -270.0485,  454.3311,   30.6621,  112.2121,   68.7431,
          597.0440]], dtype=torch.float64)
	q_value: tensor([[-37.3658]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.7113597346400304, distance: 1.8843009382899818 entropy 7.047370555849406
epoch: 12, step: 23
	action: tensor([[-146.4551,  -17.5735,  455.9530,   85.6808,  -19.0050,   63.7318,
          178.3303]], dtype=torch.float64)
	q_value: tensor([[-22.5805]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7602647483208238, distance: 1.518258387872823 entropy 6.5735420704836125
epoch: 12, step: 24
	action: tensor([[-478.8792, -247.3269, -490.5097, -398.8768, -406.1688, -306.3157,
         -175.3972]], dtype=torch.float64)
	q_value: tensor([[-27.6881]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3913283073351892, distance: 1.349806464884066 entropy 7.006262307162304
epoch: 12, step: 25
	action: tensor([[  16.5034,  -52.3368,  -59.0758,  178.0666, -353.4323,  455.3728,
         -140.0064]], dtype=torch.float64)
	q_value: tensor([[-27.3338]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07933243892052333, distance: 1.0980145978229991 entropy 6.957332827521054
epoch: 12, step: 26
	action: tensor([[-243.6162, -827.9429,  137.5863,   69.3451, -306.0474,  -34.9524,
           25.9659]], dtype=torch.float64)
	q_value: tensor([[-30.1659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4875174352506042, distance: 1.395686011330475 entropy 7.061742794382332
epoch: 12, step: 27
	action: tensor([[  22.1564,   15.1982, -769.4071,  714.1860,  287.9180,   25.7322,
          333.6568]], dtype=torch.float64)
	q_value: tensor([[-23.7905]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.43303782236608757, distance: 1.3698894459336155 entropy 6.844684875355685
epoch: 12, step: 28
	action: tensor([[ 162.5107, -220.6339,  151.0796, -454.6843,  -92.0259,  181.9177,
         -766.4606]], dtype=torch.float64)
	q_value: tensor([[-26.8393]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12239175191351592, distance: 1.0720302528737746 entropy 6.940587201573097
epoch: 12, step: 29
	action: tensor([[-616.0060, -174.8677, -105.6458,  173.3167, -311.0580, -191.1960,
           83.2974]], dtype=torch.float64)
	q_value: tensor([[-24.3151]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14255442014620878, distance: 1.2231934326180327 entropy 6.859089107692477
epoch: 12, step: 30
	action: tensor([[-130.6481, -750.3504,   51.4381, -285.8759,  -85.1266,  -90.0849,
         -215.6916]], dtype=torch.float64)
	q_value: tensor([[-29.3192]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0287714095237726, distance: 1.6299461304108365 entropy 6.972033539512838
epoch: 12, step: 31
	action: tensor([[-104.1980, -597.6570, -122.5946,  882.2856,  356.0770,  444.0504,
          129.8934]], dtype=torch.float64)
	q_value: tensor([[-34.5246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11684008563881632, distance: 1.2093505039028039 entropy 7.144885377120111
epoch: 12, step: 32
	action: tensor([[-298.1290,  -50.8666,  111.3488,  377.7746, -424.8161,  -42.9809,
            1.5185]], dtype=torch.float64)
	q_value: tensor([[-34.8761]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.778569109945654, distance: 1.9075120709137932 entropy 7.311297335259772
epoch: 12, step: 33
	action: tensor([[-374.6162, -915.1626,   60.7301,  230.1739, -579.7051, -121.8181,
          192.5225]], dtype=torch.float64)
	q_value: tensor([[-29.7502]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5041000739489365, distance: 1.4034439076058338 entropy 7.02623480810001
epoch: 12, step: 34
	action: tensor([[-690.6953, -212.1083, -327.0585,  773.7025,  -71.7015, -205.9854,
           76.9151]], dtype=torch.float64)
	q_value: tensor([[-32.1849]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.119362306829629, distance: 1.2107153050559518 entropy 7.29118144295432
epoch: 12, step: 35
	action: tensor([[-266.5741, -988.1827,   77.1646,   45.6931, -111.2800,   84.3305,
         -259.0943]], dtype=torch.float64)
	q_value: tensor([[-29.3103]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7427222245254521, distance: 1.5106740814686614 entropy 7.2125207432097
epoch: 12, step: 36
	action: tensor([[ 100.7502, -144.6133,  -30.1510, -115.7981,   21.9816,  -28.2806,
          161.8312]], dtype=torch.float64)
	q_value: tensor([[-22.5816]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4873190339471456, distance: 0.8193704365702184 entropy 6.83644789579549
epoch: 12, step: 37
	action: tensor([[-488.9038, -353.1217,   52.9295, -231.4313,  268.3374,  197.9733,
         -110.7389]], dtype=torch.float64)
	q_value: tensor([[-31.0175]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.9972710729980063, distance: 1.9811607002482676 entropy 7.0110127435435485
epoch: 12, step: 38
	action: tensor([[-434.8582,  -98.9962,  227.3776,   85.2691,  -51.1463, -388.8485,
          806.5750]], dtype=torch.float64)
	q_value: tensor([[-31.1469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5465280692917129, distance: 1.4231005833324983 entropy 7.195055633328542
epoch: 12, step: 39
	action: tensor([[-368.2577, -391.1185, -161.6556,  102.5183, -236.3590,  -29.8557,
          179.5205]], dtype=torch.float64)
	q_value: tensor([[-27.1632]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08467149687730258, distance: 1.1918066578123812 entropy 7.031501964570919
epoch: 12, step: 40
	action: tensor([[-330.4514, -364.0719, -621.6845, -242.3856, -832.6394,  109.1221,
          157.8313]], dtype=torch.float64)
	q_value: tensor([[-37.6030]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.721919070893748, distance: 1.8879665589934929 entropy 7.281364324310139
epoch: 12, step: 41
	action: tensor([[-583.8962, -360.6365,   20.9648,   85.8333,   17.5721,   12.0106,
          224.0873]], dtype=torch.float64)
	q_value: tensor([[-29.4680]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5481342243642344, distance: 0.769239218186314 entropy 7.052180878020751
epoch: 12, step: 42
	action: tensor([[-317.5857,  -53.2721,  349.2823,   12.1717, -323.3886,   65.0121,
         -224.4015]], dtype=torch.float64)
	q_value: tensor([[-27.4358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.3211144275945386, distance: 1.7434318224939511 entropy 7.093021649704079
epoch: 12, step: 43
	action: tensor([[-219.7418, -428.0728,  183.3300, 1118.0747, -192.8264,  624.0133,
          235.2633]], dtype=torch.float64)
	q_value: tensor([[-35.7447]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2878576423686101, distance: 1.2986455240747279 entropy 7.268076778242312
epoch: 12, step: 44
	action: tensor([[-779.5980, -749.2421,  -85.6470,  190.6221,  214.1540,  155.7954,
          405.5066]], dtype=torch.float64)
	q_value: tensor([[-34.6555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07171831034900955, distance: 1.1846689849609195 entropy 7.2199587786970385
epoch: 12, step: 45
	action: tensor([[  13.3969, -228.3941, -154.7440,  -35.2167,  274.6768, -315.3534,
          275.8605]], dtype=torch.float64)
	q_value: tensor([[-30.9690]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2202714791901781, distance: 1.0104815849108517 entropy 7.239129364836488
epoch: 12, step: 46
	action: tensor([[-371.1942, -171.6062, -208.0609,   29.3868,  100.4596,  222.0413,
         -258.6376]], dtype=torch.float64)
	q_value: tensor([[-27.6716]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.047512233131830284, distance: 1.1168282647443604 entropy 7.053163801463198
epoch: 12, step: 47
	action: tensor([[ -56.9114,   59.3326,  298.1474,  161.5803, -174.4419,   17.4695,
          315.6186]], dtype=torch.float64)
	q_value: tensor([[-22.5246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 6.897440959821415
epoch: 12, step: 48
	action: tensor([[-267.5321, -177.1328,  -35.0026,  143.4670, -149.0752,  190.5034,
          153.2847]], dtype=torch.float64)
	q_value: tensor([[-30.9209]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9794448711611139, distance: 1.610009351985733 entropy 6.563206761576444
epoch: 12, step: 49
	action: tensor([[-149.5056, -423.0536, -184.9180, -176.5500,   61.8983,  -44.8769,
         -364.0931]], dtype=torch.float64)
	q_value: tensor([[-29.6942]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.29947969934342566, distance: 1.3044920686165422 entropy 7.051330852776632
epoch: 12, step: 50
	action: tensor([[  20.2259, -222.9712, -385.7692,  202.0333, -181.4443,  161.2866,
         -119.8931]], dtype=torch.float64)
	q_value: tensor([[-32.1958]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5109304501066134, distance: 1.4066269376575329 entropy 7.0280897469912675
epoch: 12, step: 51
	action: tensor([[-5.2171e+02, -2.4883e+02, -5.7737e-02,  5.1028e+01, -1.6005e+02,
          1.1332e+02,  3.6427e+01]], dtype=torch.float64)
	q_value: tensor([[-21.9891]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1298288729482526, distance: 1.6700483641797808 entropy 6.840662975337486
epoch: 12, step: 52
	action: tensor([[-349.7190, -318.5238,   77.0327,  178.0629,  235.5300,  -32.5754,
          514.8804]], dtype=torch.float64)
	q_value: tensor([[-23.5594]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2813064243873443, distance: 0.9701269616942518 entropy 7.060451272672536
epoch: 12, step: 53
	action: tensor([[ -19.4521,  103.5351,  -55.4919, -424.4833, -271.2807,  352.5367,
          291.7920]], dtype=torch.float64)
	q_value: tensor([[-31.5046]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09368032801346882, distance: 1.08942514896252 entropy 7.105119208149573
epoch: 12, step: 54
	action: tensor([[ -41.8047, -311.8438,   94.2998,   12.1154, -108.1212,   22.6135,
          -99.1127]], dtype=torch.float64)
	q_value: tensor([[-24.0812]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0471616713171938, distance: 1.637316973901769 entropy 6.739678003390716
epoch: 12, step: 55
	action: tensor([[-455.7716,  -61.8699,   11.2714,  208.3395, -131.4285, -122.1224,
          134.2241]], dtype=torch.float64)
	q_value: tensor([[-24.1752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7134011894573082, distance: 1.497911742389948 entropy 6.889787344856323
epoch: 12, step: 56
	action: tensor([[-265.1554,   37.6003,  122.4949,  516.4768,  295.3525,   35.2575,
         -335.3797]], dtype=torch.float64)
	q_value: tensor([[-25.6197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0538082605926129, distance: 1.1131309785038606 entropy 6.937434976046317
epoch: 12, step: 57
	action: tensor([[  50.9144, -636.1204,   54.9405,  658.5496,  160.4284, -283.0223,
         -163.9759]], dtype=torch.float64)
	q_value: tensor([[-33.8476]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.013252852680622595, distance: 1.1519022082149322 entropy 7.270890234847594
epoch: 12, step: 58
	action: tensor([[-227.7399, -570.3612,  208.7747, -336.0047, -576.3508,   48.8946,
           56.1012]], dtype=torch.float64)
	q_value: tensor([[-24.5526]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.574596499750192, distance: 1.836163205944057 entropy 7.133899184045593
epoch: 12, step: 59
	action: tensor([[-169.8950, -350.8482, -254.1435, -172.3676,    8.9648,   -4.6169,
          163.1520]], dtype=torch.float64)
	q_value: tensor([[-21.5359]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 6.687855704948179
epoch: 12, step: 60
	action: tensor([[-400.9909, -263.2083,  -41.4626,   29.0502,  186.3447,  -35.2410,
          125.8380]], dtype=torch.float64)
	q_value: tensor([[-30.9209]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2126587724288571, distance: 1.7022130480858895 entropy 6.563206761576444
epoch: 12, step: 61
	action: tensor([[-349.5780, -763.7729,  443.7199,  537.6286,   33.3821,  285.1937,
          618.9251]], dtype=torch.float64)
	q_value: tensor([[-35.2073]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6834121676148217, distance: 1.4847451804932854 entropy 7.171276782606506
epoch: 12, step: 62
	action: tensor([[-632.6540,  -71.8291,  -57.9539, -236.7316, -172.9127,  368.2017,
          563.7986]], dtype=torch.float64)
	q_value: tensor([[-33.2632]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6476320599853553, distance: 1.468881666270159 entropy 7.191265715142833
epoch: 12, step: 63
	action: tensor([[-406.3522, -381.1151, -214.5670,  395.5289, -104.8770, -456.7401,
         -238.8054]], dtype=torch.float64)
	q_value: tensor([[-32.2141]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3782642165577754, distance: 1.3434544112900275 entropy 7.228446409419214
epoch: 12, step: 64
	action: tensor([[-430.3547,  295.9448, -425.7160,  412.5485,  -59.4598,   50.2436,
          498.9803]], dtype=torch.float64)
	q_value: tensor([[-25.3955]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5784710868893781, distance: 0.7429684597353707 entropy 6.988904077067021
epoch: 12, step: 65
	action: tensor([[-303.9395, -151.0285,  328.5533,  128.8469,    9.4360,  517.4463,
          214.9725]], dtype=torch.float64)
	q_value: tensor([[-30.4966]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.44554973671878595, distance: 1.3758567306788072 entropy 7.126612408369792
epoch: 12, step: 66
	action: tensor([[-568.0735, -212.0184,  106.8528,   98.2868, -174.2693,  158.1044,
          124.6222]], dtype=torch.float64)
	q_value: tensor([[-34.4634]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.023539359699672024, distance: 1.1577344785222814 entropy 7.283760965674832
epoch: 12, step: 67
	action: tensor([[-318.3611,  -62.3835,  374.5154,  113.6751,  276.5815,  413.1075,
          209.1035]], dtype=torch.float64)
	q_value: tensor([[-26.8396]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.01434343129778859, distance: 1.152521945957856 entropy 6.992887679469068
epoch: 12, step: 68
	action: tensor([[ -82.3602,    3.9734,  131.1183,  -18.6942,  -64.0846, -146.5712,
          663.1568]], dtype=torch.float64)
	q_value: tensor([[-27.1862]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 6.982908509968686
epoch: 12, step: 69
	action: tensor([[-316.8586, -197.8455,  250.7006,  -13.9590,   28.0453, -135.4118,
          -68.6342]], dtype=torch.float64)
	q_value: tensor([[-30.9209]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0347447733253268, distance: 1.6323439128860635 entropy 6.563206761576444
epoch: 12, step: 70
	action: tensor([[-232.6706, -331.4670, -237.1302,  237.6813,   78.8165,  534.5200,
          192.4979]], dtype=torch.float64)
	q_value: tensor([[-28.6150]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.4959023763237358, distance: 1.8078837057581012 entropy 6.922745732265116
epoch: 12, step: 71
	action: tensor([[ -41.7304, -471.8366, -188.9663,  224.0700,   39.9900,  539.8625,
          171.5619]], dtype=torch.float64)
	q_value: tensor([[-28.7270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.616436926053076, distance: 1.4549098185317442 entropy 7.125428392460542
epoch: 12, step: 72
	action: tensor([[-204.1663, -848.1564,  182.4869,  499.6205,   15.6307,  307.5960,
          -69.6195]], dtype=torch.float64)
	q_value: tensor([[-33.7721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6883799369564656, distance: 1.4869343169901819 entropy 7.16834871370034
epoch: 12, step: 73
	action: tensor([[-619.6555, -443.9772, -221.7782, -251.2210, -197.2924,   79.8789,
          874.8858]], dtype=torch.float64)
	q_value: tensor([[-31.3473]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.67940665570329, distance: 1.8731648911921839 entropy 7.159845262003627
epoch: 12, step: 74
	action: tensor([[ -40.3763,  -31.2023,  -79.9080,  -30.5438,  277.2687,   67.8124,
         -242.1433]], dtype=torch.float64)
	q_value: tensor([[-26.8480]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.876379705796662, distance: 1.5675343152245218 entropy 7.039970653055581
epoch: 12, step: 75
	action: tensor([[ 182.5595, -117.3108,  374.9300,   24.4861, -311.5174, -345.1361,
          252.6639]], dtype=torch.float64)
	q_value: tensor([[-34.8027]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6324095313479405, distance: 1.462080396691333 entropy 7.1685209700177355
epoch: 12, step: 76
	action: tensor([[-418.0253, -482.8407,  -71.1780,  601.9616,  -26.9497,  -18.8395,
         -534.2441]], dtype=torch.float64)
	q_value: tensor([[-29.4264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.41716186331127436, distance: 1.3622801284244312 entropy 7.047043578100606
epoch: 12, step: 77
	action: tensor([[ -78.2577, -248.8946,  347.4581,  208.2366,  -73.9463,   45.3421,
          202.1525]], dtype=torch.float64)
	q_value: tensor([[-27.9156]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4898372000336485, distance: 0.8173556838818701 entropy 7.15654513779353
epoch: 12, step: 78
	action: tensor([[-198.8470, -353.7338, -190.5300, -137.1187,  239.4692,  668.4273,
         -542.7841]], dtype=torch.float64)
	q_value: tensor([[-29.3512]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.37060064156880124, distance: 1.3397141939946193 entropy 7.053736980779607
epoch: 12, step: 79
	action: tensor([[-612.3845, -600.7019, -108.5648,  476.6470,   74.8393,    2.9314,
          236.0455]], dtype=torch.float64)
	q_value: tensor([[-32.6244]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 7.199236715516823
epoch: 12, step: 80
	action: tensor([[-227.8034,   18.6442, -454.2373,  -66.5191, -290.2877, -189.9967,
           50.3526]], dtype=torch.float64)
	q_value: tensor([[-30.9209]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 6.563206761576444
epoch: 12, step: 81
	action: tensor([[-407.1771, -153.8950, -170.0040,  -70.4317,  -60.1838,  178.1826,
           14.1765]], dtype=torch.float64)
	q_value: tensor([[-30.9209]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 6.563206761576444
epoch: 12, step: 82
	action: tensor([[-254.7292,  -48.2756, -213.4755,  351.1536, -219.9940, -265.2448,
         -182.4245]], dtype=torch.float64)
	q_value: tensor([[-30.9209]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3519042953395195, distance: 1.330545306165923 entropy 6.563206761576444
epoch: 12, step: 83
	action: tensor([[-269.8745, -172.7777,  -42.8708,  -97.8798,   -9.6892, -141.2906,
          -87.9228]], dtype=torch.float64)
	q_value: tensor([[-24.3886]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5801850987942818, distance: 1.4385026765697964 entropy 6.629703523705798
epoch: 12, step: 84
	action: tensor([[ -92.2824,  -66.4696, -341.6031,   43.7359,   54.8216,  -39.9501,
           60.9427]], dtype=torch.float64)
	q_value: tensor([[-28.8877]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3970235145713725, distance: 1.3525662652539976 entropy 6.888051164468258
epoch: 12, step: 85
	action: tensor([[ -26.2308, -539.9971, -103.1749, -174.2754, -361.1112,  -28.8188,
         -214.8734]], dtype=torch.float64)
	q_value: tensor([[-32.9639]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9269816001418234, distance: 1.5885302052345651 entropy 7.151362317615991
epoch: 12, step: 86
	action: tensor([[-569.4032, -202.2459,   69.5192, -707.8484, -591.6899,  434.1679,
         -259.3590]], dtype=torch.float64)
	q_value: tensor([[-35.3161]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.297248594162158, distance: 0.9593068703569873 entropy 7.262109712814454
epoch: 12, step: 87
	action: tensor([[-361.4119, -241.9650, -298.5159,  383.4619, -182.4103,  -35.9575,
         -415.3697]], dtype=torch.float64)
	q_value: tensor([[-29.9051]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8047454703943628, distance: 1.537321402726902 entropy 6.970674639573185
epoch: 12, step: 88
	action: tensor([[-366.4628, -495.2679, -244.5178,  -82.5257, -251.9079,  352.3395,
          501.1413]], dtype=torch.float64)
	q_value: tensor([[-31.5085]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.3449868113781451, distance: 1.7523743817276802 entropy 7.0650833213588085
epoch: 12, step: 89
	action: tensor([[-159.7325,   54.4590,  -74.2916,  399.3169,  142.1788,   22.5580,
          247.7301]], dtype=torch.float64)
	q_value: tensor([[-25.9039]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5852996770121799, distance: 1.4408287935271262 entropy 6.864671915573261
epoch: 12, step: 90
	action: tensor([[-557.5930,   77.8236,  278.9686,  580.9808, -109.6800,  174.7832,
          209.9520]], dtype=torch.float64)
	q_value: tensor([[-32.6705]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10436078916303015, distance: 1.0829869978094553 entropy 7.1512493354795135
epoch: 12, step: 91
	action: tensor([[-122.1196,  113.4974,  406.9403,  540.3986, -492.1278, -274.4567,
           11.1755]], dtype=torch.float64)
	q_value: tensor([[-38.8549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6013585870842744, distance: 1.4481081232916406 entropy 7.35827246965511
epoch: 12, step: 92
	action: tensor([[-544.7421, -616.2371, -163.4867,   95.3344, -157.7604,  607.2457,
          277.2804]], dtype=torch.float64)
	q_value: tensor([[-33.2815]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.22812624621166355, distance: 1.268172115297129 entropy 7.160075926915208
epoch: 12, step: 93
	action: tensor([[ -62.8604,  202.5554, -103.3845, -243.6511,   66.9703, -426.5350,
          505.6988]], dtype=torch.float64)
	q_value: tensor([[-25.0186]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3514677636932937, distance: 1.3303304714214261 entropy 6.914493708486027
epoch: 12, step: 94
	action: tensor([[-188.1994,   -4.8013,  544.4367,  185.3425,   41.7326,  -68.4272,
          216.0530]], dtype=torch.float64)
	q_value: tensor([[-34.4631]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10774881905117728, distance: 1.2044182876305591 entropy 7.107245667166219
epoch: 12, step: 95
	action: tensor([[-436.1893, -336.4549, -380.9603,  520.1806, -132.7075,   28.5212,
          -15.2740]], dtype=torch.float64)
	q_value: tensor([[-33.1609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38923658992265464, distance: 0.8943205266335603 entropy 7.214383952867913
epoch: 12, step: 96
	action: tensor([[ -61.9264,   69.8812,  907.1016, -283.4692,   42.3212, -121.2000,
          568.3466]], dtype=torch.float64)
	q_value: tensor([[-32.9475]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1749557249343368, distance: 1.6876481339768807 entropy 7.229141638155076
epoch: 12, step: 97
	action: tensor([[-365.1217, -148.0063,  403.2306,  123.6931,  597.7956,  -21.9042,
          -16.1459]], dtype=torch.float64)
	q_value: tensor([[-33.0068]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0587931625627882, distance: 1.641961810552499 entropy 7.162188785809983
epoch: 12, step: 98
	action: tensor([[-476.1527,   48.0785, -224.4657,  -74.6520, -278.6983,   27.7985,
          129.3082]], dtype=torch.float64)
	q_value: tensor([[-30.7944]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6608146702077013, distance: 1.4747461784874458 entropy 7.035144909595265
epoch: 12, step: 99
	action: tensor([[ -79.8820, -461.2817,  -44.5771,  693.4119, -127.1977,  133.6057,
          -57.2838]], dtype=torch.float64)
	q_value: tensor([[-30.7177]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5811920036757334, distance: 1.438960915513876 entropy 7.017454327577446
epoch: 12, step: 100
	action: tensor([[ 103.3327, -434.8016, -301.0479, -366.7852,   47.5475,  -72.7160,
         -191.0738]], dtype=torch.float64)
	q_value: tensor([[-26.1601]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16769854186760513, distance: 1.2365795561595045 entropy 7.077358667266401
epoch: 12, step: 101
	action: tensor([[ -50.2864, -288.9812, -396.0527,  184.2214,  -79.1013,  365.2948,
         -326.3351]], dtype=torch.float64)
	q_value: tensor([[-29.7493]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5595550404438445, distance: 1.4290816627043326 entropy 7.098335063989145
epoch: 12, step: 102
	action: tensor([[-567.9239, -604.3003,  108.3786,  300.9479, -493.3748,  109.8462,
         -165.7613]], dtype=torch.float64)
	q_value: tensor([[-23.5750]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8149878657419263, distance: 1.5416775783115653 entropy 6.908234633837918
epoch: 12, step: 103
	action: tensor([[-156.5898, -205.2975, -361.7072,  404.8323, -627.7009,  312.4752,
         -469.6091]], dtype=torch.float64)
	q_value: tensor([[-31.3297]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0670910409737282, distance: 1.1821087448638208 entropy 7.266592591686987
epoch: 12, step: 104
	action: tensor([[  82.5318,   26.1425, -410.1306,  607.9210,   70.6223, -350.3688,
          374.7264]], dtype=torch.float64)
	q_value: tensor([[-32.4805]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6878629274931793, distance: 1.4867066377939904 entropy 7.173393734648656
epoch: 12, step: 105
	action: tensor([[-103.6540, -643.1714,   16.5079, -108.3550,   79.0147, -308.7977,
           98.7768]], dtype=torch.float64)
	q_value: tensor([[-31.2877]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.32228157869661334, distance: 1.315887214063216 entropy 7.060890142341856
epoch: 12, step: 106
	action: tensor([[-563.4792, -255.3227,  134.0596,  294.2589,  188.1444,  -90.3762,
          -11.3747]], dtype=torch.float64)
	q_value: tensor([[-31.8425]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.33576953121958053, distance: 1.3225815493174464 entropy 7.272278376640808
epoch: 12, step: 107
	action: tensor([[  65.1224,  223.6866,  211.6715, -389.3001, -101.1459,  247.6554,
         -123.0524]], dtype=torch.float64)
	q_value: tensor([[-29.8462]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14884095761780425, distance: 1.226553930269839 entropy 7.073339172316237
epoch: 12, step: 108
	action: tensor([[-383.8038, -200.2332,  242.0122,  -37.6910,  -46.7254,  434.8450,
          507.3650]], dtype=torch.float64)
	q_value: tensor([[-28.9604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5677859702404138, distance: 1.4328478624436283 entropy 6.896507506928955
epoch: 12, step: 109
	action: tensor([[ 235.5743, -550.0149, -258.5214, -535.5922, -309.9024,   73.6026,
          -15.2548]], dtype=torch.float64)
	q_value: tensor([[-40.0953]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.034878727199594506, distance: 1.164129844156582 entropy 7.261854896359096
epoch: 12, step: 110
	action: tensor([[-520.6402, -688.7395,  114.4962,  388.9877,   99.1054,  145.6125,
          239.0164]], dtype=torch.float64)
	q_value: tensor([[-27.3564]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2995195211944006, distance: 1.735302704546606 entropy 7.113373824754853
epoch: 12, step: 111
	action: tensor([[-241.0253, -334.8888,   51.3733,   44.9234,   49.5253,    2.4360,
         -104.0850]], dtype=torch.float64)
	q_value: tensor([[-21.1542]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 6.781261605217329
epoch: 12, step: 112
	action: tensor([[-111.2177, -296.0975,   93.9850,  166.2653,  275.7055, -314.6875,
            2.1149]], dtype=torch.float64)
	q_value: tensor([[-30.9209]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1954235246448932, distance: 1.6955704923093744 entropy 6.563206761576444
epoch: 12, step: 113
	action: tensor([[-119.0722, -336.9024,  192.5325,  -76.7842,  112.2827,  114.7981,
          -65.9140]], dtype=torch.float64)
	q_value: tensor([[-22.7090]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.38655665522043314, distance: 1.3474898518239167 entropy 6.874409208395406
epoch: 12, step: 114
	action: tensor([[ 203.0898,  236.0230,  333.4418, -261.6308,  235.9326,   11.9255,
         -524.8621]], dtype=torch.float64)
	q_value: tensor([[-28.7261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 7.113559988128884
epoch: 12, step: 115
	action: tensor([[-289.7811, -314.3852,  -78.9269,    5.6482, -180.5636,   -4.5857,
          -39.7862]], dtype=torch.float64)
	q_value: tensor([[-30.9209]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 6.563206761576444
epoch: 12, step: 116
	action: tensor([[-209.2859, -156.5630,  -62.4458,  -59.6196, -104.3875, -167.6348,
          177.8047]], dtype=torch.float64)
	q_value: tensor([[-30.9209]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8368829927063661, distance: 1.5509487240757258 entropy 6.563206761576444
epoch: 12, step: 117
	action: tensor([[-311.9690,  -27.3790,  215.3388, -237.5262,  102.9305, -399.9165,
          299.6998]], dtype=torch.float64)
	q_value: tensor([[-33.3572]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8549453487282295, distance: 1.5585554302927405 entropy 7.011582737039684
epoch: 12, step: 118
	action: tensor([[-505.4906, -283.3548,  124.6059,   43.1586,  351.3377, -212.7592,
         -188.3045]], dtype=torch.float64)
	q_value: tensor([[-33.4376]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.031275942349068386, distance: 1.1263068772211042 entropy 7.110315378920701
epoch: 12, step: 119
	action: tensor([[ -49.1889, -216.4339,   84.4966,  849.6872,  384.2743,  -78.9197,
         -370.4691]], dtype=torch.float64)
	q_value: tensor([[-27.1142]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8057374973374112, distance: 1.537743859732908 entropy 6.850937279973892
epoch: 12, step: 120
	action: tensor([[-210.8700,  -62.5997, -159.7002, -383.0827,  -97.3116,   95.2556,
         -122.5263]], dtype=torch.float64)
	q_value: tensor([[-27.5092]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.7627422025798771, distance: 1.9020716572344054 entropy 6.881383287968491
epoch: 12, step: 121
	action: tensor([[-118.0992, -301.9679, -456.6637, -300.8808,   29.3560, -124.5152,
         -276.5622]], dtype=torch.float64)
	q_value: tensor([[-35.1619]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6829720432858737, distance: 1.4845510761596563 entropy 7.225068275654782
epoch: 12, step: 122
	action: tensor([[-163.6065,   70.4875,  983.3705,  150.4060, -366.6769,   98.4901,
         -199.7935]], dtype=torch.float64)
	q_value: tensor([[-33.2321]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08944995073111128, distance: 1.1944289884791286 entropy 7.085646526036003
epoch: 12, step: 123
	action: tensor([[-183.1353,  277.3648, -246.9533, -108.2941,   37.0505,  -76.1257,
         -122.3849]], dtype=torch.float64)
	q_value: tensor([[-37.7390]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 7.087564309496197
epoch: 12, step: 124
	action: tensor([[-226.0477, -302.5049, -116.2389,  278.3321,  258.7863,  195.1056,
          185.0162]], dtype=torch.float64)
	q_value: tensor([[-30.9209]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6420051254715997, distance: 1.4663712848379313 entropy 6.563206761576444
epoch: 12, step: 125
	action: tensor([[-103.5648, -338.3323, -136.4782,  163.0377, -306.7281,  259.0360,
          404.1369]], dtype=torch.float64)
	q_value: tensor([[-28.1768]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09510790719600248, distance: 1.0885668129777566 entropy 7.020522387025896
epoch: 12, step: 126
	action: tensor([[-431.8235, -469.0264,  -96.9915,   53.9850,   34.4607,  350.2564,
          614.1083]], dtype=torch.float64)
	q_value: tensor([[-30.9680]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4316766797778875, distance: 1.3692387101979104 entropy 7.132815543733213
epoch: 12, step: 127
	action: tensor([[-845.8359,  634.3240,   40.5102,  164.9879,   65.2263,  -89.4035,
          420.6259]], dtype=torch.float64)
	q_value: tensor([[-38.5385]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6767403821525695, distance: 1.4818000503899929 entropy 7.416091350243861
LOSS epoch 12 actor 247.9352829999471 critic 193.6545204245496
epoch: 13, step: 0
	action: tensor([[-711.6693, -853.4279,  492.0983, -376.8164,  127.2254,   62.2481,
          222.4734]], dtype=torch.float64)
	q_value: tensor([[-34.4742]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.3103561048285055, distance: 1.7393867425846494 entropy 7.2148055295146305
epoch: 13, step: 1
	action: tensor([[-565.7688,  294.7740, -281.6796,   83.4673, -285.8568,  291.8711,
         -246.5603]], dtype=torch.float64)
	q_value: tensor([[-27.1516]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11172044430591788, distance: 1.206575463558806 entropy 7.099856330763926
epoch: 13, step: 2
	action: tensor([[ 104.3033, -107.8308,   59.6688,  -70.5816,  336.1715,  -73.1493,
         -384.9159]], dtype=torch.float64)
	q_value: tensor([[-32.0778]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 7.1835273542102405
epoch: 13, step: 3
	action: tensor([[-295.1231, -450.0979, -141.4958,  150.3039,   25.0353, -593.7365,
         -151.0670]], dtype=torch.float64)
	q_value: tensor([[-30.2900]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8796884503140965, distance: 1.568915774988049 entropy 6.692129093849127
epoch: 13, step: 4
	action: tensor([[ 110.9423, -388.8393, -176.0255,  218.1156, -214.9654, -345.4621,
          316.4670]], dtype=torch.float64)
	q_value: tensor([[-28.8705]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25218398887071813, distance: 0.98958720858945 entropy 7.168933060009226
epoch: 13, step: 5
	action: tensor([[-3.3055e+02, -1.8891e+02,  1.7330e+01, -6.8062e+02, -6.3090e-01,
          1.3917e+02,  4.6915e+02]], dtype=torch.float64)
	q_value: tensor([[-22.3888]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.5818461226981002, distance: 1.8387465492689945 entropy 7.089992207655575
epoch: 13, step: 6
	action: tensor([[ -44.6223, -345.3914,  170.8525,  259.9318, -307.9676,  289.0075,
          230.8555]], dtype=torch.float64)
	q_value: tensor([[-21.8490]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09809419684852128, distance: 1.086769105664817 entropy 6.801043570541517
epoch: 13, step: 7
	action: tensor([[-173.1339, -443.5775,   -6.1133,  215.3385, -321.8401, -651.3565,
         -432.6888]], dtype=torch.float64)
	q_value: tensor([[-23.1416]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23139736059583083, distance: 1.2698598804840986 entropy 7.008459315386952
epoch: 13, step: 8
	action: tensor([[-279.9018,  275.9991,  199.3169,   48.4191, -428.6053, -104.7067,
          176.2803]], dtype=torch.float64)
	q_value: tensor([[-25.0213]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.162762453441606, distance: 1.0470828380538337 entropy 7.158743881818557
epoch: 13, step: 9
	action: tensor([[-256.7357, -585.0453,    2.3272,  442.6253,  -97.4566,  594.5373,
           18.2940]], dtype=torch.float64)
	q_value: tensor([[-29.2119]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.35930554435248974, distance: 1.3341824924959291 entropy 7.18970546494228
epoch: 13, step: 10
	action: tensor([[-417.9071, -159.4989, -156.9005,  513.8330, -728.0075, -168.5248,
          356.7132]], dtype=torch.float64)
	q_value: tensor([[-34.3940]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17480402702942266, distance: 1.0395257297851737 entropy 7.364242713247447
epoch: 13, step: 11
	action: tensor([[  33.6890, -446.1142, -347.9101, -728.3103,  230.4101,   65.3682,
          -79.1494]], dtype=torch.float64)
	q_value: tensor([[-24.0221]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0880026521606273, distance: 1.0928321951192237 entropy 7.07020848042832
epoch: 13, step: 12
	action: tensor([[-461.0238, -155.7522,  368.2192,  117.8604,  261.3887,  877.6083,
         -155.5918]], dtype=torch.float64)
	q_value: tensor([[-29.4733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5148053279016933, distance: 1.4084294750993942 entropy 7.284286902464335
epoch: 13, step: 13
	action: tensor([[ 116.5432,  -66.4936,  146.0219, -197.3345,  359.3247,  304.1531,
          739.2816]], dtype=torch.float64)
	q_value: tensor([[-33.8627]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5215776070578553, distance: 1.411574315036968 entropy 7.41806986810938
epoch: 13, step: 14
	action: tensor([[-138.0568, -501.1319,  471.7742,  249.5155,  -67.5588,  564.4051,
          784.8589]], dtype=torch.float64)
	q_value: tensor([[-30.2845]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15624350165280299, distance: 1.0511513649711122 entropy 7.29082287947689
epoch: 13, step: 15
	action: tensor([[ 151.2827,  -47.3497,   24.3475, -342.3305,  -25.5459, -143.4836,
         -118.8467]], dtype=torch.float64)
	q_value: tensor([[-27.5508]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5469215086834422, distance: 1.4232815914349048 entropy 7.16829664288386
epoch: 13, step: 16
	action: tensor([[-234.4795, -121.3801,   -2.4653,   64.8465, -285.4398, -182.0030,
         -153.3737]], dtype=torch.float64)
	q_value: tensor([[-11.7582]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15962434227129596, distance: 1.2322968970191885 entropy 6.424002400911462
epoch: 13, step: 17
	action: tensor([[-812.8816, -769.1710, -151.4108,  239.4459, -618.0943,  755.1021,
         -322.8440]], dtype=torch.float64)
	q_value: tensor([[-30.8722]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8594496692478106, distance: 1.5604465848245397 entropy 7.420753005453082
epoch: 13, step: 18
	action: tensor([[-590.8530,  264.4413,  240.8970,  347.7480,  361.8333,  113.3849,
         -346.4857]], dtype=torch.float64)
	q_value: tensor([[-30.4615]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36156756783729704, distance: 0.9143535675643027 entropy 7.4218668033708965
epoch: 13, step: 19
	action: tensor([[-262.3053,  -37.2949,  490.4892,   34.7328, -165.7304, -324.3258,
         1140.1429]], dtype=torch.float64)
	q_value: tensor([[-29.8268]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6735124056665737, distance: 0.6538679269796218 entropy 7.245906983769344
epoch: 13, step: 20
	action: tensor([[-526.1586, -138.3586,  -89.8178,  -28.4277,  237.8548,  -45.7195,
          507.8613]], dtype=torch.float64)
	q_value: tensor([[-29.1173]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6993486492849179, distance: 1.49175649909149 entropy 7.32071024574645
epoch: 13, step: 21
	action: tensor([[-300.4760, -658.1964,   60.9437,  483.1412,  -38.5052, -154.9295,
          -55.6413]], dtype=torch.float64)
	q_value: tensor([[-24.9411]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7837162967517495, distance: 1.528338605315845 entropy 7.077683501494684
epoch: 13, step: 22
	action: tensor([[-991.6906, -137.0980,   63.4537, -398.5527,  -90.1787,  212.2085,
          543.2505]], dtype=torch.float64)
	q_value: tensor([[-26.9317]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.467742572296232, distance: 1.7976561297328981 entropy 7.240163988697437
epoch: 13, step: 23
	action: tensor([[-486.3161, -824.7271, -131.2580,  333.6617, -491.7135,   74.9941,
          -58.5573]], dtype=torch.float64)
	q_value: tensor([[-26.4808]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5609236452355395, distance: 1.4297085784125783 entropy 7.129557512030907
epoch: 13, step: 24
	action: tensor([[-682.4076,  282.9796, -225.9063,    3.3517, -292.4430,  119.6634,
          783.1482]], dtype=torch.float64)
	q_value: tensor([[-28.6411]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 7.330943309707365
epoch: 13, step: 25
	action: tensor([[  -0.9806,  -65.4518,  -95.8227,  187.1803, -140.8103,  -46.3318,
           44.1294]], dtype=torch.float64)
	q_value: tensor([[-30.2900]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.46768723374107646, distance: 1.3863518030968836 entropy 6.692129093849127
epoch: 13, step: 26
	action: tensor([[-470.6909, -110.4561,  -25.0944,   19.0203, -260.9521,  -63.1132,
          169.2322]], dtype=torch.float64)
	q_value: tensor([[-27.4573]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3002881401115485, distance: 1.3048977850725718 entropy 7.081242439699281
epoch: 13, step: 27
	action: tensor([[ 147.2550,  -36.9356, -237.0465,  218.2133, -299.0585,   16.2401,
           54.6647]], dtype=torch.float64)
	q_value: tensor([[-29.5579]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 7.091267679783491
epoch: 13, step: 28
	action: tensor([[-124.5151, -233.8561,   12.4605,  120.8221,   44.7877, -191.3664,
         -176.3201]], dtype=torch.float64)
	q_value: tensor([[-30.2900]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2376964864929647, distance: 1.7118167959302477 entropy 6.692129093849127
epoch: 13, step: 29
	action: tensor([[  14.8301, -258.3640, -105.5586,  -86.9709, -471.3641,  597.0621,
          312.9379]], dtype=torch.float64)
	q_value: tensor([[-21.6064]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5972822777321173, distance: 0.726201370543802 entropy 6.9513237925603
epoch: 13, step: 30
	action: tensor([[-348.9446, -449.1314, -274.2701,  256.7918,   80.3679,   24.5900,
          334.7843]], dtype=torch.float64)
	q_value: tensor([[-18.0923]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1457939127842801, distance: 1.0576403817498963 entropy 6.689977975855441
epoch: 13, step: 31
	action: tensor([[-18.9371,  61.4558, 169.7682, 473.3057, 130.1991, 120.9862, 244.7430]],
       dtype=torch.float64)
	q_value: tensor([[-28.7361]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4636306894000688, distance: 1.3844346068719269 entropy 7.232099380316626
epoch: 13, step: 32
	action: tensor([[-621.6166, -492.2130, -150.0317,  264.5547, -227.5408, -475.1585,
         -143.5686]], dtype=torch.float64)
	q_value: tensor([[-23.7477]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1242532883588805, distance: 1.213357493270654 entropy 7.068526997892454
epoch: 13, step: 33
	action: tensor([[-652.5769,    2.1374,   38.4833,   23.9557,  118.3895,  -76.3819,
          -10.4719]], dtype=torch.float64)
	q_value: tensor([[-25.9192]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 7.183316178103142
epoch: 13, step: 34
	action: tensor([[ -87.0453,   72.1188,   61.6710,  362.6953, -303.2107,   91.8641,
          112.0222]], dtype=torch.float64)
	q_value: tensor([[-30.2900]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 6.692129093849127
epoch: 13, step: 35
	action: tensor([[-195.6710, -116.0274,   25.3964,  469.4668, -112.0962,   99.9765,
           84.7194]], dtype=torch.float64)
	q_value: tensor([[-30.2900]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3057153065892051, distance: 0.9535105192513957 entropy 6.692129093849127
epoch: 13, step: 36
	action: tensor([[   9.6728,  -98.1994,  311.0717, -146.6064,   80.2221,  140.3857,
          405.0548]], dtype=torch.float64)
	q_value: tensor([[-25.3334]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 6.952670204058464
epoch: 13, step: 37
	action: tensor([[-110.8121,  142.5920,  175.0680,  488.5716,  -61.4612,  303.3457,
          402.4966]], dtype=torch.float64)
	q_value: tensor([[-30.2900]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07078129469649075, distance: 1.1841509868496256 entropy 6.692129093849127
epoch: 13, step: 38
	action: tensor([[-537.5664, -102.2965, -115.7115, -152.5419,   62.4931, -156.1889,
          217.5743]], dtype=torch.float64)
	q_value: tensor([[-27.0476]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2949510669844795, distance: 0.9608737367465763 entropy 6.736200074218873
epoch: 13, step: 39
	action: tensor([[-675.3159, -277.4794,  204.8518, -154.1441,  279.7996,  147.3623,
         -303.3721]], dtype=torch.float64)
	q_value: tensor([[-37.5414]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4053932014504956, distance: 0.882412453160595 entropy 7.162442056483395
epoch: 13, step: 40
	action: tensor([[-862.5226, -656.3223,  -40.8959,  232.2133, -456.9678, -503.2271,
         -369.0358]], dtype=torch.float64)
	q_value: tensor([[-39.3175]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7131571431619457, distance: 1.4978050619371421 entropy 7.387861067150313
epoch: 13, step: 41
	action: tensor([[ 145.5573,  145.7592,  -86.5844, -146.4032,  248.4713, -250.5007,
         -292.5192]], dtype=torch.float64)
	q_value: tensor([[-33.6140]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 7.367379528189772
epoch: 13, step: 42
	action: tensor([[ -61.1861, -198.4048, -546.2991,  331.8890, -150.9757,  -28.6586,
         -268.2915]], dtype=torch.float64)
	q_value: tensor([[-30.2900]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.4463898147175138, distance: 1.7898618988978945 entropy 6.692129093849127
epoch: 13, step: 43
	action: tensor([[-606.8738, -612.7628, -154.3686,  103.4577,  -14.9023, -107.0110,
          -20.2580]], dtype=torch.float64)
	q_value: tensor([[-19.9244]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31697838428548, distance: 0.9457447025163339 entropy 6.835357206244185
epoch: 13, step: 44
	action: tensor([[-452.0063, -527.6612, -192.6392,    5.4316, -593.0223,  583.9316,
          548.7945]], dtype=torch.float64)
	q_value: tensor([[-28.6746]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 7.326493618675412
epoch: 13, step: 45
	action: tensor([[-176.0611, -364.1325, -148.6624, -234.0820,   29.0172,  185.3637,
         -236.7605]], dtype=torch.float64)
	q_value: tensor([[-30.2900]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 6.692129093849127
epoch: 13, step: 46
	action: tensor([[ 119.0856, -246.1866,   72.1894,   35.0920, -100.5173, -345.6432,
          239.8256]], dtype=torch.float64)
	q_value: tensor([[-30.2900]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35708361256081556, distance: 0.9175588768810389 entropy 6.692129093849127
epoch: 13, step: 47
	action: tensor([[-281.6251,  426.8155,  349.6834,  158.5127,  211.2906,  113.5454,
          210.0747]], dtype=torch.float64)
	q_value: tensor([[-26.9126]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 7.06910156328659
epoch: 13, step: 48
	action: tensor([[ -43.7767, -365.9258,  -36.5261,  350.8725,  411.9881,  -76.0881,
          305.5536]], dtype=torch.float64)
	q_value: tensor([[-30.2900]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 6.692129093849127
epoch: 13, step: 49
	action: tensor([[-148.3166, -355.6090,  101.1557, -123.1927, -176.6130, -105.9147,
          -56.1621]], dtype=torch.float64)
	q_value: tensor([[-30.2900]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.535733844151338, distance: 0.7797227503814013 entropy 6.692129093849127
epoch: 13, step: 50
	action: tensor([[ -67.8833, -125.7055,  -99.0314,  385.5058,    3.2709, -170.7546,
          361.6917]], dtype=torch.float64)
	q_value: tensor([[-24.4913]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6486639732314807, distance: 1.4693415751385699 entropy 6.96275877996102
epoch: 13, step: 51
	action: tensor([[-750.6962, -258.0203,  140.2492, -177.8632, -754.5801,  506.7353,
         -495.3381]], dtype=torch.float64)
	q_value: tensor([[-32.5636]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.065053802460711, distance: 1.6444564585555286 entropy 7.279818996321775
epoch: 13, step: 52
	action: tensor([[-494.5288, -335.2705,  -79.7802,  189.4813, -142.8859,  263.5025,
          170.4390]], dtype=torch.float64)
	q_value: tensor([[-28.0619]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6393076674515419, distance: 1.465166324891791 entropy 7.092434173433255
epoch: 13, step: 53
	action: tensor([[-168.2866, -132.7561,  367.0020,  267.9455,  155.4097, -823.9793,
         -209.4586]], dtype=torch.float64)
	q_value: tensor([[-25.5715]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07363389790577624, distance: 1.1857272499907485 entropy 7.078748634214823
epoch: 13, step: 54
	action: tensor([[-128.9964, -505.7511, -427.8372,  343.9827,  341.2096, -392.6046,
         -407.8687]], dtype=torch.float64)
	q_value: tensor([[-32.3103]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11965390710755153, distance: 1.0737011390201388 entropy 7.371570160502657
epoch: 13, step: 55
	action: tensor([[-1024.7322,  -822.2276,  -164.2629,   884.3317,     3.1280,   -24.3134,
           678.4754]], dtype=torch.float64)
	q_value: tensor([[-30.1182]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12598984460516793, distance: 1.214294226355947 entropy 7.270721932718117
epoch: 13, step: 56
	action: tensor([[-121.6758, -539.1866, -396.7514,  197.2733, -414.4692,  373.1805,
          217.1536]], dtype=torch.float64)
	q_value: tensor([[-27.2311]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.027638473032807087, distance: 1.1284194850382077 entropy 7.341414022308155
epoch: 13, step: 57
	action: tensor([[-471.2207, -207.6059,  161.7943,  -36.2099,   15.6490,  270.2132,
          219.0521]], dtype=torch.float64)
	q_value: tensor([[-27.7182]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.202337373316411, distance: 1.6982382470016646 entropy 7.213231442555691
epoch: 13, step: 58
	action: tensor([[-326.2907, -134.7905,  466.5775,  831.3579,  166.5193,  156.0700,
          -60.2637]], dtype=torch.float64)
	q_value: tensor([[-31.2208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.43511082830280534, distance: 1.3708799161973444 entropy 7.2512948930536085
epoch: 13, step: 59
	action: tensor([[ -584.9142,  -719.6147,   379.9497,  -216.8562,   433.9191, -1453.6791,
           656.7234]], dtype=torch.float64)
	q_value: tensor([[-35.5367]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9207743017445502, distance: 1.5859696113524797 entropy 7.606607140943047
epoch: 13, step: 60
	action: tensor([[-645.7949,  190.3802, -893.2327,  505.5977,   54.7211,  -63.5776,
          387.1798]], dtype=torch.float64)
	q_value: tensor([[-32.6001]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33508903705507087, distance: 0.9331220241831176 entropy 7.338700456432186
epoch: 13, step: 61
	action: tensor([[ 233.6196, -474.8133,  490.3520,  -41.3167,  426.4969, -262.7181,
         -499.7765]], dtype=torch.float64)
	q_value: tensor([[-31.2251]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2683573047578699, distance: 0.9788276159687276 entropy 7.304231193093807
epoch: 13, step: 62
	action: tensor([[-417.7329,  106.0646,  303.6424, -132.4940, -316.3757, -164.0034,
          105.3536]], dtype=torch.float64)
	q_value: tensor([[-27.5674]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.39750267381102145, distance: 1.352798200885069 entropy 7.189477796979362
epoch: 13, step: 63
	action: tensor([[-551.3210,   42.8197, -247.9782,  487.5435,  459.8212, -681.3188,
         -505.5213]], dtype=torch.float64)
	q_value: tensor([[-29.9817]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23048479283458112, distance: 1.0038418483203182 entropy 7.15485914953836
epoch: 13, step: 64
	action: tensor([[-1029.4428,   -92.9486,  -212.5209,   118.5749,  -316.4774,  -664.2107,
           449.8260]], dtype=torch.float64)
	q_value: tensor([[-40.4361]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10660674826265648, distance: 1.2037972598126474 entropy 7.38979674659384
epoch: 13, step: 65
	action: tensor([[-390.2432, -353.9450, -629.2753,  721.0889,  -99.1743,  190.3734,
          558.3619]], dtype=torch.float64)
	q_value: tensor([[-34.0011]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4913381313829077, distance: 1.3974772751759228 entropy 7.381296175940947
epoch: 13, step: 66
	action: tensor([[ 116.0409, -447.3786, -101.6914,  176.2271,  -26.0776, -131.2114,
          336.2840]], dtype=torch.float64)
	q_value: tensor([[-23.9153]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1999053715520649, distance: 1.2535168956952008 entropy 7.0534612922552276
epoch: 13, step: 67
	action: tensor([[-248.1593, -268.0853, -269.2981, -160.4904,  260.4693,  -67.0475,
          309.2798]], dtype=torch.float64)
	q_value: tensor([[-21.8438]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.35835533951879284, distance: 1.3337160895276683 entropy 7.140419049625281
epoch: 13, step: 68
	action: tensor([[-411.3972, -250.3925, -430.3024,  -92.1423,   -1.9486,   34.1189,
          -15.6481]], dtype=torch.float64)
	q_value: tensor([[-33.0643]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0075330820853639, distance: 1.6213920850276482 entropy 7.257701412994922
epoch: 13, step: 69
	action: tensor([[ -32.7625, -571.8373,  136.5721, -358.7139,  101.3274, -782.2302,
          387.2771]], dtype=torch.float64)
	q_value: tensor([[-33.7656]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18083706488695817, distance: 1.0357187535212509 entropy 7.355664777548824
epoch: 13, step: 70
	action: tensor([[-578.3219,   30.2703,   -9.1827, -130.2167,  116.2164,  495.3605,
         -132.2521]], dtype=torch.float64)
	q_value: tensor([[-32.7713]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1925504773546889, distance: 1.0282870923637175 entropy 7.193362571461516
epoch: 13, step: 71
	action: tensor([[-838.3675,  517.4190,  -32.7483,  188.8180,  175.2530,  -76.6017,
         -351.3235]], dtype=torch.float64)
	q_value: tensor([[-38.6659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03466411654832302, distance: 1.1640091306800542 entropy 7.394102662760122
epoch: 13, step: 72
	action: tensor([[-267.3590,  -90.1830,  -67.4671,  325.3884,  238.7398, -164.2517,
          107.0916]], dtype=torch.float64)
	q_value: tensor([[-32.2835]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7888131411568715, distance: 1.5305206079657196 entropy 6.771477613486601
epoch: 13, step: 73
	action: tensor([[-116.5427, -112.9728,  171.1672, -242.0109,   26.4970,  343.3180,
         -132.7525]], dtype=torch.float64)
	q_value: tensor([[-21.2084]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9429252369345342, distance: 1.5950883311996809 entropy 6.774830387590678
epoch: 13, step: 74
	action: tensor([[-176.2421, -223.2958,    8.5175,  251.4821,  -30.7960,  340.4976,
          149.9062]], dtype=torch.float64)
	q_value: tensor([[-26.7892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1453168716674449, distance: 1.2246712495747627 entropy 6.948524253147772
epoch: 13, step: 75
	action: tensor([[-294.0007,  497.9774, -462.5283, -134.6403,  248.6388,  356.9328,
         -487.2538]], dtype=torch.float64)
	q_value: tensor([[-26.3273]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29361924340469014, distance: 0.9617808443750188 entropy 7.079471984875849
epoch: 13, step: 76
	action: tensor([[-715.5845, -173.3961,  348.5664,  779.6258, -485.1044,  282.6979,
          315.7636]], dtype=torch.float64)
	q_value: tensor([[-37.1545]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3585639966336154, distance: 1.3338185217326088 entropy 7.353690508164271
epoch: 13, step: 77
	action: tensor([[-518.0924, -494.9134,   43.0759, -159.1455,  345.8273,  611.8832,
          344.8118]], dtype=torch.float64)
	q_value: tensor([[-35.1371]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0055497257382688, distance: 1.6205909542836852 entropy 7.370845650114594
epoch: 13, step: 78
	action: tensor([[-574.2126,  -31.0604,   19.0227,    2.4226,  106.7327,   74.0626,
          609.1517]], dtype=torch.float64)
	q_value: tensor([[-26.0249]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09500131918410037, distance: 1.1974682699235595 entropy 7.054444018266602
epoch: 13, step: 79
	action: tensor([[-422.6332,  253.8917, -249.6698,  147.5980, -174.8363,  170.1767,
          363.9180]], dtype=torch.float64)
	q_value: tensor([[-26.8583]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.33966307674684004, distance: 1.324507699116519 entropy 7.08862875635421
epoch: 13, step: 80
	action: tensor([[ 167.4704,    2.7990, -343.4275, -347.6309,  448.9918,   78.3424,
          104.2832]], dtype=torch.float64)
	q_value: tensor([[-30.6720]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 7.123128417142548
epoch: 13, step: 81
	action: tensor([[ -95.5635, -125.6832, -149.4279,  557.8254,  -80.2099,  101.5857,
          356.8951]], dtype=torch.float64)
	q_value: tensor([[-30.2900]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27464849066365615, distance: 0.9746102012022271 entropy 6.692129093849127
epoch: 13, step: 82
	action: tensor([[-459.2836, -289.6814,  268.6189,  493.8537,  237.8039, -504.9881,
         -130.2365]], dtype=torch.float64)
	q_value: tensor([[-24.7642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 6.914580081866225
epoch: 13, step: 83
	action: tensor([[-366.2682,  -25.4697,   99.9740, -199.7760, -228.4636, -293.1079,
          -54.4404]], dtype=torch.float64)
	q_value: tensor([[-30.2900]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16770527500135568, distance: 1.2365831213106304 entropy 6.692129093849127
epoch: 13, step: 84
	action: tensor([[-377.7514, -483.5996, -160.9306,    5.1813,   52.0016,  -47.6496,
          161.1001]], dtype=torch.float64)
	q_value: tensor([[-28.7095]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 7.12342405159171
epoch: 13, step: 85
	action: tensor([[-118.9795, -241.5978, -234.3867, -181.5878, -138.4019,  165.9404,
         -702.6781]], dtype=torch.float64)
	q_value: tensor([[-30.2900]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.29419710005807076, distance: 1.3018378807451887 entropy 6.692129093849127
epoch: 13, step: 86
	action: tensor([[-142.5503, -199.2537,   11.7734,  539.7775,  -22.9495, -137.1883,
         -137.9310]], dtype=torch.float64)
	q_value: tensor([[-29.9153]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.3545934314412844, distance: 1.7559601564595604 entropy 7.068325240502436
epoch: 13, step: 87
	action: tensor([[-392.0160,   25.8952, -241.6889,  589.8094, -113.2384,  222.7264,
         -177.9008]], dtype=torch.float64)
	q_value: tensor([[-24.5506]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12169463734588071, distance: 1.0724559436986212 entropy 6.980771522069236
epoch: 13, step: 88
	action: tensor([[-907.5039, -208.7561, -373.6944,  437.2390, -500.2960,  586.1830,
         -202.3644]], dtype=torch.float64)
	q_value: tensor([[-30.5530]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5913956380226235, distance: 1.443596348745431 entropy 7.228769285302517
epoch: 13, step: 89
	action: tensor([[-706.6175, -933.4408,   52.2577, -316.8149,  163.8029,  152.6878,
         -404.3857]], dtype=torch.float64)
	q_value: tensor([[-35.2732]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3806855925489152, distance: 1.3446340039710307 entropy 7.427951070001817
epoch: 13, step: 90
	action: tensor([[ -56.5059, -175.0124, -240.6587, -189.3297, -396.9460,   80.6542,
          268.3718]], dtype=torch.float64)
	q_value: tensor([[-27.7886]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3848031628793096, distance: 1.3466375387847729 entropy 7.164018048883373
epoch: 13, step: 91
	action: tensor([[-268.9999, -252.5504,   41.5647,  -60.2424,  125.9008,  111.1169,
           76.8005]], dtype=torch.float64)
	q_value: tensor([[-33.7699]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6956582026683726, distance: 1.4901358075723432 entropy 7.276620462698136
epoch: 13, step: 92
	action: tensor([[-833.0434, -660.3362,  439.2875,  327.8968,  192.3839,  205.2464,
          555.9284]], dtype=torch.float64)
	q_value: tensor([[-34.9276]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6430852062435854, distance: 1.4668534815868546 entropy 7.3389068836739595
epoch: 13, step: 93
	action: tensor([[-645.2661, -645.1168,  390.2086,  -53.7011, -240.6336, -538.7302,
          146.2898]], dtype=torch.float64)
	q_value: tensor([[-31.9904]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18636083845460538, distance: 1.246421966962287 entropy 7.299335049545346
epoch: 13, step: 94
	action: tensor([[-591.3331, -884.2756, -527.3879,  160.8735, -140.9255,  438.2039,
          533.7991]], dtype=torch.float64)
	q_value: tensor([[-29.0363]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9587166388003245, distance: 1.6015573672087164 entropy 7.23226103751667
epoch: 13, step: 95
	action: tensor([[ 177.2377,   29.8819,  348.8317, -114.7463,   59.3131,  -81.1446,
          -48.3242]], dtype=torch.float64)
	q_value: tensor([[-25.8935]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23081815489808355, distance: 1.0036243873543658 entropy 7.086189022639357
epoch: 13, step: 96
	action: tensor([[-272.3764,  -31.9754,  -43.1225, -143.8269, -170.4796,  206.8502,
          209.3867]], dtype=torch.float64)
	q_value: tensor([[-28.7649]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5927002492864679, distance: 1.4441879509170277 entropy 7.173007918212639
epoch: 13, step: 97
	action: tensor([[ 544.1718,  170.9647,  152.9178,  375.0708, -477.4437, -190.7007,
          387.6122]], dtype=torch.float64)
	q_value: tensor([[-26.9882]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 7.157353219991463
epoch: 13, step: 98
	action: tensor([[-258.6310,  -51.3010, -283.2018,  199.8325,   31.2227,   92.8699,
           64.5293]], dtype=torch.float64)
	q_value: tensor([[-30.2900]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9626315141794306, distance: 1.603157079899706 entropy 6.692129093849127
epoch: 13, step: 99
	action: tensor([[-533.0113, -209.4907, -353.4070,   -8.9307,  -89.2663,  -44.2997,
          101.4670]], dtype=torch.float64)
	q_value: tensor([[-27.4757]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3221823856057411, distance: 0.9421349579020314 entropy 6.764254507900142
epoch: 13, step: 100
	action: tensor([[-1169.6387,   168.0927,    86.4137,  -283.7310,    46.9774,    15.2693,
          -195.8863]], dtype=torch.float64)
	q_value: tensor([[-28.5668]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3241754126203985, distance: 0.9407488307979991 entropy 7.08641821474289
epoch: 13, step: 101
	action: tensor([[-220.4116, -787.9544,  216.2555,  403.6677, -111.9243,   88.6977,
          452.6746]], dtype=torch.float64)
	q_value: tensor([[-33.0910]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7963460645311107, distance: 1.5337398324476146 entropy 7.168228574971001
epoch: 13, step: 102
	action: tensor([[ 123.5443, -342.0103,  197.7397,  -62.2555,  -66.5168,   49.9819,
         -262.0367]], dtype=torch.float64)
	q_value: tensor([[-29.1113]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4019240053903561, distance: 1.3549364601793774 entropy 7.178613881334739
epoch: 13, step: 103
	action: tensor([[-685.0628,  -65.0642,  195.2290,  177.9841, -329.1984,  174.7544,
          512.0396]], dtype=torch.float64)
	q_value: tensor([[-32.0678]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9592436939802944, distance: 1.6017728277591876 entropy 7.279026865794821
epoch: 13, step: 104
	action: tensor([[-395.7506, -567.3512,  963.2794,  503.6568,   80.7815, -432.6783,
          -44.8511]], dtype=torch.float64)
	q_value: tensor([[-33.0472]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.014073194283586332, distance: 1.1523684107612469 entropy 7.410060268745341
epoch: 13, step: 105
	action: tensor([[-315.8325, -945.3670, -159.0679,   40.1657,  -43.2143, -275.3554,
          341.9944]], dtype=torch.float64)
	q_value: tensor([[-29.1148]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2612929621052009, distance: 1.2851821337490528 entropy 7.208484508976999
epoch: 13, step: 106
	action: tensor([[-986.6668,   28.6983,  338.7422,  333.6409, -857.8355,  434.3693,
          286.7940]], dtype=torch.float64)
	q_value: tensor([[-34.8419]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8912966646518032, distance: 1.5737528210072045 entropy 7.434311907809204
epoch: 13, step: 107
	action: tensor([[   5.8086, -575.7513,  -31.7651,  323.9266, -509.4208, -545.8942,
         -195.2796]], dtype=torch.float64)
	q_value: tensor([[-29.9891]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.055313584544963934, distance: 1.1755671931116178 entropy 7.263986726922795
epoch: 13, step: 108
	action: tensor([[-460.4748, -118.4045,  -58.2348, -162.2198,  599.0921,  307.7991,
           44.2087]], dtype=torch.float64)
	q_value: tensor([[-23.0536]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9587647814176614, distance: 1.6015770491494776 entropy 7.056249363001463
epoch: 13, step: 109
	action: tensor([[-483.5120,  -88.0253, -341.9931, -122.8888,   13.3378, -133.7795,
         -401.9527]], dtype=torch.float64)
	q_value: tensor([[-25.8380]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0822933129841852, distance: 1.6513063292215988 entropy 7.052375049266781
epoch: 13, step: 110
	action: tensor([[-738.9178, -107.0804,  -39.3785, -218.2042, -315.6442,  107.9034,
           49.8605]], dtype=torch.float64)
	q_value: tensor([[-25.2342]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.4138720246867567, distance: 1.7779265446241475 entropy 7.074896859401575
epoch: 13, step: 111
	action: tensor([[  31.5484,  -96.8037, -127.9699,  350.0357, -416.8162,  247.3290,
          -96.1360]], dtype=torch.float64)
	q_value: tensor([[-24.1263]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16367945617789226, distance: 1.2344496386894883 entropy 6.925979396296195
epoch: 13, step: 112
	action: tensor([[-443.7919, -198.9886, -460.6306,    4.1963,  753.4784,   93.1334,
          629.8486]], dtype=torch.float64)
	q_value: tensor([[-27.8278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 7.386709789954298
epoch: 13, step: 113
	action: tensor([[-569.5134, -127.3902,   75.0050,    7.2373,   53.9426, -221.6663,
           -6.6954]], dtype=torch.float64)
	q_value: tensor([[-30.2900]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07353615358017351, distance: 1.1856732740737228 entropy 6.692129093849127
epoch: 13, step: 114
	action: tensor([[-351.6661, -399.4294, -293.5205,  403.1421,  -90.5330, -619.0479,
          366.4284]], dtype=torch.float64)
	q_value: tensor([[-28.0075]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25988685589505534, distance: 0.9844774024453444 entropy 7.1088133717488
epoch: 13, step: 115
	action: tensor([[-398.0852, -475.8655, -178.1853,  881.2157,  -24.1650, -659.3985,
         -529.3903]], dtype=torch.float64)
	q_value: tensor([[-28.8537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08247306656853759, distance: 1.190598258379478 entropy 7.35441737522212
epoch: 13, step: 116
	action: tensor([[  41.5306, -497.2455,   13.6224,  852.0004,  -93.0399,   28.6111,
           25.3794]], dtype=torch.float64)
	q_value: tensor([[-33.8566]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6259916352068144, distance: 0.6998377272606949 entropy 7.2461930648246105
epoch: 13, step: 117
	action: tensor([[ 101.8074, -546.1741,  299.8923,  -86.4633,  -84.8470,   22.7320,
          317.0864]], dtype=torch.float64)
	q_value: tensor([[-23.6456]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.018166434170965085, distance: 1.1546917982977287 entropy 6.98165935896583
epoch: 13, step: 118
	action: tensor([[-374.8554, -407.5043, -409.7390, -126.4328,  113.4941,   -5.1374,
           12.8125]], dtype=torch.float64)
	q_value: tensor([[-32.4161]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.4576035076161022, distance: 1.7939593681422121 entropy 7.263332169062145
epoch: 13, step: 119
	action: tensor([[-182.7378, -458.6761,  166.9845,  261.8820,  172.0500,  152.3026,
          391.9461]], dtype=torch.float64)
	q_value: tensor([[-21.3920]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6265993716944069, distance: 1.459476119779158 entropy 6.878161867290856
epoch: 13, step: 120
	action: tensor([[-400.7852, -205.7651, -331.7379,  394.0686,  141.9400, -325.8408,
         -529.7596]], dtype=torch.float64)
	q_value: tensor([[-35.5126]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1156055501467167, distance: 1.0761670667742687 entropy 7.3911836653612255
epoch: 13, step: 121
	action: tensor([[  41.5090, -449.6717,   23.0129,  276.1657, -435.9628,   -2.5232,
         -547.3735]], dtype=torch.float64)
	q_value: tensor([[-34.0497]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 7.441007327057831
epoch: 13, step: 122
	action: tensor([[-133.0846, -271.1107, -207.9757,  247.2717,  -77.3253,   61.9338,
          170.3169]], dtype=torch.float64)
	q_value: tensor([[-30.2900]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9755372721937672, distance: 1.6084194166148498 entropy 6.692129093849127
epoch: 13, step: 123
	action: tensor([[-225.5850, -162.4607,  491.8825,  253.1160, -179.4049,  286.4332,
          -52.1256]], dtype=torch.float64)
	q_value: tensor([[-26.2983]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5056689633681526, distance: 1.4041756655723336 entropy 7.098528877932223
epoch: 13, step: 124
	action: tensor([[-197.8019, -500.2195, -279.7831,  297.0709, -360.9157,  118.7546,
          762.9512]], dtype=torch.float64)
	q_value: tensor([[-30.3688]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.37382956189583716, distance: 1.3412913440098357 entropy 7.30866096080193
epoch: 13, step: 125
	action: tensor([[-550.1275, -510.2345, -214.2502,  781.3449,   46.7916, -188.9991,
         -221.0147]], dtype=torch.float64)
	q_value: tensor([[-35.4661]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13689085264627088, distance: 1.063137783167983 entropy 7.492675311764672
epoch: 13, step: 126
	action: tensor([[  82.5169, -415.1152,   90.3566, -531.0955,   39.0267,  397.8329,
          468.9889]], dtype=torch.float64)
	q_value: tensor([[-31.9840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4143652748162159, distance: 1.3609353214558901 entropy 7.321517491870992
epoch: 13, step: 127
	action: tensor([[-586.6512, -440.8928, -129.4480,  -23.5450,   12.1039,  273.3082,
          111.0513]], dtype=torch.float64)
	q_value: tensor([[-26.0577]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9183319182573342, distance: 1.5849609613165587 entropy 7.133990175280276
LOSS epoch 13 actor 221.38818421806326 critic 246.29165377924937
epoch: 14, step: 0
	action: tensor([[-602.3097,  -47.8856, -427.5685, -245.9615, -105.1019, -132.9103,
         -176.4088]], dtype=torch.float64)
	q_value: tensor([[-30.7404]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.6777867971123261, distance: 1.8725985865271355 entropy 7.328740307873774
epoch: 14, step: 1
	action: tensor([[ 486.3394, -407.3585, -337.4609,  133.3676, -196.8773, -326.4068,
          132.8196]], dtype=torch.float64)
	q_value: tensor([[-20.1211]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12620901988977118, distance: 1.21441240252508 entropy 6.919770098621391
epoch: 14, step: 2
	action: tensor([[-179.0638,   92.2694,   53.7593,  508.1749, -384.5656,  -57.4250,
          277.3518]], dtype=torch.float64)
	q_value: tensor([[-26.8710]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.561327664308759, distance: 1.4298935945611833 entropy 7.281624363268113
epoch: 14, step: 3
	action: tensor([[ -88.7808, -621.3558,  143.6858, -128.3649, -426.0965,  557.3073,
          375.9086]], dtype=torch.float64)
	q_value: tensor([[-27.7900]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6165751990868249, distance: 1.4549720450526695 entropy 7.378904409827602
epoch: 14, step: 4
	action: tensor([[-858.9009, -436.6569,  170.1376, -511.9683,  -83.9156,  583.1106,
          243.7738]], dtype=torch.float64)
	q_value: tensor([[-27.0049]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03242990270756663, distance: 1.1256358394945256 entropy 7.22488984181566
epoch: 14, step: 5
	action: tensor([[ 249.0103, -727.4691, -633.4422, -137.9322, -645.0745, -188.9242,
         -351.7066]], dtype=torch.float64)
	q_value: tensor([[-30.8466]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4883800920445569, distance: 1.3960906531461479 entropy 7.3722751369503055
epoch: 14, step: 6
	action: tensor([[-222.1213, -252.8910,  -43.9827,  238.6218,  263.6099, -319.3219,
          143.8153]], dtype=torch.float64)
	q_value: tensor([[-22.2211]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.25677045051250036, distance: 1.2828759801592304 entropy 7.071252543300702
epoch: 14, step: 7
	action: tensor([[-329.3699, -387.9788,   21.8048,   83.7589, -170.6042,  283.9695,
         -395.5750]], dtype=torch.float64)
	q_value: tensor([[-23.8427]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.54405214030117, distance: 1.4219609637438126 entropy 7.049510306083103
epoch: 14, step: 8
	action: tensor([[-341.8326, -281.4385, -155.6906, -588.6160,  -11.0972,  593.6441,
          129.4860]], dtype=torch.float64)
	q_value: tensor([[-22.3065]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5520819055560076, distance: 1.4256535872691214 entropy 7.070891133734954
epoch: 14, step: 9
	action: tensor([[  76.4185, -198.9291, -450.9075,    2.2369, -935.2170, -382.4832,
          218.8200]], dtype=torch.float64)
	q_value: tensor([[-26.2358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.484124439787032, distance: 0.8219192839438397 entropy 7.2012576890275835
epoch: 14, step: 10
	action: tensor([[ 193.0539, -437.6890,  -34.8262, -275.6081, -312.0166,  122.5709,
          213.1797]], dtype=torch.float64)
	q_value: tensor([[-23.5542]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.036838935313346255, distance: 1.1230682571185246 entropy 7.012176020442483
epoch: 14, step: 11
	action: tensor([[-597.1447, -799.0116,  138.3668,  291.4863,  -93.4784,  234.5022,
          -41.6694]], dtype=torch.float64)
	q_value: tensor([[-31.5558]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22961578098137914, distance: 1.0044085065941932 entropy 7.36222212429203
epoch: 14, step: 12
	action: tensor([[  94.2266, -334.0780,   33.2786,   15.4507, -540.5085, -424.7804,
          353.2550]], dtype=torch.float64)
	q_value: tensor([[-26.3815]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03637753899203855, distance: 1.1233372243445647 entropy 7.165236028975346
epoch: 14, step: 13
	action: tensor([[-824.0271,  261.8925,  -77.6559,  569.2878,  316.3275, -221.1889,
          845.8548]], dtype=torch.float64)
	q_value: tensor([[-29.8387]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33283806343344957, distance: 0.9347001741106598 entropy 7.244293674953694
epoch: 14, step: 14
	action: tensor([[-433.7296,  658.5339,  500.3796,  453.6900,  318.5623,  470.3016,
         -569.0365]], dtype=torch.float64)
	q_value: tensor([[-41.3017]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06500468751035893, distance: 1.1809525626489594 entropy 7.579559358054091
epoch: 14, step: 15
	action: tensor([[ -467.4207, -1010.5804,  -230.0482,  -746.7004,  -654.6833,  -597.7918,
           743.0314]], dtype=torch.float64)
	q_value: tensor([[-38.9011]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.6055666832998834, distance: 1.8471739253340056 entropy 7.524005764773863
epoch: 14, step: 16
	action: tensor([[-798.3440, -911.7832,  183.1614,  302.7120, -202.0282,  109.2312,
         -182.3447]], dtype=torch.float64)
	q_value: tensor([[-27.2254]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3647617240478491, distance: 1.336857479425607 entropy 7.272647181716711
epoch: 14, step: 17
	action: tensor([[-273.8151, -130.3519, -390.7141, -203.2483,  386.2112,  134.9369,
          341.6749]], dtype=torch.float64)
	q_value: tensor([[-22.6068]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4728678181727062, distance: 1.3887963927135991 entropy 7.074513168229855
epoch: 14, step: 18
	action: tensor([[-279.6714, -351.9768, -761.3440, -285.5251,  189.1866, 1008.5545,
          877.7686]], dtype=torch.float64)
	q_value: tensor([[-32.3644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.35491573012595534, distance: 1.332026410098471 entropy 7.4130190532578615
epoch: 14, step: 19
	action: tensor([[-309.7110, -225.4404, -534.5835,  379.5661, -370.2947,  779.6605,
         -211.9828]], dtype=torch.float64)
	q_value: tensor([[-33.5225]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1682673267654562, distance: 1.685051218129334 entropy 7.429623330057827
epoch: 14, step: 20
	action: tensor([[ -18.5763,  -60.9444,  -33.9361,  321.1373, -109.5174,  263.3379,
           18.1642]], dtype=torch.float64)
	q_value: tensor([[-21.5835]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14255539065051637, distance: 1.2231939521181205 entropy 7.0237363356587315
epoch: 14, step: 21
	action: tensor([[-892.3058,  298.5152,  531.5301, -158.4904,   23.8357,  223.5805,
         1013.4655]], dtype=torch.float64)
	q_value: tensor([[-29.2250]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1432132277311189, distance: 1.223546034194577 entropy 7.440248790172804
epoch: 14, step: 22
	action: tensor([[-235.0821, -563.3780,  420.6421,  358.8724, -546.3478,  -12.0947,
          942.6697]], dtype=torch.float64)
	q_value: tensor([[-37.0026]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08638216242396668, distance: 1.192746103244374 entropy 7.279260893661536
epoch: 14, step: 23
	action: tensor([[ -25.3321, -237.1231,  -46.3347,   33.8403,  209.8969,   98.3116,
          -17.2590]], dtype=torch.float64)
	q_value: tensor([[-26.3170]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8313039176130594, distance: 1.5485916224923681 entropy 7.202060808646886
epoch: 14, step: 24
	action: tensor([[  46.1321, -463.2945,  871.8287, -236.1428, -289.8152,  592.0791,
          197.1627]], dtype=torch.float64)
	q_value: tensor([[-35.3421]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6156724336359205, distance: 1.4545657286855593 entropy 7.482532172321818
epoch: 14, step: 25
	action: tensor([[ -12.8265, -779.9890,  419.6899,  755.0370, -370.5520,  121.4154,
          362.7785]], dtype=torch.float64)
	q_value: tensor([[-24.8174]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3128489403088004, distance: 0.948599308132217 entropy 7.241986886573433
epoch: 14, step: 26
	action: tensor([[-665.1916,   16.3965,   66.9640, -170.6700, -195.3621,  306.6202,
          326.1251]], dtype=torch.float64)
	q_value: tensor([[-24.2084]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.612829907550823, distance: 1.4532856235394835 entropy 7.26899987750779
epoch: 14, step: 27
	action: tensor([[-238.0064, -495.2263,  278.4935,   17.9092,  341.6057,  398.4508,
          444.1506]], dtype=torch.float64)
	q_value: tensor([[-25.9291]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8072046934169821, distance: 1.5383684559591535 entropy 7.102541187982508
epoch: 14, step: 28
	action: tensor([[-125.0997, -250.4746, -261.5445,  218.2966,  403.2636,  416.8932,
          531.3076]], dtype=torch.float64)
	q_value: tensor([[-29.0015]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1008566165076088, distance: 1.2006656106131344 entropy 7.340880679833066
epoch: 14, step: 29
	action: tensor([[-446.4437, -475.1905, -478.0796, -182.4997, -284.0737,  384.0107,
         -365.0492]], dtype=torch.float64)
	q_value: tensor([[-31.4184]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7045876549013319, distance: 1.4940542342820984 entropy 7.463633146781939
epoch: 14, step: 30
	action: tensor([[-116.8523,  -56.1742,  -54.9662, -185.4591,  -30.1102,   16.0441,
          160.8281]], dtype=torch.float64)
	q_value: tensor([[-24.8784]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7730672918794819, distance: 1.5237695913339375 entropy 7.197953273750689
epoch: 14, step: 31
	action: tensor([[-650.7475,   -5.4499,   88.6114,   -9.4648, -424.1936,   87.5784,
         -445.9210]], dtype=torch.float64)
	q_value: tensor([[-28.7628]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 7.269896106369446
epoch: 14, step: 32
	action: tensor([[-188.7273, -201.1747,  126.9000, -115.8066,   -5.1625, -186.0727,
         -136.1154]], dtype=torch.float64)
	q_value: tensor([[-30.2740]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3278935558159324, distance: 1.3186766766644613 entropy 6.8158064858707705
epoch: 14, step: 33
	action: tensor([[-216.8794, -855.4923,  341.5701,  765.3340, -515.2626,  571.4932,
         -371.1540]], dtype=torch.float64)
	q_value: tensor([[-29.5275]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6018971670970117, distance: 1.448351621697183 entropy 7.238300685706538
epoch: 14, step: 34
	action: tensor([[  -5.7117, -107.9036,   40.6610,  400.6500, -708.2015,  -50.3672,
          274.5276]], dtype=torch.float64)
	q_value: tensor([[-23.4540]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14657011821305854, distance: 1.2253411058469579 entropy 7.063879435366898
epoch: 14, step: 35
	action: tensor([[ -41.1795, -202.9326, -468.2208,  416.5820, -319.1917, -352.1438,
          -41.4448]], dtype=torch.float64)
	q_value: tensor([[-33.4360]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16210591644634753, distance: 1.0474933033433378 entropy 7.3728181306438385
epoch: 14, step: 36
	action: tensor([[ -615.4617,  -586.2641,  -302.6862,   383.2671,  -301.2149, -1000.7574,
            73.3787]], dtype=torch.float64)
	q_value: tensor([[-37.5269]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6534719932921302, distance: 1.4714825452565672 entropy 7.499289695134805
epoch: 14, step: 37
	action: tensor([[ -95.9270, -396.4610, -175.9904,  109.9918, -820.4784,   45.3264,
         -261.1538]], dtype=torch.float64)
	q_value: tensor([[-28.6890]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.514628078528595, distance: 1.4083470715917898 entropy 7.467019305009133
epoch: 14, step: 38
	action: tensor([[-686.3108, -422.8067,  -87.9094,   58.3491, -294.3168,   80.5245,
         -123.9527]], dtype=torch.float64)
	q_value: tensor([[-34.1525]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06523247841245539, distance: 1.1810788512323842 entropy 7.420662342776678
epoch: 14, step: 39
	action: tensor([[ 174.6406, -314.0130, -309.6310,  394.0637,  307.7710, -122.1480,
          491.8207]], dtype=torch.float64)
	q_value: tensor([[-25.3621]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.35287115779428646, distance: 1.3310210145330597 entropy 7.167162019996302
epoch: 14, step: 40
	action: tensor([[-255.4076, -786.8598, -168.6769,  375.9204, -312.7355,   99.8426,
         -190.8744]], dtype=torch.float64)
	q_value: tensor([[-26.2731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09061132894670809, distance: 1.195065463018863 entropy 7.2787823108959975
epoch: 14, step: 41
	action: tensor([[-638.8069, -166.4202, 1202.5590,  562.0206,  -58.9403,  240.7179,
         -300.8203]], dtype=torch.float64)
	q_value: tensor([[-27.2125]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1907460689631273, distance: 1.2487234615082983 entropy 7.3767193429774
epoch: 14, step: 42
	action: tensor([[-495.3434,  363.8246,  -12.1634,  614.4384,  298.1862,  -75.5013,
          645.5188]], dtype=torch.float64)
	q_value: tensor([[-34.5459]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4342729527099023, distance: 0.8607165716419065 entropy 7.565020278444891
epoch: 14, step: 43
	action: tensor([[-849.8556, -805.9625,  295.4154,  333.9795,  240.0559,   34.5359,
          448.1323]], dtype=torch.float64)
	q_value: tensor([[-37.3386]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.018727086907489143, distance: 1.155009669718037 entropy 7.446527301051349
epoch: 14, step: 44
	action: tensor([[-480.3856, -122.9399, -167.2364,  764.7963, -477.0292, -815.7744,
           72.4166]], dtype=torch.float64)
	q_value: tensor([[-29.9664]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12518156504411926, distance: 1.213858314162755 entropy 7.328321391129287
epoch: 14, step: 45
	action: tensor([[-564.6278, -248.8224,  279.4740,  196.6288,  -96.6352,  276.2176,
         -294.6714]], dtype=torch.float64)
	q_value: tensor([[-32.2005]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.057957429313743036, distance: 1.1770388282013597 entropy 7.455432813694628
epoch: 14, step: 46
	action: tensor([[-1.1212e+03, -1.0129e+00,  7.2048e+00, -9.8991e+01,  3.7737e+02,
          8.5481e+01,  1.7890e+02]], dtype=torch.float64)
	q_value: tensor([[-33.5982]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18301209722175726, distance: 1.0343428251063855 entropy 7.5133007085092505
epoch: 14, step: 47
	action: tensor([[-293.7751, -410.2746,   53.8786,  -11.0971, -714.4213, -787.0251,
         -282.9845]], dtype=torch.float64)
	q_value: tensor([[-30.1524]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03198493941124925, distance: 1.1258946367971518 entropy 7.376957678044562
epoch: 14, step: 48
	action: tensor([[-184.5102, -626.1365, -141.0307,  -85.6497,  175.2298, -690.4262,
           20.5937]], dtype=torch.float64)
	q_value: tensor([[-33.9344]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9607745952021371, distance: 1.6023984970317884 entropy 7.411326862524996
epoch: 14, step: 49
	action: tensor([[-712.9908,   89.4215,  -85.9055,  547.0320, -141.0557,   62.9427,
          252.1262]], dtype=torch.float64)
	q_value: tensor([[-32.0523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24018049624518334, distance: 0.9974977204685618 entropy 7.491968407824231
epoch: 14, step: 50
	action: tensor([[  -74.6357, -1137.1274,  -495.4383,  -169.7547,   162.5494,   221.6898,
           303.9989]], dtype=torch.float64)
	q_value: tensor([[-29.2422]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7386077147737504, distance: 1.5088897017650402 entropy 7.414761339697445
epoch: 14, step: 51
	action: tensor([[-757.8904, -689.3999, -281.2329,  369.7939,  227.9980,  289.0169,
          226.7351]], dtype=torch.float64)
	q_value: tensor([[-25.5988]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.053188789279843385, distance: 1.113495302062338 entropy 7.239246117862409
epoch: 14, step: 52
	action: tensor([[-323.4670, -333.8291, -483.3324,   44.2392,  -90.1876,  319.7901,
          167.3119]], dtype=torch.float64)
	q_value: tensor([[-31.0575]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0397620082785197, distance: 1.6343551750997725 entropy 7.3608853019718605
epoch: 14, step: 53
	action: tensor([[-392.2975, -866.3942, -462.0406,  640.7310,  -18.5578, -337.0092,
          -48.4494]], dtype=torch.float64)
	q_value: tensor([[-31.6515]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.33804948015730685, distance: 1.3237097876525152 entropy 7.453254853936314
epoch: 14, step: 54
	action: tensor([[-142.7528, -341.4335,   -3.7784,   35.7021,  -84.5057,  548.1775,
           56.6200]], dtype=torch.float64)
	q_value: tensor([[-31.7121]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27205328230399095, distance: 0.9763521553859782 entropy 7.38301402346197
epoch: 14, step: 55
	action: tensor([[ -245.2740, -1059.4146,  -118.9381,   205.8942,    59.3220,   -34.5768,
           283.2439]], dtype=torch.float64)
	q_value: tensor([[-30.8041]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12888887410711725, distance: 1.068054646128507 entropy 7.451648814044476
epoch: 14, step: 56
	action: tensor([[-1014.7407,  -496.5714,   467.0487,   466.3883,  -359.9618,  -236.3651,
          -512.8135]], dtype=torch.float64)
	q_value: tensor([[-32.1753]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11982574687883574, distance: 1.2109659101964445 entropy 7.346706513189532
epoch: 14, step: 57
	action: tensor([[-791.9023, -639.6136,  329.5118, -110.1295, -579.9086, -137.5120,
          601.5255]], dtype=torch.float64)
	q_value: tensor([[-33.6732]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4201538384336969, distance: 1.3637174233862088 entropy 7.548121781698646
epoch: 14, step: 58
	action: tensor([[-445.6665, -124.0900,   90.4833,  487.8371, -230.7180,  203.5441,
           97.8390]], dtype=torch.float64)
	q_value: tensor([[-34.4128]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19059429278988194, distance: 1.029531939529417 entropy 7.494852065247015
epoch: 14, step: 59
	action: tensor([[-866.9723, -336.3550, -452.2996, -369.6331,  -57.3740, -888.7421,
         -131.3404]], dtype=torch.float64)
	q_value: tensor([[-33.1116]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.727851474948435, distance: 1.5042149382144945 entropy 7.507913182816977
epoch: 14, step: 60
	action: tensor([[-205.3484, -178.5835, -388.6886,  -51.4323,   98.8245, -173.2285,
          456.9852]], dtype=torch.float64)
	q_value: tensor([[-24.4347]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4780665147275387, distance: 1.3912452110014684 entropy 7.152075096737578
epoch: 14, step: 61
	action: tensor([[ 175.7941, -233.1974,  -17.8741,  174.3078, -633.6718,  446.9360,
         -295.2599]], dtype=torch.float64)
	q_value: tensor([[-27.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33668423039294226, distance: 0.9320020216819586 entropy 7.219808403239081
epoch: 14, step: 62
	action: tensor([[  65.8715, -324.0768, -201.5534, -155.1703, -107.8729, -469.8415,
          291.8780]], dtype=torch.float64)
	q_value: tensor([[-17.6776]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2128756010424906, distance: 1.2602735542869126 entropy 6.754887650542108
epoch: 14, step: 63
	action: tensor([[-122.3545,  -44.8043,  181.5252, -190.0723, -182.1403,  161.9694,
         -191.3436]], dtype=torch.float64)
	q_value: tensor([[-21.4299]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19533089530965886, distance: 1.0265151342842624 entropy 7.134073377201198
epoch: 14, step: 64
	action: tensor([[-360.8843, -230.6583, -313.9251,  304.8755, -128.1840,   60.1493,
         -190.0892]], dtype=torch.float64)
	q_value: tensor([[-27.0057]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.9352220170331322, distance: 1.9605466090896608 entropy 7.188723583605799
epoch: 14, step: 65
	action: tensor([[-446.8461, -432.0068,  238.9807,   94.1952, -432.1964, -339.7298,
          428.5240]], dtype=torch.float64)
	q_value: tensor([[-25.7578]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0153394523401014, distance: 1.6245414493971166 entropy 7.246048238870311
epoch: 14, step: 66
	action: tensor([[  37.4000, -159.1202, -416.4573, -309.1317, -117.8308,  640.4261,
          438.5181]], dtype=torch.float64)
	q_value: tensor([[-25.4104]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.31010149659432096, distance: 1.3098126022761025 entropy 7.1537489093122755
epoch: 14, step: 67
	action: tensor([[-460.5935,  -35.3730, -588.0903,  339.1651,   49.8189, -109.4950,
           73.8141]], dtype=torch.float64)
	q_value: tensor([[-22.5677]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.02256637482847168, distance: 1.1571840717625497 entropy 7.057837462684212
epoch: 14, step: 68
	action: tensor([[-875.1171,  442.4931, -618.4111, -461.4278, -167.0064,   27.6101,
          669.6846]], dtype=torch.float64)
	q_value: tensor([[-26.4783]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6421091928021323, distance: 1.4664177520885482 entropy 7.402291060551756
epoch: 14, step: 69
	action: tensor([[-211.1006, -403.0218, -786.4884,  377.5458, -396.5973,  321.6433,
         -115.3042]], dtype=torch.float64)
	q_value: tensor([[-35.7849]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.010473010412075268, distance: 1.138336117132076 entropy 7.345703393434273
epoch: 14, step: 70
	action: tensor([[-1114.6895, -1087.5973,   411.5768,  -717.9940,   254.7489,  -563.2367,
           577.8164]], dtype=torch.float64)
	q_value: tensor([[-30.9856]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0326251508603397, distance: 1.6314934734565545 entropy 7.47639784219652
epoch: 14, step: 71
	action: tensor([[-386.9648, -363.8723, -669.1685,  451.6506, -337.5957,  381.8426,
          983.2672]], dtype=torch.float64)
	q_value: tensor([[-34.9981]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.42791059418764954, distance: 1.3674366043432888 entropy 7.554501106324298
epoch: 14, step: 72
	action: tensor([[-177.8425, -684.4677, -757.5120,  528.9999, -508.7221,  191.9657,
          782.7811]], dtype=torch.float64)
	q_value: tensor([[-32.0176]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06137748954741773, distance: 1.1789398005284857 entropy 7.4946056978432285
epoch: 14, step: 73
	action: tensor([[-1507.5329,  -681.6412,   101.1388,  -209.2395,  -187.4979,   140.1585,
           319.7329]], dtype=torch.float64)
	q_value: tensor([[-34.5690]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5104970084264482, distance: 1.4064251631565767 entropy 7.542661116877933
epoch: 14, step: 74
	action: tensor([[-11.2427, 323.6524, 115.8613,  31.9068, -21.4836, 420.4807,  54.9207]],
       dtype=torch.float64)
	q_value: tensor([[-22.9118]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44417106843636556, distance: 0.8531536783373346 entropy 7.084124996340167
epoch: 14, step: 75
	action: tensor([[-263.2654, -246.2435,  478.6929,  -86.0229,  -81.4170, -763.3400,
         -542.1159]], dtype=torch.float64)
	q_value: tensor([[-31.6199]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -2.166329753480648, distance: 2.0362671929553287 entropy 7.362760780909113
epoch: 14, step: 76
	action: tensor([[ -98.2399, -275.6036,   24.8810,  283.0540, -247.7705,  -16.0256,
          -45.6457]], dtype=torch.float64)
	q_value: tensor([[-21.9185]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5533222411732355, distance: 1.4262231241530599 entropy 6.821974758889725
epoch: 14, step: 77
	action: tensor([[-162.1547, -380.5700, -447.5302,   63.4283,  440.1328,  254.8281,
          -48.5976]], dtype=torch.float64)
	q_value: tensor([[-29.9454]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10978610500920738, distance: 1.079701929865363 entropy 7.435677223514166
epoch: 14, step: 78
	action: tensor([[-696.1162, -280.3283, -348.3575, -447.7328,   32.6246, -705.0072,
          654.6852]], dtype=torch.float64)
	q_value: tensor([[-29.7443]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1838303990613568, distance: 1.6910877626104275 entropy 7.383806281724809
epoch: 14, step: 79
	action: tensor([[-222.4897,  -15.3005,  218.3803,  856.9217,   68.8846,   49.9725,
          197.6062]], dtype=torch.float64)
	q_value: tensor([[-30.1484]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 7.342567657726566
epoch: 14, step: 80
	action: tensor([[-221.7902, -225.3925,  -91.8315,   34.8509, -232.5106,   75.3444,
         -453.7186]], dtype=torch.float64)
	q_value: tensor([[-30.2740]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.049250997034278, distance: 1.638152280687029 entropy 6.8158064858707705
epoch: 14, step: 81
	action: tensor([[-494.1133, -213.8725,  563.0888,  484.7543, -465.3931,  557.8651,
          234.0354]], dtype=torch.float64)
	q_value: tensor([[-30.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13244726044484123, distance: 1.21777116393309 entropy 7.281121481081356
epoch: 14, step: 82
	action: tensor([[ 129.8048,   49.4676, -235.8629,   32.8019, -201.6442, -326.1467,
          620.4558]], dtype=torch.float64)
	q_value: tensor([[-30.0666]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0969962713346304, distance: 1.1985585904486151 entropy 7.305474855044191
epoch: 14, step: 83
	action: tensor([[-102.6484,  190.1781, -190.8847,  695.6479, -775.9928, -238.8641,
           -9.6148]], dtype=torch.float64)
	q_value: tensor([[-27.9669]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2458721042840999, distance: 0.9937547012789568 entropy 7.14922643118981
epoch: 14, step: 84
	action: tensor([[-714.6746, -586.2167, -720.7638, -154.8592, -592.2448,  270.2237,
          398.2725]], dtype=torch.float64)
	q_value: tensor([[-40.6449]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1639882312227865, distance: 1.6833876649140596 entropy 7.7068312497452585
epoch: 14, step: 85
	action: tensor([[-844.3943, -646.0291,  -31.4647,  -83.1138, -119.9989,   -9.5168,
          -58.2664]], dtype=torch.float64)
	q_value: tensor([[-31.9427]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24538400700174856, distance: 1.277051276189598 entropy 7.455256030780589
epoch: 14, step: 86
	action: tensor([[-366.7397,  184.6362,    4.6036,  455.7463,  208.6795,   87.8346,
         -258.7149]], dtype=torch.float64)
	q_value: tensor([[-30.9061]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5333730966144843, distance: 1.417035116306188 entropy 7.221560552148155
epoch: 14, step: 87
	action: tensor([[-121.9119,  128.3098, -362.0114,   13.0574,   12.0861, -206.4253,
          273.1113]], dtype=torch.float64)
	q_value: tensor([[-26.7174]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07206458600779353, distance: 1.1023400034429356 entropy 6.93475012015108
epoch: 14, step: 88
	action: tensor([[-344.4663, -771.2678, -471.2341, 1168.7945,  154.5384,   13.8624,
         -357.2900]], dtype=torch.float64)
	q_value: tensor([[-38.4985]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.32377097390098464, distance: 1.3166281018421933 entropy 7.471673462789795
epoch: 14, step: 89
	action: tensor([[-538.5539,  861.8432,  -56.3387, -126.8946,  123.6666,  364.9290,
         -594.6725]], dtype=torch.float64)
	q_value: tensor([[-32.7611]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3804265058903078, distance: 1.3445078372760788 entropy 7.572133310864472
epoch: 14, step: 90
	action: tensor([[-558.3177, -658.2259, -116.5742,  868.0740, -150.4971,  655.6257,
         -190.1610]], dtype=torch.float64)
	q_value: tensor([[-34.3524]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.022215374111941744, distance: 1.1315618459310326 entropy 7.280698127944116
epoch: 14, step: 91
	action: tensor([[ 415.5157, -975.0869,  119.0540, -685.4713,  535.3478,  490.9450,
          -36.2430]], dtype=torch.float64)
	q_value: tensor([[-33.7619]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24815337174794583, distance: 0.9922504887076657 entropy 7.557348838513483
epoch: 14, step: 92
	action: tensor([[ -36.8149,  -33.5298, -204.0043,  216.1635, -231.5246, -314.4608,
          223.9725]], dtype=torch.float64)
	q_value: tensor([[-24.5405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.743416087388562, distance: 1.5109747881201394 entropy 7.207028236924112
epoch: 14, step: 93
	action: tensor([[ 179.4818, -245.7958,   88.5249, -461.3589, -346.6950,  850.1095,
          405.1652]], dtype=torch.float64)
	q_value: tensor([[-26.2826]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04543117106588057, distance: 1.1700499861749782 entropy 7.426069782410548
epoch: 14, step: 94
	action: tensor([[-444.6767, -901.1380,  -85.1325,  754.3032, -254.9026, -488.2672,
          283.3273]], dtype=torch.float64)
	q_value: tensor([[-26.0589]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44197577289889844, distance: 0.8548368210089353 entropy 7.278980677077785
epoch: 14, step: 95
	action: tensor([[-397.1700,  -88.1952,  551.8519,  294.0084,  291.4515, -742.3637,
         -382.7192]], dtype=torch.float64)
	q_value: tensor([[-40.0957]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18709938706340046, distance: 1.0317522360720868 entropy 7.462388441951871
epoch: 14, step: 96
	action: tensor([[-420.2396, -292.3384, -251.4899,  473.6859, -242.5452,  694.7847,
          -59.5320]], dtype=torch.float64)
	q_value: tensor([[-30.7523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.754418795367626, distance: 1.5157351740940594 entropy 7.395685522803122
epoch: 14, step: 97
	action: tensor([[-783.8934, -412.6204,  327.5414,  327.7111,  361.6410,  301.1964,
           84.4101]], dtype=torch.float64)
	q_value: tensor([[-33.4376]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7708344681777028, distance: 1.5228098475379592 entropy 7.410208611617887
epoch: 14, step: 98
	action: tensor([[-154.3126,  286.3264,  718.6264,  376.7425, -260.7501,  731.8906,
          210.2540]], dtype=torch.float64)
	q_value: tensor([[-33.9296]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07569577377610726, distance: 1.1868652774586674 entropy 7.560731467720183
epoch: 14, step: 99
	action: tensor([[-233.4552, -585.7902, -949.1072,  603.3975, -472.3532,  -58.1930,
          769.1045]], dtype=torch.float64)
	q_value: tensor([[-37.9282]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18689476990430942, distance: 1.0318820803142532 entropy 7.625210320729713
epoch: 14, step: 100
	action: tensor([[-333.6223, -586.7043,  340.9936,   59.1830,  108.8208, -119.0557,
          498.2834]], dtype=torch.float64)
	q_value: tensor([[-25.3538]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18608629345391847, distance: 1.2462777365103435 entropy 7.309826109471052
epoch: 14, step: 101
	action: tensor([[-617.2341, -489.0490, -459.0462,  433.7641,  227.0704,  607.5260,
          305.5183]], dtype=torch.float64)
	q_value: tensor([[-32.7275]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09139249131722416, distance: 1.090799308640881 entropy 7.383535411516207
epoch: 14, step: 102
	action: tensor([[ -817.2522, -1072.7820,    91.5184,    36.6243,  -185.2758,   290.5936,
          -151.3211]], dtype=torch.float64)
	q_value: tensor([[-28.6402]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7995487644603332, distance: 1.535106473612582 entropy 7.376125504368203
epoch: 14, step: 103
	action: tensor([[-769.5319,   27.0306, -120.9683,  -12.4742, -533.7730, -296.1816,
          300.4329]], dtype=torch.float64)
	q_value: tensor([[-31.9106]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.3228274299777891, distance: 1.744075037670788 entropy 7.563816472590598
epoch: 14, step: 104
	action: tensor([[-926.1495,  -64.2821,   60.8764, -115.8415,   38.9633,  420.5231,
          663.3030]], dtype=torch.float64)
	q_value: tensor([[-27.8164]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6096212067863573, distance: 1.4518392588917426 entropy 7.180689962956132
epoch: 14, step: 105
	action: tensor([[-647.7964,   -5.6824,  273.7154, -362.3032, -482.9194, -196.2697,
         -627.3719]], dtype=torch.float64)
	q_value: tensor([[-33.6197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44833306878752854, distance: 0.8499535048830654 entropy 7.317093533847992
epoch: 14, step: 106
	action: tensor([[-253.9610, -729.8647,  419.9960,  179.8452, -582.9583,  -76.8666,
          430.3592]], dtype=torch.float64)
	q_value: tensor([[-35.5568]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.47696352783643037, distance: 1.3907260152815082 entropy 7.31715957343049
epoch: 14, step: 107
	action: tensor([[-179.7770, -827.7341,   38.6956,  371.2679,  900.1108,  127.6548,
          785.7620]], dtype=torch.float64)
	q_value: tensor([[-29.1457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.197645901258917, distance: 1.6964284686941287 entropy 7.328967857408699
epoch: 14, step: 108
	action: tensor([[-378.0572,  -61.3852, -120.6056, -186.2578, -114.6727,  219.7273,
          315.9219]], dtype=torch.float64)
	q_value: tensor([[-22.0183]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6680532603592244, distance: 1.477956493554553 entropy 7.019251687687358
epoch: 14, step: 109
	action: tensor([[-338.1706, -117.2908,  -87.3186,  -44.9015,  241.9546, -165.3932,
          805.4676]], dtype=torch.float64)
	q_value: tensor([[-30.1796]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1214731656126982, distance: 1.6667691925742434 entropy 7.333565756840662
epoch: 14, step: 110
	action: tensor([[-534.6691,   60.4194, -463.9724,   57.9357,  228.7525,  616.9614,
         -756.0804]], dtype=torch.float64)
	q_value: tensor([[-27.5430]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6150991278400395, distance: 1.4543076364711882 entropy 7.293366605416101
epoch: 14, step: 111
	action: tensor([[ -92.7113,  147.3682, -367.0907,  253.4716,  177.0618,  683.1529,
         -153.9062]], dtype=torch.float64)
	q_value: tensor([[-34.1937]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.279913302121247, distance: 1.29463388446125 entropy 7.359702664735145
epoch: 14, step: 112
	action: tensor([[ -10.8274, -811.8036,  625.8713, -275.9577, -215.3697,  443.0340,
         -291.9805]], dtype=torch.float64)
	q_value: tensor([[-35.5720]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 7.476002429232559
epoch: 14, step: 113
	action: tensor([[ -16.8584,  172.4974, -182.1656,  268.3493,  135.5309,  148.8003,
            6.1252]], dtype=torch.float64)
	q_value: tensor([[-30.2740]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 6.8158064858707705
epoch: 14, step: 114
	action: tensor([[-200.0212, -173.1777, -387.0402,  451.4293,  142.4487,  319.9840,
         -129.2003]], dtype=torch.float64)
	q_value: tensor([[-30.2740]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6760744390093716, distance: 1.4815057613655072 entropy 6.8158064858707705
epoch: 14, step: 115
	action: tensor([[ -36.7119, -497.0014, -212.8851,  414.7570,  259.8244,  -73.3122,
          874.3266]], dtype=torch.float64)
	q_value: tensor([[-33.5527]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42931552769081216, distance: 0.8644795443547127 entropy 7.4647086986733076
epoch: 14, step: 116
	action: tensor([[-160.0902, -119.7590,   18.5974,  556.2083, -348.2831,  -10.6088,
         -109.4539]], dtype=torch.float64)
	q_value: tensor([[-28.0061]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 7.2231611048022994
epoch: 14, step: 117
	action: tensor([[ -78.4476, -384.3359,  -57.8457,   -7.6465,   18.7773,   30.1991,
           72.2867]], dtype=torch.float64)
	q_value: tensor([[-30.2740]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5650203482277711, distance: 1.4315835110604918 entropy 6.8158064858707705
epoch: 14, step: 118
	action: tensor([[-337.0541,   42.5873,  631.6274,   89.0385, -105.8153,  287.4510,
           94.8404]], dtype=torch.float64)
	q_value: tensor([[-25.5608]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2569304583037653, distance: 1.7191579584606633 entropy 7.092740494589383
epoch: 14, step: 119
	action: tensor([[-426.2373, -509.2988, -109.1689,   48.4103,  832.2044, -103.9488,
          306.3653]], dtype=torch.float64)
	q_value: tensor([[-31.6736]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.6845389285425614, distance: 1.874958010964702 entropy 7.186386969199013
epoch: 14, step: 120
	action: tensor([[-493.1110, -338.0065,  163.7403, -151.0383,  921.4095, -141.0899,
          670.2344]], dtype=torch.float64)
	q_value: tensor([[-30.3413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.02681218153868148, distance: 1.128898836285951 entropy 7.359752409142025
epoch: 14, step: 121
	action: tensor([[-288.3229, -629.8863, -267.6779,  177.1627,  264.8385,   26.3200,
         -649.1561]], dtype=torch.float64)
	q_value: tensor([[-32.6417]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0429199584867956, distance: 1.635619836405414 entropy 7.346739262179801
epoch: 14, step: 122
	action: tensor([[-118.2379, -171.7936, -491.7027, -108.3863, -871.4037, -468.4078,
         -341.1971]], dtype=torch.float64)
	q_value: tensor([[-26.1550]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4669320427702328, distance: 1.3859950870841913 entropy 7.218915587137963
epoch: 14, step: 123
	action: tensor([[-458.8147,   98.1902, -292.2479,  -99.9680, -239.2431, -129.1588,
         -266.2010]], dtype=torch.float64)
	q_value: tensor([[-31.8661]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.061989010679558754, distance: 1.108308480792272 entropy 7.2984043661532585
epoch: 14, step: 124
	action: tensor([[-275.1462,   43.3097,  191.6159,  533.2563, -663.6359, -345.4507,
           47.8161]], dtype=torch.float64)
	q_value: tensor([[-38.4459]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.257349906280812, distance: 1.283171692164668 entropy 7.387305250700529
epoch: 14, step: 125
	action: tensor([[ 218.4884, -532.8328, -166.8023,  668.6740, -588.6262,  244.9126,
          363.1837]], dtype=torch.float64)
	q_value: tensor([[-38.7785]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4724103863375424, distance: 1.388580715173678 entropy 7.4221121758848465
epoch: 14, step: 126
	action: tensor([[  68.1466, -228.2914, -332.7618,   38.8185, -430.0027,   63.9730,
         -154.6899]], dtype=torch.float64)
	q_value: tensor([[-33.2176]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16973553034149447, distance: 1.2376576600743836 entropy 7.410639306609083
epoch: 14, step: 127
	action: tensor([[-605.4010, -963.8261,  352.6778,  544.4485, -185.4571,  164.0205,
         -124.4761]], dtype=torch.float64)
	q_value: tensor([[-29.9476]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.27937929898500746, distance: 1.2943637838699664 entropy 7.4848441849550715
LOSS epoch 14 actor 256.1487672126699 critic 131.69574998974315
epoch: 15, step: 0
	action: tensor([[-874.8391, -240.9974,   18.7074, -626.6518, -935.8940, 1048.1456,
          672.3881]], dtype=torch.float64)
	q_value: tensor([[-32.3233]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6357526746762541, distance: 1.4635767872064616 entropy 7.589044245129059
epoch: 15, step: 1
	action: tensor([[-222.0854, -354.1980, -148.1448,  657.1651,  115.4384,  -76.4084,
          356.9556]], dtype=torch.float64)
	q_value: tensor([[-20.6124]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38911560456530714, distance: 0.8944090996627876 entropy 7.138300530595708
epoch: 15, step: 2
	action: tensor([[ 149.9986,  268.1800,  112.6490, -691.9306, -189.4389, -230.8394,
           82.1840]], dtype=torch.float64)
	q_value: tensor([[-26.8563]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 7.4646322134388114
epoch: 15, step: 3
	action: tensor([[ -56.8732, -300.8267,  442.8713,   83.2484,  -28.2265, -203.0606,
          351.7143]], dtype=torch.float64)
	q_value: tensor([[-28.6631]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1242376628602615, distance: 1.2133490612821025 entropy 6.9332735429202765
epoch: 15, step: 4
	action: tensor([[  55.3314, -484.2617, -726.1830,  741.7944, -526.5879,  699.6453,
         -233.8454]], dtype=torch.float64)
	q_value: tensor([[-28.3144]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2003766480871818, distance: 1.2537630380654454 entropy 7.6757995924684765
epoch: 15, step: 5
	action: tensor([[-1040.7528,  -514.6501,   368.0533,   328.4055,   404.0659,    86.0702,
           482.9959]], dtype=torch.float64)
	q_value: tensor([[-27.6433]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6223570614733336, distance: 1.4575716579723836 entropy 7.498483068021934
epoch: 15, step: 6
	action: tensor([[-3.3581e+02, -2.9454e+02,  2.8507e+02,  7.0757e+02,  6.1050e-01,
          3.1410e+02,  6.5228e+02]], dtype=torch.float64)
	q_value: tensor([[-31.4834]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.007272389758494091, distance: 1.1484977748904703 entropy 7.58739310146849
epoch: 15, step: 7
	action: tensor([[  11.7785, -417.4201,  651.4729,   70.3962,  185.9126, -593.6867,
          566.0514]], dtype=torch.float64)
	q_value: tensor([[-27.9790]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14571587142306597, distance: 1.0576886943357904 entropy 7.530039887738214
epoch: 15, step: 8
	action: tensor([[-1172.6987,   585.2435,  -572.5011,  1378.6776,  -534.3550,   -78.4723,
           498.7222]], dtype=torch.float64)
	q_value: tensor([[-30.0041]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3507572842992498, distance: 0.9220622374813189 entropy 7.5483571536282446
epoch: 15, step: 9
	action: tensor([[-848.7290, -520.2349, -254.8374,  207.3170,  189.6050,  439.7602,
          -98.7438]], dtype=torch.float64)
	q_value: tensor([[-28.5627]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16467646853957119, distance: 1.234978348736879 entropy 7.516854706335307
epoch: 15, step: 10
	action: tensor([[ 356.7444, -671.1310,  187.9002,  623.2448,  -35.2183, -341.2104,
         1218.2827]], dtype=torch.float64)
	q_value: tensor([[-31.3446]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13259777856810995, distance: 1.0657785082080582 entropy 7.601412352710668
epoch: 15, step: 11
	action: tensor([[-201.2184,  161.8253, -304.7368, -295.0401,  -26.7428,   31.6660,
         -308.4298]], dtype=torch.float64)
	q_value: tensor([[-27.7893]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7753676855075147, distance: 1.5247577471893972 entropy 7.394027219421218
epoch: 15, step: 12
	action: tensor([[-315.1672, -210.2102,  -16.2580, -516.2739, -812.5558, -101.8662,
          361.6935]], dtype=torch.float64)
	q_value: tensor([[-34.3970]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8885972524784183, distance: 1.5726293260706956 entropy 7.379588946403785
epoch: 15, step: 13
	action: tensor([[-1082.8493,   -47.4971,  -231.9738,  -397.9436,   152.8598,   182.3191,
          -124.4636]], dtype=torch.float64)
	q_value: tensor([[-33.7819]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19057733688516387, distance: 1.029542723091908 entropy 7.585287730594978
epoch: 15, step: 14
	action: tensor([[ 258.3494, -549.7846, -479.3221,  653.9368,   74.3644,  173.5394,
          293.0021]], dtype=torch.float64)
	q_value: tensor([[-27.0176]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26966987313194335, distance: 0.9779492125336778 entropy 7.51253324855408
epoch: 15, step: 15
	action: tensor([[ 121.1388, -214.3644, -139.0945,   21.9972,  -89.8254, -595.7800,
           51.9873]], dtype=torch.float64)
	q_value: tensor([[-20.4588]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7223046061168065, distance: 0.6030329340685391 entropy 7.303144048064616
epoch: 15, step: 16
	action: tensor([[-187.8395, -271.2452, -349.2918,  398.8187,  217.4096,  299.8338,
          858.4710]], dtype=torch.float64)
	q_value: tensor([[-23.6176]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3804255631077138, distance: 1.344507378150295 entropy 7.487949834484972
epoch: 15, step: 17
	action: tensor([[-950.4343, -604.3454, -404.0645,   85.7418, -431.4309,   69.5565,
           56.2620]], dtype=torch.float64)
	q_value: tensor([[-24.5908]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8140648124859717, distance: 1.5412855009009296 entropy 7.706024703849205
epoch: 15, step: 18
	action: tensor([[-986.2289, -323.5063, -444.0718,  741.0731, -369.9082,   88.8163,
          841.3466]], dtype=torch.float64)
	q_value: tensor([[-29.1074]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.003646282510202603, distance: 1.146428656867982 entropy 7.5257435108698285
epoch: 15, step: 19
	action: tensor([[ -93.9749,  -94.2176, -270.5022,  190.8852, -276.5530, -127.1341,
          490.6338]], dtype=torch.float64)
	q_value: tensor([[-28.6978]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4745342146384648, distance: 1.3895818098259747 entropy 7.540166209465204
