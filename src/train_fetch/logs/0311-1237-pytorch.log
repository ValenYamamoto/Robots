epoch: 0, step: 0
	action: tensor([[-0.2437,  0.0097, -0.6718, -0.0397,  0.5068, -0.5371,  0.0905]],
       dtype=torch.float64)
	q_value: tensor([[-25.4870]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03394486878144265, distance: 1.1247542660810592 entropy 0.9217694997787476
epoch: 0, step: 1
	action: tensor([[-0.8278,  0.3994,  0.4714, -0.2171,  0.4870,  0.2565, -0.4830]],
       dtype=torch.float64)
	q_value: tensor([[-18.0178]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4099822948851599, distance: 1.3588249823931735 entropy 0.9217694997787476
epoch: 0, step: 2
	action: tensor([[ 0.9252,  0.3784,  0.0726, -0.4440,  0.0733, -0.7493, -0.3329]],
       dtype=torch.float64)
	q_value: tensor([[-17.2397]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6901316705128893, distance: 0.6370085898548131 entropy 0.9217694997787476
epoch: 0, step: 3
	action: tensor([[ 0.8865,  0.3207,  0.7328,  0.2386,  0.0255,  0.1401, -0.3469]],
       dtype=torch.float64)
	q_value: tensor([[-23.6189]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9542532182637878, distance: 0.2447580400326826 entropy 0.9217694997787476
epoch: 0, step: 4
	action: tensor([[ 0.2456, -0.4502,  1.0619, -0.8945, -0.3760, -0.4232, -0.4262]],
       dtype=torch.float64)
	q_value: tensor([[-23.5360]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4533137592610985, distance: 1.379546634020416 entropy 0.9217694997787476
epoch: 0, step: 5
	action: tensor([[ 0.9504,  0.5578,  0.2390,  0.6065,  0.3042, -0.3170, -0.1627]],
       dtype=torch.float64)
	q_value: tensor([[-26.4307]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9499739962416246, distance: 0.25594968474855767 entropy 0.9217694997787476
epoch: 0, step: 6
	action: tensor([[-0.3805,  0.0265,  0.8083, -0.2780,  0.1645, -0.2570, -0.3072]],
       dtype=torch.float64)
	q_value: tensor([[-23.0079]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.389479490738321, distance: 1.348909345864197 entropy 0.9217694997787476
epoch: 0, step: 7
	action: tensor([[ 0.2671, -0.0958, -0.5527,  0.4342, -0.5510, -0.9386,  0.5841]],
       dtype=torch.float64)
	q_value: tensor([[-19.1551]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6239049602090143, distance: 0.7017872861564026 entropy 0.9217694997787476
epoch: 0, step: 8
	action: tensor([[ 0.5576,  0.7795,  0.5342, -0.6464, -0.2986, -0.4019,  0.6841]],
       dtype=torch.float64)
	q_value: tensor([[-22.8256]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8073368674881185, distance: 0.5022917000581754 entropy 0.9217694997787476
epoch: 0, step: 9
	action: tensor([[-0.0887,  0.2769, -0.2957,  1.1875,  0.0105,  0.9318, -0.0451]],
       dtype=torch.float64)
	q_value: tensor([[-22.1749]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5045363892734478, distance: 0.8054944917504477 entropy 0.9217694997787476
epoch: 0, step: 10
	action: tensor([[ 0.7495,  0.9482, -0.7308, -0.1277, -1.0252,  0.7510, -0.2943]],
       dtype=torch.float64)
	q_value: tensor([[-19.8353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9268956205586065, distance: 0.3094057573658952 entropy 0.9217694997787476
epoch: 0, step: 11
	action: tensor([[ 0.2197,  0.5133, -0.5435,  0.0837,  0.1448, -0.9578,  0.7366]],
       dtype=torch.float64)
	q_value: tensor([[-24.6039]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7919210954447535, distance: 0.5220002604373426 entropy 0.9217694997787476
epoch: 0, step: 12
	action: tensor([[-0.5048, -1.2774,  0.2607,  0.8618, -0.9692,  0.6757,  0.1657]],
       dtype=torch.float64)
	q_value: tensor([[-21.0379]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.389113181335893, distance: 1.348731527209738 entropy 0.9217694997787476
epoch: 0, step: 13
	action: tensor([[ 0.1125,  0.6300, -0.4486,  0.2344,  0.1174,  1.1024,  0.5635]],
       dtype=torch.float64)
	q_value: tensor([[-26.9066]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6508063447678698, distance: 0.6762228866581672 entropy 0.9217694997787476
epoch: 0, step: 14
	action: tensor([[-0.0793,  1.7250,  0.6531,  0.0073,  0.8392, -0.9629,  0.9946]],
       dtype=torch.float64)
	q_value: tensor([[-19.0336]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 0, step: 15
	action: tensor([[ 0.5673,  0.1051, -0.2430,  0.2959,  0.5262,  0.3566,  0.5955]],
       dtype=torch.float64)
	q_value: tensor([[-25.4864]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9133185903297719, distance: 0.3369144795410354 entropy 0.9217694997787476
epoch: 0, step: 16
	action: tensor([[-0.1659,  0.5132, -0.5278, -0.1186,  0.2065,  1.0422,  0.6606]],
       dtype=torch.float64)
	q_value: tensor([[-19.5793]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5138114151203921, distance: 0.7979194880506125 entropy 0.9217694997787476
epoch: 0, step: 17
	action: tensor([[ 0.1451, -0.0539, -0.1797,  0.3925, -0.4982, -1.0589, -0.5255]],
       dtype=torch.float64)
	q_value: tensor([[-18.9820]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5571008995820287, distance: 0.7615687102589038 entropy 0.9217694997787476
epoch: 0, step: 18
	action: tensor([[-0.1943,  0.9539, -0.3710,  0.9480,  0.7501,  0.4448, -0.8127]],
       dtype=torch.float64)
	q_value: tensor([[-23.7253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 0, step: 19
	action: tensor([[ 0.0879, -0.1308, -0.5832,  0.1511, -0.3209,  0.3993, -0.9899]],
       dtype=torch.float64)
	q_value: tensor([[-25.4864]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30783843202827166, distance: 0.9520514834242876 entropy 0.9217694997787476
epoch: 0, step: 20
	action: tensor([[ 0.5174, -0.9827,  0.9612,  0.4912,  0.7860, -0.3773,  1.6449]],
       dtype=torch.float64)
	q_value: tensor([[-17.6762]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04959620116678587, distance: 1.1723784269914272 entropy 0.9217694997787476
epoch: 0, step: 21
	action: tensor([[ 0.5093,  0.3167, -0.0152,  0.1558, -0.0449, -0.1451,  0.3598]],
       dtype=torch.float64)
	q_value: tensor([[-40.2280]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8820237751601894, distance: 0.393055557038403 entropy 0.9217694997787476
epoch: 0, step: 22
	action: tensor([[-0.0132,  0.5918,  0.5583,  0.4664,  0.0077,  1.0143, -0.2669]],
       dtype=torch.float64)
	q_value: tensor([[-16.2727]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8561558874772583, distance: 0.43401300068347176 entropy 0.9217694997787476
epoch: 0, step: 23
	action: tensor([[-0.2866, -0.3021,  0.2420, -0.0505, -0.7523,  0.1544, -1.0914]],
       dtype=torch.float64)
	q_value: tensor([[-19.7274]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.26519225329732143, distance: 1.2871671730651328 entropy 0.9217694997787476
epoch: 0, step: 24
	action: tensor([[-0.7505, -0.5359, -0.0202,  0.2711,  0.8170, -0.1504,  0.4130]],
       dtype=torch.float64)
	q_value: tensor([[-20.8853]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9443107483588828, distance: 1.5956569632836117 entropy 0.9217694997787476
epoch: 0, step: 25
	action: tensor([[ 0.5207,  0.0907, -0.5937, -0.6211, -0.7153, -0.4464,  0.2851]],
       dtype=torch.float64)
	q_value: tensor([[-23.4841]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5454823721450961, distance: 0.7714931226626651 entropy 0.9217694997787476
epoch: 0, step: 26
	action: tensor([[ 1.2190, -0.2319,  0.2456,  0.2425,  0.6278, -0.5774,  0.1204]],
       dtype=torch.float64)
	q_value: tensor([[-20.0382]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5181621161482166, distance: 0.7943413390927211 entropy 0.9217694997787476
epoch: 0, step: 27
	action: tensor([[-0.0211, -1.2532, -0.8028,  0.0663, -0.2763, -0.1244,  0.4821]],
       dtype=torch.float64)
	q_value: tensor([[-29.1015]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.802698758865217, distance: 1.5364494387649263 entropy 0.9217694997787476
epoch: 0, step: 28
	action: tensor([[-0.7621, -0.0508, -0.7380,  0.4928,  0.2772,  0.2590, -0.2109]],
       dtype=torch.float64)
	q_value: tensor([[-23.1162]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5832801908017942, distance: 1.4399107774642808 entropy 0.9217694997787476
epoch: 0, step: 29
	action: tensor([[-0.2313,  1.1904,  0.9106, -0.8401,  0.5272, -0.6294,  0.2372]],
       dtype=torch.float64)
	q_value: tensor([[-17.3766]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06539110531366998, distance: 1.1811667869214635 entropy 0.9217694997787476
epoch: 0, step: 30
	action: tensor([[ 0.0027, -0.4427, -0.7185, -0.0873,  0.7694,  0.2054,  0.2903]],
       dtype=torch.float64)
	q_value: tensor([[-22.2701]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.013912389906774791, distance: 1.1522770400239606 entropy 0.9217694997787476
epoch: 0, step: 31
	action: tensor([[-0.0982, -0.0604, -0.4195, -0.2920, -0.8702,  1.0742,  0.2961]],
       dtype=torch.float64)
	q_value: tensor([[-19.7068]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0381805015185851, distance: 1.1222858358324856 entropy 0.9217694997787476
epoch: 0, step: 32
	action: tensor([[-0.2341,  0.3976,  0.8057,  0.0897,  0.5914, -0.1697, -0.2019]],
       dtype=torch.float64)
	q_value: tensor([[-20.3967]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33337205325670427, distance: 0.9343260368389182 entropy 0.9217694997787476
epoch: 0, step: 33
	action: tensor([[-0.3039,  0.2131,  0.9205, -0.1020,  0.9644, -0.1476,  0.1341]],
       dtype=torch.float64)
	q_value: tensor([[-19.3208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04425860948348426, distance: 1.1187341375159034 entropy 0.9217694997787476
epoch: 0, step: 34
	action: tensor([[ 0.1318,  0.2846,  0.6785,  0.2623, -1.3381,  0.7748,  0.0830]],
       dtype=torch.float64)
	q_value: tensor([[-21.9969]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7427620709528238, distance: 0.5803957124760573 entropy 0.9217694997787476
epoch: 0, step: 35
	action: tensor([[ 0.5942,  0.0984,  0.4894, -1.2144,  0.0077, -0.9476,  0.4796]],
       dtype=torch.float64)
	q_value: tensor([[-25.6228]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0691167610221669, distance: 1.10408954803673 entropy 0.9217694997787476
epoch: 0, step: 36
	action: tensor([[-0.0187,  0.3495,  0.4099,  0.6797,  0.2444,  0.5470,  0.4237]],
       dtype=torch.float64)
	q_value: tensor([[-26.6535]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8814671882030568, distance: 0.39398164268165065 entropy 0.9217694997787476
epoch: 0, step: 37
	action: tensor([[-0.2524, -0.5693, -0.5146, -0.0027,  0.4430,  0.8589, -1.1310]],
       dtype=torch.float64)
	q_value: tensor([[-18.4280]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08417201871422364, distance: 1.1915322199076026 entropy 0.9217694997787476
epoch: 0, step: 38
	action: tensor([[ 0.1124,  0.5770, -0.4366,  0.9403, -0.3826, -0.1351,  0.8962]],
       dtype=torch.float64)
	q_value: tensor([[-20.8705]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 0, step: 39
	action: tensor([[ 0.3613, -0.2676,  0.1234, -0.3007, -0.8799,  0.1944, -0.4418]],
       dtype=torch.float64)
	q_value: tensor([[-25.4864]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21972303971114782, distance: 1.0108368948445914 entropy 0.9217694997787476
epoch: 0, step: 40
	action: tensor([[ 0.1830,  0.1794, -0.3535, -1.1241, -0.8208, -0.1813, -0.1840]],
       dtype=torch.float64)
	q_value: tensor([[-20.4497]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3164641766643166, distance: 0.9461006338868992 entropy 0.9217694997787476
epoch: 0, step: 41
	action: tensor([[ 0.3609, -0.1507, -1.4190,  0.6259, -0.1885,  0.2768,  1.0851]],
       dtype=torch.float64)
	q_value: tensor([[-21.8795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2576285984313883, distance: 0.9859781933474694 entropy 0.9217694997787476
epoch: 0, step: 42
	action: tensor([[ 0.6156,  0.5563, -0.6088, -0.2652, -1.4481, -0.3293,  0.3618]],
       dtype=torch.float64)
	q_value: tensor([[-23.3923]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9695792494120768, distance: 0.1995913225736804 entropy 0.9217694997787476
epoch: 0, step: 43
	action: tensor([[-0.1551, -0.1669,  0.5729,  0.1351, -0.2065,  0.4591,  0.5474]],
       dtype=torch.float64)
	q_value: tensor([[-24.3326]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28058459270701586, distance: 0.9706140208912345 entropy 0.9217694997787476
epoch: 0, step: 44
	action: tensor([[ 0.1478,  0.6319,  0.1864,  0.1369, -0.6293, -0.9524,  0.7243]],
       dtype=torch.float64)
	q_value: tensor([[-18.8805]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7930853090234292, distance: 0.5205379011561839 entropy 0.9217694997787476
epoch: 0, step: 45
	action: tensor([[ 0.2278,  1.3388, -1.0471, -0.9827, -0.2623, -0.6499,  0.4639]],
       dtype=torch.float64)
	q_value: tensor([[-21.5722]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9743382016515518, distance: 0.18331594300799506 entropy 0.9217694997787476
epoch: 0, step: 46
	action: tensor([[-0.1361, -0.5278,  0.4441,  0.8264,  0.1008,  0.5396,  1.0105]],
       dtype=torch.float64)
	q_value: tensor([[-24.1618]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5676604094621284, distance: 0.7524353601740892 entropy 0.9217694997787476
epoch: 0, step: 47
	action: tensor([[-0.5645,  0.1411,  0.2740, -0.6653,  0.5434, -0.0054,  1.1747]],
       dtype=torch.float64)
	q_value: tensor([[-25.6688]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6156940569531499, distance: 1.4545754622275484 entropy 0.9217694997787476
epoch: 0, step: 48
	action: tensor([[ 0.4140,  0.7482, -0.4752, -0.4697, -0.3248,  0.6013,  0.0403]],
       dtype=torch.float64)
	q_value: tensor([[-23.2160]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.89568963597356, distance: 0.3695901802313275 entropy 0.9217694997787476
epoch: 0, step: 49
	action: tensor([[-1.1004, -0.2504, -0.3954,  0.3147,  0.8366, -1.3574,  1.0889]],
       dtype=torch.float64)
	q_value: tensor([[-18.9257]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1825552775540977, distance: 1.6905939840556712 entropy 0.9217694997787476
epoch: 0, step: 50
	action: tensor([[ 0.1713, -0.5442, -0.2030,  0.3118, -0.2324, -0.3610,  0.1205]],
       dtype=torch.float64)
	q_value: tensor([[-33.7019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04430499900285456, distance: 1.1187069867776285 entropy 0.9217694997787476
epoch: 0, step: 51
	action: tensor([[ 1.0098, -0.4542,  1.2640,  1.3686, -0.6657, -0.2956,  0.4741]],
       dtype=torch.float64)
	q_value: tensor([[-19.0814]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.320639389616699, distance: 0.9432066946477284 entropy 0.9217694997787476
epoch: 0, step: 52
	action: tensor([[-0.3472,  0.9831, -0.5521,  0.8293, -0.7166, -0.0703,  1.0652]],
       dtype=torch.float64)
	q_value: tensor([[-37.5317]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 0, step: 53
	action: tensor([[ 0.1914, -0.4476, -0.7131, -0.6430,  0.1417, -0.5451,  0.2896]],
       dtype=torch.float64)
	q_value: tensor([[-25.4864]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.012616890054104601, distance: 1.1515406589057646 entropy 0.9217694997787476
epoch: 0, step: 54
	action: tensor([[ 0.5111, -0.1044, -1.9637, -0.8189, -0.1264, -0.0592, -0.8438]],
       dtype=torch.float64)
	q_value: tensor([[-19.6078]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8367686828624302, distance: 0.4623367712733648 entropy 0.9217694997787476
epoch: 0, step: 55
	action: tensor([[ 1.1726,  0.9884,  0.4081, -0.6828, -0.5744,  0.4402, -0.5604]],
       dtype=torch.float64)
	q_value: tensor([[-25.5581]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9887427424257308, distance: 0.12141518190587544 entropy 0.9217694997787476
epoch: 0, step: 56
	action: tensor([[-0.3023,  0.0100, -0.3794,  0.7156,  0.6722, -0.8043,  0.3371]],
       dtype=torch.float64)
	q_value: tensor([[-28.9611]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07181609612974338, distance: 1.1847230295443478 entropy 0.9217694997787476
epoch: 0, step: 57
	action: tensor([[ 0.1205, -0.9676, -0.2872,  0.0426, -0.4739,  0.2187,  0.2590]],
       dtype=torch.float64)
	q_value: tensor([[-24.1127]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4955402888674161, distance: 1.3994447326534036 entropy 0.9217694997787476
epoch: 0, step: 58
	action: tensor([[-0.6208,  0.3670, -1.1033, -0.2187, -0.5291,  1.3760,  0.0601]],
       dtype=torch.float64)
	q_value: tensor([[-19.9237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.30414119131986883, distance: 1.3068297102675748 entropy 0.9217694997787476
epoch: 0, step: 59
	action: tensor([[-0.7305,  0.7612,  0.3171, -0.4204, -0.3216, -0.3462, -0.4542]],
       dtype=torch.float64)
	q_value: tensor([[-21.3370]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3070888862326011, distance: 1.3083057625027361 entropy 0.9217694997787476
epoch: 0, step: 60
	action: tensor([[-0.8560, -0.8413, -0.4887,  0.4283, -0.2020, -0.1781, -0.7152]],
       dtype=torch.float64)
	q_value: tensor([[-17.5733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2150665689060438, distance: 1.70313896314953 entropy 0.9217694997787476
epoch: 0, step: 61
	action: tensor([[ 0.7924,  0.2205, -0.4354,  0.6152,  0.1373,  0.0902,  0.6999]],
       dtype=torch.float64)
	q_value: tensor([[-21.8056]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9621156280824105, distance: 0.22273411413745925 entropy 0.9217694997787476
epoch: 0, step: 62
	action: tensor([[ 0.5929,  0.5896, -0.6055,  0.2439, -0.1959,  0.6395,  0.3703]],
       dtype=torch.float64)
	q_value: tensor([[-20.5194]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8548200657757222, distance: 0.4360235945853571 entropy 0.9217694997787476
epoch: 0, step: 63
	action: tensor([[ 0.1531,  0.0149,  0.4985, -0.6426, -0.5822,  0.4300, -0.1908]],
       dtype=torch.float64)
	q_value: tensor([[-17.7452]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16100544226220226, distance: 1.0481809565401279 entropy 0.9217694997787476
epoch: 0, step: 64
	action: tensor([[ 0.5916,  0.3171,  0.1048, -0.0175,  0.3598, -0.3955, -0.5375]],
       dtype=torch.float64)
	q_value: tensor([[-20.7625]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8124077957006459, distance: 0.49563741879385154 entropy 0.9217694997787476
epoch: 0, step: 65
	action: tensor([[ 0.6500, -0.3515, -0.5871, -0.4644,  0.2096, -0.3335,  0.5199]],
       dtype=torch.float64)
	q_value: tensor([[-19.8694]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09035173847328093, distance: 1.0914238509034853 entropy 0.9217694997787476
epoch: 0, step: 66
	action: tensor([[ 0.3697,  0.9694, -0.1224,  0.1415, -0.7007,  0.0248,  0.3918]],
       dtype=torch.float64)
	q_value: tensor([[-20.6008]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 0, step: 67
	action: tensor([[ 0.0545,  0.5201, -0.8093,  0.3050,  1.1179, -0.0263, -0.4623]],
       dtype=torch.float64)
	q_value: tensor([[-25.4864]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5439970277594077, distance: 0.7727526978910879 entropy 0.9217694997787476
epoch: 0, step: 68
	action: tensor([[ 0.6550, -0.1466, -0.7089, -1.2993, -0.0840, -0.5317,  0.3149]],
       dtype=torch.float64)
	q_value: tensor([[-21.9614]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05311605015875609, distance: 1.1135380735848643 entropy 0.9217694997787476
epoch: 0, step: 69
	action: tensor([[ 0.5884, -0.1931,  1.1481,  0.0025, -0.4460,  0.4870,  1.0818]],
       dtype=torch.float64)
	q_value: tensor([[-23.7709]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6575220403176969, distance: 0.6696887556727793 entropy 0.9217694997787476
epoch: 0, step: 70
	action: tensor([[ 0.3025, -0.5900, -0.0207,  0.1691, -0.6574,  0.0357,  0.4627]],
       dtype=torch.float64)
	q_value: tensor([[-28.1696]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11814519587068595, distance: 1.0746207838872248 entropy 0.9217694997787476
epoch: 0, step: 71
	action: tensor([[-0.3857,  0.1294,  0.3799, -0.5979,  0.3165, -0.3239, -0.6799]],
       dtype=torch.float64)
	q_value: tensor([[-19.5245]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4836732270151902, distance: 1.3938814010367144 entropy 0.9217694997787476
epoch: 0, step: 72
	action: tensor([[-0.0318, -0.0951, -0.7846,  0.2631, -0.6891,  0.1163,  0.1572]],
       dtype=torch.float64)
	q_value: tensor([[-17.4398]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13065219623501667, distance: 1.0669731087863026 entropy 0.9217694997787476
epoch: 0, step: 73
	action: tensor([[-0.2616,  0.1553, -0.1224, -0.4510, -0.1628, -0.9604, -0.0989]],
       dtype=torch.float64)
	q_value: tensor([[-15.7569]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.025314300556568137, distance: 1.1297672739644096 entropy 0.9217694997787476
epoch: 0, step: 74
	action: tensor([[-0.5905, -0.4182,  0.2632,  0.6690,  0.1153,  0.2110,  0.6884]],
       dtype=torch.float64)
	q_value: tensor([[-18.6142]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13461367810730707, distance: 1.2189354302567883 entropy 0.9217694997787476
epoch: 0, step: 75
	action: tensor([[ 0.4118, -0.0934, -0.1245, -1.1550,  0.8332, -0.0963,  0.9437]],
       dtype=torch.float64)
	q_value: tensor([[-22.0868]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03004431310936928, distance: 1.1614075571834448 entropy 0.9217694997787476
epoch: 0, step: 76
	action: tensor([[-0.1169, -0.2482,  0.9692, -0.6460, -0.1960, -0.7541, -0.5810]],
       dtype=torch.float64)
	q_value: tensor([[-24.4002]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5781662386801496, distance: 1.437583460251039 entropy 0.9217694997787476
epoch: 0, step: 77
	action: tensor([[ 0.7432,  0.6418, -0.0123, -0.4307, -1.0291,  0.0058, -0.4633]],
       dtype=torch.float64)
	q_value: tensor([[-24.6913]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9371395081651057, distance: 0.28690993073822046 entropy 0.9217694997787476
epoch: 0, step: 78
	action: tensor([[ 0.1740,  0.5020, -1.5282, -0.4955,  1.2690,  1.0454,  0.3134]],
       dtype=torch.float64)
	q_value: tensor([[-23.4684]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8905459435066866, distance: 0.37859303860863425 entropy 0.9217694997787476
epoch: 0, step: 79
	action: tensor([[ 0.9114,  0.2299, -0.4639, -0.8929, -0.3393, -0.6208, -0.2948]],
       dtype=torch.float64)
	q_value: tensor([[-25.9332]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3896277496715428, distance: 0.8940340996675921 entropy 0.9217694997787476
epoch: 0, step: 80
	action: tensor([[ 0.3431, -0.6910, -0.1505, -0.2505,  0.2148, -0.2781,  0.0464]],
       dtype=torch.float64)
	q_value: tensor([[-22.8929]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3255365706782407, distance: 1.317505844203951 entropy 0.9217694997787476
epoch: 0, step: 81
	action: tensor([[ 0.8040,  0.4429, -0.4394,  0.6634,  0.0548,  0.3736, -0.4892]],
       dtype=torch.float64)
	q_value: tensor([[-19.9386]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.905979378381593, distance: 0.3508877869573389 entropy 0.9217694997787476
epoch: 0, step: 82
	action: tensor([[-0.8979, -0.9288, -0.0025,  0.1745,  0.3730, -0.2775,  0.4496]],
       dtype=torch.float64)
	q_value: tensor([[-20.3031]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.2893693530187087, distance: 1.731468622266751 entropy 0.9217694997787476
epoch: 0, step: 83
	action: tensor([[ 0.4578,  0.7085,  0.1443,  0.3086, -1.0460, -0.8244, -0.1284]],
       dtype=torch.float64)
	q_value: tensor([[-25.6371]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9832220575767188, distance: 0.14822656456545436 entropy 0.9217694997787476
epoch: 0, step: 84
	action: tensor([[-0.1873,  0.1123,  0.1498, -0.0350,  0.2953, -0.6191,  0.7891]],
       dtype=torch.float64)
	q_value: tensor([[-22.6689]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.009824154753289793, distance: 1.149951623264537 entropy 0.9217694997787476
epoch: 0, step: 85
	action: tensor([[ 0.0458, -0.8090,  0.2999,  0.4639,  0.4301, -0.5175, -0.5819]],
       dtype=torch.float64)
	q_value: tensor([[-20.1514]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2378033120474592, distance: 1.2731586161076376 entropy 0.9217694997787476
epoch: 0, step: 86
	action: tensor([[-0.0307,  0.8613, -0.0476, -1.0982,  0.5372, -0.0944,  0.0788]],
       dtype=torch.float64)
	q_value: tensor([[-25.4282]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4415371265158733, distance: 0.8551727360121977 entropy 0.9217694997787476
epoch: 0, step: 87
	action: tensor([[-0.4070,  0.9680, -1.3963, -0.7700, -0.1821, -0.2407,  0.3546]],
       dtype=torch.float64)
	q_value: tensor([[-18.8564]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8304539288443697, distance: 0.4711948753651011 entropy 0.9217694997787476
epoch: 0, step: 88
	action: tensor([[-0.5139, -0.9636, -1.6272,  0.4667, -0.3995, -0.3717, -0.2752]],
       dtype=torch.float64)
	q_value: tensor([[-21.7253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1253981151025005, distance: 1.6683103296371653 entropy 0.9217694997787476
epoch: 0, step: 89
	action: tensor([[ 0.4720, -0.3626, -0.4081, -0.8736,  0.5006,  0.0686,  0.3498]],
       dtype=torch.float64)
	q_value: tensor([[-25.2174]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05808306851586931, distance: 1.17710871656592 entropy 0.9217694997787476
epoch: 0, step: 90
	action: tensor([[-0.4454,  1.0208, -0.7692, -0.3304, -0.6331,  0.2472,  0.9099]],
       dtype=torch.float64)
	q_value: tensor([[-19.5327]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.40687695134121094, distance: 0.8813108031448416 entropy 0.9217694997787476
epoch: 0, step: 91
	action: tensor([[ 0.6037,  0.0294, -0.3478, -0.1365, -1.0096,  0.1481, -0.4483]],
       dtype=torch.float64)
	q_value: tensor([[-20.8671]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 0, step: 92
	action: tensor([[ 1.1699,  0.9367,  0.1233,  0.2324,  0.0949,  0.0724, -0.6295]],
       dtype=torch.float64)
	q_value: tensor([[-25.4864]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 0, step: 93
	action: tensor([[ 0.2168,  0.0090,  0.9377, -0.3369,  0.0980,  1.7439, -0.5651]],
       dtype=torch.float64)
	q_value: tensor([[-25.4864]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7878810742199817, distance: 0.5270434286930391 entropy 0.9217694997787476
epoch: 0, step: 94
	action: tensor([[-0.1885,  0.4117,  0.2178, -0.4138,  0.3016,  0.3009,  0.1891]],
       dtype=torch.float64)
	q_value: tensor([[-28.4170]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2660098626407944, distance: 0.9803966202705594 entropy 0.9217694997787476
epoch: 0, step: 95
	action: tensor([[ 0.3957,  0.7550,  0.0322, -0.3894,  0.9194,  0.1053, -1.0828]],
       dtype=torch.float64)
	q_value: tensor([[-14.5784]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8672043327240501, distance: 0.4170120898387508 entropy 0.9217694997787476
epoch: 0, step: 96
	action: tensor([[ 0.3540, -1.5770,  0.2376, -0.1167, -0.8827, -1.3977,  0.7238]],
       dtype=torch.float64)
	q_value: tensor([[-21.6120]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5579744121820842, distance: 1.4283572831735212 entropy 0.9217694997787476
epoch: 0, step: 97
	action: tensor([[-1.0058, -0.4396, -0.6222, -0.5072, -0.2814,  0.0622, -0.4635]],
       dtype=torch.float64)
	q_value: tensor([[-35.3602]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.062665014387496, distance: 1.6435050560669568 entropy 0.9217694997787476
epoch: 0, step: 98
	action: tensor([[ 0.4856,  0.0224,  1.1806, -0.7823, -0.8320, -0.3769, -0.0385]],
       dtype=torch.float64)
	q_value: tensor([[-17.8844]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2182923475694194, distance: 1.0117631903506767 entropy 0.9217694997787476
epoch: 0, step: 99
	action: tensor([[-0.0414, -0.2007,  0.8793, -0.1304, -1.2234,  0.2474,  0.9334]],
       dtype=torch.float64)
	q_value: tensor([[-28.4170]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.048229649828189114, distance: 1.1164075862846101 entropy 0.9217694997787476
epoch: 0, step: 100
	action: tensor([[ 0.4713,  0.5464,  0.3893, -0.0371,  0.0734, -0.3975,  0.2828]],
       dtype=torch.float64)
	q_value: tensor([[-26.0661]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.853364301716131, distance: 0.4382042134406092 entropy 0.9217694997787476
epoch: 0, step: 101
	action: tensor([[-0.0950, -0.5966,  0.7079, -0.9826,  0.6605, -0.4876,  1.0273]],
       dtype=torch.float64)
	q_value: tensor([[-18.4940]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8710386105651855, distance: 1.5653017403152905 entropy 0.9217694997787476
epoch: 0, step: 102
	action: tensor([[ 0.4359,  0.0969,  0.0204,  0.2166, -0.7514,  0.4625, -0.0651]],
       dtype=torch.float64)
	q_value: tensor([[-29.0770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8199430683955742, distance: 0.4855809223895246 entropy 0.9217694997787476
epoch: 0, step: 103
	action: tensor([[-0.2483,  1.2936, -0.7128,  0.2647,  0.0381, -0.8483,  0.3699]],
       dtype=torch.float64)
	q_value: tensor([[-18.4241]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 0, step: 104
	action: tensor([[-0.0885,  0.3404,  0.9167, -0.9226, -0.3811, -0.4281, -0.5375]],
       dtype=torch.float64)
	q_value: tensor([[-25.4864]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24321408789526755, distance: 1.2759382435545064 entropy 0.9217694997787476
epoch: 0, step: 105
	action: tensor([[-0.8900, -0.0292,  0.2634,  0.3301, -0.3258, -0.3977,  0.0577]],
       dtype=torch.float64)
	q_value: tensor([[-23.3551]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6762748306188477, distance: 1.4815943231989797 entropy 0.9217694997787476
epoch: 0, step: 106
	action: tensor([[ 0.2594, -0.5981, -1.8618, -0.5688, -0.3963,  0.4960, -0.2876]],
       dtype=torch.float64)
	q_value: tensor([[-18.2788]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14318340142207175, distance: 1.059255259034891 entropy 0.9217694997787476
epoch: 0, step: 107
	action: tensor([[ 0.2322,  0.4094,  0.1686,  1.0642, -0.7864,  0.3983,  0.1746]],
       dtype=torch.float64)
	q_value: tensor([[-23.8357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7379958060787197, distance: 0.5857480006071475 entropy 0.9217694997787476
epoch: 0, step: 108
	action: tensor([[-0.1992, -0.0447,  1.1478, -0.0564, -0.7293, -0.3623, -0.0403]],
       dtype=torch.float64)
	q_value: tensor([[-20.2167]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09526330272614425, distance: 1.1976115109457715 entropy 0.9217694997787476
epoch: 0, step: 109
	action: tensor([[ 0.2436,  1.1558,  0.1893, -0.3389, -0.3857,  0.3064,  1.1618]],
       dtype=torch.float64)
	q_value: tensor([[-23.4678]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 0, step: 110
	action: tensor([[-0.3338, -0.4294,  1.5884,  0.3364,  0.0609, -0.2413, -0.1096]],
       dtype=torch.float64)
	q_value: tensor([[-25.4864]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23424298314662728, distance: 1.2713262863791253 entropy 0.9217694997787476
epoch: 0, step: 111
	action: tensor([[-0.4278, -0.8202, -0.2632, -0.5471,  0.1387, -0.3760, -0.6162]],
       dtype=torch.float64)
	q_value: tensor([[-28.8347]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8157492600204796, distance: 1.5420009141942772 entropy 0.9217694997787476
epoch: 0, step: 112
	action: tensor([[-0.1181, -0.7820, -0.1093, -0.6096, -0.7432, -0.1812,  0.3260]],
       dtype=torch.float64)
	q_value: tensor([[-20.0779]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7086337794390345, distance: 1.4958263773718852 entropy 0.9217694997787476
epoch: 0, step: 113
	action: tensor([[-0.1668,  1.0343,  0.2527, -0.2294, -0.2466,  0.0950,  0.0779]],
       dtype=torch.float64)
	q_value: tensor([[-20.7413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 0, step: 114
	action: tensor([[ 0.7134, -0.2635, -1.0216, -0.0700,  1.0340, -1.3693, -0.0934]],
       dtype=torch.float64)
	q_value: tensor([[-25.4864]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24253483061488168, distance: 0.9959511260681491 entropy 0.9217694997787476
epoch: 0, step: 115
	action: tensor([[-1.2175,  0.3097, -1.5048,  0.0193, -0.5688,  0.5214, -0.4584]],
       dtype=torch.float64)
	q_value: tensor([[-33.5410]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1517546887592798, distance: 1.6786226245696803 entropy 0.9217694997787476
epoch: 0, step: 116
	action: tensor([[ 0.4042, -0.4790,  0.2677, -0.0260, -0.7449,  1.9087, -0.3397]],
       dtype=torch.float64)
	q_value: tensor([[-20.2885]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5736011045460112, distance: 0.7472479439970151 entropy 0.9217694997787476
epoch: 0, step: 117
	action: tensor([[ 0.2316, -0.9938,  0.1682,  1.0575,  0.5494, -0.6832, -0.2587]],
       dtype=torch.float64)
	q_value: tensor([[-30.8755]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12974398257793862, distance: 1.0675303003881433 entropy 0.9217694997787476
epoch: 0, step: 118
	action: tensor([[-0.5302,  0.6241, -0.0295, -1.4472,  0.4340,  0.0458, -1.1952]],
       dtype=torch.float64)
	q_value: tensor([[-32.2624]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20799411530761613, distance: 1.2577348727073272 entropy 0.9217694997787476
epoch: 0, step: 119
	action: tensor([[ 0.8810,  0.1991,  0.3335, -1.2592, -0.5660,  0.3457,  1.0179]],
       dtype=torch.float64)
	q_value: tensor([[-21.9657]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3970123848972691, distance: 0.8886093719938446 entropy 0.9217694997787476
epoch: 0, step: 120
	action: tensor([[ 0.7197,  0.9233,  0.2108,  0.4685, -0.3093, -0.8220, -0.5031]],
       dtype=torch.float64)
	q_value: tensor([[-28.3251]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9355506537088178, distance: 0.290513254493546 entropy 0.9217694997787476
epoch: 0, step: 121
	action: tensor([[ 0.1030,  0.2196, -0.6310,  0.0776,  0.0108,  0.3949,  1.2399]],
       dtype=torch.float64)
	q_value: tensor([[-24.0867]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5413081055061173, distance: 0.7750277025428136 entropy 0.9217694997787476
epoch: 0, step: 122
	action: tensor([[-0.0940,  0.2203,  1.4810, -0.0790, -1.0040, -0.0163,  0.3640]],
       dtype=torch.float64)
	q_value: tensor([[-20.0307]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2807812830401861, distance: 0.9704813274266059 entropy 0.9217694997787476
epoch: 0, step: 123
	action: tensor([[-0.1578,  0.9998,  0.0686, -0.0116, -0.6295, -0.7170, -0.7732]],
       dtype=torch.float64)
	q_value: tensor([[-27.1037]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.643678101631883, distance: 0.6830900352640721 entropy 0.9217694997787476
epoch: 0, step: 124
	action: tensor([[-0.7620,  1.2151, -1.5830,  0.0710,  1.3574,  0.0667, -0.1053]],
       dtype=torch.float64)
	q_value: tensor([[-20.8962]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 0, step: 125
	action: tensor([[-0.8950,  0.3192,  0.3592, -0.0728, -0.1644,  0.0589, -0.0246]],
       dtype=torch.float64)
	q_value: tensor([[-25.4864]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5952359545148824, distance: 1.4453371246351012 entropy 0.9217694997787476
epoch: 0, step: 126
	action: tensor([[ 0.3356,  1.4216, -0.1344, -0.0741, -0.2933, -0.7476, -0.9564]],
       dtype=torch.float64)
	q_value: tensor([[-16.1133]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 0, step: 127
	action: tensor([[-0.0700, -0.7188, -0.5858, -0.7018, -0.0918, -0.0478,  0.1912]],
       dtype=torch.float64)
	q_value: tensor([[-25.4864]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3866683619116038, distance: 1.3475441303853402 entropy 0.9217694997787476
LOSS epoch 0 actor 242.94912880913762 critic 329.89916719439964 
epoch: 1, step: 0
	action: tensor([[-0.8169, -1.2791,  0.0638, -0.0185, -0.2260, -1.2727,  0.3402]],
       dtype=torch.float64)
	q_value: tensor([[-23.0363]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7409043310115175, distance: 1.5098859578293768 entropy 0.9217694997787476
epoch: 1, step: 1
	action: tensor([[ 0.2940, -0.4308, -0.0596, -1.0731, -0.2639,  1.0892, -0.0639]],
       dtype=torch.float64)
	q_value: tensor([[-40.0962]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0850479279655314, distance: 1.192013445840418 entropy 0.9217694997787476
epoch: 1, step: 2
	action: tensor([[ 0.1656,  1.2044, -0.6256,  0.2319, -0.7871, -0.5148,  0.1547]],
       dtype=torch.float64)
	q_value: tensor([[-29.2868]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 1, step: 3
	action: tensor([[ 0.2565, -0.1639, -0.2832, -0.1006, -0.0176,  0.4581,  0.4201]],
       dtype=torch.float64)
	q_value: tensor([[-33.5590]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4419219813933051, distance: 0.8548780215960348 entropy 0.9217694997787476
epoch: 1, step: 4
	action: tensor([[-0.0563, -0.3541,  0.1517,  0.0764,  0.0679,  0.3544,  0.0726]],
       dtype=torch.float64)
	q_value: tensor([[-19.8067]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11789772049659464, distance: 1.0747715590057578 entropy 0.9217694997787476
epoch: 1, step: 5
	action: tensor([[ 0.5783,  0.5176,  0.1397,  0.4919, -0.8665, -0.5509, -0.3663]],
       dtype=torch.float64)
	q_value: tensor([[-20.3188]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9880579749962531, distance: 0.12505345106872626 entropy 0.9217694997787476
epoch: 1, step: 6
	action: tensor([[ 0.8751,  0.0608, -1.0070,  0.6079, -0.6314, -0.3125, -0.5104]],
       dtype=torch.float64)
	q_value: tensor([[-27.6044]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.901898473776269, distance: 0.3584219310064312 entropy 0.9217694997787476
epoch: 1, step: 7
	action: tensor([[-0.0029, -0.1987, -0.5470,  0.0614, -0.4299, -0.1752,  0.3814]],
       dtype=torch.float64)
	q_value: tensor([[-28.9661]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19221189390986915, distance: 1.0285026628023437 entropy 0.9217694997787476
epoch: 1, step: 8
	action: tensor([[ 0.9142, -0.0121, -0.6767, -0.4182,  0.1822,  0.4975,  1.1513]],
       dtype=torch.float64)
	q_value: tensor([[-20.6371]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6927612680662969, distance: 0.6342999472195501 entropy 0.9217694997787476
epoch: 1, step: 9
	action: tensor([[-0.5713,  0.5604, -0.0176, -1.2495, -0.3015, -0.1914, -0.5029]],
       dtype=torch.float64)
	q_value: tensor([[-29.5839]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.27289556339609566, distance: 1.2910797803045793 entropy 0.9217694997787476
epoch: 1, step: 10
	action: tensor([[-0.5413,  0.2749, -0.3720,  0.0835,  0.3627,  0.0157,  0.5120]],
       dtype=torch.float64)
	q_value: tensor([[-26.2818]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12063665435994397, distance: 1.2114042835820888 entropy 0.9217694997787476
epoch: 1, step: 11
	action: tensor([[ 0.3150, -0.0546,  0.2300, -0.5063,  0.6809,  0.5319, -0.0365]],
       dtype=torch.float64)
	q_value: tensor([[-20.2649]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.404997756422496, distance: 0.8827058299101976 entropy 0.9217694997787476
epoch: 1, step: 12
	action: tensor([[-0.1733, -0.2752, -0.4217,  0.5151,  0.6801, -0.9363, -0.3470]],
       dtype=torch.float64)
	q_value: tensor([[-21.9033]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17843111149859792, distance: 1.242249392744649 entropy 0.9217694997787476
epoch: 1, step: 13
	action: tensor([[-0.0816,  0.5229, -0.6303,  0.6229, -0.0636,  0.0481,  0.0152]],
       dtype=torch.float64)
	q_value: tensor([[-30.6371]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3394252035492853, distance: 0.9300744050416273 entropy 0.9217694997787476
epoch: 1, step: 14
	action: tensor([[ 1.4279, -0.0959, -0.8470,  0.0604, -0.6418,  0.9557, -0.6682]],
       dtype=torch.float64)
	q_value: tensor([[-19.3449]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7166769901759731, distance: 0.6091126467441149 entropy 0.9217694997787476
epoch: 1, step: 15
	action: tensor([[-0.7966,  0.5309,  1.0994,  0.0363, -0.2513, -0.4461, -0.1510]],
       dtype=torch.float64)
	q_value: tensor([[-35.1389]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.41015235209079237, distance: 1.3589069234973843 entropy 0.9217694997787476
epoch: 1, step: 16
	action: tensor([[ 0.3164,  0.5073,  0.1869,  0.2286, -0.1049,  0.6508,  0.4239]],
       dtype=torch.float64)
	q_value: tensor([[-27.5736]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8641910489751179, distance: 0.42171678858171424 entropy 0.9217694997787476
epoch: 1, step: 17
	action: tensor([[ 1.3094, -0.4764, -0.6970, -0.5019, -0.0543, -0.8355, -0.2803]],
       dtype=torch.float64)
	q_value: tensor([[-21.5895]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3229543758373208, distance: 1.3162219432847622 entropy 0.9217694997787476
epoch: 1, step: 18
	action: tensor([[-0.5555, -0.6142, -0.3708,  0.1118,  0.1370,  1.0260,  0.2121]],
       dtype=torch.float64)
	q_value: tensor([[-35.5765]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4437613909445097, distance: 1.3750054043714912 entropy 0.9217694997787476
epoch: 1, step: 19
	action: tensor([[ 0.2985, -0.7786,  0.6001, -0.2149,  0.1781, -0.8604,  0.4618]],
       dtype=torch.float64)
	q_value: tensor([[-25.5859]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5246126839853769, distance: 1.4129814408751094 entropy 0.9217694997787476
epoch: 1, step: 20
	action: tensor([[ 0.9528,  0.2994,  0.7642, -0.2504, -0.3836, -0.7927,  0.8222]],
       dtype=torch.float64)
	q_value: tensor([[-34.7388]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7409129929407012, distance: 0.5824779778439042 entropy 0.9217694997787476
epoch: 1, step: 21
	action: tensor([[ 0.0455, -0.1759, -0.0306, -0.1956, -0.2747,  0.0623,  0.0550]],
       dtype=torch.float64)
	q_value: tensor([[-33.5628]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1339440576724087, distance: 1.0649510993786626 entropy 0.9217694997787476
epoch: 1, step: 22
	action: tensor([[ 0.0017, -0.7437,  0.4863,  0.2117, -0.1787,  0.3475,  0.6980]],
       dtype=torch.float64)
	q_value: tensor([[-18.5218]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04614218050267849, distance: 1.1704478005906245 entropy 0.9217694997787476
epoch: 1, step: 23
	action: tensor([[ 0.0725,  0.0207,  0.4993, -0.7035, -0.1787,  0.1586, -0.0110]],
       dtype=torch.float64)
	q_value: tensor([[-29.0602]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03429339235002615, distance: 1.1638005774857372 entropy 0.9217694997787476
epoch: 1, step: 24
	action: tensor([[-0.0412,  0.8276,  0.8002, -0.8934, -0.2494,  0.4124,  1.4338]],
       dtype=torch.float64)
	q_value: tensor([[-22.5477]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.230608733449442, distance: 1.0037610040531153 entropy 0.9217694997787476
epoch: 1, step: 25
	action: tensor([[ 0.8284,  0.4460,  0.7267, -0.2331,  0.1723, -1.0693,  1.7769]],
       dtype=torch.float64)
	q_value: tensor([[-33.1519]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8187658734271799, distance: 0.48716567714039094 entropy 0.9217694997787476
epoch: 1, step: 26
	action: tensor([[ 0.3507, -0.5059, -0.6151, -0.0184, -0.1664, -0.3906,  0.2991]],
       dtype=torch.float64)
	q_value: tensor([[-40.8699]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05332046493909204, distance: 1.113417870931198 entropy 0.9217694997787476
epoch: 1, step: 27
	action: tensor([[ 0.7601, -0.1506, -0.1141,  0.1384,  0.8079, -0.2126, -0.1093]],
       dtype=torch.float64)
	q_value: tensor([[-24.1618]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6486709805227759, distance: 0.6782873304731657 entropy 0.9217694997787476
epoch: 1, step: 28
	action: tensor([[-0.0531, -1.1040, -0.3865, -0.4692,  0.1487, -0.6026,  0.3789]],
       dtype=torch.float64)
	q_value: tensor([[-28.2955]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7912902112490574, distance: 1.5315799403157344 entropy 0.9217694997787476
epoch: 1, step: 29
	action: tensor([[ 0.4052, -0.0269, -0.1964, -0.3690,  0.4565,  0.2415,  0.2852]],
       dtype=torch.float64)
	q_value: tensor([[-30.4152]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5049464779907199, distance: 0.8051610741438967 entropy 0.9217694997787476
epoch: 1, step: 30
	action: tensor([[ 0.3654,  0.2429, -0.5459,  0.2891,  0.4298, -0.5779, -1.3790]],
       dtype=torch.float64)
	q_value: tensor([[-20.6482]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7195097378046383, distance: 0.6060599525333795 entropy 0.9217694997787476
epoch: 1, step: 31
	action: tensor([[-0.9036,  0.2775,  0.6278, -0.1737, -0.6930, -1.2209,  0.3720]],
       dtype=torch.float64)
	q_value: tensor([[-31.3487]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6322426592998156, distance: 1.4620056646522406 entropy 0.9217694997787476
epoch: 1, step: 32
	action: tensor([[-0.0593, -1.2441,  0.2065, -0.6437, -0.7819, -0.5549, -0.1196]],
       dtype=torch.float64)
	q_value: tensor([[-31.6065]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9272139406827107, distance: 1.5886259686924367 entropy 0.9217694997787476
epoch: 1, step: 33
	action: tensor([[ 0.1071, -0.3952,  0.5672,  0.5185,  0.1810,  0.0673,  1.0959]],
       dtype=torch.float64)
	q_value: tensor([[-32.3622]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4550954711253429, distance: 0.8447280235958146 entropy 0.9217694997787476
epoch: 1, step: 34
	action: tensor([[ 0.0344,  1.2542,  0.0923,  0.1463, -0.9848,  0.0877,  0.9859]],
       dtype=torch.float64)
	q_value: tensor([[-30.9773]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 1, step: 35
	action: tensor([[ 1.0527,  0.0895,  0.3386,  0.5241,  0.4463, -0.0589, -0.1952]],
       dtype=torch.float64)
	q_value: tensor([[-33.5590]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9135912059149394, distance: 0.33638425934502975 entropy 0.9217694997787476
epoch: 1, step: 36
	action: tensor([[-0.4360,  0.3738, -0.3171, -0.5570, -0.2323,  0.0967, -0.1327]],
       dtype=torch.float64)
	q_value: tensor([[-30.4173]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.006776737130099186, distance: 1.140460202416832 entropy 0.9217694997787476
epoch: 1, step: 37
	action: tensor([[ 0.1748, -0.6007, -0.2409,  0.0034, -1.0398, -0.3826, -0.7704]],
       dtype=torch.float64)
	q_value: tensor([[-19.5072]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.02140586475218975, distance: 1.1565272415333796 entropy 0.9217694997787476
epoch: 1, step: 38
	action: tensor([[-0.8343,  0.4669,  0.2975, -0.3447, -0.2890,  0.3600,  0.0958]],
       dtype=torch.float64)
	q_value: tensor([[-28.9064]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4866645948337016, distance: 1.3952858586770676 entropy 0.9217694997787476
epoch: 1, step: 39
	action: tensor([[ 0.0495, -0.6252,  0.1458,  0.7538,  0.1071, -0.4354,  0.3445]],
       dtype=torch.float64)
	q_value: tensor([[-22.1239]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18490773515806425, distance: 1.0331421474845555 entropy 0.9217694997787476
epoch: 1, step: 40
	action: tensor([[ 0.6892,  0.3569, -0.4082,  0.2858,  0.6463, -0.0639,  0.1302]],
       dtype=torch.float64)
	q_value: tensor([[-29.9499]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.945690984776616, distance: 0.26668135752357935 entropy 0.9217694997787476
epoch: 1, step: 41
	action: tensor([[ 0.9763, -0.4960,  0.1177, -0.0243,  0.2500,  0.4727, -0.5915]],
       dtype=torch.float64)
	q_value: tensor([[-24.9137]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3917698336966384, distance: 0.8924639272355382 entropy 0.9217694997787476
epoch: 1, step: 42
	action: tensor([[ 0.3741,  0.3026, -0.7407, -0.0804, -0.3361, -0.6031,  0.5660]],
       dtype=torch.float64)
	q_value: tensor([[-29.0261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8229366627149075, distance: 0.48152741278641625 entropy 0.9217694997787476
epoch: 1, step: 43
	action: tensor([[-0.1842, -0.0262,  0.5687,  0.6595, -0.3903,  0.1621, -0.7537]],
       dtype=torch.float64)
	q_value: tensor([[-23.6608]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6050678273082996, distance: 0.7191474593810551 entropy 0.9217694997787476
epoch: 1, step: 44
	action: tensor([[-0.2723, -0.0101,  0.5659, -0.6810, -0.7316, -0.2267,  0.9013]],
       dtype=torch.float64)
	q_value: tensor([[-25.8465]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4789250392142337, distance: 1.391649199819553 entropy 0.9217694997787476
epoch: 1, step: 45
	action: tensor([[-0.9572, -0.6713,  0.6601,  0.8771,  0.4495,  0.9442, -0.6878]],
       dtype=torch.float64)
	q_value: tensor([[-28.4241]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.02123973829871728, distance: 1.132126242720638 entropy 0.9217694997787476
epoch: 1, step: 46
	action: tensor([[ 0.2784,  1.2237,  0.0905,  0.3940, -0.5857,  0.5043,  0.3621]],
       dtype=torch.float64)
	q_value: tensor([[-32.4957]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 1, step: 47
	action: tensor([[ 0.1576,  0.5478,  0.5868, -0.2802,  0.2263,  0.5711, -0.1477]],
       dtype=torch.float64)
	q_value: tensor([[-33.5590]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7346860099300575, distance: 0.5894361517173019 entropy 0.9217694997787476
epoch: 1, step: 48
	action: tensor([[ 0.7558, -0.7295,  0.4039,  1.0267,  0.4172,  0.1740, -0.3726]],
       dtype=torch.float64)
	q_value: tensor([[-22.6160]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7500056694553248, distance: 0.5721656391545931 entropy 0.9217694997787476
epoch: 1, step: 49
	action: tensor([[ 0.4587,  0.4291, -0.3118,  0.0376, -0.4040, -0.6853,  0.9707]],
       dtype=torch.float64)
	q_value: tensor([[-36.9574]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8795535298617599, distance: 0.3971492362562226 entropy 0.9217694997787476
epoch: 1, step: 50
	action: tensor([[ 0.3639, -0.2771, -0.0914,  0.6620, -0.5620,  0.3294,  0.3797]],
       dtype=torch.float64)
	q_value: tensor([[-26.2925]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7332545415102163, distance: 0.5910241271517337 entropy 0.9217694997787476
epoch: 1, step: 51
	action: tensor([[ 0.2857,  0.6401, -0.1551,  0.0119, -1.4379,  0.5321, -1.7177]],
       dtype=torch.float64)
	q_value: tensor([[-23.7439]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7049170028446594, distance: 0.6216254494375449 entropy 0.9217694997787476
epoch: 1, step: 52
	action: tensor([[-0.1804,  0.4956,  0.9024, -0.4549, -0.0820,  0.1863, -0.9128]],
       dtype=torch.float64)
	q_value: tensor([[-37.5137]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17469565316316626, distance: 1.039593988554901 entropy 0.9217694997787476
epoch: 1, step: 53
	action: tensor([[-0.8687,  0.2486, -0.6608, -0.2190, -0.8267,  1.4010, -0.7645]],
       dtype=torch.float64)
	q_value: tensor([[-27.4166]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6344496017749244, distance: 1.4629937139677283 entropy 0.9217694997787476
epoch: 1, step: 54
	action: tensor([[-0.4306, -0.0677,  0.3547,  0.5213, -0.9577, -0.2121,  0.8949]],
       dtype=torch.float64)
	q_value: tensor([[-30.7568]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1070134570651603, distance: 1.0813820350589198 entropy 0.9217694997787476
epoch: 1, step: 55
	action: tensor([[ 0.2999, -0.3707, -0.1966, -0.2287, -0.2789, -0.6391, -0.3089]],
       dtype=torch.float64)
	q_value: tensor([[-28.2766]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.01620973335536524, distance: 1.135031603305234 entropy 0.9217694997787476
epoch: 1, step: 56
	action: tensor([[-0.6512, -0.0041, -0.4457,  0.1285,  0.3230,  1.2124, -0.1727]],
       dtype=torch.float64)
	q_value: tensor([[-24.3957]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10364330645028219, distance: 1.2021843224868618 entropy 0.9217694997787476
epoch: 1, step: 57
	action: tensor([[ 0.0637, -0.8267,  0.6853, -0.2334, -0.8327, -0.0055,  0.1414]],
       dtype=torch.float64)
	q_value: tensor([[-24.3919]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.621381964516794, distance: 1.4571335647359507 entropy 0.9217694997787476
epoch: 1, step: 58
	action: tensor([[ 0.6885, -0.6639, -0.2098,  0.4420, -0.6141, -0.3410,  0.4966]],
       dtype=torch.float64)
	q_value: tensor([[-29.6825]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2974317038433627, distance: 0.9591818831867908 entropy 0.9217694997787476
epoch: 1, step: 59
	action: tensor([[ 0.1430,  0.9265, -0.0233,  0.1479,  0.7849, -0.9356,  0.2722]],
       dtype=torch.float64)
	q_value: tensor([[-29.3871]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6272985422177293, distance: 0.6986139267859388 entropy 0.9217694997787476
epoch: 1, step: 60
	action: tensor([[-1.2843,  0.4060, -1.0926, -0.3178, -0.4793, -0.5475, -1.3840]],
       dtype=torch.float64)
	q_value: tensor([[-27.7432]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5356573208077939, distance: 1.4180901828606804 entropy 0.9217694997787476
epoch: 1, step: 61
	action: tensor([[ 0.6062, -0.4719,  0.1231,  0.8736,  0.4695, -0.0050,  0.0526]],
       dtype=torch.float64)
	q_value: tensor([[-31.5257]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8217907891295858, distance: 0.48308301353295274 entropy 0.9217694997787476
epoch: 1, step: 62
	action: tensor([[-1.1839, -0.1540,  0.6918,  0.4066,  0.6004,  0.3715,  0.2650]],
       dtype=torch.float64)
	q_value: tensor([[-30.7437]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7153419436545498, distance: 1.498759837914645 entropy 0.9217694997787476
epoch: 1, step: 63
	action: tensor([[ 0.8330,  1.0554, -0.5332,  0.0746, -0.3703, -1.1249, -0.0128]],
       dtype=torch.float64)
	q_value: tensor([[-29.8161]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9440193104717804, distance: 0.27075458203191904 entropy 0.9217694997787476
epoch: 1, step: 64
	action: tensor([[ 0.4150,  0.4843,  0.3599, -0.3116, -0.2604, -0.2394, -0.4426]],
       dtype=torch.float64)
	q_value: tensor([[-30.5151]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7537116732060339, distance: 0.5679088118877526 entropy 0.9217694997787476
epoch: 1, step: 65
	action: tensor([[ 0.1111, -0.1739, -0.2814, -0.4820,  0.0661,  0.0536, -0.4963]],
       dtype=torch.float64)
	q_value: tensor([[-24.1351]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11766759022793583, distance: 1.074911747589101 entropy 0.9217694997787476
epoch: 1, step: 66
	action: tensor([[-0.6912,  0.3296,  0.6297,  0.4512, -0.5168, -0.9287, -0.8861]],
       dtype=torch.float64)
	q_value: tensor([[-19.1057]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08398067358496308, distance: 1.1914270687216155 entropy 0.9217694997787476
epoch: 1, step: 67
	action: tensor([[-0.6723,  0.2597,  0.0660, -0.5103, -1.5121, -0.0877, -0.2123]],
       dtype=torch.float64)
	q_value: tensor([[-30.7878]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5189084284239156, distance: 1.410335667137482 entropy 0.9217694997787476
epoch: 1, step: 68
	action: tensor([[-0.3722,  0.5858, -0.9910,  0.4972,  0.7378, -0.1045,  0.7950]],
       dtype=torch.float64)
	q_value: tensor([[-31.0039]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.029496219612520802, distance: 1.1273410180180767 entropy 0.9217694997787476
epoch: 1, step: 69
	action: tensor([[ 0.6820,  0.5637,  0.4539,  0.3122, -0.0818, -0.7296, -0.0987]],
       dtype=torch.float64)
	q_value: tensor([[-27.6521]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9723443934175033, distance: 0.1903041624335476 entropy 0.9217694997787476
epoch: 1, step: 70
	action: tensor([[ 0.6251,  0.3288,  0.1239,  0.3845,  0.2537, -0.5607,  0.7889]],
       dtype=torch.float64)
	q_value: tensor([[-28.3224]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9290241960361414, distance: 0.30486800833074096 entropy 0.9217694997787476
epoch: 1, step: 71
	action: tensor([[-0.8115, -0.9323, -0.6969, -0.3708, -0.2749, -0.2597,  0.2042]],
       dtype=torch.float64)
	q_value: tensor([[-27.8575]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9134615263763808, distance: 1.582947679158123 entropy 0.9217694997787476
epoch: 1, step: 72
	action: tensor([[-0.1174,  0.3645,  0.0169, -0.3764, -0.0524, -0.5103, -0.4308]],
       dtype=torch.float64)
	q_value: tensor([[-28.2421]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18069950096427378, distance: 1.0358057151910738 entropy 0.9217694997787476
epoch: 1, step: 73
	action: tensor([[ 2.2223,  0.7739, -0.3730,  0.9720, -0.0702, -0.2061, -0.8111]],
       dtype=torch.float64)
	q_value: tensor([[-20.3741]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 1, step: 74
	action: tensor([[-0.4351,  0.5674, -0.1473,  1.0667, -0.8873,  0.5915,  1.0460]],
       dtype=torch.float64)
	q_value: tensor([[-33.5590]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 1, step: 75
	action: tensor([[ 0.0513,  0.8353, -1.1329, -0.1265, -0.0831,  0.3579, -0.1458]],
       dtype=torch.float64)
	q_value: tensor([[-33.5590]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.649689072841497, distance: 0.6773038362413881 entropy 0.9217694997787476
epoch: 1, step: 76
	action: tensor([[ 0.8741,  0.4330, -0.1026, -0.0270,  0.3646,  0.8706, -0.0888]],
       dtype=torch.float64)
	q_value: tensor([[-23.0952]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.08890237812499313 entropy 0.9217694997787476
epoch: 1, step: 77
	action: tensor([[-0.3377, -0.4811,  0.0261,  0.4143,  0.1762, -0.3487, -0.0530]],
       dtype=torch.float64)
	q_value: tensor([[-33.5590]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3202697955961977, distance: 1.3148858058179915 entropy 0.9217694997787476
epoch: 1, step: 78
	action: tensor([[ 0.3605, -1.1121,  0.1864, -1.0547,  0.5101, -0.4776,  0.6678]],
       dtype=torch.float64)
	q_value: tensor([[-24.3000]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9658858334785285, distance: 1.60448565942962 entropy 0.9217694997787476
epoch: 1, step: 79
	action: tensor([[ 0.2533,  0.1263, -0.4273, -0.0677, -0.9508,  0.2209, -0.0220]],
       dtype=torch.float64)
	q_value: tensor([[-35.3153]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5491225894546801, distance: 0.7683974799638995 entropy 0.9217694997787476
epoch: 1, step: 80
	action: tensor([[ 0.3220,  0.0391, -0.7483, -0.7027,  1.1876,  0.5748, -0.4259]],
       dtype=torch.float64)
	q_value: tensor([[-23.1691]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.66112101693239, distance: 0.6661607043679147 entropy 0.9217694997787476
epoch: 1, step: 81
	action: tensor([[ 0.6915,  0.4294,  0.4549,  0.3482,  0.1742,  0.0599, -0.2596]],
       dtype=torch.float64)
	q_value: tensor([[-27.3541]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9908722057664746, distance: 0.10933006691504513 entropy 0.9217694997787476
epoch: 1, step: 82
	action: tensor([[-0.2402, -0.1124,  0.6973,  0.6875,  0.4862,  0.5206, -0.3505]],
       dtype=torch.float64)
	q_value: tensor([[-24.7770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6700956594744678, distance: 0.6572804396092502 entropy 0.9217694997787476
epoch: 1, step: 83
	action: tensor([[-0.8085, -0.2319, -0.7592,  0.0854,  0.2174,  0.4416, -0.0181]],
       dtype=torch.float64)
	q_value: tensor([[-26.9616]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8088215299710246, distance: 1.539056461664926 entropy 0.9217694997787476
epoch: 1, step: 84
	action: tensor([[ 0.7944,  0.1770, -0.4646,  0.1312, -0.4831, -0.2512, -0.9802]],
       dtype=torch.float64)
	q_value: tensor([[-22.0286]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8804573370500223, distance: 0.3956563646156494 entropy 0.9217694997787476
epoch: 1, step: 85
	action: tensor([[ 0.4407, -0.5063,  0.2597, -0.6727, -0.9113, -0.2943,  0.6204]],
       dtype=torch.float64)
	q_value: tensor([[-27.6854]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.312793594893201, distance: 1.3111576639746008 entropy 0.9217694997787476
epoch: 1, step: 86
	action: tensor([[ 0.5460,  0.7394,  0.1860, -1.3406,  0.6865,  0.1261,  0.4304]],
       dtype=torch.float64)
	q_value: tensor([[-30.2814]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6345121963650089, distance: 0.6918200395426923 entropy 0.9217694997787476
epoch: 1, step: 87
	action: tensor([[ 0.3437, -0.0823,  0.7115, -0.1493,  0.4025,  1.8959, -0.1050]],
       dtype=torch.float64)
	q_value: tensor([[-29.0779]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8259040893802301, distance: 0.47747537476384433 entropy 0.9217694997787476
epoch: 1, step: 88
	action: tensor([[ 0.3667,  0.0388, -0.5649,  0.5641,  0.9180,  0.2625,  1.6391]],
       dtype=torch.float64)
	q_value: tensor([[-35.5754]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7667891337065714, distance: 0.552625708071316 entropy 0.9217694997787476
epoch: 1, step: 89
	action: tensor([[ 0.5681,  0.0073,  0.3641, -0.6520,  0.3294, -0.1244, -0.0753]],
       dtype=torch.float64)
	q_value: tensor([[-35.3607]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29933989446275655, distance: 0.9578784182018081 entropy 0.9217694997787476
epoch: 1, step: 90
	action: tensor([[ 0.5466, -1.2002,  0.0468, -0.3537, -0.3493,  0.2623, -0.8844]],
       dtype=torch.float64)
	q_value: tensor([[-24.1249]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8070743123645339, distance: 1.5383129620472535 entropy 0.9217694997787476
epoch: 1, step: 91
	action: tensor([[ 1.1771,  0.0892,  1.5834,  0.4658,  0.5837, -0.3320, -1.4135]],
       dtype=torch.float64)
	q_value: tensor([[-31.3968]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6913314964331936, distance: 0.6357741289045362 entropy 0.9217694997787476
epoch: 1, step: 92
	action: tensor([[ 0.3386, -0.3624, -0.6442, -0.4864, -0.7891,  0.7205, -0.6046]],
       dtype=torch.float64)
	q_value: tensor([[-46.2250]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09647275383558152, distance: 1.087745561791611 entropy 0.9217694997787476
epoch: 1, step: 93
	action: tensor([[ 0.2344, -0.2425,  0.6694, -0.7390, -0.4142,  0.4222,  0.4533]],
       dtype=torch.float64)
	q_value: tensor([[-26.4567]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07295561105911919, distance: 1.1853526389310987 entropy 0.9217694997787476
epoch: 1, step: 94
	action: tensor([[-0.8454, -0.5253, -1.2874,  0.5948, -0.4586, -0.8621, -0.4429]],
       dtype=torch.float64)
	q_value: tensor([[-27.6073]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9817451059460607, distance: 1.6109445445491948 entropy 0.9217694997787476
epoch: 1, step: 95
	action: tensor([[ 1.1287,  0.1289,  0.7664,  0.8282, -0.7243,  0.9630,  1.2087]],
       dtype=torch.float64)
	q_value: tensor([[-32.3510]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.822900419246467, distance: 0.48157669269626213 entropy 0.9217694997787476
epoch: 1, step: 96
	action: tensor([[ 1.1080, -0.6044, -0.4710,  0.1023, -0.5743, -0.6339, -0.0334]],
       dtype=torch.float64)
	q_value: tensor([[-39.0997]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03306945343119927, distance: 1.1252637629812685 entropy 0.9217694997787476
epoch: 1, step: 97
	action: tensor([[ 0.7064, -0.6609,  0.0667,  0.4039, -0.1736,  0.1723, -0.0844]],
       dtype=torch.float64)
	q_value: tensor([[-32.9526]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4492108329102019, distance: 0.8492770498950585 entropy 0.9217694997787476
epoch: 1, step: 98
	action: tensor([[-0.1095,  0.8015,  0.2097, -0.2510,  0.4642, -0.8295, -0.3273]],
       dtype=torch.float64)
	q_value: tensor([[-27.0766]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35162799187016547, distance: 0.9214437354102393 entropy 0.9217694997787476
epoch: 1, step: 99
	action: tensor([[ 1.0706,  0.9438,  0.1570, -0.7568,  0.9480, -1.4575,  0.7156]],
       dtype=torch.float64)
	q_value: tensor([[-24.3596]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9200238249908821, distance: 0.3236212328341663 entropy 0.9217694997787476
epoch: 1, step: 100
	action: tensor([[ 0.6832,  1.1189, -0.1640,  1.0365, -0.7982, -0.2765, -0.7642]],
       dtype=torch.float64)
	q_value: tensor([[-39.6795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 1, step: 101
	action: tensor([[ 1.1352,  0.8160, -0.0806,  0.7165, -0.0969, -0.8257, -0.0290]],
       dtype=torch.float64)
	q_value: tensor([[-33.5590]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 1, step: 102
	action: tensor([[-0.8549, -0.1811, -0.3165,  0.3723, -0.9852,  0.5155,  0.3000]],
       dtype=torch.float64)
	q_value: tensor([[-33.5590]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8386682581504428, distance: 1.551702224091294 entropy 0.9217694997787476
epoch: 1, step: 103
	action: tensor([[ 1.6178,  0.3956,  0.3610, -0.5649, -0.7391, -0.1483,  0.4536]],
       dtype=torch.float64)
	q_value: tensor([[-24.7863]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.508599043910695, distance: 0.8021852862130923 entropy 0.9217694997787476
epoch: 1, step: 104
	action: tensor([[ 0.5865, -0.4398,  0.6749, -0.7186, -1.0287,  0.7143,  0.1546]],
       dtype=torch.float64)
	q_value: tensor([[-36.3381]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09619789791474675, distance: 1.0879109970762602 entropy 0.9217694997787476
epoch: 1, step: 105
	action: tensor([[ 0.4035, -0.2152,  0.0152,  0.8571, -0.3342, -0.8210, -0.2444]],
       dtype=torch.float64)
	q_value: tensor([[-34.1455]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7325528533300352, distance: 0.5918009768728932 entropy 0.9217694997787476
epoch: 1, step: 106
	action: tensor([[ 0.9204,  0.1595, -0.2854,  0.4232,  0.5147, -0.2058,  0.4474]],
       dtype=torch.float64)
	q_value: tensor([[-31.2297]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9505900986920233, distance: 0.25436870939309586 entropy 0.9217694997787476
epoch: 1, step: 107
	action: tensor([[ 0.6245,  0.8354, -0.2087, -0.0074,  0.0439, -0.3574,  0.7394]],
       dtype=torch.float64)
	q_value: tensor([[-27.9759]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9580572154500377, distance: 0.23436098953944426 entropy 0.9217694997787476
epoch: 1, step: 108
	action: tensor([[-1.1006,  0.1771, -0.9586, -0.5622, -0.6833, -1.0244, -0.0081]],
       dtype=torch.float64)
	q_value: tensor([[-23.7710]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14561690821297524, distance: 1.2248316515030748 entropy 0.9217694997787476
epoch: 1, step: 109
	action: tensor([[-0.4724,  0.4343, -0.5926, -0.0524, -0.0255, -0.9394,  1.1426]],
       dtype=torch.float64)
	q_value: tensor([[-29.4615]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18832234221620925, distance: 1.03097584248879 entropy 0.9217694997787476
epoch: 1, step: 110
	action: tensor([[ 0.3157, -0.1316, -0.1689,  0.1886,  0.2764, -0.6411,  1.0429]],
       dtype=torch.float64)
	q_value: tensor([[-28.2394]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39361542835373964, distance: 0.8911088661551697 entropy 0.9217694997787476
epoch: 1, step: 111
	action: tensor([[ 0.2910,  0.0687,  0.5927,  0.0589,  1.4737, -0.7720, -0.3390]],
       dtype=torch.float64)
	q_value: tensor([[-28.6444]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5682422807297476, distance: 0.7519288510113948 entropy 0.9217694997787476
epoch: 1, step: 112
	action: tensor([[ 0.0659, -0.4930, -0.3965,  0.2133, -0.5863,  0.1153, -0.3466]],
       dtype=torch.float64)
	q_value: tensor([[-34.8069]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.008661495342566994, distance: 1.1492894351346203 entropy 0.9217694997787476
epoch: 1, step: 113
	action: tensor([[ 0.0590, -0.0764,  0.0472, -0.5173, -0.9398, -0.1000,  0.6358]],
       dtype=torch.float64)
	q_value: tensor([[-21.3661]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05444579302572283, distance: 1.112755908287437 entropy 0.9217694997787476
epoch: 1, step: 114
	action: tensor([[ 0.0128, -1.0799, -0.1173,  0.0174, -0.5204, -0.0874,  0.4979]],
       dtype=torch.float64)
	q_value: tensor([[-25.6983]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6612554317256685, distance: 1.474941856023056 entropy 0.9217694997787476
epoch: 1, step: 115
	action: tensor([[ 0.4788,  0.5624, -1.4049, -0.3924, -0.3754,  0.5864,  0.9407]],
       dtype=torch.float64)
	q_value: tensor([[-28.3309]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9217326101665485, distance: 0.3201452912864803 entropy 0.9217694997787476
epoch: 1, step: 116
	action: tensor([[ 0.2165,  0.6434, -0.0419,  0.6767, -0.3824,  0.1958, -0.0644]],
       dtype=torch.float64)
	q_value: tensor([[-30.6561]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 1, step: 117
	action: tensor([[ 0.9789,  0.1000,  0.7248,  0.5997, -0.0277, -0.0697, -0.5911]],
       dtype=torch.float64)
	q_value: tensor([[-33.5590]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8841638243104463, distance: 0.38947429399638306 entropy 0.9217694997787476
epoch: 1, step: 118
	action: tensor([[-0.5101,  1.2885, -0.3660, -0.2403, -0.2787, -0.4616,  0.8765]],
       dtype=torch.float64)
	q_value: tensor([[-32.6328]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 1, step: 119
	action: tensor([[ 0.7824,  0.8840,  0.8457,  0.6194,  0.5468, -0.4789,  0.7543]],
       dtype=torch.float64)
	q_value: tensor([[-33.5590]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
epoch: 1, step: 120
	action: tensor([[ 0.3887,  0.0671,  0.2855,  0.9699, -0.5105, -0.5871, -0.1210]],
       dtype=torch.float64)
	q_value: tensor([[-33.5590]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9440274565867727, distance: 0.27073488168326243 entropy 0.9217694997787476
epoch: 1, step: 121
	action: tensor([[ 0.2431, -0.0292, -0.8094, -0.0444, -0.5397,  0.5799,  0.4093]],
       dtype=torch.float64)
	q_value: tensor([[-29.8684]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4425925209403716, distance: 0.8543642925188453 entropy 0.9217694997787476
epoch: 1, step: 122
	action: tensor([[ 0.3417,  0.3376,  0.3679,  0.4339, -0.1508, -0.5495, -1.0746]],
       dtype=torch.float64)
	q_value: tensor([[-21.7383]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8572529576787349, distance: 0.43235476781718823 entropy 0.9217694997787476
epoch: 1, step: 123
	action: tensor([[-0.1133,  0.6647, -1.3554, -1.4296, -0.3471, -0.7322,  0.6995]],
       dtype=torch.float64)
	q_value: tensor([[-29.3617]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8564872769867611, distance: 0.43351277065606453 entropy 0.9217694997787476
epoch: 1, step: 124
	action: tensor([[ 0.0702,  0.5019,  0.3603, -0.4543, -0.6367,  0.0988,  0.1639]],
       dtype=torch.float64)
	q_value: tensor([[-34.2312]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.47302330555244754, distance: 0.8307156604346414 entropy 0.9217694997787476
epoch: 1, step: 125
	action: tensor([[ 0.7089,  0.0944,  0.0768, -1.4295, -0.7145, -0.0043,  0.3089]],
       dtype=torch.float64)
	q_value: tensor([[-24.0382]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07615383217032412, distance: 1.099908413520215 entropy 0.9217694997787476
epoch: 1, step: 126
	action: tensor([[ 0.2921, -0.0857, -0.6452,  0.1524,  0.6034, -1.2778,  0.4025]],
       dtype=torch.float64)
	q_value: tensor([[-33.1240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3314223108329709, distance: 0.935691389918502 entropy 0.9217694997787476
epoch: 1, step: 127
	action: tensor([[-0.0712,  1.4026,  0.4018,  0.5927, -0.1953, -0.2000, -0.2486]],
       dtype=torch.float64)
	q_value: tensor([[-34.4550]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.9217694997787476
LOSS epoch 1 actor 419.16229763232633 critic 396.52946452510724 
epoch: 2, step: 0
	action: tensor([[ 0.0988, -0.1966,  0.6611,  1.0017, -0.0587, -0.2973, -0.5601]],
       dtype=torch.float64)
	q_value: tensor([[-32.0181]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7814167350260051, distance: 0.5350140012851239 entropy 0.8164090514183044
epoch: 2, step: 1
	action: tensor([[ 0.2120, -0.4440, -0.5574, -0.2047,  0.4500,  0.4009,  0.5068]],
       dtype=torch.float64)
	q_value: tensor([[-30.3810]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1418847073441145, distance: 1.0600577222775545 entropy 0.8164090514183044
epoch: 2, step: 2
	action: tensor([[-0.4923, -0.4174,  0.2951,  0.4562, -0.1824, -0.1610, -0.0575]],
       dtype=torch.float64)
	q_value: tensor([[-22.9618]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.295103163158033, distance: 1.3022935072060948 entropy 0.8164090514183044
epoch: 2, step: 3
	action: tensor([[ 0.4330,  0.1816,  0.4212, -0.9696, -0.0214, -0.5917,  0.4832]],
       dtype=torch.float64)
	q_value: tensor([[-23.3347]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11708610345930903, distance: 1.0752658907691062 entropy 0.8164090514183044
epoch: 2, step: 4
	action: tensor([[ 0.6035, -0.0934, -0.7775, -0.3412,  0.0920, -0.0504,  0.3487]],
       dtype=torch.float64)
	q_value: tensor([[-27.2669]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5103913106476623, distance: 0.8007210609941277 entropy 0.8164090514183044
epoch: 2, step: 5
	action: tensor([[ 0.7218, -0.6103,  0.0641, -0.4823,  0.1334,  0.4479,  0.2784]],
       dtype=torch.float64)
	q_value: tensor([[-22.2687]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.052320757899162684, distance: 1.1738990790878001 entropy 0.8164090514183044
epoch: 2, step: 6
	action: tensor([[-0.4654,  0.0800,  0.2269, -0.6153, -0.1020, -0.0116, -0.4667]],
       dtype=torch.float64)
	q_value: tensor([[-25.5466]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.503551018433017, distance: 1.403187728191019 entropy 0.8164090514183044
epoch: 2, step: 7
	action: tensor([[ 0.0840,  1.6868, -0.5043, -0.8313,  0.1387, -1.2395,  1.0987]],
       dtype=torch.float64)
	q_value: tensor([[-20.2567]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8967604626627754, distance: 0.36768822162066384 entropy 0.8164090514183044
epoch: 2, step: 8
	action: tensor([[ 0.6497,  1.2779,  0.2055, -0.3480,  0.3335,  0.0603, -0.4524]],
       dtype=torch.float64)
	q_value: tensor([[-31.7413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 2, step: 9
	action: tensor([[ 0.3909, -0.4524, -0.3726, -0.5027,  0.0308,  0.0125,  0.0617]],
       dtype=torch.float64)
	q_value: tensor([[-32.0181]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.031018409163147576, distance: 1.1619565894736008 entropy 0.8164090514183044
epoch: 2, step: 10
	action: tensor([[ 0.7970,  0.3244, -0.6728, -0.8600, -0.8808, -0.0963, -0.5055]],
       dtype=torch.float64)
	q_value: tensor([[-21.2569]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.634078561076703, distance: 0.6922303249866484 entropy 0.8164090514183044
epoch: 2, step: 11
	action: tensor([[ 0.9937, -0.4735, -0.0470,  0.8584,  0.3037, -0.5135, -0.1851]],
       dtype=torch.float64)
	q_value: tensor([[-28.5794]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6993119716843315, distance: 0.6275014907821256 entropy 0.8164090514183044
epoch: 2, step: 12
	action: tensor([[-0.4850,  0.0679, -0.1137, -0.0797, -0.4217, -0.4999, -0.5644]],
       dtype=torch.float64)
	q_value: tensor([[-34.3824]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.19822737470862672, distance: 1.2526401043548285 entropy 0.8164090514183044
epoch: 2, step: 13
	action: tensor([[ 2.1376e-01,  4.1492e-01, -4.3580e-01, -4.1606e-01, -2.6512e-01,
          5.3643e-01,  1.9693e-04]], dtype=torch.float64)
	q_value: tensor([[-21.4719]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6731911910295171, distance: 0.6541895017008906 entropy 0.8164090514183044
epoch: 2, step: 14
	action: tensor([[-0.6226, -0.0641, -0.0487,  0.0199, -0.7422,  0.7007,  0.4109]],
       dtype=torch.float64)
	q_value: tensor([[-20.8985]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4548352077752862, distance: 1.3802685566344506 entropy 0.8164090514183044
epoch: 2, step: 15
	action: tensor([[-0.1586, -0.9131,  0.9809,  0.0197, -0.5022,  0.6102,  0.7192]],
       dtype=torch.float64)
	q_value: tensor([[-23.5064]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.32539986179931724, distance: 1.3174379021361187 entropy 0.8164090514183044
epoch: 2, step: 16
	action: tensor([[-0.2147,  0.2079,  0.2973,  0.0181,  0.1618,  0.5824,  0.4069]],
       dtype=torch.float64)
	q_value: tensor([[-34.3178]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42272151306115513, distance: 0.8694595456465816 entropy 0.8164090514183044
epoch: 2, step: 17
	action: tensor([[-0.0790,  0.3818, -0.3846, -0.6240,  0.3062, -0.6897, -0.1384]],
       dtype=torch.float64)
	q_value: tensor([[-20.3053]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27677537844330224, distance: 0.9731802680800407 entropy 0.8164090514183044
epoch: 2, step: 18
	action: tensor([[-0.1155, -0.1386, -0.2610,  0.4954,  0.0987, -0.7176,  1.4049]],
       dtype=torch.float64)
	q_value: tensor([[-21.1204]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1332903087142273, distance: 1.0653529667998585 entropy 0.8164090514183044
epoch: 2, step: 19
	action: tensor([[-0.3123, -1.0120, -0.3699, -0.8336, -0.7372, -0.4538, -0.1194]],
       dtype=torch.float64)
	q_value: tensor([[-32.0264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5273892751536737, distance: 1.4142675010041679 entropy 0.8164090514183044
epoch: 2, step: 20
	action: tensor([[-0.1981,  0.7175, -0.2198,  0.8163, -0.1722,  0.1003, -0.2061]],
       dtype=torch.float64)
	q_value: tensor([[-28.6367]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 2, step: 21
	action: tensor([[ 0.3153, -0.9353, -0.6185, -0.4478,  0.4750, -0.3377, -0.4449]],
       dtype=torch.float64)
	q_value: tensor([[-32.0181]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5500332939791246, distance: 1.4247124079386977 entropy 0.8164090514183044
epoch: 2, step: 22
	action: tensor([[ 0.0810, -0.4555,  0.8023, -0.5343, -0.5173,  0.2753,  0.1555]],
       dtype=torch.float64)
	q_value: tensor([[-26.9143]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.32678085537795165, distance: 1.318124072272619 entropy 0.8164090514183044
epoch: 2, step: 23
	action: tensor([[ 0.4835, -0.3127, -0.5825, -0.0272, -0.0454,  0.0784, -0.7948]],
       dtype=torch.float64)
	q_value: tensor([[-27.6590]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36381369263606766, distance: 0.9127437168775562 entropy 0.8164090514183044
epoch: 2, step: 24
	action: tensor([[ 0.1680,  0.6958, -0.0642,  1.1088, -0.1451, -0.2072, -1.0041]],
       dtype=torch.float64)
	q_value: tensor([[-22.8301]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 2, step: 25
	action: tensor([[-0.6599, -0.0925,  0.5072, -0.6903,  0.0063,  1.1412, -0.4812]],
       dtype=torch.float64)
	q_value: tensor([[-32.0181]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5387765153018313, distance: 1.419529649589687 entropy 0.8164090514183044
epoch: 2, step: 26
	action: tensor([[-0.0280, -0.1843, -0.0519, -1.0204,  0.3058, -0.1270,  0.4700]],
       dtype=torch.float64)
	q_value: tensor([[-28.0013]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3311443325390624, distance: 1.3202897965879064 entropy 0.8164090514183044
epoch: 2, step: 27
	action: tensor([[ 0.6061, -0.2677,  0.4099, -0.4558, -0.2070,  0.6205, -0.5889]],
       dtype=torch.float64)
	q_value: tensor([[-23.3512]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38121706934669397, distance: 0.9001727374174849 entropy 0.8164090514183044
epoch: 2, step: 28
	action: tensor([[ 0.7636,  0.3165,  0.0152, -0.5101, -0.0323,  0.2397,  0.1138]],
       dtype=torch.float64)
	q_value: tensor([[-26.8236]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7951254399830274, distance: 0.5179653526551617 entropy 0.8164090514183044
epoch: 2, step: 29
	action: tensor([[-0.7576,  0.0462, -0.4627, -0.6227,  0.8853,  1.2449,  0.3324]],
       dtype=torch.float64)
	q_value: tensor([[-22.9898]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.053292719621280904, distance: 1.174441081903833 entropy 0.8164090514183044
epoch: 2, step: 30
	action: tensor([[ 0.2138, -0.1558, -1.1719,  0.3019,  0.4337, -0.2090,  0.1284]],
       dtype=torch.float64)
	q_value: tensor([[-29.2317]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30805665779485103, distance: 0.9519013894629325 entropy 0.8164090514183044
epoch: 2, step: 31
	action: tensor([[ 0.0887,  1.1810,  0.9087, -0.0080, -0.9115, -0.4022,  0.1767]],
       dtype=torch.float64)
	q_value: tensor([[-25.2753]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 2, step: 32
	action: tensor([[ 0.8159, -0.8753,  0.0758,  0.0334,  0.6952, -0.6191, -0.6025]],
       dtype=torch.float64)
	q_value: tensor([[-32.0181]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.30870983003400054, distance: 1.3091167375900163 entropy 0.8164090514183044
epoch: 2, step: 33
	action: tensor([[-0.2279,  0.9483, -0.1878, -0.6140,  0.3033, -0.5375, -0.4551]],
       dtype=torch.float64)
	q_value: tensor([[-34.4880]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44212279357820283, distance: 0.8547242031662537 entropy 0.8164090514183044
epoch: 2, step: 34
	action: tensor([[ 0.1893,  0.5067,  0.1725,  0.0930, -0.7116,  0.3160, -0.9953]],
       dtype=torch.float64)
	q_value: tensor([[-22.5029]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7289102936719141, distance: 0.5958174340263304 entropy 0.8164090514183044
epoch: 2, step: 35
	action: tensor([[ 0.1741,  0.5969, -0.0549, -1.0773,  0.6856,  0.3118,  0.5667]],
       dtype=torch.float64)
	q_value: tensor([[-26.5788]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5790537805797578, distance: 0.7424547669201128 entropy 0.8164090514183044
epoch: 2, step: 36
	action: tensor([[ 0.4563,  0.7161,  0.4743,  0.3584, -0.1366,  0.4062, -0.8485]],
       dtype=torch.float64)
	q_value: tensor([[-25.7256]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 2, step: 37
	action: tensor([[ 0.0743, -0.1773, -0.3072,  0.1943, -0.2421, -0.2800,  0.1118]],
       dtype=torch.float64)
	q_value: tensor([[-32.0181]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2925596740341826, distance: 0.9625019085098634 entropy 0.8164090514183044
epoch: 2, step: 38
	action: tensor([[ 0.4740,  0.2710,  0.0636,  0.7817, -0.3865,  0.4896,  0.3434]],
       dtype=torch.float64)
	q_value: tensor([[-19.8328]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8861148528869819, distance: 0.3861804078197315 entropy 0.8164090514183044
epoch: 2, step: 39
	action: tensor([[ 0.4761,  1.6268, -0.6429, -0.6573, -0.3121, -0.1827,  0.4599]],
       dtype=torch.float64)
	q_value: tensor([[-23.7482]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 2, step: 40
	action: tensor([[-0.0957,  0.2803, -0.7109, -0.9238,  0.2525,  0.3165,  0.3010]],
       dtype=torch.float64)
	q_value: tensor([[-32.0181]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.46923887745594994, distance: 0.8336931732033522 entropy 0.8164090514183044
epoch: 2, step: 41
	action: tensor([[ 0.0676, -0.3774,  0.0875, -0.2617,  0.1412,  0.4075, -0.3213]],
       dtype=torch.float64)
	q_value: tensor([[-22.8052]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.046456220915814805, distance: 1.1174472005566696 entropy 0.8164090514183044
epoch: 2, step: 42
	action: tensor([[ 0.9205,  0.4386, -0.2253, -0.0741, -1.2326, -0.9584,  0.1664]],
       dtype=torch.float64)
	q_value: tensor([[-19.6853]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8767939302062355, distance: 0.4016730974412933 entropy 0.8164090514183044
epoch: 2, step: 43
	action: tensor([[ 0.6622, -0.0201, -0.2017,  0.3407, -0.5354,  0.2216,  0.4364]],
       dtype=torch.float64)
	q_value: tensor([[-31.5194]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8709576538151345, distance: 0.4110766593717332 entropy 0.8164090514183044
epoch: 2, step: 44
	action: tensor([[ 0.1333,  0.1829, -0.9179,  0.9265,  0.0027,  0.9451, -0.0119]],
       dtype=torch.float64)
	q_value: tensor([[-22.5523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30301493872072316, distance: 0.9553630232739164 entropy 0.8164090514183044
epoch: 2, step: 45
	action: tensor([[-0.2401,  0.0651, -1.0623,  0.7032,  0.6284, -0.1303,  0.0284]],
       dtype=torch.float64)
	q_value: tensor([[-25.0694]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13348004379626133, distance: 1.2183263364372514 entropy 0.8164090514183044
epoch: 2, step: 46
	action: tensor([[-0.1099, -0.3103,  0.0034, -0.5860,  0.3244, -0.6956, -0.6419]],
       dtype=torch.float64)
	q_value: tensor([[-26.7780]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4352978459218275, distance: 1.3709692369455513 entropy 0.8164090514183044
epoch: 2, step: 47
	action: tensor([[ 0.5803,  0.5809,  0.6006, -0.9858,  0.0317, -0.3744,  0.0829]],
       dtype=torch.float64)
	q_value: tensor([[-25.0500]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.539038681888627, distance: 0.7769426001217005 entropy 0.8164090514183044
epoch: 2, step: 48
	action: tensor([[ 0.4171,  0.0138, -0.0541,  0.1196,  0.5989,  0.3471, -0.1459]],
       dtype=torch.float64)
	q_value: tensor([[-28.2105]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.778526782252203, distance: 0.538539175387822 entropy 0.8164090514183044
epoch: 2, step: 49
	action: tensor([[ 0.8446,  0.3388, -0.2019,  0.2115, -0.5678, -0.2953, -0.3172]],
       dtype=torch.float64)
	q_value: tensor([[-21.2025]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9538969896076716, distance: 0.24570915337848517 entropy 0.8164090514183044
epoch: 2, step: 50
	action: tensor([[ 1.1092,  0.4967,  0.4660, -0.6557,  0.1992,  0.4905,  0.4966]],
       dtype=torch.float64)
	q_value: tensor([[-24.4802]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8925672051420036, distance: 0.375081056203484 entropy 0.8164090514183044
epoch: 2, step: 51
	action: tensor([[ 0.8329, -0.4741, -0.3697,  0.0912,  1.3985, -0.1595, -0.2607]],
       dtype=torch.float64)
	q_value: tensor([[-29.3315]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34374091142840113, distance: 0.9270312167636441 entropy 0.8164090514183044
epoch: 2, step: 52
	action: tensor([[ 0.3800, -0.5784, -0.7297,  0.0989, -0.4544,  0.3785,  0.4342]],
       dtype=torch.float64)
	q_value: tensor([[-34.7644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11311197113278015, distance: 1.0776831429528824 entropy 0.8164090514183044
epoch: 2, step: 53
	action: tensor([[ 0.4814,  0.1875, -0.2679, -0.6670,  0.1120,  0.9293, -0.5242]],
       dtype=torch.float64)
	q_value: tensor([[-23.0906]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7414122299588464, distance: 0.5819165162892085 entropy 0.8164090514183044
epoch: 2, step: 54
	action: tensor([[-0.0512, -0.1915,  0.2436, -0.1140, -0.9453, -1.1739,  0.3765]],
       dtype=torch.float64)
	q_value: tensor([[-24.5669]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15721871830263723, distance: 1.0505437271694849 entropy 0.8164090514183044
epoch: 2, step: 55
	action: tensor([[-0.6384,  0.1870,  0.0902, -0.2888,  0.2183,  1.1353,  0.5966]],
       dtype=torch.float64)
	q_value: tensor([[-30.9908]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.020509730181037744, distance: 1.1560197882758216 entropy 0.8164090514183044
epoch: 2, step: 56
	action: tensor([[ 0.4637, -0.8531, -0.4808,  0.7900, -0.0496, -0.4499,  0.6158]],
       dtype=torch.float64)
	q_value: tensor([[-26.6089]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11850096727291048, distance: 1.0744039920132011 entropy 0.8164090514183044
epoch: 2, step: 57
	action: tensor([[ 0.3577,  0.3802, -0.4624,  0.6268, -0.9924,  0.4900,  0.6440]],
       dtype=torch.float64)
	q_value: tensor([[-32.5432]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6629612238600415, distance: 0.6643495234644322 entropy 0.8164090514183044
epoch: 2, step: 58
	action: tensor([[ 0.5026, -0.1381,  0.1851, -0.0884, -0.3416,  0.6010,  0.9193]],
       dtype=torch.float64)
	q_value: tensor([[-25.6418]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6709620360097601, distance: 0.6564168154646504 entropy 0.8164090514183044
epoch: 2, step: 59
	action: tensor([[ 0.3048,  0.3280, -1.0298, -0.7435,  0.2348,  0.6624,  0.9137]],
       dtype=torch.float64)
	q_value: tensor([[-26.5218]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.835643412570137, distance: 0.46392764335484865 entropy 0.8164090514183044
epoch: 2, step: 60
	action: tensor([[ 0.6566, -0.3095, -0.6475,  0.6083,  0.0781, -0.4978,  0.0826]],
       dtype=torch.float64)
	q_value: tensor([[-27.8246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5818595397355195, distance: 0.7399762648966307 entropy 0.8164090514183044
epoch: 2, step: 61
	action: tensor([[ 0.8015, -1.0782,  0.1775, -0.0160, -0.5952,  0.2874,  0.6743]],
       dtype=torch.float64)
	q_value: tensor([[-28.0776]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3501006086661338, distance: 1.3296574149700138 entropy 0.8164090514183044
epoch: 2, step: 62
	action: tensor([[ 0.4046,  0.6804,  0.2985,  0.1737, -0.4290, -0.5774, -0.0243]],
       dtype=torch.float64)
	q_value: tensor([[-32.5888]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9138496690686612, distance: 0.3358807917866197 entropy 0.8164090514183044
epoch: 2, step: 63
	action: tensor([[ 0.5288,  1.4454, -0.6612, -0.4081, -0.3757, -0.8670,  0.6655]],
       dtype=torch.float64)
	q_value: tensor([[-23.9179]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9035102459684061, distance: 0.35546536628238523 entropy 0.8164090514183044
epoch: 2, step: 64
	action: tensor([[ 0.3405, -0.1515, -0.4458,  0.2251,  0.1144,  1.2566,  0.8179]],
       dtype=torch.float64)
	q_value: tensor([[-29.6233]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7370855687212545, distance: 0.5867646015845939 entropy 0.8164090514183044
epoch: 2, step: 65
	action: tensor([[ 0.1151,  0.9334,  0.0510, -0.5740, -0.6189,  0.4249,  0.2973]],
       dtype=torch.float64)
	q_value: tensor([[-28.3637]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6708172089891357, distance: 0.6565612614608326 entropy 0.8164090514183044
epoch: 2, step: 66
	action: tensor([[ 0.6049, -0.0081, -0.1022, -0.1419, -0.0634,  0.3802,  0.5266]],
       dtype=torch.float64)
	q_value: tensor([[-26.2655]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7254945526212584, distance: 0.5995593455141854 entropy 0.8164090514183044
epoch: 2, step: 67
	action: tensor([[-0.3034,  0.0578, -0.5927, -0.3707, -0.6590, -0.3452, -0.8109]],
       dtype=torch.float64)
	q_value: tensor([[-22.0229]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10598358157864707, distance: 1.08200543075329 entropy 0.8164090514183044
epoch: 2, step: 68
	action: tensor([[-0.8193, -0.2862, -0.0036, -0.3071,  0.8875,  1.3193,  0.1413]],
       dtype=torch.float64)
	q_value: tensor([[-23.7318]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.30650973814174853, distance: 1.3080158867558074 entropy 0.8164090514183044
epoch: 2, step: 69
	action: tensor([[-0.3610,  0.4453,  0.2461, -0.2643,  0.5570,  0.1747, -0.1273]],
       dtype=torch.float64)
	q_value: tensor([[-29.5576]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09528990910408985, distance: 1.0884573351727764 entropy 0.8164090514183044
epoch: 2, step: 70
	action: tensor([[-0.0091, -0.4794,  0.5977,  0.6191, -0.0292,  0.1134, -0.2273]],
       dtype=torch.float64)
	q_value: tensor([[-18.7417]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42066931653318995, distance: 0.8710036174743789 entropy 0.8164090514183044
epoch: 2, step: 71
	action: tensor([[ 0.3055, -0.9868, -0.5331, -1.0944,  0.1010, -0.0369, -0.4975]],
       dtype=torch.float64)
	q_value: tensor([[-26.6656]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5925751675859303, distance: 1.4441312406127322 entropy 0.8164090514183044
epoch: 2, step: 72
	action: tensor([[ 0.2592, -1.0973, -0.4768,  0.3740,  0.0842, -0.0116,  0.4584]],
       dtype=torch.float64)
	q_value: tensor([[-27.2890]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4000955663402539, distance: 1.3540525937799743 entropy 0.8164090514183044
epoch: 2, step: 73
	action: tensor([[ 0.0171,  0.6140,  0.2395,  0.4637, -0.5587,  0.6659, -0.8416]],
       dtype=torch.float64)
	q_value: tensor([[-28.6232]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 2, step: 74
	action: tensor([[-0.0627, -0.0221,  0.1602,  0.1901,  0.1466,  0.2142,  0.6501]],
       dtype=torch.float64)
	q_value: tensor([[-32.0181]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39345895442892465, distance: 0.8912238314051879 entropy 0.8164090514183044
epoch: 2, step: 75
	action: tensor([[-0.1177,  0.4604,  0.4791,  0.3217,  0.5187, -0.2262, -0.2041]],
       dtype=torch.float64)
	q_value: tensor([[-21.2930]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5389875567279414, distance: 0.7769856842291988 entropy 0.8164090514183044
epoch: 2, step: 76
	action: tensor([[ 0.8052, -0.4603, -0.0941,  0.1172, -0.5265, -0.8353, -0.9683]],
       dtype=torch.float64)
	q_value: tensor([[-22.1450]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21253710739320386, distance: 1.0154808600712093 entropy 0.8164090514183044
epoch: 2, step: 77
	action: tensor([[-0.3524,  0.8983, -0.1224, -0.7046,  0.6878, -0.9886,  0.1619]],
       dtype=torch.float64)
	q_value: tensor([[-33.6664]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12603444522960106, distance: 1.069803098512207 entropy 0.8164090514183044
epoch: 2, step: 78
	action: tensor([[-0.7237, -0.2596,  0.1189,  0.5360, -0.2590,  0.1639,  0.4856]],
       dtype=torch.float64)
	q_value: tensor([[-25.2087]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.37327101744834645, distance: 1.3410186584399906 entropy 0.8164090514183044
epoch: 2, step: 79
	action: tensor([[-0.3725, -0.6157, -0.1466, -0.2968, -0.1848, -0.8813,  0.1075]],
       dtype=torch.float64)
	q_value: tensor([[-24.3590]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.603033794188264, distance: 1.4488653699596696 entropy 0.8164090514183044
epoch: 2, step: 80
	action: tensor([[-0.2679,  0.1563,  0.2412,  0.2423,  0.0250, -0.4972, -0.2370]],
       dtype=torch.float64)
	q_value: tensor([[-27.4766]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12015631807526927, distance: 1.0733947162287378 entropy 0.8164090514183044
epoch: 2, step: 81
	action: tensor([[ 0.6373,  0.7059,  0.0284, -0.0831,  0.4268, -0.7175,  0.4723]],
       dtype=torch.float64)
	q_value: tensor([[-21.1574]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9255371935661502, distance: 0.31226721750746306 entropy 0.8164090514183044
epoch: 2, step: 82
	action: tensor([[ 0.6352, -0.2687,  0.4788, -0.4804,  0.1245,  0.0187,  1.2759]],
       dtype=torch.float64)
	q_value: tensor([[-26.1207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2031974040327993, distance: 1.0214851642298541 entropy 0.8164090514183044
epoch: 2, step: 83
	action: tensor([[ 0.5801, -0.2564,  1.0955,  0.7507,  0.3304, -0.5038,  0.1394]],
       dtype=torch.float64)
	q_value: tensor([[-31.0445]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7219193874789214, distance: 0.6034510523107148 entropy 0.8164090514183044
epoch: 2, step: 84
	action: tensor([[ 0.3962, -0.7576,  0.1151, -0.9680,  0.6895, -0.0114, -0.4766]],
       dtype=torch.float64)
	q_value: tensor([[-36.2061]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7048271568071156, distance: 1.4941591911223013 entropy 0.8164090514183044
epoch: 2, step: 85
	action: tensor([[ 0.4216, -0.4472,  0.2625, -0.1765, -0.5547, -0.6416,  0.0543]],
       dtype=torch.float64)
	q_value: tensor([[-26.8305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05606791412679346, distance: 1.1759872610021767 entropy 0.8164090514183044
epoch: 2, step: 86
	action: tensor([[ 0.6710, -0.2919, -0.3649,  0.3525, -0.5458,  0.3626,  0.0037]],
       dtype=torch.float64)
	q_value: tensor([[-26.7458]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6715114642775568, distance: 0.6558685434195252 entropy 0.8164090514183044
epoch: 2, step: 87
	action: tensor([[ 0.1185,  0.1575, -0.0657,  0.1939, -0.0056, -0.4637,  1.0085]],
       dtype=torch.float64)
	q_value: tensor([[-22.8233]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5156180691804826, distance: 0.7964355924819607 entropy 0.8164090514183044
epoch: 2, step: 88
	action: tensor([[ 0.4958, -0.1942,  0.6443, -0.2509, -0.7010,  0.1123,  0.2343]],
       dtype=torch.float64)
	q_value: tensor([[-24.7802]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37002575134910665, distance: 0.9082765295568123 entropy 0.8164090514183044
epoch: 2, step: 89
	action: tensor([[-0.3483,  0.2870, -1.0382, -1.1348,  0.1028, -0.0330, -0.0938]],
       dtype=torch.float64)
	q_value: tensor([[-27.1765]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.46442615742617377, distance: 0.8374644340694263 entropy 0.8164090514183044
epoch: 2, step: 90
	action: tensor([[ 0.4380, -1.0974,  0.1661,  0.9118,  0.6107, -0.8556,  0.0085]],
       dtype=torch.float64)
	q_value: tensor([[-24.2825]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.038338775456017826, distance: 1.1660743155986735 entropy 0.8164090514183044
epoch: 2, step: 91
	action: tensor([[-0.2275, -0.6766,  0.5893, -0.5105,  0.1488,  0.6199,  0.7911]],
       dtype=torch.float64)
	q_value: tensor([[-40.6512]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6173552050363715, distance: 1.4553230185143493 entropy 0.8164090514183044
epoch: 2, step: 92
	action: tensor([[-0.1570,  0.6696, -0.0188, -0.1130,  0.0856,  0.2697,  0.0407]],
       dtype=torch.float64)
	q_value: tensor([[-29.6541]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5259023009012667, distance: 0.7879354079240483 entropy 0.8164090514183044
epoch: 2, step: 93
	action: tensor([[-0.1184,  0.2532, -0.3247,  0.1580, -1.1899,  0.1476, -0.2269]],
       dtype=torch.float64)
	q_value: tensor([[-17.9627]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3323821120430932, distance: 0.9350195156609793 entropy 0.8164090514183044
epoch: 2, step: 94
	action: tensor([[ 0.3142,  0.1608,  0.3207,  0.0604, -0.6110,  0.4169,  0.1454]],
       dtype=torch.float64)
	q_value: tensor([[-24.2609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7507021794985032, distance: 0.5713680269059864 entropy 0.8164090514183044
epoch: 2, step: 95
	action: tensor([[-0.0980, -0.3174,  0.4177, -0.1825, -0.1526,  0.2068, -0.1606]],
       dtype=torch.float64)
	q_value: tensor([[-23.2373]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10907996012478494, distance: 1.2051417229762873 entropy 0.8164090514183044
epoch: 2, step: 96
	action: tensor([[ 1.0403, -0.0426,  0.1673, -0.8680,  0.2991, -0.5303,  0.4339]],
       dtype=torch.float64)
	q_value: tensor([[-21.1313]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16275207323668373, distance: 1.0470893289839083 entropy 0.8164090514183044
epoch: 2, step: 97
	action: tensor([[ 0.1862,  0.5623, -0.8243,  0.2271,  0.2283, -0.1990,  0.2230]],
       dtype=torch.float64)
	q_value: tensor([[-30.8697]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7221459185417451, distance: 0.6032052094628505 entropy 0.8164090514183044
epoch: 2, step: 98
	action: tensor([[-0.3878,  0.1185, -0.4620,  0.1463,  0.0178, -0.4210, -0.2135]],
       dtype=torch.float64)
	q_value: tensor([[-20.4838]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.023723490595903618, distance: 1.1578386098869615 entropy 0.8164090514183044
epoch: 2, step: 99
	action: tensor([[ 1.0167,  0.1772, -0.6782, -0.3048, -0.0343,  0.3218,  0.3831]],
       dtype=torch.float64)
	q_value: tensor([[-19.2812]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7870151363006164, distance: 0.5281181137006263 entropy 0.8164090514183044
epoch: 2, step: 100
	action: tensor([[-0.4635,  0.0812,  0.9973,  0.4435, -0.5240, -0.0902, -0.2648]],
       dtype=torch.float64)
	q_value: tensor([[-25.3170]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20030397142899403, distance: 1.0233381452472488 entropy 0.8164090514183044
epoch: 2, step: 101
	action: tensor([[-0.9956,  0.0386, -0.0231,  0.6586, -0.4218, -0.2056, -0.3527]],
       dtype=torch.float64)
	q_value: tensor([[-27.3273]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5035828917198106, distance: 1.4032026009712961 entropy 0.8164090514183044
epoch: 2, step: 102
	action: tensor([[ 0.5543,  0.5218, -0.2805,  0.3997,  1.2529,  0.1193, -0.3071]],
       dtype=torch.float64)
	q_value: tensor([[-23.7332]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9119051934386548, distance: 0.3396501779067972 entropy 0.8164090514183044
epoch: 2, step: 103
	action: tensor([[-0.0069, -0.3087,  0.1662, -0.5459,  0.1008,  0.5089, -0.5936]],
       dtype=torch.float64)
	q_value: tensor([[-28.5762]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09673937547511646, distance: 1.1984182423107954 entropy 0.8164090514183044
epoch: 2, step: 104
	action: tensor([[-2.5175e-01,  2.6105e-01,  6.2789e-01,  3.9274e-04,  2.4243e-01,
         -2.6681e-01,  7.8914e-01]], dtype=torch.float64)
	q_value: tensor([[-21.3982]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08782779282350595, distance: 1.0929369557059356 entropy 0.8164090514183044
epoch: 2, step: 105
	action: tensor([[ 0.4736,  0.0750, -0.5072,  0.8651,  0.5228, -0.3338, -0.0383]],
       dtype=torch.float64)
	q_value: tensor([[-25.2452]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7691239042797726, distance: 0.5498524671712746 entropy 0.8164090514183044
epoch: 2, step: 106
	action: tensor([[ 0.1718,  1.3574, -0.9243,  0.3285,  0.6682,  0.6342,  0.6912]],
       dtype=torch.float64)
	q_value: tensor([[-27.6676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 2, step: 107
	action: tensor([[-0.7097,  0.3000,  0.6279, -0.0410,  0.6024,  0.4651,  0.7569]],
       dtype=torch.float64)
	q_value: tensor([[-32.0181]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17436221249300288, distance: 1.2401029125851317 entropy 0.8164090514183044
epoch: 2, step: 108
	action: tensor([[-0.5157,  1.1073, -0.3255,  0.4142, -0.9020, -0.1786, -0.0174]],
       dtype=torch.float64)
	q_value: tensor([[-25.9915]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 2, step: 109
	action: tensor([[-0.0818,  0.0228, -1.3761, -0.1760,  0.0022,  0.0549,  0.4037]],
       dtype=torch.float64)
	q_value: tensor([[-32.0181]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3362520346153697, distance: 0.9323056039003529 entropy 0.8164090514183044
epoch: 2, step: 110
	action: tensor([[ 0.3906,  0.9806, -0.1325,  0.1656,  0.1896,  0.1406, -0.1753]],
       dtype=torch.float64)
	q_value: tensor([[-23.3467]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 2, step: 111
	action: tensor([[-0.1243,  0.2481, -0.3104,  0.0150, -0.5696, -0.2793,  0.0775]],
       dtype=torch.float64)
	q_value: tensor([[-32.0181]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3954926403916236, distance: 0.8897284740595254 entropy 0.8164090514183044
epoch: 2, step: 112
	action: tensor([[-0.2461, -0.1944,  0.1565, -0.0202, -0.5683, -1.0278,  1.4485]],
       dtype=torch.float64)
	q_value: tensor([[-19.4512]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10934675332795285, distance: 1.2052866648643885 entropy 0.8164090514183044
epoch: 2, step: 113
	action: tensor([[ 0.5589,  0.6073, -0.5033, -0.1885, -0.3256,  0.1284, -0.3907]],
       dtype=torch.float64)
	q_value: tensor([[-34.6110]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9394028604219075, distance: 0.2816973460511865 entropy 0.8164090514183044
epoch: 2, step: 114
	action: tensor([[ 0.1145, -1.6786, -0.1122,  0.5119,  0.7485,  0.1749,  0.2509]],
       dtype=torch.float64)
	q_value: tensor([[-21.1469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 2, step: 115
	action: tensor([[-0.2893,  0.3010, -0.0592,  1.3427,  0.0791,  0.0491, -0.1154]],
       dtype=torch.float64)
	q_value: tensor([[-32.0181]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.48446264639322767, distance: 0.821649815753772 entropy 0.8164090514183044
epoch: 2, step: 116
	action: tensor([[-0.6017, -0.2912, -0.0917, -0.5731,  0.7061, -0.4647, -0.0230]],
       dtype=torch.float64)
	q_value: tensor([[-26.6313]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8786942401395967, distance: 1.5685008024061746 entropy 0.8164090514183044
epoch: 2, step: 117
	action: tensor([[ 0.4996, -0.1573,  0.2655, -0.4483, -0.1316, -0.2465,  0.8631]],
       dtype=torch.float64)
	q_value: tensor([[-23.7187]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19391771475971442, distance: 1.0274161349903168 entropy 0.8164090514183044
epoch: 2, step: 118
	action: tensor([[-1.0813,  0.1090, -0.0791, -0.1405, -0.1680, -0.2338,  0.4321]],
       dtype=torch.float64)
	q_value: tensor([[-25.8009]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.028621998068759, distance: 1.629886109576114 entropy 0.8164090514183044
epoch: 2, step: 119
	action: tensor([[ 1.4472,  1.2841, -0.8469, -0.4158,  0.1386,  0.2858, -0.1351]],
       dtype=torch.float64)
	q_value: tensor([[-22.9439]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 2, step: 120
	action: tensor([[ 0.2599, -0.4422, -0.6193,  0.4935,  0.0438,  0.1752,  0.1657]],
       dtype=torch.float64)
	q_value: tensor([[-32.0181]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3037190541033099, distance: 0.954880333030054 entropy 0.8164090514183044
epoch: 2, step: 121
	action: tensor([[ 0.6534,  0.2014, -0.0604,  0.0699, -0.2689, -0.2687,  0.0577]],
       dtype=torch.float64)
	q_value: tensor([[-22.0478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8311029738378534, distance: 0.4702921121202094 entropy 0.8164090514183044
epoch: 2, step: 122
	action: tensor([[-1.1946,  0.0954, -0.7017,  0.5528, -0.3481,  0.5640, -0.6438]],
       dtype=torch.float64)
	q_value: tensor([[-21.4209]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9462896508040297, distance: 1.5964687796099237 entropy 0.8164090514183044
epoch: 2, step: 123
	action: tensor([[ 0.0088,  0.4859,  0.0758,  0.6936, -0.3256, -0.8680, -0.4477]],
       dtype=torch.float64)
	q_value: tensor([[-24.2880]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7483811696737332, distance: 0.5740216369704535 entropy 0.8164090514183044
epoch: 2, step: 124
	action: tensor([[ 0.2041, -0.0367, -0.0330,  0.3059,  0.7005, -0.3907,  0.0758]],
       dtype=torch.float64)
	q_value: tensor([[-27.1287]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.477911660722495, distance: 0.8268537302279372 entropy 0.8164090514183044
epoch: 2, step: 125
	action: tensor([[ 0.3930,  0.4148, -1.1153, -0.2000,  1.3354, -0.8439, -0.3931]],
       dtype=torch.float64)
	q_value: tensor([[-24.2532]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7341363955513633, distance: 0.5900463626236923 entropy 0.8164090514183044
epoch: 2, step: 126
	action: tensor([[ 0.5678, -1.4898, -0.4961,  0.1961,  0.8037,  0.2687,  0.7882]],
       dtype=torch.float64)
	q_value: tensor([[-33.9251]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3819409519117005, distance: 1.345245155190203 entropy 0.8164090514183044
epoch: 2, step: 127
	action: tensor([[ 0.2865,  1.3426,  0.4917, -0.9573, -0.5704,  0.3311, -0.3422]],
       dtype=torch.float64)
	q_value: tensor([[-33.3373]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6351443402335111, distance: 0.6912214982839711 entropy 0.8164090514183044
LOSS epoch 2 actor 325.77685957825895 critic 225.0122299438243 
epoch: 3, step: 0
	action: tensor([[ 0.2870,  0.2672, -0.0467, -0.6376, -0.1391,  0.3494, -0.5831]],
       dtype=torch.float64)
	q_value: tensor([[-30.3252]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5643124493123797, distance: 0.7553431039223084 entropy 0.8164090514183044
epoch: 3, step: 1
	action: tensor([[-0.3489, -0.2045, -0.3924,  0.0469,  0.1124,  0.5140, -0.1753]],
       dtype=torch.float64)
	q_value: tensor([[-20.1256]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10936263246490396, distance: 1.2052952910416732 entropy 0.8164090514183044
epoch: 3, step: 2
	action: tensor([[-0.4329,  0.0683, -0.7605,  0.1938, -0.1484, -1.7773, -0.4598]],
       dtype=torch.float64)
	q_value: tensor([[-16.8925]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26212621422428783, distance: 0.9829869088447372 entropy 0.8164090514183044
epoch: 3, step: 3
	action: tensor([[-0.5642,  0.3114, -0.1076, -0.7685,  0.2763, -0.3310,  0.1001]],
       dtype=torch.float64)
	q_value: tensor([[-32.1342]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.37359152608267787, distance: 1.3411751399276204 entropy 0.8164090514183044
epoch: 3, step: 4
	action: tensor([[ 0.1655, -0.3952,  0.4118,  1.0688,  0.3758,  0.3513,  0.2530]],
       dtype=torch.float64)
	q_value: tensor([[-17.7558]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.881034770154985, distance: 0.3946996281774002 entropy 0.8164090514183044
epoch: 3, step: 5
	action: tensor([[0.5956, 0.0646, 1.0854, 0.2541, 0.6649, 0.6858, 0.7491]],
       dtype=torch.float64)
	q_value: tensor([[-27.5612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8435974211839801, distance: 0.452562586731322 entropy 0.8164090514183044
epoch: 3, step: 6
	action: tensor([[ 0.1852,  0.4266, -0.5845,  0.0069,  0.6261,  0.4171,  0.3257]],
       dtype=torch.float64)
	q_value: tensor([[-29.3821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7599368747390727, distance: 0.5606856242395132 entropy 0.8164090514183044
epoch: 3, step: 7
	action: tensor([[-0.1329, -0.3100,  0.5400, -0.1507,  1.2154, -0.4715,  0.8531]],
       dtype=torch.float64)
	q_value: tensor([[-19.0043]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.27809484675172236, distance: 1.2937138726399762 entropy 0.8164090514183044
epoch: 3, step: 8
	action: tensor([[ 1.0671,  0.1841, -0.1983, -1.1020, -0.5229,  0.2017,  0.1835]],
       dtype=torch.float64)
	q_value: tensor([[-30.5510]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33136125889754775, distance: 0.9357341108073173 entropy 0.8164090514183044
epoch: 3, step: 9
	action: tensor([[ 0.0304,  0.3198, -1.1285, -0.5033, -0.3854,  0.0573, -1.4070]],
       dtype=torch.float64)
	q_value: tensor([[-28.0142]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6808601902426363, distance: 0.6464682261033535 entropy 0.8164090514183044
epoch: 3, step: 10
	action: tensor([[ 0.3045,  0.2196,  0.3263,  0.4207, -0.2222,  0.6213,  0.1962]],
       dtype=torch.float64)
	q_value: tensor([[-24.7375]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8946139338019916, distance: 0.37149099435696037 entropy 0.8164090514183044
epoch: 3, step: 11
	action: tensor([[ 0.4110, -1.3072, -0.3735, -0.1033,  0.2624, -0.0745, -0.4246]],
       dtype=torch.float64)
	q_value: tensor([[-20.7184]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7557255612208214, distance: 1.5162995610810897 entropy 0.8164090514183044
epoch: 3, step: 12
	action: tensor([[-0.0204, -0.4900,  0.5498, -0.3705, -0.7568, -0.6162,  0.0205]],
       dtype=torch.float64)
	q_value: tensor([[-26.1197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4465637253145258, distance: 1.3763391971183891 entropy 0.8164090514183044
epoch: 3, step: 13
	action: tensor([[-0.2743,  0.4910,  0.2921, -0.1624,  0.4219, -0.3171, -0.1280]],
       dtype=torch.float64)
	q_value: tensor([[-25.1821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1335881482138621, distance: 1.065169900046161 entropy 0.8164090514183044
epoch: 3, step: 14
	action: tensor([[ 1.7806,  0.2904, -0.3012, -0.2900, -0.5727, -0.3717, -0.0296]],
       dtype=torch.float64)
	q_value: tensor([[-18.1147]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2811607133383741, distance: 0.9702253005815407 entropy 0.8164090514183044
epoch: 3, step: 15
	action: tensor([[ 0.5257,  1.3146,  0.3274, -0.4583,  0.1160,  0.5846, -0.1460]],
       dtype=torch.float64)
	q_value: tensor([[-29.4087]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 3, step: 16
	action: tensor([[ 0.9704,  0.4056, -0.5345, -0.4092, -0.1651,  0.6155, -0.0537]],
       dtype=torch.float64)
	q_value: tensor([[-28.0345]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.923396398675035, distance: 0.31672422851752746 entropy 0.8164090514183044
epoch: 3, step: 17
	action: tensor([[-0.1132, -0.7211,  0.2269, -0.3058,  0.4307,  0.1903, -0.1941]],
       dtype=torch.float64)
	q_value: tensor([[-23.5273]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.557398446617628, distance: 1.4280932349803104 entropy 0.8164090514183044
epoch: 3, step: 18
	action: tensor([[ 0.5279,  1.3450,  0.8313,  0.0392,  0.4812,  0.5808, -0.1088]],
       dtype=torch.float64)
	q_value: tensor([[-20.8157]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 3, step: 19
	action: tensor([[ 1.3936, -0.5151, -0.9266, -0.4612,  0.3944,  0.0345, -0.8338]],
       dtype=torch.float64)
	q_value: tensor([[-28.0345]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21802900904108635, distance: 1.2629481152898587 entropy 0.8164090514183044
epoch: 3, step: 20
	action: tensor([[-0.6377, -0.1893,  0.1106, -0.8065,  0.6071, -0.8499, -0.2619]],
       dtype=torch.float64)
	q_value: tensor([[-30.2440]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8549283124141511, distance: 1.5585482731813995 entropy 0.8164090514183044
epoch: 3, step: 21
	action: tensor([[-0.6254, -0.4269,  0.6456,  0.3152,  0.0707,  0.2135, -0.4656]],
       dtype=torch.float64)
	q_value: tensor([[-23.6493]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4271917864579826, distance: 1.3670923784033946 entropy 0.8164090514183044
epoch: 3, step: 22
	action: tensor([[ 0.0527, -0.4365,  0.4046, -0.3940, -0.8160, -0.1732,  0.6683]],
       dtype=torch.float64)
	q_value: tensor([[-23.1964]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3626296691856876, distance: 1.335812840098379 entropy 0.8164090514183044
epoch: 3, step: 23
	action: tensor([[ 0.6311,  1.5467, -0.2560,  0.1236, -1.0685,  0.1652,  0.4114]],
       dtype=torch.float64)
	q_value: tensor([[-25.0832]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 3, step: 24
	action: tensor([[ 0.6189,  0.0954, -1.2388, -0.7090, -0.0805, -1.0818, -0.2368]],
       dtype=torch.float64)
	q_value: tensor([[-28.0345]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6618138781397485, distance: 0.665479350410257 entropy 0.8164090514183044
epoch: 3, step: 25
	action: tensor([[ 0.5260,  0.1174, -0.0141,  0.4363, -0.0458, -0.5900,  1.0425]],
       dtype=torch.float64)
	q_value: tensor([[-27.5945]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8240402394194563, distance: 0.4800244674350671 entropy 0.8164090514183044
epoch: 3, step: 26
	action: tensor([[ 0.0429,  0.5136,  0.7390,  0.6054, -0.3185,  0.2315, -0.2762]],
       dtype=torch.float64)
	q_value: tensor([[-25.6396]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8947478634631175, distance: 0.37125486503660543 entropy 0.8164090514183044
epoch: 3, step: 27
	action: tensor([[ 0.3636,  0.2849, -1.3512,  0.3053, -1.1727,  0.6340, -0.0406]],
       dtype=torch.float64)
	q_value: tensor([[-23.3592]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5261740695455707, distance: 0.7877095400973176 entropy 0.8164090514183044
epoch: 3, step: 28
	action: tensor([[ 0.4155,  0.4455,  0.1808,  0.7050,  0.0570, -0.5408,  0.0854]],
       dtype=torch.float64)
	q_value: tensor([[-25.7199]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9069742117330182, distance: 0.3490264762353776 entropy 0.8164090514183044
epoch: 3, step: 29
	action: tensor([[ 0.5415, -0.0482,  0.1603, -0.2532, -0.5953,  0.8110, -0.2296]],
       dtype=torch.float64)
	q_value: tensor([[-22.9516]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6864776180226704, distance: 0.640753472208823 entropy 0.8164090514183044
epoch: 3, step: 30
	action: tensor([[-0.3845, -0.1321,  0.7306,  0.0831, -0.6044, -0.0307, -0.0990]],
       dtype=torch.float64)
	q_value: tensor([[-24.2442]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1577094263256389, distance: 1.23127901567904 entropy 0.8164090514183044
epoch: 3, step: 31
	action: tensor([[ 0.6503, -0.1306, -0.5842,  0.4062, -0.1709,  0.1968,  0.5563]],
       dtype=torch.float64)
	q_value: tensor([[-22.5637]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7534164120625599, distance: 0.5682491267973813 entropy 0.8164090514183044
epoch: 3, step: 32
	action: tensor([[-0.0526, -0.8595, -0.6384,  0.0533, -1.3001, -0.1150,  0.1633]],
       dtype=torch.float64)
	q_value: tensor([[-20.9780]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4571018307990584, distance: 1.3813433625151923 entropy 0.8164090514183044
epoch: 3, step: 33
	action: tensor([[ 1.4493, -0.8485, -0.3591,  0.4665, -0.2417, -0.4713,  0.2801]],
       dtype=torch.float64)
	q_value: tensor([[-26.4951]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11974210681676967, distance: 1.2109206856802657 entropy 0.8164090514183044
epoch: 3, step: 34
	action: tensor([[ 0.9508,  1.0109, -0.6708,  0.1938, -0.8439, -0.5395,  0.1218]],
       dtype=torch.float64)
	q_value: tensor([[-33.5371]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 3, step: 35
	action: tensor([[-0.7461, -0.1496, -0.2035,  0.4559, -0.1510, -0.1062, -1.3004]],
       dtype=torch.float64)
	q_value: tensor([[-28.0345]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5543463831835975, distance: 1.4266932179613956 entropy 0.8164090514183044
epoch: 3, step: 36
	action: tensor([[-0.3162,  0.1348, -0.9260,  0.0099,  0.6100, -0.8456,  0.5695]],
       dtype=torch.float64)
	q_value: tensor([[-24.5644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.026481926961710145, distance: 1.1290903678568065 entropy 0.8164090514183044
epoch: 3, step: 37
	action: tensor([[ 0.3558,  0.8841,  1.0778, -0.0693,  0.5595,  0.0326, -0.1724]],
       dtype=torch.float64)
	q_value: tensor([[-24.4642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8348759511648166, distance: 0.4650095345505127 entropy 0.8164090514183044
epoch: 3, step: 38
	action: tensor([[ 0.2522,  0.0028, -0.2340, -0.0941, -0.6476, -0.3913, -0.7627]],
       dtype=torch.float64)
	q_value: tensor([[-25.1786]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.49984624003883926, distance: 0.8092979909499826 entropy 0.8164090514183044
epoch: 3, step: 39
	action: tensor([[ 0.0484, -0.1649, -0.6620, -0.4976,  0.1066, -0.5204, -0.1170]],
       dtype=torch.float64)
	q_value: tensor([[-21.8803]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17823323821674086, distance: 1.0373635376846213 entropy 0.8164090514183044
epoch: 3, step: 40
	action: tensor([[-0.5172, -0.4210,  0.0545, -0.2136, -0.6301,  0.2002,  0.5264]],
       dtype=torch.float64)
	q_value: tensor([[-19.3577]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7060780985268036, distance: 1.494707271118377 entropy 0.8164090514183044
epoch: 3, step: 41
	action: tensor([[-0.0719,  0.9138, -0.7329, -0.6525, -0.0712, -0.1674,  0.4757]],
       dtype=torch.float64)
	q_value: tensor([[-21.4609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.780878319449779, distance: 0.5356725208251961 entropy 0.8164090514183044
epoch: 3, step: 42
	action: tensor([[-0.0021,  0.5259, -0.1814,  0.6761, -0.1940, -0.2503, -0.2644]],
       dtype=torch.float64)
	q_value: tensor([[-20.9362]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6131311378456894, distance: 0.7117682006855989 entropy 0.8164090514183044
epoch: 3, step: 43
	action: tensor([[ 0.0372,  0.6106, -0.2281,  0.2982, -0.0219,  0.0646, -0.1214]],
       dtype=torch.float64)
	q_value: tensor([[-18.8405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6388177844060401, distance: 0.6877330130378612 entropy 0.8164090514183044
epoch: 3, step: 44
	action: tensor([[-0.0536,  0.4159, -0.2874,  0.0092, -0.3189, -0.4131,  0.1034]],
       dtype=torch.float64)
	q_value: tensor([[-16.0510]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5470784056629661, distance: 0.7701373867588619 entropy 0.8164090514183044
epoch: 3, step: 45
	action: tensor([[-0.1033,  0.5403, -0.2986, -0.5530, -0.2311, -0.4881, -0.2171]],
       dtype=torch.float64)
	q_value: tensor([[-17.0261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.48252625063622334, distance: 0.8231914576836618 entropy 0.8164090514183044
epoch: 3, step: 46
	action: tensor([[-0.1061, -0.1029, -0.0488, -0.2474, -0.4772, -0.3137, -0.5449]],
       dtype=torch.float64)
	q_value: tensor([[-18.5593]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.00898003663691449, distance: 1.1391945401099735 entropy 0.8164090514183044
epoch: 3, step: 47
	action: tensor([[ 0.4843,  0.2405,  0.3008, -0.6091,  0.0640,  0.3105,  0.4462]],
       dtype=torch.float64)
	q_value: tensor([[-19.3398]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6103107080546543, distance: 0.7143580274378398 entropy 0.8164090514183044
epoch: 3, step: 48
	action: tensor([[ 0.4377, -0.2545, -0.5835,  0.4922,  0.6273,  0.5715, -0.5699]],
       dtype=torch.float64)
	q_value: tensor([[-21.3155]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7291240486214592, distance: 0.5955824858925759 entropy 0.8164090514183044
epoch: 3, step: 49
	action: tensor([[-0.1072, -0.0218,  0.5069, -0.7986, -0.0199, -0.4378,  0.1082]],
       dtype=torch.float64)
	q_value: tensor([[-23.3167]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.40948300459704456, distance: 1.3585843736310825 entropy 0.8164090514183044
epoch: 3, step: 50
	action: tensor([[ 0.9405, -0.3978,  0.4913,  0.1361,  0.1959, -0.5198, -0.1812]],
       dtype=torch.float64)
	q_value: tensor([[-21.7186]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35867589592627835, distance: 0.9164219331862897 entropy 0.8164090514183044
epoch: 3, step: 51
	action: tensor([[ 1.1871, -0.2281, -0.7001,  0.0842,  0.6356, -0.1254, -1.0502]],
       dtype=torch.float64)
	q_value: tensor([[-28.2482]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4715743822896643, distance: 0.8318569035280602 entropy 0.8164090514183044
epoch: 3, step: 52
	action: tensor([[ 0.9597,  0.9400, -0.3573, -0.9932, -0.6648, -0.3583, -0.5564]],
       dtype=torch.float64)
	q_value: tensor([[-30.1885]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8305237179769851, distance: 0.4710978879619071 entropy 0.8164090514183044
epoch: 3, step: 53
	action: tensor([[ 0.1140, -0.2132, -0.1410, -0.0011, -0.7421, -0.2772, -0.2955]],
       dtype=torch.float64)
	q_value: tensor([[-29.1127]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2674867999817152, distance: 0.9794097449386254 entropy 0.8164090514183044
epoch: 3, step: 54
	action: tensor([[ 0.6576,  0.3456,  0.5973,  1.3273, -0.0097,  0.2988,  0.2656]],
       dtype=torch.float64)
	q_value: tensor([[-20.3241]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7902908723338471, distance: 0.5240411127130196 entropy 0.8164090514183044
epoch: 3, step: 55
	action: tensor([[ 0.1385,  0.3431, -0.2622, -0.3655,  0.2233, -0.0675,  0.4829]],
       dtype=torch.float64)
	q_value: tensor([[-29.4251]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5395987769618398, distance: 0.7764704412088751 entropy 0.8164090514183044
epoch: 3, step: 56
	action: tensor([[ 0.0447,  0.7783, -0.3759,  0.3119,  0.4126, -0.2764, -0.0707]],
       dtype=torch.float64)
	q_value: tensor([[-17.4025]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6192172182957709, distance: 0.7061473673969061 entropy 0.8164090514183044
epoch: 3, step: 57
	action: tensor([[ 0.9181,  0.4032,  1.5898, -0.4588, -0.0821,  0.7613, -0.0237]],
       dtype=torch.float64)
	q_value: tensor([[-18.6877]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9761895151153377, distance: 0.1765797156280514 entropy 0.8164090514183044
epoch: 3, step: 58
	action: tensor([[ 0.7233,  0.3050, -0.0364,  0.1128,  0.0599,  0.7134, -0.1709]],
       dtype=torch.float64)
	q_value: tensor([[-34.0160]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.09688373705843133 entropy 0.8164090514183044
epoch: 3, step: 59
	action: tensor([[ 0.1152,  0.1642, -0.0306,  0.2837,  0.2980,  0.6337,  0.7994]],
       dtype=torch.float64)
	q_value: tensor([[-28.0345]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7480645702568645, distance: 0.5743826548327545 entropy 0.8164090514183044
epoch: 3, step: 60
	action: tensor([[ 0.6210,  0.2577, -0.3497, -0.5839, -0.7217,  0.3696,  0.3808]],
       dtype=torch.float64)
	q_value: tensor([[-20.8370]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6951597161947458, distance: 0.6318192765367481 entropy 0.8164090514183044
epoch: 3, step: 61
	action: tensor([[-0.3432,  0.2495, -0.5603,  0.0616, -0.3005,  0.2377, -0.1522]],
       dtype=torch.float64)
	q_value: tensor([[-23.8926]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08407823849382334, distance: 1.0951809529368282 entropy 0.8164090514183044
epoch: 3, step: 62
	action: tensor([[ 1.0731, -0.2204, -0.9599,  0.6082,  0.2825, -0.7321,  0.3349]],
       dtype=torch.float64)
	q_value: tensor([[-15.7604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6226057160521185, distance: 0.7029984254032279 entropy 0.8164090514183044
epoch: 3, step: 63
	action: tensor([[ 0.8651, -0.0369,  0.2006, -0.5927, -0.3901, -0.5936, -0.5083]],
       dtype=torch.float64)
	q_value: tensor([[-31.1201]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26945216494988133, distance: 0.9780949628409996 entropy 0.8164090514183044
epoch: 3, step: 64
	action: tensor([[-1.1942,  0.4038,  0.3260, -0.0055, -1.3530, -0.0682,  0.7831]],
       dtype=torch.float64)
	q_value: tensor([[-26.5239]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8226732468977918, distance: 1.544938168596565 entropy 0.8164090514183044
epoch: 3, step: 65
	action: tensor([[ 0.7088,  0.0683,  0.6877, -0.3897, -0.9766,  0.0122, -0.6749]],
       dtype=torch.float64)
	q_value: tensor([[-28.7032]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5900593149826728, distance: 0.732684838113857 entropy 0.8164090514183044
epoch: 3, step: 66
	action: tensor([[ 0.2461,  0.2282,  0.3602, -0.1187, -0.4547,  0.0184,  0.4456]],
       dtype=torch.float64)
	q_value: tensor([[-30.1470]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6104691376993214, distance: 0.7142128002066707 entropy 0.8164090514183044
epoch: 3, step: 67
	action: tensor([[ 0.6728,  0.2582, -1.1906, -0.7113, -0.2336, -0.3235,  0.2336]],
       dtype=torch.float64)
	q_value: tensor([[-19.6737]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7482474349417405, distance: 0.5741741621850295 entropy 0.8164090514183044
epoch: 3, step: 68
	action: tensor([[-0.1523, -0.1220, -0.0699,  0.3312,  0.6757,  0.9301,  0.0867]],
       dtype=torch.float64)
	q_value: tensor([[-23.6862]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5537710491744463, distance: 0.7644262022656081 entropy 0.8164090514183044
epoch: 3, step: 69
	action: tensor([[ 1.2741, -0.7721,  0.0651, -0.6919, -0.5021, -0.3368,  0.4091]],
       dtype=torch.float64)
	q_value: tensor([[-21.4901]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6286923679852781, distance: 1.4604147947081765 entropy 0.8164090514183044
epoch: 3, step: 70
	action: tensor([[ 0.5672, -0.0855,  0.4380, -2.0270,  0.2853, -0.1179, -0.2331]],
       dtype=torch.float64)
	q_value: tensor([[-31.3193]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24442621505401996, distance: 1.27656010853137 entropy 0.8164090514183044
epoch: 3, step: 71
	action: tensor([[-0.5674,  0.1159, -0.4906,  0.1332, -0.1811, -0.3600,  0.3172]],
       dtype=torch.float64)
	q_value: tensor([[-31.8163]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2689346119879934, distance: 1.289069446959321 entropy 0.8164090514183044
epoch: 3, step: 72
	action: tensor([[ 0.2831,  0.4493,  0.0900, -0.3921,  0.0115,  0.6600, -0.3210]],
       dtype=torch.float64)
	q_value: tensor([[-18.2978]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7613952355159197, distance: 0.5589799738129363 entropy 0.8164090514183044
epoch: 3, step: 73
	action: tensor([[-0.0623, -0.5722,  0.2587, -0.2566, -0.1519, -0.0327, -0.4585]],
       dtype=torch.float64)
	q_value: tensor([[-20.2144]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4236626489611668, distance: 1.365401069885 entropy 0.8164090514183044
epoch: 3, step: 74
	action: tensor([[ 0.7169, -0.2096, -0.5823,  0.2054,  0.0560,  0.2114,  0.1953]],
       dtype=torch.float64)
	q_value: tensor([[-20.5562]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6784217784521995, distance: 0.6489332211914385 entropy 0.8164090514183044
epoch: 3, step: 75
	action: tensor([[ 0.0149,  0.8464, -0.1987, -0.5290, -0.0157,  0.9930,  0.3752]],
       dtype=torch.float64)
	q_value: tensor([[-19.9387]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.69472707323353, distance: 0.6322674706528674 entropy 0.8164090514183044
epoch: 3, step: 76
	action: tensor([[ 0.8800,  1.1232, -0.7615,  0.5693, -0.1181,  0.3430, -0.0447]],
       dtype=torch.float64)
	q_value: tensor([[-23.2121]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 3, step: 77
	action: tensor([[ 0.8904,  0.5833, -0.2482, -0.1029, -0.0311,  0.2624,  1.3052]],
       dtype=torch.float64)
	q_value: tensor([[-28.0345]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.989524615832168, distance: 0.11712286106937587 entropy 0.8164090514183044
epoch: 3, step: 78
	action: tensor([[-0.0638,  0.0985,  0.5483, -0.9279,  0.9693,  0.4114, -0.1787]],
       dtype=torch.float64)
	q_value: tensor([[-25.7037]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1500821227238287, distance: 1.2272163130725107 entropy 0.8164090514183044
epoch: 3, step: 79
	action: tensor([[ 0.9464,  0.1374,  0.2951, -0.0084, -0.6732, -0.5915,  0.1067]],
       dtype=torch.float64)
	q_value: tensor([[-22.3386]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7149593613222118, distance: 0.61095621139204 entropy 0.8164090514183044
epoch: 3, step: 80
	action: tensor([[0.5790, 0.3018, 0.4449, 0.8709, 0.3042, 0.2903, 0.0784]],
       dtype=torch.float64)
	q_value: tensor([[-26.1387]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9669362963059122, distance: 0.20808100818340833 entropy 0.8164090514183044
epoch: 3, step: 81
	action: tensor([[ 0.3234,  0.0954,  0.3222,  0.5533, -0.3722, -0.3671,  1.2853]],
       dtype=torch.float64)
	q_value: tensor([[-24.5575]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8365223608520784, distance: 0.4626854812931989 entropy 0.8164090514183044
epoch: 3, step: 82
	action: tensor([[ 0.8383,  0.5635,  0.2702, -1.5544,  0.0434,  0.2293, -0.5888]],
       dtype=torch.float64)
	q_value: tensor([[-26.7717]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5110247187653623, distance: 0.8002029458407842 entropy 0.8164090514183044
epoch: 3, step: 83
	action: tensor([[ 0.1580, -0.0347,  0.4699, -0.0358, -0.9013, -0.7366,  0.2572]],
       dtype=torch.float64)
	q_value: tensor([[-29.6626]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31814420597669446, distance: 0.9449372313119764 entropy 0.8164090514183044
epoch: 3, step: 84
	action: tensor([[ 0.3538, -0.4018, -0.7442,  0.5028, -0.3762,  0.1049,  0.8483]],
       dtype=torch.float64)
	q_value: tensor([[-24.9715]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35735408378576194, distance: 0.9173658507076526 entropy 0.8164090514183044
epoch: 3, step: 85
	action: tensor([[ 0.4761,  1.7432,  0.5593, -0.3972,  0.1662,  0.6042,  0.4705]],
       dtype=torch.float64)
	q_value: tensor([[-23.1076]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 3, step: 86
	action: tensor([[ 0.1972,  0.4772,  0.3738,  0.2019,  0.6294,  0.6553, -0.0073]],
       dtype=torch.float64)
	q_value: tensor([[-28.0345]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9090379672892327, distance: 0.3451332266727161 entropy 0.8164090514183044
epoch: 3, step: 87
	action: tensor([[ 0.9870,  0.2279, -0.5223,  0.7883,  1.1473,  0.6816, -0.3573]],
       dtype=torch.float64)
	q_value: tensor([[-19.9283]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9763240476982047, distance: 0.1760801588934878 entropy 0.8164090514183044
epoch: 3, step: 88
	action: tensor([[-0.1074, -0.5777, -1.0817, -0.6605,  1.0512,  0.5823,  0.1677]],
       dtype=torch.float64)
	q_value: tensor([[-30.3682]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10747136696781867, distance: 1.0811047413851074 entropy 0.8164090514183044
epoch: 3, step: 89
	action: tensor([[ 0.3245, -0.2123,  0.8826, -0.4997, -0.3830,  0.2432,  0.1669]],
       dtype=torch.float64)
	q_value: tensor([[-26.0280]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14372949135742596, distance: 1.058917648375577 entropy 0.8164090514183044
epoch: 3, step: 90
	action: tensor([[-0.2120,  0.0298, -0.4514, -1.2951,  0.0309, -0.5187, -0.2134]],
       dtype=torch.float64)
	q_value: tensor([[-25.1486]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.006967097770323338, distance: 1.1483237138597087 entropy 0.8164090514183044
epoch: 3, step: 91
	action: tensor([[ 1.0101,  0.1830, -0.3490, -0.2186,  0.2452, -0.9215,  0.1314]],
       dtype=torch.float64)
	q_value: tensor([[-21.9708]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5872692720031415, distance: 0.7351739247331791 entropy 0.8164090514183044
epoch: 3, step: 92
	action: tensor([[ 0.9462, -0.3560, -0.3199,  1.3272, -0.3174,  0.7531,  0.0789]],
       dtype=torch.float64)
	q_value: tensor([[-27.0088]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9373547279912557, distance: 0.2864183528996245 entropy 0.8164090514183044
epoch: 3, step: 93
	action: tensor([[ 0.4336, -0.3381, -0.2125,  0.6055,  0.7814,  0.2546,  0.2238]],
       dtype=torch.float64)
	q_value: tensor([[-30.1847]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7276653164270458, distance: 0.5971840101568818 entropy 0.8164090514183044
epoch: 3, step: 94
	action: tensor([[ 0.0352,  0.5093, -0.1817, -0.2440, -0.4096,  0.8277,  0.1994]],
       dtype=torch.float64)
	q_value: tensor([[-24.5984]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.58638712397265, distance: 0.7359591655952389 entropy 0.8164090514183044
epoch: 3, step: 95
	action: tensor([[ 0.1158, -0.5481,  0.5479,  0.4484, -1.3370, -0.3951,  0.3243]],
       dtype=torch.float64)
	q_value: tensor([[-21.2746]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2765968679229398, distance: 0.9733003636854588 entropy 0.8164090514183044
epoch: 3, step: 96
	action: tensor([[-0.0706,  0.6231,  0.6271,  0.5958,  0.8649, -0.0085, -0.1026]],
       dtype=torch.float64)
	q_value: tensor([[-29.5260]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.795653031337832, distance: 0.5172979925829458 entropy 0.8164090514183044
epoch: 3, step: 97
	action: tensor([[-0.1996,  1.0767, -1.0469,  0.1337, -0.4153, -0.3982,  1.1108]],
       dtype=torch.float64)
	q_value: tensor([[-22.9240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 3, step: 98
	action: tensor([[-0.6947,  1.8581,  0.0419,  0.6849,  0.3350,  0.1672,  0.2075]],
       dtype=torch.float64)
	q_value: tensor([[-28.0345]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 3, step: 99
	action: tensor([[ 0.3246,  0.3958,  0.2492,  0.2312, -0.1843,  1.0883,  0.0521]],
       dtype=torch.float64)
	q_value: tensor([[-28.0345]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8761900507015747, distance: 0.40265627000539306 entropy 0.8164090514183044
epoch: 3, step: 100
	action: tensor([[-0.4031,  0.6163, -0.8046,  0.4501,  0.1386,  0.0410, -0.1117]],
       dtype=torch.float64)
	q_value: tensor([[-23.2579]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10554437728523569, distance: 1.0822711770357978 entropy 0.8164090514183044
epoch: 3, step: 101
	action: tensor([[-0.1920,  0.1382,  0.5088, -0.6293, -0.9347, -0.3109,  0.6315]],
       dtype=torch.float64)
	q_value: tensor([[-17.6767]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1952421540235516, distance: 1.251078739962961 entropy 0.8164090514183044
epoch: 3, step: 102
	action: tensor([[ 1.3786,  0.8799, -1.0412, -0.6017, -0.3586, -0.5619,  0.7061]],
       dtype=torch.float64)
	q_value: tensor([[-25.9803]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4748367495778155, distance: 0.8292850899609634 entropy 0.8164090514183044
epoch: 3, step: 103
	action: tensor([[ 0.6786,  0.3836,  1.4217,  0.0537, -0.2741,  0.9668, -0.8323]],
       dtype=torch.float64)
	q_value: tensor([[-30.6030]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9824429400382181, distance: 0.15162911125658834 entropy 0.8164090514183044
epoch: 3, step: 104
	action: tensor([[-0.1898,  0.4547,  0.3108, -0.8179, -0.3965,  1.0629, -1.0710]],
       dtype=torch.float64)
	q_value: tensor([[-34.5338]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23109540888018076, distance: 1.0034434912899088 entropy 0.8164090514183044
epoch: 3, step: 105
	action: tensor([[ 0.4918, -0.4126, -0.4957, -0.2754, -0.8722, -1.0814,  0.2000]],
       dtype=torch.float64)
	q_value: tensor([[-29.0168]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3335630007440744, distance: 0.9341922140765622 entropy 0.8164090514183044
epoch: 3, step: 106
	action: tensor([[ 1.2386, -0.2107, -0.7324, -0.2822, -0.1206, -0.5446,  0.7318]],
       dtype=torch.float64)
	q_value: tensor([[-27.6159]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13719827853347055, distance: 1.0629484298365657 entropy 0.8164090514183044
epoch: 3, step: 107
	action: tensor([[ 0.2648, -0.2739,  0.3190,  0.0114, -0.4603, -0.3547, -0.5362]],
       dtype=torch.float64)
	q_value: tensor([[-28.1409]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2454872231187547, distance: 0.9940082581896961 entropy 0.8164090514183044
epoch: 3, step: 108
	action: tensor([[ 0.7279,  0.0255,  0.4272, -0.1726, -0.1449,  0.0599,  0.2654]],
       dtype=torch.float64)
	q_value: tensor([[-22.5782]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6893216532272843, distance: 0.6378406387490335 entropy 0.8164090514183044
epoch: 3, step: 109
	action: tensor([[-0.0497, -0.3525,  0.7835,  0.1230, -0.9394, -0.3756,  0.1278]],
       dtype=torch.float64)
	q_value: tensor([[-21.7887]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0019448935322829985, distance: 1.1454565273185597 entropy 0.8164090514183044
epoch: 3, step: 110
	action: tensor([[ 0.3903,  0.5907,  0.0007, -0.1313,  0.2123, -0.0446, -0.5662]],
       dtype=torch.float64)
	q_value: tensor([[-26.2454]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8482745581231336, distance: 0.44574440310990054 entropy 0.8164090514183044
epoch: 3, step: 111
	action: tensor([[ 0.0913, -0.1509, -0.2011,  0.0288, -0.3341,  0.1188,  0.2237]],
       dtype=torch.float64)
	q_value: tensor([[-19.0590]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3153571466218742, distance: 0.9468664592210011 entropy 0.8164090514183044
epoch: 3, step: 112
	action: tensor([[ 0.4974,  0.3419,  0.3540, -0.1943, -0.0815, -0.0505, -0.2969]],
       dtype=torch.float64)
	q_value: tensor([[-16.7294]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7828979074024937, distance: 0.5331982287425349 entropy 0.8164090514183044
epoch: 3, step: 113
	action: tensor([[-1.0178, -0.2415, -0.3575, -0.4239,  0.5867,  0.0774,  0.7577]],
       dtype=torch.float64)
	q_value: tensor([[-20.6335]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1056755811851384, distance: 1.6605517844896784 entropy 0.8164090514183044
epoch: 3, step: 114
	action: tensor([[-0.1572,  0.2950,  0.0644, -0.8430, -0.2953,  0.3268,  0.4169]],
       dtype=torch.float64)
	q_value: tensor([[-24.8425]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.012251193150787598, distance: 1.1373128606503258 entropy 0.8164090514183044
epoch: 3, step: 115
	action: tensor([[ 0.4255, -0.3933,  0.6875, -0.7261,  0.1163,  0.0601,  0.8113]],
       dtype=torch.float64)
	q_value: tensor([[-21.1713]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17075131313526315, distance: 1.2381949261954213 entropy 0.8164090514183044
epoch: 3, step: 116
	action: tensor([[ 0.7226,  0.3776, -0.5099, -0.3622,  0.3425,  0.0773, -0.1085]],
       dtype=torch.float64)
	q_value: tensor([[-26.6312]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8718854523368359, distance: 0.4095961982986931 entropy 0.8164090514183044
epoch: 3, step: 117
	action: tensor([[ 1.1688,  0.2692,  1.2797,  0.6129, -0.2369, -0.5632, -0.4843]],
       dtype=torch.float64)
	q_value: tensor([[-20.1928]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7688165959468718, distance: 0.5502182868839433 entropy 0.8164090514183044
epoch: 3, step: 118
	action: tensor([[-0.4298, -0.2845,  0.2363, -0.1299, -0.8808, -0.2130,  0.0783]],
       dtype=torch.float64)
	q_value: tensor([[-36.5605]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.45330761066929726, distance: 1.379543715766171 entropy 0.8164090514183044
epoch: 3, step: 119
	action: tensor([[-0.1044, -0.4654, -0.0246, -0.6692, -0.3595,  0.7293,  0.4524]],
       dtype=torch.float64)
	q_value: tensor([[-21.7320]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.37426972854851237, distance: 1.3415061976043505 entropy 0.8164090514183044
epoch: 3, step: 120
	action: tensor([[-0.4158, -0.0388,  0.1276,  1.0984, -0.8194,  0.7506, -0.6940]],
       dtype=torch.float64)
	q_value: tensor([[-23.4421]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3270142951800792, distance: 0.9387708870917022 entropy 0.8164090514183044
epoch: 3, step: 121
	action: tensor([[ 0.4236,  1.3768, -1.0306, -0.2475, -0.5437, -0.5027,  0.6047]],
       dtype=torch.float64)
	q_value: tensor([[-25.3897]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 3, step: 122
	action: tensor([[ 0.3317,  0.3834,  0.2831, -0.6200, -1.0666,  0.2792,  0.3934]],
       dtype=torch.float64)
	q_value: tensor([[-28.0345]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5450844535856457, distance: 0.7718307600300575 entropy 0.8164090514183044
epoch: 3, step: 123
	action: tensor([[-0.0092,  0.6844, -0.1664,  0.3180, -0.4365, -0.3969, -0.3474]],
       dtype=torch.float64)
	q_value: tensor([[-27.0521]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6605644875201878, distance: 0.6667074864877884 entropy 0.8164090514183044
epoch: 3, step: 124
	action: tensor([[ 0.5038,  1.2767, -0.5199, -0.5822,  0.0417, -0.0064,  0.8195]],
       dtype=torch.float64)
	q_value: tensor([[-19.0826]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.8164090514183044
epoch: 3, step: 125
	action: tensor([[ 1.2466, -0.1827, -0.7069,  0.1637, -0.0523, -0.0516,  0.8543]],
       dtype=torch.float64)
	q_value: tensor([[-28.0345]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.584487763340457, distance: 0.7376470370864763 entropy 0.8164090514183044
epoch: 3, step: 126
	action: tensor([[ 0.0682,  0.5549,  0.2420,  0.1458,  0.8690, -0.8058, -0.8127]],
       dtype=torch.float64)
	q_value: tensor([[-27.0534]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5155133143161525, distance: 0.7965217084029345 entropy 0.8164090514183044
epoch: 3, step: 127
	action: tensor([[ 0.1287,  0.8102,  0.0300, -1.0169, -0.6169, -0.1676,  0.4938]],
       dtype=torch.float64)
	q_value: tensor([[-25.6783]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5508566200230933, distance: 0.7669184656563853 entropy 0.8164090514183044
LOSS epoch 3 actor 346.3095410775203 critic 1321.7696880605195 
epoch: 4, step: 0
	action: tensor([[ 0.7923, -0.0781, -0.3206, -0.9240,  0.1808,  0.4534,  0.2708]],
       dtype=torch.float64)
	q_value: tensor([[-21.1950]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3244929159305777, distance: 0.9405278222974273 entropy 0.7110485434532166
epoch: 4, step: 1
	action: tensor([[ 1.2104,  0.1780,  0.3467, -0.6965,  0.2621,  0.5395, -0.4695]],
       dtype=torch.float64)
	q_value: tensor([[-19.4972]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6309937185059853, distance: 0.69514207001829 entropy 0.7110485434532166
epoch: 4, step: 2
	action: tensor([[ 0.5710,  0.4715,  0.3958, -0.4260, -0.6834, -0.5648, -0.0626]],
       dtype=torch.float64)
	q_value: tensor([[-22.9182]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7632759770052788, distance: 0.5567726065776247 entropy 0.7110485434532166
epoch: 4, step: 3
	action: tensor([[ 0.6646, -0.1394, -0.3497, -0.0557, -0.5076,  0.3489,  1.2272]],
       dtype=torch.float64)
	q_value: tensor([[-20.6456]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6292191683553299, distance: 0.6968115334260795 entropy 0.7110485434532166
epoch: 4, step: 4
	action: tensor([[-0.1100,  0.3962, -0.1548, -0.0764, -0.3558,  0.3085, -0.2153]],
       dtype=torch.float64)
	q_value: tensor([[-21.7849]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.43773391328802735, distance: 0.8580797203926652 entropy 0.7110485434532166
epoch: 4, step: 5
	action: tensor([[-0.1906, -0.2898,  0.4438,  0.6483, -0.0345,  0.4218,  0.2006]],
       dtype=torch.float64)
	q_value: tensor([[-14.2448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5003631438937409, distance: 0.8088796821963203 entropy 0.7110485434532166
epoch: 4, step: 6
	action: tensor([[-0.6069, -0.8636, -0.2256, -0.1544,  0.5835, -0.5305,  0.3268]],
       dtype=torch.float64)
	q_value: tensor([[-19.2217]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0858693393061563, distance: 1.6527236563939007 entropy 0.7110485434532166
epoch: 4, step: 7
	action: tensor([[ 0.0337,  0.2465, -0.0540,  0.4629,  0.1842,  0.2421, -0.3389]],
       dtype=torch.float64)
	q_value: tensor([[-22.6412]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.630324977832967, distance: 0.6957716790138186 entropy 0.7110485434532166
epoch: 4, step: 8
	action: tensor([[ 0.8526,  0.5715,  0.0124, -0.0917,  0.3298,  0.0522,  0.0749]],
       dtype=torch.float64)
	q_value: tensor([[-14.9243]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.983685298922647, distance: 0.14616596350849484 entropy 0.7110485434532166
epoch: 4, step: 9
	action: tensor([[-0.2268,  0.2075, -0.2934, -0.6140, -1.1822,  0.0679,  0.3178]],
       dtype=torch.float64)
	q_value: tensor([[-17.5750]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07397485945071403, distance: 1.1012047651125974 entropy 0.7110485434532166
epoch: 4, step: 10
	action: tensor([[ 0.2322, -0.7768,  0.9670,  0.7388,  0.4464, -0.5510,  0.5744]],
       dtype=torch.float64)
	q_value: tensor([[-20.5428]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18713336494251975, distance: 1.031730673092224 entropy 0.7110485434532166
epoch: 4, step: 11
	action: tensor([[-0.0588,  0.3250, -0.2120,  0.2660,  0.7844,  0.2578, -0.4765]],
       dtype=torch.float64)
	q_value: tensor([[-28.9605]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5851131954843678, distance: 0.7370916720886516 entropy 0.7110485434532166
epoch: 4, step: 12
	action: tensor([[ 0.2709,  0.1718,  0.2183, -0.4563, -1.2553,  0.2291, -0.6718]],
       dtype=torch.float64)
	q_value: tensor([[-16.0439]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42011112461390265, distance: 0.8714231275568208 entropy 0.7110485434532166
epoch: 4, step: 13
	action: tensor([[ 0.4036,  0.1403, -0.4383, -0.3954,  0.2600, -0.1386,  0.5365]],
       dtype=torch.float64)
	q_value: tensor([[-23.7620]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5799785586252195, distance: 0.7416387679243549 entropy 0.7110485434532166
epoch: 4, step: 14
	action: tensor([[ 0.7983,  0.7446,  0.5664,  0.1220,  0.5253, -0.5141, -0.2662]],
       dtype=torch.float64)
	q_value: tensor([[-15.9086]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9774242635415432, distance: 0.17194029066842956 entropy 0.7110485434532166
epoch: 4, step: 15
	action: tensor([[ 1.5353,  0.5719, -0.3292,  0.8305, -0.1846,  0.2007,  0.8500]],
       dtype=torch.float64)
	q_value: tensor([[-21.4373]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 4, step: 16
	action: tensor([[-0.0188,  0.1093,  0.7410, -0.4763, -0.0435,  0.2182, -0.3818]],
       dtype=torch.float64)
	q_value: tensor([[-22.7756]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09492498817018602, distance: 1.0886768313549922 entropy 0.7110485434532166
epoch: 4, step: 17
	action: tensor([[ 0.7019,  0.8566,  0.0867, -0.2781,  1.2344, -0.3665,  0.6269]],
       dtype=torch.float64)
	q_value: tensor([[-18.1368]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9533551806203746, distance: 0.24714874024064495 entropy 0.7110485434532166
epoch: 4, step: 18
	action: tensor([[-0.1454, -0.4036,  0.5721, -0.8107,  0.1806, -0.7043,  0.3143]],
       dtype=torch.float64)
	q_value: tensor([[-22.8369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.741498602351522, distance: 1.5101436415443692 entropy 0.7110485434532166
epoch: 4, step: 19
	action: tensor([[-0.4439, -0.2636, -0.8291, -0.2783,  0.6780,  0.7551, -0.2671]],
       dtype=torch.float64)
	q_value: tensor([[-21.1604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.19612434301928716, distance: 1.251540355341499 entropy 0.7110485434532166
epoch: 4, step: 20
	action: tensor([[ 0.3405,  0.1865, -0.1871, -0.2221,  0.4332, -0.4809, -0.2197]],
       dtype=torch.float64)
	q_value: tensor([[-18.0340]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5126062434341347, distance: 0.7989078234584016 entropy 0.7110485434532166
epoch: 4, step: 21
	action: tensor([[-0.0361,  0.5337,  0.2523, -0.4067,  0.1489,  0.9504, -0.1616]],
       dtype=torch.float64)
	q_value: tensor([[-16.0385]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6292008675296608, distance: 0.6968287296616428 entropy 0.7110485434532166
epoch: 4, step: 22
	action: tensor([[ 0.2631,  0.8169,  0.4186,  0.2762,  0.3895, -0.2432, -0.9239]],
       dtype=torch.float64)
	q_value: tensor([[-18.0631]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 4, step: 23
	action: tensor([[-0.1253, -0.6587,  0.6101,  0.7241,  0.1697,  0.1113,  0.5988]],
       dtype=torch.float64)
	q_value: tensor([[-22.7756]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.300473006881247, distance: 0.9571035609633988 entropy 0.7110485434532166
epoch: 4, step: 24
	action: tensor([[-0.2800, -0.3776, -1.1943, -0.3350,  0.3908, -0.1288, -0.5889]],
       dtype=torch.float64)
	q_value: tensor([[-23.5125]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17444007676788198, distance: 1.240144023458987 entropy 0.7110485434532166
epoch: 4, step: 25
	action: tensor([[-0.1697, -0.1199,  0.2195,  0.2341,  0.2191,  0.1306,  0.5631]],
       dtype=torch.float64)
	q_value: tensor([[-18.0469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20352720058296092, distance: 1.0212737460247585 entropy 0.7110485434532166
epoch: 4, step: 26
	action: tensor([[-0.2657, -0.7282,  0.3706,  0.0755,  0.2297, -0.3773, -0.1887]],
       dtype=torch.float64)
	q_value: tensor([[-16.9623]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6921513316441883, distance: 1.4885940998267306 entropy 0.7110485434532166
epoch: 4, step: 27
	action: tensor([[ 0.1232, -0.1894, -0.1798, -0.2060, -0.1587,  0.0044,  0.2422]],
       dtype=torch.float64)
	q_value: tensor([[-19.9254]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12853567197480653, distance: 1.0682711517705386 entropy 0.7110485434532166
epoch: 4, step: 28
	action: tensor([[ 0.5830,  0.8428,  0.6316, -0.1751,  0.0664, -0.3390, -0.2613]],
       dtype=torch.float64)
	q_value: tensor([[-14.2647]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8906857280075335, distance: 0.37835120953440826 entropy 0.7110485434532166
epoch: 4, step: 29
	action: tensor([[ 0.0640, -0.5041, -0.0717,  0.0079,  0.9056,  0.0952, -0.1059]],
       dtype=torch.float64)
	q_value: tensor([[-19.8971]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.058425554344889497, distance: 1.177299207487216 entropy 0.7110485434532166
epoch: 4, step: 30
	action: tensor([[-0.2659,  0.1035, -0.0943, -0.5706, -0.1717, -0.7179, -0.2077]],
       dtype=torch.float64)
	q_value: tensor([[-18.1645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1528284466785501, distance: 1.2286806971604465 entropy 0.7110485434532166
epoch: 4, step: 31
	action: tensor([[-0.2215,  0.0821, -0.0037, -0.0791, -0.2108,  0.1053,  0.5722]],
       dtype=torch.float64)
	q_value: tensor([[-15.9866]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06851890352715473, distance: 1.1044440405406406 entropy 0.7110485434532166
epoch: 4, step: 32
	action: tensor([[ 0.4747, -0.2696,  0.9842, -0.9398,  0.3558, -0.8641,  0.5700]],
       dtype=torch.float64)
	q_value: tensor([[-15.0631]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08388258101435997, distance: 1.1913731596479555 entropy 0.7110485434532166
epoch: 4, step: 33
	action: tensor([[ 0.7766,  0.4202, -0.0199, -0.1815, -0.1782, -0.6292,  0.4210]],
       dtype=torch.float64)
	q_value: tensor([[-26.1359]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8416395400759153, distance: 0.45538641477545816 entropy 0.7110485434532166
epoch: 4, step: 34
	action: tensor([[ 0.5716, -0.2431,  0.6814, -0.0586,  0.0220,  0.2184,  0.7460]],
       dtype=torch.float64)
	q_value: tensor([[-18.6606]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5193588765800448, distance: 0.7933542567839117 entropy 0.7110485434532166
epoch: 4, step: 35
	action: tensor([[ 0.2262, -0.7170, -0.3601, -0.5276,  0.2455, -0.0790,  0.5857]],
       dtype=torch.float64)
	q_value: tensor([[-20.8133]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4430813325017364, distance: 1.3746815301566675 entropy 0.7110485434532166
epoch: 4, step: 36
	action: tensor([[ 0.8485,  1.2074, -0.2279,  0.6786, -0.4691, -0.1308,  0.5194]],
       dtype=torch.float64)
	q_value: tensor([[-18.7407]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 4, step: 37
	action: tensor([[-0.5226, -0.2710, -0.7399, -0.9792,  0.2963, -0.4554, -0.4048]],
       dtype=torch.float64)
	q_value: tensor([[-22.7756]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12755696500500657, distance: 1.2151389424991839 entropy 0.7110485434532166
epoch: 4, step: 38
	action: tensor([[ 0.3089, -0.0540, -0.1739,  0.0043,  0.6216, -0.0405, -0.0244]],
       dtype=torch.float64)
	q_value: tensor([[-17.0823]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5134998309196335, distance: 0.7981751288587733 entropy 0.7110485434532166
epoch: 4, step: 39
	action: tensor([[ 0.6846,  0.9970, -0.1077, -1.1703, -0.9480, -0.0095,  0.1041]],
       dtype=torch.float64)
	q_value: tensor([[-15.7642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9051687039875612, distance: 0.35239727074125 entropy 0.7110485434532166
epoch: 4, step: 40
	action: tensor([[ 0.4342, -0.4807,  0.7216,  0.4739, -0.6966, -0.1136,  0.1874]],
       dtype=torch.float64)
	q_value: tensor([[-25.5550]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4828694013743028, distance: 0.8229184722404601 entropy 0.7110485434532166
epoch: 4, step: 41
	action: tensor([[ 0.4164,  0.7964, -0.4439,  0.0310, -0.0500, -0.2954,  0.9096]],
       dtype=torch.float64)
	q_value: tensor([[-22.4518]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9191214657171655, distance: 0.3254417970399897 entropy 0.7110485434532166
epoch: 4, step: 42
	action: tensor([[-0.7986,  0.1972,  0.5483,  0.1216,  0.7323,  0.5078,  0.5659]],
       dtype=torch.float64)
	q_value: tensor([[-17.7391]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17728935952191627, distance: 1.2416474549644279 entropy 0.7110485434532166
epoch: 4, step: 43
	action: tensor([[-0.5606,  0.1443,  0.2689, -0.6119, -0.3785,  0.6350, -0.1325]],
       dtype=torch.float64)
	q_value: tensor([[-20.4383]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4782770004057266, distance: 1.3913442683727497 entropy 0.7110485434532166
epoch: 4, step: 44
	action: tensor([[ 0.4653,  1.3984, -0.4385,  0.0338,  0.3106,  0.7138,  0.9139]],
       dtype=torch.float64)
	q_value: tensor([[-18.5110]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 4, step: 45
	action: tensor([[ 0.4843, -0.0789,  1.4085,  0.6047,  0.7496,  0.1251, -0.1297]],
       dtype=torch.float64)
	q_value: tensor([[-22.7756]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7629227579815749, distance: 0.557187835602489 entropy 0.7110485434532166
epoch: 4, step: 46
	action: tensor([[ 1.2985,  0.6969, -0.4337,  0.4924,  0.0765, -0.1662,  0.6354]],
       dtype=torch.float64)
	q_value: tensor([[-26.8968]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 4, step: 47
	action: tensor([[ 0.1494,  0.1430, -0.2084,  0.4040, -0.5649, -0.4618,  0.4913]],
       dtype=torch.float64)
	q_value: tensor([[-22.7756]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6517289994716039, distance: 0.6753289229499415 entropy 0.7110485434532166
epoch: 4, step: 48
	action: tensor([[ 1.0543,  0.0793, -0.4167, -0.2007, -1.8880, -0.2855,  0.8105]],
       dtype=torch.float64)
	q_value: tensor([[-17.1002]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.689256294584085, distance: 0.6379077277606285 entropy 0.7110485434532166
epoch: 4, step: 49
	action: tensor([[ 0.7083, -0.0779,  0.5994, -0.4836,  0.3531, -0.4194, -0.2241]],
       dtype=torch.float64)
	q_value: tensor([[-28.9359]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3201554733874059, distance: 0.9435425632339255 entropy 0.7110485434532166
epoch: 4, step: 50
	action: tensor([[ 0.9729,  0.7455, -0.2003,  0.4948,  0.4803, -0.6802, -0.0694]],
       dtype=torch.float64)
	q_value: tensor([[-20.6281]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9667591609983004, distance: 0.20863764967699888 entropy 0.7110485434532166
epoch: 4, step: 51
	action: tensor([[-0.7239,  0.4597,  0.2785,  0.1593, -0.4429, -0.1279,  0.9184]],
       dtype=torch.float64)
	q_value: tensor([[-22.2039]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15213947391039695, distance: 1.2283134898537735 entropy 0.7110485434532166
epoch: 4, step: 52
	action: tensor([[ 0.6601,  0.7298,  0.0327,  0.5822, -0.8247,  0.0455,  0.3876]],
       dtype=torch.float64)
	q_value: tensor([[-18.9064]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 4, step: 53
	action: tensor([[ 0.2359,  1.2177,  0.0828,  0.7203, -0.9059, -0.0293, -0.0727]],
       dtype=torch.float64)
	q_value: tensor([[-22.7756]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 4, step: 54
	action: tensor([[ 0.9511, -0.0631,  0.0321, -0.1026, -0.2448, -0.2796,  0.0145]],
       dtype=torch.float64)
	q_value: tensor([[-22.7756]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5605331370572952, distance: 0.7586120905189666 entropy 0.7110485434532166
epoch: 4, step: 55
	action: tensor([[ 0.4455, -0.0957, -0.5627, -0.0203, -0.0724, -0.2235, -0.5780]],
       dtype=torch.float64)
	q_value: tensor([[-19.1425]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5556331883617296, distance: 0.7628295374317439 entropy 0.7110485434532166
epoch: 4, step: 56
	action: tensor([[-0.1366, -0.0865, -0.7147, -0.0495,  0.1006, -0.7372, -0.3273]],
       dtype=torch.float64)
	q_value: tensor([[-16.1141]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.149197271929425, distance: 1.055531334164928 entropy 0.7110485434532166
epoch: 4, step: 57
	action: tensor([[ 0.1541, -0.0743,  0.1087, -0.4366, -0.1451, -0.3994,  0.0103]],
       dtype=torch.float64)
	q_value: tensor([[-17.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07757824446519046, distance: 1.0990601513892586 entropy 0.7110485434532166
epoch: 4, step: 58
	action: tensor([[ 0.3529,  0.4583, -0.2883, -1.4190,  0.8055,  0.6001, -0.2217]],
       dtype=torch.float64)
	q_value: tensor([[-15.6215]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6075304534720151, distance: 0.7169018069667017 entropy 0.7110485434532166
epoch: 4, step: 59
	action: tensor([[ 0.3266, -0.4074, -0.6303,  0.2500, -1.1545, -0.5668,  0.2867]],
       dtype=torch.float64)
	q_value: tensor([[-20.3161]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4217117018041323, distance: 0.8702196696018334 entropy 0.7110485434532166
epoch: 4, step: 60
	action: tensor([[ 0.4910,  0.0696, -0.0360,  0.4396, -0.0171, -0.0370, -0.2712]],
       dtype=torch.float64)
	q_value: tensor([[-21.4193]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8617699887921376, distance: 0.42545914684480973 entropy 0.7110485434532166
epoch: 4, step: 61
	action: tensor([[-0.1571,  0.3598, -0.1281,  0.5922,  0.7815, -0.6761,  0.9340]],
       dtype=torch.float64)
	q_value: tensor([[-16.2317]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30697158448341966, distance: 0.9526474607937492 entropy 0.7110485434532166
epoch: 4, step: 62
	action: tensor([[ 0.0730,  0.0256,  0.2076, -0.2483, -0.4909, -0.7811,  0.0338]],
       dtype=torch.float64)
	q_value: tensor([[-22.1372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15890201343419863, distance: 1.0494940722584616 entropy 0.7110485434532166
epoch: 4, step: 63
	action: tensor([[ 0.4672, -0.6300, -0.3239,  0.4226,  0.1064,  0.3415,  0.1177]],
       dtype=torch.float64)
	q_value: tensor([[-17.7838]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3949150226858492, distance: 0.8901534483834439 entropy 0.7110485434532166
epoch: 4, step: 64
	action: tensor([[ 0.0380, -0.1167, -0.4791,  0.6519, -0.4248,  0.0008,  0.0960]],
       dtype=torch.float64)
	q_value: tensor([[-18.3861]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3830391358175472, distance: 0.8988464372920194 entropy 0.7110485434532166
epoch: 4, step: 65
	action: tensor([[-0.3640,  0.0436, -0.3251, -0.3461, -0.2552, -0.0412,  0.1453]],
       dtype=torch.float64)
	q_value: tensor([[-15.6325]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11761598249404748, distance: 1.2097705140505983 entropy 0.7110485434532166
epoch: 4, step: 66
	action: tensor([[-0.0534, -0.5582,  0.0694,  0.8578,  0.7261,  0.4423,  0.5998]],
       dtype=torch.float64)
	q_value: tensor([[-13.6184]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5027837448821812, distance: 0.8069179052009858 entropy 0.7110485434532166
epoch: 4, step: 67
	action: tensor([[ 0.4973,  0.2211,  0.3254, -0.1824, -0.5702, -0.1006,  0.3035]],
       dtype=torch.float64)
	q_value: tensor([[-22.9889]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7051838598484866, distance: 0.6213443037465486 entropy 0.7110485434532166
epoch: 4, step: 68
	action: tensor([[ 0.8307,  0.2433,  0.1709,  1.1172,  0.1622,  0.3232, -0.5174]],
       dtype=torch.float64)
	q_value: tensor([[-17.7439]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8924675777480398, distance: 0.3752549308982168 entropy 0.7110485434532166
epoch: 4, step: 69
	action: tensor([[ 0.2977, -0.6439, -1.1150, -0.2057,  0.1875, -0.2902,  0.1184]],
       dtype=torch.float64)
	q_value: tensor([[-23.2057]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.024332318663968344, distance: 1.158182853168254 entropy 0.7110485434532166
epoch: 4, step: 70
	action: tensor([[ 0.3722, -0.2704,  0.0026,  0.5591, -0.6588,  0.0759,  0.4437]],
       dtype=torch.float64)
	q_value: tensor([[-18.8065]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.692940453267712, distance: 0.6341149546903467 entropy 0.7110485434532166
epoch: 4, step: 71
	action: tensor([[ 0.5340, -0.3694, -0.1114,  0.1066,  1.1017, -0.1173, -0.0646]],
       dtype=torch.float64)
	q_value: tensor([[-18.3355]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.40014551460558834, distance: 0.886297753671162 entropy 0.7110485434532166
epoch: 4, step: 72
	action: tensor([[ 0.0493,  0.4192, -0.5148,  0.0454, -0.1355, -0.7296,  0.2655]],
       dtype=torch.float64)
	q_value: tensor([[-21.5417]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6181616329836125, distance: 0.7071254616691938 entropy 0.7110485434532166
epoch: 4, step: 73
	action: tensor([[ 0.3691,  0.5265, -1.2905, -0.8088, -0.3861, -0.1507, -0.3090]],
       dtype=torch.float64)
	q_value: tensor([[-15.9871]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9388576431623936, distance: 0.28296178140225253 entropy 0.7110485434532166
epoch: 4, step: 74
	action: tensor([[ 0.4648,  0.7385, -0.0928, -0.6992,  0.4599,  0.2874, -0.2958]],
       dtype=torch.float64)
	q_value: tensor([[-19.5981]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9102047685307648, distance: 0.3429125109862989 entropy 0.7110485434532166
epoch: 4, step: 75
	action: tensor([[ 1.0400,  0.5334, -0.3831,  0.7047,  0.2245, -0.9339, -0.2313]],
       dtype=torch.float64)
	q_value: tensor([[-17.2440]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.08816487498841538 entropy 0.7110485434532166
epoch: 4, step: 76
	action: tensor([[-0.0568, -0.4165, -0.3080, -0.1373,  0.0597,  0.0614,  0.3157]],
       dtype=torch.float64)
	q_value: tensor([[-22.7756]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13559192229698303, distance: 1.2194607895115666 entropy 0.7110485434532166
epoch: 4, step: 77
	action: tensor([[-0.2304, -0.5679, -0.5199, -0.4737,  0.0383,  0.2386, -0.6579]],
       dtype=torch.float64)
	q_value: tensor([[-15.0038]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4030548385381163, distance: 1.3554828158306538 entropy 0.7110485434532166
epoch: 4, step: 78
	action: tensor([[ 0.0501,  0.2475, -0.1455,  0.6181,  0.7523, -0.2724, -0.5357]],
       dtype=torch.float64)
	q_value: tensor([[-16.2803]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5653208188538823, distance: 0.7544685021641186 entropy 0.7110485434532166
epoch: 4, step: 79
	action: tensor([[-0.1566,  0.5843,  0.0779,  0.3926,  0.4540,  0.4443,  0.3412]],
       dtype=torch.float64)
	q_value: tensor([[-18.8181]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 4, step: 80
	action: tensor([[ 0.1767,  0.4444, -0.2136, -0.0447,  0.3655, -0.0081, -0.6738]],
       dtype=torch.float64)
	q_value: tensor([[-22.7756]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6867395527267848, distance: 0.6404857550531483 entropy 0.7110485434532166
epoch: 4, step: 81
	action: tensor([[ 0.2040, -0.3543,  0.6316,  0.3690,  0.6500, -0.6927, -0.2161]],
       dtype=torch.float64)
	q_value: tensor([[-14.7248]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2712357238164238, distance: 0.9769002731093073 entropy 0.7110485434532166
epoch: 4, step: 82
	action: tensor([[ 0.6896, -0.2512, -0.7139, -0.7582, -0.7748,  0.2460, -0.3148]],
       dtype=torch.float64)
	q_value: tensor([[-22.8344]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17900298744609167, distance: 1.0368775744529817 entropy 0.7110485434532166
epoch: 4, step: 83
	action: tensor([[ 0.2858,  0.5417, -0.2532, -0.0636,  0.3343, -0.3633, -0.4973]],
       dtype=torch.float64)
	q_value: tensor([[-20.3020]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7660300964812435, distance: 0.5535242998382882 entropy 0.7110485434532166
epoch: 4, step: 84
	action: tensor([[-0.3060,  0.6614, -0.0724, -0.0320, -0.2626, -0.2257,  0.2417]],
       dtype=torch.float64)
	q_value: tensor([[-15.7121]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36517910720865443, distance: 0.9117637028630166 entropy 0.7110485434532166
epoch: 4, step: 85
	action: tensor([[ 0.4585, -0.2071, -0.1108, -0.1767, -0.4286,  1.0579, -0.6330]],
       dtype=torch.float64)
	q_value: tensor([[-14.2503]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6222373375199295, distance: 0.7033414437983088 entropy 0.7110485434532166
epoch: 4, step: 86
	action: tensor([[ 0.2636,  0.3592, -0.8190, -0.9718,  0.5313,  0.4353, -0.5905]],
       dtype=torch.float64)
	q_value: tensor([[-20.9041]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8023484320358389, distance: 0.5087528149853303 entropy 0.7110485434532166
epoch: 4, step: 87
	action: tensor([[-0.7230,  0.0996,  0.0763, -0.8320, -0.7051,  0.9152, -0.2096]],
       dtype=torch.float64)
	q_value: tensor([[-17.8362]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7454513218788776, distance: 1.5118564741008573 entropy 0.7110485434532166
epoch: 4, step: 88
	action: tensor([[ 1.5462,  0.3021, -0.6049,  0.3925,  0.2495, -0.2045,  0.2244]],
       dtype=torch.float64)
	q_value: tensor([[-22.1011]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7796505617150997, distance: 0.5371711342827588 entropy 0.7110485434532166
epoch: 4, step: 89
	action: tensor([[-0.3741, -0.2499, -0.5617, -0.1302, -0.2547,  0.3317, -0.2758]],
       dtype=torch.float64)
	q_value: tensor([[-24.1442]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3163454699525112, distance: 1.3129301903084851 entropy 0.7110485434532166
epoch: 4, step: 90
	action: tensor([[ 0.4292,  0.1928, -0.5904,  0.4656, -0.5539,  0.1387,  0.4400]],
       dtype=torch.float64)
	q_value: tensor([[-14.1040]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7426425159549541, distance: 0.5805305704029046 entropy 0.7110485434532166
epoch: 4, step: 91
	action: tensor([[-0.7433, -0.6148,  0.6585, -0.1436,  0.1506, -0.8256,  0.2226]],
       dtype=torch.float64)
	q_value: tensor([[-16.3911]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.1727608850669866, distance: 1.6867963804604158 entropy 0.7110485434532166
epoch: 4, step: 92
	action: tensor([[-0.1883,  1.3209, -0.1956,  0.1127,  0.6643,  0.0639, -0.6876]],
       dtype=torch.float64)
	q_value: tensor([[-23.9670]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 4, step: 93
	action: tensor([[-0.0267, -0.5299,  0.0136, -0.0356,  0.4680,  0.8948,  0.6196]],
       dtype=torch.float64)
	q_value: tensor([[-22.7756]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21081512049178897, distance: 1.016590556690774 entropy 0.7110485434532166
epoch: 4, step: 94
	action: tensor([[ 0.8875,  0.7914,  0.3499, -0.2138, -0.1287,  0.2672,  0.5520]],
       dtype=torch.float64)
	q_value: tensor([[-20.9326]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9787935294441853, distance: 0.16664446377214862 entropy 0.7110485434532166
epoch: 4, step: 95
	action: tensor([[-0.6896,  0.4255, -0.6660,  0.0731,  0.0231, -0.7264,  0.7290]],
       dtype=torch.float64)
	q_value: tensor([[-20.1433]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13916325300876253, distance: 1.2213768294919132 entropy 0.7110485434532166
epoch: 4, step: 96
	action: tensor([[ 0.9815,  0.3305, -0.2997,  0.0537,  0.4202,  0.5030,  0.4211]],
       dtype=torch.float64)
	q_value: tensor([[-18.0921]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9783696743761519, distance: 0.16830159117856397 entropy 0.7110485434532166
epoch: 4, step: 97
	action: tensor([[ 0.1839,  0.4874, -0.2020,  0.2344, -0.0401,  0.0235, -0.0181]],
       dtype=torch.float64)
	q_value: tensor([[-19.6054]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7560468018071104, distance: 0.5652101487117508 entropy 0.7110485434532166
epoch: 4, step: 98
	action: tensor([[ 0.0873,  0.5382, -0.3563,  0.7984,  0.8216,  0.4694,  0.3135]],
       dtype=torch.float64)
	q_value: tensor([[-13.4584]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 4, step: 99
	action: tensor([[ 0.2608, -0.4960, -0.2307,  0.1579,  0.2189,  0.3154,  0.0351]],
       dtype=torch.float64)
	q_value: tensor([[-22.7756]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23772197555478236, distance: 0.9991102009424095 entropy 0.7110485434532166
epoch: 4, step: 100
	action: tensor([[-0.0075,  0.0537, -0.2746, -0.2325, -0.1659,  0.1379,  0.1361]],
       dtype=torch.float64)
	q_value: tensor([[-16.3373]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.251379572946541, distance: 0.9901193085577484 entropy 0.7110485434532166
epoch: 4, step: 101
	action: tensor([[ 0.5892,  0.7921, -0.2420,  1.4648, -0.4799, -0.2031, -0.3861]],
       dtype=torch.float64)
	q_value: tensor([[-13.2524]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 4, step: 102
	action: tensor([[-0.9009,  0.4510,  0.0312, -0.3759,  0.4747, -0.5311,  0.3453]],
       dtype=torch.float64)
	q_value: tensor([[-22.7756]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7586073522335084, distance: 1.5175434533385799 entropy 0.7110485434532166
epoch: 4, step: 103
	action: tensor([[-0.1063, -0.1382,  0.4575,  0.6153,  0.6120,  0.0599,  0.0803]],
       dtype=torch.float64)
	q_value: tensor([[-17.3403]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.539294265421175, distance: 0.7767271793701772 entropy 0.7110485434532166
epoch: 4, step: 104
	action: tensor([[ 0.1470, -0.6011, -0.2754, -0.0086,  0.1793, -0.8026, -0.2643]],
       dtype=torch.float64)
	q_value: tensor([[-19.5418]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2878347693015999, distance: 1.298633991690632 entropy 0.7110485434532166
epoch: 4, step: 105
	action: tensor([[ 0.8647,  0.9087,  0.0513, -0.8171,  0.1314, -0.1149,  0.2295]],
       dtype=torch.float64)
	q_value: tensor([[-20.4350]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9549852038877709, distance: 0.24279198006836836 entropy 0.7110485434532166
epoch: 4, step: 106
	action: tensor([[ 0.2288,  0.3733, -0.6407, -0.0031,  0.6230,  0.1779,  0.5153]],
       dtype=torch.float64)
	q_value: tensor([[-20.9099]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7485566500220826, distance: 0.5738214391451542 entropy 0.7110485434532166
epoch: 4, step: 107
	action: tensor([[-0.2448, -0.1972, -0.9321,  0.3075,  0.7121, -0.2340,  0.1772]],
       dtype=torch.float64)
	q_value: tensor([[-16.6277]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20697482811820067, distance: 1.257204131881305 entropy 0.7110485434532166
epoch: 4, step: 108
	action: tensor([[ 0.7823, -0.2990, -0.1594,  0.0066, -0.1663,  0.5181, -0.1076]],
       dtype=torch.float64)
	q_value: tensor([[-19.4256]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6243887290014527, distance: 0.7013357885329123 entropy 0.7110485434532166
epoch: 4, step: 109
	action: tensor([[-0.1066, -0.4059,  0.2093, -1.3807, -0.3785,  1.1784,  0.3676]],
       dtype=torch.float64)
	q_value: tensor([[-17.8455]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.46817958749699096, distance: 1.3865843179905857 entropy 0.7110485434532166
epoch: 4, step: 110
	action: tensor([[ 0.1998,  0.1821,  0.2129, -1.1146, -0.2425,  0.2585,  0.0195]],
       dtype=torch.float64)
	q_value: tensor([[-26.0635]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07190794345943374, distance: 1.102433041200826 entropy 0.7110485434532166
epoch: 4, step: 111
	action: tensor([[-0.2568,  1.0354, -0.0491, -0.5718, -0.1707,  0.2635,  0.3947]],
       dtype=torch.float64)
	q_value: tensor([[-19.0575]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.43741168806717456, distance: 0.8583255606910997 entropy 0.7110485434532166
epoch: 4, step: 112
	action: tensor([[ 0.0120,  0.1764,  0.1554,  0.0749,  0.1092, -0.1511,  1.0374]],
       dtype=torch.float64)
	q_value: tensor([[-17.7784]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4333724247052231, distance: 0.861401346370072 entropy 0.7110485434532166
epoch: 4, step: 113
	action: tensor([[ 0.6696,  0.3259,  0.5908,  0.2341, -0.5805, -0.1689, -0.4128]],
       dtype=torch.float64)
	q_value: tensor([[-17.9720]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9450314918060827, distance: 0.26829567304147295 entropy 0.7110485434532166
epoch: 4, step: 114
	action: tensor([[ 1.4025,  0.4869, -0.0128,  0.4367,  0.0245,  0.0458, -0.3452]],
       dtype=torch.float64)
	q_value: tensor([[-21.2572]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8434526554711096, distance: 0.452771983509659 entropy 0.7110485434532166
epoch: 4, step: 115
	action: tensor([[ 0.5641,  1.1495, -0.8547, -0.3283, -0.2253,  0.0926, -0.2473]],
       dtype=torch.float64)
	q_value: tensor([[-23.1384]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 4, step: 116
	action: tensor([[ 0.6753, -0.0824, -0.3184,  0.2025,  0.1912, -0.5763,  0.0513]],
       dtype=torch.float64)
	q_value: tensor([[-22.7756]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6288975523948712, distance: 0.6971136757094721 entropy 0.7110485434532166
epoch: 4, step: 117
	action: tensor([[-0.0879,  0.4048,  0.4752,  0.2196,  0.3308,  0.5183, -1.1622]],
       dtype=torch.float64)
	q_value: tensor([[-18.8696]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7145602718491204, distance: 0.6113837660508732 entropy 0.7110485434532166
epoch: 4, step: 118
	action: tensor([[-0.1683,  0.2679,  0.0319, -0.5074,  0.3083, -1.0984,  0.2249]],
       dtype=torch.float64)
	q_value: tensor([[-19.9194]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03923375109896177, distance: 1.1665767447443862 entropy 0.7110485434532166
epoch: 4, step: 119
	action: tensor([[-0.4629, -0.1259,  0.2614, -0.6141,  0.5482, -1.0992, -0.3691]],
       dtype=torch.float64)
	q_value: tensor([[-18.7990]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6929061744036036, distance: 1.4889260823077406 entropy 0.7110485434532166
epoch: 4, step: 120
	action: tensor([[ 1.7072, -0.1831, -1.1854,  0.9755, -0.4888, -0.3676,  0.4023]],
       dtype=torch.float64)
	q_value: tensor([[-21.1504]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7498668385352762, distance: 0.5723244892709286 entropy 0.7110485434532166
epoch: 4, step: 121
	action: tensor([[-0.2284,  0.1251, -0.2673, -0.6131, -0.4394,  0.1574,  0.2256]],
       dtype=torch.float64)
	q_value: tensor([[-28.8149]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.036662478730579484, distance: 1.1231711286518364 entropy 0.7110485434532166
epoch: 4, step: 122
	action: tensor([[ 0.7215,  0.2211,  0.5538, -0.7126, -0.2222,  0.3327, -0.3756]],
       dtype=torch.float64)
	q_value: tensor([[-15.7047]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6498920949958813, distance: 0.6771075426030644 entropy 0.7110485434532166
epoch: 4, step: 123
	action: tensor([[-0.1254, -0.7609, -0.0215,  0.6308, -0.3006, -1.0803,  0.7658]],
       dtype=torch.float64)
	q_value: tensor([[-21.0150]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21645317089547378, distance: 1.2621308745143005 entropy 0.7110485434532166
epoch: 4, step: 124
	action: tensor([[ 0.9355, -0.2584, -0.4904,  0.0245,  0.4141,  0.1393,  0.0298]],
       dtype=torch.float64)
	q_value: tensor([[-27.0504]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5696385312814551, distance: 0.7507120444582316 entropy 0.7110485434532166
epoch: 4, step: 125
	action: tensor([[ 0.5875,  0.1936, -0.6169, -0.8592, -0.4870,  0.7093, -0.5662]],
       dtype=torch.float64)
	q_value: tensor([[-19.1747]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6668314977306732, distance: 0.6605240901670282 entropy 0.7110485434532166
epoch: 4, step: 126
	action: tensor([[ 1.1788,  0.5790, -0.3351, -0.4328, -0.0222,  0.3600,  0.1271]],
       dtype=torch.float64)
	q_value: tensor([[-20.8130]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8744937341383212, distance: 0.4054052770235013 entropy 0.7110485434532166
epoch: 4, step: 127
	action: tensor([[ 0.7128,  0.0783, -0.0119,  0.1829, -0.1237, -0.4935,  0.0315]],
       dtype=torch.float64)
	q_value: tensor([[-20.4560]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7817104689636634, distance: 0.5346544023920602 entropy 0.7110485434532166
LOSS epoch 4 actor 248.11040783841025 critic 1360.6115756294766 
epoch: 5, step: 0
	action: tensor([[ 0.6793, -0.4171, -0.2180,  0.1624,  0.2584, -0.6330,  0.4989]],
       dtype=torch.float64)
	q_value: tensor([[-14.2682]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25635550633526816, distance: 0.9868232577924606 entropy 0.7110485434532166
epoch: 5, step: 1
	action: tensor([[ 0.1975,  0.3047,  0.4017,  0.6495,  0.0235, -0.7541,  0.2505]],
       dtype=torch.float64)
	q_value: tensor([[-17.1073]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8097796300327651, distance: 0.4990972814257656 entropy 0.7110485434532166
epoch: 5, step: 2
	action: tensor([[ 0.0339,  0.2590,  0.4645,  0.4433,  0.1153, -0.6152, -0.4375]],
       dtype=torch.float64)
	q_value: tensor([[-16.2263]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6158244828893651, distance: 0.7092862413350702 entropy 0.7110485434532166
epoch: 5, step: 3
	action: tensor([[ 0.4150,  0.2205,  0.1747,  0.5071, -0.1703,  0.2139,  0.1368]],
       dtype=torch.float64)
	q_value: tensor([[-15.4887]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9221430842215987, distance: 0.3193046852148686 entropy 0.7110485434532166
epoch: 5, step: 4
	action: tensor([[ 0.5361, -0.6036, -0.5206, -0.1183, -0.3758,  0.2837, -0.0463]],
       dtype=torch.float64)
	q_value: tensor([[-13.3856]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10469082257845874, distance: 1.0827874448946064 entropy 0.7110485434532166
epoch: 5, step: 5
	action: tensor([[ 1.0008,  0.7084, -0.2930, -0.4550, -0.3045, -0.4945, -0.1755]],
       dtype=torch.float64)
	q_value: tensor([[-14.0191]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8672801315142475, distance: 0.416893059133941 entropy 0.7110485434532166
epoch: 5, step: 6
	action: tensor([[ 1.2836,  0.1410,  0.0167,  0.1104, -0.1436,  0.4724,  0.3011]],
       dtype=torch.float64)
	q_value: tensor([[-15.8173]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.837723701952695, distance: 0.4609822880224209 entropy 0.7110485434532166
epoch: 5, step: 7
	action: tensor([[ 1.4523, -0.0893,  0.1980, -0.1881, -0.1650, -0.2973,  0.9023]],
       dtype=torch.float64)
	q_value: tensor([[-17.2567]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.289096140514819, distance: 0.9648551722215553 entropy 0.7110485434532166
epoch: 5, step: 8
	action: tensor([[ 0.5745,  0.2342, -0.1328,  0.5594, -0.1036, -0.4896,  0.1356]],
       dtype=torch.float64)
	q_value: tensor([[-20.4526]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9388723793339332, distance: 0.2829276804545673 entropy 0.7110485434532166
epoch: 5, step: 9
	action: tensor([[ 1.0371, -1.3187, -0.3505,  0.0783, -0.8574, -0.4899, -0.9129]],
       dtype=torch.float64)
	q_value: tensor([[-14.5349]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.625651062150959, distance: 1.4590506195179067 entropy 0.7110485434532166
epoch: 5, step: 10
	action: tensor([[ 1.3444, -0.1784,  0.3411, -0.5136, -0.1029, -0.6485, -0.5021]],
       dtype=torch.float64)
	q_value: tensor([[-23.1061]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09349170503327298, distance: 1.0895385084835034 entropy 0.7110485434532166
epoch: 5, step: 11
	action: tensor([[ 0.4133,  0.5614, -1.1010,  0.6985,  1.0695, -0.1403,  0.4273]],
       dtype=torch.float64)
	q_value: tensor([[-20.7497]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6453865104687588, distance: 0.6814505075891159 entropy 0.7110485434532166
epoch: 5, step: 12
	action: tensor([[ 0.7262,  0.7858, -0.1043,  0.0636, -0.6671, -0.5592,  1.2410]],
       dtype=torch.float64)
	q_value: tensor([[-19.0529]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9873406617262708, distance: 0.12875443449827026 entropy 0.7110485434532166
epoch: 5, step: 13
	action: tensor([[ 0.0428,  1.0948, -1.0312, -0.2136, -0.4256,  0.2822, -0.2548]],
       dtype=torch.float64)
	q_value: tensor([[-18.2775]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 5, step: 14
	action: tensor([[-0.0051,  0.2682,  0.8400, -0.5991,  0.2944,  0.2388,  0.4333]],
       dtype=torch.float64)
	q_value: tensor([[-18.1712]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13714381830393985, distance: 1.0629819760735368 entropy 0.7110485434532166
epoch: 5, step: 15
	action: tensor([[ 0.0635,  1.4239,  0.0876, -0.4675,  0.0822, -0.5737,  0.0725]],
       dtype=torch.float64)
	q_value: tensor([[-15.4174]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6631929478504328, distance: 0.6641211043578845 entropy 0.7110485434532166
epoch: 5, step: 16
	action: tensor([[-0.5112,  1.0181, -0.3783, -0.4469,  0.4924, -0.2708,  0.1438]],
       dtype=torch.float64)
	q_value: tensor([[-15.2811]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28129603947558357, distance: 0.970133970694222 entropy 0.7110485434532166
epoch: 5, step: 17
	action: tensor([[ 0.2111, -0.7889, -1.3585,  1.2417,  0.1170, -0.2348,  0.6921]],
       dtype=torch.float64)
	q_value: tensor([[-12.5688]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.35450145084490314, distance: 1.331822754210625 entropy 0.7110485434532166
epoch: 5, step: 18
	action: tensor([[ 0.9110,  0.2999,  0.5830,  0.1842, -0.0444, -0.2888, -1.2111]],
       dtype=torch.float64)
	q_value: tensor([[-22.1926]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9231772832496997, distance: 0.3171768809170011 entropy 0.7110485434532166
epoch: 5, step: 19
	action: tensor([[ 0.4783, -0.0762,  0.0270,  0.4702,  0.2119, -0.6174, -0.1748]],
       dtype=torch.float64)
	q_value: tensor([[-19.9040]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6944042111506412, distance: 0.6326017309637131 entropy 0.7110485434532166
epoch: 5, step: 20
	action: tensor([[ 0.8138,  0.9354, -0.6729, -0.3632, -0.1610, -0.2712, -0.1926]],
       dtype=torch.float64)
	q_value: tensor([[-15.5936]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9752710142701095, distance: 0.17995331245187512 entropy 0.7110485434532166
epoch: 5, step: 21
	action: tensor([[-0.3176,  0.0757, -0.4718, -0.6991, -0.2998,  0.6212,  0.2466]],
       dtype=torch.float64)
	q_value: tensor([[-14.8544]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04150636029147159, distance: 1.167851590374265 entropy 0.7110485434532166
epoch: 5, step: 22
	action: tensor([[ 0.9538,  0.0567,  0.0775, -1.3718, -0.2789,  0.2244, -1.5276]],
       dtype=torch.float64)
	q_value: tensor([[-14.0867]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15311797994701648, distance: 1.0530964519231882 entropy 0.7110485434532166
epoch: 5, step: 23
	action: tensor([[ 0.5144, -0.2191, -0.4402,  0.4967, -0.7630, -0.0808,  0.3675]],
       dtype=torch.float64)
	q_value: tensor([[-21.5623]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6638674145002026, distance: 0.6634558094873153 entropy 0.7110485434532166
epoch: 5, step: 24
	action: tensor([[ 0.6298,  0.3578, -0.0193,  0.2203, -0.1160, -0.5896,  0.6412]],
       dtype=torch.float64)
	q_value: tensor([[-14.7289]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9099105840522834, distance: 0.3434737715732514 entropy 0.7110485434532166
epoch: 5, step: 25
	action: tensor([[ 0.8022, -0.2754, -0.0684, -0.2981,  0.4785,  0.7220,  0.8169]],
       dtype=torch.float64)
	q_value: tensor([[-15.0606]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5733157276486274, distance: 0.7474979582840169 entropy 0.7110485434532166
epoch: 5, step: 26
	action: tensor([[ 0.9788,  0.4802,  0.6721, -1.0845,  0.1347, -0.1574, -0.2592]],
       dtype=torch.float64)
	q_value: tensor([[-17.5545]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6614731296582772, distance: 0.6658145267708583 entropy 0.7110485434532166
epoch: 5, step: 27
	action: tensor([[ 0.5833, -0.4923, -1.1822, -0.1722, -0.0285, -0.1629, -0.1503]],
       dtype=torch.float64)
	q_value: tensor([[-19.2274]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18867916467367984, distance: 1.0307492033977759 entropy 0.7110485434532166
epoch: 5, step: 28
	action: tensor([[-0.2059,  0.0022, -0.4876, -0.0176, -0.9574, -0.5413,  0.4079]],
       dtype=torch.float64)
	q_value: tensor([[-14.8638]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22313893766297532, distance: 1.0086218460897836 entropy 0.7110485434532166
epoch: 5, step: 29
	action: tensor([[-0.2833,  0.1508, -0.0334, -0.1297,  0.4309, -0.6481,  0.4639]],
       dtype=torch.float64)
	q_value: tensor([[-14.9026]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11342753970493424, distance: 1.207501482925655 entropy 0.7110485434532166
epoch: 5, step: 30
	action: tensor([[ 0.9081, -0.2336, -0.3751,  0.0775, -0.0098,  0.2781, -0.5976]],
       dtype=torch.float64)
	q_value: tensor([[-13.5912]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6480439612796656, distance: 0.6788923326158255 entropy 0.7110485434532166
epoch: 5, step: 31
	action: tensor([[ 0.5589,  0.4161,  0.3076, -0.3276,  0.0783,  0.7859,  1.1901]],
       dtype=torch.float64)
	q_value: tensor([[-15.5086]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9404831867304114, distance: 0.2791750020867613 entropy 0.7110485434532166
epoch: 5, step: 32
	action: tensor([[ 1.6988,  1.0122, -0.3389,  0.6084, -0.3392,  0.1262, -0.0855]],
       dtype=torch.float64)
	q_value: tensor([[-18.5102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 5, step: 33
	action: tensor([[-0.1920,  0.0553, -0.6880, -0.1973, -0.6154, -0.2849, -0.0563]],
       dtype=torch.float64)
	q_value: tensor([[-18.1712]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24223784463600706, distance: 0.9961463525584988 entropy 0.7110485434532166
epoch: 5, step: 34
	action: tensor([[ 0.1997,  0.1826, -0.6408,  0.7317,  0.5978, -0.2308, -0.2864]],
       dtype=torch.float64)
	q_value: tensor([[-12.5835]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5561166109309696, distance: 0.7624144868981373 entropy 0.7110485434532166
epoch: 5, step: 35
	action: tensor([[ 0.3893, -0.4658, -0.9281,  0.2569, -0.3087,  0.3584,  0.3375]],
       dtype=torch.float64)
	q_value: tensor([[-15.4313]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23809980849222145, distance: 0.9988625593212953 entropy 0.7110485434532166
epoch: 5, step: 36
	action: tensor([[ 0.3102,  0.4397, -0.4746, -0.1104,  0.1465, -0.0348,  0.0865]],
       dtype=torch.float64)
	q_value: tensor([[-13.9716]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7808881189468855, distance: 0.5356605425946631 entropy 0.7110485434532166
epoch: 5, step: 37
	action: tensor([[ 0.6674,  0.1628,  0.1264,  0.0516, -0.3926, -0.1330, -0.4714]],
       dtype=torch.float64)
	q_value: tensor([[-10.8512]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8218481001420415, distance: 0.4830053289880871 entropy 0.7110485434532166
epoch: 5, step: 38
	action: tensor([[-0.0868,  0.4799, -1.1436,  0.0214, -0.2918, -0.2205,  0.2119]],
       dtype=torch.float64)
	q_value: tensor([[-14.4854]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.546762260531769, distance: 0.7704061228004141 entropy 0.7110485434532166
epoch: 5, step: 39
	action: tensor([[ 0.0718, -0.3781, -0.5184,  0.5268,  0.2135,  0.2593,  0.3825]],
       dtype=torch.float64)
	q_value: tensor([[-12.5270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2607548381050755, distance: 0.983899950497067 entropy 0.7110485434532166
epoch: 5, step: 40
	action: tensor([[ 0.8691,  0.3336,  0.0556,  0.4154, -0.0709,  0.1237, -0.0339]],
       dtype=torch.float64)
	q_value: tensor([[-13.7489]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.09845431288094716 entropy 0.7110485434532166
epoch: 5, step: 41
	action: tensor([[-5.2608e-01,  3.5919e-01, -3.0761e-04, -5.1077e-01,  3.2716e-01,
          4.6689e-01,  2.6496e-01]], dtype=torch.float64)
	q_value: tensor([[-18.1712]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13984285030767563, distance: 1.2217410971387523 entropy 0.7110485434532166
epoch: 5, step: 42
	action: tensor([[ 0.3408,  0.2627,  0.1713, -0.4101, -0.4575,  0.7498, -0.6740]],
       dtype=torch.float64)
	q_value: tensor([[-12.8696]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.646159388915275, distance: 0.6807074934178005 entropy 0.7110485434532166
epoch: 5, step: 43
	action: tensor([[-0.5381, -0.6773, -0.8321, -0.0062,  0.2734,  0.2323, -0.6240]],
       dtype=torch.float64)
	q_value: tensor([[-16.4303]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8202782301354421, distance: 1.5439228003377243 entropy 0.7110485434532166
epoch: 5, step: 44
	action: tensor([[ 0.2653, -0.0680,  0.0007,  0.1346, -0.0503, -0.3079,  0.5316]],
       dtype=torch.float64)
	q_value: tensor([[-14.9388]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.444241475484471, distance: 0.8530996419864079 entropy 0.7110485434532166
epoch: 5, step: 45
	action: tensor([[-0.0095, -1.2513, -0.8065, -0.1575, -0.1016,  0.8680, -0.8292]],
       dtype=torch.float64)
	q_value: tensor([[-13.2106]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7081840544615292, distance: 1.49562950816462 entropy 0.7110485434532166
epoch: 5, step: 46
	action: tensor([[ 0.3552, -0.2562, -0.9302,  0.4325, -0.3763, -0.4294, -0.0625]],
       dtype=torch.float64)
	q_value: tensor([[-18.6411]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4225560111789127, distance: 0.8695841708339748 entropy 0.7110485434532166
epoch: 5, step: 47
	action: tensor([[ 1.4234,  0.2170,  0.5714, -0.7021,  1.2274, -0.8833, -0.0055]],
       dtype=torch.float64)
	q_value: tensor([[-14.4439]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7267405173502163, distance: 0.5981971149731288 entropy 0.7110485434532166
epoch: 5, step: 48
	action: tensor([[ 0.3617, -0.1734, -0.2787,  0.0947, -0.8753, -0.0438,  0.0373]],
       dtype=torch.float64)
	q_value: tensor([[-23.7771]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4631091387847325, distance: 0.8384934976193404 entropy 0.7110485434532166
epoch: 5, step: 49
	action: tensor([[ 0.4761, -0.1551,  0.0223,  0.0294,  0.6659, -0.3145, -0.9880]],
       dtype=torch.float64)
	q_value: tensor([[-14.3154]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4580934238988379, distance: 0.8424010585373672 entropy 0.7110485434532166
epoch: 5, step: 50
	action: tensor([[-0.4103, -0.0059, -0.1605,  0.4573,  0.4757,  0.4064,  0.0314]],
       dtype=torch.float64)
	q_value: tensor([[-16.1772]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14506117720901135, distance: 1.0580939049830236 entropy 0.7110485434532166
epoch: 5, step: 51
	action: tensor([[ 0.6653,  0.6959, -0.9328,  0.4933,  0.9198, -0.2875,  0.2092]],
       dtype=torch.float64)
	q_value: tensor([[-13.2717]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8561955029122056, distance: 0.43395323182168266 entropy 0.7110485434532166
epoch: 5, step: 52
	action: tensor([[ 0.4140,  0.6539,  0.6888,  0.5353, -0.1581,  0.7177, -0.1481]],
       dtype=torch.float64)
	q_value: tensor([[-17.7514]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 5, step: 53
	action: tensor([[ 0.6973, -0.1537, -0.4133,  0.0563, -0.0575,  0.1799,  0.3355]],
       dtype=torch.float64)
	q_value: tensor([[-18.1712]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6670270155142227, distance: 0.6603302495764066 entropy 0.7110485434532166
epoch: 5, step: 54
	action: tensor([[ 0.8418,  0.1326,  0.0420, -0.3941,  0.0359, -0.5262, -0.5879]],
       dtype=torch.float64)
	q_value: tensor([[-13.2833]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5412717227894905, distance: 0.7750584389199903 entropy 0.7110485434532166
epoch: 5, step: 55
	action: tensor([[ 0.8609,  0.6057, -0.7217, -0.5479,  0.1547,  0.1051, -0.1986]],
       dtype=torch.float64)
	q_value: tensor([[-16.0549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.917726056662989, distance: 0.3282372382717347 entropy 0.7110485434532166
epoch: 5, step: 56
	action: tensor([[ 0.8104, -0.1307,  0.4644,  0.4421, -0.3799,  0.1997,  0.6710]],
       dtype=torch.float64)
	q_value: tensor([[-14.5158]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8740523978398929, distance: 0.4061174448513526 entropy 0.7110485434532166
epoch: 5, step: 57
	action: tensor([[ 1.4032,  0.4987, -0.9555, -0.7620,  0.0813,  0.6015,  0.0231]],
       dtype=torch.float64)
	q_value: tensor([[-17.3931]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5785874398973061, distance: 0.7428659132924077 entropy 0.7110485434532166
epoch: 5, step: 58
	action: tensor([[ 1.0754,  0.5761, -0.0051,  0.8029, -0.7983, -0.2908, -0.5225]],
       dtype=torch.float64)
	q_value: tensor([[-19.2285]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 5, step: 59
	action: tensor([[ 0.9781,  0.1785, -0.1447, -0.2119, -0.2386,  0.0083,  0.2528]],
       dtype=torch.float64)
	q_value: tensor([[-18.1712]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7669998618866656, distance: 0.5523759767224955 entropy 0.7110485434532166
epoch: 5, step: 60
	action: tensor([[ 0.2598, -0.5575, -0.4009,  0.7179, -0.5425, -0.9864,  0.2188]],
       dtype=torch.float64)
	q_value: tensor([[-14.4773]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37278817125798513, distance: 0.9062829578038947 entropy 0.7110485434532166
epoch: 5, step: 61
	action: tensor([[ 0.4479, -0.2885,  0.6843,  0.2942,  0.4949, -0.0331,  0.2507]],
       dtype=torch.float64)
	q_value: tensor([[-19.5432]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5612545818563694, distance: 0.757989152143503 entropy 0.7110485434532166
epoch: 5, step: 62
	action: tensor([[-0.4397,  0.1800, -0.1083, -0.4885,  0.8358, -0.3433,  0.2881]],
       dtype=torch.float64)
	q_value: tensor([[-17.1528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.35442201336633317, distance: 1.3317836999177555 entropy 0.7110485434532166
epoch: 5, step: 63
	action: tensor([[-0.2123,  0.7814,  0.0326, -0.6884, -0.0063,  0.4417,  0.4845]],
       dtype=torch.float64)
	q_value: tensor([[-13.4967]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38384444014511065, distance: 0.8982596243136997 entropy 0.7110485434532166
epoch: 5, step: 64
	action: tensor([[ 0.3046,  1.4509,  0.0773,  0.0225, -1.0450,  0.5683, -0.4225]],
       dtype=torch.float64)
	q_value: tensor([[-14.3536]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 5, step: 65
	action: tensor([[ 0.5299, -0.3454,  0.2128, -0.2197,  0.5116, -0.4262,  0.3802]],
       dtype=torch.float64)
	q_value: tensor([[-18.1712]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09461816943568324, distance: 1.088861345422625 entropy 0.7110485434532166
epoch: 5, step: 66
	action: tensor([[-0.0303,  0.0007,  0.1852, -0.4402, -0.3948, -0.0652,  0.0171]],
       dtype=torch.float64)
	q_value: tensor([[-16.0734]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.022923108449677154, distance: 1.1311522515680899 entropy 0.7110485434532166
epoch: 5, step: 67
	action: tensor([[-0.0241,  1.0240,  1.0790,  0.4145, -0.1633,  0.3949, -0.1006]],
       dtype=torch.float64)
	q_value: tensor([[-12.7658]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 5, step: 68
	action: tensor([[-0.3866,  0.2037, -1.1169, -1.0787, -0.2604, -0.6212,  0.2145]],
       dtype=torch.float64)
	q_value: tensor([[-18.1712]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.607503783968312, distance: 0.7169261643859846 entropy 0.7110485434532166
epoch: 5, step: 69
	action: tensor([[ 0.8022,  0.1314, -0.5664, -0.7192,  0.4236, -0.3746,  0.4537]],
       dtype=torch.float64)
	q_value: tensor([[-15.1170]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4369514018625099, distance: 0.8586766119223624 entropy 0.7110485434532166
epoch: 5, step: 70
	action: tensor([[ 1.3070,  0.9491, -0.7783,  0.2374,  0.0550,  0.0272,  0.2206]],
       dtype=torch.float64)
	q_value: tensor([[-15.6576]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 5, step: 71
	action: tensor([[ 0.5126, -0.0767, -0.0274,  0.1538, -0.3246, -0.3824,  0.7785]],
       dtype=torch.float64)
	q_value: tensor([[-18.1712]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6211344893778614, distance: 0.7043673703564509 entropy 0.7110485434532166
epoch: 5, step: 72
	action: tensor([[ 0.1345,  0.7497, -0.2754, -0.6627, -0.2486, -0.6962, -0.3525]],
       dtype=torch.float64)
	q_value: tensor([[-14.9755]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7174576874726335, distance: 0.6082728620900091 entropy 0.7110485434532166
epoch: 5, step: 73
	action: tensor([[ 0.2319,  0.5366, -0.4134,  1.5282,  0.5869,  0.1127,  0.3262]],
       dtype=torch.float64)
	q_value: tensor([[-13.9981]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 5, step: 74
	action: tensor([[-0.0035, -0.0800, -1.0510,  0.0235,  0.2528, -0.6965, -0.2508]],
       dtype=torch.float64)
	q_value: tensor([[-18.1712]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30389525934588035, distance: 0.954759501368669 entropy 0.7110485434532166
epoch: 5, step: 75
	action: tensor([[ 1.1298, -0.2649,  0.1735,  0.2364,  0.2176,  0.3474, -0.2466]],
       dtype=torch.float64)
	q_value: tensor([[-14.6660]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6776602647916237, distance: 0.6497011203675629 entropy 0.7110485434532166
epoch: 5, step: 76
	action: tensor([[ 0.0365,  0.0168,  0.0208,  0.7503,  0.0397,  0.0031, -0.2739]],
       dtype=torch.float64)
	q_value: tensor([[-17.6624]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6720312547894367, distance: 0.6553494245176827 entropy 0.7110485434532166
epoch: 5, step: 77
	action: tensor([[ 0.0924, -0.2534,  0.3023, -0.7758,  0.0654, -0.1779,  0.0194]],
       dtype=torch.float64)
	q_value: tensor([[-13.9098]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.346566894785427, distance: 1.327916171622177 entropy 0.7110485434532166
epoch: 5, step: 78
	action: tensor([[ 0.0798,  0.0728,  0.0473,  0.0877, -0.3667,  0.1361, -0.6373]],
       dtype=torch.float64)
	q_value: tensor([[-13.7320]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.45785716867527215, distance: 0.8425846694880673 entropy 0.7110485434532166
epoch: 5, step: 79
	action: tensor([[ 0.0982,  0.5215,  0.1107,  0.0971, -0.1487,  0.3363, -0.1153]],
       dtype=torch.float64)
	q_value: tensor([[-12.6686]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6904661426591937, distance: 0.6366647032169074 entropy 0.7110485434532166
epoch: 5, step: 80
	action: tensor([[ 0.2062,  0.5891,  0.0534, -0.3378, -0.2003,  0.1627,  0.8197]],
       dtype=torch.float64)
	q_value: tensor([[-12.1074]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7015245740414144, distance: 0.6251885040129517 entropy 0.7110485434532166
epoch: 5, step: 81
	action: tensor([[ 0.4680,  0.6838,  0.5363,  0.3401,  0.2767, -1.0828, -0.5107]],
       dtype=torch.float64)
	q_value: tensor([[-14.0615]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8696309848858694, distance: 0.4131843709292777 entropy 0.7110485434532166
epoch: 5, step: 82
	action: tensor([[ 0.9397,  0.1875, -0.4646, -0.3367,  0.3092, -0.3580, -0.0127]],
       dtype=torch.float64)
	q_value: tensor([[-18.8661]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6479740276404217, distance: 0.6789597771941909 entropy 0.7110485434532166
epoch: 5, step: 83
	action: tensor([[-0.0790,  0.7591, -0.8417, -1.1630, -0.5003, -0.2740,  0.6842]],
       dtype=torch.float64)
	q_value: tensor([[-15.2409]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7972970699214618, distance: 0.5152128739885597 entropy 0.7110485434532166
epoch: 5, step: 84
	action: tensor([[ 0.0487, -0.1997, -0.3911, -0.4497, -0.8783,  0.7490, -0.0875]],
       dtype=torch.float64)
	q_value: tensor([[-17.3490]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.008012732573852377, distance: 1.139750370862576 entropy 0.7110485434532166
epoch: 5, step: 85
	action: tensor([[-0.0305,  0.8776, -0.0632, -0.4248,  0.5424, -0.7596, -0.0948]],
       dtype=torch.float64)
	q_value: tensor([[-15.7071]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5049062471692647, distance: 0.8051937894281119 entropy 0.7110485434532166
epoch: 5, step: 86
	action: tensor([[0.1938, 0.2973, 0.2010, 0.3965, 0.3354, 1.2265, 0.0844]],
       dtype=torch.float64)
	q_value: tensor([[-13.5856]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9258506352474642, distance: 0.3116092999208183 entropy 0.7110485434532166
epoch: 5, step: 87
	action: tensor([[ 0.1161,  0.6068,  0.1796, -0.1783,  0.2278, -0.0041, -0.1175]],
       dtype=torch.float64)
	q_value: tensor([[-16.1440]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6625186540774043, distance: 0.6647855631543161 entropy 0.7110485434532166
epoch: 5, step: 88
	action: tensor([[ 0.6296,  0.3904, -1.2087, -0.4700, -0.2220,  0.4665, -0.1872]],
       dtype=torch.float64)
	q_value: tensor([[-11.6446]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9203247952932478, distance: 0.32301172513639975 entropy 0.7110485434532166
epoch: 5, step: 89
	action: tensor([[ 0.2885, -0.3287, -0.8101,  0.6369, -0.4154, -0.1333, -0.1588]],
       dtype=torch.float64)
	q_value: tensor([[-15.0711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3769293753985482, distance: 0.9032861087941073 entropy 0.7110485434532166
epoch: 5, step: 90
	action: tensor([[ 0.6969,  0.5314,  0.2768,  0.4265,  0.2824, -1.3222,  0.5003]],
       dtype=torch.float64)
	q_value: tensor([[-14.2806]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9454579751345145, distance: 0.26725283556209045 entropy 0.7110485434532166
epoch: 5, step: 91
	action: tensor([[ 0.6726,  0.8319,  0.4048,  0.3756,  0.2273,  0.0170, -0.0927]],
       dtype=torch.float64)
	q_value: tensor([[-20.4978]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 5, step: 92
	action: tensor([[-0.1888, -0.2737,  0.4351,  0.2481, -0.2750, -0.3793,  0.2667]],
       dtype=torch.float64)
	q_value: tensor([[-18.1712]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04440434354375844, distance: 1.1694752306476826 entropy 0.7110485434532166
epoch: 5, step: 93
	action: tensor([[ 0.2475, -0.2423, -0.0566, -0.8205,  1.1650, -0.6865,  0.2209]],
       dtype=torch.float64)
	q_value: tensor([[-14.7311]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2258074554184888, distance: 1.266974349524101 entropy 0.7110485434532166
epoch: 5, step: 94
	action: tensor([[ 0.8248, -0.6671, -0.3392, -0.4202,  0.2378, -0.2915,  0.1700]],
       dtype=torch.float64)
	q_value: tensor([[-17.4915]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.313123894997406, distance: 1.3113225978520966 entropy 0.7110485434532166
epoch: 5, step: 95
	action: tensor([[ 1.0283, -0.7874,  0.5488, -1.0289,  0.1721, -1.0576,  0.7100]],
       dtype=torch.float64)
	q_value: tensor([[-16.2172]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.30375629639916335, distance: 1.306636851841313 entropy 0.7110485434532166
epoch: 5, step: 96
	action: tensor([[ 1.1857, -0.3877, -0.1906, -0.7032, -0.1011,  0.2719,  0.6841]],
       dtype=torch.float64)
	q_value: tensor([[-23.9037]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09592719088624668, distance: 1.1979744189577644 entropy 0.7110485434532166
epoch: 5, step: 97
	action: tensor([[ 0.8822,  0.5367, -0.5222,  0.0445, -1.5405, -0.0345,  0.0809]],
       dtype=torch.float64)
	q_value: tensor([[-18.4635]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9828418464345663, distance: 0.1498966643117069 entropy 0.7110485434532166
epoch: 5, step: 98
	action: tensor([[ 0.5550, -0.4943, -0.5200, -0.9363,  1.0748, -0.4582,  0.0707]],
       dtype=torch.float64)
	q_value: tensor([[-19.4515]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2957995144273009, distance: 1.3026435688444038 entropy 0.7110485434532166
epoch: 5, step: 99
	action: tensor([[ 1.2309,  0.6689, -0.3047,  0.3326, -0.8691, -0.4181, -0.7095]],
       dtype=torch.float64)
	q_value: tensor([[-17.6708]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8694642912719656, distance: 0.413448441320335 entropy 0.7110485434532166
epoch: 5, step: 100
	action: tensor([[ 0.8302,  0.0250, -0.0266,  0.0927,  0.4363, -0.8273, -1.1108]],
       dtype=torch.float64)
	q_value: tensor([[-19.5193]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6607541024765813, distance: 0.6665212429067843 entropy 0.7110485434532166
epoch: 5, step: 101
	action: tensor([[ 0.5355,  0.0996,  0.3513, -0.2773, -0.6671,  0.2835, -0.1797]],
       dtype=torch.float64)
	q_value: tensor([[-19.3781]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6778447475240995, distance: 0.6495151740207676 entropy 0.7110485434532166
epoch: 5, step: 102
	action: tensor([[ 5.2913e-01,  3.5473e-01, -2.9060e-04,  1.3933e-01, -9.8068e-02,
          7.2430e-01,  4.7723e-01]], dtype=torch.float64)
	q_value: tensor([[-15.9717]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9437092032799227, distance: 0.27150347406760833 entropy 0.7110485434532166
epoch: 5, step: 103
	action: tensor([[ 0.8785, -0.0269, -0.0749, -0.5996, -0.0829, -0.6628,  0.6752]],
       dtype=torch.float64)
	q_value: tensor([[-14.5905]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25191082752554317, distance: 0.9897679297082048 entropy 0.7110485434532166
epoch: 5, step: 104
	action: tensor([[ 0.4415, -0.8664, -0.1300, -0.3165,  1.0662, -0.2496,  0.0835]],
       dtype=torch.float64)
	q_value: tensor([[-17.1366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5302119765429909, distance: 1.4155737207811505 entropy 0.7110485434532166
epoch: 5, step: 105
	action: tensor([[-0.3290,  0.1250, -0.0170, -0.0766,  0.3988,  0.0270,  1.0187]],
       dtype=torch.float64)
	q_value: tensor([[-18.3325]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.024181660986067444, distance: 1.1580976779021548 entropy 0.7110485434532166
epoch: 5, step: 106
	action: tensor([[ 0.2508, -0.5338,  0.9457, -0.4241, -0.0643,  0.0076, -0.5737]],
       dtype=torch.float64)
	q_value: tensor([[-15.4229]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.27955199984159074, distance: 1.2944511427131034 entropy 0.7110485434532166
epoch: 5, step: 107
	action: tensor([[ 0.2154,  0.0142, -0.3106, -0.0276, -0.0770, -0.4625,  0.2023]],
       dtype=torch.float64)
	q_value: tensor([[-17.9600]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4193351612242373, distance: 0.8720059688704075 entropy 0.7110485434532166
epoch: 5, step: 108
	action: tensor([[ 0.2679,  0.0930,  0.3263, -0.0472, -0.0628,  0.0214,  0.3232]],
       dtype=torch.float64)
	q_value: tensor([[-12.1176]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5757434569604745, distance: 0.7453683843535005 entropy 0.7110485434532166
epoch: 5, step: 109
	action: tensor([[ 0.0511, -0.3076,  0.0310, -1.0810, -0.0813, -0.5383,  0.5500]],
       dtype=torch.float64)
	q_value: tensor([[-12.6722]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4212537032635604, distance: 1.3642453994745385 entropy 0.7110485434532166
epoch: 5, step: 110
	action: tensor([[ 1.3175,  0.5470, -0.0629, -0.3892, -0.2979, -0.8212, -1.2336]],
       dtype=torch.float64)
	q_value: tensor([[-15.8522]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6317936873520369, distance: 0.6943881618294042 entropy 0.7110485434532166
epoch: 5, step: 111
	action: tensor([[ 0.6951,  0.5526, -0.8060, -0.0375, -0.1931,  0.4579, -0.1990]],
       dtype=torch.float64)
	q_value: tensor([[-21.8064]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.95347607203616, distance: 0.24682825936220693 entropy 0.7110485434532166
epoch: 5, step: 112
	action: tensor([[ 0.4513,  0.3935, -0.4975, -0.6789, -0.7013,  0.3912,  0.8863]],
       dtype=torch.float64)
	q_value: tensor([[-13.8172]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7522998869111185, distance: 0.5695341836391019 entropy 0.7110485434532166
epoch: 5, step: 113
	action: tensor([[0.6568, 0.8733, 0.2677, 0.5776, 0.6053, 0.4811, 0.9841]],
       dtype=torch.float64)
	q_value: tensor([[-17.3754]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 5, step: 114
	action: tensor([[ 0.1474, -0.1041, -0.1501, -0.2467,  0.1416, -0.5789, -0.0621]],
       dtype=torch.float64)
	q_value: tensor([[-18.1712]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14356834607987512, distance: 1.0590172848508685 entropy 0.7110485434532166
epoch: 5, step: 115
	action: tensor([[ 0.9759, -0.5753, -0.3217,  0.3196, -0.2684, -0.2781, -0.3141]],
       dtype=torch.float64)
	q_value: tensor([[-12.6045]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29432441441551205, distance: 0.9613006576534551 entropy 0.7110485434532166
epoch: 5, step: 116
	action: tensor([[ 0.8164,  0.3756, -1.0306, -0.2921, -0.2538,  0.3369,  0.8431]],
       dtype=torch.float64)
	q_value: tensor([[-17.3083]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9242599617505809, distance: 0.3149339304588588 entropy 0.7110485434532166
epoch: 5, step: 117
	action: tensor([[ 0.4795,  0.1254, -0.7539,  0.2896, -0.1069,  0.3197,  0.3561]],
       dtype=torch.float64)
	q_value: tensor([[-16.7033]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.771549491707249, distance: 0.5469564619398978 entropy 0.7110485434532166
epoch: 5, step: 118
	action: tensor([[-0.4499,  0.5244,  0.0726, -0.5943,  0.1845,  0.0415, -0.3264]],
       dtype=torch.float64)
	q_value: tensor([[-12.7091]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07180937758633399, distance: 1.1847193163954202 entropy 0.7110485434532166
epoch: 5, step: 119
	action: tensor([[ 0.1502,  1.0487, -0.0232,  1.2109,  0.4852,  0.3906, -0.2343]],
       dtype=torch.float64)
	q_value: tensor([[-11.6123]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
epoch: 5, step: 120
	action: tensor([[ 0.4559, -0.1725,  0.5872, -0.1868, -0.2075,  0.9206, -0.2576]],
       dtype=torch.float64)
	q_value: tensor([[-18.1712]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6801830223274316, distance: 0.6471537180945378 entropy 0.7110485434532166
epoch: 5, step: 121
	action: tensor([[ 0.7978,  0.0399,  0.0067, -0.2662, -0.1083,  0.3043, -0.3454]],
       dtype=torch.float64)
	q_value: tensor([[-17.4401]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7128397192785669, distance: 0.6132236246933774 entropy 0.7110485434532166
epoch: 5, step: 122
	action: tensor([[ 0.4645,  0.1599, -0.2586, -0.4580, -0.2992,  0.2749,  0.4995]],
       dtype=torch.float64)
	q_value: tensor([[-14.2127]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6435511797790964, distance: 0.6832116827605036 entropy 0.7110485434532166
epoch: 5, step: 123
	action: tensor([[-0.3994,  1.4171,  0.0719, -0.6881,  0.0743, -0.6118, -0.4065]],
       dtype=torch.float64)
	q_value: tensor([[-13.7158]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3596668039677199, distance: 0.9157136790444363 entropy 0.7110485434532166
epoch: 5, step: 124
	action: tensor([[ 0.5013,  0.4882,  0.6919, -0.6747,  0.1130,  0.4601, -0.0043]],
       dtype=torch.float64)
	q_value: tensor([[-15.5953]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7559972875054939, distance: 0.5652675051351912 entropy 0.7110485434532166
epoch: 5, step: 125
	action: tensor([[-0.3062,  0.2991, -0.4367, -0.4391, -0.2373, -0.7957,  0.4014]],
       dtype=torch.float64)
	q_value: tensor([[-16.0531]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19152574492207963, distance: 1.0289393839298426 entropy 0.7110485434532166
epoch: 5, step: 126
	action: tensor([[ 0.3988,  0.1182, -0.1490,  0.4281,  0.1242,  0.0797, -0.7135]],
       dtype=torch.float64)
	q_value: tensor([[-12.8942]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.810306783064167, distance: 0.49840523362086075 entropy 0.7110485434532166
epoch: 5, step: 127
	action: tensor([[ 0.0641,  0.7524, -0.1585,  0.4379, -0.6484,  0.1252, -0.3507]],
       dtype=torch.float64)
	q_value: tensor([[-13.4477]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.7110485434532166
LOSS epoch 5 actor 195.65527928525444 critic 1056.4390175569122 
epoch: 6, step: 0
	action: tensor([[ 0.9935,  0.2451, -0.2578, -0.2335, -0.4279,  0.3333,  0.0635]],
       dtype=torch.float64)
	q_value: tensor([[-16.1867]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8533380435546691, distance: 0.43824344645840096 entropy 0.6056879162788391
epoch: 6, step: 1
	action: tensor([[ 0.7983,  0.3757, -0.4644,  0.1664, -0.5122,  0.1243, -0.5617]],
       dtype=torch.float64)
	q_value: tensor([[-13.7049]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9682695772775276, distance: 0.20384244611951563 entropy 0.6056879162788391
epoch: 6, step: 2
	action: tensor([[ 0.8931,  0.7285, -0.6045, -0.3162, -0.2320, -0.4550,  0.7404]],
       dtype=torch.float64)
	q_value: tensor([[-12.7639]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.93755931672631, distance: 0.28595027376410365 entropy 0.6056879162788391
epoch: 6, step: 3
	action: tensor([[ 0.6792,  0.0995, -0.1274,  0.1206,  0.2116,  0.0784, -0.3322]],
       dtype=torch.float64)
	q_value: tensor([[-14.2162]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8653205342746759, distance: 0.41995947652381826 entropy 0.6056879162788391
epoch: 6, step: 4
	action: tensor([[ 0.0581,  0.0088, -0.0443, -0.4444, -0.5011, -1.0384,  0.1098]],
       dtype=torch.float64)
	q_value: tensor([[-11.6861]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22852245262619286, distance: 1.0051209815689488 entropy 0.6056879162788391
epoch: 6, step: 5
	action: tensor([[ 0.0144,  0.3109, -0.6241,  0.2966, -0.5868,  0.5382, -0.0732]],
       dtype=torch.float64)
	q_value: tensor([[-13.6151]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36767467120510944, distance: 0.9099698069064796 entropy 0.6056879162788391
epoch: 6, step: 6
	action: tensor([[-0.6019,  0.4228, -0.4684,  0.1724, -0.3020, -0.2362, -0.2242]],
       dtype=torch.float64)
	q_value: tensor([[-11.3339]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05572790727983867, distance: 1.1757979379939336 entropy 0.6056879162788391
epoch: 6, step: 7
	action: tensor([[ 1.1903,  0.7587, -0.4821,  0.0529,  0.3477, -0.1016,  0.7534]],
       dtype=torch.float64)
	q_value: tensor([[-9.9413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9256815687511711, distance: 0.3119643447463669 entropy 0.6056879162788391
epoch: 6, step: 8
	action: tensor([[ 1.7629, -0.0645, -1.1191,  0.1473, -0.3616,  0.2313,  0.5497]],
       dtype=torch.float64)
	q_value: tensor([[-15.3659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 6, step: 9
	action: tensor([[-0.0590, -0.3540, -0.7756, -0.0079,  0.1756, -0.9775,  0.0708]],
       dtype=torch.float64)
	q_value: tensor([[-16.1867]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.02054590820534241, distance: 1.13252744622703 entropy 0.6056879162788391
epoch: 6, step: 10
	action: tensor([[-0.1486, -0.0531,  0.0760,  0.2813, -0.8644,  0.6031, -0.1354]],
       dtype=torch.float64)
	q_value: tensor([[-14.5573]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2461794541662038, distance: 0.9935521749446256 entropy 0.6056879162788391
epoch: 6, step: 11
	action: tensor([[ 0.0210, -0.0445, -0.3076,  0.8418,  0.3501,  0.0296,  0.7618]],
       dtype=torch.float64)
	q_value: tensor([[-13.5978]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.49217693507934657, distance: 0.8154792304603873 entropy 0.6056879162788391
epoch: 6, step: 12
	action: tensor([[ 0.3279,  0.1419,  0.0428,  0.2078, -0.0130, -0.6724, -0.1942]],
       dtype=torch.float64)
	q_value: tensor([[-14.0387]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6228529405228094, distance: 0.7027681266236476 entropy 0.6056879162788391
epoch: 6, step: 13
	action: tensor([[ 0.7712,  0.0792,  0.1563,  0.3293, -0.4294,  0.2182,  0.1731]],
       dtype=torch.float64)
	q_value: tensor([[-12.3370]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9416100193006262, distance: 0.27651956124795785 entropy 0.6056879162788391
epoch: 6, step: 14
	action: tensor([[-0.4164,  0.5143, -0.4505, -0.0467, -0.4025,  0.2658,  0.2716]],
       dtype=torch.float64)
	q_value: tensor([[-13.2619]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14890069322227795, distance: 1.055715290346394 entropy 0.6056879162788391
epoch: 6, step: 15
	action: tensor([[ 0.3498, -0.1042, -0.0518, -0.3335,  0.2866,  0.7693, -0.0913]],
       dtype=torch.float64)
	q_value: tensor([[-10.8096]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5708175931869138, distance: 0.7496829757312072 entropy 0.6056879162788391
epoch: 6, step: 16
	action: tensor([[ 0.7316,  0.0985,  0.2274,  0.0125,  0.8184, -0.2821, -0.0186]],
       dtype=torch.float64)
	q_value: tensor([[-12.1392]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7859565147110099, distance: 0.5294289679980619 entropy 0.6056879162788391
epoch: 6, step: 17
	action: tensor([[ 0.9418,  0.1310, -0.0190,  0.1893, -0.4307,  0.0618, -0.0280]],
       dtype=torch.float64)
	q_value: tensor([[-14.4990]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.901054311482475, distance: 0.3599607356604935 entropy 0.6056879162788391
epoch: 6, step: 18
	action: tensor([[ 0.1698,  0.1872, -0.5530,  0.4252,  0.0695, -0.3547, -0.1901]],
       dtype=torch.float64)
	q_value: tensor([[-13.2100]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.611147648128644, distance: 0.7135904977243845 entropy 0.6056879162788391
epoch: 6, step: 19
	action: tensor([[ 0.0317,  0.4806,  0.3996,  0.9009,  0.7732,  0.1671, -0.1794]],
       dtype=torch.float64)
	q_value: tensor([[-10.9711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8629387124662303, distance: 0.42365671740240807 entropy 0.6056879162788391
epoch: 6, step: 20
	action: tensor([[ 0.8396,  0.1808, -0.6572, -0.6959,  0.0200, -0.2801, -0.3350]],
       dtype=torch.float64)
	q_value: tensor([[-14.5801]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5183411404323176, distance: 0.794193758749011 entropy 0.6056879162788391
epoch: 6, step: 21
	action: tensor([[ 0.5827,  0.4372, -0.8028,  0.3229, -0.5381, -0.2881,  0.3055]],
       dtype=torch.float64)
	q_value: tensor([[-13.0241]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9219429072078756, distance: 0.31971490199578934 entropy 0.6056879162788391
epoch: 6, step: 22
	action: tensor([[ 0.4205,  0.0322, -0.2345, -0.1478,  0.5311, -0.6484,  0.2239]],
       dtype=torch.float64)
	q_value: tensor([[-12.2075]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4395168839102801, distance: 0.8567181357571101 entropy 0.6056879162788391
epoch: 6, step: 23
	action: tensor([[ 0.4384,  0.4259, -1.1880,  0.3777, -0.0486, -0.3115,  0.3461]],
       dtype=torch.float64)
	q_value: tensor([[-13.0678]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8110913690120909, distance: 0.4973734441553408 entropy 0.6056879162788391
epoch: 6, step: 24
	action: tensor([[ 0.8257,  0.0075, -0.2505,  0.0475, -0.3326, -0.0577,  0.4848]],
       dtype=torch.float64)
	q_value: tensor([[-12.4361]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7623989783852283, distance: 0.5578029992532437 entropy 0.6056879162788391
epoch: 6, step: 25
	action: tensor([[ 1.3685,  0.2569, -0.0546, -0.0440,  0.8647, -0.1559, -0.5056]],
       dtype=torch.float64)
	q_value: tensor([[-12.7227]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7294568302168654, distance: 0.5952165253855308 entropy 0.6056879162788391
epoch: 6, step: 26
	action: tensor([[ 0.3441,  0.6611, -0.4620, -0.2666, -0.3113, -0.0929, -0.0561]],
       dtype=torch.float64)
	q_value: tensor([[-17.7829]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8954189303253807, distance: 0.3700694486239566 entropy 0.6056879162788391
epoch: 6, step: 27
	action: tensor([[ 0.4356,  0.5222, -0.7070, -0.4474,  0.1888,  0.0377,  0.0911]],
       dtype=torch.float64)
	q_value: tensor([[-10.5754]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9045662768553089, distance: 0.3535148215793235 entropy 0.6056879162788391
epoch: 6, step: 28
	action: tensor([[ 0.5364,  0.4496, -0.5000,  0.1039, -0.6605,  0.5441,  0.6375]],
       dtype=torch.float64)
	q_value: tensor([[-10.9941]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8694965287591812, distance: 0.4133973849389555 entropy 0.6056879162788391
epoch: 6, step: 29
	action: tensor([[ 1.4569,  0.0642,  0.5436,  0.3832,  0.4046, -0.9938, -0.3487]],
       dtype=torch.float64)
	q_value: tensor([[-14.1197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.687338852242217, distance: 0.6398728041292246 entropy 0.6056879162788391
epoch: 6, step: 30
	action: tensor([[ 0.5064,  0.0736, -0.3532, -0.2421, -0.7412,  0.0974,  0.0952]],
       dtype=torch.float64)
	q_value: tensor([[-20.8291]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6472353418038197, distance: 0.6796717632139841 entropy 0.6056879162788391
epoch: 6, step: 31
	action: tensor([[ 0.3562, -0.2223, -0.2794,  0.2472, -0.1795, -0.2647,  0.7399]],
       dtype=torch.float64)
	q_value: tensor([[-12.5346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4833537468116954, distance: 0.8225330085112761 entropy 0.6056879162788391
epoch: 6, step: 32
	action: tensor([[ 0.1768,  0.8229, -0.9773,  0.4348, -0.3887, -0.3418,  0.4504]],
       dtype=torch.float64)
	q_value: tensor([[-12.9248]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 6, step: 33
	action: tensor([[-0.0326,  0.2435, -0.8057, -0.0355,  0.3676, -0.6671,  0.0621]],
       dtype=torch.float64)
	q_value: tensor([[-16.1867]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.45070727345408146, distance: 0.8481225636742388 entropy 0.6056879162788391
epoch: 6, step: 34
	action: tensor([[-0.2767,  0.1669, -0.5435,  0.3150, -0.1699, -0.4628,  0.2949]],
       dtype=torch.float64)
	q_value: tensor([[-11.6985]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19022977799779894, distance: 1.0297637376100202 entropy 0.6056879162788391
epoch: 6, step: 35
	action: tensor([[ 1.0573,  0.1399,  0.0726,  0.1371, -0.1794,  0.2431,  0.5660]],
       dtype=torch.float64)
	q_value: tensor([[-10.9556]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9052819546839884, distance: 0.35218678559360095 entropy 0.6056879162788391
epoch: 6, step: 36
	action: tensor([[-0.3830, -0.4292,  0.4676, -0.8759, -0.2146,  0.5118,  0.0812]],
       dtype=torch.float64)
	q_value: tensor([[-14.4846]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8197218899466876, distance: 1.543686844117153 entropy 0.6056879162788391
epoch: 6, step: 37
	action: tensor([[ 1.0998, -0.3588, -0.0718, -0.4221,  0.3889, -0.6546, -0.6237]],
       dtype=torch.float64)
	q_value: tensor([[-15.4817]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.020110990356614122, distance: 1.155793922637805 entropy 0.6056879162788391
epoch: 6, step: 38
	action: tensor([[ 0.7961,  0.8777, -0.3989,  0.1951,  0.3081, -0.4411, -0.3563]],
       dtype=torch.float64)
	q_value: tensor([[-16.9880]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9557568910457048, distance: 0.24070189635297354 entropy 0.6056879162788391
epoch: 6, step: 39
	action: tensor([[ 0.7417, -0.2071,  0.1855, -0.1943,  0.0131, -0.7774, -0.2247]],
       dtype=torch.float64)
	q_value: tensor([[-13.6405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2561286123627484, distance: 0.9869737915318657 entropy 0.6056879162788391
epoch: 6, step: 40
	action: tensor([[ 0.6924,  1.3293, -0.2849, -0.4325, -0.7288, -0.6733,  0.3905]],
       dtype=torch.float64)
	q_value: tensor([[-14.8720]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9407945205566711, distance: 0.27844385923150344 entropy 0.6056879162788391
epoch: 6, step: 41
	action: tensor([[ 0.2123,  0.6286,  0.1487, -0.0480, -0.9881, -0.5743,  0.4215]],
       dtype=torch.float64)
	q_value: tensor([[-16.4395]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8543677542224408, distance: 0.436702287338131 entropy 0.6056879162788391
epoch: 6, step: 42
	action: tensor([[ 0.0772, -0.0571, -0.6097,  0.0503,  0.5475, -0.5249, -0.1227]],
       dtype=torch.float64)
	q_value: tensor([[-14.7932]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2596296581141957, distance: 0.984648446164207 entropy 0.6056879162788391
epoch: 6, step: 43
	action: tensor([[ 0.7216, -0.1383, -0.2906,  0.0716, -0.2395, -0.0937, -0.6091]],
       dtype=torch.float64)
	q_value: tensor([[-11.9430]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6235852600497358, distance: 0.7020855004692436 entropy 0.6056879162788391
epoch: 6, step: 44
	action: tensor([[ 0.7766,  0.5558,  0.1975, -0.2138, -1.1584, -0.6060,  0.1408]],
       dtype=torch.float64)
	q_value: tensor([[-12.7261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9303453805173696, distance: 0.3020171849723663 entropy 0.6056879162788391
epoch: 6, step: 45
	action: tensor([[ 0.5014,  0.4146, -0.2911,  0.4421,  0.6409, -0.2223,  0.4418]],
       dtype=torch.float64)
	q_value: tensor([[-16.7361]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8897599904608819, distance: 0.3799498823263547 entropy 0.6056879162788391
epoch: 6, step: 46
	action: tensor([[ 0.7778,  1.0845,  0.4532,  0.4821,  0.0344, -0.0520, -0.3645]],
       dtype=torch.float64)
	q_value: tensor([[-13.4616]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 6, step: 47
	action: tensor([[-0.2299,  0.3183,  0.2331, -0.3797, -0.5970, -0.5437,  0.1360]],
       dtype=torch.float64)
	q_value: tensor([[-16.1867]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10051403083419175, distance: 1.0853102132108556 entropy 0.6056879162788391
epoch: 6, step: 48
	action: tensor([[ 0.4626, -0.0184, -0.4906,  0.6291, -0.5132,  0.2805, -0.2916]],
       dtype=torch.float64)
	q_value: tensor([[-12.7176]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7280337151540885, distance: 0.5967799553407787 entropy 0.6056879162788391
epoch: 6, step: 49
	action: tensor([[ 1.3598, -0.0056, -1.2297,  0.3303, -0.1553, -0.4786, -0.7249]],
       dtype=torch.float64)
	q_value: tensor([[-11.9425]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.652217046327629, distance: 0.6748555735956233 entropy 0.6056879162788391
epoch: 6, step: 50
	action: tensor([[ 0.4269, -0.5672, -0.3266,  0.6031,  0.0020, -0.7875,  0.0062]],
       dtype=torch.float64)
	q_value: tensor([[-17.9250]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27757283688681844, distance: 0.9726435849742917 entropy 0.6056879162788391
epoch: 6, step: 51
	action: tensor([[ 1.2552,  1.0371, -0.3178, -0.2316,  0.1694, -0.3796,  0.2614]],
       dtype=torch.float64)
	q_value: tensor([[-16.1193]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8742549516500673, distance: 0.40579074654694514 entropy 0.6056879162788391
epoch: 6, step: 52
	action: tensor([[ 0.5833,  0.1605, -0.6789,  0.6607, -0.6303, -0.0631,  0.3009]],
       dtype=torch.float64)
	q_value: tensor([[-15.6681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8571121349873506, distance: 0.4325679783758988 entropy 0.6056879162788391
epoch: 6, step: 53
	action: tensor([[ 0.4498,  0.8988, -1.1110,  0.2973,  0.4848, -0.1585, -0.2084]],
       dtype=torch.float64)
	q_value: tensor([[-12.4455]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 6, step: 54
	action: tensor([[ 0.0148, -1.0427, -0.3325, -0.4903,  0.8831,  0.5491,  0.6406]],
       dtype=torch.float64)
	q_value: tensor([[-16.1867]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5355882412692841, distance: 1.418058287034783 entropy 0.6056879162788391
epoch: 6, step: 55
	action: tensor([[ 0.7324,  0.4702, -0.0755, -0.0893, -0.7731, -1.0992, -0.0382]],
       dtype=torch.float64)
	q_value: tensor([[-17.7118]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8840928587063925, distance: 0.38959357904033776 entropy 0.6056879162788391
epoch: 6, step: 56
	action: tensor([[ 0.9673,  0.0561, -1.3223, -0.5574, -1.4765,  0.0791, -0.4181]],
       dtype=torch.float64)
	q_value: tensor([[-15.7034]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5946366230861488, distance: 0.7285828561187824 entropy 0.6056879162788391
epoch: 6, step: 57
	action: tensor([[ 0.3297,  0.7759, -0.8009,  0.3263, -0.5380,  0.5216,  0.4531]],
       dtype=torch.float64)
	q_value: tensor([[-18.4591]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6053309023388106, distance: 0.7189078976670386 entropy 0.6056879162788391
epoch: 6, step: 58
	action: tensor([[ 0.3095,  0.0998,  0.8468,  0.4982, -0.0179, -0.2908,  0.3886]],
       dtype=torch.float64)
	q_value: tensor([[-13.6955]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8232821491727736, distance: 0.48105740461551616 entropy 0.6056879162788391
epoch: 6, step: 59
	action: tensor([[-0.0468,  0.1960, -0.3119, -0.0179,  0.1914,  0.2143, -0.3905]],
       dtype=torch.float64)
	q_value: tensor([[-15.6042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42500587127762435, distance: 0.8677375640874955 entropy 0.6056879162788391
epoch: 6, step: 60
	action: tensor([[-0.6446, -0.4124,  0.0967,  0.0955, -0.0535, -0.2695,  0.8869]],
       dtype=torch.float64)
	q_value: tensor([[-9.1279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7830836443137439, distance: 1.5280675440215377 entropy 0.6056879162788391
epoch: 6, step: 61
	action: tensor([[ 0.6822,  0.1341,  0.3966, -0.0370, -0.1352,  0.0509,  0.4136]],
       dtype=torch.float64)
	q_value: tensor([[-15.8830]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8213824137061154, distance: 0.48363620138372215 entropy 0.6056879162788391
epoch: 6, step: 62
	action: tensor([[ 1.1601,  0.6404,  0.2989, -0.0689, -0.3242, -0.1071, -0.4760]],
       dtype=torch.float64)
	q_value: tensor([[-13.1005]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9378943825632707, distance: 0.28518201624084527 entropy 0.6056879162788391
epoch: 6, step: 63
	action: tensor([[ 0.1451, -0.3083, -0.3731,  0.6296, -0.2394,  0.2906,  0.3495]],
       dtype=torch.float64)
	q_value: tensor([[-16.1807]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.446478396468878, distance: 0.8513810533203832 entropy 0.6056879162788391
epoch: 6, step: 64
	action: tensor([[ 0.9517, -0.1835, -0.6023,  0.4516, -0.5681, -0.4829, -0.6338]],
       dtype=torch.float64)
	q_value: tensor([[-12.1788]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7653901133309238, distance: 0.5542808166085803 entropy 0.6056879162788391
epoch: 6, step: 65
	action: tensor([[ 0.6079, -0.7242,  0.3278,  0.4565, -0.0569, -1.1518,  0.1794]],
       dtype=torch.float64)
	q_value: tensor([[-15.8557]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.005578226957832388, distance: 1.1411480845437214 entropy 0.6056879162788391
epoch: 6, step: 66
	action: tensor([[ 0.3804,  0.5750,  0.1044, -0.1297,  0.3180, -0.3345, -0.0922]],
       dtype=torch.float64)
	q_value: tensor([[-19.7608]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.813155805354324, distance: 0.49464827350927554 entropy 0.6056879162788391
epoch: 6, step: 67
	action: tensor([[ 0.7144,  0.9340, -0.2398,  0.4845, -0.1042,  0.0911,  0.2947]],
       dtype=torch.float64)
	q_value: tensor([[-11.3157]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 6, step: 68
	action: tensor([[ 0.7579,  0.6165,  0.1455,  0.2222,  0.1163,  0.0774, -0.5243]],
       dtype=torch.float64)
	q_value: tensor([[-16.1867]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9725455233335206, distance: 0.1896108905978685 entropy 0.6056879162788391
epoch: 6, step: 69
	action: tensor([[-0.0335,  0.9649, -0.4826,  0.6120,  0.6118, -0.7151,  0.4830]],
       dtype=torch.float64)
	q_value: tensor([[-13.1472]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 6, step: 70
	action: tensor([[-0.1261,  0.3165, -0.3414, -0.4830, -0.2127, -0.2770,  0.2754]],
       dtype=torch.float64)
	q_value: tensor([[-16.1867]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3302751830354972, distance: 0.9364937630798964 entropy 0.6056879162788391
epoch: 6, step: 71
	action: tensor([[ 0.1446,  0.2420,  0.0185, -0.3029, -0.5201,  0.0880,  0.5865]],
       dtype=torch.float64)
	q_value: tensor([[-10.1696]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4943956617321159, distance: 0.8136958277007705 entropy 0.6056879162788391
epoch: 6, step: 72
	action: tensor([[ 0.1807,  0.0010, -0.1766, -0.7772,  0.0960, -0.3768, -0.1512]],
       dtype=torch.float64)
	q_value: tensor([[-12.1829]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09120046283105243, distance: 1.0909145693402862 entropy 0.6056879162788391
epoch: 6, step: 73
	action: tensor([[ 0.5292,  0.3598, -0.7086,  0.1550, -0.3541, -0.1531, -0.2624]],
       dtype=torch.float64)
	q_value: tensor([[-11.1097]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8824609134726533, distance: 0.39232668518352554 entropy 0.6056879162788391
epoch: 6, step: 74
	action: tensor([[ 1.3493e+00,  6.6039e-01, -7.3728e-04,  4.9031e-01, -5.2194e-01,
          1.1268e-01,  3.0893e-01]], dtype=torch.float64)
	q_value: tensor([[-11.0271]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 6, step: 75
	action: tensor([[ 0.5951, -0.1209, -1.1100,  0.1806,  0.0758, -0.3589,  0.7842]],
       dtype=torch.float64)
	q_value: tensor([[-16.1867]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6009491238108854, distance: 0.7228876873156168 entropy 0.6056879162788391
epoch: 6, step: 76
	action: tensor([[ 9.4589e-01,  1.7711e-04, -2.2299e+00, -1.0184e+00,  1.7089e-01,
         -6.1094e-01, -5.2514e-02]], dtype=torch.float64)
	q_value: tensor([[-14.3711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6947463435998955, distance: 0.6322475143825639 entropy 0.6056879162788391
epoch: 6, step: 77
	action: tensor([[ 0.7057,  0.3814, -1.3159,  0.9031, -0.3106,  0.0879, -0.0207]],
       dtype=torch.float64)
	q_value: tensor([[-18.6095]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7087500285716575, distance: 0.6175749032154161 entropy 0.6056879162788391
epoch: 6, step: 78
	action: tensor([[ 1.2999, -0.8382, -0.1989, -0.2646, -0.2129,  0.3867, -0.3606]],
       dtype=torch.float64)
	q_value: tensor([[-14.5262]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.28111383636487863, distance: 1.2952409130837228 entropy 0.6056879162788391
epoch: 6, step: 79
	action: tensor([[ 0.9320, -0.0856,  0.1619,  0.3643, -0.6279, -0.1369,  0.2250]],
       dtype=torch.float64)
	q_value: tensor([[-16.7663]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8100113183963854, distance: 0.4987932386338164 entropy 0.6056879162788391
epoch: 6, step: 80
	action: tensor([[ 0.8057, -0.8306, -0.5712,  0.1269, -0.4189, -1.5014, -0.2517]],
       dtype=torch.float64)
	q_value: tensor([[-14.6023]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04148816135384936, distance: 1.1678413870035027 entropy 0.6056879162788391
epoch: 6, step: 81
	action: tensor([[ 0.1558,  0.7329, -0.5308,  0.0493, -0.5518,  0.0502, -0.2324]],
       dtype=torch.float64)
	q_value: tensor([[-21.0059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7203301226072375, distance: 0.6051729939974584 entropy 0.6056879162788391
epoch: 6, step: 82
	action: tensor([[ 0.3984, -0.4529,  0.7092, -0.3804, -0.1784,  0.5891,  0.0573]],
       dtype=torch.float64)
	q_value: tensor([[-10.9403]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16667005257192413, distance: 1.0446364802148116 entropy 0.6056879162788391
epoch: 6, step: 83
	action: tensor([[-0.1014,  0.4112, -0.0010,  0.2656, -0.3306,  0.3453,  0.9146]],
       dtype=torch.float64)
	q_value: tensor([[-15.6843]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5401317451021387, distance: 0.7760208834959564 entropy 0.6056879162788391
epoch: 6, step: 84
	action: tensor([[ 0.4943,  0.4110, -0.0567,  0.6489, -0.3774,  0.3989, -0.3772]],
       dtype=torch.float64)
	q_value: tensor([[-12.9678]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8656566256360945, distance: 0.4194351467091561 entropy 0.6056879162788391
epoch: 6, step: 85
	action: tensor([[ 0.1868,  0.1869, -0.2209, -0.1692,  0.4877,  0.8767, -0.0418]],
       dtype=torch.float64)
	q_value: tensor([[-12.7521]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7617570071492572, distance: 0.558556051413054 entropy 0.6056879162788391
epoch: 6, step: 86
	action: tensor([[ 0.7884,  0.2080, -0.1976, -0.8172, -0.8483, -0.2523,  0.2539]],
       dtype=torch.float64)
	q_value: tensor([[-11.9644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.48300319235087363, distance: 0.8228120134577033 entropy 0.6056879162788391
epoch: 6, step: 87
	action: tensor([[ 0.7064,  0.7372, -1.0692,  0.0630, -0.7292,  0.4823,  1.8718]],
       dtype=torch.float64)
	q_value: tensor([[-15.4599]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.938264163728752, distance: 0.284331751938968 entropy 0.6056879162788391
epoch: 6, step: 88
	action: tensor([[ 0.6851,  0.2670, -0.2504, -0.1190, -0.5940, -0.5661, -0.2169]],
       dtype=torch.float64)
	q_value: tensor([[-20.9251]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8223756404257339, distance: 0.48228966533493667 entropy 0.6056879162788391
epoch: 6, step: 89
	action: tensor([[ 0.0963,  0.4943, -0.5893,  0.4557,  0.4767, -0.2337, -0.8293]],
       dtype=torch.float64)
	q_value: tensor([[-12.6993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5898148313444396, distance: 0.7329032877112628 entropy 0.6056879162788391
epoch: 6, step: 90
	action: tensor([[ 1.0451, -0.0425,  0.5530, -0.7387, -0.1336,  0.1983,  0.0900]],
       dtype=torch.float64)
	q_value: tensor([[-12.7971]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.43107786953700067, distance: 0.863143704180452 entropy 0.6056879162788391
epoch: 6, step: 91
	action: tensor([[ 0.7339,  0.4189, -0.3923,  0.0876, -0.4668,  0.5094,  0.3498]],
       dtype=torch.float64)
	q_value: tensor([[-16.2410]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9639945332228913, distance: 0.21714054125022267 entropy 0.6056879162788391
epoch: 6, step: 92
	action: tensor([[ 0.8577, -0.2186, -0.1410,  1.1670,  0.1249,  0.4430,  1.2578]],
       dtype=torch.float64)
	q_value: tensor([[-13.3347]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.992191756853815, distance: 0.10111913772799119 entropy 0.6056879162788391
epoch: 6, step: 93
	action: tensor([[ 0.3883,  0.6054, -1.3830,  0.2919, -0.7630,  0.2968,  0.2379]],
       dtype=torch.float64)
	q_value: tensor([[-19.0674]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7051336519891138, distance: 0.6213972096683861 entropy 0.6056879162788391
epoch: 6, step: 94
	action: tensor([[ 1.0283,  0.6992,  0.0864,  0.9861, -0.0516,  0.3503,  0.1101]],
       dtype=torch.float64)
	q_value: tensor([[-14.2527]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 6, step: 95
	action: tensor([[ 0.5276, -0.2494,  0.5355,  0.3050, -0.2733, -0.2821, -1.5053]],
       dtype=torch.float64)
	q_value: tensor([[-16.1867]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5973568466525322, distance: 0.726134134169291 entropy 0.6056879162788391
epoch: 6, step: 96
	action: tensor([[ 0.9407,  0.5520, -0.4655,  0.0119, -0.6050, -1.2426,  0.7420]],
       dtype=torch.float64)
	q_value: tensor([[-18.4242]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.884765592394687, distance: 0.38846131863113453 entropy 0.6056879162788391
epoch: 6, step: 97
	action: tensor([[ 0.2061,  0.7236, -0.7398,  0.0793, -0.2762, -0.4754, -0.0059]],
       dtype=torch.float64)
	q_value: tensor([[-16.9293]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8524968545396856, distance: 0.43949843614661477 entropy 0.6056879162788391
epoch: 6, step: 98
	action: tensor([[ 0.6981, -0.4973, -0.4809,  0.3659,  0.2777, -0.4897, -1.1306]],
       dtype=torch.float64)
	q_value: tensor([[-11.0912]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3439203603210613, distance: 0.926904463396064 entropy 0.6056879162788391
epoch: 6, step: 99
	action: tensor([[ 1.3542, -0.0630, -0.6263, -0.5343, -0.0518, -0.4027,  0.3840]],
       dtype=torch.float64)
	q_value: tensor([[-16.7555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06288560822882905, distance: 1.1077786659760558 entropy 0.6056879162788391
epoch: 6, step: 100
	action: tensor([[ 1.5571,  1.0421, -0.6051,  0.7719, -0.4810, -0.5306,  0.0940]],
       dtype=torch.float64)
	q_value: tensor([[-16.2358]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 6, step: 101
	action: tensor([[-0.0407,  0.2923, -0.6677, -0.4117, -0.5585, -0.5260,  0.2683]],
       dtype=torch.float64)
	q_value: tensor([[-16.1867]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5884970979834809, distance: 0.7340795817204471 entropy 0.6056879162788391
epoch: 6, step: 102
	action: tensor([[ 0.4517, -0.0428, -0.1083,  0.0532,  0.3963,  0.1390,  0.2209]],
       dtype=torch.float64)
	q_value: tensor([[-11.8262]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.670005068422719, distance: 0.6573706773417065 entropy 0.6056879162788391
epoch: 6, step: 103
	action: tensor([[ 1.1975,  0.2741, -0.0169,  0.1179,  0.3203, -0.1535, -0.3873]],
       dtype=torch.float64)
	q_value: tensor([[-11.5270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8610301299792815, distance: 0.4265962357276756 entropy 0.6056879162788391
epoch: 6, step: 104
	action: tensor([[ 1.0437,  0.3006,  0.2295,  0.3456, -0.0621, -0.0231,  0.2359]],
       dtype=torch.float64)
	q_value: tensor([[-15.2446]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9542921676777398, distance: 0.24465382273158495 entropy 0.6056879162788391
epoch: 6, step: 105
	action: tensor([[ 0.0694,  0.4317, -0.7211, -0.2817, -0.0207, -0.5708, -0.0971]],
       dtype=torch.float64)
	q_value: tensor([[-14.3074]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6918520906315113, distance: 0.6352377606111236 entropy 0.6056879162788391
epoch: 6, step: 106
	action: tensor([[-0.1116,  0.0032,  0.0351,  0.0539, -0.3196,  0.3071,  1.1441]],
       dtype=torch.float64)
	q_value: tensor([[-10.4427]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24311800654539106, distance: 0.9955676585608587 entropy 0.6056879162788391
epoch: 6, step: 107
	action: tensor([[ 0.7232,  1.2078, -0.4278, -0.3494,  0.2480, -0.3629,  0.2464]],
       dtype=torch.float64)
	q_value: tensor([[-14.5808]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9517655503180508, distance: 0.25132480671749563 entropy 0.6056879162788391
epoch: 6, step: 108
	action: tensor([[ 0.3843, -0.2407, -0.4360,  0.0415,  1.0727,  0.1331, -0.8648]],
       dtype=torch.float64)
	q_value: tensor([[-13.7098]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.472666232080326, distance: 0.8309970545623707 entropy 0.6056879162788391
epoch: 6, step: 109
	action: tensor([[ 0.7968,  0.4291, -0.1866,  0.2593, -0.1932,  0.7270,  0.1645]],
       dtype=torch.float64)
	q_value: tensor([[-14.4109]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9674095149245884, distance: 0.20658657975839986 entropy 0.6056879162788391
epoch: 6, step: 110
	action: tensor([[ 0.5838,  0.5114, -0.3383,  0.8678,  0.8800, -1.1961,  0.4555]],
       dtype=torch.float64)
	q_value: tensor([[-13.7099]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.820327535919645, distance: 0.48506222572620344 entropy 0.6056879162788391
epoch: 6, step: 111
	action: tensor([[ 0.7349,  0.2771, -0.6789,  0.0685,  0.0136, -0.2013, -0.3909]],
       dtype=torch.float64)
	q_value: tensor([[-18.9940]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8886109810027518, distance: 0.38192482019253154 entropy 0.6056879162788391
epoch: 6, step: 112
	action: tensor([[ 0.7504,  0.4829, -0.0053,  0.5024,  0.1331, -0.0444,  0.6596]],
       dtype=torch.float64)
	q_value: tensor([[-12.0033]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9678227991679084, distance: 0.20527252469629875 entropy 0.6056879162788391
epoch: 6, step: 113
	action: tensor([[ 1.0280,  0.3111, -0.1924,  0.8047,  0.5530, -0.2615,  0.6712]],
       dtype=torch.float64)
	q_value: tensor([[-13.3739]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9867431326194943, distance: 0.13175804708841718 entropy 0.6056879162788391
epoch: 6, step: 114
	action: tensor([[ 0.6311,  0.2282, -0.8538,  0.0700, -0.0917, -0.7443,  0.6851]],
       dtype=torch.float64)
	q_value: tensor([[-16.7727]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8330720424969648, distance: 0.4675426488624808 entropy 0.6056879162788391
epoch: 6, step: 115
	action: tensor([[ 0.0224, -0.5910, -0.4988,  0.2374, -0.0715, -0.5058,  0.7649]],
       dtype=torch.float64)
	q_value: tensor([[-14.0129]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15651592629732147, distance: 1.2306441800102026 entropy 0.6056879162788391
epoch: 6, step: 116
	action: tensor([[ 0.3826, -0.4008, -0.4754, -0.7473,  0.2848,  0.3045,  0.3476]],
       dtype=torch.float64)
	q_value: tensor([[-14.7182]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.02093001944545314, distance: 1.1323053535410095 entropy 0.6056879162788391
epoch: 6, step: 117
	action: tensor([[ 1.0122,  0.4319, -0.5240,  0.3591, -0.6505,  0.7627,  0.7417]],
       dtype=torch.float64)
	q_value: tensor([[-12.7843]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9801127111063552, distance: 0.16137805786461096 entropy 0.6056879162788391
epoch: 6, step: 118
	action: tensor([[ 0.4201,  0.9982, -0.2549,  0.1342, -0.1940, -0.8698,  0.5743]],
       dtype=torch.float64)
	q_value: tensor([[-16.7501]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9156601924982191, distance: 0.33233263881395714 entropy 0.6056879162788391
epoch: 6, step: 119
	action: tensor([[ 0.2916,  0.7327,  0.2624,  0.2881,  0.9463, -0.0997, -0.4865]],
       dtype=torch.float64)
	q_value: tensor([[-13.4837]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 6, step: 120
	action: tensor([[-0.4758, -0.2298, -0.1052,  0.3021, -0.0243, -1.0027, -0.4153]],
       dtype=torch.float64)
	q_value: tensor([[-16.1867]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3527022912753657, distance: 1.3309379423578886 entropy 0.6056879162788391
epoch: 6, step: 121
	action: tensor([[ 0.9639, -0.0812, -0.6737, -0.1920, -0.4492, -0.1926, -0.5417]],
       dtype=torch.float64)
	q_value: tensor([[-14.7860]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5254917600246057, distance: 0.7882764870891831 entropy 0.6056879162788391
epoch: 6, step: 122
	action: tensor([[ 0.1423, -0.1765, -0.7047, -0.4316, -0.2537,  0.3422, -0.6851]],
       dtype=torch.float64)
	q_value: tensor([[-13.5789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23711683952013374, distance: 0.9995066951499845 entropy 0.6056879162788391
epoch: 6, step: 123
	action: tensor([[ 0.9770, -0.3629, -0.8094,  0.0441, -0.5334, -0.0972,  0.8350]],
       dtype=torch.float64)
	q_value: tensor([[-11.2882]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4141292466059815, distance: 0.8759062042773021 entropy 0.6056879162788391
epoch: 6, step: 124
	action: tensor([[ 0.9499,  0.4057, -0.5140,  0.0314,  0.0371, -0.3968, -0.0714]],
       dtype=torch.float64)
	q_value: tensor([[-15.9345]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9151656653243277, distance: 0.3333055323738145 entropy 0.6056879162788391
epoch: 6, step: 125
	action: tensor([[ 0.6780,  0.2831, -0.9667,  0.6244,  0.0620, -0.1172, -0.5159]],
       dtype=torch.float64)
	q_value: tensor([[-13.0852]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8660033776553312, distance: 0.41889349754187966 entropy 0.6056879162788391
epoch: 6, step: 126
	action: tensor([[ 1.4301,  0.8267, -0.4396,  0.4160, -0.6340, -0.2991, -0.5269]],
       dtype=torch.float64)
	q_value: tensor([[-13.6594]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 6, step: 127
	action: tensor([[ 0.2158,  0.1188, -0.1275, -0.2033,  0.0077, -0.1838,  0.0930]],
       dtype=torch.float64)
	q_value: tensor([[-16.1867]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4682458983228962, distance: 0.834472669909191 entropy 0.6056879162788391
LOSS epoch 6 actor 160.1066199523196 critic 512.8372324388538 
epoch: 7, step: 0
	action: tensor([[-0.0726,  0.2797, -0.9643,  0.2717, -0.1670, -0.2056,  0.0969]],
       dtype=torch.float64)
	q_value: tensor([[-10.5129]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38412191895415015, distance: 0.8980573409173932 entropy 0.6056879162788391
epoch: 7, step: 1
	action: tensor([[ 0.0015,  0.0867, -0.9882, -0.9914,  0.3123,  0.0027, -0.0815]],
       dtype=torch.float64)
	q_value: tensor([[-10.8408]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.536601130588116, distance: 0.7789941176007269 entropy 0.6056879162788391
epoch: 7, step: 2
	action: tensor([[-0.0417,  0.3927, -0.4329,  0.1932, -0.3000,  0.5088,  0.3640]],
       dtype=torch.float64)
	q_value: tensor([[-12.7078]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4757280528779503, distance: 0.8285810627646972 entropy 0.6056879162788391
epoch: 7, step: 3
	action: tensor([[ 0.1589,  0.0584, -0.1497, -0.2894,  0.6795,  0.2620,  0.3967]],
       dtype=torch.float64)
	q_value: tensor([[-11.8071]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4651882872866606, distance: 0.836868359517686 entropy 0.6056879162788391
epoch: 7, step: 4
	action: tensor([[-0.5228,  0.1416,  0.0059, -0.6701,  0.1847,  0.3526,  0.1024]],
       dtype=torch.float64)
	q_value: tensor([[-12.3073]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3761133469198912, distance: 1.3424057286431987 entropy 0.6056879162788391
epoch: 7, step: 5
	action: tensor([[ 1.3092, -0.0404, -0.1677,  0.2897, -0.3025, -0.3162,  0.3877]],
       dtype=torch.float64)
	q_value: tensor([[-12.3347]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6474999951264082, distance: 0.6794167615633478 entropy 0.6056879162788391
epoch: 7, step: 6
	action: tensor([[ 0.1590, -0.2535,  0.3237, -0.0599,  0.1736, -0.4394, -0.0151]],
       dtype=torch.float64)
	q_value: tensor([[-16.8441]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05541517021185216, distance: 1.112185366276528 entropy 0.6056879162788391
epoch: 7, step: 7
	action: tensor([[ 0.8401, -0.7619,  0.1414,  0.2787,  0.0023, -0.4006, -0.3185]],
       dtype=torch.float64)
	q_value: tensor([[-12.8887]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.02022869076577205, distance: 1.1327108281619902 entropy 0.6056879162788391
epoch: 7, step: 8
	action: tensor([[-0.0638, -0.5543, -0.6134, -0.2120, -0.4683, -0.3229,  0.2702]],
       dtype=torch.float64)
	q_value: tensor([[-17.0400]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.22771532948303919, distance: 1.2679599397429269 entropy 0.6056879162788391
epoch: 7, step: 9
	action: tensor([[ 0.6007,  0.2135, -0.2254, -0.0394,  0.3303, -0.2682,  0.3652]],
       dtype=torch.float64)
	q_value: tensor([[-13.3357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7840211040342651, distance: 0.5318171663688755 entropy 0.6056879162788391
epoch: 7, step: 10
	action: tensor([[ 0.4786, -0.2813, -0.6666, -0.3049,  0.0265,  0.3049,  0.2961]],
       dtype=torch.float64)
	q_value: tensor([[-12.7391]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36223031627765656, distance: 0.9138788550351107 entropy 0.6056879162788391
epoch: 7, step: 11
	action: tensor([[ 0.0938, -0.0373, -0.4415,  0.6338, -0.7926,  0.4201,  0.5540]],
       dtype=torch.float64)
	q_value: tensor([[-12.5794]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42057969987327004, distance: 0.8710709826414588 entropy 0.6056879162788391
epoch: 7, step: 12
	action: tensor([[-0.2386, -0.0201,  0.2978,  0.8773, -0.2246, -0.6178, -0.0447]],
       dtype=torch.float64)
	q_value: tensor([[-14.2158]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44022435144159255, distance: 0.8561772705375943 entropy 0.6056879162788391
epoch: 7, step: 13
	action: tensor([[ 0.3820,  0.6251,  0.2019,  0.0894,  0.5703, -0.9731, -0.4932]],
       dtype=torch.float64)
	q_value: tensor([[-15.3195]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.76288080021233, distance: 0.5572371387834972 entropy 0.6056879162788391
epoch: 7, step: 14
	action: tensor([[ 0.8373,  0.3550,  0.4368, -0.7485,  0.0993, -0.4378,  0.3143]],
       dtype=torch.float64)
	q_value: tensor([[-16.0370]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6088155311905057, distance: 0.7157271554239956 entropy 0.6056879162788391
epoch: 7, step: 15
	action: tensor([[ 0.2040,  0.1064, -1.1203, -0.4494, -0.0573, -0.2585, -0.7404]],
       dtype=torch.float64)
	q_value: tensor([[-16.2783]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6406312077563175, distance: 0.6860043558439394 entropy 0.6056879162788391
epoch: 7, step: 16
	action: tensor([[ 0.5827,  0.1599,  0.1732,  0.1313, -0.4883,  0.3623,  0.1450]],
       dtype=torch.float64)
	q_value: tensor([[-12.3222]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8869668677158864, distance: 0.3847331201125717 entropy 0.6056879162788391
epoch: 7, step: 17
	action: tensor([[-0.3629, -0.4532, -0.9752, -0.4951, -0.4846,  0.7775, -0.0329]],
       dtype=torch.float64)
	q_value: tensor([[-13.7716]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4590337407630669, distance: 1.3822587916838724 entropy 0.6056879162788391
epoch: 7, step: 18
	action: tensor([[ 0.0552,  0.3086,  0.0323, -0.5337, -0.3944,  0.3437, -0.4015]],
       dtype=torch.float64)
	q_value: tensor([[-14.2339]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3508873242112335, distance: 0.9219698907410417 entropy 0.6056879162788391
epoch: 7, step: 19
	action: tensor([[-0.1064,  0.7697, -0.2513, -0.4736, -0.2401, -0.0414, -0.4372]],
       dtype=torch.float64)
	q_value: tensor([[-13.1160]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5394474831383762, distance: 0.7765980098670331 entropy 0.6056879162788391
epoch: 7, step: 20
	action: tensor([[ 0.5634,  0.3791, -0.8201, -0.5596, -0.0531, -0.2108, -0.5637]],
       dtype=torch.float64)
	q_value: tensor([[-11.8764]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8210666290770486, distance: 0.4840635317469857 entropy 0.6056879162788391
epoch: 7, step: 21
	action: tensor([[-0.3225,  0.0228,  0.4134,  0.3275, -0.4632,  0.2876, -0.6044]],
       dtype=torch.float64)
	q_value: tensor([[-12.5974]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2757199989655319, distance: 0.9738900752624011 entropy 0.6056879162788391
epoch: 7, step: 22
	action: tensor([[-0.3885, -0.4846, -0.4822, -0.0345,  0.7939, -0.1244,  0.3864]],
       dtype=torch.float64)
	q_value: tensor([[-13.8612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5681602562629626, distance: 1.4330188879959689 entropy 0.6056879162788391
epoch: 7, step: 23
	action: tensor([[ 0.9314,  0.3016,  0.0715, -0.3083,  0.5636, -0.0409, -0.4479]],
       dtype=torch.float64)
	q_value: tensor([[-14.3476]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.817692709026713, distance: 0.4886059058965305 entropy 0.6056879162788391
epoch: 7, step: 24
	action: tensor([[ 0.6347,  0.3668, -0.5631,  0.5024,  0.6800,  0.3463,  0.1799]],
       dtype=torch.float64)
	q_value: tensor([[-14.6626]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9285984351047097, distance: 0.30578104350058694 entropy 0.6056879162788391
epoch: 7, step: 25
	action: tensor([[ 0.1758, -0.4066,  0.1372, -0.3279,  0.3878, -0.3584, -0.3147]],
       dtype=torch.float64)
	q_value: tensor([[-14.1888]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.22841879487606054, distance: 1.2683231502450747 entropy 0.6056879162788391
epoch: 7, step: 26
	action: tensor([[ 0.3792, -0.1971, -0.2070, -0.4431, -0.8053, -0.3047, -0.1010]],
       dtype=torch.float64)
	q_value: tensor([[-12.8169]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1923108988129162, distance: 1.0284396327045453 entropy 0.6056879162788391
epoch: 7, step: 27
	action: tensor([[ 0.1955,  0.0593, -0.2475, -0.0683, -0.1786, -0.1382,  0.4640]],
       dtype=torch.float64)
	q_value: tensor([[-14.0331]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.47929907115866477, distance: 0.8257543486077666 entropy 0.6056879162788391
epoch: 7, step: 28
	action: tensor([[ 0.9086,  0.1932, -0.0781,  0.3006,  0.0144, -0.8929, -0.2086]],
       dtype=torch.float64)
	q_value: tensor([[-11.2520]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8376304422893972, distance: 0.4611147315109462 entropy 0.6056879162788391
epoch: 7, step: 29
	action: tensor([[ 0.3136,  0.2869, -0.3336, -0.3295,  0.4159,  0.0835, -0.1397]],
       dtype=torch.float64)
	q_value: tensor([[-16.2962]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6980566783618514, distance: 0.6288099533086187 entropy 0.6056879162788391
epoch: 7, step: 30
	action: tensor([[ 0.0286,  0.5864, -0.4585,  0.0078,  0.6609, -0.0718,  0.3041]],
       dtype=torch.float64)
	q_value: tensor([[-10.5404]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6343228958576524, distance: 0.6919991767532845 entropy 0.6056879162788391
epoch: 7, step: 31
	action: tensor([[ 0.6667, -0.2154, -0.5825, -0.4360, -0.8831,  0.1228, -0.0614]],
       dtype=torch.float64)
	q_value: tensor([[-11.8655]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3601018710292586, distance: 0.9154025405960727 entropy 0.6056879162788391
epoch: 7, step: 32
	action: tensor([[ 0.6796,  0.3418, -0.1962, -0.2446,  0.1758,  0.9317, -0.4976]],
       dtype=torch.float64)
	q_value: tensor([[-15.0047]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9665067665620349, distance: 0.2094282344309554 entropy 0.6056879162788391
epoch: 7, step: 33
	action: tensor([[ 0.5229,  0.4228,  0.6629, -0.1393, -0.4676, -0.6117,  0.6637]],
       dtype=torch.float64)
	q_value: tensor([[-14.7442]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7714255430337975, distance: 0.5471048208452943 entropy 0.6056879162788391
epoch: 7, step: 34
	action: tensor([[ 0.2345, -0.0392, -0.1884, -0.1594, -0.1059,  0.4062, -0.1328]],
       dtype=torch.float64)
	q_value: tensor([[-16.4864]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4861577347764825, distance: 0.8202979100273667 entropy 0.6056879162788391
epoch: 7, step: 35
	action: tensor([[-0.3885,  0.2710, -0.5296, -0.2169, -0.0373,  0.2133, -0.1492]],
       dtype=torch.float64)
	q_value: tensor([[-10.9418]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0734568635830205, distance: 1.1015127155911812 entropy 0.6056879162788391
epoch: 7, step: 36
	action: tensor([[-0.4401,  0.6061,  0.3878,  1.0575,  0.1762, -0.7479, -0.3898]],
       dtype=torch.float64)
	q_value: tensor([[-9.7055]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5447621268464999, distance: 0.7721041488398636 entropy 0.6056879162788391
epoch: 7, step: 37
	action: tensor([[ 1.0232, -0.0520,  0.2653, -0.2436, -0.6795,  0.0799,  0.1110]],
       dtype=torch.float64)
	q_value: tensor([[-16.4160]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.584299132942008, distance: 0.7378144531745666 entropy 0.6056879162788391
epoch: 7, step: 38
	action: tensor([[ 0.1417,  0.8060, -0.0696, -0.2087, -0.4260, -0.2954, -0.4790]],
       dtype=torch.float64)
	q_value: tensor([[-16.3818]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7716031123053769, distance: 0.546892268924979 entropy 0.6056879162788391
epoch: 7, step: 39
	action: tensor([[-0.0085, -0.4309, -0.2845,  0.4129, -0.3869,  0.4040,  0.3195]],
       dtype=torch.float64)
	q_value: tensor([[-12.8676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16681612764532694, distance: 1.044544918619837 entropy 0.6056879162788391
epoch: 7, step: 40
	action: tensor([[-0.4941,  0.2773,  0.3753, -0.6914, -0.2855, -0.2340, -0.0037]],
       dtype=torch.float64)
	q_value: tensor([[-12.8304]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.49092436713985954, distance: 1.397283400216644 entropy 0.6056879162788391
epoch: 7, step: 41
	action: tensor([[-0.1459, -0.0385,  0.1478,  0.0079,  0.0274,  0.2440, -0.0079]],
       dtype=torch.float64)
	q_value: tensor([[-13.3675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16945671530831763, distance: 1.0428883807477558 entropy 0.6056879162788391
epoch: 7, step: 42
	action: tensor([[ 0.0497,  0.0377,  0.5896,  0.5491, -0.5542, -0.2210, -0.6656]],
       dtype=torch.float64)
	q_value: tensor([[-11.1348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6554907200925097, distance: 0.6716718630382679 entropy 0.6056879162788391
epoch: 7, step: 43
	action: tensor([[ 0.0752, -0.8201, -0.4220,  0.3745, -0.1149, -0.2909, -0.4538]],
       dtype=torch.float64)
	q_value: tensor([[-15.3872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2482809639058019, distance: 1.2785357233760901 entropy 0.6056879162788391
epoch: 7, step: 44
	action: tensor([[ 7.7833e-01, -4.7506e-01, -9.1410e-01,  5.8674e-04, -1.7480e-02,
         -1.0860e-01, -4.4171e-02]], dtype=torch.float64)
	q_value: tensor([[-14.2314]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26805785511882374, distance: 0.9790279047114896 entropy 0.6056879162788391
epoch: 7, step: 45
	action: tensor([[ 0.6185,  0.4709, -0.4328, -0.3466,  0.2944,  0.4195,  0.2796]],
       dtype=torch.float64)
	q_value: tensor([[-13.7867]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.944136042999356, distance: 0.270472142135734 entropy 0.6056879162788391
epoch: 7, step: 46
	action: tensor([[ 0.6582,  0.1515,  0.1556,  1.1008,  0.0976,  0.1291, -0.6790]],
       dtype=torch.float64)
	q_value: tensor([[-13.0285]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9589805031150881, distance: 0.2317671380339348 entropy 0.6056879162788391
epoch: 7, step: 47
	action: tensor([[-0.4055, -0.1976, -0.2266,  0.4817, -0.2382, -0.1638,  0.2428]],
       dtype=torch.float64)
	q_value: tensor([[-16.8019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12565252064804966, distance: 1.2141123236425821 entropy 0.6056879162788391
epoch: 7, step: 48
	action: tensor([[ 0.7139, -0.0699, -0.0639,  0.4916, -0.1258, -0.7701, -0.0191]],
       dtype=torch.float64)
	q_value: tensor([[-12.2357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7555087259894356, distance: 0.5658331337779375 entropy 0.6056879162788391
epoch: 7, step: 49
	action: tensor([[-2.2083e-01,  4.1881e-04, -7.0793e-01, -5.4631e-02, -2.2477e-01,
          2.4645e-01,  5.6630e-01]], dtype=torch.float64)
	q_value: tensor([[-15.7138]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06269865362746918, distance: 1.107889161552902 entropy 0.6056879162788391
epoch: 7, step: 50
	action: tensor([[ 0.0478,  0.0380,  0.3968,  0.5988, -0.9632, -0.2863,  0.1271]],
       dtype=torch.float64)
	q_value: tensor([[-12.0194]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6877950908676955, distance: 0.6394057789374306 entropy 0.6056879162788391
epoch: 7, step: 51
	action: tensor([[-0.0947,  0.3882, -0.0428,  0.6149,  0.4026,  0.6057, -0.7379]],
       dtype=torch.float64)
	q_value: tensor([[-15.3540]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6526643440399812, distance: 0.6744214544524382 entropy 0.6056879162788391
epoch: 7, step: 52
	action: tensor([[-0.0851,  0.6069,  0.0071, -0.2878,  0.8580,  0.3989,  0.4593]],
       dtype=torch.float64)
	q_value: tensor([[-13.5482]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5986961980195902, distance: 0.7249254226139502 entropy 0.6056879162788391
epoch: 7, step: 53
	action: tensor([[-0.0733, -0.3962, -0.6999,  0.4891, -0.0251, -0.1235,  0.3732]],
       dtype=torch.float64)
	q_value: tensor([[-13.4532]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08668893603925976, distance: 1.192914495739902 entropy 0.6056879162788391
epoch: 7, step: 54
	action: tensor([[ 0.2596, -0.5155, -0.0528,  0.0037, -0.5793, -0.1894, -0.3974]],
       dtype=torch.float64)
	q_value: tensor([[-12.8723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.015321993933025024, distance: 1.135543595102472 entropy 0.6056879162788391
epoch: 7, step: 55
	action: tensor([[ 0.4814, -0.0939, -0.1056,  0.1535,  0.0092,  0.1845, -0.2488]],
       dtype=torch.float64)
	q_value: tensor([[-13.4039]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6853281890969787, distance: 0.6419269558522162 entropy 0.6056879162788391
epoch: 7, step: 56
	action: tensor([[ 0.4939,  0.3249, -0.8146,  0.2109, -0.2536,  0.0205, -0.0868]],
       dtype=torch.float64)
	q_value: tensor([[-11.5813]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8361587919703466, distance: 0.463199694184683 entropy 0.6056879162788391
epoch: 7, step: 57
	action: tensor([[-0.5800, -0.7959, -0.9444,  0.2923, -0.6640,  0.6062, -0.4286]],
       dtype=torch.float64)
	q_value: tensor([[-11.5641]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0936965703032024, distance: 1.6558216780381132 entropy 0.6056879162788391
epoch: 7, step: 58
	action: tensor([[ 0.5784, -0.0533,  0.1956, -0.6819,  0.2997,  0.6535,  0.2938]],
       dtype=torch.float64)
	q_value: tensor([[-15.3407]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.48756298669511056, distance: 0.819175469849194 entropy 0.6056879162788391
epoch: 7, step: 59
	action: tensor([[ 0.2055, -0.0645, -0.7329,  0.0048, -0.1873,  0.5478,  0.8014]],
       dtype=torch.float64)
	q_value: tensor([[-15.1549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4496113912023234, distance: 0.8489681777740458 entropy 0.6056879162788391
epoch: 7, step: 60
	action: tensor([[-0.0383,  0.3722, -0.3212,  0.0473,  0.2249, -0.0910,  0.7285]],
       dtype=torch.float64)
	q_value: tensor([[-13.9474]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.48332804334652435, distance: 0.8225534690153988 entropy 0.6056879162788391
epoch: 7, step: 61
	action: tensor([[-0.9071, -0.4404, -0.0801,  0.2918,  0.1818, -0.8982, -0.2139]],
       dtype=torch.float64)
	q_value: tensor([[-12.1887]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0522664944381557, distance: 1.6393571179745674 entropy 0.6056879162788391
epoch: 7, step: 62
	action: tensor([[-0.1650, -0.1792,  0.2791, -0.6579,  0.5667,  0.3437, -0.0975]],
       dtype=torch.float64)
	q_value: tensor([[-16.3793]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2959513060844874, distance: 1.3027198632995165 entropy 0.6056879162788391
epoch: 7, step: 63
	action: tensor([[ 0.4107,  0.2154, -0.4871,  0.4751, -0.7344, -0.3519,  0.1366]],
       dtype=torch.float64)
	q_value: tensor([[-12.6416]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8305721608084269, distance: 0.47103055421366247 entropy 0.6056879162788391
epoch: 7, step: 64
	action: tensor([[ 0.6328,  0.3403, -0.1246,  0.2209,  0.2847,  0.3448,  0.0972]],
       dtype=torch.float64)
	q_value: tensor([[-12.7691]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9623163416887147, distance: 0.22214330141115485 entropy 0.6056879162788391
epoch: 7, step: 65
	action: tensor([[ 0.5617, -0.4635, -0.1060,  0.6686, -0.2278,  0.2268, -0.5481]],
       dtype=torch.float64)
	q_value: tensor([[-12.3414]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7337057171463683, distance: 0.5905240838905155 entropy 0.6056879162788391
epoch: 7, step: 66
	action: tensor([[-0.3881, -0.6790, -0.9070,  0.3738,  0.3017, -0.7588, -0.4203]],
       dtype=torch.float64)
	q_value: tensor([[-14.9422]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6599940432293641, distance: 1.4743817892349067 entropy 0.6056879162788391
epoch: 7, step: 67
	action: tensor([[ 0.4066,  0.0618, -0.2014, -0.6541,  0.7983, -0.0092,  0.6457]],
       dtype=torch.float64)
	q_value: tensor([[-15.9585]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3839130117536943, distance: 0.8982096395166818 entropy 0.6056879162788391
epoch: 7, step: 68
	action: tensor([[ 0.6736, -0.2618, -1.0689,  0.2008, -0.4368, -0.4230, -0.3199]],
       dtype=torch.float64)
	q_value: tensor([[-14.6325]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5433000325599211, distance: 0.773343044114588 entropy 0.6056879162788391
epoch: 7, step: 69
	action: tensor([[ 0.3773, -0.5234,  0.0393, -0.2648,  0.0710, -0.6663,  0.6845]],
       dtype=torch.float64)
	q_value: tensor([[-14.2535]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.22591617492753424, distance: 1.2670305336232264 entropy 0.6056879162788391
epoch: 7, step: 70
	action: tensor([[ 1.3336,  0.8219, -0.6922, -0.0247,  0.3898,  0.3935, -0.0536]],
       dtype=torch.float64)
	q_value: tensor([[-15.5100]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 7, step: 71
	action: tensor([[ 0.4392,  0.1671,  0.5271, -0.0871, -0.0943,  0.6493, -0.7899]],
       dtype=torch.float64)
	q_value: tensor([[-16.9480]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8396108125160837, distance: 0.4582940690516 entropy 0.6056879162788391
epoch: 7, step: 72
	action: tensor([[ 0.7107,  0.1547, -0.4091,  0.5724, -0.3224, -0.2268,  0.0063]],
       dtype=torch.float64)
	q_value: tensor([[-15.8521]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9423685732600925, distance: 0.2747175336791465 entropy 0.6056879162788391
epoch: 7, step: 73
	action: tensor([[-0.0195,  0.1414, -0.1017,  0.1762,  0.3416, -0.4547,  0.1084]],
       dtype=torch.float64)
	q_value: tensor([[-13.1248]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34758405439194195, distance: 0.9243128203079448 entropy 0.6056879162788391
epoch: 7, step: 74
	action: tensor([[-0.5137, -0.2160, -0.0469, -0.0057, -0.5581, -0.2847, -0.8682]],
       dtype=torch.float64)
	q_value: tensor([[-11.7735]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.44627975119351615, distance: 1.376204096296219 entropy 0.6056879162788391
epoch: 7, step: 75
	action: tensor([[ 0.3797,  0.5929, -0.3251,  0.0775,  0.2215, -0.6138, -0.5998]],
       dtype=torch.float64)
	q_value: tensor([[-13.2589]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8471266941088178, distance: 0.447427343953389 entropy 0.6056879162788391
epoch: 7, step: 76
	action: tensor([[ 0.1472, -0.6125,  0.0095,  0.2016,  0.4394,  0.0829,  0.0351]],
       dtype=torch.float64)
	q_value: tensor([[-13.1994]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04293970545084558, distance: 1.1195057867767968 entropy 0.6056879162788391
epoch: 7, step: 77
	action: tensor([[ 0.0626, -0.1875, -0.0265,  0.2702, -0.0183, -0.4640, -0.2992]],
       dtype=torch.float64)
	q_value: tensor([[-13.5099]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2654527439435075, distance: 0.9807686236904557 entropy 0.6056879162788391
epoch: 7, step: 78
	action: tensor([[ 0.5738,  0.2896,  0.2229, -0.1473,  0.0833,  0.6658, -0.1944]],
       dtype=torch.float64)
	q_value: tensor([[-12.4053]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9235059791292458, distance: 0.3164976125033555 entropy 0.6056879162788391
epoch: 7, step: 79
	action: tensor([[ 0.2779, -0.6308,  0.0938,  0.6138,  0.3373, -0.0929, -0.1565]],
       dtype=torch.float64)
	q_value: tensor([[-13.8766]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35628789512807846, distance: 0.9181265182080784 entropy 0.6056879162788391
epoch: 7, step: 80
	action: tensor([[ 0.3800, -0.1850, -0.6289,  0.1092,  0.5276,  0.1358,  0.3449]],
       dtype=torch.float64)
	q_value: tensor([[-15.1402]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.511577348307962, distance: 0.7997506317654971 entropy 0.6056879162788391
epoch: 7, step: 81
	action: tensor([[ 0.0858,  0.2041,  0.0335, -0.0156, -0.2285, -0.2887,  0.0990]],
       dtype=torch.float64)
	q_value: tensor([[-13.0153]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4701836732591955, distance: 0.8329508235677286 entropy 0.6056879162788391
epoch: 7, step: 82
	action: tensor([[ 0.5402, -0.5867,  0.7085, -0.3884,  0.5038,  0.4577, -0.1823]],
       dtype=torch.float64)
	q_value: tensor([[-10.9009]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.021002771864571113, distance: 1.1562990100706145 entropy 0.6056879162788391
epoch: 7, step: 83
	action: tensor([[ 1.0929,  0.1765, -0.3777,  0.2133, -0.7964,  0.0231,  0.5911]],
       dtype=torch.float64)
	q_value: tensor([[-16.5067]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.89795426212569, distance: 0.3655561783683618 entropy 0.6056879162788391
epoch: 7, step: 84
	action: tensor([[ 0.8377,  0.1024, -0.1268, -0.1431, -0.2991, -0.0722,  0.0817]],
       dtype=torch.float64)
	q_value: tensor([[-16.4250]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7455593448989555, distance: 0.5772313975627779 entropy 0.6056879162788391
epoch: 7, step: 85
	action: tensor([[-0.5456, -0.0292, -0.4025, -0.0511, -0.5008, -0.5636, -0.9165]],
       dtype=torch.float64)
	q_value: tensor([[-13.0861]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2554779924107897, distance: 1.2822161580098101 entropy 0.6056879162788391
epoch: 7, step: 86
	action: tensor([[ 0.0181,  0.5125,  0.4531, -0.5122,  0.0376, -0.5433,  0.4407]],
       dtype=torch.float64)
	q_value: tensor([[-13.3914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2448352487617519, distance: 0.9944376266354382 entropy 0.6056879162788391
epoch: 7, step: 87
	action: tensor([[ 0.9553, -0.8597, -0.5140, -0.4329, -0.3064,  0.0671, -0.4706]],
       dtype=torch.float64)
	q_value: tensor([[-13.7352]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4760768090044616, distance: 1.3903084802271535 entropy 0.6056879162788391
epoch: 7, step: 88
	action: tensor([[ 0.3373,  0.7504, -0.5692, -0.7742, -0.2436, -0.8332, -0.8407]],
       dtype=torch.float64)
	q_value: tensor([[-16.0633]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8496867435496892, distance: 0.44366516937633405 entropy 0.6056879162788391
epoch: 7, step: 89
	action: tensor([[ 0.6078, -0.1960, -0.0975,  0.9700, -0.2231,  0.3942, -0.1664]],
       dtype=torch.float64)
	q_value: tensor([[-15.1747]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9310766762536715, distance: 0.3004275800718527 entropy 0.6056879162788391
epoch: 7, step: 90
	action: tensor([[-0.3280, -0.4799,  0.1655, -0.0550, -0.2330, -0.0110,  0.2215]],
       dtype=torch.float64)
	q_value: tensor([[-15.2282]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4684886764049856, distance: 1.386730265830379 entropy 0.6056879162788391
epoch: 7, step: 91
	action: tensor([[-0.7014,  0.3207, -0.9860,  0.2673,  0.1589,  0.3449, -0.2776]],
       dtype=torch.float64)
	q_value: tensor([[-13.0820]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.38892674630549395, distance: 1.3486410164996963 entropy 0.6056879162788391
epoch: 7, step: 92
	action: tensor([[ 0.0587,  0.0570,  0.1130, -0.1003,  0.4608, -0.4180, -0.3728]],
       dtype=torch.float64)
	q_value: tensor([[-11.6217]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22566490216878776, distance: 1.0069807435312876 entropy 0.6056879162788391
epoch: 7, step: 93
	action: tensor([[ 0.5815,  0.2316,  0.1510,  0.7644, -0.5223, -0.3081, -0.2550]],
       dtype=torch.float64)
	q_value: tensor([[-11.7684]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9810422387977827, distance: 0.15756154020512278 entropy 0.6056879162788391
epoch: 7, step: 94
	action: tensor([[-0.2228,  0.3576, -0.0275,  0.1324,  0.3603,  0.1492,  0.5686]],
       dtype=torch.float64)
	q_value: tensor([[-14.8767]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37605677224220113, distance: 0.90391840832946 entropy 0.6056879162788391
epoch: 7, step: 95
	action: tensor([[ 0.2384, -0.6701,  0.6870, -0.0363,  1.5097, -0.2603,  0.5555]],
       dtype=torch.float64)
	q_value: tensor([[-12.1241]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12403653135944692, distance: 1.21324051944889 entropy 0.6056879162788391
epoch: 7, step: 96
	action: tensor([[ 1.0901,  0.7463, -0.2927,  0.1161,  0.5905, -0.0163,  0.5029]],
       dtype=torch.float64)
	q_value: tensor([[-21.4929]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9407098906546801, distance: 0.27864279572991674 entropy 0.6056879162788391
epoch: 7, step: 97
	action: tensor([[ 0.9339,  0.7053,  0.0747,  1.5428, -0.5940, -0.2925,  0.0584]],
       dtype=torch.float64)
	q_value: tensor([[-16.1421]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.6056879162788391
epoch: 7, step: 98
	action: tensor([[ 0.1392,  0.4439, -0.8375,  0.1834, -0.7051,  0.0508, -0.8875]],
       dtype=torch.float64)
	q_value: tensor([[-16.9480]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6059103669867296, distance: 0.7183799430640816 entropy 0.6056879162788391
epoch: 7, step: 99
	action: tensor([[ 0.4757, -0.0502, -0.5999,  0.5665,  0.0232, -0.2123,  0.0255]],
       dtype=torch.float64)
	q_value: tensor([[-12.9294]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7308308538157218, distance: 0.5937031206979595 entropy 0.6056879162788391
epoch: 7, step: 100
	action: tensor([[ 0.5746,  0.2291, -0.5627,  0.5275, -0.8082, -0.0351, -0.7906]],
       dtype=torch.float64)
	q_value: tensor([[-12.6958]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.876535818235323, distance: 0.40209362214507127 entropy 0.6056879162788391
epoch: 7, step: 101
	action: tensor([[ 1.0983,  0.1480, -0.3023,  0.2781,  0.8112, -0.1141, -0.1730]],
       dtype=torch.float64)
	q_value: tensor([[-14.6352]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8878162269722941, distance: 0.38328490392214337 entropy 0.6056879162788391
epoch: 7, step: 102
	action: tensor([[ 0.6972,  0.9367, -0.7558, -0.4533, -0.6032,  0.1791, -0.2848]],
       dtype=torch.float64)
	q_value: tensor([[-16.4859]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9831875467245572, distance: 0.1483789312003652 entropy 0.6056879162788391
epoch: 7, step: 103
	action: tensor([[ 0.5182,  0.9829,  0.1466, -0.5967,  0.0060, -0.0416,  0.1106]],
       dtype=torch.float64)
	q_value: tensor([[-15.3054]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.925223882138029, distance: 0.3129234792931198 entropy 0.6056879162788391
epoch: 7, step: 104
	action: tensor([[-0.5777,  0.0051, -0.2869, -0.3178, -0.1338,  0.8083,  0.9372]],
       dtype=torch.float64)
	q_value: tensor([[-14.6350]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.34570078906630086, distance: 1.3274890481008612 entropy 0.6056879162788391
epoch: 7, step: 105
	action: tensor([[ 0.3930, -0.4513,  0.2984,  0.1845,  1.0471, -0.2289, -0.4424]],
       dtype=torch.float64)
	q_value: tensor([[-16.6334]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29776589623350014, distance: 0.9589537278485154 entropy 0.6056879162788391
epoch: 7, step: 106
	action: tensor([[ 0.0184,  0.1282, -0.2906, -0.0992,  0.2310,  0.2755, -0.5114]],
       dtype=torch.float64)
	q_value: tensor([[-16.1831]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.424983161063459, distance: 0.8677547001853746 entropy 0.6056879162788391
epoch: 7, step: 107
	action: tensor([[0.2683, 0.2392, 1.2836, 0.3057, 0.0648, 0.2138, 0.1273]],
       dtype=torch.float64)
	q_value: tensor([[-10.0161]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8673566286515422, distance: 0.41677289731255796 entropy 0.6056879162788391
epoch: 7, step: 108
	action: tensor([[ 0.5861,  0.0390, -0.4307,  0.6488,  0.2591,  0.0920, -0.5669]],
       dtype=torch.float64)
	q_value: tensor([[-18.2900]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.887783698583454, distance: 0.38334046782598724 entropy 0.6056879162788391
epoch: 7, step: 109
	action: tensor([[ 0.5717,  0.5465, -0.3041,  0.6100, -0.6980, -0.5407, -0.0726]],
       dtype=torch.float64)
	q_value: tensor([[-13.6939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9463799647101834, distance: 0.26498435963168465 entropy 0.6056879162788391
epoch: 7, step: 110
	action: tensor([[ 0.2025,  0.7882,  0.0883,  0.0711,  0.2053, -0.3031,  0.3493]],
       dtype=torch.float64)
	q_value: tensor([[-14.0178]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7765922312305025, distance: 0.5408861099878669 entropy 0.6056879162788391
epoch: 7, step: 111
	action: tensor([[ 0.0998,  0.3076, -0.5230,  0.6022,  0.5352,  0.3031, -0.5693]],
       dtype=torch.float64)
	q_value: tensor([[-12.1853]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.604673509432115, distance: 0.7195063842431709 entropy 0.6056879162788391
epoch: 7, step: 112
	action: tensor([[-0.1165,  0.2051, -0.2047,  0.1113,  0.1617,  0.4360, -0.0892]],
       dtype=torch.float64)
	q_value: tensor([[-12.9636]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4534346152589598, distance: 0.8460143994557217 entropy 0.6056879162788391
epoch: 7, step: 113
	action: tensor([[ 0.2391, -0.3216, -0.8224,  0.6252,  0.1780, -0.2852, -0.3287]],
       dtype=torch.float64)
	q_value: tensor([[-10.1893]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29264089870838095, distance: 0.9624466521545084 entropy 0.6056879162788391
epoch: 7, step: 114
	action: tensor([[ 0.5575,  0.4428, -0.3092, -0.9138,  0.5385, -0.3772, -0.5838]],
       dtype=torch.float64)
	q_value: tensor([[-13.7352]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6034917219408285, distance: 0.7205810265192771 entropy 0.6056879162788391
epoch: 7, step: 115
	action: tensor([[-0.0492, -0.8214, -0.3294, -0.1417,  0.4839, -0.2425, -0.2032]],
       dtype=torch.float64)
	q_value: tensor([[-14.3125]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5789700324128202, distance: 1.4379495096100543 entropy 0.6056879162788391
epoch: 7, step: 116
	action: tensor([[-0.0975,  0.1090,  0.2013,  0.4066,  0.1557,  0.0720, -0.2356]],
       dtype=torch.float64)
	q_value: tensor([[-13.3842]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.523210944557085, distance: 0.7901687175351589 entropy 0.6056879162788391
epoch: 7, step: 117
	action: tensor([[ 0.2402, -0.4880,  0.1231,  0.7405, -0.1810, -0.4644, -0.2337]],
       dtype=torch.float64)
	q_value: tensor([[-11.7939]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4524679822066513, distance: 0.8467621821071712 entropy 0.6056879162788391
epoch: 7, step: 118
	action: tensor([[ 0.8081,  0.5362, -0.5674,  0.5270,  0.3134, -0.4078,  0.8301]],
       dtype=torch.float64)
	q_value: tensor([[-15.6079]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9663023448068484, distance: 0.21006637171315412 entropy 0.6056879162788391
epoch: 7, step: 119
	action: tensor([[ 0.4552,  0.0963,  0.4099,  0.3054, -0.7434, -0.1064, -0.1410]],
       dtype=torch.float64)
	q_value: tensor([[-15.8794]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8312905976660602, distance: 0.4700308211801865 entropy 0.6056879162788391
epoch: 7, step: 120
	action: tensor([[-0.3661,  0.3750, -0.4650, -0.2389,  0.0811,  0.3832,  0.6031]],
       dtype=torch.float64)
	q_value: tensor([[-14.6089]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1750527096747686, distance: 1.0393690812457899 entropy 0.6056879162788391
epoch: 7, step: 121
	action: tensor([[-0.0156, -0.4812, -0.8940, -0.6260,  0.3672,  0.5692,  0.3774]],
       dtype=torch.float64)
	q_value: tensor([[-12.5495]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.012525852098639545, distance: 1.1514888938875256 entropy 0.6056879162788391
epoch: 7, step: 122
	action: tensor([[-0.5041, -0.0315, -0.3262,  0.1770, -0.7148,  0.8050,  0.1214]],
       dtype=torch.float64)
	q_value: tensor([[-14.3426]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.30094785400357127, distance: 1.3052287694160758 entropy 0.6056879162788391
epoch: 7, step: 123
	action: tensor([[-0.1461, -0.1307, -0.4919, -0.3893, -0.2748, -0.8254,  0.6968]],
       dtype=torch.float64)
	q_value: tensor([[-13.9898]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06432259717859268, distance: 1.1069289957365638 entropy 0.6056879162788391
epoch: 7, step: 124
	action: tensor([[ 0.8239, -0.3761,  0.0798,  0.1701,  0.3129, -0.7537,  0.2623]],
       dtype=torch.float64)
	q_value: tensor([[-14.5292]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30713454398414464, distance: 0.9525354508976347 entropy 0.6056879162788391
epoch: 7, step: 125
	action: tensor([[ 0.0411,  0.0383,  0.0164, -0.2065,  0.2217,  0.1794, -0.7064]],
       dtype=torch.float64)
	q_value: tensor([[-16.9964]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34848312167644035, distance: 0.9236757222122123 entropy 0.6056879162788391
epoch: 7, step: 126
	action: tensor([[ 0.5309,  0.4755,  0.0642, -0.6572, -0.0866,  1.1542,  0.1245]],
       dtype=torch.float64)
	q_value: tensor([[-11.1893]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9064882414454126, distance: 0.34993695254606355 entropy 0.6056879162788391
epoch: 7, step: 127
	action: tensor([[ 0.0912,  0.3422, -0.0775,  0.1568, -0.4995,  0.1228, -0.2381]],
       dtype=torch.float64)
	q_value: tensor([[-17.6239]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6482418795647118, distance: 0.6787014222656689 entropy 0.6056879162788391
LOSS epoch 7 actor 108.74330916208437 critic 156.12538350077378 
epoch: 8, step: 0
	action: tensor([[ 0.1132, -0.2246,  0.2395, -0.2554,  0.0439,  0.1443, -0.1268]],
       dtype=torch.float64)
	q_value: tensor([[-12.0847]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11220226350258389, distance: 1.0782357072420703 entropy 0.5003275275230408
epoch: 8, step: 1
	action: tensor([[-0.2058,  0.0258, -0.2826,  0.9717,  0.0108, -0.2700, -0.9787]],
       dtype=torch.float64)
	q_value: tensor([[-12.3026]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3233229200726121, distance: 0.9413419792531799 entropy 0.5003275275230408
epoch: 8, step: 2
	action: tensor([[-0.0291,  0.4207, -0.4470, -0.2273, -0.3325,  0.0627,  0.2322]],
       dtype=torch.float64)
	q_value: tensor([[-16.1604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.535029159921204, distance: 0.780314275284781 entropy 0.5003275275230408
epoch: 8, step: 3
	action: tensor([[-0.3502,  0.2026, -0.0970, -0.4670,  0.3608, -0.1534,  0.2211]],
       dtype=torch.float64)
	q_value: tensor([[-11.6452]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1358667698787459, distance: 1.2196083537751272 entropy 0.5003275275230408
epoch: 8, step: 4
	action: tensor([[ 0.6540, -0.7747,  0.1245,  0.2024, -0.0041, -0.0331, -0.2449]],
       dtype=torch.float64)
	q_value: tensor([[-11.3575]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05276240050645542, distance: 1.1137460006436022 entropy 0.5003275275230408
epoch: 8, step: 5
	action: tensor([[ 0.8754,  0.7601, -0.7190, -0.3700, -0.2934, -0.0683, -0.0381]],
       dtype=torch.float64)
	q_value: tensor([[-16.1141]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9641764122855964, distance: 0.21659141187558575 entropy 0.5003275275230408
epoch: 8, step: 6
	action: tensor([[ 0.3882,  0.7688, -0.5381,  0.4276,  0.0984,  0.5466, -0.2883]],
       dtype=torch.float64)
	q_value: tensor([[-15.1809]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 8, step: 7
	action: tensor([[-0.7461,  0.1536,  0.0538,  0.0431, -0.2543, -0.2674,  0.0634]],
       dtype=torch.float64)
	q_value: tensor([[-17.8099]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.49407015818121947, distance: 1.3987567296246803 entropy 0.5003275275230408
epoch: 8, step: 8
	action: tensor([[ 0.3856, -0.1958, -0.2234, -0.2307,  0.0740,  0.9282, -0.2659]],
       dtype=torch.float64)
	q_value: tensor([[-12.4506]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5752682715353201, distance: 0.7457856897285806 entropy 0.5003275275230408
epoch: 8, step: 9
	action: tensor([[ 0.4910, -0.3398,  0.2815, -0.3128,  0.2379,  0.1188, -0.9430]],
       dtype=torch.float64)
	q_value: tensor([[-14.4674]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1911277314005424, distance: 1.0291926272422576 entropy 0.5003275275230408
epoch: 8, step: 10
	action: tensor([[-0.0264,  0.0562, -0.1382,  0.1194, -0.1616,  0.1383, -0.1928]],
       dtype=torch.float64)
	q_value: tensor([[-15.3020]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4008177586125201, distance: 0.8858009870479445 entropy 0.5003275275230408
epoch: 8, step: 11
	action: tensor([[-0.0325,  0.1627,  0.9927, -0.3543,  0.1334, -0.6261,  0.3631]],
       dtype=torch.float64)
	q_value: tensor([[-10.6757]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05155555327026429, distance: 1.173472195764116 entropy 0.5003275275230408
epoch: 8, step: 12
	action: tensor([[-0.7740,  0.3072, -0.2060, -0.2962,  0.3257,  0.0206,  0.2502]],
       dtype=torch.float64)
	q_value: tensor([[-17.9359]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4991778877685349, distance: 1.4011456319684068 entropy 0.5003275275230408
epoch: 8, step: 13
	action: tensor([[ 0.6618,  0.4252,  0.1217,  0.3279, -0.6081, -0.0115,  0.2484]],
       dtype=torch.float64)
	q_value: tensor([[-12.4453]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9732979738513665, distance: 0.18699448653154896 entropy 0.5003275275230408
epoch: 8, step: 14
	action: tensor([[-0.1293, -0.1772,  0.3271,  0.1651,  0.4218,  0.3471,  0.0576]],
       dtype=torch.float64)
	q_value: tensor([[-14.8670]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2808641994546278, distance: 0.9704253839775291 entropy 0.5003275275230408
epoch: 8, step: 15
	action: tensor([[ 0.4037,  0.0684,  0.0662,  0.0558, -0.3013,  0.0589,  0.8707]],
       dtype=torch.float64)
	q_value: tensor([[-13.5640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7027194357475611, distance: 0.6239358666897616 entropy 0.5003275275230408
epoch: 8, step: 16
	action: tensor([[-0.2493,  0.6426, -0.0726, -0.6407,  0.5251,  0.5439, -0.3041]],
       dtype=torch.float64)
	q_value: tensor([[-14.9207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41046137363108703, distance: 0.8786437534880305 entropy 0.5003275275230408
epoch: 8, step: 17
	action: tensor([[ 0.0822, -0.9207, -0.0096,  0.2617, -0.9895, -0.0362,  0.6620]],
       dtype=torch.float64)
	q_value: tensor([[-12.8758]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.310814863169669, distance: 1.3101691583797963 entropy 0.5003275275230408
epoch: 8, step: 18
	action: tensor([[ 0.2711,  0.4146, -0.3857, -0.4461, -0.1977, -0.3618,  0.6459]],
       dtype=torch.float64)
	q_value: tensor([[-18.6208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6776837467740232, distance: 0.6496774550442 entropy 0.5003275275230408
epoch: 8, step: 19
	action: tensor([[ 0.2479,  1.2660,  0.4156,  0.0439, -0.5151,  0.7271,  0.0989]],
       dtype=torch.float64)
	q_value: tensor([[-13.8930]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 8, step: 20
	action: tensor([[ 0.6418,  0.3040, -0.4044, -0.1913, -0.4221, -0.7812,  0.1792]],
       dtype=torch.float64)
	q_value: tensor([[-17.8099]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7939304426871011, distance: 0.5194737567166163 entropy 0.5003275275230408
epoch: 8, step: 21
	action: tensor([[ 0.2978, -0.3705, -0.4632,  0.4664,  0.2722, -0.1216,  0.1616]],
       dtype=torch.float64)
	q_value: tensor([[-14.8844]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38883239235312983, distance: 0.8946164042112517 entropy 0.5003275275230408
epoch: 8, step: 22
	action: tensor([[ 0.5665, -0.0017, -0.7964, -0.0097, -0.1854, -0.2186, -0.1140]],
       dtype=torch.float64)
	q_value: tensor([[-13.8403]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6796457230157524, distance: 0.6476971059567321 entropy 0.5003275275230408
epoch: 8, step: 23
	action: tensor([[ 0.2560,  0.1204, -0.1547, -0.4497,  0.0411,  0.0927,  0.2201]],
       dtype=torch.float64)
	q_value: tensor([[-12.7499]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.466214583845723, distance: 0.8360650043034662 entropy 0.5003275275230408
epoch: 8, step: 24
	action: tensor([[-0.4422,  0.7078, -0.6933,  0.4815, -0.2406, -0.1928,  0.0685]],
       dtype=torch.float64)
	q_value: tensor([[-12.0345]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 8, step: 25
	action: tensor([[ 0.0081,  0.4487, -0.3836, -0.3811, -0.9898, -0.7517,  0.1349]],
       dtype=torch.float64)
	q_value: tensor([[-17.8099]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7219583278556626, distance: 0.603408799393818 entropy 0.5003275275230408
epoch: 8, step: 26
	action: tensor([[ 0.6794,  0.8341,  0.0236,  0.8282, -0.4518, -0.5619, -0.0372]],
       dtype=torch.float64)
	q_value: tensor([[-15.9228]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 8, step: 27
	action: tensor([[-0.2766,  0.0297, -0.7443,  0.3745,  0.6063,  0.0601, -0.1477]],
       dtype=torch.float64)
	q_value: tensor([[-17.8099]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.007301817906200991, distance: 1.1401587027858415 entropy 0.5003275275230408
epoch: 8, step: 28
	action: tensor([[ 0.2607, -0.3889,  0.3267, -0.6240,  0.0428,  0.9434,  0.0211]],
       dtype=torch.float64)
	q_value: tensor([[-13.1634]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1301221106369158, distance: 1.0672983531163327 entropy 0.5003275275230408
epoch: 8, step: 29
	action: tensor([[ 0.0440,  0.6872,  0.3239, -0.3731,  0.0274,  0.2279,  0.2069]],
       dtype=torch.float64)
	q_value: tensor([[-17.1394]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5966078923528044, distance: 0.7268091594532696 entropy 0.5003275275230408
epoch: 8, step: 30
	action: tensor([[-0.0418,  0.2643, -0.1569,  0.6100,  0.4512,  0.6955, -0.0431]],
       dtype=torch.float64)
	q_value: tensor([[-13.6150]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6996584470871569, distance: 0.6271398593276413 entropy 0.5003275275230408
epoch: 8, step: 31
	action: tensor([[ 0.4941, -0.1346, -0.4233,  0.3838, -0.4354,  0.1527,  0.3368]],
       dtype=torch.float64)
	q_value: tensor([[-13.4579]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7164282128479817, distance: 0.6093800097217469 entropy 0.5003275275230408
epoch: 8, step: 32
	action: tensor([[ 0.6476,  0.3748, -0.5975,  0.6238, -0.0657, -0.3087, -0.1468]],
       dtype=torch.float64)
	q_value: tensor([[-13.1697]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9243611141408516, distance: 0.31472355980232125 entropy 0.5003275275230408
epoch: 8, step: 33
	action: tensor([[-0.1248,  1.0737, -0.6934,  0.3557, -0.1973, -0.1536, -0.0582]],
       dtype=torch.float64)
	q_value: tensor([[-14.1792]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 8, step: 34
	action: tensor([[ 0.3736, -0.2173, -0.1703, -0.2294, -0.0481, -0.1697, -0.1510]],
       dtype=torch.float64)
	q_value: tensor([[-17.8099]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2739364034328128, distance: 0.9750884777514974 entropy 0.5003275275230408
epoch: 8, step: 35
	action: tensor([[-0.9015,  0.5072, -0.5081,  0.1977, -0.0239, -1.0609, -0.2638]],
       dtype=torch.float64)
	q_value: tensor([[-12.0598]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.279088649101886, distance: 1.2942167484865117 entropy 0.5003275275230408
epoch: 8, step: 36
	action: tensor([[ 1.1540, -0.8453, -0.1826, -0.0982,  0.2140, -0.1169, -0.2241]],
       dtype=torch.float64)
	q_value: tensor([[-14.5329]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.32597703458441196, distance: 1.3177247237064562 entropy 0.5003275275230408
epoch: 8, step: 37
	action: tensor([[ 0.4985,  0.2414, -0.3339,  0.0825,  0.6217, -0.6347, -0.0838]],
       dtype=torch.float64)
	q_value: tensor([[-18.1759]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7331149525855376, distance: 0.5911787495255529 entropy 0.5003275275230408
epoch: 8, step: 38
	action: tensor([[ 0.2221,  0.9225,  0.2020,  0.1795,  0.3568, -0.5730,  0.8829]],
       dtype=torch.float64)
	q_value: tensor([[-14.7940]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7450590151812784, distance: 0.577798650042768 entropy 0.5003275275230408
epoch: 8, step: 39
	action: tensor([[ 0.2873,  0.1415, -0.6575, -0.0785, -0.4945, -0.0504,  0.8094]],
       dtype=torch.float64)
	q_value: tensor([[-16.1824]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.623795056684501, distance: 0.7018898177696471 entropy 0.5003275275230408
epoch: 8, step: 40
	action: tensor([[-0.0444, -0.0734,  0.0730,  0.0055,  0.1855,  0.5873, -0.1600]],
       dtype=torch.float64)
	q_value: tensor([[-15.0041]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3964605076353549, distance: 0.8890159235856305 entropy 0.5003275275230408
epoch: 8, step: 41
	action: tensor([[ 0.2948,  0.0857, -0.1673, -0.9956,  0.5120, -0.0357,  0.0793]],
       dtype=torch.float64)
	q_value: tensor([[-12.1180]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17842599119847635, distance: 1.0372418689514569 entropy 0.5003275275230408
epoch: 8, step: 42
	action: tensor([[ 0.0351,  0.4221, -0.2128, -0.0342,  0.2245, -0.0953, -0.5629]],
       dtype=torch.float64)
	q_value: tensor([[-13.7514]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5653757937303927, distance: 0.7544207909715466 entropy 0.5003275275230408
epoch: 8, step: 43
	action: tensor([[-0.0141, -0.2299, -0.2203, -0.0773, -0.0700, -0.1792,  0.0283]],
       dtype=torch.float64)
	q_value: tensor([[-10.9322]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06190533731517278, distance: 1.1083579118929916 entropy 0.5003275275230408
epoch: 8, step: 44
	action: tensor([[ 0.1455, -0.3088, -0.2318, -0.2487, -0.2191, -0.0467,  0.0473]],
       dtype=torch.float64)
	q_value: tensor([[-11.3525]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07329715952822369, distance: 1.10160764288618 entropy 0.5003275275230408
epoch: 8, step: 45
	action: tensor([[ 0.2442, -0.1269, -0.2839,  0.8462,  0.1589, -0.1258, -0.7366]],
       dtype=torch.float64)
	q_value: tensor([[-11.9421]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6486574479553718, distance: 0.6783003935566607 entropy 0.5003275275230408
epoch: 8, step: 46
	action: tensor([[ 0.3425, -0.1984, -1.0306, -0.2216,  0.6518,  0.0189, -0.3019]],
       dtype=torch.float64)
	q_value: tensor([[-15.3846]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44649987624089404, distance: 0.8513645339576109 entropy 0.5003275275230408
epoch: 8, step: 47
	action: tensor([[-0.1080,  0.1629, -0.2428,  0.4468, -0.2555,  0.1958,  0.3284]],
       dtype=torch.float64)
	q_value: tensor([[-13.4652]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42797455322875566, distance: 0.8654946103488962 entropy 0.5003275275230408
epoch: 8, step: 48
	action: tensor([[ 0.1389, -0.0325, -0.4460, -0.3441, -0.2649, -0.7361, -0.0744]],
       dtype=torch.float64)
	q_value: tensor([[-12.1604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34895290026881776, distance: 0.9233426523074278 entropy 0.5003275275230408
epoch: 8, step: 49
	action: tensor([[-0.2033,  0.1924,  0.1047,  0.0710,  0.6016, -0.0781,  0.3231]],
       dtype=torch.float64)
	q_value: tensor([[-13.1588]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2142179932200523, distance: 1.0143964793034594 entropy 0.5003275275230408
epoch: 8, step: 50
	action: tensor([[ 0.2839,  0.6057,  0.4442, -0.0967, -0.0490,  0.8361, -0.4107]],
       dtype=torch.float64)
	q_value: tensor([[-12.9520]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.880057857563696, distance: 0.39631690191626634 entropy 0.5003275275230408
epoch: 8, step: 51
	action: tensor([[-0.0378, -0.0642, -0.7030, -0.0642,  0.4114,  0.3560, -0.0487]],
       dtype=torch.float64)
	q_value: tensor([[-16.1314]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29930182952218276, distance: 0.9579044373012092 entropy 0.5003275275230408
epoch: 8, step: 52
	action: tensor([[ 0.2820,  0.9120, -0.3176,  0.2686, -0.6144,  0.3392,  0.6376]],
       dtype=torch.float64)
	q_value: tensor([[-11.5536]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 8, step: 53
	action: tensor([[ 0.2358, -0.0305,  0.0954,  0.0807,  0.6724,  0.3849, -0.2600]],
       dtype=torch.float64)
	q_value: tensor([[-17.8099]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6417823861337141, distance: 0.6849047238749585 entropy 0.5003275275230408
epoch: 8, step: 54
	action: tensor([[ 0.8932,  0.9094,  0.1371, -0.2192,  0.0123,  0.4504,  0.3230]],
       dtype=torch.float64)
	q_value: tensor([[-12.8468]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 8, step: 55
	action: tensor([[ 0.2729, -0.0141,  0.2363,  0.9045,  0.4509,  0.3080,  0.5831]],
       dtype=torch.float64)
	q_value: tensor([[-17.8099]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9345429298004712, distance: 0.29277566403252303 entropy 0.5003275275230408
epoch: 8, step: 56
	action: tensor([[-0.0687,  0.0768, -0.4669,  0.4978,  0.6356, -0.7199, -0.0716]],
       dtype=torch.float64)
	q_value: tensor([[-16.7786]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17164133259741332, distance: 1.0415158983071837 entropy 0.5003275275230408
epoch: 8, step: 57
	action: tensor([[-0.0488, -1.3668, -0.2614,  0.4272, -0.2179, -0.1832,  0.4588]],
       dtype=torch.float64)
	q_value: tensor([[-14.9704]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6989165087724882, distance: 1.4915668118820467 entropy 0.5003275275230408
epoch: 8, step: 58
	action: tensor([[ 0.5638,  0.3594, -0.6630, -0.0054, -0.4551, -0.0296, -0.4004]],
       dtype=torch.float64)
	q_value: tensor([[-18.3407]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.887190690785259, distance: 0.384352015841682 entropy 0.5003275275230408
epoch: 8, step: 59
	action: tensor([[ 0.9620,  0.0480, -0.1073,  0.1619,  0.6929,  0.4993, -0.1901]],
       dtype=torch.float64)
	q_value: tensor([[-12.7450]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9069132496380767, distance: 0.3491408203372684 entropy 0.5003275275230408
epoch: 8, step: 60
	action: tensor([[-0.3549,  0.7430,  0.1205,  0.9454,  0.0507, -0.0382, -0.3237]],
       dtype=torch.float64)
	q_value: tensor([[-16.0675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 8, step: 61
	action: tensor([[ 0.5342,  0.1422, -1.0175, -0.0883,  0.4550, -0.2438, -0.0480]],
       dtype=torch.float64)
	q_value: tensor([[-17.8099]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.750440840349133, distance: 0.5716674312911365 entropy 0.5003275275230408
epoch: 8, step: 62
	action: tensor([[-0.1166,  0.2282, -0.4290, -0.1368, -0.1273, -0.2851,  0.0645]],
       dtype=torch.float64)
	q_value: tensor([[-13.9526]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3483437932129265, distance: 0.9237744820784415 entropy 0.5003275275230408
epoch: 8, step: 63
	action: tensor([[ 0.1677, -0.1587,  0.6024,  0.1941,  1.1182,  0.2513,  0.0126]],
       dtype=torch.float64)
	q_value: tensor([[-10.6200]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.522751592704686, distance: 0.7905492611731615 entropy 0.5003275275230408
epoch: 8, step: 64
	action: tensor([[-0.1743, -0.2593,  0.2033,  0.0665, -0.3442, -0.4837, -0.1424]],
       dtype=torch.float64)
	q_value: tensor([[-16.6657]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16228101342037093, distance: 1.2337076705712438 entropy 0.5003275275230408
epoch: 8, step: 65
	action: tensor([[-0.0223, -0.1718, -0.4822,  0.0052,  0.0809, -0.3209, -0.0785]],
       dtype=torch.float64)
	q_value: tensor([[-13.4078]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07738330177386088, distance: 1.0991762817947757 entropy 0.5003275275230408
epoch: 8, step: 66
	action: tensor([[ 0.2485,  0.0016,  0.0509, -0.1763, -0.4568,  0.4599,  0.3028]],
       dtype=torch.float64)
	q_value: tensor([[-11.5228]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4840257091132065, distance: 0.8219979315564736 entropy 0.5003275275230408
epoch: 8, step: 67
	action: tensor([[ 0.0822, -0.1438,  0.0804,  0.6221,  0.2675,  0.3610,  0.0656]],
       dtype=torch.float64)
	q_value: tensor([[-14.2794]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6732823936140961, distance: 0.6540982129490314 entropy 0.5003275275230408
epoch: 8, step: 68
	action: tensor([[-0.0875, -0.0802,  0.4180, -0.1514,  0.5674,  0.0602, -0.2358]],
       dtype=torch.float64)
	q_value: tensor([[-13.7937]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0542458554208306, distance: 1.1128735482861718 entropy 0.5003275275230408
epoch: 8, step: 69
	action: tensor([[ 0.4406, -0.4857,  0.2794, -0.5017, -0.3667,  0.0468,  0.2295]],
       dtype=torch.float64)
	q_value: tensor([[-12.8691]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17484679366468403, distance: 1.240358740190008 entropy 0.5003275275230408
epoch: 8, step: 70
	action: tensor([[-0.1743, -0.4599, -0.1718,  0.0617, -0.3515, -0.5597, -0.1690]],
       dtype=torch.float64)
	q_value: tensor([[-15.6938]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2764795414230603, distance: 1.2928960915459324 entropy 0.5003275275230408
epoch: 8, step: 71
	action: tensor([[ 1.1512, -0.2249,  0.2793, -0.3466,  0.6388, -0.3592,  0.3427]],
       dtype=torch.float64)
	q_value: tensor([[-13.9201]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2926491278739174, distance: 0.9624410537562964 entropy 0.5003275275230408
epoch: 8, step: 72
	action: tensor([[ 0.2707,  0.1207, -0.3157, -0.0546, -0.2118,  0.3172,  0.2325]],
       dtype=torch.float64)
	q_value: tensor([[-19.4114]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6490652499245285, distance: 0.6779066289146163 entropy 0.5003275275230408
epoch: 8, step: 73
	action: tensor([[ 0.3583,  0.2562, -0.6613, -0.1681,  0.2926,  0.1996, -0.4908]],
       dtype=torch.float64)
	q_value: tensor([[-11.8689]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7816834366112047, distance: 0.534687506404778 entropy 0.5003275275230408
epoch: 8, step: 74
	action: tensor([[ 0.0470,  0.1310,  0.3198, -0.3461, -0.2047, -0.0279,  0.2156]],
       dtype=torch.float64)
	q_value: tensor([[-11.5499]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24009762982130667, distance: 0.9975521128702581 entropy 0.5003275275230408
epoch: 8, step: 75
	action: tensor([[-0.0327, -0.0955, -0.2307, -0.2853, -0.3654,  0.7552,  0.3938]],
       dtype=torch.float64)
	q_value: tensor([[-12.8575]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18985193971219427, distance: 1.0300039531542595 entropy 0.5003275275230408
epoch: 8, step: 76
	action: tensor([[ 0.1082,  0.0248, -0.8672, -0.7119,  0.2992,  0.2194, -0.7155]],
       dtype=torch.float64)
	q_value: tensor([[-14.8325]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.48562230410731166, distance: 0.8207251795726889 entropy 0.5003275275230408
epoch: 8, step: 77
	action: tensor([[ 0.1763, -0.1713, -0.0432, -0.1902, -0.0086,  0.8577,  0.3155]],
       dtype=torch.float64)
	q_value: tensor([[-12.3981]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.48270998769338347, distance: 0.8230453012801089 entropy 0.5003275275230408
epoch: 8, step: 78
	action: tensor([[ 0.1599,  0.4157, -0.3776, -0.3134, -0.2876,  0.1318,  0.0682]],
       dtype=torch.float64)
	q_value: tensor([[-15.0364]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6729018145336294, distance: 0.6544790673016099 entropy 0.5003275275230408
epoch: 8, step: 79
	action: tensor([[ 0.1529,  0.3056, -0.1982, -0.1411,  0.2505, -0.3616,  0.2466]],
       dtype=torch.float64)
	q_value: tensor([[-11.8938]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5492648225676642, distance: 0.7682762716447132 entropy 0.5003275275230408
epoch: 8, step: 80
	action: tensor([[-0.3734, -0.4438, -0.2263,  0.1724, -0.3846,  0.1626, -0.1900]],
       dtype=torch.float64)
	q_value: tensor([[-11.8726]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.37281177020514544, distance: 1.3407944089614 entropy 0.5003275275230408
epoch: 8, step: 81
	action: tensor([[ 0.3014, -0.2180,  0.1112,  0.2900, -1.1675,  0.0675, -0.0567]],
       dtype=torch.float64)
	q_value: tensor([[-12.4517]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5376798883774346, distance: 0.7780868693709951 entropy 0.5003275275230408
epoch: 8, step: 82
	action: tensor([[ 0.3918,  0.3496, -0.4074,  0.4966, -0.9776,  0.1706,  0.1943]],
       dtype=torch.float64)
	q_value: tensor([[-16.3736]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7729210125142408, distance: 0.5453121419496588 entropy 0.5003275275230408
epoch: 8, step: 83
	action: tensor([[-0.1980, -0.0841,  0.2790,  0.2002,  0.4676, -0.8535, -0.3905]],
       dtype=torch.float64)
	q_value: tensor([[-15.1870]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09836714792999524, distance: 1.1993072543046748 entropy 0.5003275275230408
epoch: 8, step: 84
	action: tensor([[ 1.1353,  0.1910, -0.5049,  0.2796, -0.0426,  0.3231,  0.2717]],
       dtype=torch.float64)
	q_value: tensor([[-15.5512]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9564971428334937, distance: 0.23867975530989644 entropy 0.5003275275230408
epoch: 8, step: 85
	action: tensor([[-0.1099, -0.0938,  0.4293, -0.1877, -0.0258,  0.4932, -0.1703]],
       dtype=torch.float64)
	q_value: tensor([[-16.0680]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1815150666017823, distance: 1.0352900448825069 entropy 0.5003275275230408
epoch: 8, step: 86
	action: tensor([[ 0.2576,  0.3708, -0.6752,  0.1966,  0.3997,  0.1344, -0.7887]],
       dtype=torch.float64)
	q_value: tensor([[-13.6970]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7305201387059082, distance: 0.594045692139091 entropy 0.5003275275230408
epoch: 8, step: 87
	action: tensor([[ 0.3232, -0.9316,  0.1634,  0.1136,  0.1121,  0.8337, -0.2034]],
       dtype=torch.float64)
	q_value: tensor([[-12.6493]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1164333811158359, distance: 1.0756632796914436 entropy 0.5003275275230408
epoch: 8, step: 88
	action: tensor([[ 0.0702, -0.4854, -0.7616,  0.1618,  0.1905, -0.2051,  0.2141]],
       dtype=torch.float64)
	q_value: tensor([[-17.3024]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04101295967017782, distance: 1.1675749300568092 entropy 0.5003275275230408
epoch: 8, step: 89
	action: tensor([[ 0.4496,  0.7878, -0.0784, -0.0860, -0.3376, -0.2524,  0.0710]],
       dtype=torch.float64)
	q_value: tensor([[-13.3438]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9129157342293095, distance: 0.33769648527119994 entropy 0.5003275275230408
epoch: 8, step: 90
	action: tensor([[-0.1215, -0.1765, -0.4865,  0.0142, -0.1910, -0.0581,  0.4841]],
       dtype=torch.float64)
	q_value: tensor([[-13.5415]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.059301242742521465, distance: 1.1098952133432856 entropy 0.5003275275230408
epoch: 8, step: 91
	action: tensor([[ 0.8750,  1.0585,  0.3027, -0.7346, -0.2750,  0.6800,  0.0618]],
       dtype=torch.float64)
	q_value: tensor([[-12.3462]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9632864645735348, distance: 0.219265244356558 entropy 0.5003275275230408
epoch: 8, step: 92
	action: tensor([[ 0.5244, -0.2035, -0.1421, -0.4690, -0.1716,  0.0980,  0.6552]],
       dtype=torch.float64)
	q_value: tensor([[-20.4456]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2627920419666785, distance: 0.9825433047472455 entropy 0.5003275275230408
epoch: 8, step: 93
	action: tensor([[-0.1631,  0.4424, -0.1504,  0.8523, -0.2606,  0.6752, -0.0296]],
       dtype=torch.float64)
	q_value: tensor([[-15.1261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.477950037224696, distance: 0.8268233404119341 entropy 0.5003275275230408
epoch: 8, step: 94
	action: tensor([[ 0.0093,  0.2925, -0.0687, -0.3236, -0.7018, -0.2262,  0.5328]],
       dtype=torch.float64)
	q_value: tensor([[-13.8622]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4249803768290672, distance: 0.8677568010191169 entropy 0.5003275275230408
epoch: 8, step: 95
	action: tensor([[-0.0425, -0.2689,  0.6791,  0.3305,  0.0098,  0.0131, -0.1026]],
       dtype=torch.float64)
	q_value: tensor([[-14.4255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.309304720392088, distance: 0.9510425267157386 entropy 0.5003275275230408
epoch: 8, step: 96
	action: tensor([[ 0.6416,  0.1476,  0.1119, -0.1530, -0.3161, -0.0367, -0.9432]],
       dtype=torch.float64)
	q_value: tensor([[-15.1810]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.735138973686009, distance: 0.5889327721779303 entropy 0.5003275275230408
epoch: 8, step: 97
	action: tensor([[-0.0691, -0.0972,  0.2529,  0.3641, -0.2838, -0.4581,  0.5799]],
       dtype=torch.float64)
	q_value: tensor([[-15.5152]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2991123439177572, distance: 0.9580339487224497 entropy 0.5003275275230408
epoch: 8, step: 98
	action: tensor([[ 0.8790,  0.2733,  0.2298,  0.1293,  0.5069, -0.5544,  0.0576]],
       dtype=torch.float64)
	q_value: tensor([[-15.1448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8992345100749716, distance: 0.36325583879694856 entropy 0.5003275275230408
epoch: 8, step: 99
	action: tensor([[-0.0474,  0.0484, -0.0548,  0.9081,  0.0408,  0.4006,  0.4448]],
       dtype=torch.float64)
	q_value: tensor([[-16.7546]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6559721251700505, distance: 0.6712024137355725 entropy 0.5003275275230408
epoch: 8, step: 100
	action: tensor([[ 0.3551, -0.4105, -0.1520, -0.4096, -0.1951,  0.3026,  0.5227]],
       dtype=torch.float64)
	q_value: tensor([[-14.6228]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04656798414067265, distance: 1.1173817116062839 entropy 0.5003275275230408
epoch: 8, step: 101
	action: tensor([[-0.0071, -1.0140, -0.5894,  0.1519,  0.7166, -0.1590,  0.0122]],
       dtype=torch.float64)
	q_value: tensor([[-14.8269]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6661028811653154, distance: 1.4770921870086136 entropy 0.5003275275230408
epoch: 8, step: 102
	action: tensor([[ 1.3601,  0.8879, -0.0056,  0.3381, -0.0713,  0.3843,  0.7860]],
       dtype=torch.float64)
	q_value: tensor([[-16.2595]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 8, step: 103
	action: tensor([[ 0.5659, -0.2140, -0.1603,  0.1884, -0.6702, -0.5736,  0.4089]],
       dtype=torch.float64)
	q_value: tensor([[-17.8099]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5621124207246937, distance: 0.7572477760792037 entropy 0.5003275275230408
epoch: 8, step: 104
	action: tensor([[ 0.4283, -0.2643, -0.4014,  0.2547, -0.3360, -0.1485, -0.2701]],
       dtype=torch.float64)
	q_value: tensor([[-15.8267]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5262296418050161, distance: 0.7876633458309069 entropy 0.5003275275230408
epoch: 8, step: 105
	action: tensor([[-0.5353,  0.4113,  0.1081,  0.3230, -0.3769,  0.0083,  0.3290]],
       dtype=torch.float64)
	q_value: tensor([[-12.7078]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14412799028901624, distance: 1.0586712153531719 entropy 0.5003275275230408
epoch: 8, step: 106
	action: tensor([[-0.0293,  0.1391, -0.3054, -0.0140,  0.1722,  0.5279, -0.4786]],
       dtype=torch.float64)
	q_value: tensor([[-12.9819]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4692078882345737, distance: 0.8337175110119327 entropy 0.5003275275230408
epoch: 8, step: 107
	action: tensor([[ 0.6811, -0.2002, -0.1021,  0.3500,  0.6822,  0.5509,  0.4702]],
       dtype=torch.float64)
	q_value: tensor([[-10.9633]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8771196245168498, distance: 0.4011418362201055 entropy 0.5003275275230408
epoch: 8, step: 108
	action: tensor([[-0.1603, -0.2965,  0.2715, -0.0326, -0.2836,  0.1552,  0.0874]],
       dtype=torch.float64)
	q_value: tensor([[-16.5093]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08850353736047412, distance: 1.1939100710324064 entropy 0.5003275275230408
epoch: 8, step: 109
	action: tensor([[-1.0882,  0.4306, -0.0299,  0.3690, -0.4218,  0.0041, -0.0980]],
       dtype=torch.float64)
	q_value: tensor([[-13.1283]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4365522089921843, distance: 1.3715681780028275 entropy 0.5003275275230408
epoch: 8, step: 110
	action: tensor([[ 0.2094, -0.6363,  0.0170,  0.1734,  0.0795,  0.6557, -0.0830]],
       dtype=torch.float64)
	q_value: tensor([[-13.8842]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25421475143898675, distance: 0.9882426379923653 entropy 0.5003275275230408
epoch: 8, step: 111
	action: tensor([[ 0.1630,  0.3659, -0.5183, -0.3332, -0.4544, -0.3116, -0.1508]],
       dtype=torch.float64)
	q_value: tensor([[-14.8103]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6623810468106468, distance: 0.6649210817573569 entropy 0.5003275275230408
epoch: 8, step: 112
	action: tensor([[-0.4190, -0.4139, -0.3086, -0.3456,  0.2637,  1.0842, -0.3926]],
       dtype=torch.float64)
	q_value: tensor([[-12.0805]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.19265618791263184, distance: 1.2497246214636033 entropy 0.5003275275230408
epoch: 8, step: 113
	action: tensor([[ 0.2127,  0.3990, -0.0472, -0.4544,  0.6552,  0.5844,  0.0552]],
       dtype=torch.float64)
	q_value: tensor([[-15.4361]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7391035475803301, distance: 0.5845084314071566 entropy 0.5003275275230408
epoch: 8, step: 114
	action: tensor([[-0.2036, -0.1819,  0.4347,  0.1055,  0.0051,  0.0487,  0.3748]],
       dtype=torch.float64)
	q_value: tensor([[-13.2526]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.026787459991114648, distance: 1.1289131747047623 entropy 0.5003275275230408
epoch: 8, step: 115
	action: tensor([[ 0.2313,  0.5532,  0.2605, -0.1660,  0.3683, -0.3292,  0.4071]],
       dtype=torch.float64)
	q_value: tensor([[-14.2953]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6597522275292911, distance: 0.6675047163172066 entropy 0.5003275275230408
epoch: 8, step: 116
	action: tensor([[ 0.3830, -0.3044,  0.1055, -0.0552,  0.0806, -0.1111, -1.1014]],
       dtype=torch.float64)
	q_value: tensor([[-13.7124]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25024785018319884, distance: 0.9908674295991379 entropy 0.5003275275230408
epoch: 8, step: 117
	action: tensor([[-0.7717,  0.6544,  0.0594, -0.0582, -0.7419, -0.0068,  0.3653]],
       dtype=torch.float64)
	q_value: tensor([[-15.3402]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.157186097809777, distance: 1.2310006918436869 entropy 0.5003275275230408
epoch: 8, step: 118
	action: tensor([[-0.5569, -0.0392, -0.4313,  0.0689, -0.1427,  0.3379,  0.4820]],
       dtype=torch.float64)
	q_value: tensor([[-14.9138]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.34705041014325166, distance: 1.328154559420979 entropy 0.5003275275230408
epoch: 8, step: 119
	action: tensor([[ 0.0497,  0.2904,  0.0684,  0.0302, -0.3276,  0.0453,  0.4199]],
       dtype=torch.float64)
	q_value: tensor([[-12.8628]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5267558484564174, distance: 0.7872258038496462 entropy 0.5003275275230408
epoch: 8, step: 120
	action: tensor([[-0.1156, -0.2157, -0.5815,  0.1525,  0.4201, -0.1804, -0.9772]],
       dtype=torch.float64)
	q_value: tensor([[-12.4867]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.008893018856020118, distance: 1.1392445532321023 entropy 0.5003275275230408
epoch: 8, step: 121
	action: tensor([[ 0.6917,  0.1497,  0.2081,  0.3419,  0.1157, -0.4450, -0.0855]],
       dtype=torch.float64)
	q_value: tensor([[-13.7008]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8793902870886319, distance: 0.3974182760654879 entropy 0.5003275275230408
epoch: 8, step: 122
	action: tensor([[ 0.6340,  0.3650, -0.2321, -0.2716, -0.2823, -0.0805, -0.2374]],
       dtype=torch.float64)
	q_value: tensor([[-15.0564]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8365269607972468, distance: 0.46267897170929756 entropy 0.5003275275230408
epoch: 8, step: 123
	action: tensor([[-0.1920,  0.3747, -0.2310,  0.1113,  0.5010, -0.3882, -0.6231]],
       dtype=torch.float64)
	q_value: tensor([[-12.8318]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29092265505599013, distance: 0.9636148811385884 entropy 0.5003275275230408
epoch: 8, step: 124
	action: tensor([[-0.1696, -0.1336, -0.3710,  0.0461, -0.5874, -0.6166,  0.4731]],
       dtype=torch.float64)
	q_value: tensor([[-12.2171]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13015717822396178, distance: 1.0672768397777517 entropy 0.5003275275230408
epoch: 8, step: 125
	action: tensor([[-0.0942, -0.5390,  0.3158,  0.3794,  0.1224, -0.2715, -0.4906]],
       dtype=torch.float64)
	q_value: tensor([[-14.5723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.047093038340261906, distance: 1.170979600526507 entropy 0.5003275275230408
epoch: 8, step: 126
	action: tensor([[-0.5715, -0.0885, -0.5605, -0.0852,  0.2439,  0.1456, -0.5864]],
       dtype=torch.float64)
	q_value: tensor([[-14.9768]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.40631130393332815, distance: 1.35705492993424 entropy 0.5003275275230408
epoch: 8, step: 127
	action: tensor([[-0.0519, -0.6254, -0.6023, -1.0223, -0.0209, -0.2543, -0.0444]],
       dtype=torch.float64)
	q_value: tensor([[-11.6597]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.22669574005836335, distance: 1.2674333245594964 entropy 0.5003275275230408
LOSS epoch 8 actor 145.5308034346828 critic 388.69373051236323 
epoch: 9, step: 0
	action: tensor([[-0.1194,  0.6033,  0.0016, -0.3055,  0.5120, -0.2411,  0.6063]],
       dtype=torch.float64)
	q_value: tensor([[-15.3190]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3728022559940838, distance: 0.9062727819531142 entropy 0.5003275275230408
epoch: 9, step: 1
	action: tensor([[ 0.2275, -0.2529,  0.8201, -0.6473,  0.2090,  0.4962,  0.1989]],
       dtype=torch.float64)
	q_value: tensor([[-14.8144]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.02650405552555013, distance: 1.129077535383039 entropy 0.5003275275230408
epoch: 9, step: 2
	action: tensor([[-0.2813,  0.0218, -0.0342,  0.1808,  0.7150,  0.4498,  0.1521]],
       dtype=torch.float64)
	q_value: tensor([[-18.1989]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2713346580400767, distance: 0.9768339607545053 entropy 0.5003275275230408
epoch: 9, step: 3
	action: tensor([[ 0.1165, -0.3472,  0.0075, -0.0837,  0.2800, -0.1818,  0.4207]],
       dtype=torch.float64)
	q_value: tensor([[-14.2587]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.004464374779801661, distance: 1.1468958002201328 entropy 0.5003275275230408
epoch: 9, step: 4
	action: tensor([[ 0.8559,  0.3446, -0.7093, -0.3560,  0.2026,  0.2405, -1.0188]],
       dtype=torch.float64)
	q_value: tensor([[-14.5309]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.874774883683435, distance: 0.4049509434805444 entropy 0.5003275275230408
epoch: 9, step: 5
	action: tensor([[ 0.3180,  0.4677,  0.3698,  0.5157, -0.0130, -0.6873,  0.3442]],
       dtype=torch.float64)
	q_value: tensor([[-16.7094]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8764587433461206, distance: 0.4022191099007506 entropy 0.5003275275230408
epoch: 9, step: 6
	action: tensor([[ 0.3618, -0.1760, -0.1623, -0.2958,  0.6433, -0.2610,  0.0088]],
       dtype=torch.float64)
	q_value: tensor([[-17.0098]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21208064412717775, distance: 1.015775135878367 entropy 0.5003275275230408
epoch: 9, step: 7
	action: tensor([[ 0.8159, -0.2841, -0.0703, -0.2849, -0.5374,  0.2938,  0.1626]],
       dtype=torch.float64)
	q_value: tensor([[-14.4395]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3793469303698459, distance: 0.9015320008941358 entropy 0.5003275275230408
epoch: 9, step: 8
	action: tensor([[ 0.1566, -0.6141, -0.1792,  0.0070,  0.8324,  0.5493,  0.7959]],
       dtype=torch.float64)
	q_value: tensor([[-17.1316]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14064489132511837, distance: 1.0608232383906568 entropy 0.5003275275230408
epoch: 9, step: 9
	action: tensor([[-0.3514,  0.0469, -0.3168,  0.3426,  0.0271, -0.1977,  0.2780]],
       dtype=torch.float64)
	q_value: tensor([[-19.4176]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.01736823358167039, distance: 1.1343631062917316 entropy 0.5003275275230408
epoch: 9, step: 10
	action: tensor([[-0.2559, -0.6110, -0.5324,  0.1300,  0.3771,  0.0393,  0.3708]],
       dtype=torch.float64)
	q_value: tensor([[-12.6997]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4513634975037002, distance: 1.378620687972142 entropy 0.5003275275230408
epoch: 9, step: 11
	action: tensor([[ 5.4620e-01,  2.9026e-01,  8.2383e-02,  2.2555e-02,  1.3940e-01,
          2.0077e-01, -3.2816e-04]], dtype=torch.float64)
	q_value: tensor([[-15.1623]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8910528364225409, distance: 0.3777153697692484 entropy 0.5003275275230408
epoch: 9, step: 12
	action: tensor([[-0.0370, -0.2926, -0.9125,  0.3851,  0.5631, -0.1982, -0.7279]],
       dtype=torch.float64)
	q_value: tensor([[-13.4404]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.024091000046321698, distance: 1.1580464191502018 entropy 0.5003275275230408
epoch: 9, step: 13
	action: tensor([[-0.0947,  0.3111, -0.8600, -0.4804,  0.4066, -0.2486,  0.3221]],
       dtype=torch.float64)
	q_value: tensor([[-15.7988]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4795873434628575, distance: 0.8255257384295082 entropy 0.5003275275230408
epoch: 9, step: 14
	action: tensor([[-1.0600,  1.0796, -1.0163,  0.0621, -0.8615, -0.3292,  0.4064]],
       dtype=torch.float64)
	q_value: tensor([[-13.3891]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 9, step: 15
	action: tensor([[-0.0333,  0.1478,  0.2222, -0.0238,  0.5477,  0.6391,  0.1492]],
       dtype=torch.float64)
	q_value: tensor([[-19.0617]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5785824403133305, distance: 0.7428703199117781 entropy 0.5003275275230408
epoch: 9, step: 16
	action: tensor([[-0.0029,  0.3032,  0.3360,  0.0259,  0.2977,  0.0707,  0.5966]],
       dtype=torch.float64)
	q_value: tensor([[-14.2822]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.527424694867711, distance: 0.7866693053447181 entropy 0.5003275275230408
epoch: 9, step: 17
	action: tensor([[-0.4949,  0.7878, -0.1881,  0.3644,  0.5782, -0.1350,  0.0976]],
       dtype=torch.float64)
	q_value: tensor([[-15.0397]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 9, step: 18
	action: tensor([[ 0.0728, -0.1661, -0.1335,  0.8559, -0.3746, -0.5164,  0.3440]],
       dtype=torch.float64)
	q_value: tensor([[-19.0617]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5451406027511272, distance: 0.7717831259243877 entropy 0.5003275275230408
epoch: 9, step: 19
	action: tensor([[ 0.6029,  0.2183, -0.6804, -0.1134, -0.3407,  0.2425, -0.2941]],
       dtype=torch.float64)
	q_value: tensor([[-16.7623]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8235175453208959, distance: 0.48073690276875186 entropy 0.5003275275230408
epoch: 9, step: 20
	action: tensor([[-0.2758, -0.0287,  0.2730,  0.6486,  0.0497, -0.2222, -0.0265]],
       dtype=torch.float64)
	q_value: tensor([[-13.8086]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.339348813582238, distance: 0.9301281811617699 entropy 0.5003275275230408
epoch: 9, step: 21
	action: tensor([[-0.5015,  0.1392,  0.5077,  0.4895, -0.1542, -0.1653,  0.2294]],
       dtype=torch.float64)
	q_value: tensor([[-14.8659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14778830894498518, distance: 1.0564049734479244 entropy 0.5003275275230408
epoch: 9, step: 22
	action: tensor([[-0.2426,  0.3138, -0.5346,  0.0148, -0.0263, -0.1615, -0.0881]],
       dtype=torch.float64)
	q_value: tensor([[-15.7398]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25556217489236366, distance: 0.9873494966340015 entropy 0.5003275275230408
epoch: 9, step: 23
	action: tensor([[ 0.4287,  0.4656,  0.0639,  0.1397, -0.4631,  0.2046, -0.6495]],
       dtype=torch.float64)
	q_value: tensor([[-10.8360]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8783716746338851, distance: 0.3990929507952589 entropy 0.5003275275230408
epoch: 9, step: 24
	action: tensor([[ 0.2469,  0.6497, -0.2304, -0.0162,  0.2520, -0.1124,  0.3084]],
       dtype=torch.float64)
	q_value: tensor([[-15.2762]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7950888777786517, distance: 0.5180115690088016 entropy 0.5003275275230408
epoch: 9, step: 25
	action: tensor([[-0.3494, -0.6109, -0.6814,  0.3103,  0.9470,  0.5550,  0.1096]],
       dtype=torch.float64)
	q_value: tensor([[-12.9869]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3218441472981246, distance: 1.3156695380376713 entropy 0.5003275275230408
epoch: 9, step: 26
	action: tensor([[-0.1397,  0.7492,  0.0246, -0.0562, -0.2599, -0.5063,  0.8068]],
       dtype=torch.float64)
	q_value: tensor([[-17.8671]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5086358198954201, distance: 0.802155268255814 entropy 0.5003275275230408
epoch: 9, step: 27
	action: tensor([[-0.1224,  0.2207, -0.7473, -0.3335, -0.1556,  0.0525,  0.0745]],
       dtype=torch.float64)
	q_value: tensor([[-15.7136]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3368367734648301, distance: 0.9318948490255853 entropy 0.5003275275230408
epoch: 9, step: 28
	action: tensor([[ 0.0578,  0.2858, -0.6016,  0.5081, -0.4666,  0.6217, -0.0229]],
       dtype=torch.float64)
	q_value: tensor([[-12.0816]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3793487097511108, distance: 0.9015307085696631 entropy 0.5003275275230408
epoch: 9, step: 29
	action: tensor([[-0.1172, -0.3304, -0.0869, -0.2068,  0.5641,  0.1903, -0.4084]],
       dtype=torch.float64)
	q_value: tensor([[-14.1993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13081410770725954, distance: 1.2168927460439283 entropy 0.5003275275230408
epoch: 9, step: 30
	action: tensor([[ 0.5641,  0.5433,  0.6685, -0.9032,  0.0501, -0.4538, -0.1928]],
       dtype=torch.float64)
	q_value: tensor([[-12.7607]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5190568509143174, distance: 0.7936034819392335 entropy 0.5003275275230408
epoch: 9, step: 31
	action: tensor([[-1.0198, -0.5775, -0.2455,  0.6360, -0.0876,  0.0808, -0.4975]],
       dtype=torch.float64)
	q_value: tensor([[-18.8859]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -1.0700159644638343, distance: 1.646431022905886 entropy 0.5003275275230408
epoch: 9, step: 32
	action: tensor([[ 0.6428, -0.8231,  0.3333,  0.0551, -0.4699,  0.1108, -0.3071]],
       dtype=torch.float64)
	q_value: tensor([[-17.0817]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08458244151131455, distance: 1.1917577310372953 entropy 0.5003275275230408
epoch: 9, step: 33
	action: tensor([[ 0.2320, -0.1775,  0.2810,  0.4822, -0.1323, -0.1291, -0.0037]],
       dtype=torch.float64)
	q_value: tensor([[-18.1749]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6204722396997319, distance: 0.7049827119292702 entropy 0.5003275275230408
epoch: 9, step: 34
	action: tensor([[-0.4366, -0.0582, -0.0727, -0.1460,  0.0195,  0.0990,  1.5067]],
       dtype=torch.float64)
	q_value: tensor([[-14.8240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.30162586396740343, distance: 1.3055688456287804 entropy 0.5003275275230408
epoch: 9, step: 35
	action: tensor([[ 1.0082, -0.1384, -0.3649, -0.2438,  0.1050,  0.0592, -0.1783]],
       dtype=torch.float64)
	q_value: tensor([[-21.5531]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4805412823168742, distance: 0.824768779236664 entropy 0.5003275275230408
epoch: 9, step: 36
	action: tensor([[-0.4625,  0.3355, -0.1113, -0.1845,  0.0524, -0.0516,  0.0644]],
       dtype=torch.float64)
	q_value: tensor([[-16.0440]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05060201758024019, distance: 1.1729400311091103 entropy 0.5003275275230408
epoch: 9, step: 37
	action: tensor([[ 0.2734, -0.4877,  0.0714, -0.6663, -0.3278, -0.1093,  0.1267]],
       dtype=torch.float64)
	q_value: tensor([[-11.6699]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3598521906505734, distance: 1.3344507370467755 entropy 0.5003275275230408
epoch: 9, step: 38
	action: tensor([[ 0.4211, -0.5929, -0.7950, -0.1322, -0.4394, -0.5685, -0.1822]],
       dtype=torch.float64)
	q_value: tensor([[-15.9034]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04259134813101839, distance: 1.119709510908111 entropy 0.5003275275230408
epoch: 9, step: 39
	action: tensor([[ 0.6164,  0.0094, -0.3000, -0.3782,  0.1541, -0.2795,  0.1558]],
       dtype=torch.float64)
	q_value: tensor([[-16.3286]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4845755441126329, distance: 0.8215598441263301 entropy 0.5003275275230408
epoch: 9, step: 40
	action: tensor([[ 0.1323, -0.0921,  0.1847,  0.1280, -0.2597,  0.1708,  0.4467]],
       dtype=torch.float64)
	q_value: tensor([[-14.6680]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.47567744135628776, distance: 0.8286210560702717 entropy 0.5003275275230408
epoch: 9, step: 41
	action: tensor([[-0.4161, -0.2615, -0.2841, -0.2883, -0.0196,  0.0462,  0.0082]],
       dtype=torch.float64)
	q_value: tensor([[-14.4834]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.41524050474411056, distance: 1.3613563396783723 entropy 0.5003275275230408
epoch: 9, step: 42
	action: tensor([[ 0.4778, -0.2885, -0.0109, -0.1925,  0.2906,  0.2395,  0.2627]],
       dtype=torch.float64)
	q_value: tensor([[-12.2855]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3719402253860138, distance: 0.9068953660054756 entropy 0.5003275275230408
epoch: 9, step: 43
	action: tensor([[ 0.8808,  0.2826, -0.5662,  0.0358, -0.1225, -0.6264, -0.7125]],
       dtype=torch.float64)
	q_value: tensor([[-14.5865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8367640688495712, distance: 0.4623433055976925 entropy 0.5003275275230408
epoch: 9, step: 44
	action: tensor([[ 0.5512,  0.5331, -0.2642, -0.4403,  0.2719, -0.3906,  0.0091]],
       dtype=torch.float64)
	q_value: tensor([[-16.9415]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8294669527524299, distance: 0.4725643651660494 entropy 0.5003275275230408
epoch: 9, step: 45
	action: tensor([[ 0.1308,  0.7369, -0.6712,  0.4173, -0.4402, -0.5936, -0.1291]],
       dtype=torch.float64)
	q_value: tensor([[-14.5200]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.741330368939263, distance: 0.5820086175318073 entropy 0.5003275275230408
epoch: 9, step: 46
	action: tensor([[-0.5290,  0.0404, -0.1256,  0.3250,  0.3510, -0.2673,  0.0902]],
       dtype=torch.float64)
	q_value: tensor([[-14.2967]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23195318887140837, distance: 1.2701464428966136 entropy 0.5003275275230408
epoch: 9, step: 47
	action: tensor([[ 0.5693,  0.1294, -0.2745,  0.5348,  0.4600, -0.7785, -0.2006]],
       dtype=torch.float64)
	q_value: tensor([[-13.4785]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7899651942922478, distance: 0.5244478724819871 entropy 0.5003275275230408
epoch: 9, step: 48
	action: tensor([[ 0.5556,  1.5256, -0.3186, -0.0676, -0.5845,  0.4494, -0.3773]],
       dtype=torch.float64)
	q_value: tensor([[-17.6437]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 9, step: 49
	action: tensor([[-0.3639, -0.8497, -0.3680, -0.1563, -0.8508,  0.0121, -0.1357]],
       dtype=torch.float64)
	q_value: tensor([[-19.0617]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.773196174346322, distance: 1.5238249709622402 entropy 0.5003275275230408
epoch: 9, step: 50
	action: tensor([[-0.3141,  0.2363, -0.4946, -0.3225,  0.1161, -0.2345,  0.1680]],
       dtype=torch.float64)
	q_value: tensor([[-16.7117]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07580069339296613, distance: 1.100118612585117 entropy 0.5003275275230408
epoch: 9, step: 51
	action: tensor([[-0.1912, -0.5334, -0.1976, -0.1697,  0.4096,  0.0734, -0.2896]],
       dtype=torch.float64)
	q_value: tensor([[-11.5283]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.39264413340873694, distance: 1.3504445927788018 entropy 0.5003275275230408
epoch: 9, step: 52
	action: tensor([[-0.2269,  0.0559,  0.3490, -0.2144, -0.3950,  1.5868,  0.1222]],
       dtype=torch.float64)
	q_value: tensor([[-13.1279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4095556643392336, distance: 0.8793184254815958 entropy 0.5003275275230408
epoch: 9, step: 53
	action: tensor([[-0.5331, -0.1274, -0.6799, -0.0834, -0.6620,  0.1567, -0.1508]],
       dtype=torch.float64)
	q_value: tensor([[-22.8277]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4523178146231248, distance: 1.37907385670905 entropy 0.5003275275230408
epoch: 9, step: 54
	action: tensor([[-0.1322, -0.3489,  0.2157,  0.6039, -0.0670, -0.2186,  0.4300]],
       dtype=torch.float64)
	q_value: tensor([[-13.3561]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21942603759505153, distance: 1.0110292573958073 entropy 0.5003275275230408
epoch: 9, step: 55
	action: tensor([[-0.3621,  0.0312, -0.3188,  0.2516, -0.1351, -0.3001,  0.0724]],
       dtype=torch.float64)
	q_value: tensor([[-16.3075]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.047683670220112884, distance: 1.1713098101425534 entropy 0.5003275275230408
epoch: 9, step: 56
	action: tensor([[-0.4544, -0.2367, -0.3264, -0.5817,  0.5744, -0.3550,  0.0514]],
       dtype=torch.float64)
	q_value: tensor([[-12.2270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5277764894338597, distance: 1.4144467578177713 entropy 0.5003275275230408
epoch: 9, step: 57
	action: tensor([[-0.0809,  0.2979, -0.5257, -0.2830, -0.3115, -0.5885, -0.6220]],
       dtype=torch.float64)
	q_value: tensor([[-13.9708]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.45072350911314196, distance: 0.848110029437145 entropy 0.5003275275230408
epoch: 9, step: 58
	action: tensor([[ 0.0417, -0.3920,  0.0069, -0.0548, -0.1840,  0.3979,  0.8971]],
       dtype=torch.float64)
	q_value: tensor([[-13.5386]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07619465139602632, distance: 1.0998841140711864 entropy 0.5003275275230408
epoch: 9, step: 59
	action: tensor([[ 0.5292, -0.6474, -0.6363, -0.3180,  0.1070, -0.3461, -0.0992]],
       dtype=torch.float64)
	q_value: tensor([[-17.4494]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.19433869684275829, distance: 1.250605819166208 entropy 0.5003275275230408
epoch: 9, step: 60
	action: tensor([[ 0.0075, -0.2301, -0.4897, -0.4613, -0.4091,  0.3396,  0.0573]],
       dtype=torch.float64)
	q_value: tensor([[-15.6232]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.006144465854705183, distance: 1.140823144734498 entropy 0.5003275275230408
epoch: 9, step: 61
	action: tensor([[ 0.8752, -0.4971, -0.1841,  0.6772, -0.5037, -0.2442, -0.6049]],
       dtype=torch.float64)
	q_value: tensor([[-13.8455]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6424482336455366, distance: 0.6842678844145756 entropy 0.5003275275230408
epoch: 9, step: 62
	action: tensor([[ 0.5634, -0.4168, -0.5180,  0.0605, -0.1075, -0.5731, -0.5813]],
       dtype=torch.float64)
	q_value: tensor([[-19.3556]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26484142703462443, distance: 0.9811766544511098 entropy 0.5003275275230408
epoch: 9, step: 63
	action: tensor([[ 0.2730,  0.2297, -0.1519, -0.1183,  0.5862,  0.0965,  0.6638]],
       dtype=torch.float64)
	q_value: tensor([[-16.1425]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.66674293570637, distance: 0.6606118737708793 entropy 0.5003275275230408
epoch: 9, step: 64
	action: tensor([[ 0.2142, -0.1963, -0.0801, -0.1627, -0.3621, -0.0009,  0.0770]],
       dtype=torch.float64)
	q_value: tensor([[-15.3186]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24206418164386623, distance: 0.9962604938127717 entropy 0.5003275275230408
epoch: 9, step: 65
	action: tensor([[ 1.0950e-01,  1.9128e-01, -2.9933e-05,  5.6711e-01, -2.3832e-01,
          1.1220e-03,  3.3041e-01]], dtype=torch.float64)
	q_value: tensor([[-13.2271]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7040467720695325, distance: 0.6225413940255627 entropy 0.5003275275230408
epoch: 9, step: 66
	action: tensor([[ 0.5661,  0.5600,  0.0457, -0.5042,  0.2035, -0.6379, -0.9009]],
       dtype=torch.float64)
	q_value: tensor([[-14.1184]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7567925337344138, distance: 0.5643456020211973 entropy 0.5003275275230408
epoch: 9, step: 67
	action: tensor([[ 0.2544,  0.0460, -0.2459, -0.0286,  0.0722,  0.5830,  0.1283]],
       dtype=torch.float64)
	q_value: tensor([[-17.2874]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6649622087680283, distance: 0.6623744802026049 entropy 0.5003275275230408
epoch: 9, step: 68
	action: tensor([[-0.0025, -0.1898, -0.2348, -0.3951,  0.1892, -0.3609,  0.0692]],
       dtype=torch.float64)
	q_value: tensor([[-13.0765]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06085851016674915, distance: 1.1786515335051992 entropy 0.5003275275230408
epoch: 9, step: 69
	action: tensor([[-0.3117,  0.3210,  0.4628, -0.3646, -0.6672, -0.0633, -0.1511]],
       dtype=torch.float64)
	q_value: tensor([[-12.7115]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05039561407189286, distance: 1.1728248063020774 entropy 0.5003275275230408
epoch: 9, step: 70
	action: tensor([[-0.3768,  0.3112, -0.4037, -0.2644, -0.1806, -0.7648,  0.3534]],
       dtype=torch.float64)
	q_value: tensor([[-16.1664]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13376511487317644, distance: 1.0650611127616971 entropy 0.5003275275230408
epoch: 9, step: 71
	action: tensor([[0.2645, 0.2478, 0.3831, 0.6349, 0.7964, 0.0779, 0.1156]],
       dtype=torch.float64)
	q_value: tensor([[-13.8252]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.915858816116452, distance: 0.33194107981893145 entropy 0.5003275275230408
epoch: 9, step: 72
	action: tensor([[-0.1279,  0.3881, -0.5068, -0.3720,  0.3455,  0.2139,  0.8781]],
       dtype=torch.float64)
	q_value: tensor([[-16.5113]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42746020175068056, distance: 0.8658836388513856 entropy 0.5003275275230408
epoch: 9, step: 73
	action: tensor([[ 0.3750,  0.7125, -0.0211, -0.0010, -0.1063, -0.1735,  0.8139]],
       dtype=torch.float64)
	q_value: tensor([[-16.1413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8646658928016553, distance: 0.4209788955434658 entropy 0.5003275275230408
epoch: 9, step: 74
	action: tensor([[-0.2811,  0.0636, -0.1729,  0.0976, -0.7694,  0.0932, -0.4758]],
       dtype=torch.float64)
	q_value: tensor([[-15.9028]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07146808209458377, distance: 1.1026942546530336 entropy 0.5003275275230408
epoch: 9, step: 75
	action: tensor([[-0.3350,  0.2169, -0.6400, -0.6047, -0.5201, -0.4689, -0.3445]],
       dtype=torch.float64)
	q_value: tensor([[-13.6220]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26755682067827957, distance: 0.9793629330917543 entropy 0.5003275275230408
epoch: 9, step: 76
	action: tensor([[ 0.4735,  0.1724,  0.6460,  0.2590,  0.3362, -0.1719, -0.4188]],
       dtype=torch.float64)
	q_value: tensor([[-13.8476]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.847833503436729, distance: 0.44639180604067047 entropy 0.5003275275230408
epoch: 9, step: 77
	action: tensor([[ 0.3230, -0.6498, -0.2760,  0.5366,  0.2600, -0.0030, -0.1226]],
       dtype=torch.float64)
	q_value: tensor([[-16.5967]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26561265494430697, distance: 0.9806618611727353 entropy 0.5003275275230408
epoch: 9, step: 78
	action: tensor([[-0.1401,  0.3738,  0.3634, -0.7525,  0.6004, -0.4141,  0.2892]],
       dtype=torch.float64)
	q_value: tensor([[-15.8832]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12694608626701864, distance: 1.2148097338056836 entropy 0.5003275275230408
epoch: 9, step: 79
	action: tensor([[-0.1186, -0.2615, -0.2786, -0.5017, -0.1506,  0.3557, -0.0326]],
       dtype=torch.float64)
	q_value: tensor([[-15.5442]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1789002593922533, distance: 1.2424966455059814 entropy 0.5003275275230408
epoch: 9, step: 80
	action: tensor([[-0.4663, -0.1739, -0.6946, -0.4448, -0.4404,  0.2379, -0.5540]],
       dtype=torch.float64)
	q_value: tensor([[-13.1204]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3625176558709804, distance: 1.335757934535051 entropy 0.5003275275230408
epoch: 9, step: 81
	action: tensor([[ 0.1455, -0.4457, -0.2407,  0.7952, -0.0754,  0.0613,  0.0166]],
       dtype=torch.float64)
	q_value: tensor([[-13.0547]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.413573887330855, distance: 0.8763212509485676 entropy 0.5003275275230408
epoch: 9, step: 82
	action: tensor([[-0.1244,  0.1168, -0.7970,  0.0572, -0.0049,  0.3966,  0.0180]],
       dtype=torch.float64)
	q_value: tensor([[-15.4738]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24187878368678617, distance: 0.9963823335266848 entropy 0.5003275275230408
epoch: 9, step: 83
	action: tensor([[-0.0786,  0.6083,  0.2074,  0.2408, -0.0400, -0.3628,  0.5030]],
       dtype=torch.float64)
	q_value: tensor([[-11.9766]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5649872935762341, distance: 0.7547578949817613 entropy 0.5003275275230408
epoch: 9, step: 84
	action: tensor([[-0.2810, -0.4298,  0.2307, -0.4018,  0.2219,  0.5384,  0.2715]],
       dtype=torch.float64)
	q_value: tensor([[-14.3168]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.39724944765791603, distance: 1.3526756324598965 entropy 0.5003275275230408
epoch: 9, step: 85
	action: tensor([[-0.4560,  0.0669, -0.3547, -0.5238,  0.3700, -0.8825, -0.2444]],
       dtype=torch.float64)
	q_value: tensor([[-15.9030]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3182086021888926, distance: 1.3138590109204138 entropy 0.5003275275230408
epoch: 9, step: 86
	action: tensor([[ 0.2104,  0.1491, -0.1159,  0.3176, -0.4103,  0.3677, -0.0934]],
       dtype=torch.float64)
	q_value: tensor([[-14.6826]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6664091911527417, distance: 0.6609425801033789 entropy 0.5003275275230408
epoch: 9, step: 87
	action: tensor([[-0.8521,  0.0414, -0.4935,  0.1253,  0.1253,  0.2146,  0.4396]],
       dtype=torch.float64)
	q_value: tensor([[-13.0458]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6702558415821391, distance: 1.4789319556052924 entropy 0.5003275275230408
epoch: 9, step: 88
	action: tensor([[ 0.9211, -0.0113, -0.5054,  0.5133, -0.0870,  0.7270, -0.3492]],
       dtype=torch.float64)
	q_value: tensor([[-14.6593]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9576630780228643, distance: 0.23545956287246678 entropy 0.5003275275230408
epoch: 9, step: 89
	action: tensor([[-0.1557,  0.3102, -0.0675, -0.0557, -0.4848, -0.6389, -0.2564]],
       dtype=torch.float64)
	q_value: tensor([[-17.3963]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37420460669302746, distance: 0.9052590478600931 entropy 0.5003275275230408
epoch: 9, step: 90
	action: tensor([[ 0.4218,  0.3379, -0.3332, -0.3639, -1.0083, -0.0239,  0.2776]],
       dtype=torch.float64)
	q_value: tensor([[-13.6661]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7602734107196036, distance: 0.5602924837639066 entropy 0.5003275275230408
epoch: 9, step: 91
	action: tensor([[ 0.1985, -0.1386, -0.0930, -0.4063,  0.5079, -0.7006, -0.2560]],
       dtype=torch.float64)
	q_value: tensor([[-17.4146]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0021902981088633977, distance: 1.143090339484256 entropy 0.5003275275230408
epoch: 9, step: 92
	action: tensor([[-0.0684,  0.3716,  0.4367,  0.4117,  0.2961,  0.3020, -0.0056]],
       dtype=torch.float64)
	q_value: tensor([[-15.0799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.745751405337178, distance: 0.5770134995152447 entropy 0.5003275275230408
epoch: 9, step: 93
	action: tensor([[ 0.1296, -0.7535, -0.6330, -0.6655,  0.3868, -0.3369,  0.6142]],
       dtype=torch.float64)
	q_value: tensor([[-14.5605]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.43147980052610424, distance: 1.36914456046251 entropy 0.5003275275230408
epoch: 9, step: 94
	action: tensor([[ 0.3813,  0.1483, -0.2801, -0.8123, -0.5697,  0.1688,  0.4592]],
       dtype=torch.float64)
	q_value: tensor([[-17.5130]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42902250880344495, distance: 0.8647014501007185 entropy 0.5003275275230408
epoch: 9, step: 95
	action: tensor([[ 0.0896, -0.1622, -0.2391,  0.6875,  0.5712,  0.5292,  1.0793]],
       dtype=torch.float64)
	q_value: tensor([[-17.4762]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6382918170238994, distance: 0.68823358245015 entropy 0.5003275275230408
epoch: 9, step: 96
	action: tensor([[ 0.5584,  0.0023,  0.1481,  0.3089, -0.0777,  0.0108, -0.2949]],
       dtype=torch.float64)
	q_value: tensor([[-19.5681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8249058163953578, distance: 0.4788423495370608 entropy 0.5003275275230408
epoch: 9, step: 97
	action: tensor([[ 0.8299,  0.3697, -0.3098, -0.0918,  0.2447,  0.3734, -0.4074]],
       dtype=torch.float64)
	q_value: tensor([[-14.4852]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.962817778678518, distance: 0.22066037864489563 entropy 0.5003275275230408
epoch: 9, step: 98
	action: tensor([[ 0.0492,  0.2683,  0.0845, -0.3597, -0.1944, -0.1703,  0.0285]],
       dtype=torch.float64)
	q_value: tensor([[-15.0733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36470937454675023, distance: 0.9121009680652516 entropy 0.5003275275230408
epoch: 9, step: 99
	action: tensor([[ 0.3858, -0.0387, -0.1123,  0.4214, -0.2097,  0.3212, -0.0326]],
       dtype=torch.float64)
	q_value: tensor([[-12.8760]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7814227940253577, distance: 0.5350065860972496 entropy 0.5003275275230408
epoch: 9, step: 100
	action: tensor([[-0.4149,  0.1647,  0.0777,  0.2055,  0.3895, -0.3081, -0.4099]],
       dtype=torch.float64)
	q_value: tensor([[-13.2712]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03868360546927696, distance: 1.1662679248776673 entropy 0.5003275275230408
epoch: 9, step: 101
	action: tensor([[-0.3923, -0.0710, -0.1700,  0.2303,  0.1001, -0.1413, -0.3590]],
       dtype=torch.float64)
	q_value: tensor([[-13.0431]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12659764627728087, distance: 1.2146219160296245 entropy 0.5003275275230408
epoch: 9, step: 102
	action: tensor([[-0.3698, -0.4584, -0.2478,  0.1926,  0.1119, -0.2996,  0.2449]],
       dtype=torch.float64)
	q_value: tensor([[-12.3633]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4501237003698697, distance: 1.3780317331201017 entropy 0.5003275275230408
epoch: 9, step: 103
	action: tensor([[ 0.4618, -0.0410, -0.4212, -0.6477,  0.3338, -0.4368,  0.1514]],
       dtype=torch.float64)
	q_value: tensor([[-14.4885]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2512030193689291, distance: 0.9902360558400854 entropy 0.5003275275230408
epoch: 9, step: 104
	action: tensor([[ 0.0646, -0.5489,  0.3111, -0.2415, -0.5315,  0.5119, -0.2937]],
       dtype=torch.float64)
	q_value: tensor([[-14.9762]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13045861941932535, distance: 1.216701456812942 entropy 0.5003275275230408
epoch: 9, step: 105
	action: tensor([[ 0.5277,  0.0010, -0.4100,  0.3466,  0.2444,  0.2146, -0.1622]],
       dtype=torch.float64)
	q_value: tensor([[-16.6872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7985989322639531, distance: 0.5135557280749836 entropy 0.5003275275230408
epoch: 9, step: 106
	action: tensor([[-0.3059, -0.4197,  0.5070, -0.0957, -0.0937,  0.2098,  0.5850]],
       dtype=torch.float64)
	q_value: tensor([[-13.3818]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3743358702200694, distance: 1.3415384796203516 entropy 0.5003275275230408
epoch: 9, step: 107
	action: tensor([[ 0.2902,  0.3512,  0.0940, -0.2275, -0.2883,  0.2072,  0.2844]],
       dtype=torch.float64)
	q_value: tensor([[-17.6643]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6857966193959517, distance: 0.6414489816328502 entropy 0.5003275275230408
epoch: 9, step: 108
	action: tensor([[-0.2306, -0.5379, -0.2283, -0.0662, -0.5858, -0.2480,  0.3987]],
       dtype=torch.float64)
	q_value: tensor([[-14.2264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.38122053322257776, distance: 1.3448944650654773 entropy 0.5003275275230408
epoch: 9, step: 109
	action: tensor([[ 0.6415, -0.2620, -0.2747, -0.0585,  0.5194, -0.6744, -0.2691]],
       dtype=torch.float64)
	q_value: tensor([[-15.7720]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2780982007286965, distance: 0.9722898574766328 entropy 0.5003275275230408
epoch: 9, step: 110
	action: tensor([[ 0.3525,  0.2620,  0.4704, -0.0478, -0.2578, -0.4463, -0.7976]],
       dtype=torch.float64)
	q_value: tensor([[-17.0305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.651989050650129, distance: 0.675076744505838 entropy 0.5003275275230408
epoch: 9, step: 111
	action: tensor([[ 0.2444, -0.1869, -0.6264,  0.2127,  0.7914, -0.2742,  0.2465]],
       dtype=torch.float64)
	q_value: tensor([[-16.6513]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33053115513032516, distance: 0.9363147797319475 entropy 0.5003275275230408
epoch: 9, step: 112
	action: tensor([[-0.0940,  0.0570,  0.2174, -0.4297,  0.5349, -0.1543, -0.2273]],
       dtype=torch.float64)
	q_value: tensor([[-16.1598]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04775532427267204, distance: 1.1713498640548632 entropy 0.5003275275230408
epoch: 9, step: 113
	action: tensor([[ 0.7814,  0.6617, -0.3481, -0.0683, -0.4188, -0.4402,  0.4508]],
       dtype=torch.float64)
	q_value: tensor([[-12.7955]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9835926178801127, distance: 0.14658054753870967 entropy 0.5003275275230408
epoch: 9, step: 114
	action: tensor([[ 0.1317, -0.5912, -0.1731, -0.0740,  0.3699, -0.8892,  0.3383]],
       dtype=torch.float64)
	q_value: tensor([[-16.5593]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3303180768087117, distance: 1.3198799738816287 entropy 0.5003275275230408
epoch: 9, step: 115
	action: tensor([[-0.3434,  0.2646, -0.7385,  0.1557,  0.2396,  0.0232, -0.0700]],
       dtype=torch.float64)
	q_value: tensor([[-18.6118]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09550435864003548, distance: 1.088328325334905 entropy 0.5003275275230408
epoch: 9, step: 116
	action: tensor([[-0.0369,  0.2953,  0.1876, -0.0860,  0.4193, -0.1494,  0.2945]],
       dtype=torch.float64)
	q_value: tensor([[-11.7930]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36453913162171536, distance: 0.9122231706676058 entropy 0.5003275275230408
epoch: 9, step: 117
	action: tensor([[ 0.1506,  0.7971,  0.4323,  0.3434, -0.1940, -0.2454, -0.1993]],
       dtype=torch.float64)
	q_value: tensor([[-13.5900]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.5003275275230408
epoch: 9, step: 118
	action: tensor([[ 0.3881, -0.3230, -0.2892,  0.4126,  0.1884,  0.0365,  0.0410]],
       dtype=torch.float64)
	q_value: tensor([[-19.0617]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5266700064288461, distance: 0.7872971982818394 entropy 0.5003275275230408
epoch: 9, step: 119
	action: tensor([[-0.1306, -0.2621, -0.2836, -0.0120,  0.4428,  0.1059, -0.0709]],
       dtype=torch.float64)
	q_value: tensor([[-14.0296]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.010474831002253993, distance: 1.1503220470217976 entropy 0.5003275275230408
epoch: 9, step: 120
	action: tensor([[ 0.2681, -0.1649, -0.8572, -0.0684,  0.0669,  0.8835,  0.1940]],
       dtype=torch.float64)
	q_value: tensor([[-12.2436]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.49346523907226447, distance: 0.8144441728003283 entropy 0.5003275275230408
epoch: 9, step: 121
	action: tensor([[-0.8429,  0.3833,  0.4143, -0.5686,  0.2194, -0.1902, -0.4893]],
       dtype=torch.float64)
	q_value: tensor([[-15.7551]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8514440853166136, distance: 1.5570838261399498 entropy 0.5003275275230408
epoch: 9, step: 122
	action: tensor([[ 0.2241, -0.2609,  0.2966, -0.4410,  0.0476, -0.0215,  0.5214]],
       dtype=torch.float64)
	q_value: tensor([[-14.4559]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.02890413601383801, distance: 1.1607645863276121 entropy 0.5003275275230408
epoch: 9, step: 123
	action: tensor([[ 0.6933,  0.0659,  0.3477,  0.0938,  0.2172,  0.2451, -0.0320]],
       dtype=torch.float64)
	q_value: tensor([[-15.9221]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8690122309538952, distance: 0.4141637325538693 entropy 0.5003275275230408
epoch: 9, step: 124
	action: tensor([[-0.2058, -1.0959,  0.2160,  0.4002,  0.8162, -0.4929,  0.6463]],
       dtype=torch.float64)
	q_value: tensor([[-15.5372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7091434226440714, distance: 1.4960494447489225 entropy 0.5003275275230408
epoch: 9, step: 125
	action: tensor([[ 0.5744,  0.2888, -0.2319, -0.3941,  0.1090,  0.3859, -0.3420]],
       dtype=torch.float64)
	q_value: tensor([[-23.2314]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8124523497490146, distance: 0.4955785571664387 entropy 0.5003275275230408
epoch: 9, step: 126
	action: tensor([[ 0.3507,  0.3113,  0.0191, -0.0544,  0.2264, -0.5996,  0.0781]],
       dtype=torch.float64)
	q_value: tensor([[-13.8131]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.657364731557918, distance: 0.6698425404787134 entropy 0.5003275275230408
epoch: 9, step: 127
	action: tensor([[ 0.1311, -0.1767,  0.0734, -0.4036,  0.1110,  0.3849, -0.3527]],
       dtype=torch.float64)
	q_value: tensor([[-14.4343]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18729787089246974, distance: 1.0316262682518773 entropy 0.5003275275230408
LOSS epoch 9 actor 136.89501750391187 critic 218.48385171803426 
epoch: 10, step: 0
	action: tensor([[-0.0520, -0.3609, -0.5390,  0.3860,  0.7647,  0.1816,  0.4673]],
       dtype=torch.float64)
	q_value: tensor([[-13.8343]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09104628696117201, distance: 1.091007101061 entropy 0.3949669301509857
epoch: 10, step: 1
	action: tensor([[-0.1077,  0.0968, -0.4525, -0.4733, -0.2079,  0.2341, -0.4963]],
       dtype=torch.float64)
	q_value: tensor([[-17.7368]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21335069136439622, distance: 1.014956141722608 entropy 0.3949669301509857
epoch: 10, step: 2
	action: tensor([[-0.2749, -0.8105,  0.0200,  0.1983,  0.2761, -0.4499,  0.4014]],
       dtype=torch.float64)
	q_value: tensor([[-13.0323]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6729943017049604, distance: 1.480143847042942 entropy 0.3949669301509857
epoch: 10, step: 3
	action: tensor([[ 0.3856,  0.5248,  0.3310,  0.3060,  0.1073, -0.1304,  0.1195]],
       dtype=torch.float64)
	q_value: tensor([[-19.3935]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9110756239182343, distance: 0.3412456363960667 entropy 0.3949669301509857
epoch: 10, step: 4
	action: tensor([[ 0.4044,  0.5545,  0.2425, -0.4608, -0.4958, -0.1808, -0.0268]],
       dtype=torch.float64)
	q_value: tensor([[-15.6074]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7531803326414774, distance: 0.5685210829159234 entropy 0.3949669301509857
epoch: 10, step: 5
	action: tensor([[ 0.5475, -0.1319,  0.4738,  0.5233, -0.1006,  0.3313, -0.2135]],
       dtype=torch.float64)
	q_value: tensor([[-17.3335]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9081550900395565, distance: 0.3468041131524131 entropy 0.3949669301509857
epoch: 10, step: 6
	action: tensor([[ 0.6066, -0.3217, -0.5629, -0.0264, -0.4645,  0.6046,  0.2029]],
       dtype=torch.float64)
	q_value: tensor([[-17.9706]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5031268176499921, distance: 0.8066394757195365 entropy 0.3949669301509857
epoch: 10, step: 7
	action: tensor([[-0.3873, -0.0121, -0.1949, -0.2883, -0.1461, -0.1470, -0.0287]],
       dtype=torch.float64)
	q_value: tensor([[-17.0336]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.22139088467777435, distance: 1.2646898426008737 entropy 0.3949669301509857
epoch: 10, step: 8
	action: tensor([[-0.6514,  0.6837,  0.0090,  0.5478, -0.1011, -0.2416, -0.2457]],
       dtype=torch.float64)
	q_value: tensor([[-12.5411]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 10, step: 9
	action: tensor([[ 0.0537,  0.0444, -0.6911, -0.0751, -0.3715, -0.2055,  0.0607]],
       dtype=torch.float64)
	q_value: tensor([[-20.3723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.43426026143377694, distance: 0.8607262260595852 entropy 0.3949669301509857
epoch: 10, step: 10
	action: tensor([[ 0.6272,  0.3667, -0.5902, -0.2702, -0.0939,  0.1270,  0.1217]],
       dtype=torch.float64)
	q_value: tensor([[-13.5860]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8875295406142923, distance: 0.3837743349618647 entropy 0.3949669301509857
epoch: 10, step: 11
	action: tensor([[ 0.4847, -0.1816, -0.2203,  0.0440,  0.2131, -0.1708, -0.1815]],
       dtype=torch.float64)
	q_value: tensor([[-15.3229]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5003614717899894, distance: 0.8088810357089796 entropy 0.3949669301509857
epoch: 10, step: 12
	action: tensor([[-0.5038, -0.6006, -0.0155, -0.3216,  0.1500, -0.3970,  0.0054]],
       dtype=torch.float64)
	q_value: tensor([[-14.3917]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8473142218538177, distance: 1.555346227451744 entropy 0.3949669301509857
epoch: 10, step: 13
	action: tensor([[ 0.3239, -0.1420, -0.0405,  0.9825,  0.0963, -0.0286,  0.0087]],
       dtype=torch.float64)
	q_value: tensor([[-16.5485]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8224158387815825, distance: 0.4822350885059275 entropy 0.3949669301509857
epoch: 10, step: 14
	action: tensor([[ 0.3120, -0.3930, -0.1539,  0.1544,  0.1621, -0.3257, -0.2361]],
       dtype=torch.float64)
	q_value: tensor([[-17.7394]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19856925324114127, distance: 1.0244474677708995 entropy 0.3949669301509857
epoch: 10, step: 15
	action: tensor([[ 0.5619,  0.0915,  0.1526, -0.0417, -0.4041,  0.2872, -0.0951]],
       dtype=torch.float64)
	q_value: tensor([[-15.1627]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7764030459345066, distance: 0.5411150770586256 entropy 0.3949669301509857
epoch: 10, step: 16
	action: tensor([[ 0.6800, -0.2458, -0.4734, -0.3482, -0.2917, -0.1704,  0.2675]],
       dtype=torch.float64)
	q_value: tensor([[-16.1138]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29567354007765445, distance: 0.960381300463222 entropy 0.3949669301509857
epoch: 10, step: 17
	action: tensor([[-0.5690,  0.0015,  0.0600,  0.4237, -0.4892, -0.5220,  0.0638]],
       dtype=torch.float64)
	q_value: tensor([[-16.7249]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13070826885939835, distance: 1.2168357970108756 entropy 0.3949669301509857
epoch: 10, step: 18
	action: tensor([[ 0.5176, -0.1718, -0.3451,  0.0539, -0.3487,  0.0044, -0.6754]],
       dtype=torch.float64)
	q_value: tensor([[-16.0347]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5638023606597032, distance: 0.7557851400095945 entropy 0.3949669301509857
epoch: 10, step: 19
	action: tensor([[ 0.0247,  0.3049, -0.5843, -0.2947,  1.0036,  0.0897, -0.5017]],
       dtype=torch.float64)
	q_value: tensor([[-15.1066]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5629354506684202, distance: 0.7565358002770255 entropy 0.3949669301509857
epoch: 10, step: 20
	action: tensor([[-0.1534, -0.0642, -0.4401, -0.3629,  0.4212, -0.2116,  0.3584]],
       dtype=torch.float64)
	q_value: tensor([[-15.3812]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0035395300039586797, distance: 1.1463676854934084 entropy 0.3949669301509857
epoch: 10, step: 21
	action: tensor([[-0.0788, -0.2565,  0.2927, -0.1651, -1.0656,  0.3705, -0.3258]],
       dtype=torch.float64)
	q_value: tensor([[-14.1317]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06800612751634505, distance: 1.1826154963741187 entropy 0.3949669301509857
epoch: 10, step: 22
	action: tensor([[-0.2369, -0.3988,  0.0546, -0.1694, -0.3199,  0.2352, -0.1879]],
       dtype=torch.float64)
	q_value: tensor([[-18.9929]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.30611248124575363, distance: 1.3078170142098697 entropy 0.3949669301509857
epoch: 10, step: 23
	action: tensor([[ 0.1721, -0.2262,  0.2958, -0.5871,  0.2223, -0.0840, -0.3395]],
       dtype=torch.float64)
	q_value: tensor([[-14.2731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13938865699478087, distance: 1.221497659227244 entropy 0.3949669301509857
epoch: 10, step: 24
	action: tensor([[-0.1987, -0.6674, -0.2461, -0.0513, -0.0504,  0.0703,  0.4488]],
       dtype=torch.float64)
	q_value: tensor([[-14.7863]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4673820874386745, distance: 1.3862076776634695 entropy 0.3949669301509857
epoch: 10, step: 25
	action: tensor([[-0.0104,  0.1148,  0.1503, -0.5651,  0.0443, -0.4413, -0.2541]],
       dtype=torch.float64)
	q_value: tensor([[-16.1360]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.004627843108911511, distance: 1.1416932605127224 entropy 0.3949669301509857
epoch: 10, step: 26
	action: tensor([[-0.1164, -0.4059, -0.2676,  0.1490, -0.2052,  0.5071, -0.7077]],
       dtype=torch.float64)
	q_value: tensor([[-14.2846]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0352244959218595, distance: 1.1643243046579983 entropy 0.3949669301509857
epoch: 10, step: 27
	action: tensor([[-0.3917, -0.0352, -0.2357, -0.2526,  0.4188, -0.8894,  0.3053]],
       dtype=torch.float64)
	q_value: tensor([[-14.5735]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.33339660393447, distance: 1.3214062773827955 entropy 0.3949669301509857
epoch: 10, step: 28
	action: tensor([[-0.1350, -0.0093, -0.1776, -0.0297,  0.5425,  0.1005,  0.1466]],
       dtype=torch.float64)
	q_value: tensor([[-17.0542]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17840655254240578, distance: 1.0372541395859187 entropy 0.3949669301509857
epoch: 10, step: 29
	action: tensor([[ 0.3517,  0.1655, -0.3537, -0.7147, -0.2733,  0.2828, -0.3040]],
       dtype=torch.float64)
	q_value: tensor([[-13.6121]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4994474770085935, distance: 0.8096205455786342 entropy 0.3949669301509857
epoch: 10, step: 30
	action: tensor([[-0.3268, -0.3024, -1.0235, -0.1126,  0.6371, -0.4644,  0.6975]],
       dtype=torch.float64)
	q_value: tensor([[-15.3767]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.25452874627048017, distance: 1.2817313351383524 entropy 0.3949669301509857
epoch: 10, step: 31
	action: tensor([[ 0.5605, -0.3691, -0.3199, -0.0595, -0.4830,  0.6114,  0.0251]],
       dtype=torch.float64)
	q_value: tensor([[-18.9994]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4431042857019123, distance: 0.8539719996642103 entropy 0.3949669301509857
epoch: 10, step: 32
	action: tensor([[-0.2504, -0.0179, -0.1442,  0.2274, -0.2524,  0.4666,  0.1522]],
       dtype=torch.float64)
	q_value: tensor([[-16.8899]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18138492905531867, distance: 1.035372346184455 entropy 0.3949669301509857
epoch: 10, step: 33
	action: tensor([[ 0.0632, -0.3636, -0.2428, -0.3980, -0.2035,  0.1097,  0.0588]],
       dtype=torch.float64)
	q_value: tensor([[-14.0321]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08594152262219623, distance: 1.1925041880469351 entropy 0.3949669301509857
epoch: 10, step: 34
	action: tensor([[ 0.0821,  0.6401, -0.5419, -0.3043,  0.2725,  0.3526, -0.2409]],
       dtype=torch.float64)
	q_value: tensor([[-14.2557]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.714696864066225, distance: 0.6112374650127611 entropy 0.3949669301509857
epoch: 10, step: 35
	action: tensor([[ 0.0963,  1.2259, -0.3825,  0.0384,  0.1830, -0.2707, -0.2715]],
       dtype=torch.float64)
	q_value: tensor([[-13.5030]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.3949669301509857
epoch: 10, step: 36
	action: tensor([[ 0.3737, -0.5204, -0.3783, -0.0808,  0.4769,  0.0201, -0.4351]],
       dtype=torch.float64)
	q_value: tensor([[-20.3723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09008791951843986, distance: 1.0915821084617376 entropy 0.3949669301509857
epoch: 10, step: 37
	action: tensor([[-0.1801, -0.0913, -0.4786,  0.3890,  0.3404,  0.4104, -0.1063]],
       dtype=torch.float64)
	q_value: tensor([[-15.2388]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21894553507836434, distance: 1.0113403921421626 entropy 0.3949669301509857
epoch: 10, step: 38
	action: tensor([[ 0.0949, -0.0960, -0.3523, -0.0888, -0.0316, -0.0222, -0.2012]],
       dtype=torch.float64)
	q_value: tensor([[-13.8788]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30892973532264945, distance: 0.9513006567244159 entropy 0.3949669301509857
epoch: 10, step: 39
	action: tensor([[ 0.1976, -0.1661,  0.0314, -0.2832, -0.0836,  0.4235,  0.4939]],
       dtype=torch.float64)
	q_value: tensor([[-12.1639]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30593050932481447, distance: 0.9533627311915748 entropy 0.3949669301509857
epoch: 10, step: 40
	action: tensor([[ 0.6542,  0.0340, -0.9200,  0.2979,  0.4862, -0.8122,  0.3977]],
       dtype=torch.float64)
	q_value: tensor([[-16.4081]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.669602152075703, distance: 0.6577718722802786 entropy 0.3949669301509857
epoch: 10, step: 41
	action: tensor([[ 0.3261,  0.0188,  0.0455, -0.1203,  0.0678,  0.3349,  0.0529]],
       dtype=torch.float64)
	q_value: tensor([[-21.1600]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6204843891629248, distance: 0.704971427866608 entropy 0.3949669301509857
epoch: 10, step: 42
	action: tensor([[-0.5599, -0.2381,  0.1983, -0.3664,  0.3378,  0.0813,  0.1927]],
       dtype=torch.float64)
	q_value: tensor([[-13.9487]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7089387251124499, distance: 1.4959598540234629 entropy 0.3949669301509857
epoch: 10, step: 43
	action: tensor([[ 0.3638,  0.6711, -0.7246, -0.0403, -0.3055, -0.0384,  0.5023]],
       dtype=torch.float64)
	q_value: tensor([[-15.5918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8672995433378208, distance: 0.41686257029101936 entropy 0.3949669301509857
epoch: 10, step: 44
	action: tensor([[-0.2871,  0.2364, -0.0822,  0.4494, -0.3012,  0.1815, -0.0256]],
       dtype=torch.float64)
	q_value: tensor([[-16.4445]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3352136995185494, distance: 0.9330345457464261 entropy 0.3949669301509857
epoch: 10, step: 45
	action: tensor([[ 0.2969, -0.0740, -0.2284, -0.0408, -0.2324, -0.4300, -0.2324]],
       dtype=torch.float64)
	q_value: tensor([[-13.5646]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4490757249636418, distance: 0.8493812068477938 entropy 0.3949669301509857
epoch: 10, step: 46
	action: tensor([[-0.2638,  0.0506, -0.1778,  0.0497,  0.2258, -0.4462,  0.7941]],
       dtype=torch.float64)
	q_value: tensor([[-14.1662]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.024379403415544898, distance: 1.1582094715440856 entropy 0.3949669301509857
epoch: 10, step: 47
	action: tensor([[ 0.2480, -0.1002, -0.5693,  0.6080, -0.2252, -0.6040,  0.1424]],
       dtype=torch.float64)
	q_value: tensor([[-16.6662]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.528754060110662, distance: 0.785562066666075 entropy 0.3949669301509857
epoch: 10, step: 48
	action: tensor([[-0.5298, -0.2357, -0.7367, -0.2123,  0.1337, -0.0478,  0.5215]],
       dtype=torch.float64)
	q_value: tensor([[-17.1157]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4347221145295628, distance: 1.3706942455218831 entropy 0.3949669301509857
epoch: 10, step: 49
	action: tensor([[-0.1377,  0.2258, -0.7451, -0.2678,  0.2861, -0.2736, -0.1536]],
       dtype=torch.float64)
	q_value: tensor([[-15.6331]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3200423076857537, distance: 0.9436210901683454 entropy 0.3949669301509857
epoch: 10, step: 50
	action: tensor([[ 0.2115, -1.3903, -0.6770, -0.3570,  0.1575, -0.0106,  0.2244]],
       dtype=torch.float64)
	q_value: tensor([[-12.8875]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7569179005761095, distance: 1.5168143444945212 entropy 0.3949669301509857
epoch: 10, step: 51
	action: tensor([[ 0.3759,  0.6852,  0.2456, -0.5865,  0.6206, -0.1206, -0.0059]],
       dtype=torch.float64)
	q_value: tensor([[-18.8839]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7281477023716386, distance: 0.5966548802494573 entropy 0.3949669301509857
epoch: 10, step: 52
	action: tensor([[ 0.1991,  0.3222,  0.0191,  0.6281,  0.5274,  0.1633, -0.0373]],
       dtype=torch.float64)
	q_value: tensor([[-16.1927]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8276148792372122, distance: 0.47512357710372144 entropy 0.3949669301509857
epoch: 10, step: 53
	action: tensor([[ 0.2849, -0.0279,  0.2428, -0.3150,  0.2731, -0.7693, -0.1670]],
       dtype=torch.float64)
	q_value: tensor([[-15.5617]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14011655959456404, distance: 1.0611492854778375 entropy 0.3949669301509857
epoch: 10, step: 54
	action: tensor([[ 0.6126,  0.0060,  0.2389,  0.4327, -0.0402,  0.4303, -0.3844]],
       dtype=torch.float64)
	q_value: tensor([[-16.7832]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9491781898754545, distance: 0.25797745730397903 entropy 0.3949669301509857
epoch: 10, step: 55
	action: tensor([[-0.0185, -1.2008,  0.0311, -0.2254, -0.2264, -0.2974,  0.1406]],
       dtype=torch.float64)
	q_value: tensor([[-16.8979]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.917115477683067, distance: 1.58445835888757 entropy 0.3949669301509857
epoch: 10, step: 56
	action: tensor([[ 0.1159,  0.4533,  0.2554, -0.3280, -0.1296,  0.0800, -0.2990]],
       dtype=torch.float64)
	q_value: tensor([[-20.1348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5521328795824265, distance: 0.7658280751794005 entropy 0.3949669301509857
epoch: 10, step: 57
	action: tensor([[ 0.8041, -0.3998,  0.1487, -0.1026, -0.1416,  0.1664,  0.8843]],
       dtype=torch.float64)
	q_value: tensor([[-14.7071]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3814430470016976, distance: 0.9000083522271315 entropy 0.3949669301509857
epoch: 10, step: 58
	action: tensor([[ 0.2065, -0.0463, -0.2251,  0.1486, -0.6934, -0.2755, -0.6745]],
       dtype=torch.float64)
	q_value: tensor([[-20.7155]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5091208968147953, distance: 0.8017592248509212 entropy 0.3949669301509857
epoch: 10, step: 59
	action: tensor([[-0.1176, -0.1003,  0.0863, -0.7363,  0.2903,  0.2129,  0.1731]],
       dtype=torch.float64)
	q_value: tensor([[-15.5901]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20203164352535197, distance: 1.2546270407923517 entropy 0.3949669301509857
epoch: 10, step: 60
	action: tensor([[-0.3544, -0.1215, -0.5400,  0.1700, -0.2850, -0.3681, -0.2761]],
       dtype=torch.float64)
	q_value: tensor([[-14.9476]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13029974632551666, distance: 1.2166159570372712 entropy 0.3949669301509857
epoch: 10, step: 61
	action: tensor([[-0.1543,  0.3836, -0.0663, -0.5132, -0.5796,  0.0663, -0.2343]],
       dtype=torch.float64)
	q_value: tensor([[-13.8212]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23683836971452166, distance: 0.999689099910459 entropy 0.3949669301509857
epoch: 10, step: 62
	action: tensor([[ 0.0650, -0.5781, -0.4356, -0.5301, -0.1505,  0.6573,  0.3054]],
       dtype=torch.float64)
	q_value: tensor([[-15.6920]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.19871055366559442, distance: 1.252892639202654 entropy 0.3949669301509857
epoch: 10, step: 63
	action: tensor([[ 0.4271,  0.1064, -0.3279, -0.1688,  0.1797, -0.3613,  0.3407]],
       dtype=torch.float64)
	q_value: tensor([[-17.3345]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.574278720190204, distance: 0.7466539600200653 entropy 0.3949669301509857
epoch: 10, step: 64
	action: tensor([[-0.1620,  0.5576,  0.0301, -0.6628, -0.1617,  0.2848,  0.1789]],
       dtype=torch.float64)
	q_value: tensor([[-15.2119]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29872096956090644, distance: 0.9583013935754475 entropy 0.3949669301509857
epoch: 10, step: 65
	action: tensor([[-0.3940,  0.2363, -0.9002, -0.2441,  0.5279,  0.0118, -0.2271]],
       dtype=torch.float64)
	q_value: tensor([[-15.8102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11197182728533983, distance: 1.0783756312510686 entropy 0.3949669301509857
epoch: 10, step: 66
	action: tensor([[-0.1127, -0.1735, -0.4079, -0.3863, -0.4769,  0.0709,  0.3183]],
       dtype=torch.float64)
	q_value: tensor([[-13.6841]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.022959845999969586, distance: 1.1574066855662681 entropy 0.3949669301509857
epoch: 10, step: 67
	action: tensor([[ 0.3352, -0.2414,  0.0849,  0.1864,  0.1301,  0.1112, -0.0311]],
       dtype=torch.float64)
	q_value: tensor([[-14.8500]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5159050036620219, distance: 0.7961996643101569 entropy 0.3949669301509857
epoch: 10, step: 68
	action: tensor([[ 0.0352,  0.2261, -0.2701,  0.0444, -0.5190,  0.5908,  0.7308]],
       dtype=torch.float64)
	q_value: tensor([[-14.7223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4779770749849044, distance: 0.826801928916608 entropy 0.3949669301509857
epoch: 10, step: 69
	action: tensor([[-0.8194, -0.0462,  0.2861, -0.4095,  0.0179,  0.2899,  0.1243]],
       dtype=torch.float64)
	q_value: tensor([[-17.5112]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8962355953001622, distance: 1.5758063296987845 entropy 0.3949669301509857
epoch: 10, step: 70
	action: tensor([[ 0.3259, -0.1477, -0.2161, -0.4674, -0.5373,  0.5049, -0.1563]],
       dtype=torch.float64)
	q_value: tensor([[-16.5496]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25909551884064363, distance: 0.9850035688309371 entropy 0.3949669301509857
epoch: 10, step: 71
	action: tensor([[ 0.3231,  0.1531, -0.0342, -0.5126, -0.2946,  0.3116,  0.4706]],
       dtype=torch.float64)
	q_value: tensor([[-16.4542]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4928172898894708, distance: 0.8149649167087557 entropy 0.3949669301509857
epoch: 10, step: 72
	action: tensor([[-0.2415,  0.2106,  0.1087,  0.0137,  0.3478,  0.3758, -0.2630]],
       dtype=torch.float64)
	q_value: tensor([[-17.0121]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3096803825546697, distance: 0.9507838603435806 entropy 0.3949669301509857
epoch: 10, step: 73
	action: tensor([[-0.2602,  0.2557,  0.3698, -0.5833, -0.2495, -0.0104,  0.2970]],
       dtype=torch.float64)
	q_value: tensor([[-13.1513]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17013250569400418, distance: 1.2378676553661438 entropy 0.3949669301509857
epoch: 10, step: 74
	action: tensor([[-0.8115,  0.5803, -0.4886, -0.0464,  0.8221,  0.1260, -0.2300]],
       dtype=torch.float64)
	q_value: tensor([[-16.0930]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.19938309855088154, distance: 1.2532440619779477 entropy 0.3949669301509857
epoch: 10, step: 75
	action: tensor([[-0.2576, -0.1980,  0.4664,  0.2112, -0.1982,  0.3250,  0.2905]],
       dtype=torch.float64)
	q_value: tensor([[-14.8568]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12095136294394626, distance: 1.072909635837381 entropy 0.3949669301509857
epoch: 10, step: 76
	action: tensor([[ 0.5498,  0.1157, -0.0214, -0.5304,  0.3011, -0.3204, -0.4117]],
       dtype=torch.float64)
	q_value: tensor([[-16.7086]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42730926681453785, distance: 0.8659977649609554 entropy 0.3949669301509857
epoch: 10, step: 77
	action: tensor([[ 0.1616, -0.3487, -0.3903,  0.3929,  0.1862,  0.1501, -0.5055]],
       dtype=torch.float64)
	q_value: tensor([[-15.6465]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3175603986972285, distance: 0.9453416740153128 entropy 0.3949669301509857
epoch: 10, step: 78
	action: tensor([[ 0.2433, -0.1225, -0.0878, -0.4006,  0.3913, -0.3999,  0.1809]],
       dtype=torch.float64)
	q_value: tensor([[-14.5471]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0949208022781608, distance: 1.088679348869197 entropy 0.3949669301509857
epoch: 10, step: 79
	action: tensor([[-0.3072, -0.1800,  0.2331,  0.5589, -0.3304,  0.4353,  0.5334]],
       dtype=torch.float64)
	q_value: tensor([[-15.1754]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3008416893505169, distance: 0.9568513092174322 entropy 0.3949669301509857
epoch: 10, step: 80
	action: tensor([[ 0.2998, -0.2317, -0.6871, -0.4198,  0.5323,  0.3138, -0.2708]],
       dtype=torch.float64)
	q_value: tensor([[-17.9917]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.328690111513832, distance: 0.937601331656985 entropy 0.3949669301509857
epoch: 10, step: 81
	action: tensor([[-0.5601, -0.5678, -0.2340, -0.1345,  0.0112, -0.0316,  0.2638]],
       dtype=torch.float64)
	q_value: tensor([[-14.4811]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7902559090466901, distance: 1.531137704537533 entropy 0.3949669301509857
epoch: 10, step: 82
	action: tensor([[-0.0310, -0.0367, -0.1505,  0.3256,  0.0202, -0.6161,  0.3280]],
       dtype=torch.float64)
	q_value: tensor([[-15.9063]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25347458374390663, distance: 0.9887329157754552 entropy 0.3949669301509857
epoch: 10, step: 83
	action: tensor([[-0.4042,  0.2245, -0.3742,  0.3348, -0.0950,  0.0946,  0.1730]],
       dtype=torch.float64)
	q_value: tensor([[-16.0643]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06804812874357591, distance: 1.1047231008066827 entropy 0.3949669301509857
epoch: 10, step: 84
	action: tensor([[-0.3139,  0.0509, -0.2001,  0.7107, -0.0626, -0.4885, -0.6314]],
       dtype=torch.float64)
	q_value: tensor([[-13.1123]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18471547143522715, distance: 1.033263988932394 entropy 0.3949669301509857
epoch: 10, step: 85
	action: tensor([[ 0.2243,  0.1848, -0.7701, -0.7281, -0.1496,  0.2998, -0.2638]],
       dtype=torch.float64)
	q_value: tensor([[-16.3649]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6137879506577293, distance: 0.7111637360807398 entropy 0.3949669301509857
epoch: 10, step: 86
	action: tensor([[-0.1821, -0.2985,  0.1795, -0.4539,  0.1355, -0.4537, -0.5246]],
       dtype=torch.float64)
	q_value: tensor([[-14.9950]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.47900761013613713, distance: 1.3916880483583534 entropy 0.3949669301509857
epoch: 10, step: 87
	action: tensor([[ 0.1150, -0.0120, -0.8264,  0.0662, -0.2046, -0.1708,  0.3289]],
       dtype=torch.float64)
	q_value: tensor([[-14.9631]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38017564212475063, distance: 0.9009299256286695 entropy 0.3949669301509857
epoch: 10, step: 88
	action: tensor([[ 0.1282,  0.4996, -0.5898, -0.3074,  0.5508,  0.2110, -0.0744]],
       dtype=torch.float64)
	q_value: tensor([[-14.4792]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7106645746236231, distance: 0.6155417268671056 entropy 0.3949669301509857
epoch: 10, step: 89
	action: tensor([[ 0.1182,  0.1112,  0.4799,  0.0252,  0.0124, -0.4201,  0.2160]],
       dtype=torch.float64)
	q_value: tensor([[-13.7531]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37653130342625696, distance: 0.9035746117786555 entropy 0.3949669301509857
epoch: 10, step: 90
	action: tensor([[ 0.9201,  0.0648, -0.4612,  0.2437,  0.6176,  0.2895,  0.0104]],
       dtype=torch.float64)
	q_value: tensor([[-16.0149]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.926214850368054, distance: 0.31084305917734645 entropy 0.3949669301509857
epoch: 10, step: 91
	action: tensor([[-0.2757, -0.4878,  0.4015,  0.3514,  0.3632, -0.1287, -0.0746]],
       dtype=torch.float64)
	q_value: tensor([[-18.1700]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18140515736159646, distance: 1.2438159580621488 entropy 0.3949669301509857
epoch: 10, step: 92
	action: tensor([[ 0.2707, -0.3071, -0.6795, -0.4093,  0.1007, -0.2741,  0.7102]],
       dtype=torch.float64)
	q_value: tensor([[-17.1700]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.126595763118441, distance: 1.0694594945426512 entropy 0.3949669301509857
epoch: 10, step: 93
	action: tensor([[ 0.2238, -0.0167, -0.2984, -0.5941, -0.0831,  0.7906, -0.0242]],
       dtype=torch.float64)
	q_value: tensor([[-17.1109]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4433159670605371, distance: 0.8538096827978428 entropy 0.3949669301509857
epoch: 10, step: 94
	action: tensor([[-0.1857, -0.2056, -0.1880, -0.4833,  0.0663,  0.2534, -0.2674]],
       dtype=torch.float64)
	q_value: tensor([[-16.5093]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15800746127936272, distance: 1.231437492651671 entropy 0.3949669301509857
epoch: 10, step: 95
	action: tensor([[ 0.1568,  0.2985,  0.0520,  0.0848, -0.3785, -0.1473,  0.5690]],
       dtype=torch.float64)
	q_value: tensor([[-12.8899]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6414561126454795, distance: 0.685216566988233 entropy 0.3949669301509857
epoch: 10, step: 96
	action: tensor([[-5.2286e-02, -4.0145e-02, -1.7596e-01,  1.3473e-01, -4.0843e-01,
         -2.2138e-04,  3.4285e-02]], dtype=torch.float64)
	q_value: tensor([[-15.4095]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28507161884503496, distance: 0.9675824048414902 entropy 0.3949669301509857
epoch: 10, step: 97
	action: tensor([[ 0.3108,  0.2931, -0.2659, -0.3491,  0.2986, -0.4670, -0.0143]],
       dtype=torch.float64)
	q_value: tensor([[-13.0195]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5730062660912711, distance: 0.747768978297808 entropy 0.3949669301509857
epoch: 10, step: 98
	action: tensor([[ 0.0237, -0.1456,  0.0750,  0.1213, -0.3609,  0.3305, -0.1306]],
       dtype=torch.float64)
	q_value: tensor([[-14.4818]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3595499439465102, distance: 0.9157972335428997 entropy 0.3949669301509857
epoch: 10, step: 99
	action: tensor([[ 0.3144,  0.6946, -0.0793, -0.0258,  0.1054, -0.0476,  0.0498]],
       dtype=torch.float64)
	q_value: tensor([[-13.9788]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8354294124269356, distance: 0.46422957310305546 entropy 0.3949669301509857
epoch: 10, step: 100
	action: tensor([[-0.0438,  0.0306,  0.4546,  0.2478, -0.0394, -0.6763,  0.4196]],
       dtype=torch.float64)
	q_value: tensor([[-13.9908]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24659258121474514, distance: 0.9932798823324743 entropy 0.3949669301509857
epoch: 10, step: 101
	action: tensor([[-0.0241,  0.0836, -0.1000, -0.4008,  0.1402,  0.0429, -0.4413]],
       dtype=torch.float64)
	q_value: tensor([[-17.6999]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1511797129809227, distance: 1.0543008791237616 entropy 0.3949669301509857
epoch: 10, step: 102
	action: tensor([[ 0.1104,  0.0404, -0.2588,  0.2723,  0.1735,  0.1275,  0.1351]],
       dtype=torch.float64)
	q_value: tensor([[-12.4203]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5090661845405366, distance: 0.8018039047406127 entropy 0.3949669301509857
epoch: 10, step: 103
	action: tensor([[ 0.1534, -0.3843, -0.0727, -0.1218, -0.0957,  0.1155, -0.1931]],
       dtype=torch.float64)
	q_value: tensor([[-13.1855]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04147344248918872, distance: 1.1203630272507805 entropy 0.3949669301509857
epoch: 10, step: 104
	action: tensor([[ 0.5088,  0.1161, -0.1725, -0.8386,  0.0100,  0.6638,  0.0226]],
       dtype=torch.float64)
	q_value: tensor([[-13.6028]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5465846752398521, distance: 0.7705570363118033 entropy 0.3949669301509857
epoch: 10, step: 105
	action: tensor([[ 0.4035,  0.0930,  0.1983,  0.0634,  0.0016, -0.9610, -0.2859]],
       dtype=torch.float64)
	q_value: tensor([[-17.6507]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5293004421375564, distance: 0.7851065280151779 entropy 0.3949669301509857
epoch: 10, step: 106
	action: tensor([[ 0.0970, -0.1066,  0.5360, -0.4120,  0.0413, -0.2034, -0.0637]],
       dtype=torch.float64)
	q_value: tensor([[-18.2763]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.049787056009343855, distance: 1.1724850127093993 entropy 0.3949669301509857
epoch: 10, step: 107
	action: tensor([[ 0.6391, -0.3461, -0.4336, -0.2882,  0.1234,  0.2646, -0.4735]],
       dtype=torch.float64)
	q_value: tensor([[-15.6883]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3045236141968094, distance: 0.9543284863396975 entropy 0.3949669301509857
epoch: 10, step: 108
	action: tensor([[-0.0539, -0.3265, -0.0058, -0.3647,  0.5204, -0.6264,  0.1465]],
       dtype=torch.float64)
	q_value: tensor([[-15.1340]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3685298361515983, distance: 1.338701741490739 entropy 0.3949669301509857
epoch: 10, step: 109
	action: tensor([[ 0.7854, -0.1013, -0.6025,  0.1019,  0.2463, -0.0653, -0.5828]],
       dtype=torch.float64)
	q_value: tensor([[-16.6949]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6728965355466817, distance: 0.6544843485484457 entropy 0.3949669301509857
epoch: 10, step: 110
	action: tensor([[-0.1335,  0.1929,  0.0468, -0.1165,  0.1355,  0.2635, -0.4479]],
       dtype=torch.float64)
	q_value: tensor([[-16.5846]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3316253014519419, distance: 0.9355493338817846 entropy 0.3949669301509857
epoch: 10, step: 111
	action: tensor([[-0.3183,  0.7135,  0.0271, -0.2789, -0.4535,  0.5049,  0.3325]],
       dtype=torch.float64)
	q_value: tensor([[-12.7356]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.297371817789312, distance: 0.9592227620553677 entropy 0.3949669301509857
epoch: 10, step: 112
	action: tensor([[-0.0692, -0.0800, -0.5660, -0.0123, -0.2143,  0.3466,  0.6941]],
       dtype=torch.float64)
	q_value: tensor([[-17.3135]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1687023129916282, distance: 1.0433619134632903 entropy 0.3949669301509857
epoch: 10, step: 113
	action: tensor([[ 0.4648, -0.4501,  0.4582, -0.6279,  0.0137, -0.4300,  0.9148]],
       dtype=torch.float64)
	q_value: tensor([[-15.8704]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2953697243375424, distance: 1.3024275208672156 entropy 0.3949669301509857
epoch: 10, step: 114
	action: tensor([[-0.3797, -0.7750, -0.6244, -0.3326,  0.1091,  0.2198,  0.1820]],
       dtype=torch.float64)
	q_value: tensor([[-21.9508]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7059694352136532, distance: 1.4946596700060077 entropy 0.3949669301509857
epoch: 10, step: 115
	action: tensor([[ 0.0199,  0.1362, -0.5492, -0.3380,  0.1358, -0.9019, -0.2843]],
       dtype=torch.float64)
	q_value: tensor([[-16.3514]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31744784788857117, distance: 0.9454196256634039 entropy 0.3949669301509857
epoch: 10, step: 116
	action: tensor([[-0.6344,  0.2925, -0.2802,  0.3624, -0.5181, -0.1464, -0.6053]],
       dtype=torch.float64)
	q_value: tensor([[-15.9580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17741889297814506, distance: 1.2417157603734432 entropy 0.3949669301509857
epoch: 10, step: 117
	action: tensor([[ 0.0716, -0.3053,  0.5071, -0.4167,  0.0722,  0.1498,  0.1829]],
       dtype=torch.float64)
	q_value: tensor([[-14.2964]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15197197718977185, distance: 1.2282242012024605 entropy 0.3949669301509857
epoch: 10, step: 118
	action: tensor([[ 0.0098,  0.3113,  0.0979, -0.4194, -0.3395,  0.4361,  0.1394]],
       dtype=torch.float64)
	q_value: tensor([[-16.5821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3859779622186962, distance: 0.8967031029273074 entropy 0.3949669301509857
epoch: 10, step: 119
	action: tensor([[ 0.2371, -0.2811,  0.3890, -0.1057, -0.2051,  0.2931, -0.0005]],
       dtype=torch.float64)
	q_value: tensor([[-15.9136]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27636946823792863, distance: 0.9734533287443405 entropy 0.3949669301509857
epoch: 10, step: 120
	action: tensor([[-0.2232, -0.5084,  0.0618, -0.2719,  0.6241, -0.0396, -0.6318]],
       dtype=torch.float64)
	q_value: tensor([[-15.7849]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5297859947949559, distance: 1.4153766727415158 entropy 0.3949669301509857
epoch: 10, step: 121
	action: tensor([[ 0.4388, -0.0325, -0.0826,  0.2280, -0.0548, -0.3496,  0.1087]],
       dtype=torch.float64)
	q_value: tensor([[-15.2930]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6330906746312135, distance: 0.6931641101609292 entropy 0.3949669301509857
epoch: 10, step: 122
	action: tensor([[ 0.1960,  0.4476,  0.5105,  0.5171, -0.6606,  0.3950,  0.3832]],
       dtype=torch.float64)
	q_value: tensor([[-14.9239]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8589549619133355, distance: 0.4297695082827241 entropy 0.3949669301509857
epoch: 10, step: 123
	action: tensor([[ 0.3819,  0.2632, -0.5525, -0.4832, -0.1534,  0.0588, -0.1159]],
       dtype=torch.float64)
	q_value: tensor([[-19.0123]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7053208455993296, distance: 0.6211999337581866 entropy 0.3949669301509857
epoch: 10, step: 124
	action: tensor([[-0.3811, -0.8878, -0.5530, -0.3009, -0.4107, -0.3090,  0.7250]],
       dtype=torch.float64)
	q_value: tensor([[-14.2621]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6451847971720295, distance: 1.4677903803592125 entropy 0.3949669301509857
epoch: 10, step: 125
	action: tensor([[ 0.0548,  0.2013, -0.1008,  0.4382, -0.4057, -0.1993, -0.1950]],
       dtype=torch.float64)
	q_value: tensor([[-20.1821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5805704623215315, distance: 0.7411160166448296 entropy 0.3949669301509857
epoch: 10, step: 126
	action: tensor([[ 0.0670, -0.0488, -0.0906,  0.5553, -0.1630,  0.0236, -0.4149]],
       dtype=torch.float64)
	q_value: tensor([[-13.9481]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5439851721679541, distance: 0.7727627431992246 entropy 0.3949669301509857
epoch: 10, step: 127
	action: tensor([[ 0.5854, -0.1000,  0.0089, -0.0841,  0.0738,  0.0243, -0.8776]],
       dtype=torch.float64)
	q_value: tensor([[-14.1231]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6013301651916021, distance: 0.7225424733027707 entropy 0.3949669301509857
LOSS epoch 10 actor 137.4917998836706 critic 138.82580437012652 
epoch: 11, step: 0
	action: tensor([[ 0.3178,  0.5654,  0.1356, -0.2367,  0.1051, -0.1312, -0.5108]],
       dtype=torch.float64)
	q_value: tensor([[-16.0217]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7791746008979935, distance: 0.5377509735100497 entropy 0.3949669301509857
epoch: 11, step: 1
	action: tensor([[-0.0602, -0.9613,  0.0985,  0.5654,  0.6233,  0.4061, -0.2868]],
       dtype=torch.float64)
	q_value: tensor([[-14.6893]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0004298745352622202, distance: 1.1445901897996396 entropy 0.3949669301509857
epoch: 11, step: 2
	action: tensor([[ 0.0354, -0.0413, -0.5753, -0.2339,  0.3851,  0.3478,  0.1231]],
       dtype=torch.float64)
	q_value: tensor([[-19.9184]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35868938690708807, distance: 0.916412294147576 entropy 0.3949669301509857
epoch: 11, step: 3
	action: tensor([[-0.2819, -0.0382,  0.4174,  0.1574,  0.5755,  0.0447,  0.9199]],
       dtype=torch.float64)
	q_value: tensor([[-13.5570]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05967918990509724, distance: 1.1096722280964375 entropy 0.3949669301509857
epoch: 11, step: 4
	action: tensor([[ 3.2418e-01, -2.3918e-05, -5.5992e-01, -1.9010e-01,  9.1844e-02,
         -1.8253e-02,  2.6713e-01]], dtype=torch.float64)
	q_value: tensor([[-19.9335]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5037924695994731, distance: 0.8060989745295485 entropy 0.3949669301509857
epoch: 11, step: 5
	action: tensor([[ 0.5525, -0.1473, -0.1057, -0.6009, -0.0423, -0.1086,  0.1212]],
       dtype=torch.float64)
	q_value: tensor([[-13.9245]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18763174859701215, distance: 1.0314143379633463 entropy 0.3949669301509857
epoch: 11, step: 6
	action: tensor([[ 0.0621, -0.5943,  0.2601, -0.1526, -0.3864,  0.0609, -0.1068]],
       dtype=torch.float64)
	q_value: tensor([[-15.9061]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2641165506721299, distance: 1.2866198635407264 entropy 0.3949669301509857
epoch: 11, step: 7
	action: tensor([[ 0.4333, -0.0623, -0.7201,  0.7690, -0.2643,  0.0254, -0.1047]],
       dtype=torch.float64)
	q_value: tensor([[-15.6645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6268347378959925, distance: 0.6990484829280322 entropy 0.3949669301509857
epoch: 11, step: 8
	action: tensor([[ 0.1530,  0.0587,  0.0062, -1.0272,  0.4167,  0.5913,  0.0453]],
       dtype=torch.float64)
	q_value: tensor([[-15.7970]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19324111577154068, distance: 1.0278472342217677 entropy 0.3949669301509857
epoch: 11, step: 9
	action: tensor([[ 0.1697,  0.3611, -0.1296, -0.3971,  0.0016,  0.3165, -0.3766]],
       dtype=torch.float64)
	q_value: tensor([[-17.1305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6012739729857683, distance: 0.7225933924108404 entropy 0.3949669301509857
epoch: 11, step: 10
	action: tensor([[ 0.5429, -0.1469, -0.1702,  0.1640, -0.1470, -0.0732,  0.2983]],
       dtype=torch.float64)
	q_value: tensor([[-13.3269]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6234762179590357, distance: 0.7021871852853994 entropy 0.3949669301509857
epoch: 11, step: 11
	action: tensor([[ 0.0165, -0.3266,  0.6230, -0.3525, -0.1113, -0.6100, -0.2763]],
       dtype=torch.float64)
	q_value: tensor([[-14.9945]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.36969627937023075, distance: 1.3392721298501777 entropy 0.3949669301509857
epoch: 11, step: 12
	action: tensor([[ 0.0621,  0.1748, -0.2094, -0.0602,  0.5451,  0.0654,  0.0084]],
       dtype=torch.float64)
	q_value: tensor([[-17.3273]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4713916413966862, distance: 0.8320007280589976 entropy 0.3949669301509857
epoch: 11, step: 13
	action: tensor([[ 0.1273,  0.2451,  0.0628, -0.3144,  0.2304, -0.4606,  0.1178]],
       dtype=torch.float64)
	q_value: tensor([[-13.2067]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3224807323925978, distance: 0.9419275910023416 entropy 0.3949669301509857
epoch: 11, step: 14
	action: tensor([[ 0.0945,  0.4035,  0.0457, -0.7776, -0.1251, -0.0040, -0.1472]],
       dtype=torch.float64)
	q_value: tensor([[-14.3103]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.317315474941556, distance: 0.9455112977169401 entropy 0.3949669301509857
epoch: 11, step: 15
	action: tensor([[ 0.1725,  0.3037, -0.3985,  0.1773, -0.0162,  0.2605,  0.1394]],
       dtype=torch.float64)
	q_value: tensor([[-15.1721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6418442096015989, distance: 0.684845618723951 entropy 0.3949669301509857
epoch: 11, step: 16
	action: tensor([[ 0.3444, -0.0504, -0.1058,  0.2357,  0.4345, -0.1664, -0.1195]],
       dtype=torch.float64)
	q_value: tensor([[-12.6536]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5892789557942208, distance: 0.7333818724986609 entropy 0.3949669301509857
epoch: 11, step: 17
	action: tensor([[-0.6880,  0.3150, -0.3381, -0.3005,  0.0017,  0.0643,  0.1519]],
       dtype=torch.float64)
	q_value: tensor([[-14.6375]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3294930085733967, distance: 1.319470613157557 entropy 0.3949669301509857
epoch: 11, step: 18
	action: tensor([[ 0.0030, -0.6301, -0.4683,  0.3213, -0.5259,  0.3322,  0.1221]],
       dtype=torch.float64)
	q_value: tensor([[-12.9517]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12529109458822907, distance: 1.2139173935666945 entropy 0.3949669301509857
epoch: 11, step: 19
	action: tensor([[ 0.8207,  0.5144, -0.0345,  0.4179,  0.4145,  0.4146,  0.3949]],
       dtype=torch.float64)
	q_value: tensor([[-15.3767]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9454391265658645, distance: 0.26729901003312123 entropy 0.3949669301509857
epoch: 11, step: 20
	action: tensor([[-0.4325, -0.2832, -0.4255, -0.1168, -0.2195, -0.4121, -0.0828]],
       dtype=torch.float64)
	q_value: tensor([[-18.3144]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.36763927595532864, distance: 1.3382660957086319 entropy 0.3949669301509857
epoch: 11, step: 21
	action: tensor([[-1.3738e-04, -7.0615e-01, -8.5175e-01,  1.4138e-02,  3.3694e-01,
         -2.1205e-01, -2.8030e-01]], dtype=torch.float64)
	q_value: tensor([[-14.0670]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3340946741259796, distance: 1.321752128584149 entropy 0.3949669301509857
epoch: 11, step: 22
	action: tensor([[-0.9604,  0.3500, -0.5671,  0.7158, -0.0939,  0.1375,  0.0649]],
       dtype=torch.float64)
	q_value: tensor([[-15.6731]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4374187854742517, distance: 1.371981803601355 entropy 0.3949669301509857
epoch: 11, step: 23
	action: tensor([[ 0.4925,  0.1799, -0.6297, -0.0409, -0.0039,  0.1299,  0.3564]],
       dtype=torch.float64)
	q_value: tensor([[-15.2956]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7711740107962245, distance: 0.5474057658050406 entropy 0.3949669301509857
epoch: 11, step: 24
	action: tensor([[ 0.2100, -0.2791, -0.3572, -0.1972, -0.1562, -0.1017,  0.0897]],
       dtype=torch.float64)
	q_value: tensor([[-14.8100]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17016610835943702, distance: 1.0424429037768845 entropy 0.3949669301509857
epoch: 11, step: 25
	action: tensor([[ 0.5600, -0.6634, -0.0429, -0.1656, -0.0578,  0.2201, -0.0716]],
       dtype=torch.float64)
	q_value: tensor([[-13.5703]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.011236105509567773, distance: 1.1507552819468208 entropy 0.3949669301509857
epoch: 11, step: 26
	action: tensor([[ 0.0115, -0.2683, -0.1191,  0.8048, -0.2436,  0.1305, -0.0742]],
       dtype=torch.float64)
	q_value: tensor([[-16.1548]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5044004047134436, distance: 0.805605021865068 entropy 0.3949669301509857
epoch: 11, step: 27
	action: tensor([[-0.4173,  0.1838, -0.2180, -0.3526, -0.6346,  0.1139,  0.1965]],
       dtype=torch.float64)
	q_value: tensor([[-15.6026]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13610253138619588, distance: 1.2197349187001758 entropy 0.3949669301509857
epoch: 11, step: 28
	action: tensor([[ 0.2257,  0.0683, -0.0584, -0.0473,  0.0951, -0.1865, -0.4512]],
       dtype=torch.float64)
	q_value: tensor([[-14.6979]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4793897016652364, distance: 0.8256824822181004 entropy 0.3949669301509857
epoch: 11, step: 29
	action: tensor([[ 0.2180,  0.5709, -0.2936, -0.6160,  0.4457,  0.3930,  0.4444]],
       dtype=torch.float64)
	q_value: tensor([[-13.1587]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7736282611058414, distance: 0.5444622790557202 entropy 0.3949669301509857
epoch: 11, step: 30
	action: tensor([[ 1.1739e-01, -5.1475e-01, -5.5373e-01, -1.2917e-01,  5.4483e-01,
          1.0634e-01, -3.2848e-04]], dtype=torch.float64)
	q_value: tensor([[-16.8096]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07749040671034346, distance: 1.1878549159412655 entropy 0.3949669301509857
epoch: 11, step: 31
	action: tensor([[-0.8036, -0.5150, -0.3242, -0.0690, -0.4544, -0.2536, -0.1300]],
       dtype=torch.float64)
	q_value: tensor([[-14.7465]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.9675699140474039, distance: 1.6051727554545772 entropy 0.3949669301509857
epoch: 11, step: 32
	action: tensor([[-0.0601, -0.0388, -0.5225, -0.0962, -0.2442, -0.1309, -0.1870]],
       dtype=torch.float64)
	q_value: tensor([[-15.6950]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17380484736579394, distance: 1.0401548886618193 entropy 0.3949669301509857
epoch: 11, step: 33
	action: tensor([[-0.5212, -0.4192, -0.3838,  0.0021, -0.2681, -0.6806, -0.5831]],
       dtype=torch.float64)
	q_value: tensor([[-12.3425]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5116790938585389, distance: 1.406975375951357 entropy 0.3949669301509857
epoch: 11, step: 34
	action: tensor([[-0.8707,  0.0429, -0.2567,  0.0592,  0.0161,  0.2386, -0.1598]],
       dtype=torch.float64)
	q_value: tensor([[-15.6719]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6967022876596354, distance: 1.4905945052645855 entropy 0.3949669301509857
epoch: 11, step: 35
	action: tensor([[ 0.2531, -0.1728,  0.0123,  0.0553, -0.0813,  0.2939,  0.4228]],
       dtype=torch.float64)
	q_value: tensor([[-13.8606]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4762536819505355, distance: 0.8281655956429533 entropy 0.3949669301509857
epoch: 11, step: 36
	action: tensor([[ 0.6515, -0.1573, -0.7552, -0.0538,  0.2571,  0.1237,  0.2286]],
       dtype=torch.float64)
	q_value: tensor([[-15.2050]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5901579263506518, distance: 0.7325967090230506 entropy 0.3949669301509857
epoch: 11, step: 37
	action: tensor([[-0.4329, -0.1928, -0.4411, -0.5046,  0.0434,  0.0198, -0.2303]],
       dtype=torch.float64)
	q_value: tensor([[-16.2089]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3534789744495199, distance: 1.3313199808789231 entropy 0.3949669301509857
epoch: 11, step: 38
	action: tensor([[-0.2362, -0.3562, -0.2236,  0.1776, -0.2225,  0.2154,  0.0352]],
       dtype=torch.float64)
	q_value: tensor([[-12.5515]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13035569985607487, distance: 1.2166460698971124 entropy 0.3949669301509857
epoch: 11, step: 39
	action: tensor([[ 0.6028, -0.1544,  0.1889, -0.2207,  0.2847, -0.2526, -0.7364]],
       dtype=torch.float64)
	q_value: tensor([[-13.3559]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36508551714882864, distance: 0.9118309099096238 entropy 0.3949669301509857
epoch: 11, step: 40
	action: tensor([[ 0.8762, -0.0951, -0.1948,  0.2527, -0.7506, -0.0513, -0.2786]],
       dtype=torch.float64)
	q_value: tensor([[-16.7484]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7728732280152836, distance: 0.5453695142699364 entropy 0.3949669301509857
epoch: 11, step: 41
	action: tensor([[-0.4800,  0.2556,  0.1975, -0.3096, -0.3948, -0.2254, -0.3723]],
       dtype=torch.float64)
	q_value: tensor([[-17.5347]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.25267233890450314, distance: 1.2807826536535802 entropy 0.3949669301509857
epoch: 11, step: 42
	action: tensor([[ 0.4315,  0.7275, -0.0808, -0.1910,  0.0040, -0.2548, -0.1992]],
       dtype=torch.float64)
	q_value: tensor([[-14.1325]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8813789796796518, distance: 0.3941282100134344 entropy 0.3949669301509857
epoch: 11, step: 43
	action: tensor([[ 0.3145,  0.2305,  0.0402, -0.0091,  0.0671,  0.3466, -0.1043]],
       dtype=torch.float64)
	q_value: tensor([[-14.8855]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7597123626093256, distance: 0.5609477455162674 entropy 0.3949669301509857
epoch: 11, step: 44
	action: tensor([[-0.3093, -0.3153, -0.7064, -0.2566, -0.0263,  0.3413, -0.0614]],
       dtype=torch.float64)
	q_value: tensor([[-13.5881]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.25813393149103314, distance: 1.283571693059295 entropy 0.3949669301509857
epoch: 11, step: 45
	action: tensor([[-0.2100, -0.2302, -0.1767, -0.2308, -0.2466, -0.0707,  0.0419]],
       dtype=torch.float64)
	q_value: tensor([[-13.0509]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18285447012983913, distance: 1.244578662460774 entropy 0.3949669301509857
epoch: 11, step: 46
	action: tensor([[ 0.1477, -0.0676, -0.5879,  0.4306, -0.2575,  0.2311, -0.5571]],
       dtype=torch.float64)
	q_value: tensor([[-13.0263]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4113307161445213, distance: 0.8779956838484319 entropy 0.3949669301509857
epoch: 11, step: 47
	action: tensor([[-0.1303, -0.7979, -0.3301, -0.7501,  0.2743,  0.1674,  0.7988]],
       dtype=torch.float64)
	q_value: tensor([[-13.5644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6240495253376992, distance: 1.4583317385929138 entropy 0.3949669301509857
epoch: 11, step: 48
	action: tensor([[-0.2339, -0.1288, -0.1922,  0.3904,  0.0335,  0.4261, -0.2918]],
       dtype=torch.float64)
	q_value: tensor([[-20.2118]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2122982134826793, distance: 1.0156348824384187 entropy 0.3949669301509857
epoch: 11, step: 49
	action: tensor([[ 0.3015, -0.1099, -0.2922,  0.5961,  0.0682, -0.1920,  0.5692]],
       dtype=torch.float64)
	q_value: tensor([[-13.2266]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.628420860664263, distance: 0.6975612632695121 entropy 0.3949669301509857
epoch: 11, step: 50
	action: tensor([[-0.2155, -0.1686, -0.5861, -0.1152,  0.2468,  0.0496,  0.0302]],
       dtype=torch.float64)
	q_value: tensor([[-16.9537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06376135249964987, distance: 1.1802630124148092 entropy 0.3949669301509857
epoch: 11, step: 51
	action: tensor([[-0.3324,  0.1007,  0.2308, -0.0844, -0.0450, -0.2772,  0.3184]],
       dtype=torch.float64)
	q_value: tensor([[-12.6782]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11889848099073186, distance: 1.2104644393515984 entropy 0.3949669301509857
epoch: 11, step: 52
	action: tensor([[ 0.5372,  0.5193,  0.0121, -0.3564,  0.0327,  0.6713, -0.5572]],
       dtype=torch.float64)
	q_value: tensor([[-14.4864]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9208272361087134, distance: 0.321991640239133 entropy 0.3949669301509857
epoch: 11, step: 53
	action: tensor([[ 0.7943, -0.7683,  0.1444, -0.2830,  0.3366,  0.6505, -0.0091]],
       dtype=torch.float64)
	q_value: tensor([[-16.8835]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.008260727173263538, distance: 1.149061090746452 entropy 0.3949669301509857
epoch: 11, step: 54
	action: tensor([[-0.2950, -0.2944,  0.1811,  0.5300,  0.0170, -0.3697,  0.2333]],
       dtype=torch.float64)
	q_value: tensor([[-19.0988]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.013949036523826397, distance: 1.1522978636544285 entropy 0.3949669301509857
epoch: 11, step: 55
	action: tensor([[ 0.0423,  0.1501,  0.5948,  0.1717,  0.3675, -0.3363, -0.5155]],
       dtype=torch.float64)
	q_value: tensor([[-16.4707]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44574418135631966, distance: 0.8519455206152177 entropy 0.3949669301509857
epoch: 11, step: 56
	action: tensor([[ 0.0528,  0.2419, -0.1083, -0.2699,  0.1029,  0.3189,  0.0604]],
       dtype=torch.float64)
	q_value: tensor([[-16.2112]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4686370843114481, distance: 0.8341656726779301 entropy 0.3949669301509857
epoch: 11, step: 57
	action: tensor([[-0.1062,  0.2756, -0.3069, -0.0294,  0.3186, -0.2078,  0.4967]],
       dtype=torch.float64)
	q_value: tensor([[-12.9623]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31100628641305617, distance: 0.9498703313970518 entropy 0.3949669301509857
epoch: 11, step: 58
	action: tensor([[ 0.3607, -0.1996, -0.0804,  0.3141,  0.3091, -0.3972, -0.2328]],
       dtype=torch.float64)
	q_value: tensor([[-14.3965]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4643963091861548, distance: 0.8374877702456743 entropy 0.3949669301509857
epoch: 11, step: 59
	action: tensor([[ 0.7174, -0.2956, -0.4232,  0.3298,  0.1382, -0.4163, -0.3808]],
       dtype=torch.float64)
	q_value: tensor([[-15.6264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5245972077579639, distance: 0.7890191743904741 entropy 0.3949669301509857
epoch: 11, step: 60
	action: tensor([[ 0.0824,  0.9498, -0.0378, -0.6896,  0.1506, -0.4206,  0.2177]],
       dtype=torch.float64)
	q_value: tensor([[-17.3563]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.66070369697472, distance: 0.6665707572710602 entropy 0.3949669301509857
epoch: 11, step: 61
	action: tensor([[-0.0499,  0.6322,  0.0108,  0.2480, -0.3637,  0.0917, -0.1103]],
       dtype=torch.float64)
	q_value: tensor([[-16.5286]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6197840551826203, distance: 0.7056215827932111 entropy 0.3949669301509857
epoch: 11, step: 62
	action: tensor([[-0.3192,  0.0201, -0.0314, -0.4575, -0.1138, -0.1068, -0.0494]],
       dtype=torch.float64)
	q_value: tensor([[-14.0732]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2269261105879632, distance: 1.267552329447877 entropy 0.3949669301509857
epoch: 11, step: 63
	action: tensor([[ 0.2340, -0.1038, -0.3378, -0.0886, -0.3798,  0.0152,  0.3623]],
       dtype=torch.float64)
	q_value: tensor([[-12.8296]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3723444406175125, distance: 0.9066034830220375 entropy 0.3949669301509857
epoch: 11, step: 64
	action: tensor([[-0.5552,  0.2897, -0.4426,  0.1193, -0.0560,  0.4461, -0.1548]],
       dtype=torch.float64)
	q_value: tensor([[-14.4121]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0855156749433037, distance: 1.1922703472161265 entropy 0.3949669301509857
epoch: 11, step: 65
	action: tensor([[ 0.1832,  0.3656, -0.4608, -0.2203, -0.1640, -0.4302,  0.2914]],
       dtype=torch.float64)
	q_value: tensor([[-12.6255]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6449329345601014, distance: 0.6818861801855967 entropy 0.3949669301509857
epoch: 11, step: 66
	action: tensor([[ 0.5387, -0.4066, -0.5557, -0.6070,  0.3583, -0.2017, -0.0057]],
       dtype=torch.float64)
	q_value: tensor([[-14.0785]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.028407870274070657, distance: 1.1604846199278236 entropy 0.3949669301509857
epoch: 11, step: 67
	action: tensor([[-0.3505, -0.4260,  0.6229, -0.4447,  1.0396,  0.3606,  0.5552]],
       dtype=torch.float64)
	q_value: tensor([[-16.1357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6627992614425846, distance: 1.4756270397232625 entropy 0.3949669301509857
epoch: 11, step: 68
	action: tensor([[-0.2616, -0.4962, -0.4053, -0.1104,  0.6057, -0.0423, -0.3351]],
       dtype=torch.float64)
	q_value: tensor([[-21.6092]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4183542522663084, distance: 1.362853113868758 entropy 0.3949669301509857
epoch: 11, step: 69
	action: tensor([[ 0.1953, -0.6899,  0.0775, -0.0517, -0.0450, -0.2867, -0.3783]],
       dtype=torch.float64)
	q_value: tensor([[-14.4605]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3011556237432311, distance: 1.3053329919696814 entropy 0.3949669301509857
epoch: 11, step: 70
	action: tensor([[-0.0173, -0.1008,  0.1786, -0.1816,  0.4757,  0.1019, -0.2260]],
       dtype=torch.float64)
	q_value: tensor([[-15.9503]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13193133562776083, distance: 1.0661878593526743 entropy 0.3949669301509857
epoch: 11, step: 71
	action: tensor([[ 0.0718,  0.3870, -0.7197, -0.1304,  0.1853, -0.9438, -0.1224]],
       dtype=torch.float64)
	q_value: tensor([[-13.3910]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5969833059135279, distance: 0.7264708812355773 entropy 0.3949669301509857
epoch: 11, step: 72
	action: tensor([[ 0.4975,  0.5477, -0.2406,  0.1436,  0.0801,  0.2812, -0.1580]],
       dtype=torch.float64)
	q_value: tensor([[-16.4449]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8972160519142263, distance: 0.3668760326918038 entropy 0.3949669301509857
epoch: 11, step: 73
	action: tensor([[ 0.2876,  0.4812,  0.0849, -0.1204,  0.2220, -0.4810, -0.6322]],
       dtype=torch.float64)
	q_value: tensor([[-14.1550]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7045190476459505, distance: 0.6220444768512906 entropy 0.3949669301509857
epoch: 11, step: 74
	action: tensor([[ 0.1129,  0.0752, -0.0239, -0.1293, -0.8132,  0.4027,  0.5164]],
       dtype=torch.float64)
	q_value: tensor([[-15.3424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4136875403801493, distance: 0.8762363285634132 entropy 0.3949669301509857
epoch: 11, step: 75
	action: tensor([[ 0.2508, -0.0357, -0.4283, -0.3783, -0.3131,  0.2010, -0.1658]],
       dtype=torch.float64)
	q_value: tensor([[-17.6736]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41501433007736854, distance: 0.8752443320843522 entropy 0.3949669301509857
epoch: 11, step: 76
	action: tensor([[ 0.5799, -0.0068, -0.6254, -0.2460, -0.0167,  0.0742, -0.1335]],
       dtype=torch.float64)
	q_value: tensor([[-13.8020]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6442637551434414, distance: 0.6825284382952316 entropy 0.3949669301509857
epoch: 11, step: 77
	action: tensor([[-0.3189,  0.0552,  0.1815, -0.2907, -0.7355,  0.6600,  0.5395]],
       dtype=torch.float64)
	q_value: tensor([[-14.3346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12195877687392342, distance: 1.2121186778290896 entropy 0.3949669301509857
epoch: 11, step: 78
	action: tensor([[-0.6292, -0.5934, -0.5547,  0.3120, -0.0530, -0.2449,  0.1808]],
       dtype=torch.float64)
	q_value: tensor([[-18.8718]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8444898204850013, distance: 1.5541567702504113 entropy 0.3949669301509857
epoch: 11, step: 79
	action: tensor([[-0.0916,  0.6423,  0.0586, -0.3797, -0.3493, -0.2280,  0.1691]],
       dtype=torch.float64)
	q_value: tensor([[-16.7104]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4085899722363371, distance: 0.8800372095293942 entropy 0.3949669301509857
epoch: 11, step: 80
	action: tensor([[ 0.2173,  0.4572,  0.2293,  0.1527, -0.4226,  0.1642,  0.1593]],
       dtype=torch.float64)
	q_value: tensor([[-15.2241]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7638755351258482, distance: 0.556067082125766 entropy 0.3949669301509857
epoch: 11, step: 81
	action: tensor([[-0.3082, -0.0674, -0.1132, -0.3660,  0.1302, -0.3680,  0.4668]],
       dtype=torch.float64)
	q_value: tensor([[-15.3207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.32015558549345347, distance: 1.3148289324079847 entropy 0.3949669301509857
epoch: 11, step: 82
	action: tensor([[ 0.3239, -0.0031, -0.4849,  0.3261,  0.0679, -0.2991, -0.0057]],
       dtype=torch.float64)
	q_value: tensor([[-14.6885]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5816028283460776, distance: 0.7402033789961373 entropy 0.3949669301509857
epoch: 11, step: 83
	action: tensor([[-0.2113, -0.1646, -0.2616, -0.2972, -0.1945, -0.0072, -0.1456]],
       dtype=torch.float64)
	q_value: tensor([[-14.3600]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1274139731402133, distance: 1.2150618907413455 entropy 0.3949669301509857
epoch: 11, step: 84
	action: tensor([[ 0.3473,  0.4758, -0.0789,  0.1913,  0.6243,  0.2523, -0.1345]],
       dtype=torch.float64)
	q_value: tensor([[-12.4224]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8633671316043916, distance: 0.4229940769733381 entropy 0.3949669301509857
epoch: 11, step: 85
	action: tensor([[ 0.3463, -0.1886, -0.0060, -0.0490, -0.8652,  0.4640, -0.1978]],
       dtype=torch.float64)
	q_value: tensor([[-14.5777]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4564140317894936, distance: 0.8437053675981956 entropy 0.3949669301509857
epoch: 11, step: 86
	action: tensor([[ 0.4503,  0.2247,  0.2397,  0.0727,  0.1816,  0.4861, -0.3041]],
       dtype=torch.float64)
	q_value: tensor([[-17.2061]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8897537698572733, distance: 0.3799606020472523 entropy 0.3949669301509857
epoch: 11, step: 87
	action: tensor([[ 0.0861,  0.2131, -0.6026,  0.0242, -0.2066,  0.3827,  0.3482]],
       dtype=torch.float64)
	q_value: tensor([[-15.2465]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.521597390588346, distance: 0.791504636404666 entropy 0.3949669301509857
epoch: 11, step: 88
	action: tensor([[-0.1922, -0.3163, -0.1905,  0.1185,  0.2883, -0.2060, -0.5087]],
       dtype=torch.float64)
	q_value: tensor([[-14.0413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15263873065381084, distance: 1.2285795936488688 entropy 0.3949669301509857
epoch: 11, step: 89
	action: tensor([[ 0.1832, -0.5914, -0.0980,  0.2532, -0.2851, -0.1657, -0.3322]],
       dtype=torch.float64)
	q_value: tensor([[-13.9443]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05277565243564564, distance: 1.1137382099188857 entropy 0.3949669301509857
epoch: 11, step: 90
	action: tensor([[ 0.6233, -0.0277,  0.1916,  0.0147, -0.6872, -0.1144, -0.2423]],
       dtype=torch.float64)
	q_value: tensor([[-15.1781]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6687172644213992, distance: 0.6586521216709659 entropy 0.3949669301509857
epoch: 11, step: 91
	action: tensor([[-0.2969,  0.6840,  0.0781, -0.2981, -0.2441, -0.2741,  0.1283]],
       dtype=torch.float64)
	q_value: tensor([[-16.7306]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.275501312386204, distance: 0.9740370906535227 entropy 0.3949669301509857
epoch: 11, step: 92
	action: tensor([[-0.1105, -0.0414, -0.4020,  0.2638, -0.1488, -0.5830,  0.3123]],
       dtype=torch.float64)
	q_value: tensor([[-14.5470]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19733985626213535, distance: 1.0252329192876255 entropy 0.3949669301509857
epoch: 11, step: 93
	action: tensor([[ 0.4321, -0.2212, -0.2445,  0.0187, -0.0574, -0.0330, -0.4418]],
       dtype=torch.float64)
	q_value: tensor([[-15.3912]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4367774664659214, distance: 0.8588092316684968 entropy 0.3949669301509857
epoch: 11, step: 94
	action: tensor([[ 0.2224,  0.5568, -0.0590, -0.4501, -0.2546,  0.1981, -0.1235]],
       dtype=torch.float64)
	q_value: tensor([[-13.8982]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6980148816560504, distance: 0.6288534735213052 entropy 0.3949669301509857
epoch: 11, step: 95
	action: tensor([[-0.4790,  0.6778, -0.2890,  0.0603, -0.3499, -0.1590,  0.6601]],
       dtype=torch.float64)
	q_value: tensor([[-15.0769]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21533442296355376, distance: 1.0136756019233901 entropy 0.3949669301509857
epoch: 11, step: 96
	action: tensor([[-0.0700,  0.2084,  0.5084,  0.5239, -0.0739, -0.1749, -0.0791]],
       dtype=torch.float64)
	q_value: tensor([[-15.6797]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6120234346031548, distance: 0.7127864583697295 entropy 0.3949669301509857
epoch: 11, step: 97
	action: tensor([[-0.1902, -0.0738, -0.0583, -0.2879, -0.1683, -0.0732, -0.4744]],
       dtype=torch.float64)
	q_value: tensor([[-16.1990]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08867229486041683, distance: 1.1940026171120508 entropy 0.3949669301509857
epoch: 11, step: 98
	action: tensor([[ 0.4100, -0.7444, -0.0211, -0.8663,  0.1520, -0.5872, -0.1530]],
       dtype=torch.float64)
	q_value: tensor([[-12.6366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6261808639413371, distance: 1.459288353395023 entropy 0.3949669301509857
epoch: 11, step: 99
	action: tensor([[ 0.1876, -0.0799, -0.6481, -0.3546,  0.1933,  0.0014,  0.4856]],
       dtype=torch.float64)
	q_value: tensor([[-18.8563]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36438955353197455, distance: 0.9123305262716442 entropy 0.3949669301509857
epoch: 11, step: 100
	action: tensor([[ 0.3596, -0.4200,  0.6686,  0.0656, -0.1361, -0.0061,  0.3715]],
       dtype=torch.float64)
	q_value: tensor([[-15.1579]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24898367899552465, distance: 0.9917024378623424 entropy 0.3949669301509857
epoch: 11, step: 101
	action: tensor([[ 0.7309, -0.1491,  0.1012,  0.1242,  0.7671,  0.1883,  0.2045]],
       dtype=torch.float64)
	q_value: tensor([[-18.5545]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7280402340889067, distance: 0.5967728029966107 entropy 0.3949669301509857
epoch: 11, step: 102
	action: tensor([[ 0.5320,  0.1064, -0.8836,  0.4937, -0.1194,  0.0851,  0.1406]],
       dtype=torch.float64)
	q_value: tensor([[-18.5644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7626099293918249, distance: 0.5575553251634493 entropy 0.3949669301509857
epoch: 11, step: 103
	action: tensor([[ 0.4612, -0.3819, -0.6803,  0.0278,  0.4704,  0.3328,  1.0188]],
       dtype=torch.float64)
	q_value: tensor([[-15.5406]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4151778024552346, distance: 0.8751220314224513 entropy 0.3949669301509857
epoch: 11, step: 104
	action: tensor([[ 0.0189,  0.5138, -0.2653, -0.3545, -0.1552, -0.1309, -0.3980]],
       dtype=torch.float64)
	q_value: tensor([[-20.4154]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5602503195599315, distance: 0.7588561524540713 entropy 0.3949669301509857
epoch: 11, step: 105
	action: tensor([[ 0.4906, -0.0594, -0.3247, -0.5303,  0.3608,  0.1263,  0.0710]],
       dtype=torch.float64)
	q_value: tensor([[-13.1025]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4145354237780291, distance: 0.8756025239965722 entropy 0.3949669301509857
epoch: 11, step: 106
	action: tensor([[ 0.4255, -0.5926, -0.3619, -0.1707,  0.4357, -0.2910, -0.1013]],
       dtype=torch.float64)
	q_value: tensor([[-14.9928]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13217497049905869, distance: 1.2176247523560635 entropy 0.3949669301509857
epoch: 11, step: 107
	action: tensor([[-0.9615,  0.3082,  0.2282,  0.0298, -0.0565, -0.2799, -0.0759]],
       dtype=torch.float64)
	q_value: tensor([[-16.4246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7097342345704256, distance: 1.496307997504425 entropy 0.3949669301509857
epoch: 11, step: 108
	action: tensor([[ 0.3651, -0.1497, -0.0659, -0.1721,  0.3719, -0.5350, -0.7903]],
       dtype=torch.float64)
	q_value: tensor([[-14.8814]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21254994562106533, distance: 1.0154725822031792 entropy 0.3949669301509857
epoch: 11, step: 109
	action: tensor([[ 0.2215, -0.1034,  0.3986,  0.3492,  0.4570, -0.3695, -0.1341]],
       dtype=torch.float64)
	q_value: tensor([[-16.4218]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.520092690685656, distance: 0.7927484025476643 entropy 0.3949669301509857
epoch: 11, step: 110
	action: tensor([[-0.1445, -0.3923, -0.2782, -0.4394, -0.7659, -0.1738,  0.6020]],
       dtype=torch.float64)
	q_value: tensor([[-16.8296]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3172032856708862, distance: 1.3133579156872635 entropy 0.3949669301509857
epoch: 11, step: 111
	action: tensor([[-0.0217,  0.0149, -0.2109, -0.8584,  0.4017, -0.6239,  0.1195]],
       dtype=torch.float64)
	q_value: tensor([[-17.6185]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09233294872538722, distance: 1.1960083456759132 entropy 0.3949669301509857
epoch: 11, step: 112
	action: tensor([[-0.1509, -0.0882,  0.0567, -0.0408, -0.0494,  0.0946, -0.1667]],
       dtype=torch.float64)
	q_value: tensor([[-15.3636]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08711700945696244, distance: 1.0933626922767896 entropy 0.3949669301509857
epoch: 11, step: 113
	action: tensor([[ 0.2311, -0.0450, -0.5612,  0.2866,  0.1458,  0.2489, -0.3817]],
       dtype=torch.float64)
	q_value: tensor([[-12.5446]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5475009736743952, distance: 0.7697780404945689 entropy 0.3949669301509857
epoch: 11, step: 114
	action: tensor([[-0.2783, -0.2785,  0.2423, -0.5853, -0.0981,  0.3793, -0.1035]],
       dtype=torch.float64)
	q_value: tensor([[-13.2803]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4510682036994478, distance: 1.3784804340463048 entropy 0.3949669301509857
epoch: 11, step: 115
	action: tensor([[-0.0557, -0.2248, -0.5836,  0.1744,  0.2711,  0.1043, -0.1421]],
       dtype=torch.float64)
	q_value: tensor([[-15.4021]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10058086225324836, distance: 1.0852698934218366 entropy 0.3949669301509857
epoch: 11, step: 116
	action: tensor([[ 0.3548, -0.2102,  0.2080,  0.3977,  0.1192,  0.3529, -0.0172]],
       dtype=torch.float64)
	q_value: tensor([[-13.0789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7416133935495997, distance: 0.5816901266362393 entropy 0.3949669301509857
epoch: 11, step: 117
	action: tensor([[ 0.4928,  0.3356, -0.7662,  0.3120,  0.1194, -0.1574, -1.1027]],
       dtype=torch.float64)
	q_value: tensor([[-15.8401]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8431506367050835, distance: 0.4532085279480853 entropy 0.3949669301509857
epoch: 11, step: 118
	action: tensor([[ 0.3052,  0.2136,  0.7557, -0.0508,  0.4323,  0.6974,  0.5638]],
       dtype=torch.float64)
	q_value: tensor([[-17.3670]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8447188197981924, distance: 0.45093724260269796 entropy 0.3949669301509857
epoch: 11, step: 119
	action: tensor([[ 0.1805,  0.4230, -0.1470, -0.1921,  0.3329, -0.3409,  0.2297]],
       dtype=torch.float64)
	q_value: tensor([[-19.5719]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5876813219129188, distance: 0.7348068524827605 entropy 0.3949669301509857
epoch: 11, step: 120
	action: tensor([[-0.1786, -0.0950,  0.1227, -0.2190, -0.3235, -0.0264,  0.4848]],
       dtype=torch.float64)
	q_value: tensor([[-14.2318]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1170276338297398, distance: 1.2094520412584835 entropy 0.3949669301509857
epoch: 11, step: 121
	action: tensor([[ 0.3395, -0.3749, -0.3062,  0.2308,  0.2306,  0.0253, -0.1820]],
       dtype=torch.float64)
	q_value: tensor([[-15.2880]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34399900014862905, distance: 0.926848910847904 entropy 0.3949669301509857
epoch: 11, step: 122
	action: tensor([[ 0.0287, -0.1914,  0.2484,  0.3181, -0.2220,  0.6437, -0.6791]],
       dtype=torch.float64)
	q_value: tensor([[-14.2716]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5152037892618043, distance: 0.7967761055489324 entropy 0.3949669301509857
epoch: 11, step: 123
	action: tensor([[ 0.2185,  0.0687, -0.1168,  0.5475, -0.0555,  0.1256, -0.3585]],
       dtype=torch.float64)
	q_value: tensor([[-16.1410]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.712932668455914, distance: 0.61312437133488 entropy 0.3949669301509857
epoch: 11, step: 124
	action: tensor([[ 0.0035, -0.2096, -0.1709, -0.2027, -0.6652,  0.7631, -0.3311]],
       dtype=torch.float64)
	q_value: tensor([[-14.0011]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10297195321544994, distance: 1.0838263473050622 entropy 0.3949669301509857
epoch: 11, step: 125
	action: tensor([[ 0.1104, -0.0165,  0.1686, -0.1749, -0.0138, -0.2095,  0.0542]],
       dtype=torch.float64)
	q_value: tensor([[-16.6306]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23874259883087534, distance: 0.9984411165337823 entropy 0.3949669301509857
epoch: 11, step: 126
	action: tensor([[ 0.3859, -0.1502, -0.2606, -0.1517, -0.2921,  0.0687,  0.2124]],
       dtype=torch.float64)
	q_value: tensor([[-13.6711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.414810941258691, distance: 0.8753964720849247 entropy 0.3949669301509857
epoch: 11, step: 127
	action: tensor([[-0.4223, -0.5239,  0.2795, -0.4166, -0.3856,  0.1306, -0.2355]],
       dtype=torch.float64)
	q_value: tensor([[-14.3997]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7884366445007693, distance: 1.530359532948594 entropy 0.3949669301509857
LOSS epoch 11 actor 120.95871943293427 critic 38.70962351279056 
epoch: 12, step: 0
	action: tensor([[-0.2705,  0.0600,  0.2418, -0.5200, -0.3101, -0.3891, -0.0190]],
       dtype=torch.float64)
	q_value: tensor([[-14.0138]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2803855845361234, distance: 1.294872719553719 entropy 0.2896064221858978
epoch: 12, step: 1
	action: tensor([[-0.5596, -0.4606, -0.3390, -0.2020,  0.0777,  0.3879, -0.2129]],
       dtype=torch.float64)
	q_value: tensor([[-12.9888]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.653827789761146, distance: 1.471640854600332 entropy 0.2896064221858978
epoch: 12, step: 2
	action: tensor([[-0.2950, -0.0513, -0.0932, -0.0850, -0.5604,  0.1465,  0.7506]],
       dtype=torch.float64)
	q_value: tensor([[-12.3302]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11119251526124896, distance: 1.206288942844293 entropy 0.2896064221858978
epoch: 12, step: 3
	action: tensor([[ 0.0867, -0.2935, -0.2101,  0.1228,  0.0298, -0.0053,  0.0619]],
       dtype=torch.float64)
	q_value: tensor([[-14.5495]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15990740296362482, distance: 1.048866638909392 entropy 0.2896064221858978
epoch: 12, step: 4
	action: tensor([[ 0.0990, -0.0930, -0.2688, -0.4586,  0.1936, -0.8117, -0.5286]],
       dtype=torch.float64)
	q_value: tensor([[-11.5854]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03836344428428207, distance: 1.1221790986334075 entropy 0.2896064221858978
epoch: 12, step: 5
	action: tensor([[ 0.4261, -0.4094,  0.3897,  0.7559,  0.0208,  0.0921, -0.2038]],
       dtype=torch.float64)
	q_value: tensor([[-14.0955]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7789705007599796, distance: 0.5379994270382963 entropy 0.2896064221858978
epoch: 12, step: 6
	action: tensor([[ 0.2002, -0.3854, -0.2086,  0.4872, -0.0542,  0.8599,  0.0789]],
       dtype=torch.float64)
	q_value: tensor([[-16.2987]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5928196929612207, distance: 0.7302138669049266 entropy 0.2896064221858978
epoch: 12, step: 7
	action: tensor([[ 0.0988,  0.2942,  0.4213,  0.2416, -0.7051, -0.1768, -0.5251]],
       dtype=torch.float64)
	q_value: tensor([[-14.5919]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6993418365085103, distance: 0.6274703277738248 entropy 0.2896064221858978
epoch: 12, step: 8
	action: tensor([[ 0.5481, -0.4119, -0.1256,  0.0472, -0.2970, -0.2965, -0.2656]],
       dtype=torch.float64)
	q_value: tensor([[-14.8958]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2886524152656781, distance: 0.9651562423382674 entropy 0.2896064221858978
epoch: 12, step: 9
	action: tensor([[ 0.5326,  0.5004, -0.6071, -0.6485, -0.4634, -0.2411,  0.1646]],
       dtype=torch.float64)
	q_value: tensor([[-13.9782]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8383964325474187, distance: 0.46002577445690007 entropy 0.2896064221858978
epoch: 12, step: 10
	action: tensor([[-0.6452, -0.2149,  0.0384, -0.1969,  0.1548, -0.4742, -0.3878]],
       dtype=torch.float64)
	q_value: tensor([[-15.2263]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7838390404036342, distance: 1.528391189528632 entropy 0.2896064221858978
epoch: 12, step: 11
	action: tensor([[ 0.3357, -0.1152,  0.1130, -0.2611, -0.4213, -0.3910, -0.0086]],
       dtype=torch.float64)
	q_value: tensor([[-12.5882]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23752031772856042, distance: 0.9992423476536417 entropy 0.2896064221858978
epoch: 12, step: 12
	action: tensor([[-0.0305, -0.4767, -0.1631,  0.4054, -0.1492, -0.3015, -0.7947]],
       dtype=torch.float64)
	q_value: tensor([[-13.4267]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.003002687280962979, distance: 1.1460610202062136 entropy 0.2896064221858978
epoch: 12, step: 13
	action: tensor([[-0.0458, -0.5667, -0.1603,  0.1146, -0.0866, -0.0865,  0.0326]],
       dtype=torch.float64)
	q_value: tensor([[-14.2015]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1854682060145172, distance: 1.2459529671406073 entropy 0.2896064221858978
epoch: 12, step: 14
	action: tensor([[-0.3615, -0.1098, -0.1370,  0.1850, -0.1661,  0.2410,  0.1478]],
       dtype=torch.float64)
	q_value: tensor([[-12.5264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06405286088178652, distance: 1.1804247183240986 entropy 0.2896064221858978
epoch: 12, step: 15
	action: tensor([[ 0.2431, -0.8025,  0.1857, -0.3898, -0.6414, -0.1126, -0.2096]],
       dtype=torch.float64)
	q_value: tensor([[-11.7112]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5452576563175491, distance: 1.422515952209957 entropy 0.2896064221858978
epoch: 12, step: 16
	action: tensor([[-0.3080,  0.3483,  0.1190, -0.6404,  0.5122,  0.0951,  0.1387]],
       dtype=torch.float64)
	q_value: tensor([[-15.6112]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06200765749979209, distance: 1.1792897325016953 entropy 0.2896064221858978
epoch: 12, step: 17
	action: tensor([[-0.2625, -0.7369, -0.0200, -0.3012, -0.3498,  0.3398,  0.1781]],
       dtype=torch.float64)
	q_value: tensor([[-12.4710]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6842088476443338, distance: 1.4850964690669404 entropy 0.2896064221858978
epoch: 12, step: 18
	action: tensor([[-0.3532, -0.2797, -0.0688, -0.0906, -0.1995,  0.4580,  0.2824]],
       dtype=torch.float64)
	q_value: tensor([[-14.2828]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2719674575627178, distance: 1.2906090122341056 entropy 0.2896064221858978
epoch: 12, step: 19
	action: tensor([[ 0.3240, -0.2179, -0.2957, -0.0513, -0.0041, -0.2999,  0.2812]],
       dtype=torch.float64)
	q_value: tensor([[-12.9197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28354776206971477, distance: 0.9686130480386544 entropy 0.2896064221858978
epoch: 12, step: 20
	action: tensor([[ 0.1945, -0.2613,  0.2149,  0.0501,  0.6910,  0.0325, -0.2274]],
       dtype=torch.float64)
	q_value: tensor([[-12.9123]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2952135989829544, distance: 0.960694824632934 entropy 0.2896064221858978
epoch: 12, step: 21
	action: tensor([[-0.2681,  0.2398, -0.2570,  0.0861, -0.2582,  0.0529,  0.3363]],
       dtype=torch.float64)
	q_value: tensor([[-13.5764]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20567715467494896, distance: 1.019894429991654 entropy 0.2896064221858978
epoch: 12, step: 22
	action: tensor([[-0.1383,  0.4508, -0.8113,  0.1400, -0.5569,  0.2360,  0.5875]],
       dtype=torch.float64)
	q_value: tensor([[-11.5657]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3396179667447987, distance: 0.9299386920258296 entropy 0.2896064221858978
epoch: 12, step: 23
	action: tensor([[-0.1330, -0.0656, -0.1126,  0.0581,  0.2116, -0.1947,  0.1285]],
       dtype=torch.float64)
	q_value: tensor([[-14.4475]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0891189700808086, distance: 1.0921631571500159 entropy 0.2896064221858978
epoch: 12, step: 24
	action: tensor([[-0.2363,  0.0131,  0.4390, -0.1095,  0.3177,  0.0716,  0.3838]],
       dtype=torch.float64)
	q_value: tensor([[-11.7194]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.017149351632877563, distance: 1.154114922914817 entropy 0.2896064221858978
epoch: 12, step: 25
	action: tensor([[ 0.5309,  0.0721,  0.0079, -0.3663,  0.3753, -0.1770,  0.5580]],
       dtype=torch.float64)
	q_value: tensor([[-13.9174]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5169132221346275, distance: 0.7953701147206248 entropy 0.2896064221858978
epoch: 12, step: 26
	action: tensor([[-0.2578,  0.3677, -0.3072, -0.1006,  0.1047, -0.1995, -0.3383]],
       dtype=torch.float64)
	q_value: tensor([[-15.1705]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24380233168559762, distance: 0.9951174919242972 entropy 0.2896064221858978
epoch: 12, step: 27
	action: tensor([[ 0.3137, -0.0859, -0.2631,  0.3074,  0.2842,  0.1291,  0.0098]],
       dtype=torch.float64)
	q_value: tensor([[-10.3541]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6312608450943014, distance: 0.6948904150237432 entropy 0.2896064221858978
epoch: 12, step: 28
	action: tensor([[ 0.2431, -0.0710, -0.1252,  0.0452,  0.1789,  0.2546,  0.0853]],
       dtype=torch.float64)
	q_value: tensor([[-12.3381]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5339999108560594, distance: 0.7811774410023512 entropy 0.2896064221858978
epoch: 12, step: 29
	action: tensor([[ 0.3251,  0.8306, -0.2748,  0.1816,  0.4517,  0.4460,  0.1518]],
       dtype=torch.float64)
	q_value: tensor([[-11.7527]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 12, step: 30
	action: tensor([[ 0.3384,  0.3478, -0.4359, -0.1869,  0.9791, -0.1932, -0.0475]],
       dtype=torch.float64)
	q_value: tensor([[-17.5000]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.718403707895988, distance: 0.6072536854744564 entropy 0.2896064221858978
epoch: 12, step: 31
	action: tensor([[ 0.2461, -0.1462,  0.1057, -0.2328, -0.1770, -0.1230,  0.0933]],
       dtype=torch.float64)
	q_value: tensor([[-15.0026]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21570070936114571, distance: 1.0134389795097996 entropy 0.2896064221858978
epoch: 12, step: 32
	action: tensor([[-0.0938,  0.1625, -0.3416,  0.3261,  0.0158,  0.2325,  0.2193]],
       dtype=torch.float64)
	q_value: tensor([[-12.4597]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4132329551242555, distance: 0.8765759486166216 entropy 0.2896064221858978
epoch: 12, step: 33
	action: tensor([[ 0.4527, -0.1527,  0.2763, -0.3419,  0.3447,  0.3310, -0.4241]],
       dtype=torch.float64)
	q_value: tensor([[-11.4203]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.40864706743593715, distance: 0.879994728754189 entropy 0.2896064221858978
epoch: 12, step: 34
	action: tensor([[-0.6335, -0.1147, -0.2013,  0.0329, -0.2758,  0.2433, -0.0969]],
       dtype=torch.float64)
	q_value: tensor([[-13.3389]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.477788969041532, distance: 1.3911145835124337 entropy 0.2896064221858978
epoch: 12, step: 35
	action: tensor([[ 0.8048,  0.0053, -0.3220, -0.2630,  0.3877,  0.0016,  0.1972]],
       dtype=torch.float64)
	q_value: tensor([[-11.6529]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6265866160435218, distance: 0.6992808469764225 entropy 0.2896064221858978
epoch: 12, step: 36
	action: tensor([[ 0.1729, -0.3794,  0.1195, -0.0119,  0.4757,  0.0684,  0.0059]],
       dtype=torch.float64)
	q_value: tensor([[-15.0624]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1495519709654164, distance: 1.0553112861068892 entropy 0.2896064221858978
epoch: 12, step: 37
	action: tensor([[-0.4295, -0.5925, -0.7500,  0.1423,  0.2778, -0.3456, -0.2603]],
       dtype=torch.float64)
	q_value: tensor([[-13.2673]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6296846475206539, distance: 1.460859605246936 entropy 0.2896064221858978
epoch: 12, step: 38
	action: tensor([[ 0.5584, -0.0907, -0.6905, -0.3080,  0.3002,  1.1974,  0.0046]],
       dtype=torch.float64)
	q_value: tensor([[-13.8077]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7747811009807196, distance: 0.5430741220988853 entropy 0.2896064221858978
epoch: 12, step: 39
	action: tensor([[-0.0043,  0.3227, -0.0814, -0.5556,  0.0279,  0.1861, -0.1645]],
       dtype=torch.float64)
	q_value: tensor([[-17.0892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38882643074610634, distance: 0.8946207674484112 entropy 0.2896064221858978
epoch: 12, step: 40
	action: tensor([[ 0.2542, -0.0450, -0.3512, -0.5639,  0.2832, -0.0350,  0.0951]],
       dtype=torch.float64)
	q_value: tensor([[-11.5588]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2804670254742183, distance: 0.9706933267704928 entropy 0.2896064221858978
epoch: 12, step: 41
	action: tensor([[-0.4444,  0.0902, -0.7136, -0.1736, -0.1943,  0.1178, -0.3153]],
       dtype=torch.float64)
	q_value: tensor([[-12.2081]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10775999398555047, distance: 1.2044243626816657 entropy 0.2896064221858978
epoch: 12, step: 42
	action: tensor([[ 0.3792, -0.7676, -0.0151,  0.0703,  0.2267,  0.3588, -0.2509]],
       dtype=torch.float64)
	q_value: tensor([[-10.4937]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.035909601331707286, distance: 1.1236099390304497 entropy 0.2896064221858978
epoch: 12, step: 43
	action: tensor([[-0.0609,  0.0021, -0.2213, -0.7725, -0.0200,  0.3720, -0.2981]],
       dtype=torch.float64)
	q_value: tensor([[-14.3752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09357164359601067, distance: 1.089490468072245 entropy 0.2896064221858978
epoch: 12, step: 44
	action: tensor([[ 0.3723,  0.1301,  0.3514,  0.1487, -0.1544, -0.2248,  0.4589]],
       dtype=torch.float64)
	q_value: tensor([[-12.2611]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.719317954611016, distance: 0.6062671116890773 entropy 0.2896064221858978
epoch: 12, step: 45
	action: tensor([[ 0.0357, -0.0702,  0.0896,  0.0421,  0.2935,  0.1311,  0.5025]],
       dtype=torch.float64)
	q_value: tensor([[-14.3771]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35429532355129956, distance: 0.9195464225820362 entropy 0.2896064221858978
epoch: 12, step: 46
	action: tensor([[ 0.3865,  0.0079, -0.4114, -0.0792,  0.4092,  0.4244, -0.0220]],
       dtype=torch.float64)
	q_value: tensor([[-13.3959]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6829121003820893, distance: 0.6443866403459083 entropy 0.2896064221858978
epoch: 12, step: 47
	action: tensor([[-0.1837,  0.1419,  0.0085, -0.4115,  0.4507, -0.2105,  0.0770]],
       dtype=torch.float64)
	q_value: tensor([[-12.4871]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.025543425708998813, distance: 1.1588673327169308 entropy 0.2896064221858978
epoch: 12, step: 48
	action: tensor([[-0.2030,  0.1919, -0.6112, -0.3754,  0.1183,  0.3335,  0.2554]],
       dtype=torch.float64)
	q_value: tensor([[-11.6342]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24883308690436667, distance: 0.9918018598416262 entropy 0.2896064221858978
epoch: 12, step: 49
	action: tensor([[ 0.3618,  0.4843, -0.6229, -0.0814,  0.0390, -0.1415, -0.3142]],
       dtype=torch.float64)
	q_value: tensor([[-11.9857]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8376357994244246, distance: 0.46110712456081787 entropy 0.2896064221858978
epoch: 12, step: 50
	action: tensor([[ 0.0095, -0.0068,  0.2695,  0.3081, -0.1389, -0.0710, -0.6735]],
       dtype=torch.float64)
	q_value: tensor([[-12.1980]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5031938048623086, distance: 0.8065850993182077 entropy 0.2896064221858978
epoch: 12, step: 51
	action: tensor([[-0.4474,  0.4576, -0.8694, -0.2954,  0.2726,  0.5056, -0.4918]],
       dtype=torch.float64)
	q_value: tensor([[-13.1202]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21459791768085557, distance: 1.0141512200408753 entropy 0.2896064221858978
epoch: 12, step: 52
	action: tensor([[ 0.5798, -0.4918, -0.0533, -0.2402, -0.2870,  0.0572,  0.0576]],
       dtype=torch.float64)
	q_value: tensor([[-12.0270]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0913243628934326, distance: 1.0908402025603496 entropy 0.2896064221858978
epoch: 12, step: 53
	action: tensor([[-0.0853, -0.3103,  0.1876, -0.9919,  0.1216,  0.6062,  0.2453]],
       dtype=torch.float64)
	q_value: tensor([[-14.3005]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3580694721995106, distance: 1.333575741161646 entropy 0.2896064221858978
epoch: 12, step: 54
	action: tensor([[-0.1753, -0.2619, -0.5253,  0.1372,  0.2972, -0.0940,  0.1880]],
       dtype=torch.float64)
	q_value: tensor([[-16.1330]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06130502126173165, distance: 1.1788995522636136 entropy 0.2896064221858978
epoch: 12, step: 55
	action: tensor([[-0.1953,  0.0096, -0.0742,  0.6288, -0.6121,  0.0163, -0.3235]],
       dtype=torch.float64)
	q_value: tensor([[-12.1376]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3641790427946653, distance: 0.9124815932290318 entropy 0.2896064221858978
epoch: 12, step: 56
	action: tensor([[ 0.7207, -0.0210,  0.1156, -0.6160, -0.2197, -0.4756,  0.0342]],
       dtype=torch.float64)
	q_value: tensor([[-12.6537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2727032955492813, distance: 0.9759161457472879 entropy 0.2896064221858978
epoch: 12, step: 57
	action: tensor([[ 3.1718e-01, -1.3653e-01,  5.1604e-04, -1.1462e-01, -1.9239e-02,
          3.1247e-01, -6.0316e-01]], dtype=torch.float64)
	q_value: tensor([[-15.7466]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5080256037679007, distance: 0.8026532046378407 entropy 0.2896064221858978
epoch: 12, step: 58
	action: tensor([[ 0.2584,  0.5745,  0.1950, -0.0707,  0.4194,  0.1602, -0.3586]],
       dtype=torch.float64)
	q_value: tensor([[-12.4003]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8259067998686003, distance: 0.4774716578572179 entropy 0.2896064221858978
epoch: 12, step: 59
	action: tensor([[ 0.8002, -0.0523, -0.3035,  0.1629,  0.3779, -0.1268, -0.3481]],
       dtype=torch.float64)
	q_value: tensor([[-12.6027]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7451486810616808, distance: 0.5776970316604987 entropy 0.2896064221858978
epoch: 12, step: 60
	action: tensor([[ 0.3835, -0.1752,  0.1604,  0.3254, -0.1083,  0.5984,  0.0716]],
       dtype=torch.float64)
	q_value: tensor([[-14.8478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7968197276117797, distance: 0.5158191510842736 entropy 0.2896064221858978
epoch: 12, step: 61
	action: tensor([[ 0.2048,  0.2762, -0.1629,  0.1345,  0.0012,  0.5807,  0.1048]],
       dtype=torch.float64)
	q_value: tensor([[-14.1574]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7660335230069422, distance: 0.5535202465986322 entropy 0.2896064221858978
epoch: 12, step: 62
	action: tensor([[ 0.0512, -0.3628, -0.6556, -0.2674, -0.3689, -0.0931, -0.1965]],
       dtype=torch.float64)
	q_value: tensor([[-12.3024]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04743855401046926, distance: 1.1168714597001737 entropy 0.2896064221858978
epoch: 12, step: 63
	action: tensor([[-0.2956,  0.5751, -0.0083, -0.2099,  0.0574,  0.4235, -0.1514]],
       dtype=torch.float64)
	q_value: tensor([[-12.0938]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34808762189278597, distance: 0.9239560358440411 entropy 0.2896064221858978
epoch: 12, step: 64
	action: tensor([[-0.1754, -0.2039, -0.4557,  0.3507,  0.5860,  0.2839, -0.0617]],
       dtype=torch.float64)
	q_value: tensor([[-12.0330]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12902061929482678, distance: 1.0679738777992571 entropy 0.2896064221858978
epoch: 12, step: 65
	action: tensor([[-0.2071,  0.1308, -0.0560,  0.1678, -0.1277,  0.1257,  0.3809]],
       dtype=torch.float64)
	q_value: tensor([[-13.0781]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25961603405873546, distance: 0.984657505713575 entropy 0.2896064221858978
epoch: 12, step: 66
	action: tensor([[ 0.5847,  0.1355, -0.0086,  0.2431, -0.5720,  0.3764,  0.0418]],
       dtype=torch.float64)
	q_value: tensor([[-12.0814]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8873063029689874, distance: 0.38415501451715856 entropy 0.2896064221858978
epoch: 12, step: 67
	action: tensor([[ 0.0576,  0.1136,  0.2496, -0.5070,  0.3384,  0.6689,  0.4684]],
       dtype=torch.float64)
	q_value: tensor([[-14.5554]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4087333564536243, distance: 0.87993052289102 entropy 0.2896064221858978
epoch: 12, step: 68
	action: tensor([[-0.3017,  0.3535,  0.4054,  0.0418,  0.1067, -0.1187,  0.0010]],
       dtype=torch.float64)
	q_value: tensor([[-15.4061]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1950751710274794, distance: 1.026678234862872 entropy 0.2896064221858978
epoch: 12, step: 69
	action: tensor([[ 0.4448,  0.5419,  0.0706,  0.1964,  0.1700,  0.1070, -0.0487]],
       dtype=torch.float64)
	q_value: tensor([[-12.6803]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9043519481885874, distance: 0.3539115674293248 entropy 0.2896064221858978
epoch: 12, step: 70
	action: tensor([[ 0.2681, -0.3771, -0.4385,  0.2827,  0.0741, -0.2646, -0.0479]],
       dtype=torch.float64)
	q_value: tensor([[-12.7100]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2714893186526339, distance: 0.9767302878147329 entropy 0.2896064221858978
epoch: 12, step: 71
	action: tensor([[-0.3637, -0.3503, -0.5086, -0.3682,  0.1702,  0.0772,  0.1175]],
       dtype=torch.float64)
	q_value: tensor([[-13.0528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3541053274579704, distance: 1.3316279944644729 entropy 0.2896064221858978
epoch: 12, step: 72
	action: tensor([[-0.1486,  0.2214,  0.5749, -0.3290,  0.1605,  0.1744, -0.3386]],
       dtype=torch.float64)
	q_value: tensor([[-11.7378]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.156781647566562, distance: 1.050816100568478 entropy 0.2896064221858978
epoch: 12, step: 73
	action: tensor([[ 0.0293,  0.2303, -0.0590, -0.2973, -0.2341,  0.2616,  0.4779]],
       dtype=torch.float64)
	q_value: tensor([[-13.3547]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4162997525871588, distance: 0.8742821910073331 entropy 0.2896064221858978
epoch: 12, step: 74
	action: tensor([[-0.3059, -0.0888, -0.0403,  0.4161, -0.4758,  0.1053, -0.2049]],
       dtype=torch.float64)
	q_value: tensor([[-13.2449]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14062298527304218, distance: 1.0608367591696555 entropy 0.2896064221858978
epoch: 12, step: 75
	action: tensor([[-0.1928,  0.1274, -0.0040,  0.7546, -0.2439,  0.6150,  0.5422]],
       dtype=torch.float64)
	q_value: tensor([[-11.8647]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5421466718451486, distance: 0.7743189376179005 entropy 0.2896064221858978
epoch: 12, step: 76
	action: tensor([[-0.5568,  0.1283, -0.6620,  0.1201, -0.1099,  0.3164, -0.6665]],
       dtype=torch.float64)
	q_value: tensor([[-15.1869]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.25381233708392914, distance: 1.281365311148822 entropy 0.2896064221858978
epoch: 12, step: 77
	action: tensor([[ 0.0096, -0.1981, -0.0050,  0.0601,  0.5137,  0.6662, -0.3156]],
       dtype=torch.float64)
	q_value: tensor([[-11.5172]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4280952734551543, distance: 0.8654032785567651 entropy 0.2896064221858978
epoch: 12, step: 78
	action: tensor([[-0.0568, -0.8269,  0.2864,  0.3614, -0.0490, -0.1888, -0.3099]],
       dtype=torch.float64)
	q_value: tensor([[-12.8644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2762822978642807, distance: 1.292796197552313 entropy 0.2896064221858978
epoch: 12, step: 79
	action: tensor([[-0.0183,  0.1324,  0.1445, -0.0814, -0.1330, -0.1063,  0.3044]],
       dtype=torch.float64)
	q_value: tensor([[-15.3411]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31995038122829444, distance: 0.9436848741459112 entropy 0.2896064221858978
epoch: 12, step: 80
	action: tensor([[ 0.3579,  0.3954, -0.2750,  0.2092, -0.1961, -0.0165, -0.3208]],
       dtype=torch.float64)
	q_value: tensor([[-12.1639]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8185901945688556, distance: 0.48740173637853385 entropy 0.2896064221858978
epoch: 12, step: 81
	action: tensor([[ 0.0105,  0.0177, -0.0629, -0.0283,  0.0427,  0.2434,  0.3894]],
       dtype=torch.float64)
	q_value: tensor([[-11.8814]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38148846265377045, distance: 0.8999753114405981 entropy 0.2896064221858978
epoch: 12, step: 82
	action: tensor([[-0.0649, -0.5204,  0.3039, -0.1391,  0.0785, -0.6320, -0.0128]],
       dtype=torch.float64)
	q_value: tensor([[-12.1438]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.47239737788161973, distance: 1.3885745812414902 entropy 0.2896064221858978
epoch: 12, step: 83
	action: tensor([[ 0.0869,  0.6651, -0.1900, -0.1260,  0.4162,  0.6416,  0.2023]],
       dtype=torch.float64)
	q_value: tensor([[-14.9855]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7711144385126533, distance: 0.5474770166482532 entropy 0.2896064221858978
epoch: 12, step: 84
	action: tensor([[0.2139, 0.4062, 0.0082, 0.2755, 0.2182, 0.2277, 0.2474]],
       dtype=torch.float64)
	q_value: tensor([[-13.4288]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7984667098969753, distance: 0.5137242783549648 entropy 0.2896064221858978
epoch: 12, step: 85
	action: tensor([[ 0.0080, -0.0998,  0.0788,  0.3593,  0.1143,  0.3216,  0.2076]],
       dtype=torch.float64)
	q_value: tensor([[-12.6536]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5087201309974345, distance: 0.8020864460881096 entropy 0.2896064221858978
epoch: 12, step: 86
	action: tensor([[ 0.0281, -0.4439,  0.1923,  0.2830, -0.1024,  0.5923,  0.0329]],
       dtype=torch.float64)
	q_value: tensor([[-12.8779]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3394164301647493, distance: 0.9300805813848309 entropy 0.2896064221858978
epoch: 12, step: 87
	action: tensor([[-0.3592,  0.1069, -0.6877, -0.6092,  0.4311,  0.2489,  0.0167]],
       dtype=torch.float64)
	q_value: tensor([[-14.2858]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07504737646843607, distance: 1.1005668758310285 entropy 0.2896064221858978
epoch: 12, step: 88
	action: tensor([[ 0.0935,  0.7758, -0.0961, -0.0376,  0.2862, -0.1569, -0.0593]],
       dtype=torch.float64)
	q_value: tensor([[-11.9866]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.69273830819059, distance: 0.6343236473171693 entropy 0.2896064221858978
epoch: 12, step: 89
	action: tensor([[-0.0969,  0.3802,  0.4903, -0.4018,  0.3580, -0.3099, -0.3247]],
       dtype=torch.float64)
	q_value: tensor([[-12.1020]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11666707295214507, distance: 1.075521020836721 entropy 0.2896064221858978
epoch: 12, step: 90
	action: tensor([[-0.2395, -0.0620, -0.0125, -0.1369, -0.2847,  0.3854, -0.4266]],
       dtype=torch.float64)
	q_value: tensor([[-13.1985]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.026335792051888074, distance: 1.1593149345196638 entropy 0.2896064221858978
epoch: 12, step: 91
	action: tensor([[-0.0755, -0.2546, -0.4408,  0.4398, -0.1237,  0.1351, -0.3722]],
       dtype=torch.float64)
	q_value: tensor([[-11.8041]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1485254179042329, distance: 1.0559480130295569 entropy 0.2896064221858978
epoch: 12, step: 92
	action: tensor([[ 0.0066,  0.2213,  0.2292,  0.1954,  0.1297,  0.0717, -0.5162]],
       dtype=torch.float64)
	q_value: tensor([[-11.7744]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5523431769746836, distance: 0.765648255596985 entropy 0.2896064221858978
epoch: 12, step: 93
	action: tensor([[ 0.6916, -0.0749,  0.1056, -0.0687,  0.1711, -0.1652, -0.5586]],
       dtype=torch.float64)
	q_value: tensor([[-12.3244]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5813552114074105, distance: 0.7404223811749279 entropy 0.2896064221858978
epoch: 12, step: 94
	action: tensor([[ 0.2711, -0.2249, -0.1672, -0.3708, -0.0992, -0.0403,  0.1138]],
       dtype=torch.float64)
	q_value: tensor([[-14.4881]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15721326912982114, distance: 1.050547123415457 entropy 0.2896064221858978
epoch: 12, step: 95
	action: tensor([[ 0.1006,  0.0405, -0.5573, -0.3848, -0.0975, -0.4047, -0.1982]],
       dtype=torch.float64)
	q_value: tensor([[-12.4019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37064686093977217, distance: 0.9078286712645632 entropy 0.2896064221858978
epoch: 12, step: 96
	action: tensor([[ 0.0463,  0.3526,  0.0036,  0.2172, -0.6545,  0.1067,  0.2041]],
       dtype=torch.float64)
	q_value: tensor([[-11.6040]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6172693385512991, distance: 0.7079511983966885 entropy 0.2896064221858978
epoch: 12, step: 97
	action: tensor([[ 0.2035, -0.7296, -0.2355, -0.2333,  0.0628, -0.1936,  0.3362]],
       dtype=torch.float64)
	q_value: tensor([[-13.3496]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3824073472164664, distance: 1.3454721414066126 entropy 0.2896064221858978
epoch: 12, step: 98
	action: tensor([[-0.5427,  0.1750, -0.5122,  0.0161,  0.2036, -0.6138, -0.3735]],
       dtype=torch.float64)
	q_value: tensor([[-14.4664]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.19193153380452577, distance: 1.2493448994219125 entropy 0.2896064221858978
epoch: 12, step: 99
	action: tensor([[ 0.1857, -0.1068, -0.4012, -0.1036,  0.3541,  0.4768, -0.1289]],
       dtype=torch.float64)
	q_value: tensor([[-12.3094]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.463260159683316, distance: 0.8383755602909724 entropy 0.2896064221858978
epoch: 12, step: 100
	action: tensor([[ 0.5521,  0.3707, -0.4486, -0.3217, -0.2686,  0.0990, -0.0480]],
       dtype=torch.float64)
	q_value: tensor([[-11.7587]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8359594470665177, distance: 0.46348139507763164 entropy 0.2896064221858978
epoch: 12, step: 101
	action: tensor([[ 0.2748, -0.5253,  0.4795,  0.2655, -1.2072,  0.0952, -0.3042]],
       dtype=torch.float64)
	q_value: tensor([[-13.1711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30819422057010404, distance: 0.9518067627082568 entropy 0.2896064221858978
epoch: 12, step: 102
	action: tensor([[ 0.5568,  0.2412,  0.5950,  0.4942, -0.0210, -0.2055, -0.0906]],
       dtype=torch.float64)
	q_value: tensor([[-17.6770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9598262103990078, distance: 0.22936549976027265 entropy 0.2896064221858978
epoch: 12, step: 103
	action: tensor([[-0.0048, -0.3160, -0.4528,  0.1656, -0.0303, -0.0144, -0.0219]],
       dtype=torch.float64)
	q_value: tensor([[-15.9624]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.112358575099061, distance: 1.0781407823751192 entropy 0.2896064221858978
epoch: 12, step: 104
	action: tensor([[-0.3966, -0.2007, -0.4484, -0.1051, -0.4239,  0.1517, -0.3524]],
       dtype=torch.float64)
	q_value: tensor([[-11.3341]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3026876014937476, distance: 1.3061012139977821 entropy 0.2896064221858978
epoch: 12, step: 105
	action: tensor([[ 0.4283, -0.1964, -0.3648, -0.2818, -0.4976,  0.5359, -0.7589]],
       dtype=torch.float64)
	q_value: tensor([[-11.1042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.43374316297034377, distance: 0.861119497489711 entropy 0.2896064221858978
epoch: 12, step: 106
	action: tensor([[ 0.1904,  0.5056,  0.0416, -0.6106, -0.0584, -0.2315,  0.3556]],
       dtype=torch.float64)
	q_value: tensor([[-14.2877]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5477388765425237, distance: 0.7695756572075919 entropy 0.2896064221858978
epoch: 12, step: 107
	action: tensor([[ 0.0650,  0.0793, -0.5428, -0.7799,  0.3098, -0.5649,  0.2164]],
       dtype=torch.float64)
	q_value: tensor([[-13.7438]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2125454838027946, distance: 1.0154754591143416 entropy 0.2896064221858978
epoch: 12, step: 108
	action: tensor([[-0.1811,  0.1331, -0.4263,  0.0454,  0.1143,  0.1123,  0.0935]],
       dtype=torch.float64)
	q_value: tensor([[-13.3815]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25409642324139003, distance: 0.9883210334368686 entropy 0.2896064221858978
epoch: 12, step: 109
	action: tensor([[ 0.4358,  0.1006, -0.0680, -0.2405,  0.3071,  0.0100,  0.3048]],
       dtype=torch.float64)
	q_value: tensor([[-10.5520]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6103736725680853, distance: 0.7143003134801023 entropy 0.2896064221858978
epoch: 12, step: 110
	action: tensor([[ 1.0213,  0.2047, -0.0658, -0.0820, -0.1062, -0.0846,  0.0241]],
       dtype=torch.float64)
	q_value: tensor([[-13.2062]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8082833804252391, distance: 0.5010563548703741 entropy 0.2896064221858978
epoch: 12, step: 111
	action: tensor([[-0.5982, -0.0121,  0.1692,  0.5622, -0.2077,  0.2207,  0.0493]],
       dtype=torch.float64)
	q_value: tensor([[-15.7362]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04300549751835958, distance: 1.119467306512494 entropy 0.2896064221858978
epoch: 12, step: 112
	action: tensor([[ 0.4678,  0.4142, -0.5190, -0.1868, -0.0570,  0.1865,  0.0021]],
       dtype=torch.float64)
	q_value: tensor([[-13.3585]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8584159066432715, distance: 0.4305899858797965 entropy 0.2896064221858978
epoch: 12, step: 113
	action: tensor([[-0.1350, -0.3720,  0.1202,  0.5407,  0.2403, -0.3868,  0.0216]],
       dtype=torch.float64)
	q_value: tensor([[-12.5245]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09287591192823796, distance: 1.0899085086271814 entropy 0.2896064221858978
epoch: 12, step: 114
	action: tensor([[ 0.1490, -0.1076, -0.1272, -0.1231, -0.1812, -0.2707,  0.3515]],
       dtype=torch.float64)
	q_value: tensor([[-14.6998]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23876511088403995, distance: 0.9984263533763809 entropy 0.2896064221858978
epoch: 12, step: 115
	action: tensor([[-0.4715, -0.3479, -0.1470,  0.2206, -0.0571, -0.2292,  0.0309]],
       dtype=torch.float64)
	q_value: tensor([[-12.5189]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.422931323320495, distance: 1.3650503263000295 entropy 0.2896064221858978
epoch: 12, step: 116
	action: tensor([[ 0.2283, -0.1223,  0.1641,  0.3606,  0.1039, -0.5714,  0.1512]],
       dtype=torch.float64)
	q_value: tensor([[-12.4984]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4592319740763048, distance: 0.8415156474494417 entropy 0.2896064221858978
epoch: 12, step: 117
	action: tensor([[-0.3723, -0.0515, -0.3700,  0.1853,  0.1239, -0.2116, -0.1487]],
       dtype=torch.float64)
	q_value: tensor([[-14.6185]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12467564219587146, distance: 1.2135853858981813 entropy 0.2896064221858978
epoch: 12, step: 118
	action: tensor([[ 0.1503,  0.2618, -0.4665, -0.1583,  0.5758,  0.2522, -0.4600]],
       dtype=torch.float64)
	q_value: tensor([[-11.2776]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6258368221593009, distance: 0.6999825539706668 entropy 0.2896064221858978
epoch: 12, step: 119
	action: tensor([[-0.1949,  0.3205, -0.2961,  0.1895,  0.2510, -0.2596,  0.1616]],
       dtype=torch.float64)
	q_value: tensor([[-11.6374]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3205012991460152, distance: 0.9433025503953087 entropy 0.2896064221858978
epoch: 12, step: 120
	action: tensor([[-0.0600, -0.3870,  0.0768, -0.4911, -0.3352,  0.3135,  0.0431]],
       dtype=torch.float64)
	q_value: tensor([[-11.6716]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.280471851287726, distance: 1.2949163402345234 entropy 0.2896064221858978
epoch: 12, step: 121
	action: tensor([[ 0.4553, -0.3165, -0.2841,  0.0092, -0.1961,  0.2917,  0.0895]],
       dtype=torch.float64)
	q_value: tensor([[-13.6008]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42225352729855126, distance: 0.8698118992040116 entropy 0.2896064221858978
epoch: 12, step: 122
	action: tensor([[ 0.7874, -0.0482, -0.4194, -0.3107,  0.1968, -0.1762,  0.3504]],
       dtype=torch.float64)
	q_value: tensor([[-12.8098]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5082916872270377, distance: 0.8024361185168633 entropy 0.2896064221858978
epoch: 12, step: 123
	action: tensor([[-0.1527,  0.1942, -0.5421, -0.1101,  0.0328,  0.0196, -0.0284]],
       dtype=torch.float64)
	q_value: tensor([[-15.2994]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30703971726316603, distance: 0.9526006314621414 entropy 0.2896064221858978
epoch: 12, step: 124
	action: tensor([[-0.0502, -0.0596,  0.1036,  0.0117, -0.0411, -0.2798, -0.1779]],
       dtype=torch.float64)
	q_value: tensor([[-10.1003]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17291938036907606, distance: 1.0407121275781823 entropy 0.2896064221858978
epoch: 12, step: 125
	action: tensor([[ 0.0724,  0.2581, -0.2042,  0.1777, -0.1529,  0.2257, -0.1867]],
       dtype=torch.float64)
	q_value: tensor([[-11.6332]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5986375074913197, distance: 0.7249784307094632 entropy 0.2896064221858978
epoch: 12, step: 126
	action: tensor([[-0.1164,  0.0496, -0.2631, -0.1700,  0.1024,  0.2230,  0.0952]],
       dtype=torch.float64)
	q_value: tensor([[-10.9053]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24029314425424453, distance: 0.9974237750797206 entropy 0.2896064221858978
epoch: 12, step: 127
	action: tensor([[ 0.5005,  0.6246,  0.0363, -0.2088, -0.1930, -0.2668, -0.0991]],
       dtype=torch.float64)
	q_value: tensor([[-10.6324]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8865655292764691, distance: 0.38541553671129225 entropy 0.2896064221858978
LOSS epoch 12 actor 96.3706400404336 critic 113.82562961192905 
epoch: 13, step: 0
	action: tensor([[-0.2434,  0.4684, -0.1022,  0.0603, -0.0726, -0.0191, -0.0355]],
       dtype=torch.float64)
	q_value: tensor([[-11.6580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37544865150855067, distance: 0.9043587990801112 entropy 0.2896064221858978
epoch: 13, step: 1
	action: tensor([[ 0.2671, -0.2165,  0.2057, -0.0492,  0.2536,  0.3420, -0.1811]],
       dtype=torch.float64)
	q_value: tensor([[-9.0611]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4358154883219193, distance: 0.8595423373205211 entropy 0.2896064221858978
epoch: 13, step: 2
	action: tensor([[ 0.2656, -0.5120, -0.6775,  0.2559, -0.1005, -0.3961, -0.2826]],
       dtype=torch.float64)
	q_value: tensor([[-10.4595]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11251169400070593, distance: 1.0780477882630433 entropy 0.2896064221858978
epoch: 13, step: 3
	action: tensor([[-0.1453, -0.0794, -0.1662,  0.1484, -0.5752,  0.5395,  0.2689]],
       dtype=torch.float64)
	q_value: tensor([[-11.4153]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18430605469013794, distance: 1.0335233968250035 entropy 0.2896064221858978
epoch: 13, step: 4
	action: tensor([[-0.2878, -0.3983, -0.0787,  0.3689, -0.3960, -0.2854,  0.1065]],
       dtype=torch.float64)
	q_value: tensor([[-10.9425]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12847738958774158, distance: 1.2156348001966497 entropy 0.2896064221858978
epoch: 13, step: 5
	action: tensor([[ 0.7280, -0.2743, -0.6549, -0.3819,  0.3037, -0.5530, -0.0611]],
       dtype=torch.float64)
	q_value: tensor([[-11.0657]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1755573483775843, distance: 1.039051129960542 entropy 0.2896064221858978
epoch: 13, step: 6
	action: tensor([[-0.3860,  0.4156,  0.1613, -0.3344,  0.2789,  0.3510, -0.0057]],
       dtype=torch.float64)
	q_value: tensor([[-13.2899]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12331734827066809, distance: 1.071464779003003 entropy 0.2896064221858978
epoch: 13, step: 7
	action: tensor([[ 0.2100, -0.2260, -0.0474, -0.2329,  0.2765, -0.2216, -0.6612]],
       dtype=torch.float64)
	q_value: tensor([[-9.9347]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09355744125451915, distance: 1.089499003360754 entropy 0.2896064221858978
epoch: 13, step: 8
	action: tensor([[-0.6297, -0.0099,  0.1055,  0.0227,  0.5457,  0.3305, -0.3412]],
       dtype=torch.float64)
	q_value: tensor([[-10.3452]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.27288881583250646, distance: 1.2910763583213813 entropy 0.2896064221858978
epoch: 13, step: 9
	action: tensor([[ 0.4229,  0.3784,  0.6844,  0.1835,  0.0628,  0.1066, -0.0643]],
       dtype=torch.float64)
	q_value: tensor([[-10.4626]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.915870188257156, distance: 0.33191864723569187 entropy 0.2896064221858978
epoch: 13, step: 10
	action: tensor([[ 0.5296, -0.6555, -0.2384,  0.4052,  0.5073,  0.0189,  0.2821]],
       dtype=torch.float64)
	q_value: tensor([[-12.9616]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32124178457280195, distance: 0.9427884271210306 entropy 0.2896064221858978
epoch: 13, step: 11
	action: tensor([[-0.2454,  0.0184, -0.0494, -0.0376,  0.5912, -0.3771,  0.5202]],
       dtype=torch.float64)
	q_value: tensor([[-13.4394]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08800657099361997, distance: 1.1936374945692234 entropy 0.2896064221858978
epoch: 13, step: 12
	action: tensor([[-0.1618,  0.0821, -0.1525,  0.1802, -0.3169,  0.6475,  0.0766]],
       dtype=torch.float64)
	q_value: tensor([[-11.8065]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33987873965198623, distance: 0.9297550659209892 entropy 0.2896064221858978
epoch: 13, step: 13
	action: tensor([[-0.1582, -0.7817, -0.2167,  0.0020, -0.0707, -0.1083, -0.3257]],
       dtype=torch.float64)
	q_value: tensor([[-10.3473]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5148301302085909, distance: 1.4084410053457082 entropy 0.2896064221858978
epoch: 13, step: 14
	action: tensor([[-0.4267,  0.0238, -0.2568, -0.1946, -0.3324, -0.0075, -0.3787]],
       dtype=torch.float64)
	q_value: tensor([[-10.8684]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20528426665273214, distance: 1.2563233655364918 entropy 0.2896064221858978
epoch: 13, step: 15
	action: tensor([[-0.2110,  0.1855, -0.1103, -0.3833, -0.0141,  0.1550, -0.1741]],
       dtype=torch.float64)
	q_value: tensor([[-8.7916]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1228915305542918, distance: 1.071724960550021 entropy 0.2896064221858978
epoch: 13, step: 16
	action: tensor([[-0.3789, -0.2351, -0.3221,  0.2188,  0.3565,  0.4402, -0.5629]],
       dtype=torch.float64)
	q_value: tensor([[-8.7605]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.062200094937608474, distance: 1.1793965722280606 entropy 0.2896064221858978
epoch: 13, step: 17
	action: tensor([[ 0.2285,  0.0165,  0.1313, -0.1102,  0.4902,  0.3224, -0.2258]],
       dtype=torch.float64)
	q_value: tensor([[-9.9877]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5565643364950686, distance: 0.7620298829854254 entropy 0.2896064221858978
epoch: 13, step: 18
	action: tensor([[-0.2439,  0.2813, -0.2343,  0.3082,  0.4636, -0.2171, -0.3222]],
       dtype=torch.float64)
	q_value: tensor([[-10.0420]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2717795032399055, distance: 0.9765357400113162 entropy 0.2896064221858978
epoch: 13, step: 19
	action: tensor([[ 0.4496,  0.8369, -0.4726, -0.2764,  0.5647,  0.0684,  0.1955]],
       dtype=torch.float64)
	q_value: tensor([[-9.9186]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.933298146486569, distance: 0.29554638009931533 entropy 0.2896064221858978
epoch: 13, step: 20
	action: tensor([[ 0.0041, -0.0524,  0.2916,  0.3155,  0.2537, -0.3628, -0.1605]],
       dtype=torch.float64)
	q_value: tensor([[-12.2780]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35062874361887664, distance: 0.922153510466468 entropy 0.2896064221858978
epoch: 13, step: 21
	action: tensor([[ 0.2675,  0.0312, -0.3458, -0.5341, -0.0453,  0.1898, -0.0711]],
       dtype=torch.float64)
	q_value: tensor([[-11.1460]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38960666384772313, distance: 0.8940495421149651 entropy 0.2896064221858978
epoch: 13, step: 22
	action: tensor([[ 0.3118,  0.0878, -0.6604,  0.6796,  0.1361, -0.1714,  0.0563]],
       dtype=torch.float64)
	q_value: tensor([[-9.9789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6313752168242446, distance: 0.6947826396742387 entropy 0.2896064221858978
epoch: 13, step: 23
	action: tensor([[-0.3405, -0.2905,  0.0284,  0.3117,  0.5003,  0.2336, -0.6614]],
       dtype=torch.float64)
	q_value: tensor([[-11.6094]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0016157378801208155, distance: 1.1434194000890583 entropy 0.2896064221858978
epoch: 13, step: 24
	action: tensor([[-0.1330,  0.7699, -0.4439, -0.5950, -0.3610, -0.4501,  0.0425]],
       dtype=torch.float64)
	q_value: tensor([[-10.8857]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.640263480707933, distance: 0.6863552458808785 entropy 0.2896064221858978
epoch: 13, step: 25
	action: tensor([[ 0.0232,  0.4762, -0.4987, -0.5614,  0.1632, -0.1311, -0.1755]],
       dtype=torch.float64)
	q_value: tensor([[-11.3049]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5970766616124341, distance: 0.7263867356830371 entropy 0.2896064221858978
epoch: 13, step: 26
	action: tensor([[ 0.3264, -0.2351, -1.1784, -0.3222,  0.1978, -0.1818, -0.0131]],
       dtype=torch.float64)
	q_value: tensor([[-9.2971]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4318632978645226, distance: 0.8625476896963579 entropy 0.2896064221858978
epoch: 13, step: 27
	action: tensor([[-0.3165, -0.3226, -0.9540,  0.5218, -0.4035,  0.0651, -0.2663]],
       dtype=torch.float64)
	q_value: tensor([[-11.2734]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.38786867800672864, distance: 1.3481272290833897 entropy 0.2896064221858978
epoch: 13, step: 28
	action: tensor([[ 0.3344, -0.3519, -0.1976, -0.6130, -0.1433, -0.4302, -0.2366]],
       dtype=torch.float64)
	q_value: tensor([[-10.6063]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09138385522937376, distance: 1.1954886458816223 entropy 0.2896064221858978
epoch: 13, step: 29
	action: tensor([[ 0.2778, -0.0313, -0.1222, -0.0697,  0.6524, -0.2942, -0.0029]],
       dtype=torch.float64)
	q_value: tensor([[-11.1849]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.40379400535877963, distance: 0.8835982813420089 entropy 0.2896064221858978
epoch: 13, step: 30
	action: tensor([[-0.2491,  0.2581, -0.3503,  0.1821,  0.2189,  0.2414, -0.2221]],
       dtype=torch.float64)
	q_value: tensor([[-11.1499]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3054717956687034, distance: 0.9536777200197784 entropy 0.2896064221858978
epoch: 13, step: 31
	action: tensor([[-0.0399,  0.0588,  0.1422, -0.4359,  0.1826,  0.4323, -0.2471]],
       dtype=torch.float64)
	q_value: tensor([[-8.7307]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22456399595799714, distance: 1.0076963236492562 entropy 0.2896064221858978
epoch: 13, step: 32
	action: tensor([[-0.2370, -0.4093, -0.2206, -0.3536,  0.4820, -0.3433, -0.2374]],
       dtype=torch.float64)
	q_value: tensor([[-9.7933]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4555666182467015, distance: 1.380615474284173 entropy 0.2896064221858978
epoch: 13, step: 33
	action: tensor([[ 0.2444, -0.0385, -0.3744,  0.3524,  0.1092, -0.4380,  0.2799]],
       dtype=torch.float64)
	q_value: tensor([[-10.1728]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5197069435057291, distance: 0.7930669422210459 entropy 0.2896064221858978
epoch: 13, step: 34
	action: tensor([[-0.3774,  0.1630,  0.2057,  0.2210, -0.4661,  0.0942, -0.1047]],
       dtype=torch.float64)
	q_value: tensor([[-11.6040]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1691995812571625, distance: 1.0430498060261397 entropy 0.2896064221858978
epoch: 13, step: 35
	action: tensor([[ 0.0492, -0.2736, -0.5367, -0.4366,  0.6543, -0.2868, -0.0936]],
       dtype=torch.float64)
	q_value: tensor([[-10.2996]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03746117405517957, distance: 1.1655814298462688 entropy 0.2896064221858978
epoch: 13, step: 36
	action: tensor([[ 0.4111, -0.1562,  0.2562, -0.2652, -0.2376,  0.1938, -0.4027]],
       dtype=torch.float64)
	q_value: tensor([[-10.5252]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3796562469488617, distance: 0.9013073234047271 entropy 0.2896064221858978
epoch: 13, step: 37
	action: tensor([[ 0.3921, -0.3244, -0.1548, -0.0979,  0.3730,  0.1151, -0.3712]],
       dtype=torch.float64)
	q_value: tensor([[-11.3215]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3094913635713551, distance: 0.9509140202677566 entropy 0.2896064221858978
epoch: 13, step: 38
	action: tensor([[ 0.2227, -0.1156,  0.2538, -0.0758, -0.0020,  0.1286,  0.0139]],
       dtype=torch.float64)
	q_value: tensor([[-10.3442]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.40901455070344783, distance: 0.8797212595820707 entropy 0.2896064221858978
epoch: 13, step: 39
	action: tensor([[-0.2880, -0.0816,  0.0696, -0.4440,  0.0429, -0.0153,  0.1237]],
       dtype=torch.float64)
	q_value: tensor([[-10.3906]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2764391762738785, distance: 1.292875649245645 entropy 0.2896064221858978
epoch: 13, step: 40
	action: tensor([[-0.0343,  0.2232, -0.0259, -0.3941,  0.0690, -0.3240, -0.0426]],
       dtype=torch.float64)
	q_value: tensor([[-9.5928]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21042353254870427, distance: 1.0168427379260885 entropy 0.2896064221858978
epoch: 13, step: 41
	action: tensor([[-0.1763,  0.1218, -0.0830, -0.7002,  0.0914, -0.0383, -0.0843]],
       dtype=torch.float64)
	q_value: tensor([[-9.4391]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04707450507499322, distance: 1.1709692374685947 entropy 0.2896064221858978
epoch: 13, step: 42
	action: tensor([[ 0.4078,  0.0180, -0.6674, -0.0012,  0.4594,  0.4373,  0.5958]],
       dtype=torch.float64)
	q_value: tensor([[-9.3209]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7188745477791167, distance: 0.6067457972682678 entropy 0.2896064221858978
epoch: 13, step: 43
	action: tensor([[ 0.1610, -0.2241,  0.0181,  0.0642,  0.4552,  0.4312, -0.2083]],
       dtype=torch.float64)
	q_value: tensor([[-12.6747]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4503504006163931, distance: 0.8483980295107815 entropy 0.2896064221858978
epoch: 13, step: 44
	action: tensor([[ 0.0762,  0.2662,  0.4347, -0.4038,  0.0629,  0.2411, -0.3048]],
       dtype=torch.float64)
	q_value: tensor([[-10.3724]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3928262852315363, distance: 0.8916885179355635 entropy 0.2896064221858978
epoch: 13, step: 45
	action: tensor([[ 0.7414, -0.0674,  0.3065,  0.1129, -0.0600,  0.4230, -0.0927]],
       dtype=torch.float64)
	q_value: tensor([[-10.9445]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8393260586997777, distance: 0.45870071464888407 entropy 0.2896064221858978
epoch: 13, step: 46
	action: tensor([[ 0.1591, -0.2270, -0.2798,  0.1617, -0.1329,  0.2950,  0.1445]],
       dtype=torch.float64)
	q_value: tensor([[-12.6955]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.406021087994175, distance: 0.8819464298573038 entropy 0.2896064221858978
epoch: 13, step: 47
	action: tensor([[ 0.4626, -0.3432, -0.6303, -0.6461,  0.4664,  0.4815,  0.1412]],
       dtype=torch.float64)
	q_value: tensor([[-9.5735]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25534222671311624, distance: 0.987495344734685 entropy 0.2896064221858978
epoch: 13, step: 48
	action: tensor([[-0.1416, -0.0556, -0.5671, -0.1135, -0.4322,  0.3501,  0.0726]],
       dtype=torch.float64)
	q_value: tensor([[-12.1375]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10137129666833555, distance: 1.0847929060015775 entropy 0.2896064221858978
epoch: 13, step: 49
	action: tensor([[-0.2065,  0.2682, -0.4374, -0.1195,  0.1648, -0.2460, -0.0043]],
       dtype=torch.float64)
	q_value: tensor([[-9.3547]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2647782908671672, distance: 0.9812187857720669 entropy 0.2896064221858978
epoch: 13, step: 50
	action: tensor([[ 0.2222, -0.6571, -0.3591,  0.5048,  0.1977,  0.1985,  0.2319]],
       dtype=torch.float64)
	q_value: tensor([[-8.6938]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21878121056531896, distance: 1.0114467735053403 entropy 0.2896064221858978
epoch: 13, step: 51
	action: tensor([[ 0.5222, -0.2196,  0.4600, -0.1695, -0.0396,  0.1864, -0.1198]],
       dtype=torch.float64)
	q_value: tensor([[-11.9125]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.442992189159231, distance: 0.85405794258812 entropy 0.2896064221858978
epoch: 13, step: 52
	action: tensor([[-0.0261,  0.1997, -0.2734,  0.0576, -0.2548, -0.1494, -0.0883]],
       dtype=torch.float64)
	q_value: tensor([[-12.0470]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44220536989857384, distance: 0.8546609431954201 entropy 0.2896064221858978
epoch: 13, step: 53
	action: tensor([[ 0.3965, -0.0089, -0.6636,  0.0778,  0.1356, -0.5504,  0.0503]],
       dtype=torch.float64)
	q_value: tensor([[-8.9334]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.581924853662897, distance: 0.7399184701504937 entropy 0.2896064221858978
epoch: 13, step: 54
	action: tensor([[-0.5083, -0.1596,  0.4661, -0.5005, -0.3642,  0.3588, -0.1191]],
       dtype=torch.float64)
	q_value: tensor([[-11.7003]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6300300344812706, distance: 1.4610144005715975 entropy 0.2896064221858978
epoch: 13, step: 55
	action: tensor([[ 0.5540, -0.0338, -0.0756,  0.1837, -0.1376, -0.1376,  0.0320]],
       dtype=torch.float64)
	q_value: tensor([[-12.0062]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7128704361217468, distance: 0.6131908262896223 entropy 0.2896064221858978
epoch: 13, step: 56
	action: tensor([[ 0.2648, -0.5356, -0.2020,  0.1229, -0.4834,  0.8026,  0.4726]],
       dtype=torch.float64)
	q_value: tensor([[-10.8359]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2775713724947042, distance: 0.9726445707697785 entropy 0.2896064221858978
epoch: 13, step: 57
	action: tensor([[ 0.0328, -0.1565, -0.2620,  0.0837,  0.3702,  0.1149,  0.3497]],
       dtype=torch.float64)
	q_value: tensor([[-13.1742]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2775063127696401, distance: 0.9726883664897386 entropy 0.2896064221858978
epoch: 13, step: 58
	action: tensor([[ 0.6144, -0.1491, -0.4159,  0.1310, -0.2284, -0.1113, -0.0070]],
       dtype=torch.float64)
	q_value: tensor([[-10.2747]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6124904526296295, distance: 0.7123573288931605 entropy 0.2896064221858978
epoch: 13, step: 59
	action: tensor([[ 0.4540,  0.0819,  0.0300,  0.0329,  0.0328,  0.2984, -0.2560]],
       dtype=torch.float64)
	q_value: tensor([[-10.9311]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7750472660881225, distance: 0.5427531230293786 entropy 0.2896064221858978
epoch: 13, step: 60
	action: tensor([[ 0.0407, -0.1067, -0.1010,  0.1502,  0.5517,  0.1342, -0.5222]],
       dtype=torch.float64)
	q_value: tensor([[-10.3344]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39827060149094173, distance: 0.8876817850198233 entropy 0.2896064221858978
epoch: 13, step: 61
	action: tensor([[-0.1637, -0.0777, -0.3868, -0.3875,  0.2767, -0.0082, -0.3910]],
       dtype=torch.float64)
	q_value: tensor([[-10.0328]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.023207871287995507, distance: 1.130987405995971 entropy 0.2896064221858978
epoch: 13, step: 62
	action: tensor([[-0.6751,  0.2062,  0.1831, -0.3373,  0.1219,  0.3541,  0.1111]],
       dtype=torch.float64)
	q_value: tensor([[-8.5663]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4091834484887815, distance: 1.3584399966247473 entropy 0.2896064221858978
epoch: 13, step: 63
	action: tensor([[-0.1816,  0.4613, -0.2950, -0.5019, -0.1968, -0.3252, -0.1344]],
       dtype=torch.float64)
	q_value: tensor([[-10.7109]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3099165663816521, distance: 0.950621197298864 entropy 0.2896064221858978
epoch: 13, step: 64
	action: tensor([[ 0.1324, -0.3160, -0.1893, -0.0054,  0.0545, -0.2206, -0.3239]],
       dtype=torch.float64)
	q_value: tensor([[-9.6230]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1058052016138673, distance: 1.0821133697463867 entropy 0.2896064221858978
epoch: 13, step: 65
	action: tensor([[ 0.4152, -0.2933, -0.2124, -0.4555,  0.1753, -0.0790, -0.1153]],
       dtype=torch.float64)
	q_value: tensor([[-9.7739]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09674092979462023, distance: 1.087584122909119 entropy 0.2896064221858978
epoch: 13, step: 66
	action: tensor([[-0.5138, -0.0642,  0.0118,  0.0961,  0.3657, -0.1982,  0.0721]],
       dtype=torch.float64)
	q_value: tensor([[-10.4651]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3499457446721268, distance: 1.329581153259632 entropy 0.2896064221858978
epoch: 13, step: 67
	action: tensor([[ 0.1836, -0.1586, -0.2388, -0.2664, -0.0609,  0.2395,  0.4688]],
       dtype=torch.float64)
	q_value: tensor([[-10.0987]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2656689725309296, distance: 0.9806242586945972 entropy 0.2896064221858978
epoch: 13, step: 68
	action: tensor([[ 0.1210,  0.1437, -0.0281,  0.1256, -0.2453,  0.7189,  0.0266]],
       dtype=torch.float64)
	q_value: tensor([[-10.7964]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6430130422553871, distance: 0.6837272170538189 entropy 0.2896064221858978
epoch: 13, step: 69
	action: tensor([[-0.1658, -0.4898, -0.0352, -0.5625,  0.2008,  0.5114, -0.2892]],
       dtype=torch.float64)
	q_value: tensor([[-11.0624]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3418564871132044, distance: 1.3255915539994592 entropy 0.2896064221858978
epoch: 13, step: 70
	action: tensor([[ 0.1219,  0.6438,  0.3119, -0.0542,  0.3938,  0.1583, -0.1764]],
       dtype=torch.float64)
	q_value: tensor([[-10.3807]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.749976538931168, distance: 0.5721989739094682 entropy 0.2896064221858978
epoch: 13, step: 71
	action: tensor([[ 0.2253,  0.6558, -0.8272,  0.5148,  0.1588,  0.2684, -0.2923]],
       dtype=torch.float64)
	q_value: tensor([[-10.7964]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 13, step: 72
	action: tensor([[ 0.2869,  0.1546, -0.4968, -0.1908,  1.0422,  0.5459, -0.0243]],
       dtype=torch.float64)
	q_value: tensor([[-14.3757]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.761247074721985, distance: 0.5591534952100333 entropy 0.2896064221858978
epoch: 13, step: 73
	action: tensor([[ 0.2503,  0.0945, -0.5327, -0.0218,  0.2051,  0.0322, -0.5134]],
       dtype=torch.float64)
	q_value: tensor([[-12.5747]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6043515435606179, distance: 0.7197993185111188 entropy 0.2896064221858978
epoch: 13, step: 74
	action: tensor([[-0.2923,  0.3649,  0.1296,  0.0185, -0.0412,  0.7006, -0.2569]],
       dtype=torch.float64)
	q_value: tensor([[-9.2375]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4106637554021815, distance: 0.878492926438987 entropy 0.2896064221858978
epoch: 13, step: 75
	action: tensor([[ 0.0692,  0.4654,  0.0855, -0.8181, -0.5002,  0.2110, -0.5760]],
       dtype=torch.float64)
	q_value: tensor([[-10.7175]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3784048411177431, distance: 0.9022159591288776 entropy 0.2896064221858978
epoch: 13, step: 76
	action: tensor([[-0.2917, -0.3924, -0.7188, -0.1974,  0.2283,  0.5020, -0.1276]],
       dtype=torch.float64)
	q_value: tensor([[-12.9056]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23205674383651753, distance: 1.270199824473065 entropy 0.2896064221858978
epoch: 13, step: 77
	action: tensor([[ 0.1768, -0.1858, -0.2405,  0.1771, -0.1704, -0.0770,  0.3802]],
       dtype=torch.float64)
	q_value: tensor([[-9.7863]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36450920865531833, distance: 0.9122446480734625 entropy 0.2896064221858978
epoch: 13, step: 78
	action: tensor([[ 0.1887,  0.2718, -0.2758, -0.5354,  0.1983,  0.3013,  0.2371]],
       dtype=torch.float64)
	q_value: tensor([[-10.3942]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5568518357801053, distance: 0.7617828138089554 entropy 0.2896064221858978
epoch: 13, step: 79
	action: tensor([[ 0.4279,  0.1385, -0.0464,  0.4671,  0.6292, -0.0630, -0.0222]],
       dtype=torch.float64)
	q_value: tensor([[-10.6770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8478245025094856, distance: 0.44640500829229496 entropy 0.2896064221858978
epoch: 13, step: 80
	action: tensor([[-0.2887, -0.1663,  0.2061, -0.6442,  0.3344,  0.5672, -0.1986]],
       dtype=torch.float64)
	q_value: tensor([[-12.1888]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.30416932616906234, distance: 1.3068438066164847 entropy 0.2896064221858978
epoch: 13, step: 81
	action: tensor([[ 0.2582,  0.1510,  0.0659, -0.3027, -0.2053, -0.1927, -0.1091]],
       dtype=torch.float64)
	q_value: tensor([[-10.7774]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4355936609083685, distance: 0.8597112991683172 entropy 0.2896064221858978
epoch: 13, step: 82
	action: tensor([[ 0.2694,  0.2813, -0.4283,  0.9372, -0.2167, -0.1683,  0.7596]],
       dtype=torch.float64)
	q_value: tensor([[-10.2364]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6943197336380301, distance: 0.6326891616972912 entropy 0.2896064221858978
epoch: 13, step: 83
	action: tensor([[-0.4336,  0.2221, -0.0817, -0.0573,  0.0463, -0.2690, -0.2648]],
       dtype=torch.float64)
	q_value: tensor([[-13.5204]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0831043705625143, distance: 1.1909453893623771 entropy 0.2896064221858978
epoch: 13, step: 84
	action: tensor([[-0.2799,  0.3780,  0.0147, -0.2582, -0.0200,  0.1544,  0.7012]],
       dtype=torch.float64)
	q_value: tensor([[-8.8662]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.175273482534807, distance: 1.039229993669043 entropy 0.2896064221858978
epoch: 13, step: 85
	action: tensor([[-0.6908, -0.0734,  0.1290, -0.6234,  0.2314,  0.0432, -0.0418]],
       dtype=torch.float64)
	q_value: tensor([[-11.6401]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8507586103127227, distance: 1.5567955536338576 entropy 0.2896064221858978
epoch: 13, step: 86
	action: tensor([[ 0.1304, -0.0738,  0.1402, -0.0659, -0.2771,  0.0874, -0.3695]],
       dtype=torch.float64)
	q_value: tensor([[-10.4131]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31868103565029005, distance: 0.9445651803346533 entropy 0.2896064221858978
epoch: 13, step: 87
	action: tensor([[ 0.2721, -0.2785, -0.5072,  0.0072,  0.1224, -0.6220, -0.5204]],
       dtype=torch.float64)
	q_value: tensor([[-9.9859]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21996632700022611, distance: 1.0106792950736965 entropy 0.2896064221858978
epoch: 13, step: 88
	action: tensor([[ 0.1619,  0.1476,  0.3656,  0.2111,  0.4054, -0.8612,  0.4440]],
       dtype=torch.float64)
	q_value: tensor([[-11.6999]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44183670511649853, distance: 0.8549433332798073 entropy 0.2896064221858978
epoch: 13, step: 89
	action: tensor([[ 0.4800,  0.2549,  0.1012,  0.2685,  0.2469,  0.4647, -0.0986]],
       dtype=torch.float64)
	q_value: tensor([[-14.3343]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9491816206401562, distance: 0.2579687496754102 entropy 0.2896064221858978
epoch: 13, step: 90
	action: tensor([[-0.2322,  0.5505,  0.0576,  0.5390,  0.5689,  0.1912, -0.2531]],
       dtype=torch.float64)
	q_value: tensor([[-11.2167]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5845927527916237, distance: 0.7375538388079754 entropy 0.2896064221858978
epoch: 13, step: 91
	action: tensor([[ 0.5086,  0.1029, -0.2032, -0.1311,  0.4096, -0.1193, -0.1118]],
       dtype=torch.float64)
	q_value: tensor([[-11.0206]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6600439273639437, distance: 0.6672185237521863 entropy 0.2896064221858978
epoch: 13, step: 92
	action: tensor([[ 0.0635,  0.2593,  0.1719,  0.4495,  0.9194, -0.1756, -0.2820]],
       dtype=torch.float64)
	q_value: tensor([[-10.6278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6575945939019421, distance: 0.6696178154972185 entropy 0.2896064221858978
epoch: 13, step: 93
	action: tensor([[ 0.2400,  0.1001, -0.4630,  0.2047, -0.4353,  0.4931, -0.0570]],
       dtype=torch.float64)
	q_value: tensor([[-12.3009]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6119085875735929, distance: 0.712891948439999 entropy 0.2896064221858978
epoch: 13, step: 94
	action: tensor([[-0.3077,  0.1397, -0.2996, -0.1091, -0.4924,  0.0237, -0.0360]],
       dtype=torch.float64)
	q_value: tensor([[-10.1948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05830071640660328, distance: 1.110485298247301 entropy 0.2896064221858978
epoch: 13, step: 95
	action: tensor([[ 0.0046, -0.1532, -0.3651,  0.3037, -0.1493,  0.3584,  0.0390]],
       dtype=torch.float64)
	q_value: tensor([[-9.2083]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3309338015800074, distance: 0.9360331680272078 entropy 0.2896064221858978
epoch: 13, step: 96
	action: tensor([[ 0.7426, -0.2748,  0.1252, -0.3161,  0.2438, -0.1602, -0.0773]],
       dtype=torch.float64)
	q_value: tensor([[-9.2916]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2464550269354272, distance: 0.9933705528376985 entropy 0.2896064221858978
epoch: 13, step: 97
	action: tensor([[ 0.2229, -0.6318, -0.0014, -0.3021,  0.1526, -0.0439, -0.1576]],
       dtype=torch.float64)
	q_value: tensor([[-12.5248]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.32095035580473263, distance: 1.3152246546171316 entropy 0.2896064221858978
epoch: 13, step: 98
	action: tensor([[ 0.1491,  0.0855, -0.7174, -0.1665, -0.1685,  0.0261,  0.0215]],
       dtype=torch.float64)
	q_value: tensor([[-10.6720]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4707929303106624, distance: 0.8324717640197824 entropy 0.2896064221858978
epoch: 13, step: 99
	action: tensor([[ 0.3528,  0.4250, -0.2882, -0.2666,  0.2042, -0.2657, -0.3649]],
       dtype=torch.float64)
	q_value: tensor([[-9.3521]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.73901196735947, distance: 0.58461100987091 entropy 0.2896064221858978
epoch: 13, step: 100
	action: tensor([[ 0.5502,  0.2153, -0.4329, -0.2413, -0.1260, -0.4529, -0.5853]],
       dtype=torch.float64)
	q_value: tensor([[-10.1377]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7114033689563051, distance: 0.6147553568481706 entropy 0.2896064221858978
epoch: 13, step: 101
	action: tensor([[ 0.0768, -0.2031, -0.3846, -0.2767, -0.1292, -0.1027, -0.3794]],
       dtype=torch.float64)
	q_value: tensor([[-11.4084]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1402002355790095, distance: 1.0610976535547407 entropy 0.2896064221858978
epoch: 13, step: 102
	action: tensor([[ 0.0247,  0.1479, -0.0746,  0.1553, -0.4016,  0.1592,  0.0073]],
       dtype=torch.float64)
	q_value: tensor([[-9.1649]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4976008204580108, distance: 0.8111126114628783 entropy 0.2896064221858978
epoch: 13, step: 103
	action: tensor([[-0.1887, -0.1435,  0.3233,  0.0779,  0.0832,  0.2658, -0.0658]],
       dtype=torch.float64)
	q_value: tensor([[-9.6832]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14651515317582364, distance: 1.0571937834050371 entropy 0.2896064221858978
epoch: 13, step: 104
	action: tensor([[ 0.6516,  0.3058, -0.2496, -0.2863,  0.3558, -0.2095, -0.1278]],
       dtype=torch.float64)
	q_value: tensor([[-10.3181]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7783238373092587, distance: 0.5387858615948637 entropy 0.2896064221858978
epoch: 13, step: 105
	action: tensor([[-0.2113, -0.2461, -0.0917, -0.3272,  0.4778,  0.1922,  0.1457]],
       dtype=torch.float64)
	q_value: tensor([[-11.6477]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1876434554415427, distance: 1.247095560556796 entropy 0.2896064221858978
epoch: 13, step: 106
	action: tensor([[-0.5368,  0.8255, -0.3471, -0.0601,  0.0114, -0.2225, -0.7595]],
       dtype=torch.float64)
	q_value: tensor([[-9.7454]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1996070157390113, distance: 1.02378398090706 entropy 0.2896064221858978
epoch: 13, step: 107
	action: tensor([[-0.1157, -0.2403,  0.1800,  0.0218, -0.0361, -0.4005,  0.3074]],
       dtype=torch.float64)
	q_value: tensor([[-10.3325]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.100948279605513, distance: 1.200715596431853 entropy 0.2896064221858978
epoch: 13, step: 108
	action: tensor([[ 0.0634,  0.3036, -0.6503, -0.1399, -0.1902, -0.0184,  0.6230]],
       dtype=torch.float64)
	q_value: tensor([[-11.1335]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5502661104154818, distance: 0.767422451673983 entropy 0.2896064221858978
epoch: 13, step: 109
	action: tensor([[-0.4184, -0.0533,  0.4129, -0.0719,  0.1414, -0.2978, -0.5072]],
       dtype=torch.float64)
	q_value: tensor([[-11.3043]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3574676627235802, distance: 1.3332802307092424 entropy 0.2896064221858978
epoch: 13, step: 110
	action: tensor([[ 0.6773,  0.0758, -0.3319,  0.0101, -0.0541, -0.0250,  0.2015]],
       dtype=torch.float64)
	q_value: tensor([[-10.6770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7739573087203793, distance: 0.5440664275998865 entropy 0.2896064221858978
epoch: 13, step: 111
	action: tensor([[-0.5264,  0.1021, -0.2735, -0.0217, -0.4528, -0.0479,  0.3904]],
       dtype=torch.float64)
	q_value: tensor([[-11.1783]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2211959337317968, distance: 1.2645889075425885 entropy 0.2896064221858978
epoch: 13, step: 112
	action: tensor([[-0.1391, -0.2343, -0.5744, -0.5888, -0.0124,  0.1114, -0.5187]],
       dtype=torch.float64)
	q_value: tensor([[-10.1795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04198126552749404, distance: 1.1681178180455765 entropy 0.2896064221858978
epoch: 13, step: 113
	action: tensor([[ 0.0740, -0.4393, -0.5766,  0.0966,  0.4134, -0.2858,  0.1926]],
       dtype=torch.float64)
	q_value: tensor([[-9.1218]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.037040518456900484, distance: 1.1653451038627325 entropy 0.2896064221858978
epoch: 13, step: 114
	action: tensor([[ 0.4926,  0.5209, -0.4042,  0.6173, -0.3799,  0.2418,  0.0932]],
       dtype=torch.float64)
	q_value: tensor([[-11.3256]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7851185806629437, distance: 0.5304642558274036 entropy 0.2896064221858978
epoch: 13, step: 115
	action: tensor([[-0.1321, -0.3030, -0.4899, -0.2482,  0.0017,  0.1334, -0.3456]],
       dtype=torch.float64)
	q_value: tensor([[-11.8234]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08478563287540264, distance: 1.191869360881734 entropy 0.2896064221858978
epoch: 13, step: 116
	action: tensor([[-0.2757,  0.6930, -0.4233, -0.2028,  0.6019, -0.0596, -0.1871]],
       dtype=torch.float64)
	q_value: tensor([[-8.7693]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4067021740005248, distance: 0.881440642824111 entropy 0.2896064221858978
epoch: 13, step: 117
	action: tensor([[ 0.4488, -0.4771, -0.1443, -0.4107,  0.3494, -0.3557,  1.0110]],
       dtype=torch.float64)
	q_value: tensor([[-9.7005]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1522373697576307, distance: 1.2283656728742691 entropy 0.2896064221858978
epoch: 13, step: 118
	action: tensor([[-0.3413, -0.4574, -0.3459,  0.1589,  0.0030,  0.3244, -0.5767]],
       dtype=torch.float64)
	q_value: tensor([[-14.9758]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.30779783082608314, distance: 1.3086605167100527 entropy 0.2896064221858978
epoch: 13, step: 119
	action: tensor([[ 0.0174, -0.2469,  0.1270, -0.8672, -0.1141,  0.0792, -0.4393]],
       dtype=torch.float64)
	q_value: tensor([[-9.7679]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.31454987745558505, distance: 1.3120344178277226 entropy 0.2896064221858978
epoch: 13, step: 120
	action: tensor([[-0.3960, -0.1295, -0.2190, -0.1129,  0.4296,  0.3529, -0.0918]],
       dtype=torch.float64)
	q_value: tensor([[-11.0028]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.146297627708734, distance: 1.2251954917013368 entropy 0.2896064221858978
epoch: 13, step: 121
	action: tensor([[ 0.0905,  0.3504, -0.1381, -0.4951, -0.0782, -0.2560, -0.3808]],
       dtype=torch.float64)
	q_value: tensor([[-9.3153]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4353107101797472, distance: 0.8599267693373749 entropy 0.2896064221858978
epoch: 13, step: 122
	action: tensor([[-0.0098,  0.2015,  0.3271,  0.0247,  0.2333, -0.6512,  0.2363]],
       dtype=torch.float64)
	q_value: tensor([[-9.9350]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27201559966533084, distance: 0.9763774258133735 entropy 0.2896064221858978
epoch: 13, step: 123
	action: tensor([[-0.4386,  0.7747, -0.4481,  0.0291,  0.2068,  0.5080, -0.1421]],
       dtype=torch.float64)
	q_value: tensor([[-11.9055]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.2896064221858978
epoch: 13, step: 124
	action: tensor([[-0.1289, -0.1494,  0.1380, -0.5551, -0.2734, -0.3515, -0.0235]],
       dtype=torch.float64)
	q_value: tensor([[-14.3757]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2699835118886398, distance: 1.2896021085738039 entropy 0.2896064221858978
epoch: 13, step: 125
	action: tensor([[ 0.0460,  0.0199, -0.3490, -0.4196,  0.1408, -0.1825,  0.3983]],
       dtype=torch.float64)
	q_value: tensor([[-10.5958]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2145601779999684, distance: 1.0141755854484236 entropy 0.2896064221858978
epoch: 13, step: 126
	action: tensor([[-0.0111,  0.0049, -0.4907, -0.2820,  0.5067,  0.2341, -0.4496]],
       dtype=torch.float64)
	q_value: tensor([[-9.9735]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3139384328504272, distance: 0.9478469976776901 entropy 0.2896064221858978
epoch: 13, step: 127
	action: tensor([[ 0.1326,  0.1730,  0.3145, -0.1213,  0.7115,  0.0480,  0.0239]],
       dtype=torch.float64)
	q_value: tensor([[-9.0912]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.48734277153451044, distance: 0.8193514675585405 entropy 0.2896064221858978
LOSS epoch 13 actor 72.18886825691705 critic 184.98265433466128 
epoch: 14, step: 0
	action: tensor([[-0.4148,  0.4030, -0.2172, -0.2758, -0.2752,  0.1962, -0.0565]],
       dtype=torch.float64)
	q_value: tensor([[-9.5399]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06391761992881373, distance: 1.1071685187621285 entropy 0.18424582481384277
epoch: 14, step: 1
	action: tensor([[-0.1389,  0.2890, -0.0144, -0.3594,  0.2711, -0.0841,  0.3457]],
       dtype=torch.float64)
	q_value: tensor([[-8.0794]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18430533612805144, distance: 1.033523852051237 entropy 0.18424582481384277
epoch: 14, step: 2
	action: tensor([[-0.1408,  0.2921,  0.0256, -0.2737,  0.4464, -0.1789, -0.4906]],
       dtype=torch.float64)
	q_value: tensor([[-8.5428]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1733773631966069, distance: 1.0404239487289024 entropy 0.18424582481384277
epoch: 14, step: 3
	action: tensor([[-0.4425,  0.2470, -0.0307, -0.6164,  0.0968,  0.0439, -0.3374]],
       dtype=torch.float64)
	q_value: tensor([[-8.1258]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23775749578301664, distance: 1.273135053433727 entropy 0.18424582481384277
epoch: 14, step: 4
	action: tensor([[ 0.2452,  0.0289,  0.1724, -0.2397,  0.0343, -0.0364, -0.0286]],
       dtype=torch.float64)
	q_value: tensor([[-7.7437]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37807187890929417, distance: 0.9024575662234042 entropy 0.18424582481384277
epoch: 14, step: 5
	action: tensor([[ 0.2249,  0.2492, -0.3068, -0.1258,  0.7287, -0.2280, -0.3674]],
       dtype=torch.float64)
	q_value: tensor([[-8.6612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5605255578814184, distance: 0.7586186321153318 entropy 0.18424582481384277
epoch: 14, step: 6
	action: tensor([[-0.4496,  0.3404, -0.1893, -0.1687,  0.2787, -0.3927, -0.0358]],
       dtype=torch.float64)
	q_value: tensor([[-9.1711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08090332899725161, distance: 1.1897346780653975 entropy 0.18424582481384277
epoch: 14, step: 7
	action: tensor([[-0.0696, -0.4187,  0.1359, -0.5712,  0.2923, -0.1072, -0.0752]],
       dtype=torch.float64)
	q_value: tensor([[-7.9010]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.49291544454445657, distance: 1.3982161005057576 entropy 0.18424582481384277
epoch: 14, step: 8
	action: tensor([[ 0.1215, -0.1344, -0.2369, -0.2307, -0.0963,  0.2928,  0.3209]],
       dtype=torch.float64)
	q_value: tensor([[-8.9725]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2504039051425011, distance: 0.9907643036371013 entropy 0.18424582481384277
epoch: 14, step: 9
	action: tensor([[-0.2502, -0.4585, -0.0966, -0.0641,  0.0331, -0.4307,  0.1221]],
       dtype=torch.float64)
	q_value: tensor([[-8.6592]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.45343764115621843, distance: 1.3796054297238751 entropy 0.18424582481384277
epoch: 14, step: 10
	action: tensor([[ 0.3078,  0.4521, -0.0797, -0.3035,  0.4085, -0.1751,  0.1084]],
       dtype=torch.float64)
	q_value: tensor([[-9.3457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6924544824390643, distance: 0.6346165504573525 entropy 0.18424582481384277
epoch: 14, step: 11
	action: tensor([[-0.0436, -0.9072, -0.1593,  0.0985,  0.5526,  0.2833, -0.1040]],
       dtype=torch.float64)
	q_value: tensor([[-9.0888]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4086958280353903, distance: 1.358204945453586 entropy 0.18424582481384277
epoch: 14, step: 12
	action: tensor([[ 0.2399,  0.4253,  0.3639, -0.2359,  0.1883,  0.3061, -0.0969]],
       dtype=torch.float64)
	q_value: tensor([[-10.4641]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7134201164673126, distance: 0.6126035993743553 entropy 0.18424582481384277
epoch: 14, step: 13
	action: tensor([[ 0.2227,  0.0116, -0.1836,  0.4363,  0.1049,  0.4324, -0.2192]],
       dtype=torch.float64)
	q_value: tensor([[-9.5844]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7079077082140939, distance: 0.6184672987419106 entropy 0.18424582481384277
epoch: 14, step: 14
	action: tensor([[ 0.5702, -0.0486, -0.1495, -0.3235,  0.1658,  0.0095, -0.0114]],
       dtype=torch.float64)
	q_value: tensor([[-8.7098]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4894866309736132, distance: 0.817636467197816 entropy 0.18424582481384277
epoch: 14, step: 15
	action: tensor([[ 0.1378, -0.1510, -0.0697,  0.2169, -0.2427,  0.3493, -0.5539]],
       dtype=torch.float64)
	q_value: tensor([[-9.2638]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4782708160021495, distance: 0.8265692764768291 entropy 0.18424582481384277
epoch: 14, step: 16
	action: tensor([[ 0.0766, -0.2325,  0.1892,  0.2690, -0.3016,  0.6368, -0.9305]],
       dtype=torch.float64)
	q_value: tensor([[-8.5854]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.518326121384511, distance: 0.7942061408954307 entropy 0.18424582481384277
epoch: 14, step: 17
	action: tensor([[ 0.0618,  0.3038, -0.7015, -0.0614,  0.1933, -0.0498,  0.1926]],
       dtype=torch.float64)
	q_value: tensor([[-10.5863]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5750128352158825, distance: 0.7460099161112971 entropy 0.18424582481384277
epoch: 14, step: 18
	action: tensor([[-0.2104, -0.0318,  0.3655, -0.3449,  0.3506,  0.2052,  0.1543]],
       dtype=torch.float64)
	q_value: tensor([[-8.2532]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10873136077645884, distance: 1.2049523116392762 entropy 0.18424582481384277
epoch: 14, step: 19
	action: tensor([[ 0.1598, -0.1307, -0.2241, -0.1692,  0.0330,  0.1400,  0.1923]],
       dtype=torch.float64)
	q_value: tensor([[-9.0882]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28210703507273505, distance: 0.9695864598279794 entropy 0.18424582481384277
epoch: 14, step: 20
	action: tensor([[ 0.3022, -0.4183,  0.1601,  0.4929,  0.3900,  0.2308,  0.2475]],
       dtype=torch.float64)
	q_value: tensor([[-8.1605]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6026655132426877, distance: 0.7213313771839853 entropy 0.18424582481384277
epoch: 14, step: 21
	action: tensor([[ 0.2162,  0.1389,  0.1879,  0.0695, -0.1142,  0.2755,  0.0648]],
       dtype=torch.float64)
	q_value: tensor([[-10.9796]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6689888753471025, distance: 0.6583820596111447 entropy 0.18424582481384277
epoch: 14, step: 22
	action: tensor([[ 0.3426, -0.2966, -0.3509, -0.0097,  0.7459,  0.2832, -0.4684]],
       dtype=torch.float64)
	q_value: tensor([[-8.9730]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.40433171344721464, distance: 0.8831997403003076 entropy 0.18424582481384277
epoch: 14, step: 23
	action: tensor([[-0.0169, -0.3747, -0.4287,  0.2713, -0.5647,  0.1909, -0.0525]],
       dtype=torch.float64)
	q_value: tensor([[-9.6149]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06736933490182873, distance: 1.1051253440799185 entropy 0.18424582481384277
epoch: 14, step: 24
	action: tensor([[ 0.1688,  0.2476, -0.2952, -0.2530, -0.1049,  0.1365,  0.2063]],
       dtype=torch.float64)
	q_value: tensor([[-8.5676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5702750460342352, distance: 0.7501566786311233 entropy 0.18424582481384277
epoch: 14, step: 25
	action: tensor([[ 0.2605, -0.0985, -0.2611,  0.3020,  0.0603,  1.0427, -0.1446]],
       dtype=torch.float64)
	q_value: tensor([[-8.3566]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7294803661026202, distance: 0.5951906344106408 entropy 0.18424582481384277
epoch: 14, step: 26
	action: tensor([[-0.6075,  0.2088,  0.0535,  0.2887, -0.0571,  0.1037,  0.0336]],
       dtype=torch.float64)
	q_value: tensor([[-10.4689]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07444103975729677, distance: 1.1861728722291256 entropy 0.18424582481384277
epoch: 14, step: 27
	action: tensor([[-0.1879, -0.4668, -0.3802, -0.0660, -0.1773,  0.0815, -0.2074]],
       dtype=torch.float64)
	q_value: tensor([[-8.3452]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.27278113621367406, distance: 1.2910217480723611 entropy 0.18424582481384277
epoch: 14, step: 28
	action: tensor([[ 0.1269,  0.7382,  0.2656, -0.2147,  0.0903, -0.2306, -0.0935]],
       dtype=torch.float64)
	q_value: tensor([[-7.8721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6544650420504237, distance: 0.6726709755708817 entropy 0.18424582481384277
epoch: 14, step: 29
	action: tensor([[-0.3165, -0.0080, -0.2667, -0.0939, -0.1345, -0.3306,  0.3166]],
       dtype=torch.float64)
	q_value: tensor([[-9.6775]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09883568341832483, distance: 1.199563024016518 entropy 0.18424582481384277
epoch: 14, step: 30
	action: tensor([[ 0.1268, -0.1009,  0.0746, -0.6298, -0.2892, -0.0424,  0.0222]],
       dtype=torch.float64)
	q_value: tensor([[-8.3797]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.018935089524355853, distance: 1.155127578024836 entropy 0.18424582481384277
epoch: 14, step: 31
	action: tensor([[ 0.0916,  0.0116, -0.3234, -0.3407,  0.2991, -0.2844,  0.1387]],
       dtype=torch.float64)
	q_value: tensor([[-9.3280]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22366180373000588, distance: 1.0082823626670212 entropy 0.18424582481384277
epoch: 14, step: 32
	action: tensor([[-0.0724, -0.1570, -0.2547,  0.1870,  0.1828, -0.1532,  0.2720]],
       dtype=torch.float64)
	q_value: tensor([[-8.2832]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14313834466829944, distance: 1.0592831097885083 entropy 0.18424582481384277
epoch: 14, step: 33
	action: tensor([[ 0.0498,  0.2412, -0.2651,  0.3227,  0.3269, -0.1996,  0.3521]],
       dtype=torch.float64)
	q_value: tensor([[-8.6375]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5382256575830406, distance: 0.7776274677112258 entropy 0.18424582481384277
epoch: 14, step: 34
	action: tensor([[-0.4659, -0.1122, -0.0742,  0.3250,  0.2927, -0.3027, -0.1161]],
       dtype=torch.float64)
	q_value: tensor([[-9.4282]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2541322238962225, distance: 1.2815287589439641 entropy 0.18424582481384277
epoch: 14, step: 35
	action: tensor([[ 0.3909,  0.1816, -0.4216,  0.1261, -0.0100, -0.0047, -0.2580]],
       dtype=torch.float64)
	q_value: tensor([[-8.9074]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7343333265767408, distance: 0.5898277919752003 entropy 0.18424582481384277
epoch: 14, step: 36
	action: tensor([[ 0.3699, -0.0621,  0.2124,  0.4161,  0.0073,  0.3781,  0.3171]],
       dtype=torch.float64)
	q_value: tensor([[-8.3576]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8371730264736558, distance: 0.46176378431039805 entropy 0.18424582481384277
epoch: 14, step: 37
	action: tensor([[ 0.0893,  0.5469, -0.1809, -0.5553, -0.1222,  0.4587, -0.1730]],
       dtype=torch.float64)
	q_value: tensor([[-10.3573]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6283410616681027, distance: 0.6976361621193272 entropy 0.18424582481384277
epoch: 14, step: 38
	action: tensor([[-0.5478, -0.0822,  0.0150, -0.1385, -0.0771, -0.1654, -0.3430]],
       dtype=torch.float64)
	q_value: tensor([[-9.4785]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.48342645706102094, distance: 1.3937654784916138 entropy 0.18424582481384277
epoch: 14, step: 39
	action: tensor([[-0.3283, -0.0862, -0.2440, -0.3164,  0.0499,  0.4277,  0.1719]],
       dtype=torch.float64)
	q_value: tensor([[-7.7364]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14238925653785173, distance: 1.223105019173958 entropy 0.18424582481384277
epoch: 14, step: 40
	action: tensor([[ 0.0280, -0.0378, -0.4024,  0.1793,  0.3849,  0.4563,  0.0032]],
       dtype=torch.float64)
	q_value: tensor([[-8.1436]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4312926281982783, distance: 0.8629807775925478 entropy 0.18424582481384277
epoch: 14, step: 41
	action: tensor([[-0.0207,  0.0414, -0.4395, -0.0805, -0.1747,  0.0701,  0.2231]],
       dtype=torch.float64)
	q_value: tensor([[-8.3251]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2883735906030849, distance: 0.9653453784259372 entropy 0.18424582481384277
epoch: 14, step: 42
	action: tensor([[-0.0247, -0.3000,  0.0384,  0.1245, -0.3998, -0.1142,  0.5374]],
       dtype=torch.float64)
	q_value: tensor([[-7.7266]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07835849346537305, distance: 1.0985952219804276 entropy 0.18424582481384277
epoch: 14, step: 43
	action: tensor([[ 0.3926,  0.2866, -0.5946, -0.3047,  0.2511, -0.2668, -0.1141]],
       dtype=torch.float64)
	q_value: tensor([[-9.9942]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7023103909992121, distance: 0.6243649730434443 entropy 0.18424582481384277
epoch: 14, step: 44
	action: tensor([[-0.1796,  0.6826, -0.3587,  0.1247,  0.4237, -0.1914,  0.0927]],
       dtype=torch.float64)
	q_value: tensor([[-9.0437]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.46713554594176787, distance: 0.8353434441374301 entropy 0.18424582481384277
epoch: 14, step: 45
	action: tensor([[ 0.1752,  0.3962,  0.3398, -0.6869, -0.1791,  0.2095, -0.3749]],
       dtype=torch.float64)
	q_value: tensor([[-8.5526]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4208439260196378, distance: 0.8708723479248799 entropy 0.18424582481384277
epoch: 14, step: 46
	action: tensor([[-0.4372, -0.0708,  0.0129, -0.3198,  0.1538,  0.2438,  0.6224]],
       dtype=torch.float64)
	q_value: tensor([[-10.4323]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3400957681533665, distance: 1.324721580069403 entropy 0.18424582481384277
epoch: 14, step: 47
	action: tensor([[-0.1595, -0.5747, -0.0549, -0.5268,  0.0058, -0.2484,  0.1899]],
       dtype=torch.float64)
	q_value: tensor([[-9.9535]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6322571269094237, distance: 1.4620121439830591 entropy 0.18424582481384277
epoch: 14, step: 48
	action: tensor([[ 0.1998, -0.1231, -0.1598,  0.0091,  0.0756,  0.0387, -0.3613]],
       dtype=torch.float64)
	q_value: tensor([[-9.5491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36753108949842384, distance: 0.9100731141724253 entropy 0.18424582481384277
epoch: 14, step: 49
	action: tensor([[ 0.2039, -0.1385, -0.3390,  0.0349, -0.9823, -0.2431,  0.4009]],
       dtype=torch.float64)
	q_value: tensor([[-7.9986]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4024302300869068, distance: 0.8846082855864935 entropy 0.18424582481384277
epoch: 14, step: 50
	action: tensor([[ 0.0598,  0.4044, -0.4707,  0.3087, -0.0869, -0.2341, -0.0541]],
       dtype=torch.float64)
	q_value: tensor([[-11.0795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.587879583177441, distance: 0.7346301672384068 entropy 0.18424582481384277
epoch: 14, step: 51
	action: tensor([[ 0.0292,  0.4944, -0.1993, -0.5158, -0.2826, -0.0154, -0.2641]],
       dtype=torch.float64)
	q_value: tensor([[-8.3852]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5380631507239645, distance: 0.7777642863580148 entropy 0.18424582481384277
epoch: 14, step: 52
	action: tensor([[-0.0997,  0.1654, -0.1158, -0.0304, -0.6908,  0.2363, -0.0929]],
       dtype=torch.float64)
	q_value: tensor([[-8.9316]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30904596143500396, distance: 0.9512206571701903 entropy 0.18424582481384277
epoch: 14, step: 53
	action: tensor([[ 0.1909, -0.5286,  0.0055,  0.0183, -0.1033, -0.1903,  0.1879]],
       dtype=torch.float64)
	q_value: tensor([[-9.0429]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.043111381750344346, distance: 1.1687511073338595 entropy 0.18424582481384277
epoch: 14, step: 54
	action: tensor([[-0.0740,  0.2989,  0.0708, -0.1800,  0.4398,  0.0721, -0.4228]],
       dtype=torch.float64)
	q_value: tensor([[-9.5538]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38527684080084457, distance: 0.8972149073454809 entropy 0.18424582481384277
epoch: 14, step: 55
	action: tensor([[ 0.4222,  0.2071, -0.1035,  0.1820,  0.0738, -0.4968, -0.0358]],
       dtype=torch.float64)
	q_value: tensor([[-8.0544]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7374145038815285, distance: 0.5863974328567721 entropy 0.18424582481384277
epoch: 14, step: 56
	action: tensor([[ 0.5917,  0.2844,  0.2335, -0.4383,  0.2315, -0.0109, -0.3463]],
       dtype=torch.float64)
	q_value: tensor([[-9.9059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6920336114554205, distance: 0.6350506331563446 entropy 0.18424582481384277
epoch: 14, step: 57
	action: tensor([[ 0.4365, -0.1334, -0.4478, -0.2717, -0.1819,  0.0544, -0.1072]],
       dtype=torch.float64)
	q_value: tensor([[-10.2142]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4192657855238451, distance: 0.8720580593483769 entropy 0.18424582481384277
epoch: 14, step: 58
	action: tensor([[ 0.3147, -0.2036, -0.0384,  0.3592, -0.1434,  0.4420,  0.2118]],
       dtype=torch.float64)
	q_value: tensor([[-8.6522]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6737223224964467, distance: 0.6536576893557033 entropy 0.18424582481384277
epoch: 14, step: 59
	action: tensor([[0.5015, 0.2991, 0.0266, 0.1315, 0.2041, 0.0815, 0.1185]],
       dtype=torch.float64)
	q_value: tensor([[-9.5338]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8842305841401551, distance: 0.3893620450172117 entropy 0.18424582481384277
epoch: 14, step: 60
	action: tensor([[-0.3923, -0.3063, -0.0709, -0.1812, -0.3733,  0.0195, -0.7889]],
       dtype=torch.float64)
	q_value: tensor([[-9.3693]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.44520730817009535, distance: 1.3756937613544555 entropy 0.18424582481384277
epoch: 14, step: 61
	action: tensor([[ 0.4187,  0.1431,  0.1462, -0.2780, -0.2539, -0.1850, -0.2258]],
       dtype=torch.float64)
	q_value: tensor([[-8.7262]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5515564232507023, distance: 0.7663207709586789 entropy 0.18424582481384277
epoch: 14, step: 62
	action: tensor([[ 0.6300, -0.2431,  0.0860, -0.2465, -0.0173,  0.3330,  0.0009]],
       dtype=torch.float64)
	q_value: tensor([[-9.5473]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.45425093976412134, distance: 0.8453823796019914 entropy 0.18424582481384277
epoch: 14, step: 63
	action: tensor([[ 0.3987,  0.7326, -0.1868, -0.1087,  0.2378,  0.0145, -0.0044]],
       dtype=torch.float64)
	q_value: tensor([[-10.0022]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8873746160577984, distance: 0.38403856260141983 entropy 0.18424582481384277
epoch: 14, step: 64
	action: tensor([[-0.1066, -0.2719, -0.3482, -0.3018,  0.1976,  0.3878,  0.2265]],
       dtype=torch.float64)
	q_value: tensor([[-9.3536]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.007911281537733794, distance: 1.1488619511937872 entropy 0.18424582481384277
epoch: 14, step: 65
	action: tensor([[ 0.0815, -0.0638, -0.1031, -0.6763, -0.1844, -0.3606, -0.0349]],
       dtype=torch.float64)
	q_value: tensor([[-8.2899]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.006411082179226479, distance: 1.1406701131997083 entropy 0.18424582481384277
epoch: 14, step: 66
	action: tensor([[ 0.5897,  0.2405, -0.7038,  0.3611, -0.0514, -0.0311,  0.1886]],
       dtype=torch.float64)
	q_value: tensor([[-9.0421]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8561779226177695, distance: 0.4339797566960468 entropy 0.18424582481384277
epoch: 14, step: 67
	action: tensor([[ 0.1206, -0.0917, -0.2385, -0.0522,  0.4300, -0.4690, -0.1635]],
       dtype=torch.float64)
	q_value: tensor([[-9.9878]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2133292897282284, distance: 1.014969948112631 entropy 0.18424582481384277
epoch: 14, step: 68
	action: tensor([[ 0.2977, -0.1506,  0.3131, -0.6572, -0.1689, -0.3825,  0.0787]],
       dtype=torch.float64)
	q_value: tensor([[-9.0212]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07069659082504098, distance: 1.1841041499478526 entropy 0.18424582481384277
epoch: 14, step: 69
	action: tensor([[-0.0927,  0.0888,  0.4380, -0.0448,  0.2761,  0.2311, -0.3147]],
       dtype=torch.float64)
	q_value: tensor([[-10.5664]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3337449446349131, distance: 0.9340646835156049 entropy 0.18424582481384277
epoch: 14, step: 70
	action: tensor([[ 0.1560,  0.5370,  0.0016, -0.7372,  0.0437, -0.0439, -0.1703]],
       dtype=torch.float64)
	q_value: tensor([[-9.1157]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5174525629332605, distance: 0.7949259964259958 entropy 0.18424582481384277
epoch: 14, step: 71
	action: tensor([[ 0.3033, -0.1059, -0.2019, -0.1040,  0.2375,  0.1162,  0.1843]],
       dtype=torch.float64)
	q_value: tensor([[-9.4017]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.452439671973321, distance: 0.846784072813386 entropy 0.18424582481384277
epoch: 14, step: 72
	action: tensor([[-0.2185, -0.1081,  0.2315, -0.3125,  0.1470, -0.0816, -0.1284]],
       dtype=torch.float64)
	q_value: tensor([[-8.5728]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.22722181397660002, distance: 1.2677050676342352 entropy 0.18424582481384277
epoch: 14, step: 73
	action: tensor([[-0.1275, -0.0770, -0.0274,  0.5565,  0.4691,  0.1101, -0.1922]],
       dtype=torch.float64)
	q_value: tensor([[-8.2207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41632238087780615, distance: 0.8742652442068528 entropy 0.18424582481384277
epoch: 14, step: 74
	action: tensor([[-0.2250, -0.7801, -0.1856, -0.0809, -0.5086, -0.3668, -0.3455]],
       dtype=torch.float64)
	q_value: tensor([[-9.4577]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5643154943316708, distance: 1.4312610964487902 entropy 0.18424582481384277
epoch: 14, step: 75
	action: tensor([[ 0.1766,  0.0086, -0.3382,  0.1078, -0.3336, -0.0581, -0.2197]],
       dtype=torch.float64)
	q_value: tensor([[-9.9696]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4684743220929546, distance: 0.8342934198880329 entropy 0.18424582481384277
epoch: 14, step: 76
	action: tensor([[ 0.1594,  0.2476,  0.3450, -0.5255,  0.5208,  0.1824,  0.0879]],
       dtype=torch.float64)
	q_value: tensor([[-7.9988]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3845966911644041, distance: 0.8977111239592426 entropy 0.18424582481384277
epoch: 14, step: 77
	action: tensor([[-0.7733, -0.2115, -0.3278, -0.3118,  0.1752, -0.1913, -0.4127]],
       dtype=torch.float64)
	q_value: tensor([[-9.4700]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8251653383912154, distance: 1.5459939837652361 entropy 0.18424582481384277
epoch: 14, step: 78
	action: tensor([[ 0.1839,  0.4163, -0.3281, -0.3858,  0.3570,  0.4478, -0.6284]],
       dtype=torch.float64)
	q_value: tensor([[-8.1437]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7059309804039766, distance: 0.6205565015939897 entropy 0.18424582481384277
epoch: 14, step: 79
	action: tensor([[-0.0055,  0.0638, -0.5238, -0.1481,  0.1287,  0.2178, -0.4752]],
       dtype=torch.float64)
	q_value: tensor([[-8.5319]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37384967733191643, distance: 0.9055157271788261 entropy 0.18424582481384277
epoch: 14, step: 80
	action: tensor([[ 0.4327,  0.4361, -0.0032, -0.2330,  0.4025,  0.2422, -0.1870]],
       dtype=torch.float64)
	q_value: tensor([[-7.1291]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8476580350746765, distance: 0.4466491066727147 entropy 0.18424582481384277
epoch: 14, step: 81
	action: tensor([[-0.0046, -0.1410, -0.4538, -0.1346,  0.0993,  0.6328, -0.0037]],
       dtype=torch.float64)
	q_value: tensor([[-9.0097]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2953663983508883, distance: 0.9605906785264466 entropy 0.18424582481384277
epoch: 14, step: 82
	action: tensor([[-0.0186,  0.0166, -0.1345, -0.1967,  0.2801,  0.0123, -0.4247]],
       dtype=torch.float64)
	q_value: tensor([[-8.2807]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24149832817679007, distance: 0.9966323143960774 entropy 0.18424582481384277
epoch: 14, step: 83
	action: tensor([[ 0.3084,  0.2590, -0.2738,  0.0334,  0.1220, -0.0334, -0.2464]],
       dtype=torch.float64)
	q_value: tensor([[-7.4692]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7234350743654292, distance: 0.6018042410341004 entropy 0.18424582481384277
epoch: 14, step: 84
	action: tensor([[ 0.2769, -0.3839, -0.3041, -0.1448,  0.1489,  0.1240, -0.2974]],
       dtype=torch.float64)
	q_value: tensor([[-8.0734]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1856957824026133, distance: 1.0326425956298086 entropy 0.18424582481384277
epoch: 14, step: 85
	action: tensor([[ 0.3089, -0.0469, -0.3708, -0.1693, -0.4740, -0.0177,  0.1908]],
       dtype=torch.float64)
	q_value: tensor([[-8.2389]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4878846518465413, distance: 0.8189183245234739 entropy 0.18424582481384277
epoch: 14, step: 86
	action: tensor([[ 0.6369, -0.0634, -0.1115,  0.3106,  0.3314,  0.2824,  0.4154]],
       dtype=torch.float64)
	q_value: tensor([[-9.1241]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8647531483155914, distance: 0.42084316238153185 entropy 0.18424582481384277
epoch: 14, step: 87
	action: tensor([[ 0.2609,  0.1399,  0.3255,  0.3939,  0.2351,  0.1782, -0.4240]],
       dtype=torch.float64)
	q_value: tensor([[-10.9228]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8369914496778972, distance: 0.46202118093339417 entropy 0.18424582481384277
epoch: 14, step: 88
	action: tensor([[ 0.0760,  0.2394, -0.2629, -0.1139,  0.4173,  0.1135, -0.2487]],
       dtype=torch.float64)
	q_value: tensor([[-9.8518]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5495541142193052, distance: 0.7680296838518246 entropy 0.18424582481384277
epoch: 14, step: 89
	action: tensor([[ 0.1399,  0.2881, -0.0707, -0.0070,  0.3465, -0.3037, -0.3376]],
       dtype=torch.float64)
	q_value: tensor([[-7.5306]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5618475073794057, distance: 0.7574768014269381 entropy 0.18424582481384277
epoch: 14, step: 90
	action: tensor([[-0.3256, -0.7778, -0.0277, -0.0117, -0.1652, -0.1739,  0.3892]],
       dtype=torch.float64)
	q_value: tensor([[-8.4926]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6900543841590374, distance: 1.4876714662381683 entropy 0.18424582481384277
epoch: 14, step: 91
	action: tensor([[ 0.0308,  0.2243,  0.6781, -0.3414,  0.0668, -0.1671, -0.1546]],
       dtype=torch.float64)
	q_value: tensor([[-10.5180]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20614574604291913, distance: 1.0195935547032688 entropy 0.18424582481384277
epoch: 14, step: 92
	action: tensor([[ 0.4836,  0.3459, -0.2120, -0.4696, -0.3726,  0.0119, -0.2440]],
       dtype=torch.float64)
	q_value: tensor([[-10.2396]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7164464948290716, distance: 0.6093603659245046 entropy 0.18424582481384277
epoch: 14, step: 93
	action: tensor([[-0.7492, -0.2817, -0.5456, -0.2448,  0.1272,  0.0167, -0.1098]],
       dtype=torch.float64)
	q_value: tensor([[-9.6224]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7743646478997237, distance: 1.5243269616531787 entropy 0.18424582481384277
epoch: 14, step: 94
	action: tensor([[ 0.3589,  0.0429,  0.2741, -0.5104,  0.2305,  0.1950, -0.4284]],
       dtype=torch.float64)
	q_value: tensor([[-8.2481]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37705444733240656, distance: 0.903195443782709 entropy 0.18424582481384277
epoch: 14, step: 95
	action: tensor([[ 0.2070,  0.0675, -0.2653,  0.1583,  0.1957, -0.1800,  0.2006]],
       dtype=torch.float64)
	q_value: tensor([[-9.3829]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5365388148582018, distance: 0.7790464935972733 entropy 0.18424582481384277
epoch: 14, step: 96
	action: tensor([[-0.0203,  0.0443, -0.1950, -0.0015,  0.5108,  0.2100, -0.4572]],
       dtype=torch.float64)
	q_value: tensor([[-8.8430]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36848376882113976, distance: 0.9093874404892714 entropy 0.18424582481384277
epoch: 14, step: 97
	action: tensor([[-0.1473, -0.0478,  0.0736, -0.0872,  0.3084, -0.1202,  0.4394]],
       dtype=torch.float64)
	q_value: tensor([[-7.8884]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.013461347719117223, distance: 1.1366159495740034 entropy 0.18424582481384277
epoch: 14, step: 98
	action: tensor([[ 0.2625,  0.1659, -0.2773,  0.3735, -0.1787,  0.0598,  0.0635]],
       dtype=torch.float64)
	q_value: tensor([[-9.2178]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.693651475070272, distance: 0.6333803563494195 entropy 0.18424582481384277
epoch: 14, step: 99
	action: tensor([[-0.2661, -0.1968, -0.7957, -0.2871,  0.1050, -0.0362, -0.4338]],
       dtype=torch.float64)
	q_value: tensor([[-8.4018]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07916766900016237, distance: 1.1887790863594727 entropy 0.18424582481384277
epoch: 14, step: 100
	action: tensor([[-0.2346,  0.5173, -0.0472,  0.0391,  0.0346, -0.0521,  0.2373]],
       dtype=torch.float64)
	q_value: tensor([[-7.7546]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37390830543620057, distance: 0.9054733332748959 entropy 0.18424582481384277
epoch: 14, step: 101
	action: tensor([[ 0.1141,  0.3024, -0.3109, -0.1847, -0.5468, -0.1917,  0.1333]],
       dtype=torch.float64)
	q_value: tensor([[-8.1924]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.566522881460338, distance: 0.753424575654118 entropy 0.18424582481384277
epoch: 14, step: 102
	action: tensor([[ 0.1079, -0.0270, -0.6778, -0.1372,  0.0463,  0.1940, -0.4779]],
       dtype=torch.float64)
	q_value: tensor([[-9.0550]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.40799850827779827, distance: 0.8804771582897941 entropy 0.18424582481384277
epoch: 14, step: 103
	action: tensor([[-0.0433,  0.1032, -0.2841,  0.0075, -0.1346, -0.3506, -0.1155]],
       dtype=torch.float64)
	q_value: tensor([[-7.4160]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3346829280878353, distance: 0.9334069430650702 entropy 0.18424582481384277
epoch: 14, step: 104
	action: tensor([[-0.3970, -0.0322,  0.1547, -0.5211,  0.4775,  0.0273, -0.3161]],
       dtype=torch.float64)
	q_value: tensor([[-7.9580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.42597072639441014, distance: 1.3665074328040716 entropy 0.18424582481384277
epoch: 14, step: 105
	action: tensor([[ 0.0744,  0.0277, -0.4429, -0.1832,  0.3799,  0.0897,  0.2701]],
       dtype=torch.float64)
	q_value: tensor([[-8.2710]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3568637373901097, distance: 0.9177157644423632 entropy 0.18424582481384277
epoch: 14, step: 106
	action: tensor([[ 0.1165,  0.0510, -0.0381, -0.1953, -0.0839, -0.1821,  0.2095]],
       dtype=torch.float64)
	q_value: tensor([[-8.4304]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.301999063973853, distance: 0.9560590036023594 entropy 0.18424582481384277
epoch: 14, step: 107
	action: tensor([[-0.2380, -0.5270, -0.1707, -0.0299,  0.5631,  0.3300, -0.5668]],
       dtype=torch.float64)
	q_value: tensor([[-8.3531]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.28037051838653015, distance: 1.2948651012222874 entropy 0.18424582481384277
epoch: 14, step: 108
	action: tensor([[-0.3015, -0.1018, -0.7207, -0.2729,  0.2752,  0.0134, -0.2433]],
       dtype=torch.float64)
	q_value: tensor([[-9.0603]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07351969707574968, distance: 1.1856641862976884 entropy 0.18424582481384277
epoch: 14, step: 109
	action: tensor([[-0.0726, -0.0749,  0.0757, -0.3789,  0.0573,  0.3628, -0.4384]],
       dtype=torch.float64)
	q_value: tensor([[-7.5718]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08870987621765303, distance: 1.092408385163046 entropy 0.18424582481384277
epoch: 14, step: 110
	action: tensor([[ 0.1804,  0.2464, -0.4797,  0.0081,  0.4301, -0.3841, -0.0693]],
       dtype=torch.float64)
	q_value: tensor([[-8.1240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5680868869309059, distance: 0.7520641520886823 entropy 0.18424582481384277
epoch: 14, step: 111
	action: tensor([[-0.4980,  0.3285, -0.3484,  0.0246, -0.2948, -0.3050, -0.0335]],
       dtype=torch.float64)
	q_value: tensor([[-8.9532]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0011106281979383903, distance: 1.1437086069607665 entropy 0.18424582481384277
epoch: 14, step: 112
	action: tensor([[ 0.1653, -0.0105, -0.3895, -0.0523,  0.1696, -0.1828, -0.0665]],
       dtype=torch.float64)
	q_value: tensor([[-7.7938]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4039821531279313, distance: 0.8834588495307873 entropy 0.18424582481384277
epoch: 14, step: 113
	action: tensor([[-0.1686,  0.2613, -0.7241, -0.1770,  0.1228,  0.0952,  0.0075]],
       dtype=torch.float64)
	q_value: tensor([[-7.9435]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3432181090613061, distance: 0.9274003984158488 entropy 0.18424582481384277
epoch: 14, step: 114
	action: tensor([[-0.1602,  0.5853,  0.1524, -0.0028,  0.2511,  0.5613,  0.1325]],
       dtype=torch.float64)
	q_value: tensor([[-7.4253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5923085851142802, distance: 0.7306720189631581 entropy 0.18424582481384277
epoch: 14, step: 115
	action: tensor([[ 0.1219, -0.1985, -0.2280, -0.1007,  0.4129, -0.0033,  0.0104]],
       dtype=torch.float64)
	q_value: tensor([[-9.1805]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18823449534847203, distance: 1.0310316315986812 entropy 0.18424582481384277
epoch: 14, step: 116
	action: tensor([[-0.2237,  0.2084, -0.0244, -0.0196,  0.0727,  0.3218,  0.4166]],
       dtype=torch.float64)
	q_value: tensor([[-8.2608]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2730389254837534, distance: 0.9756909388492695 entropy 0.18424582481384277
epoch: 14, step: 117
	action: tensor([[ 0.1155, -0.3649, -0.0276, -0.2978,  0.2102,  0.0081,  0.4048]],
       dtype=torch.float64)
	q_value: tensor([[-8.7928]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08555924613945787, distance: 1.1922942750713796 entropy 0.18424582481384277
epoch: 14, step: 118
	action: tensor([[ 0.0781, -0.0725, -0.2527, -0.2246, -0.3478,  0.1757,  0.1108]],
       dtype=torch.float64)
	q_value: tensor([[-9.4718]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22820650603187542, distance: 1.0053267763100522 entropy 0.18424582481384277
epoch: 14, step: 119
	action: tensor([[-0.3902,  0.2782,  0.2877,  0.6229,  0.3636,  0.3645, -0.1397]],
       dtype=torch.float64)
	q_value: tensor([[-8.3648]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5147666258876804, distance: 0.7971352697028524 entropy 0.18424582481384277
epoch: 14, step: 120
	action: tensor([[ 0.1603, -0.0508,  0.0987, -0.5250,  0.1353,  0.0748,  0.0761]],
       dtype=torch.float64)
	q_value: tensor([[-9.9141]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12219128552222991, distance: 1.0721526843604963 entropy 0.18424582481384277
epoch: 14, step: 121
	action: tensor([[ 0.1107,  0.1573,  0.2306, -0.5391,  0.2268, -0.1711, -0.1300]],
       dtype=torch.float64)
	q_value: tensor([[-8.6689]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1616654233614082, distance: 1.0477686083840836 entropy 0.18424582481384277
epoch: 14, step: 122
	action: tensor([[-0.2496,  0.4011,  0.1253, -0.1895,  0.5443, -0.1156,  0.2989]],
       dtype=torch.float64)
	q_value: tensor([[-8.8626]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16041038096651195, distance: 1.048552604577224 entropy 0.18424582481384277
epoch: 14, step: 123
	action: tensor([[ 0.1506, -0.2096,  0.4456,  0.0497,  0.0230, -0.0529, -0.3406]],
       dtype=torch.float64)
	q_value: tensor([[-9.1680]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26458336690877016, distance: 0.9813488488347031 entropy 0.18424582481384277
epoch: 14, step: 124
	action: tensor([[-0.4946, -0.1263, -0.4433, -0.5528,  0.3516,  0.4451, -0.5547]],
       dtype=torch.float64)
	q_value: tensor([[-9.6390]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2959457292109944, distance: 1.302717060296172 entropy 0.18424582481384277
epoch: 14, step: 125
	action: tensor([[ 0.1870, -0.1873, -0.0217,  0.1095,  0.0803, -0.2268,  0.5850]],
       dtype=torch.float64)
	q_value: tensor([[-8.2368]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3106715134956425, distance: 0.9501010680773951 entropy 0.18424582481384277
epoch: 14, step: 126
	action: tensor([[-0.0749, -0.7680, -0.3722, -0.1887, -0.0387,  0.2310, -0.1579]],
       dtype=torch.float64)
	q_value: tensor([[-10.0830]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4851174189515095, distance: 1.3945596307894217 entropy 0.18424582481384277
epoch: 14, step: 127
	action: tensor([[ 0.6113,  0.1762,  0.4711, -0.1524, -0.3069, -0.1095, -0.3601]],
       dtype=torch.float64)
	q_value: tensor([[-8.7319]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7271264619670474, distance: 0.5977745266502152 entropy 0.18424582481384277
LOSS epoch 14 actor 42.57664954735427 critic 32.90710729035585 
epoch: 15, step: 0
	action: tensor([[ 0.2877, -0.4827, -0.0417,  0.0709,  0.0219,  0.0641,  0.0553]],
       dtype=torch.float64)
	q_value: tensor([[-9.9826]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16885507973884373, distance: 1.043266040269106 entropy 0.18424582481384277
epoch: 15, step: 1
	action: tensor([[ 0.0427, -0.0062, -0.3700, -0.3576,  0.2543,  0.5544, -0.0694]],
       dtype=torch.float64)
	q_value: tensor([[-7.8201]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38848079508550826, distance: 0.8948736981629709 entropy 0.18424582481384277
epoch: 15, step: 2
	action: tensor([[ 0.1257, -0.1830, -0.5286, -0.0594,  0.0512,  0.1045, -0.0261]],
       dtype=torch.float64)
	q_value: tensor([[-6.9526]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2947391657372116, distance: 0.9610181203754022 entropy 0.18424582481384277
epoch: 15, step: 3
	action: tensor([[-0.1009, -0.0389,  0.0179, -0.2065,  0.2862, -0.1108,  0.1898]],
       dtype=torch.float64)
	q_value: tensor([[-6.4019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03445293559169671, distance: 1.1244584623283875 entropy 0.18424582481384277
epoch: 15, step: 4
	action: tensor([[-0.5835,  0.1243,  0.0047,  0.0170, -0.1243, -0.2606,  0.0957]],
       dtype=torch.float64)
	q_value: tensor([[-7.1013]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3190565787452435, distance: 1.3142815322582608 entropy 0.18424582481384277
epoch: 15, step: 5
	action: tensor([[ 0.0057, -0.2480, -0.0620, -0.1048,  0.2190,  0.3439, -0.1865]],
       dtype=torch.float64)
	q_value: tensor([[-6.9507]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.134963008142287, distance: 1.0643244355953543 entropy 0.18424582481384277
epoch: 15, step: 6
	action: tensor([[-0.3109, -0.4640, -0.1418, -0.4128,  0.2948, -0.1135, -0.1323]],
       dtype=torch.float64)
	q_value: tensor([[-6.7600]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5838889012355817, distance: 1.4401875460650577 entropy 0.18424582481384277
epoch: 15, step: 7
	action: tensor([[ 0.3889, -0.1499, -0.0839, -0.2485,  0.0581,  0.1415, -0.7940]],
       dtype=torch.float64)
	q_value: tensor([[-7.0497]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36720747431596845, distance: 0.9103059128193075 entropy 0.18424582481384277
epoch: 15, step: 8
	action: tensor([[ 0.3554,  0.7489, -0.2765,  0.0102,  0.2137, -0.2444, -0.1024]],
       dtype=torch.float64)
	q_value: tensor([[-7.7910]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8502064821699312, distance: 0.442897473959237 entropy 0.18424582481384277
epoch: 15, step: 9
	action: tensor([[ 0.1522, -0.0845, -0.2171, -0.8052, -0.1303,  0.1492, -0.1899]],
       dtype=torch.float64)
	q_value: tensor([[-8.1691]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09822203004321872, distance: 1.0866920853766453 entropy 0.18424582481384277
epoch: 15, step: 10
	action: tensor([[ 0.2517, -0.0013, -0.0466, -0.0682,  0.2083, -0.0629,  0.2218]],
       dtype=torch.float64)
	q_value: tensor([[-7.6705]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4760443315972468, distance: 0.8283310950766519 entropy 0.18424582481384277
epoch: 15, step: 11
	action: tensor([[ 0.4137,  0.3550, -0.4182,  0.2237,  0.4396,  0.5210, -0.0879]],
       dtype=torch.float64)
	q_value: tensor([[-7.5821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.88707318440742, distance: 0.384552141427748 entropy 0.18424582481384277
epoch: 15, step: 12
	action: tensor([[-0.1904, -0.1202, -0.0338, -0.4772,  0.4433, -0.0639,  0.3831]],
       dtype=torch.float64)
	q_value: tensor([[-8.0773]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2370395972723789, distance: 1.2727657911479147 entropy 0.18424582481384277
epoch: 15, step: 13
	action: tensor([[ 0.3734, -0.1558,  0.1465,  0.0277, -0.1997,  0.1762, -0.1609]],
       dtype=torch.float64)
	q_value: tensor([[-7.7174]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5195299532537627, distance: 0.7932130532121509 entropy 0.18424582481384277
epoch: 15, step: 14
	action: tensor([[ 0.1252,  0.3352, -0.0607, -0.2768,  0.0278,  0.1831,  0.0517]],
       dtype=torch.float64)
	q_value: tensor([[-7.9369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5653809247413244, distance: 0.7544163377534625 entropy 0.18424582481384277
epoch: 15, step: 15
	action: tensor([[ 0.1508,  0.1152,  0.4180,  0.0695, -0.3456,  0.1838, -0.0692]],
       dtype=torch.float64)
	q_value: tensor([[-7.0442]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5909968532967029, distance: 0.7318465298769009 entropy 0.18424582481384277
epoch: 15, step: 16
	action: tensor([[-0.1354, -0.3629,  0.1236, -0.1920,  0.4538, -0.0180, -0.1055]],
       dtype=torch.float64)
	q_value: tensor([[-8.5070]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2755234057186309, distance: 1.2924117846494834 entropy 0.18424582481384277
epoch: 15, step: 17
	action: tensor([[-0.3836, -0.0648,  0.2991,  0.2299, -0.5829, -0.2250,  0.0142]],
       dtype=torch.float64)
	q_value: tensor([[-7.3182]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06201528247829469, distance: 1.1792939660129393 entropy 0.18424582481384277
epoch: 15, step: 18
	action: tensor([[-0.0464,  0.4936,  0.2432, -0.0300,  0.1554, -0.3146,  0.1558]],
       dtype=torch.float64)
	q_value: tensor([[-8.2147]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.449422843118917, distance: 0.8491135819608034 entropy 0.18424582481384277
epoch: 15, step: 19
	action: tensor([[-0.2197, -0.1488,  0.0500,  0.3851, -0.3458,  0.3104,  0.2046]],
       dtype=torch.float64)
	q_value: tensor([[-8.0170]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21018462798785076, distance: 1.0169965608961615 entropy 0.18424582481384277
epoch: 15, step: 20
	action: tensor([[-0.3759, -0.3856, -0.2298,  0.3912,  0.2911, -0.1116, -0.1853]],
       dtype=torch.float64)
	q_value: tensor([[-7.8436]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2915738201397775, distance: 1.300517827763385 entropy 0.18424582481384277
epoch: 15, step: 21
	action: tensor([[-0.3722, -0.2309, -0.4858, -0.3750,  0.7394, -0.0437, -0.4215]],
       dtype=torch.float64)
	q_value: tensor([[-7.7917]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.32692735347614676, distance: 1.3181968413885095 entropy 0.18424582481384277
epoch: 15, step: 22
	action: tensor([[ 0.1554,  0.4817, -0.5266, -0.4890, -0.2793, -0.2106, -0.0756]],
       dtype=torch.float64)
	q_value: tensor([[-7.1721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6844560722628152, distance: 0.6428158946196784 entropy 0.18424582481384277
epoch: 15, step: 23
	action: tensor([[-0.1647, -0.6857, -0.1341,  0.0133,  0.1345, -0.6119,  0.1064]],
       dtype=torch.float64)
	q_value: tensor([[-7.5447]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5313868532585613, distance: 1.4161170459937282 entropy 0.18424582481384277
epoch: 15, step: 24
	action: tensor([[ 0.0055, -0.1048, -0.2499, -0.2310,  0.4759, -0.1761, -0.3769]],
       dtype=torch.float64)
	q_value: tensor([[-9.3164]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09651459404922058, distance: 1.0877203760355763 entropy 0.18424582481384277
epoch: 15, step: 25
	action: tensor([[ 0.3674,  0.4479,  0.2569,  0.2819,  0.3081, -0.0825, -0.0696]],
       dtype=torch.float64)
	q_value: tensor([[-6.8573]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8802992953964933, distance: 0.395917817435741 entropy 0.18424582481384277
epoch: 15, step: 26
	action: tensor([[-0.2674,  0.3380, -0.5134, -0.5312,  0.3402,  0.7308, -0.6617]],
       dtype=torch.float64)
	q_value: tensor([[-8.5536]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37791871973092817, distance: 0.902568681275164 entropy 0.18424582481384277
epoch: 15, step: 27
	action: tensor([[ 0.3088, -0.3252, -0.3006, -0.2518,  0.0245,  0.2596, -0.1500]],
       dtype=torch.float64)
	q_value: tensor([[-7.6600]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24482121523251255, distance: 0.9944468666090021 entropy 0.18424582481384277
epoch: 15, step: 28
	action: tensor([[ 0.1661,  0.2038,  0.2434, -0.0578,  0.2050,  0.7371,  0.0630]],
       dtype=torch.float64)
	q_value: tensor([[-7.1560]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7548398007453186, distance: 0.5666066614412488 entropy 0.18424582481384277
epoch: 15, step: 29
	action: tensor([[-0.1892, -0.1205, -0.1933, -0.1755, -0.2130, -0.0336, -0.2262]],
       dtype=torch.float64)
	q_value: tensor([[-8.2679]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0385236068218584, distance: 1.1661780955609788 entropy 0.18424582481384277
epoch: 15, step: 30
	action: tensor([[-0.6143, -0.1144,  0.2129, -0.4833, -0.2742,  0.0669,  0.0377]],
       dtype=torch.float64)
	q_value: tensor([[-6.3390]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7471725105255886, distance: 1.5126017108707304 entropy 0.18424582481384277
epoch: 15, step: 31
	action: tensor([[ 0.2642,  0.0151,  0.4060, -0.2530,  0.3967, -0.2343,  0.0426]],
       dtype=torch.float64)
	q_value: tensor([[-7.8711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3132555791537337, distance: 0.9483185878991295 entropy 0.18424582481384277
epoch: 15, step: 32
	action: tensor([[-0.2153,  0.0991, -0.5695, -0.1908, -0.2296, -0.1651,  0.0203]],
       dtype=torch.float64)
	q_value: tensor([[-8.6240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11647931654252053, distance: 1.0756353181926241 entropy 0.18424582481384277
epoch: 15, step: 33
	action: tensor([[ 0.2299,  0.0560,  0.1032,  0.1894, -0.3504, -0.2860, -0.5629]],
       dtype=torch.float64)
	q_value: tensor([[-6.4452]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5704052869686986, distance: 0.7500429913645984 entropy 0.18424582481384277
epoch: 15, step: 34
	action: tensor([[ 0.1499,  0.2468, -0.4881, -0.0989, -0.4900, -0.3914,  0.0077]],
       dtype=torch.float64)
	q_value: tensor([[-8.3870]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.619333126531445, distance: 0.7060398854821406 entropy 0.18424582481384277
epoch: 15, step: 35
	action: tensor([[ 0.0873,  0.1505, -0.0898, -0.5475,  0.2491,  0.3104, -0.4085]],
       dtype=torch.float64)
	q_value: tensor([[-7.7877]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3829750512011364, distance: 0.8988931183189379 entropy 0.18424582481384277
epoch: 15, step: 36
	action: tensor([[ 0.9685,  0.3412, -0.3179,  0.1784, -0.3472,  0.1558, -0.4423]],
       dtype=torch.float64)
	q_value: tensor([[-6.7793]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9718070886628373, distance: 0.19214392415165432 entropy 0.18424582481384277
epoch: 15, step: 37
	action: tensor([[ 0.2068,  0.2794,  0.2402, -0.4590,  0.6579,  0.2913, -0.5444]],
       dtype=torch.float64)
	q_value: tensor([[-9.9943]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5474543451888472, distance: 0.7698177009772638 entropy 0.18424582481384277
epoch: 15, step: 38
	action: tensor([[-0.0174, -0.1035,  0.2597, -0.1499, -0.4124, -0.1819, -0.3479]],
       dtype=torch.float64)
	q_value: tensor([[-7.9052]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09403549454820259, distance: 1.089211667295148 entropy 0.18424582481384277
epoch: 15, step: 39
	action: tensor([[-0.0276, -0.2316, -0.2390,  0.0410,  0.2531, -0.1611, -0.4622]],
       dtype=torch.float64)
	q_value: tensor([[-7.7759]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08220080356440507, distance: 1.0963028164450925 entropy 0.18424582481384277
epoch: 15, step: 40
	action: tensor([[-0.0712,  0.0216, -0.3668,  0.4056, -0.2720, -0.2421,  0.0600]],
       dtype=torch.float64)
	q_value: tensor([[-6.9758]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3461367181196535, distance: 0.9253375119597057 entropy 0.18424582481384277
epoch: 15, step: 41
	action: tensor([[ 0.1751, -0.4143, -0.0239,  0.4017,  0.3514, -0.4269,  0.0034]],
       dtype=torch.float64)
	q_value: tensor([[-7.1977]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20575017496511294, distance: 1.0198475506281404 entropy 0.18424582481384277
epoch: 15, step: 42
	action: tensor([[ 0.0404,  0.2196, -0.0682, -0.0227, -0.0864, -0.2879,  0.1801]],
       dtype=torch.float64)
	q_value: tensor([[-9.1505]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4488429359507945, distance: 0.8495606377959846 entropy 0.18424582481384277
epoch: 15, step: 43
	action: tensor([[-0.4962,  0.1107, -0.2052, -0.4218, -0.0323,  0.5155, -0.1012]],
       dtype=torch.float64)
	q_value: tensor([[-7.1860]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20573295867960284, distance: 1.256557189971185 entropy 0.18424582481384277
epoch: 15, step: 44
	action: tensor([[-0.1521, -0.0426, -0.0776, -0.2441, -0.0175, -0.0509, -0.4761]],
       dtype=torch.float64)
	q_value: tensor([[-7.1059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.015170467463577197, distance: 1.1356309628977297 entropy 0.18424582481384277
epoch: 15, step: 45
	action: tensor([[-0.3056,  0.2062, -0.3982, -0.3666, -0.3610,  0.5554, -0.0939]],
       dtype=torch.float64)
	q_value: tensor([[-6.4569]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06401587818715238, distance: 1.1071104088630317 entropy 0.18424582481384277
epoch: 15, step: 46
	action: tensor([[ 0.5293,  0.0686,  0.2467,  0.1341,  0.4315, -0.1896,  0.3871]],
       dtype=torch.float64)
	q_value: tensor([[-7.2991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7591933032327918, distance: 0.5615532866899263 entropy 0.18424582481384277
epoch: 15, step: 47
	action: tensor([[-0.8116,  0.1290, -0.4823, -0.5068,  0.1384, -0.4820, -0.0467]],
       dtype=torch.float64)
	q_value: tensor([[-9.7096]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5718346177852192, distance: 1.4346967613793102 entropy 0.18424582481384277
epoch: 15, step: 48
	action: tensor([[-0.1186, -0.1494, -0.1678, -0.0316,  0.5092, -0.3512, -0.8567]],
       dtype=torch.float64)
	q_value: tensor([[-7.0970]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08030648064142953, distance: 1.1894061615282427 entropy 0.18424582481384277
epoch: 15, step: 49
	action: tensor([[ 0.1518,  0.1490, -0.5000, -0.1448, -0.3904,  0.0818, -0.1201]],
       dtype=torch.float64)
	q_value: tensor([[-8.0223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.49509707530010083, distance: 0.8131312208226904 entropy 0.18424582481384277
epoch: 15, step: 50
	action: tensor([[ 0.2900,  0.0974, -0.2749,  0.1205, -0.1033, -0.0892,  0.0606]],
       dtype=torch.float64)
	q_value: tensor([[-6.9378]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.632034329618858, distance: 0.6941612150789971 entropy 0.18424582481384277
epoch: 15, step: 51
	action: tensor([[-0.4083,  0.0462,  0.0010, -0.1710, -0.1397, -0.1022,  0.2721]],
       dtype=torch.float64)
	q_value: tensor([[-7.0897]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20667552132207034, distance: 1.257048240862248 entropy 0.18424582481384277
epoch: 15, step: 52
	action: tensor([[-0.2123, -0.1477, -0.2648, -0.2404,  0.0257,  0.1170,  0.2218]],
       dtype=torch.float64)
	q_value: tensor([[-7.0958]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09728936001016919, distance: 1.198718691489373 entropy 0.18424582481384277
epoch: 15, step: 53
	action: tensor([[-0.0601, -0.2534,  0.0414, -0.1115,  0.0405,  0.0529, -0.0586]],
       dtype=torch.float64)
	q_value: tensor([[-6.6164]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.023236573861403542, distance: 1.157563223977992 entropy 0.18424582481384277
epoch: 15, step: 54
	action: tensor([[-0.5654,  0.0022, -0.1407, -0.2135, -0.2198,  0.1038,  0.3313]],
       dtype=torch.float64)
	q_value: tensor([[-6.8379]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.41175189249999344, distance: 1.359677411343849 entropy 0.18424582481384277
epoch: 15, step: 55
	action: tensor([[ 0.4715, -0.1417,  0.2538, -0.1043, -0.1210, -0.0554,  0.2080]],
       dtype=torch.float64)
	q_value: tensor([[-7.2647]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4556126894263287, distance: 0.8443270244329569 entropy 0.18424582481384277
epoch: 15, step: 56
	action: tensor([[ 9.6751e-01,  2.3015e-01, -1.8954e-01, -8.7509e-02, -9.0348e-02,
         -4.9342e-01, -2.5223e-04]], dtype=torch.float64)
	q_value: tensor([[-8.5925]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7575540787994185, distance: 0.5634613537367505 entropy 0.18424582481384277
epoch: 15, step: 57
	action: tensor([[ 0.3077,  0.3392, -0.1165, -0.1446, -0.0914,  0.5075,  0.5949]],
       dtype=torch.float64)
	q_value: tensor([[-10.3795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7880598346341331, distance: 0.5268213024515533 entropy 0.18424582481384277
epoch: 15, step: 58
	action: tensor([[-0.0745, -0.0602, -0.2981,  0.2091,  0.2383,  0.2937,  0.0729]],
       dtype=torch.float64)
	q_value: tensor([[-9.0950]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3306771773900611, distance: 0.936212661271022 entropy 0.18424582481384277
epoch: 15, step: 59
	action: tensor([[ 0.4617, -0.5295, -0.0102, -0.6886, -0.0637,  0.2157,  0.1216]],
       dtype=torch.float64)
	q_value: tensor([[-6.7964]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24944715281269847, distance: 1.2791328109483597 entropy 0.18424582481384277
epoch: 15, step: 60
	action: tensor([[ 0.2538, -0.1016,  0.4299,  0.1194, -0.1280, -0.2766,  0.1953]],
       dtype=torch.float64)
	q_value: tensor([[-9.0480]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4386482921426077, distance: 0.8573817152836745 entropy 0.18424582481384277
epoch: 15, step: 61
	action: tensor([[-0.0160, -0.0528, -0.2705, -0.3876,  0.1936, -0.2576, -0.1159]],
       dtype=torch.float64)
	q_value: tensor([[-8.8153]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06208749804238223, distance: 1.108250295309344 entropy 0.18424582481384277
epoch: 15, step: 62
	action: tensor([[ 0.0233,  0.0396,  0.1668, -0.0717,  0.1881,  0.6619, -0.2349]],
       dtype=torch.float64)
	q_value: tensor([[-6.6228]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5093178528030644, distance: 0.8015983633247641 entropy 0.18424582481384277
epoch: 15, step: 63
	action: tensor([[ 0.1773, -0.0324, -0.0577, -0.2206,  0.2595, -0.4252, -0.3110]],
       dtype=torch.float64)
	q_value: tensor([[-7.4811]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21499008371760775, distance: 1.0138979960345271 entropy 0.18424582481384277
epoch: 15, step: 64
	action: tensor([[-0.4637,  0.5265, -0.4239, -0.1315, -0.2291, -0.1736, -0.0596]],
       dtype=torch.float64)
	q_value: tensor([[-7.5241]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12245394714854807, distance: 1.0719922653348999 entropy 0.18424582481384277
epoch: 15, step: 65
	action: tensor([[ 0.3526,  0.0622, -0.1598, -0.2695,  0.2774, -0.0757, -0.0353]],
       dtype=torch.float64)
	q_value: tensor([[-6.5473]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4909201539790986, distance: 0.8164876974203694 entropy 0.18424582481384277
epoch: 15, step: 66
	action: tensor([[ 0.3917,  0.5695, -0.5652, -0.1320, -0.2552,  0.0028,  0.6647]],
       dtype=torch.float64)
	q_value: tensor([[-7.2894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8691892143663963, distance: 0.4138838404396732 entropy 0.18424582481384277
epoch: 15, step: 67
	action: tensor([[-0.0823, -0.2069, -0.2469, -0.2305,  0.4709,  0.2501, -0.4035]],
       dtype=torch.float64)
	q_value: tensor([[-9.4600]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07656846439436393, distance: 1.0996615603805922 entropy 0.18424582481384277
epoch: 15, step: 68
	action: tensor([[ 0.2797,  0.2549, -0.3295, -0.2160, -0.0322,  0.3202,  0.0207]],
       dtype=torch.float64)
	q_value: tensor([[-6.5571]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.694911548037195, distance: 0.6320764038509128 entropy 0.18424582481384277
epoch: 15, step: 69
	action: tensor([[-0.1311,  0.1547, -0.1147, -0.0658, -0.0644, -0.2528, -0.2552]],
       dtype=torch.float64)
	q_value: tensor([[-7.1035]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24080705927857693, distance: 0.9970863569186487 entropy 0.18424582481384277
epoch: 15, step: 70
	action: tensor([[ 0.6711, -0.0911, -0.1399, -0.2792, -0.3007,  0.1495, -0.6126]],
       dtype=torch.float64)
	q_value: tensor([[-6.5601]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.539908266712581, distance: 0.7762094188332319 entropy 0.18424582481384277
epoch: 15, step: 71
	action: tensor([[-0.1858,  0.0746, -0.2279, -0.2195,  0.1026,  0.0070, -0.3962]],
       dtype=torch.float64)
	q_value: tensor([[-8.7361]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12025161486517133, distance: 1.073336584402131 entropy 0.18424582481384277
epoch: 15, step: 72
	action: tensor([[-0.6646, -0.3908, -0.1617, -0.2276,  0.2238, -0.1036,  0.0305]],
       dtype=torch.float64)
	q_value: tensor([[-5.9632]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.8267531537881361, distance: 1.54666631173165 entropy 0.18424582481384277
epoch: 15, step: 73
	action: tensor([[ 0.0152, -0.3879, -0.5548,  0.0187, -0.0327, -0.1237, -0.1553]],
       dtype=torch.float64)
	q_value: tensor([[-7.4517]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.049231247928094746, distance: 1.1721745864559947 entropy 0.18424582481384277
epoch: 15, step: 74
	action: tensor([[-0.5308,  0.2285, -0.3346, -0.0026, -0.2885,  0.3584, -0.7567]],
       dtype=torch.float64)
	q_value: tensor([[-6.8996]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18897445883713626, distance: 1.2477941808436048 entropy 0.18424582481384277
epoch: 15, step: 75
	action: tensor([[-0.0175,  0.0046,  0.2602, -0.4408, -0.1093,  0.4083, -0.2377]],
       dtype=torch.float64)
	q_value: tensor([[-6.9523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.129127658914586, distance: 1.0679082510848008 entropy 0.18424582481384277
epoch: 15, step: 76
	action: tensor([[ 0.4605, -0.5355, -0.1938, -0.1368,  0.0872,  0.2713, -0.0482]],
       dtype=torch.float64)
	q_value: tensor([[-7.8813]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12676317214214394, distance: 1.0693569957555302 entropy 0.18424582481384277
epoch: 15, step: 77
	action: tensor([[ 0.3753, -0.5440, -0.1773, -0.5316,  0.2035,  0.1069,  0.1546]],
       dtype=torch.float64)
	q_value: tensor([[-8.0003]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20421790171836562, distance: 1.255767481885925 entropy 0.18424582481384277
epoch: 15, step: 78
	action: tensor([[ 0.3900,  0.0041, -0.3671, -0.4010,  0.1007,  0.0795, -0.4623]],
       dtype=torch.float64)
	q_value: tensor([[-8.2888]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4632035742345668, distance: 0.838419751725512 entropy 0.18424582481384277
epoch: 15, step: 79
	action: tensor([[ 0.0881, -0.3802, -0.4519,  0.0367, -0.3814, -0.0314, -0.2508]],
       dtype=torch.float64)
	q_value: tensor([[-7.1226]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09692360555771229, distance: 1.087474140451037 entropy 0.18424582481384277
epoch: 15, step: 80
	action: tensor([[ 0.2774, -0.3610,  0.1807,  0.0133,  0.4563, -0.0209,  0.2656]],
       dtype=torch.float64)
	q_value: tensor([[-7.0559]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21592254438097358, distance: 1.0132956463727711 entropy 0.18424582481384277
epoch: 15, step: 81
	action: tensor([[-0.2605, -0.0316,  0.1123, -0.0243,  0.3264,  0.0577,  0.1794]],
       dtype=torch.float64)
	q_value: tensor([[-8.8328]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0024691352768707597, distance: 1.1429306104581778 entropy 0.18424582481384277
epoch: 15, step: 82
	action: tensor([[ 0.3669, -0.6867, -0.0692, -0.0123,  0.6506, -0.1652,  0.4182]],
       dtype=torch.float64)
	q_value: tensor([[-7.2427]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13546406915251197, distance: 1.219392139725448 entropy 0.18424582481384277
epoch: 15, step: 83
	action: tensor([[ 0.4241,  0.2018,  0.0023, -0.3959, -0.0373, -0.3381, -0.0921]],
       dtype=torch.float64)
	q_value: tensor([[-9.9426]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5173438052394228, distance: 0.7950155725388617 entropy 0.18424582481384277
epoch: 15, step: 84
	action: tensor([[-0.1239,  0.1704, -0.4626,  0.1485,  0.2401, -0.2751, -0.5509]],
       dtype=torch.float64)
	q_value: tensor([[-8.1468]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27799834153231995, distance: 0.9723571025828963 entropy 0.18424582481384277
epoch: 15, step: 85
	action: tensor([[-0.1104,  0.3973,  0.1400,  0.1010, -0.0024, -0.1730, -0.1690]],
       dtype=torch.float64)
	q_value: tensor([[-7.0828]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4497299715228934, distance: 0.8488767184487663 entropy 0.18424582481384277
epoch: 15, step: 86
	action: tensor([[-0.0698,  0.1289, -0.1155,  0.0847,  0.1017, -0.1189, -0.4688]],
       dtype=torch.float64)
	q_value: tensor([[-7.3664]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34541373155277644, distance: 0.9258489504966223 entropy 0.18424582481384277
epoch: 15, step: 87
	action: tensor([[-0.2700,  0.5495,  0.2095,  0.1369,  0.3330,  0.3104,  0.1677]],
       dtype=torch.float64)
	q_value: tensor([[-6.7340]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.49501037858956043, distance: 0.8132010290696738 entropy 0.18424582481384277
epoch: 15, step: 88
	action: tensor([[-0.1972,  0.0512,  0.2246,  0.2503, -0.2891,  0.0978,  0.1993]],
       dtype=torch.float64)
	q_value: tensor([[-7.8089]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2912313256743817, distance: 0.9634051212440892 entropy 0.18424582481384277
epoch: 15, step: 89
	action: tensor([[-0.0297,  0.0676, -0.1923,  0.1598, -0.2038, -0.0485,  0.2334]],
       dtype=torch.float64)
	q_value: tensor([[-7.7469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3549413287629338, distance: 0.9190863202671269 entropy 0.18424582481384277
epoch: 15, step: 90
	action: tensor([[ 0.5467,  0.2145, -0.1027, -0.5215,  0.2881,  0.0591, -0.0438]],
       dtype=torch.float64)
	q_value: tensor([[-6.9832]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6289330380440894, distance: 0.697080345125271 entropy 0.18424582481384277
epoch: 15, step: 91
	action: tensor([[-0.4554,  0.2143, -0.0426, -0.5141,  0.0561, -0.0950, -0.3664]],
       dtype=torch.float64)
	q_value: tensor([[-8.2215]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24891092318234032, distance: 1.278858296527269 entropy 0.18424582481384277
epoch: 15, step: 92
	action: tensor([[ 0.0389,  0.3238, -0.0187, -0.1961,  0.1600,  0.0122,  0.1403]],
       dtype=torch.float64)
	q_value: tensor([[-6.4205]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4682282793477892, distance: 0.8344864943737008 entropy 0.18424582481384277
epoch: 15, step: 93
	action: tensor([[ 0.2140,  0.4337,  0.0555, -0.0597,  0.1421,  0.0360,  0.2348]],
       dtype=torch.float64)
	q_value: tensor([[-6.9196]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7087757730460928, distance: 0.617547607946358 entropy 0.18424582481384277
epoch: 15, step: 94
	action: tensor([[-0.3413,  0.0738, -0.4382, -0.0547,  0.3892,  0.4806, -0.1192]],
       dtype=torch.float64)
	q_value: tensor([[-7.6297]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.095682599377924, distance: 1.0882210865641848 entropy 0.18424582481384277
epoch: 15, step: 95
	action: tensor([[ 0.2552,  0.0948, -0.4285,  0.1080,  0.2840,  0.6635,  0.0060]],
       dtype=torch.float64)
	q_value: tensor([[-6.4994]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7058878230667346, distance: 0.6206020361146193 entropy 0.18424582481384277
epoch: 15, step: 96
	action: tensor([[-0.5268,  0.0737,  0.0171, -0.5897, -0.0634,  0.1534, -0.4977]],
       dtype=torch.float64)
	q_value: tensor([[-7.5511]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.44799906689133406, distance: 1.3770218587371001 entropy 0.18424582481384277
epoch: 15, step: 97
	action: tensor([[ 0.0564,  0.0403, -0.4123, -0.1872,  0.4190,  0.4056, -0.2635]],
       dtype=torch.float64)
	q_value: tensor([[-6.9570]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41853781395723166, distance: 0.8726044664944319 entropy 0.18424582481384277
epoch: 15, step: 98
	action: tensor([[-0.2924,  0.0446, -0.2489,  0.5036,  0.0789, -0.1609,  0.1905]],
       dtype=torch.float64)
	q_value: tensor([[-6.5577]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14978566976546537, distance: 1.055166279052976 entropy 0.18424582481384277
epoch: 15, step: 99
	action: tensor([[ 0.4450,  0.6944, -0.3633, -0.4943,  0.4651, -0.1796,  0.0465]],
       dtype=torch.float64)
	q_value: tensor([[-7.4806]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.881958580093973, distance: 0.3931641456284826 entropy 0.18424582481384277
epoch: 15, step: 100
	action: tensor([[-0.1980,  0.0658, -0.3654, -0.1908,  0.0866,  0.3120,  0.4081]],
       dtype=torch.float64)
	q_value: tensor([[-8.8136]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16389729470936343, distance: 1.0463729584281498 entropy 0.18424582481384277
epoch: 15, step: 101
	action: tensor([[ 0.0640,  0.1847,  0.0667,  0.3414,  0.2222,  0.1232, -0.4387]],
       dtype=torch.float64)
	q_value: tensor([[-7.2472]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6420660905681188, distance: 0.6846334516259441 entropy 0.18424582481384277
epoch: 15, step: 102
	action: tensor([[-0.1854, -0.3641,  0.2037,  0.2366,  0.0916,  0.2930, -0.2532]],
       dtype=torch.float64)
	q_value: tensor([[-7.5350]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09561155559368473, distance: 1.0882638314155375 entropy 0.18424582481384277
epoch: 15, step: 103
	action: tensor([[ 0.8627,  0.1226, -0.3716, -0.3091,  0.2210, -0.2269, -0.3203]],
       dtype=torch.float64)
	q_value: tensor([[-7.5604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6366509236617957, distance: 0.689792905788836 entropy 0.18424582481384277
epoch: 15, step: 104
	action: tensor([[-0.3205,  0.0552, -0.0233,  0.1173,  0.0757,  0.0237, -0.2093]],
       dtype=torch.float64)
	q_value: tensor([[-9.5415]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06698858476269887, distance: 1.1053509069458498 entropy 0.18424582481384277
epoch: 15, step: 105
	action: tensor([[ 0.2039,  0.2026, -0.3814, -0.0401,  0.1936,  0.4220,  0.2504]],
       dtype=torch.float64)
	q_value: tensor([[-6.5160]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6623795969210152, distance: 0.6649225094928826 entropy 0.18424582481384277
epoch: 15, step: 106
	action: tensor([[ 0.1874,  0.2965, -0.6732, -0.5218,  0.0409,  0.2327, -0.1801]],
       dtype=torch.float64)
	q_value: tensor([[-7.3418]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6497728515068038, distance: 0.6772228410453414 entropy 0.18424582481384277
epoch: 15, step: 107
	action: tensor([[ 0.0104, -0.1479, -0.6317,  0.4013,  0.3130,  0.3090,  0.1155]],
       dtype=torch.float64)
	q_value: tensor([[-7.2706]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3090477806096078, distance: 0.9512194049612845 entropy 0.18424582481384277
epoch: 15, step: 108
	action: tensor([[ 0.5242, -0.1385, -0.1388, -0.2719,  0.2097, -0.4670,  0.2746]],
       dtype=torch.float64)
	q_value: tensor([[-7.5464]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29316384278306074, distance: 0.9620908224000361 entropy 0.18424582481384277
epoch: 15, step: 109
	action: tensor([[ 0.1321,  0.0764, -0.1503, -0.2159, -0.0192,  0.0722,  0.1460]],
       dtype=torch.float64)
	q_value: tensor([[-9.1636]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4173637322686582, distance: 0.873484998627391 entropy 0.18424582481384277
epoch: 15, step: 110
	action: tensor([[ 0.6430,  0.2640, -0.3650, -0.0697,  0.5954,  0.3247, -0.0663]],
       dtype=torch.float64)
	q_value: tensor([[-6.8194]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9105722118215587, distance: 0.34221019047786394 entropy 0.18424582481384277
epoch: 15, step: 111
	action: tensor([[-0.2642, -0.5251, -0.0244, -0.3120,  0.1197, -0.2904,  0.7607]],
       dtype=torch.float64)
	q_value: tensor([[-8.7728]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.6224159170258254, distance: 1.4575980964825133 entropy 0.18424582481384277
epoch: 15, step: 112
	action: tensor([[ 0.0696,  0.3406, -0.7956, -0.4403,  0.1390,  0.0465, -0.1929]],
       dtype=torch.float64)
	q_value: tensor([[-9.6691]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6040262097061416, distance: 0.7200951960298266 entropy 0.18424582481384277
epoch: 15, step: 113
	action: tensor([[ 0.1256,  0.5429, -0.1467, -0.1133,  0.4574,  0.8277,  0.2182]],
       dtype=torch.float64)
	q_value: tensor([[-6.9845]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7940430890833188, distance: 0.5193317540682787 entropy 0.18424582481384277
epoch: 15, step: 114
	action: tensor([[ 0.1216, -0.2542, -0.4224,  0.0378,  0.0254, -0.2848, -0.4670]],
       dtype=torch.float64)
	q_value: tensor([[-8.6743]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1947848152950996, distance: 1.0268633919246184 entropy 0.18424582481384277
epoch: 15, step: 115
	action: tensor([[ 0.2566, -0.1123,  0.1122,  0.1080,  0.8780,  0.1769, -0.0071]],
       dtype=torch.float64)
	q_value: tensor([[-7.2671]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5492048096886307, distance: 0.7683274157961575 entropy 0.18424582481384277
epoch: 15, step: 116
	action: tensor([[ 0.4973,  0.4623, -0.5553, -0.4837,  0.1446, -0.1694, -0.1700]],
       dtype=torch.float64)
	q_value: tensor([[-8.6941]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8212181322210466, distance: 0.4838585597039883 entropy 0.18424582481384277
epoch: 15, step: 117
	action: tensor([[ 0.8400, -0.1746,  0.0583,  0.1682,  0.2351,  0.0019, -0.1336]],
       dtype=torch.float64)
	q_value: tensor([[-8.2680]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6971101387787033, distance: 0.6297947867876802 entropy 0.18424582481384277
epoch: 15, step: 118
	action: tensor([[-0.3444, -0.4518,  0.0322,  0.2180,  0.0134,  0.2056, -0.1478]],
       dtype=torch.float64)
	q_value: tensor([[-9.5989]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.226711432566292, distance: 1.2674414313553535 entropy 0.18424582481384277
epoch: 15, step: 119
	action: tensor([[-0.3159,  0.0765, -0.2816, -0.2399,  0.4281,  0.4412,  0.0980]],
       dtype=torch.float64)
	q_value: tensor([[-7.4133]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08614559729665316, distance: 1.0939442692289023 entropy 0.18424582481384277
epoch: 15, step: 120
	action: tensor([[ 0.0109, -0.0531, -0.2831, -0.0744, -0.0605, -0.4381,  0.0471]],
       dtype=torch.float64)
	q_value: tensor([[-6.8410]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18040056137610794, distance: 1.035994666067102 entropy 0.18424582481384277
epoch: 15, step: 121
	action: tensor([[ 0.3470, -0.1881, -0.1336, -0.0292,  0.0829,  0.1935,  0.1324]],
       dtype=torch.float64)
	q_value: tensor([[-7.2226]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4563050929311967, distance: 0.8437899059236273 entropy 0.18424582481384277
epoch: 15, step: 122
	action: tensor([[-0.1395,  0.0946,  0.1289, -0.0990, -0.2327,  0.1819,  0.1818]],
       dtype=torch.float64)
	q_value: tensor([[-7.4660]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21891577500132509, distance: 1.0113596592244485 entropy 0.18424582481384277
epoch: 15, step: 123
	action: tensor([[ 0.4208,  0.0382,  0.0574,  0.0563, -0.2229,  0.0461, -0.2050]],
       dtype=torch.float64)
	q_value: tensor([[-7.3538]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6740695840079654, distance: 0.6533097486258107 entropy 0.18424582481384277
epoch: 15, step: 124
	action: tensor([[ 0.3461,  0.1289,  0.0664, -0.4021, -0.1582, -0.4205,  0.4534]],
       dtype=torch.float64)
	q_value: tensor([[-7.8739]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39805690494872614, distance: 0.8878393954752928 entropy 0.18424582481384277
epoch: 15, step: 125
	action: tensor([[ 0.6335,  0.0424, -0.1767,  0.6105,  0.0810,  0.1896, -0.2517]],
       dtype=torch.float64)
	q_value: tensor([[-8.8643]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9316016147342379, distance: 0.2992813249250063 entropy 0.18424582481384277
epoch: 15, step: 126
	action: tensor([[ 0.3545,  0.0410, -0.1768, -0.2260,  0.1975,  0.4434, -0.7146]],
       dtype=torch.float64)
	q_value: tensor([[-8.8608]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6575138866020069, distance: 0.6696967276006945 entropy 0.18424582481384277
epoch: 15, step: 127
	action: tensor([[ 0.4563, -0.0293, -0.2472, -0.4684,  0.1220,  0.2367,  0.2049]],
       dtype=torch.float64)
	q_value: tensor([[-7.5646]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4830906983130743, distance: 0.8227423766603101 entropy 0.18424582481384277
LOSS epoch 15 actor 32.23181745365945 critic 29.794201269060714 
epoch: 16, step: 0
	action: tensor([[-0.2106, -0.3324, -0.2511, -0.4420,  0.0531, -0.2964, -0.3645]],
       dtype=torch.float64)
	q_value: tensor([[-6.7560]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3245189095572134, distance: 1.3169999992718613 entropy 0.07888543605804443
epoch: 16, step: 1
	action: tensor([[-0.2099,  0.1142, -0.5183, -0.2263, -0.0123, -0.1656, -0.3781]],
       dtype=torch.float64)
	q_value: tensor([[-5.6869]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13377128445708242, distance: 1.0650573199130637 entropy 0.07888543605804443
epoch: 16, step: 2
	action: tensor([[ 0.1439,  0.1973, -0.2027, -0.4549,  0.7589,  0.0480, -0.3512]],
       dtype=torch.float64)
	q_value: tensor([[-4.9899]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.43014543169183717, distance: 0.863850741500053 entropy 0.07888543605804443
epoch: 16, step: 3
	action: tensor([[ 0.1540,  0.1565, -0.6403,  0.0970,  0.4269,  0.2027, -0.4910]],
       dtype=torch.float64)
	q_value: tensor([[-6.0107]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5730685480562416, distance: 0.7477144409404387 entropy 0.07888543605804443
epoch: 16, step: 4
	action: tensor([[-0.0160,  0.2819, -0.4032,  0.5608,  0.0724,  0.0458, -0.2673]],
       dtype=torch.float64)
	q_value: tensor([[-5.8380]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5006484484158732, distance: 0.8086487044546137 entropy 0.07888543605804443
epoch: 16, step: 5
	action: tensor([[ 0.1408,  0.0527, -0.1038, -0.6428,  0.3029,  0.0435,  0.0826]],
       dtype=torch.float64)
	q_value: tensor([[-6.0791]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20607358048424995, distance: 1.0196398968788052 entropy 0.07888543605804443
epoch: 16, step: 6
	action: tensor([[-0.1650, -0.2157, -0.0273, -0.1227,  0.0319,  0.0293, -0.2540]],
       dtype=torch.float64)
	q_value: tensor([[-6.0282]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07487295115202763, distance: 1.1864112613469857 entropy 0.07888543605804443
epoch: 16, step: 7
	action: tensor([[ 0.4233, -0.4340, -0.4349, -0.4543,  0.0175, -0.0913, -0.1048]],
       dtype=torch.float64)
	q_value: tensor([[-5.3651]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.004468692077216119, distance: 1.1468982649590904 entropy 0.07888543605804443
epoch: 16, step: 8
	action: tensor([[ 0.3494, -0.2378,  0.2056, -0.3114, -0.3238, -0.3111,  0.0532]],
       dtype=torch.float64)
	q_value: tensor([[-6.5166]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12036476736577673, distance: 1.0732675563561338 entropy 0.07888543605804443
epoch: 16, step: 9
	action: tensor([[-0.0683,  0.0184,  0.0830, -0.1769, -0.5321,  0.4671, -0.3270]],
       dtype=torch.float64)
	q_value: tensor([[-7.2303]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23559858195732697, distance: 1.0005007886152046 entropy 0.07888543605804443
epoch: 16, step: 10
	action: tensor([[ 0.1854, -0.2126, -0.4196, -0.4173,  0.0990, -0.1265, -0.1170]],
       dtype=torch.float64)
	q_value: tensor([[-6.7947]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1554965430743921, distance: 1.051616542326864 entropy 0.07888543605804443
epoch: 16, step: 11
	action: tensor([[ 0.2730, -0.0563, -0.1728, -0.4383, -0.1444, -0.3785, -0.1500]],
       dtype=torch.float64)
	q_value: tensor([[-5.7224]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2569838633814099, distance: 0.9864062518207751 entropy 0.07888543605804443
epoch: 16, step: 12
	action: tensor([[ 0.0375,  0.2476, -0.0142, -0.2077,  0.4968,  0.2351, -0.6066]],
       dtype=torch.float64)
	q_value: tensor([[-6.3877]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5120731675197625, distance: 0.7993445977580708 entropy 0.07888543605804443
epoch: 16, step: 13
	action: tensor([[ 0.0764, -0.0740, -0.1140, -0.5227,  0.3047,  0.0217, -0.3130]],
       dtype=torch.float64)
	q_value: tensor([[-5.8040]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11107378260341405, distance: 1.0789207630475641 entropy 0.07888543605804443
epoch: 16, step: 14
	action: tensor([[-0.0518, -0.0058,  0.0501, -0.0094, -0.2477,  0.0423, -0.1957]],
       dtype=torch.float64)
	q_value: tensor([[-5.4634]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2724401210685251, distance: 0.9760926988605576 entropy 0.07888543605804443
epoch: 16, step: 15
	action: tensor([[-0.5302, -0.2187, -0.5343,  0.1960, -0.3865,  0.1636,  0.0535]],
       dtype=torch.float64)
	q_value: tensor([[-5.7137]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.46483428191215403, distance: 1.3850037233562216 entropy 0.07888543605804443
epoch: 16, step: 16
	action: tensor([[-0.0875,  0.2602, -0.2785, -0.1456,  0.0583,  0.1497,  0.0564]],
       dtype=torch.float64)
	q_value: tensor([[-5.7811]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3512565417725342, distance: 0.9217076435940031 entropy 0.07888543605804443
epoch: 16, step: 17
	action: tensor([[ 0.2526,  0.2498, -0.4599,  0.0639,  0.4038,  0.2423, -0.1113]],
       dtype=torch.float64)
	q_value: tensor([[-5.1781]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.700963656595861, distance: 0.6257756788808952 entropy 0.07888543605804443
epoch: 16, step: 18
	action: tensor([[ 0.1394,  0.1718, -0.3678, -0.1107,  0.3510,  0.4667, -0.2492]],
       dtype=torch.float64)
	q_value: tensor([[-5.8878]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6031023142920855, distance: 0.7209347781543565 entropy 0.07888543605804443
epoch: 16, step: 19
	action: tensor([[ 0.2395,  0.2156, -0.3615, -0.2029, -0.0630, -0.1547, -0.1873]],
       dtype=torch.float64)
	q_value: tensor([[-5.5149]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5985178084514919, distance: 0.7250865284447474 entropy 0.07888543605804443
epoch: 16, step: 20
	action: tensor([[ 0.0838, -0.0530,  0.0896, -0.6797, -0.0680, -0.2267,  0.0060]],
       dtype=torch.float64)
	q_value: tensor([[-5.6982]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04791738866653317, distance: 1.1714404514091394 entropy 0.07888543605804443
epoch: 16, step: 21
	action: tensor([[ 0.2541,  0.4373,  0.1656, -0.6413,  0.4383,  0.4080, -0.1137]],
       dtype=torch.float64)
	q_value: tensor([[-6.5003]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6445170220744093, distance: 0.6822854314838103 entropy 0.07888543605804443
epoch: 16, step: 22
	action: tensor([[ 0.1601,  0.3509, -0.4741,  0.0061,  0.2855,  0.1115, -0.1203]],
       dtype=torch.float64)
	q_value: tensor([[-6.8709]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6891819517666892, distance: 0.6379840302259614 entropy 0.07888543605804443
epoch: 16, step: 23
	action: tensor([[ 0.5259,  0.5730, -0.2633,  0.0775,  0.3821,  0.2387,  0.0678]],
       dtype=torch.float64)
	q_value: tensor([[-5.5306]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9336808223002182, distance: 0.29469736971170063 entropy 0.07888543605804443
epoch: 16, step: 24
	action: tensor([[ 0.3348,  0.2081,  0.0436, -0.4976,  0.0360, -0.1544, -0.3945]],
       dtype=torch.float64)
	q_value: tensor([[-7.0908]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4817277749187119, distance: 0.8238263160014546 entropy 0.07888543605804443
epoch: 16, step: 25
	action: tensor([[-0.2096, -0.1218, -0.0205, -0.2783, -0.0667, -0.0982, -0.4375]],
       dtype=torch.float64)
	q_value: tensor([[-6.5965]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14278603498184972, distance: 1.223317407189576 entropy 0.07888543605804443
epoch: 16, step: 26
	action: tensor([[-0.0548,  0.2145, -0.5032, -0.2026,  0.2207, -0.1087, -0.2587]],
       dtype=torch.float64)
	q_value: tensor([[-5.4404]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36511370028402323, distance: 0.9118106721140716 entropy 0.07888543605804443
epoch: 16, step: 27
	action: tensor([[ 0.2646, -0.0472, -0.3629, -0.5453,  0.0516, -0.1911, -0.3339]],
       dtype=torch.float64)
	q_value: tensor([[-5.0601]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28615285632982734, distance: 0.9668504558460387 entropy 0.07888543605804443
epoch: 16, step: 28
	action: tensor([[ 0.0482,  0.3836, -0.4876, -0.1589, -0.0292, -0.0076, -0.5631]],
       dtype=torch.float64)
	q_value: tensor([[-5.9010]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5770072081967592, distance: 0.7442574259679002 entropy 0.07888543605804443
epoch: 16, step: 29
	action: tensor([[ 0.2332,  0.3641, -0.5162, -0.5016,  0.6502, -0.0916, -0.5790]],
       dtype=torch.float64)
	q_value: tensor([[-5.4135]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6732828209021221, distance: 0.6540977852273228 entropy 0.07888543605804443
epoch: 16, step: 30
	action: tensor([[ 0.1025,  0.6317, -0.3698, -0.7007,  0.0599, -0.1616,  0.0264]],
       dtype=torch.float64)
	q_value: tensor([[-6.4458]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.682283205020447, distance: 0.6450253449921529 entropy 0.07888543605804443
epoch: 16, step: 31
	action: tensor([[ 0.3907, -0.3819,  0.1711, -0.1133, -0.0529, -0.6120, -0.1383]],
       dtype=torch.float64)
	q_value: tensor([[-6.6169]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.035088236993476074, distance: 1.1240884712575434 entropy 0.07888543605804443
epoch: 16, step: 32
	action: tensor([[ 0.0293,  0.7911, -0.0021, -0.2515,  0.3829,  0.1652,  0.0271]],
       dtype=torch.float64)
	q_value: tensor([[-7.8638]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6887262754193441, distance: 0.6384515187799253 entropy 0.07888543605804443
epoch: 16, step: 33
	action: tensor([[-0.2060,  0.0179, -0.4250, -0.1750,  0.5963, -0.0457,  0.2700]],
       dtype=torch.float64)
	q_value: tensor([[-6.4786]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04627585467176998, distance: 1.1175528801430918 entropy 0.07888543605804443
epoch: 16, step: 34
	action: tensor([[ 0.1256,  0.4300,  0.0816, -0.5159,  0.4712, -0.2047, -0.1194]],
       dtype=torch.float64)
	q_value: tensor([[-5.9499]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41898096744786895, distance: 0.872271881305105 entropy 0.07888543605804443
epoch: 16, step: 35
	action: tensor([[ 0.2008,  0.0649,  0.3972, -0.1376,  0.0836, -0.1741, -0.5563]],
       dtype=torch.float64)
	q_value: tensor([[-6.3167]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37310980838489216, distance: 0.9060505549594028 entropy 0.07888543605804443
epoch: 16, step: 36
	action: tensor([[ 0.1493,  0.1323, -0.5320,  0.2125, -0.1485, -0.0031, -0.2694]],
       dtype=torch.float64)
	q_value: tensor([[-7.0679]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5287399822848179, distance: 0.7855738003728682 entropy 0.07888543605804443
epoch: 16, step: 37
	action: tensor([[ 0.4077,  0.0908, -0.1298, -0.0639,  0.2829,  0.3914,  0.2122]],
       dtype=torch.float64)
	q_value: tensor([[-5.5421]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7468943062011044, distance: 0.5757151403019237 entropy 0.07888543605804443
epoch: 16, step: 38
	action: tensor([[-0.0796, -0.3368, -0.0175, -0.1544,  0.5204,  0.1766, -0.0340]],
       dtype=torch.float64)
	q_value: tensor([[-6.5359]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09005302200327181, distance: 1.19475953425205 entropy 0.07888543605804443
epoch: 16, step: 39
	action: tensor([[ 0.0231, -0.1901, -0.5348, -0.4077,  0.4075,  0.0944, -0.3438]],
       dtype=torch.float64)
	q_value: tensor([[-5.8507]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08771612194078604, distance: 1.0930038539837281 entropy 0.07888543605804443
epoch: 16, step: 40
	action: tensor([[ 0.0660,  0.4673, -0.5214, -0.2499, -0.1404, -0.0537, -0.2728]],
       dtype=torch.float64)
	q_value: tensor([[-5.3519]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6167457330257703, distance: 0.7084352992291966 entropy 0.07888543605804443
epoch: 16, step: 41
	action: tensor([[ 0.0602,  0.1184, -0.4821, -0.4174,  0.1630,  0.3234, -0.1385]],
       dtype=torch.float64)
	q_value: tensor([[-5.5035]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4392025726922508, distance: 0.8569583199589393 entropy 0.07888543605804443
epoch: 16, step: 42
	action: tensor([[-0.1747, -0.0732, -0.0038,  0.0069,  0.0671, -0.4946,  0.2943]],
       dtype=torch.float64)
	q_value: tensor([[-5.4624]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.027443231498202714, distance: 1.1599402293570813 entropy 0.07888543605804443
epoch: 16, step: 43
	action: tensor([[ 0.1451, -0.3390, -0.7655, -0.2621, -0.3120,  0.1053,  0.0572]],
       dtype=torch.float64)
	q_value: tensor([[-6.5996]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10709990733733132, distance: 1.0813296893462678 entropy 0.07888543605804443
epoch: 16, step: 44
	action: tensor([[ 0.2886, -0.0344, -0.1772, -0.1140, -0.0223, -0.1546, -0.0014]],
       dtype=torch.float64)
	q_value: tensor([[-6.0122]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44241393967174625, distance: 0.8545011414548641 entropy 0.07888543605804443
epoch: 16, step: 45
	action: tensor([[ 0.5182,  0.2205, -0.4238, -0.2894,  0.1935,  0.2742, -0.3808]],
       dtype=torch.float64)
	q_value: tensor([[-5.9866]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7779282246411202, distance: 0.5392664173187665 entropy 0.07888543605804443
epoch: 16, step: 46
	action: tensor([[-0.0215,  0.1914, -0.0445, -0.3086,  0.1371,  0.1101, -0.2402]],
       dtype=torch.float64)
	q_value: tensor([[-6.3301]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3402528680567716, distance: 0.9294915559412158 entropy 0.07888543605804443
epoch: 16, step: 47
	action: tensor([[ 0.3308, -0.2987,  0.2489, -0.0089,  0.1941, -0.2290, -0.1885]],
       dtype=torch.float64)
	q_value: tensor([[-5.2679]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23353671028300194, distance: 1.0018492391529243 entropy 0.07888543605804443
epoch: 16, step: 48
	action: tensor([[ 0.2663, -0.1466, -0.1419, -0.4842,  0.2537, -0.4377, -0.0209]],
       dtype=torch.float64)
	q_value: tensor([[-7.0149]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.057053516496912704, distance: 1.1112204261770715 entropy 0.07888543605804443
epoch: 16, step: 49
	action: tensor([[ 0.4937,  0.1458, -0.6461,  0.0620, -0.5643,  0.3021, -0.1334]],
       dtype=torch.float64)
	q_value: tensor([[-6.6290]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7397319385027064, distance: 0.5838040883167107 entropy 0.07888543605804443
epoch: 16, step: 50
	action: tensor([[ 0.2799,  0.1582, -0.2326, -0.1739, -0.0221, -0.2972, -0.3343]],
       dtype=torch.float64)
	q_value: tensor([[-7.0588]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.572606037701596, distance: 0.7481193444196731 entropy 0.07888543605804443
epoch: 16, step: 51
	action: tensor([[ 0.0786, -0.3454, -0.2695, -0.0700, -0.0643,  0.5396, -0.1021]],
       dtype=torch.float64)
	q_value: tensor([[-6.0689]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20512715894790234, distance: 1.020247460573601 entropy 0.07888543605804443
epoch: 16, step: 52
	action: tensor([[ 0.2621,  0.2230, -0.3504, -0.1303,  0.2902,  0.0034, -0.1509]],
       dtype=torch.float64)
	q_value: tensor([[-5.8395]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6490044890232392, distance: 0.6779653128081151 entropy 0.07888543605804443
epoch: 16, step: 53
	action: tensor([[-0.2086,  0.2880, -0.3261, -0.3493,  0.3142,  0.2843, -0.0572]],
       dtype=torch.float64)
	q_value: tensor([[-5.6362]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2973896473674299, distance: 0.9592105915745222 entropy 0.07888543605804443
epoch: 16, step: 54
	action: tensor([[ 0.0386,  0.2114, -0.7424, -0.4503,  0.2780, -0.1796, -0.1853]],
       dtype=torch.float64)
	q_value: tensor([[-5.2136]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.49593208116941445, distance: 0.8124585664766264 entropy 0.07888543605804443
epoch: 16, step: 55
	action: tensor([[ 0.4803, -0.1555, -0.1348, -0.4686,  0.1053,  0.0211, -0.2676]],
       dtype=torch.float64)
	q_value: tensor([[-5.6715]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29584929076828803, distance: 0.9602614709401595 entropy 0.07888543605804443
epoch: 16, step: 56
	action: tensor([[-0.3134, -0.1185,  0.2506, -0.1128, -0.1785,  0.0368, -0.1372]],
       dtype=torch.float64)
	q_value: tensor([[-6.3792]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18059614809438873, distance: 1.2433900114860006 entropy 0.07888543605804443
epoch: 16, step: 57
	action: tensor([[-0.0434, -0.2804, -0.1933,  0.2725,  0.0507,  0.0157, -0.6251]],
       dtype=torch.float64)
	q_value: tensor([[-5.9874]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16325902194621, distance: 1.04677227775951 entropy 0.07888543605804443
epoch: 16, step: 58
	action: tensor([[ 0.5026, -0.0715, -0.0460, -0.1067,  0.2180,  0.2467, -0.6337]],
       dtype=torch.float64)
	q_value: tensor([[-6.1059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6144124694614161, distance: 0.7105885147517075 entropy 0.07888543605804443
epoch: 16, step: 59
	action: tensor([[-0.1001,  0.3744, -0.0266, -0.5042,  0.3671,  0.0986, -0.0365]],
       dtype=torch.float64)
	q_value: tensor([[-6.6856]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3089109421523951, distance: 0.9513135916129328 entropy 0.07888543605804443
epoch: 16, step: 60
	action: tensor([[ 0.3971,  0.1254, -0.1743, -0.3756,  0.1528,  0.1217, -0.5160]],
       dtype=torch.float64)
	q_value: tensor([[-5.6154]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5723229655464821, distance: 0.7483670511260762 entropy 0.07888543605804443
epoch: 16, step: 61
	action: tensor([[ 0.4990,  0.2071, -0.1842,  0.1329, -0.0416, -0.1354, -0.6929]],
       dtype=torch.float64)
	q_value: tensor([[-6.0395]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8103158145358176, distance: 0.4983933687097426 entropy 0.07888543605804443
epoch: 16, step: 62
	action: tensor([[ 0.2473,  0.0631, -0.5557,  0.0637, -0.0164,  0.0951, -0.3103]],
       dtype=torch.float64)
	q_value: tensor([[-7.0538]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.601298316156285, distance: 0.7225713340531421 entropy 0.07888543605804443
epoch: 16, step: 63
	action: tensor([[ 0.5731,  0.1972, -0.2605, -0.5057,  0.4031, -0.0582, -0.6582]],
       dtype=torch.float64)
	q_value: tensor([[-5.4812]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6414827044463778, distance: 0.685191156597728 entropy 0.07888543605804443
epoch: 16, step: 64
	action: tensor([[ 0.1641, -0.0065, -0.1879,  0.2929,  0.6202, -0.1349,  0.0284]],
       dtype=torch.float64)
	q_value: tensor([[-6.9893]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5202583545855454, distance: 0.7926115624483799 entropy 0.07888543605804443
epoch: 16, step: 65
	action: tensor([[-0.0104,  0.0891, -0.2263,  0.2697, -0.0267,  0.0620, -0.0625]],
       dtype=torch.float64)
	q_value: tensor([[-6.8038]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.46264844243970715, distance: 0.8388531685374048 entropy 0.07888543605804443
epoch: 16, step: 66
	action: tensor([[ 0.1239, -0.2167,  0.1796,  0.1461, -0.0840, -0.1969,  0.0351]],
       dtype=torch.float64)
	q_value: tensor([[-5.5464]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28511466729866763, distance: 0.9675532735645496 entropy 0.07888543605804443
epoch: 16, step: 67
	action: tensor([[ 0.5082,  0.4329, -0.6336,  0.1855,  0.0585,  0.1954, -0.1919]],
       dtype=torch.float64)
	q_value: tensor([[-6.5231]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8747452799866808, distance: 0.4049988066283798 entropy 0.07888543605804443
epoch: 16, step: 68
	action: tensor([[ 0.2240,  0.4339, -0.5717,  0.2584,  0.3843, -0.1521, -0.2668]],
       dtype=torch.float64)
	q_value: tensor([[-6.6701]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7261570528419143, distance: 0.5988354106144005 entropy 0.07888543605804443
epoch: 16, step: 69
	action: tensor([[ 0.1002,  0.0969,  0.1519,  0.1423,  0.1798,  0.4224, -0.0039]],
       dtype=torch.float64)
	q_value: tensor([[-6.5591]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.651442248118263, distance: 0.6756068841319517 entropy 0.07888543605804443
epoch: 16, step: 70
	action: tensor([[ 0.4683,  0.0900,  0.1180, -0.3708, -0.4514,  0.0791,  0.0389]],
       dtype=torch.float64)
	q_value: tensor([[-6.2366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5440376878451039, distance: 0.7727182453839031 entropy 0.07888543605804443
epoch: 16, step: 71
	action: tensor([[-0.2182,  0.5581, -0.2774, -0.2721, -0.2260,  0.0766, -0.2975]],
       dtype=torch.float64)
	q_value: tensor([[-7.4817]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.393219275801617, distance: 0.8913999004512856 entropy 0.07888543605804443
epoch: 16, step: 72
	action: tensor([[-0.0019,  0.1695, -0.4941, -0.1692,  0.2465,  0.5350,  0.0044]],
       dtype=torch.float64)
	q_value: tensor([[-5.7397]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4987636145088026, distance: 0.8101734147841869 entropy 0.07888543605804443
epoch: 16, step: 73
	action: tensor([[ 0.1967,  0.2561, -0.2844, -0.4091,  0.6849, -0.0017,  0.0224]],
       dtype=torch.float64)
	q_value: tensor([[-5.6686]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5351041348117478, distance: 0.7802513613008106 entropy 0.07888543605804443
epoch: 16, step: 74
	action: tensor([[ 0.3045,  0.6393,  0.1002, -0.0241,  0.1896, -0.2236,  0.2730]],
       dtype=torch.float64)
	q_value: tensor([[-6.3016]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8096388712681178, distance: 0.49928190761829655 entropy 0.07888543605804443
epoch: 16, step: 75
	action: tensor([[ 0.0261,  0.1165, -0.9009, -0.1416,  0.2164, -0.4673, -0.2254]],
       dtype=torch.float64)
	q_value: tensor([[-7.2402]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42383972426765215, distance: 0.8686170487385753 entropy 0.07888543605804443
epoch: 16, step: 76
	action: tensor([[ 0.4396,  0.2531,  0.0728,  0.0865, -0.1183,  0.2820, -0.2010]],
       dtype=torch.float64)
	q_value: tensor([[-6.4355]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.845069879698991, distance: 0.4504272144095549 entropy 0.07888543605804443
epoch: 16, step: 77
	action: tensor([[ 0.0903,  0.0014, -0.1365,  0.0834,  0.2356, -0.5978, -0.0513]],
       dtype=torch.float64)
	q_value: tensor([[-6.7424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3072263664228727, distance: 0.952472331120686 entropy 0.07888543605804443
epoch: 16, step: 78
	action: tensor([[ 0.4450, -0.1733, -0.1799, -0.2166,  0.4657,  0.0162, -0.1865]],
       dtype=torch.float64)
	q_value: tensor([[-6.8129]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3836806128451852, distance: 0.8983790338247486 entropy 0.07888543605804443
epoch: 16, step: 79
	action: tensor([[0.2786, 0.2016, 0.0176, 0.2350, 0.1778, 0.0702, 0.3322]],
       dtype=torch.float64)
	q_value: tensor([[-6.4652]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7666172113822954, distance: 0.552829367519348 entropy 0.07888543605804443
epoch: 16, step: 80
	action: tensor([[ 0.3456, -0.0264,  0.2249,  0.4383,  0.4704, -0.2741, -0.3631]],
       dtype=torch.float64)
	q_value: tensor([[-6.8070]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7145550696638503, distance: 0.6113893373095614 entropy 0.07888543605804443
epoch: 16, step: 81
	action: tensor([[ 0.3744, -0.1151, -0.0585,  0.1466, -0.1601, -0.1347, -0.5214]],
       dtype=torch.float64)
	q_value: tensor([[-7.7269]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5476558938846166, distance: 0.7696462563604544 entropy 0.07888543605804443
epoch: 16, step: 82
	action: tensor([[ 0.8909, -0.1898, -0.2849, -0.4041,  0.3847, -0.1796, -0.0394]],
       dtype=torch.float64)
	q_value: tensor([[-6.6888]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.287219559899236, distance: 0.9661278022540468 entropy 0.07888543605804443
epoch: 16, step: 83
	action: tensor([[ 0.1261,  0.2641, -0.1674, -0.2391, -0.4314,  0.2426,  0.2829]],
       dtype=torch.float64)
	q_value: tensor([[-8.4382]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5505563977176189, distance: 0.7671747396848689 entropy 0.07888543605804443
epoch: 16, step: 84
	action: tensor([[ 0.0647,  0.1089, -0.6273, -0.1694,  0.4310,  0.2876,  0.3644]],
       dtype=torch.float64)
	q_value: tensor([[-6.6874]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4945329884284996, distance: 0.8135853166351048 entropy 0.07888543605804443
epoch: 16, step: 85
	action: tensor([[-0.1269,  0.5510, -0.1580, -0.3655,  0.0211, -0.4142,  0.1202]],
       dtype=torch.float64)
	q_value: tensor([[-6.3843]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.409128079957772, distance: 0.8796367575899432 entropy 0.07888543605804443
epoch: 16, step: 86
	action: tensor([[ 0.3139,  0.3143, -0.3623, -0.0027,  0.2263, -0.1207, -0.5483]],
       dtype=torch.float64)
	q_value: tensor([[-6.0630]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7329491364071641, distance: 0.5913623712394142 entropy 0.07888543605804443
epoch: 16, step: 87
	action: tensor([[ 0.2433, -0.0824, -0.0503,  0.2263,  0.3589, -0.1043, -0.8516]],
       dtype=torch.float64)
	q_value: tensor([[-6.1068]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5512887215600251, distance: 0.7665494672428911 entropy 0.07888543605804443
epoch: 16, step: 88
	action: tensor([[-0.0350,  0.2819, -0.1542, -0.2442,  0.0302,  0.3891, -0.1010]],
       dtype=torch.float64)
	q_value: tensor([[-7.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.47080555564536997, distance: 0.8324618337883627 entropy 0.07888543605804443
epoch: 16, step: 89
	action: tensor([[-0.0420, -0.1214, -0.4403, -0.4266,  0.3199, -0.1434, -0.3780]],
       dtype=torch.float64)
	q_value: tensor([[-5.5634]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07336644891080046, distance: 1.101566458626822 entropy 0.07888543605804443
epoch: 16, step: 90
	action: tensor([[-0.2524,  0.2820,  0.5521, -0.6964,  0.4039,  0.4057,  0.2408]],
       dtype=torch.float64)
	q_value: tensor([[-5.3300]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08871935969082756, distance: 1.1940284260371103 entropy 0.07888543605804443
epoch: 16, step: 91
	action: tensor([[-0.0520,  0.7228, -0.5989,  0.2011,  0.5449, -0.1732, -0.3408]],
       dtype=torch.float64)
	q_value: tensor([[-7.5934]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5607926509384182, distance: 0.7583880697144105 entropy 0.07888543605804443
epoch: 16, step: 92
	action: tensor([[ 0.2057, -0.5056, -0.4574,  0.4007,  0.3572,  0.3483, -0.5838]],
       dtype=torch.float64)
	q_value: tensor([[-6.6530]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.330733461244588, distance: 0.9361732970335703 entropy 0.07888543605804443
epoch: 16, step: 93
	action: tensor([[-0.0014, -0.1130, -0.4923, -0.1965,  0.2077,  0.1200,  0.3874]],
       dtype=torch.float64)
	q_value: tensor([[-6.9754]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20973800832914757, distance: 1.0172840625580368 entropy 0.07888543605804443
epoch: 16, step: 94
	action: tensor([[ 0.4133,  0.0838,  0.3655, -0.3365, -0.0477, -0.2372, -0.1992]],
       dtype=torch.float64)
	q_value: tensor([[-6.0329]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4368814409050874, distance: 0.8587299572038352 entropy 0.07888543605804443
epoch: 16, step: 95
	action: tensor([[ 1.0948e-04,  1.6436e-01, -1.4566e-01, -3.2361e-01,  1.3783e-01,
          3.7463e-01, -2.6313e-01]], dtype=torch.float64)
	q_value: tensor([[-7.4015]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4045680516192377, distance: 0.8830245131433868 entropy 0.07888543605804443
epoch: 16, step: 96
	action: tensor([[ 0.0478, -0.1482,  0.1470, -0.0135,  0.3084,  0.5923,  0.0861]],
       dtype=torch.float64)
	q_value: tensor([[-5.3127]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4373883770748366, distance: 0.8583433429777959 entropy 0.07888543605804443
epoch: 16, step: 97
	action: tensor([[-0.2848, -0.1849, -0.2456, -0.1622,  0.5214, -0.0921,  0.1687]],
       dtype=torch.float64)
	q_value: tensor([[-6.4838]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2500764249859715, distance: 1.2794548819377725 entropy 0.07888543605804443
epoch: 16, step: 98
	action: tensor([[ 0.0698, -0.2151, -0.4688, -0.0938, -0.3068,  0.0780,  0.0496]],
       dtype=torch.float64)
	q_value: tensor([[-5.8501]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13950997840045465, distance: 1.0615234988276308 entropy 0.07888543605804443
epoch: 16, step: 99
	action: tensor([[-0.0089,  0.2899, -0.1210,  0.1185,  0.0444, -0.0890, -0.3979]],
       dtype=torch.float64)
	q_value: tensor([[-5.6009]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.47111823903518546, distance: 0.8322158604486675 entropy 0.07888543605804443
epoch: 16, step: 100
	action: tensor([[ 0.2848,  0.2157, -0.3906,  0.4378,  0.1401,  0.2195, -0.1812]],
       dtype=torch.float64)
	q_value: tensor([[-5.6407]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7196105125235435, distance: 0.6059510699510096 entropy 0.07888543605804443
epoch: 16, step: 101
	action: tensor([[ 0.0379,  0.2637,  0.0470, -0.2986, -0.0843,  0.1583, -0.0064]],
       dtype=torch.float64)
	q_value: tensor([[-6.2354]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4182769081726123, distance: 0.8728002161280413 entropy 0.07888543605804443
epoch: 16, step: 102
	action: tensor([[ 0.1686,  0.0988, -0.2371, -0.3268,  0.6092, -0.3169, -0.1195]],
       dtype=torch.float64)
	q_value: tensor([[-5.9703]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31896297651038186, distance: 0.9443697219003492 entropy 0.07888543605804443
epoch: 16, step: 103
	action: tensor([[-0.3277, -0.2066, -0.3300, -0.6765,  0.6597, -0.4406, -0.2322]],
       dtype=torch.float64)
	q_value: tensor([[-6.2330]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4382468972496627, distance: 1.372376953125023 entropy 0.07888543605804443
epoch: 16, step: 104
	action: tensor([[ 0.0169,  0.0568, -0.5383, -0.0891, -0.6350, -0.1921, -0.1762]],
       dtype=torch.float64)
	q_value: tensor([[-6.2623]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35980078969175644, distance: 0.9156178703467991 entropy 0.07888543605804443
epoch: 16, step: 105
	action: tensor([[-0.0692,  0.4126,  0.0695,  0.2447,  0.2413,  0.1290,  0.0521]],
       dtype=torch.float64)
	q_value: tensor([[-6.3211]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5619156599797346, distance: 0.757417888123692 entropy 0.07888543605804443
epoch: 16, step: 106
	action: tensor([[-0.0363,  0.0517,  0.2674, -0.0304, -0.0030, -0.1862, -0.3310]],
       dtype=torch.float64)
	q_value: tensor([[-6.1594]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20677340415632883, distance: 1.0191904059460435 entropy 0.07888543605804443
epoch: 16, step: 107
	action: tensor([[ 0.3902, -0.0566, -0.2375, -0.1739,  0.2832,  0.3682, -0.1528]],
       dtype=torch.float64)
	q_value: tensor([[-6.3026]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5795569468173711, distance: 0.7420108980150805 entropy 0.07888543605804443
epoch: 16, step: 108
	action: tensor([[-0.2242,  0.0184, -0.2725, -0.3895,  0.0991,  0.0995, -0.1080]],
       dtype=torch.float64)
	q_value: tensor([[-5.9930]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.013095795233452279, distance: 1.136826511168605 entropy 0.07888543605804443
epoch: 16, step: 109
	action: tensor([[ 0.2127,  0.1071,  0.1801,  0.1836, -0.1412, -0.2144, -0.5636]],
       dtype=torch.float64)
	q_value: tensor([[-4.9433]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.607750989464722, distance: 0.716700358383589 entropy 0.07888543605804443
epoch: 16, step: 110
	action: tensor([[ 0.2378,  0.2983, -0.5389, -0.5871,  0.0322,  0.4568, -0.4997]],
       dtype=torch.float64)
	q_value: tensor([[-6.9699]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6783164286973165, distance: 0.6490395084848491 entropy 0.07888543605804443
epoch: 16, step: 111
	action: tensor([[ 0.2077,  0.1161, -0.2316, -0.4885,  0.4410, -0.3087, -0.0425]],
       dtype=torch.float64)
	q_value: tensor([[-6.2085]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3338103358829636, distance: 0.9340188443480818 entropy 0.07888543605804443
epoch: 16, step: 112
	action: tensor([[-0.1513,  0.2351, -0.4212, -0.1209,  0.1250, -0.2813, -0.3613]],
       dtype=torch.float64)
	q_value: tensor([[-6.2124]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29267700223643234, distance: 0.9624220902572826 entropy 0.07888543605804443
epoch: 16, step: 113
	action: tensor([[ 0.3638,  0.4396, -0.2959, -0.4311,  0.1694,  0.5023,  0.1590]],
       dtype=torch.float64)
	q_value: tensor([[-5.3242]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8208623174015187, distance: 0.48433981228399925 entropy 0.07888543605804443
epoch: 16, step: 114
	action: tensor([[-0.0109, -0.1626,  0.0304,  0.2109,  0.0015, -0.1172, -0.1428]],
       dtype=torch.float64)
	q_value: tensor([[-7.0171]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26389105777557254, distance: 0.981810653016384 entropy 0.07888543605804443
epoch: 16, step: 115
	action: tensor([[ 0.2608, -0.0599,  0.0126, -0.0543, -0.1861,  0.0234, -0.3343]],
       dtype=torch.float64)
	q_value: tensor([[-5.9532]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4648238016480646, distance: 0.8371534828034786 entropy 0.07888543605804443
epoch: 16, step: 116
	action: tensor([[ 0.3693, -0.2871,  0.1648, -0.0572,  0.2823, -0.3144,  0.1082]],
       dtype=torch.float64)
	q_value: tensor([[-6.1269]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20944518501771214, distance: 1.0174725170842727 entropy 0.07888543605804443
epoch: 16, step: 117
	action: tensor([[ 0.5288,  0.2269, -0.2893, -0.4789, -0.2412, -0.1464, -0.2921]],
       dtype=torch.float64)
	q_value: tensor([[-7.3481]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6289998366231078, distance: 0.6970175989405354 entropy 0.07888543605804443
epoch: 16, step: 118
	action: tensor([[ 0.4858,  0.3344,  0.2268,  0.1972, -0.1745,  0.0145, -0.0809]],
       dtype=torch.float64)
	q_value: tensor([[-6.8126]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8976192589449373, distance: 0.3661557239373158 entropy 0.07888543605804443
epoch: 16, step: 119
	action: tensor([[ 0.2184, -0.0821,  0.2284, -0.4127, -0.0663, -0.0864, -0.3223]],
       dtype=torch.float64)
	q_value: tensor([[-7.3171]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16587611365343846, distance: 1.0451339903010501 entropy 0.07888543605804443
epoch: 16, step: 120
	action: tensor([[-0.3077, -0.5293, -0.3660,  0.2178,  0.4097,  0.4524, -0.3318]],
       dtype=torch.float64)
	q_value: tensor([[-6.4774]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21211400832092497, distance: 1.2598778146557514 entropy 0.07888543605804443
epoch: 16, step: 121
	action: tensor([[ 0.3010,  0.0211, -0.0536, -0.2042,  0.5602, -0.0575, -0.5129]],
       dtype=torch.float64)
	q_value: tensor([[-6.3321]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44586345396286486, distance: 0.8518538488395501 entropy 0.07888543605804443
epoch: 16, step: 122
	action: tensor([[ 0.2671,  0.3228,  0.3450, -0.1139,  0.4681,  0.0909,  0.1332]],
       dtype=torch.float64)
	q_value: tensor([[-6.1484]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7000592781369883, distance: 0.6267212341777793 entropy 0.07888543605804443
epoch: 16, step: 123
	action: tensor([[-0.1953, -0.1878,  0.0172, -0.2985,  0.1867,  0.5528,  0.0078]],
       dtype=torch.float64)
	q_value: tensor([[-7.0267]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.031064386601562965, distance: 1.1619824974455024 entropy 0.07888543605804443
epoch: 16, step: 124
	action: tensor([[ 0.1835,  0.1337, -0.0480, -0.0098,  0.3079,  0.1541, -0.2520]],
       dtype=torch.float64)
	q_value: tensor([[-6.0254]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5750316784082321, distance: 0.7459933775363349 entropy 0.07888543605804443
epoch: 16, step: 125
	action: tensor([[-0.1163,  0.1046, -0.2949,  0.0068, -0.1569, -0.1255, -0.5457]],
       dtype=torch.float64)
	q_value: tensor([[-5.7113]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2668021339648947, distance: 0.9798673557708817 entropy 0.07888543605804443
epoch: 16, step: 126
	action: tensor([[-0.2858,  0.3163, -0.2671, -0.4243, -0.1883, -0.0996, -0.2327]],
       dtype=torch.float64)
	q_value: tensor([[-5.4715]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14381890282539544, distance: 1.0588623610264445 entropy 0.07888543605804443
epoch: 16, step: 127
	action: tensor([[ 0.9070,  0.2395, -0.6417,  0.1077,  0.2400, -0.0370, -0.4663]],
       dtype=torch.float64)
	q_value: tensor([[-5.4001]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9051210644572927, distance: 0.35248577492324323 entropy 0.07888543605804443
LOSS epoch 16 actor 22.23218485858508 critic 52.51084803850398 
epoch: 17, step: 0
	action: tensor([[ 0.0637,  0.2620,  0.1484, -0.2985,  0.1093,  0.2112,  0.5995]],
       dtype=torch.float64)
	q_value: tensor([[-6.8971]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.45261475295585074, distance: 0.8466486834938308 entropy 0.07888543605804443
epoch: 17, step: 1
	action: tensor([[ 0.3440,  0.0998,  0.0117, -0.5125, -0.0819, -0.0986, -0.1495]],
       dtype=torch.float64)
	q_value: tensor([[-5.9101]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3692664337756325, distance: 0.9088237444942074 entropy 0.07888543605804443
epoch: 17, step: 2
	action: tensor([[ 0.3064,  0.0954, -0.2471, -0.1662, -0.2988,  0.3220, -0.1912]],
       dtype=torch.float64)
	q_value: tensor([[-5.3285]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5830957161304477, distance: 0.7388816347935856 entropy 0.07888543605804443
epoch: 17, step: 3
	action: tensor([[ 0.5532, -0.0537,  0.0949, -0.2585,  0.1713,  0.1875, -0.3407]],
       dtype=torch.float64)
	q_value: tensor([[-5.0454]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.54929477025178, distance: 0.7682507483680346 entropy 0.07888543605804443
epoch: 17, step: 4
	action: tensor([[ 0.3719, -0.0017,  0.1166, -0.3192,  0.0569,  0.0231, -0.1270]],
       dtype=torch.float64)
	q_value: tensor([[-5.5433]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42960855353862204, distance: 0.8642575763592201 entropy 0.07888543605804443
epoch: 17, step: 5
	action: tensor([[-0.0475,  0.2988, -0.6844, -0.6349,  0.4052,  0.0103,  0.1038]],
       dtype=torch.float64)
	q_value: tensor([[-5.2266]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.47375026160405664, distance: 0.8301424831080664 entropy 0.07888543605804443
epoch: 17, step: 6
	action: tensor([[ 0.2025, -0.1619, -0.3889, -0.4160,  0.1797, -0.0325, -0.0900]],
       dtype=torch.float64)
	q_value: tensor([[-4.8400]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20577986724646835, distance: 1.0198284874300911 entropy 0.07888543605804443
epoch: 17, step: 7
	action: tensor([[-0.2666, -0.0889, -0.4084,  0.0493, -0.2138,  0.3472, -0.2100]],
       dtype=torch.float64)
	q_value: tensor([[-4.5622]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.007238383437587093, distance: 1.1484783876254472 entropy 0.07888543605804443
epoch: 17, step: 8
	action: tensor([[ 0.3960,  0.4513, -0.6157, -0.3724, -0.2554, -0.1459, -0.4397]],
       dtype=torch.float64)
	q_value: tensor([[-4.0837]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8462762859936914, distance: 0.44867009902550586 entropy 0.07888543605804443
epoch: 17, step: 9
	action: tensor([[ 0.4242,  0.1367, -0.4669, -0.6562,  0.0758, -0.1433, -0.3447]],
       dtype=torch.float64)
	q_value: tensor([[-5.4220]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5225915279093507, distance: 0.790681821591928 entropy 0.07888543605804443
epoch: 17, step: 10
	action: tensor([[ 0.3459,  0.0994, -0.2381, -0.2471,  0.1941,  0.0863, -0.4965]],
       dtype=torch.float64)
	q_value: tensor([[-5.1761]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6083404677879094, distance: 0.7161616213049057 entropy 0.07888543605804443
epoch: 17, step: 11
	action: tensor([[ 0.2635,  0.1789, -0.4271,  0.3395,  0.0019, -0.2072, -0.1509]],
       dtype=torch.float64)
	q_value: tensor([[-4.6985]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7029206875803327, distance: 0.623724636103785 entropy 0.07888543605804443
epoch: 17, step: 12
	action: tensor([[ 0.5532, -0.1132,  0.1247,  0.0734, -0.0106, -0.0490, -0.3262]],
       dtype=torch.float64)
	q_value: tensor([[-5.1874]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6373859690209651, distance: 0.6890948363654477 entropy 0.07888543605804443
epoch: 17, step: 13
	action: tensor([[ 0.0371, -0.2717, -0.1925, -0.3102,  0.0234,  0.0945,  0.2449]],
       dtype=torch.float64)
	q_value: tensor([[-5.9414]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.012683661641225585, distance: 1.1370638571478313 entropy 0.07888543605804443
epoch: 17, step: 14
	action: tensor([[-0.1689,  0.1356, -0.2090, -0.0739,  0.3171, -0.0638, -0.0235]],
       dtype=torch.float64)
	q_value: tensor([[-4.7583]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20035905030321433, distance: 1.0233029035544516 entropy 0.07888543605804443
epoch: 17, step: 15
	action: tensor([[ 4.3259e-01, -4.5800e-04, -2.3547e-01, -5.7885e-01,  1.5456e-01,
         -1.2631e-01, -4.9287e-01]], dtype=torch.float64)
	q_value: tensor([[-4.2436]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3340575297472761, distance: 0.933845541487349 entropy 0.07888543605804443
epoch: 17, step: 16
	action: tensor([[ 0.4863,  0.4766,  0.2551, -0.4540,  0.4994,  0.1942, -0.1575]],
       dtype=torch.float64)
	q_value: tensor([[-5.1729]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.804982220538837, distance: 0.5053517765886348 entropy 0.07888543605804443
epoch: 17, step: 17
	action: tensor([[ 0.5230,  0.0920, -0.4329, -0.6375, -0.0526,  0.0568, -0.3571]],
       dtype=torch.float64)
	q_value: tensor([[-6.0242]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5063384657595398, distance: 0.8040283043795287 entropy 0.07888543605804443
epoch: 17, step: 18
	action: tensor([[ 0.0415,  0.2380, -0.3318, -0.3840,  0.3045,  0.2490, -0.4857]],
       dtype=torch.float64)
	q_value: tensor([[-5.3613]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.48399193478418134, distance: 0.8220248340347458 entropy 0.07888543605804443
epoch: 17, step: 19
	action: tensor([[ 0.3365,  0.1836, -0.0868, -0.4623,  0.5449,  0.2636,  0.1425]],
       dtype=torch.float64)
	q_value: tensor([[-4.1392]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5950637891714912, distance: 0.7281988698748428 entropy 0.07888543605804443
epoch: 17, step: 20
	action: tensor([[ 0.2099, -0.4047,  0.1250, -0.8107,  0.3379,  0.0480, -0.4643]],
       dtype=torch.float64)
	q_value: tensor([[-5.4525]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3595649723586276, distance: 1.3343098030156726 entropy 0.07888543605804443
epoch: 17, step: 21
	action: tensor([[ 0.3347, -0.1323, -0.0560, -0.3107,  0.6092,  0.0959, -0.2074]],
       dtype=torch.float64)
	q_value: tensor([[-5.3690]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3276531452476168, distance: 0.9383252043406767 entropy 0.07888543605804443
epoch: 17, step: 22
	action: tensor([[ 0.7803, -0.0305, -0.4337, -0.1196,  0.2752, -0.1190, -0.2726]],
       dtype=torch.float64)
	q_value: tensor([[-5.0959]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6254591893710273, distance: 0.7003357016292265 entropy 0.07888543605804443
epoch: 17, step: 23
	action: tensor([[ 0.0925,  0.0401, -0.0201, -0.3448,  0.3941,  0.4027, -0.2570]],
       dtype=torch.float64)
	q_value: tensor([[-6.2401]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4095534388502564, distance: 0.8793200826332537 entropy 0.07888543605804443
epoch: 17, step: 24
	action: tensor([[ 0.4872,  0.3493, -0.3450,  0.0923,  0.5236,  0.0430, -0.1066]],
       dtype=torch.float64)
	q_value: tensor([[-4.4755]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8800604211677849, distance: 0.39631266651977193 entropy 0.07888543605804443
epoch: 17, step: 25
	action: tensor([[ 0.4497, -0.1036, -0.1092, -0.3061,  0.0099, -0.0822, -0.1785]],
       dtype=torch.float64)
	q_value: tensor([[-5.6000]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38795858359237323, distance: 0.895255708737994 entropy 0.07888543605804443
epoch: 17, step: 26
	action: tensor([[-0.0926, -0.2883, -0.2188, -0.5843, -0.1802, -0.0128,  0.0389]],
       dtype=torch.float64)
	q_value: tensor([[-5.2133]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20928314437690143, distance: 1.258405747059632 entropy 0.07888543605804443
epoch: 17, step: 27
	action: tensor([[ 0.0660,  0.0129, -0.5240, -0.1474,  0.4872,  0.3013, -0.1437]],
       dtype=torch.float64)
	q_value: tensor([[-4.8115]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42530945995013847, distance: 0.8675084573334346 entropy 0.07888543605804443
epoch: 17, step: 28
	action: tensor([[ 0.4052,  0.2184, -0.2070,  0.1991, -0.1069, -0.1285, -0.1393]],
       dtype=torch.float64)
	q_value: tensor([[-4.4718]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.788848061176693, distance: 0.5258407395037143 entropy 0.07888543605804443
epoch: 17, step: 29
	action: tensor([[-0.2690,  0.1469, -0.0463, -0.6027,  0.3976,  0.3092,  0.0139]],
       dtype=torch.float64)
	q_value: tensor([[-5.3172]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.003674773320496705, distance: 1.1464449287608538 entropy 0.07888543605804443
epoch: 17, step: 30
	action: tensor([[-0.0222, -0.1794,  0.1106,  0.2865,  0.3226, -0.3350, -0.1677]],
       dtype=torch.float64)
	q_value: tensor([[-4.5349]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20021733204722092, distance: 1.0233935781741417 entropy 0.07888543605804443
epoch: 17, step: 31
	action: tensor([[ 0.1271,  0.2104, -0.6173,  0.3334,  0.3157,  0.3700,  0.0602]],
       dtype=torch.float64)
	q_value: tensor([[-5.5066]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5917809766220672, distance: 0.7311446608971318 entropy 0.07888543605804443
epoch: 17, step: 32
	action: tensor([[ 0.0570,  0.0721, -0.0425, -0.8173,  0.2755,  0.0170, -0.4393]],
       dtype=torch.float64)
	q_value: tensor([[-5.0388]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08373622615737064, distance: 1.095385408411307 entropy 0.07888543605804443
epoch: 17, step: 33
	action: tensor([[ 0.7548, -0.0158, -0.4270, -0.4640,  0.1785,  0.0379, -0.4191]],
       dtype=torch.float64)
	q_value: tensor([[-4.7825]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5028500758153116, distance: 0.8068640801276361 entropy 0.07888543605804443
epoch: 17, step: 34
	action: tensor([[ 0.0422,  0.3686,  0.0637, -0.5849,  0.1994,  0.1148, -0.3450]],
       dtype=torch.float64)
	q_value: tensor([[-5.9125]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3966264450035697, distance: 0.88889370200296 entropy 0.07888543605804443
epoch: 17, step: 35
	action: tensor([[ 0.1443,  0.8109, -0.0571, -0.2485,  0.4945,  0.3096, -0.4802]],
       dtype=torch.float64)
	q_value: tensor([[-4.8062]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7809534816342819, distance: 0.5355806408934919 entropy 0.07888543605804443
epoch: 17, step: 36
	action: tensor([[-0.1070, -0.0931, -0.4425, -0.1478,  0.1817,  0.1247, -0.3934]],
       dtype=torch.float64)
	q_value: tensor([[-5.5139]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13946066514691546, distance: 1.0615539154659186 entropy 0.07888543605804443
epoch: 17, step: 37
	action: tensor([[ 0.0915,  0.0352, -0.6779, -0.1148,  0.2942,  0.2696, -0.1049]],
       dtype=torch.float64)
	q_value: tensor([[-3.9474]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4601197977767658, distance: 0.8408245706813364 entropy 0.07888543605804443
epoch: 17, step: 38
	action: tensor([[ 0.3261,  0.1570, -0.0578, -0.0174,  0.1584,  0.4205, -0.2260]],
       dtype=torch.float64)
	q_value: tensor([[-4.3987]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7531687851615951, distance: 0.5685343819148521 entropy 0.07888543605804443
epoch: 17, step: 39
	action: tensor([[-0.0277,  0.5195, -0.0944, -0.1293,  0.3365,  0.1676, -0.5978]],
       dtype=torch.float64)
	q_value: tensor([[-4.9203]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5935201913728977, distance: 0.7295854796376795 entropy 0.07888543605804443
epoch: 17, step: 40
	action: tensor([[ 0.7089,  0.5946,  0.2672, -0.4000, -0.0273,  0.2114,  0.1259]],
       dtype=torch.float64)
	q_value: tensor([[-4.7603]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9386943178383926, distance: 0.2833394574157116 entropy 0.07888543605804443
epoch: 17, step: 41
	action: tensor([[ 0.1622, -0.2378, -0.3208, -0.5085, -0.3392,  0.0970,  0.2020]],
       dtype=torch.float64)
	q_value: tensor([[-7.0805]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07030268915912397, distance: 1.1033860290021447 entropy 0.07888543605804443
epoch: 17, step: 42
	action: tensor([[ 0.3264,  0.0808, -0.1510, -0.1681, -0.2140, -0.2209,  0.0191]],
       dtype=torch.float64)
	q_value: tensor([[-5.3969]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.532549235679466, distance: 0.7823924130253896 entropy 0.07888543605804443
epoch: 17, step: 43
	action: tensor([[ 0.0343,  0.1416,  0.1438, -0.3336,  0.2322,  0.1389, -0.2541]],
       dtype=torch.float64)
	q_value: tensor([[-5.1833]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3246854884469952, distance: 0.9403937506318953 entropy 0.07888543605804443
epoch: 17, step: 44
	action: tensor([[ 0.4009,  0.2393, -0.3127, -0.0331,  0.3184,  0.4167, -0.3087]],
       dtype=torch.float64)
	q_value: tensor([[-4.6334]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8214525455705204, distance: 0.4835412453403401 entropy 0.07888543605804443
epoch: 17, step: 45
	action: tensor([[ 0.2919,  0.4502,  0.0310, -0.4248,  0.1894, -0.5241, -0.7057]],
       dtype=torch.float64)
	q_value: tensor([[-4.9576]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5944217511014439, distance: 0.7287759314096739 entropy 0.07888543605804443
epoch: 17, step: 46
	action: tensor([[ 0.3336, -0.2646,  0.3271, -0.3109, -0.0638, -0.0066, -0.1196]],
       dtype=torch.float64)
	q_value: tensor([[-6.2974]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13954147883735335, distance: 1.0615040687591915 entropy 0.07888543605804443
epoch: 17, step: 47
	action: tensor([[-0.3736,  0.0821, -0.1528,  0.4088,  0.0369,  0.1466, -0.2986]],
       dtype=torch.float64)
	q_value: tensor([[-5.6466]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1449411356280348, distance: 1.058168185623675 entropy 0.07888543605804443
epoch: 17, step: 48
	action: tensor([[ 0.0412,  0.0774, -0.1847, -0.7275,  0.1423,  0.3239, -0.0195]],
       dtype=torch.float64)
	q_value: tensor([[-4.5720]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21578432705001005, distance: 1.013384954416911 entropy 0.07888543605804443
epoch: 17, step: 49
	action: tensor([[ 0.2896,  0.8620, -0.9928, -0.0149, -0.0131, -0.1355,  0.0347]],
       dtype=torch.float64)
	q_value: tensor([[-4.9328]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8425976041759274, distance: 0.4540068050682755 entropy 0.07888543605804443
epoch: 17, step: 50
	action: tensor([[ 0.1312,  0.4196,  0.0439, -0.8238,  0.1915, -0.3876, -0.1250]],
       dtype=torch.float64)
	q_value: tensor([[-6.2115]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3125324416495491, distance: 0.9488177432504674 entropy 0.07888543605804443
epoch: 17, step: 51
	action: tensor([[-0.3057,  0.3872, -0.1828, -0.1977, -0.1003,  0.4238,  0.2518]],
       dtype=torch.float64)
	q_value: tensor([[-5.6938]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2391935275827468, distance: 0.9981453607893611 entropy 0.07888543605804443
epoch: 17, step: 52
	action: tensor([[ 0.3306,  0.4758, -0.2295, -0.3791,  0.0735,  0.2109,  0.0746]],
       dtype=torch.float64)
	q_value: tensor([[-5.0106]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7735147805033702, distance: 0.5445987319679577 entropy 0.07888543605804443
epoch: 17, step: 53
	action: tensor([[ 0.1441,  0.0447, -0.2016, -0.2213, -0.0827, -0.0502,  0.4079]],
       dtype=torch.float64)
	q_value: tensor([[-5.2759]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3689987213292385, distance: 0.9090165974111779 entropy 0.07888543605804443
epoch: 17, step: 54
	action: tensor([[ 0.2845, -0.4672, -0.3494, -0.3306,  0.1586,  0.3435, -0.3451]],
       dtype=torch.float64)
	q_value: tensor([[-5.0967]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.045539814945809276, distance: 1.1179840345166623 entropy 0.07888543605804443
epoch: 17, step: 55
	action: tensor([[-0.3160,  0.1861,  0.0580,  0.1656, -0.2355,  0.2596, -0.3297]],
       dtype=torch.float64)
	q_value: tensor([[-4.8648]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20488929951884227, distance: 1.0204000996587317 entropy 0.07888543605804443
epoch: 17, step: 56
	action: tensor([[ 0.4985, -0.2052, -0.1907, -0.3001,  0.3178,  0.1466, -0.4788]],
       dtype=torch.float64)
	q_value: tensor([[-4.7669]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3646421683685841, distance: 0.9121492114963868 entropy 0.07888543605804443
epoch: 17, step: 57
	action: tensor([[-0.0741,  0.0163, -0.0719, -0.6009,  0.0268,  0.3882, -0.1741]],
       dtype=torch.float64)
	q_value: tensor([[-5.1985]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10443736227091494, distance: 1.0829407015710935 entropy 0.07888543605804443
epoch: 17, step: 58
	action: tensor([[ 0.3143,  0.1487, -0.3396, -0.1332, -0.0833, -0.1451, -0.2362]],
       dtype=torch.float64)
	q_value: tensor([[-4.6847]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6133216223204738, distance: 0.711592950751588 entropy 0.07888543605804443
epoch: 17, step: 59
	action: tensor([[ 0.3598,  0.7233,  0.3748, -0.0085,  0.0697,  0.0034, -0.1746]],
       dtype=torch.float64)
	q_value: tensor([[-4.8253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8635351816026131, distance: 0.422733868630167 entropy 0.07888543605804443
epoch: 17, step: 60
	action: tensor([[ 0.5758,  0.1551, -0.1817, -0.2665,  0.1051, -0.2559, -0.2651]],
       dtype=torch.float64)
	q_value: tensor([[-6.4587]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6365767153319078, distance: 0.6898633418646402 entropy 0.07888543605804443
epoch: 17, step: 61
	action: tensor([[ 0.2805,  0.2033, -0.1039, -0.2923, -0.0927, -0.2299,  0.1374]],
       dtype=torch.float64)
	q_value: tensor([[-5.7138]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5393877099341291, distance: 0.7766484039520756 entropy 0.07888543605804443
epoch: 17, step: 62
	action: tensor([[ 0.1827, -0.0826, -0.2722, -0.1405,  0.2485,  0.1460, -0.2214]],
       dtype=torch.float64)
	q_value: tensor([[-5.1503]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4047745244927736, distance: 0.8828714004179863 entropy 0.07888543605804443
epoch: 17, step: 63
	action: tensor([[ 0.2077, -0.1008, -0.2700, -0.7832,  0.3088, -0.0054, -0.4383]],
       dtype=torch.float64)
	q_value: tensor([[-4.3435]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11890513222882848, distance: 1.0741576580914252 entropy 0.07888543605804443
epoch: 17, step: 64
	action: tensor([[ 0.1761,  0.2993,  0.0318, -0.1957,  0.1471, -0.0484, -0.7236]],
       dtype=torch.float64)
	q_value: tensor([[-4.7435]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5839802490837398, distance: 0.7380973874131288 entropy 0.07888543605804443
epoch: 17, step: 65
	action: tensor([[ 0.0286,  0.6195, -0.8704,  0.0922, -0.1443, -0.1536, -0.6269]],
       dtype=torch.float64)
	q_value: tensor([[-5.1917]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.639599333780717, distance: 0.6869885295529083 entropy 0.07888543605804443
epoch: 17, step: 66
	action: tensor([[ 0.0642,  0.2601, -0.1150, -0.0584,  0.3745,  0.5192, -0.2959]],
       dtype=torch.float64)
	q_value: tensor([[-5.2425]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6415302067995818, distance: 0.685145762315583 entropy 0.07888543605804443
epoch: 17, step: 67
	action: tensor([[-0.0024, -0.2465, -0.1522, -0.1367, -0.0701, -0.0693, -0.2758]],
       dtype=torch.float64)
	q_value: tensor([[-4.5932]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04685963744228827, distance: 1.1172107959112456 entropy 0.07888543605804443
epoch: 17, step: 68
	action: tensor([[-0.5343, -0.1493, -0.1625, -0.5093,  0.2526,  0.2168, -0.8196]],
       dtype=torch.float64)
	q_value: tensor([[-4.4574]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.49702847230573854, distance: 1.400140839823064 entropy 0.07888543605804443
epoch: 17, step: 69
	action: tensor([[-0.0077,  0.3006, -0.2219, -0.1719,  0.0130, -0.2823, -0.0809]],
       dtype=torch.float64)
	q_value: tensor([[-4.6124]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.40576827025616724, distance: 0.882134103170183 entropy 0.07888543605804443
epoch: 17, step: 70
	action: tensor([[ 0.1765,  0.2825, -0.4896,  0.1314,  0.2596,  0.2248,  0.2009]],
       dtype=torch.float64)
	q_value: tensor([[-4.4881]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6537484910880997, distance: 0.6733680879684253 entropy 0.07888543605804443
epoch: 17, step: 71
	action: tensor([[ 0.1188,  0.2021, -0.3338, -0.4043, -0.1325,  0.2275, -0.2454]],
       dtype=torch.float64)
	q_value: tensor([[-4.8834]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4857037888771364, distance: 0.8206601697057733 entropy 0.07888543605804443
epoch: 17, step: 72
	action: tensor([[ 0.4690,  0.3294, -0.0011, -0.6031, -0.1445, -0.1903, -0.0140]],
       dtype=torch.float64)
	q_value: tensor([[-4.5280]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6033224790333016, distance: 0.720734794074885 entropy 0.07888543605804443
epoch: 17, step: 73
	action: tensor([[ 0.3713,  0.1545, -0.0272, -0.3366,  0.1475, -0.1391, -0.2890]],
       dtype=torch.float64)
	q_value: tensor([[-5.9663]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5462996716843669, distance: 0.7707991730279334 entropy 0.07888543605804443
epoch: 17, step: 74
	action: tensor([[ 0.0860,  0.0156, -0.4914, -0.1989,  0.3853,  0.6108, -0.1708]],
       dtype=torch.float64)
	q_value: tensor([[-5.1343]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5227234439038476, distance: 0.7905725746984101 entropy 0.07888543605804443
epoch: 17, step: 75
	action: tensor([[-0.5163,  0.3694, -0.2749, -0.2448,  0.4905,  0.6120, -0.2266]],
       dtype=torch.float64)
	q_value: tensor([[-4.6743]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12908911466961948, distance: 1.0679318832865898 entropy 0.07888543605804443
epoch: 17, step: 76
	action: tensor([[ 0.5065,  0.0771, -0.2100, -0.5699,  0.1498,  0.3905, -0.4176]],
       dtype=torch.float64)
	q_value: tensor([[-4.6662]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5679034028249603, distance: 0.752223880008689 entropy 0.07888543605804443
epoch: 17, step: 77
	action: tensor([[-0.0382, -0.3873, -0.6145, -0.5804,  0.0606, -0.1119, -0.4490]],
       dtype=torch.float64)
	q_value: tensor([[-5.3512]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0904548300800907, distance: 1.1949797161100333 entropy 0.07888543605804443
epoch: 17, step: 78
	action: tensor([[ 0.3768, -0.0039, -0.7163, -0.2045,  0.0748,  0.5661, -0.3254]],
       dtype=torch.float64)
	q_value: tensor([[-4.5264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6121036361558247, distance: 0.7127127818381795 entropy 0.07888543605804443
epoch: 17, step: 79
	action: tensor([[ 0.2318,  0.2692, -0.3583, -0.2440, -0.1340,  0.1598, -0.2194]],
       dtype=torch.float64)
	q_value: tensor([[-5.0291]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6457274892069405, distance: 0.6811228042946317 entropy 0.07888543605804443
epoch: 17, step: 80
	action: tensor([[ 0.4930,  0.2908, -0.4160, -0.1241,  0.0035, -0.0550,  0.1566]],
       dtype=torch.float64)
	q_value: tensor([[-4.6146]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8111812336132878, distance: 0.49725512880155154 entropy 0.07888543605804443
epoch: 17, step: 81
	action: tensor([[ 0.2610,  0.1904,  0.1235, -0.2409,  0.0473,  0.2634, -0.2234]],
       dtype=torch.float64)
	q_value: tensor([[-5.4021]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6270885029752606, distance: 0.6988107541246512 entropy 0.07888543605804443
epoch: 17, step: 82
	action: tensor([[ 0.0761,  0.0030, -0.2938, -0.1590,  0.2705,  0.2140, -0.0805]],
       dtype=torch.float64)
	q_value: tensor([[-5.1130]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3994800064488979, distance: 0.8867892686055517 entropy 0.07888543605804443
epoch: 17, step: 83
	action: tensor([[ 0.0371,  0.9337, -0.2138, -0.5056,  0.2261,  0.2021, -0.1441]],
       dtype=torch.float64)
	q_value: tensor([[-4.1677]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7397272012763625, distance: 0.583809401299615 entropy 0.07888543605804443
epoch: 17, step: 84
	action: tensor([[ 0.6378,  0.3775, -0.0575, -0.3374, -0.3733, -0.0022, -0.2376]],
       dtype=torch.float64)
	q_value: tensor([[-5.6409]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8375611005726276, distance: 0.46121318307577647 entropy 0.07888543605804443
epoch: 17, step: 85
	action: tensor([[-0.0237, -0.1071, -0.7433, -0.8261, -0.0151, -0.5973, -0.1802]],
       dtype=torch.float64)
	q_value: tensor([[-6.2217]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26102681947694417, distance: 0.9837189367477986 entropy 0.07888543605804443
epoch: 17, step: 86
	action: tensor([[ 0.3358,  0.5227, -0.4851, -0.1782, -0.2147,  0.0247, -0.0652]],
       dtype=torch.float64)
	q_value: tensor([[-5.3665]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8312981532144834, distance: 0.4700202960394927 entropy 0.07888543605804443
epoch: 17, step: 87
	action: tensor([[-0.5373,  0.1518, -0.5609, -0.5307,  0.6006,  0.1303, -0.4369]],
       dtype=torch.float64)
	q_value: tensor([[-5.1805]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1319365022686303, distance: 1.217496512411281 entropy 0.07888543605804443
epoch: 17, step: 88
	action: tensor([[ 0.3663,  0.1177, -0.8859, -0.6168,  0.2721,  0.2671, -0.0994]],
       dtype=torch.float64)
	q_value: tensor([[-4.3801]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6530599861540067, distance: 0.6740372359687221 entropy 0.07888543605804443
epoch: 17, step: 89
	action: tensor([[ 0.3199,  0.2081, -0.1652, -0.2064, -0.0789,  0.2081, -0.0813]],
       dtype=torch.float64)
	q_value: tensor([[-5.4960]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6784616824611001, distance: 0.6488929575189244 entropy 0.07888543605804443
epoch: 17, step: 90
	action: tensor([[-0.2317,  0.6249, -0.5141, -0.1942,  0.2407, -0.2878, -0.3615]],
       dtype=torch.float64)
	q_value: tensor([[-4.8362]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44537816388276996, distance: 0.8522267765558599 entropy 0.07888543605804443
epoch: 17, step: 91
	action: tensor([[ 0.3741,  0.1191, -0.1304, -0.0031,  0.4574,  0.3601,  0.2639]],
       dtype=torch.float64)
	q_value: tensor([[-4.6162]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7677277488442438, distance: 0.5515124970352883 entropy 0.07888543605804443
epoch: 17, step: 92
	action: tensor([[ 0.4294,  0.0834,  0.1887, -0.1429,  0.0108,  0.5474, -0.5153]],
       dtype=torch.float64)
	q_value: tensor([[-5.5317]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7429270165381767, distance: 0.5802096025537148 entropy 0.07888543605804443
epoch: 17, step: 93
	action: tensor([[-0.0883,  0.5373, -0.0152, -0.0634,  0.6261, -0.2273,  0.3314]],
       dtype=torch.float64)
	q_value: tensor([[-5.9080]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4428870733320266, distance: 0.8541385256340769 entropy 0.07888543605804443
epoch: 17, step: 94
	action: tensor([[ 0.3474,  0.0117, -0.0609, -0.0581,  0.1312,  0.1741, -0.5402]],
       dtype=torch.float64)
	q_value: tensor([[-5.6108]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5903732882351774, distance: 0.7324042029945677 entropy 0.07888543605804443
epoch: 17, step: 95
	action: tensor([[-0.2899,  0.1318, -0.4235, -0.3963,  0.4034,  0.5240, -0.3135]],
       dtype=torch.float64)
	q_value: tensor([[-5.0042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1813517567704036, distance: 1.0353933238818762 entropy 0.07888543605804443
epoch: 17, step: 96
	action: tensor([[ 0.0600,  0.3676, -0.2638,  0.0074,  0.0631, -0.0392, -0.1368]],
       dtype=torch.float64)
	q_value: tensor([[-4.3150]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5919664351187599, distance: 0.7309785584053107 entropy 0.07888543605804443
epoch: 17, step: 97
	action: tensor([[ 0.2524,  0.5203, -0.1940, -0.5980,  0.1254,  0.1540,  0.1657]],
       dtype=torch.float64)
	q_value: tensor([[-4.3611]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7039672777188253, distance: 0.6226249971027946 entropy 0.07888543605804443
epoch: 17, step: 98
	action: tensor([[ 0.3842, -0.0126, -0.4319, -0.5022, -0.3280, -0.2023,  0.0239]],
       dtype=torch.float64)
	q_value: tensor([[-5.5539]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39917453978071304, distance: 0.8870147816002888 entropy 0.07888543605804443
epoch: 17, step: 99
	action: tensor([[-0.1244,  0.4158, -0.2615, -0.6806,  0.1289,  0.0518, -0.2414]],
       dtype=torch.float64)
	q_value: tensor([[-5.5138]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3425875120186985, distance: 0.9278455048118247 entropy 0.07888543605804443
epoch: 17, step: 100
	action: tensor([[ 0.4961,  0.0011, -0.2078, -0.2470,  0.2286,  0.1395, -0.1079]],
       dtype=torch.float64)
	q_value: tensor([[-4.5316]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5889146433223147, distance: 0.7337070578588681 entropy 0.07888543605804443
epoch: 17, step: 101
	action: tensor([[ 5.5894e-01, -2.2168e-01, -1.4670e-01, -1.4801e-01,  5.1965e-01,
         -3.7602e-04, -3.7423e-02]], dtype=torch.float64)
	q_value: tensor([[-5.1036]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41802067851005176, distance: 0.8729924146905768 entropy 0.07888543605804443
epoch: 17, step: 102
	action: tensor([[ 0.7027, -0.0113,  0.0085,  0.3510, -0.2854,  0.1835,  0.1906]],
       dtype=torch.float64)
	q_value: tensor([[-5.8394]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8940719363197873, distance: 0.3724450529614417 entropy 0.07888543605804443
epoch: 17, step: 103
	action: tensor([[ 0.0016,  0.2960, -0.1991, -0.4817,  0.6794, -0.0775, -0.1811]],
       dtype=torch.float64)
	q_value: tensor([[-6.4275]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35623645250764713, distance: 0.9181632037776403 entropy 0.07888543605804443
epoch: 17, step: 104
	action: tensor([[ 0.3157, -0.2944, -0.5539, -0.2109,  0.0819, -0.2579, -0.0981]],
       dtype=torch.float64)
	q_value: tensor([[-4.6875]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18533355136228447, distance: 1.0328722478928425 entropy 0.07888543605804443
epoch: 17, step: 105
	action: tensor([[ 0.5268, -0.0782, -0.5171, -0.1938,  0.0811, -0.2941, -0.4163]],
       dtype=torch.float64)
	q_value: tensor([[-5.0544]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4806485275490321, distance: 0.8246836357189518 entropy 0.07888543605804443
epoch: 17, step: 106
	action: tensor([[-0.0594,  0.4632,  0.1316,  0.1599,  0.2451,  0.6251,  0.0504]],
       dtype=torch.float64)
	q_value: tensor([[-5.5125]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6871448656363537, distance: 0.6400712737937544 entropy 0.07888543605804443
epoch: 17, step: 107
	action: tensor([[ 0.2015, -0.2392, -0.0587, -0.1258,  0.2530,  0.0817, -0.5484]],
       dtype=torch.float64)
	q_value: tensor([[-5.3761]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23095478360395427, distance: 1.0035352471907484 entropy 0.07888543605804443
epoch: 17, step: 108
	action: tensor([[ 0.2137, -0.1524, -0.2048,  0.0481, -0.1070,  0.1617, -0.4569]],
       dtype=torch.float64)
	q_value: tensor([[-4.7977]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4216071242499321, distance: 0.8702983512308476 entropy 0.07888543605804443
epoch: 17, step: 109
	action: tensor([[0.2289, 0.1832, 0.0633, 0.0512, 0.3366, 0.2629, 0.4188]],
       dtype=torch.float64)
	q_value: tensor([[-4.6660]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7092742851417325, distance: 0.6170188284327853 entropy 0.07888543605804443
epoch: 17, step: 110
	action: tensor([[ 0.4256,  0.5077, -0.2441, -0.2132, -0.1060,  0.4755, -0.3356]],
       dtype=torch.float64)
	q_value: tensor([[-5.6133]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8724028226133723, distance: 0.40876831700226735 entropy 0.07888543605804443
epoch: 17, step: 111
	action: tensor([[-0.1552,  0.3569, -0.4153, -0.2562,  0.1646, -0.3865, -0.1650]],
       dtype=torch.float64)
	q_value: tensor([[-5.5654]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34467629669024225, distance: 0.9263703188348806 entropy 0.07888543605804443
epoch: 17, step: 112
	action: tensor([[-0.0419,  0.4884, -0.3446, -0.8601,  0.3320,  0.3761, -0.0928]],
       dtype=torch.float64)
	q_value: tensor([[-4.4252]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5205131733648087, distance: 0.7924010333650193 entropy 0.07888543605804443
epoch: 17, step: 113
	action: tensor([[ 0.3330,  0.6300, -0.7848,  0.0668,  0.1019,  0.1080, -0.7516]],
       dtype=torch.float64)
	q_value: tensor([[-5.2581]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8187260995884718, distance: 0.48721913116269794 entropy 0.07888543605804443
epoch: 17, step: 114
	action: tensor([[-0.1745, -0.1151, -0.6719,  0.4222,  0.5201,  0.3881, -0.0380]],
       dtype=torch.float64)
	q_value: tensor([[-5.5605]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15041930410066973, distance: 1.0547730169216556 entropy 0.07888543605804443
epoch: 17, step: 115
	action: tensor([[ 0.3153,  0.6904, -0.0042,  0.0341,  0.0092,  0.1548, -0.2032]],
       dtype=torch.float64)
	q_value: tensor([[-5.2672]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8269813222198961, distance: 0.4759958734009311 entropy 0.07888543605804443
epoch: 17, step: 116
	action: tensor([[ 0.1355, -0.2773, -0.3835, -0.4173,  0.4280, -0.0522, -0.4519]],
       dtype=torch.float64)
	q_value: tensor([[-5.5589]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.056027336960312035, distance: 1.1118249151958797 entropy 0.07888543605804443
epoch: 17, step: 117
	action: tensor([[ 0.1640, -0.0149, -0.6111, -0.3116,  0.1473, -0.2353, -0.2141]],
       dtype=torch.float64)
	q_value: tensor([[-4.5929]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3870115931230249, distance: 0.8959480401000482 entropy 0.07888543605804443
epoch: 17, step: 118
	action: tensor([[ 0.2348,  0.7266, -0.5182, -0.2474,  0.4185, -0.0285, -0.1412]],
       dtype=torch.float64)
	q_value: tensor([[-4.6065]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8391525293685856, distance: 0.4589483482960515 entropy 0.07888543605804443
epoch: 17, step: 119
	action: tensor([[ 0.3911,  0.3450, -0.5911, -0.5255, -0.1189, -0.0667, -0.1936]],
       dtype=torch.float64)
	q_value: tensor([[-5.2821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7567432435712335, distance: 0.5644027862803845 entropy 0.07888543605804443
epoch: 17, step: 120
	action: tensor([[-0.0441,  0.1807, -0.3927, -0.3636,  0.1035,  0.1938, -0.2375]],
       dtype=torch.float64)
	q_value: tensor([[-5.3085]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3729092327986424, distance: 0.9061954903002172 entropy 0.07888543605804443
epoch: 17, step: 121
	action: tensor([[ 0.4915,  0.1524, -0.2093, -0.6867,  0.0432, -0.2223, -0.2390]],
       dtype=torch.float64)
	q_value: tensor([[-4.0022]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.45264215257912876, distance: 0.8466274935317301 entropy 0.07888543605804443
epoch: 17, step: 122
	action: tensor([[ 0.4381,  0.1539, -0.3536, -0.2443, -0.1601, -0.2615, -0.3183]],
       dtype=torch.float64)
	q_value: tensor([[-5.6120]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6539995744495046, distance: 0.6731238981523566 entropy 0.07888543605804443
epoch: 17, step: 123
	action: tensor([[ 0.5095,  0.1243,  0.1358,  0.0343,  0.5734,  0.2174, -0.0566]],
       dtype=torch.float64)
	q_value: tensor([[-5.2645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8187515552514855, distance: 0.4871849207180927 entropy 0.07888543605804443
epoch: 17, step: 124
	action: tensor([[-0.2469,  0.0485, -0.1529, -0.0692,  0.4650, -0.8548,  0.0726]],
       dtype=torch.float64)
	q_value: tensor([[-5.8352]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11943593733771851, distance: 1.2107551242086028 entropy 0.07888543605804443
epoch: 17, step: 125
	action: tensor([[ 0.7100,  0.5695,  0.0738, -0.7980,  0.3156, -0.4397, -0.4367]],
       dtype=torch.float64)
	q_value: tensor([[-6.0665]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7069002781753633, distance: 0.6195329314871976 entropy 0.07888543605804443
epoch: 17, step: 126
	action: tensor([[-0.0653,  0.1089, -0.5846, -0.3951,  0.0782, -0.2247, -0.2234]],
       dtype=torch.float64)
	q_value: tensor([[-7.4163]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2673780335991891, distance: 0.9794824555060795 entropy 0.07888543605804443
epoch: 17, step: 127
	action: tensor([[ 0.1427, -0.2496, -0.0042, -0.5623,  0.3070,  0.0930,  0.0060]],
       dtype=torch.float64)
	q_value: tensor([[-4.2199]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0532165280499004, distance: 1.174398603622093 entropy 0.07888543605804443
LOSS epoch 17 actor 16.00257490304298 critic 61.47825782996003 
epoch: 18, step: 0
	action: tensor([[ 0.2452,  0.1942, -0.6904, -0.0770,  0.1705, -0.4409, -0.2263]],
       dtype=torch.float64)
	q_value: tensor([[-3.9008]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.601870524283893, distance: 0.7220526384204244 entropy 0.03264415264129639
epoch: 18, step: 1
	action: tensor([[ 0.0622,  0.2927,  0.1358, -0.3703,  0.2869,  0.0647, -0.1371]],
       dtype=torch.float64)
	q_value: tensor([[-4.2889]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4188942453534261, distance: 0.8723369759214318 entropy 0.03264415264129639
epoch: 18, step: 2
	action: tensor([[-0.0676,  0.1981, -0.1368, -0.3702, -0.1729,  0.4116,  0.2494]],
       dtype=torch.float64)
	q_value: tensor([[-3.7921]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3219036617108677, distance: 0.942328644617752 entropy 0.03264415264129639
epoch: 18, step: 3
	action: tensor([[ 0.4019,  0.3009, -0.8598, -0.3505,  0.1506,  0.0505, -0.3761]],
       dtype=torch.float64)
	q_value: tensor([[-4.1710]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7936212162431558, distance: 0.519863369835236 entropy 0.03264415264129639
epoch: 18, step: 4
	action: tensor([[ 0.2930,  0.2079, -0.5957, -0.4324, -0.0580, -0.0890, -0.2741]],
       dtype=torch.float64)
	q_value: tensor([[-4.1188]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6437284893492199, distance: 0.6830417354477096 entropy 0.03264415264129639
epoch: 18, step: 5
	action: tensor([[-0.0740,  0.0599, -0.4498, -0.4679,  0.0141,  0.0491,  0.1636]],
       dtype=torch.float64)
	q_value: tensor([[-3.8145]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22529262237944359, distance: 1.007222779268162 entropy 0.03264415264129639
epoch: 18, step: 6
	action: tensor([[ 0.4173,  0.3373, -0.0426, -0.1569, -0.1180,  0.1494,  0.0089]],
       dtype=torch.float64)
	q_value: tensor([[-3.5667]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7966694110080293, distance: 0.5160099221828853 entropy 0.03264415264129639
epoch: 18, step: 7
	action: tensor([[ 0.2851,  0.2845,  0.4356,  0.1677,  0.0903, -0.0498, -0.4339]],
       dtype=torch.float64)
	q_value: tensor([[-4.3518]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7816956952676458, distance: 0.5346724946201564 entropy 0.03264415264129639
epoch: 18, step: 8
	action: tensor([[ 0.2292,  0.0173, -0.0430, -0.3532, -0.1052,  0.0528,  0.1732]],
       dtype=torch.float64)
	q_value: tensor([[-5.1068]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3616726732971688, distance: 0.9142782992171117 entropy 0.03264415264129639
epoch: 18, step: 9
	action: tensor([[ 0.3832, -0.0953, -0.2775, -0.2770,  0.7250, -0.4627, -0.1433]],
       dtype=torch.float64)
	q_value: tensor([[-4.1036]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2782369179493386, distance: 0.9721964376883722 entropy 0.03264415264129639
epoch: 18, step: 10
	action: tensor([[ 0.1549,  0.4016,  0.0451, -0.4187, -0.0196,  0.4701, -0.0500]],
       dtype=torch.float64)
	q_value: tensor([[-4.9314]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6331977650727456, distance: 0.6930629452916154 entropy 0.03264415264129639
epoch: 18, step: 11
	action: tensor([[-0.0602, -0.0834, -0.2479, -0.2137, -0.1739,  0.1116,  0.1525]],
       dtype=torch.float64)
	q_value: tensor([[-4.4112]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1390320291035395, distance: 1.061818263418564 entropy 0.03264415264129639
epoch: 18, step: 12
	action: tensor([[ 0.3562,  0.2154, -0.2132, -0.7090,  0.0419,  0.2971, -0.5845]],
       dtype=torch.float64)
	q_value: tensor([[-3.5742]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5488046910712583, distance: 0.7686683177450045 entropy 0.03264415264129639
epoch: 18, step: 13
	action: tensor([[ 0.2579,  0.3278, -0.3551, -0.3603,  0.0737,  0.0960,  0.0804]],
       dtype=torch.float64)
	q_value: tensor([[-4.2267]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6808018827286317, distance: 0.6465272789580283 entropy 0.03264415264129639
epoch: 18, step: 14
	action: tensor([[ 0.0647,  0.1476, -0.8854, -0.2329,  0.0123,  0.5111, -0.5913]],
       dtype=torch.float64)
	q_value: tensor([[-3.8837]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4974134130176372, distance: 0.8112638799874464 entropy 0.03264415264129639
epoch: 18, step: 15
	action: tensor([[ 0.3778, -0.0637, -0.2645, -0.3914,  0.1998, -0.6657, -0.5065]],
       dtype=torch.float64)
	q_value: tensor([[-3.7188]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.265465850666045, distance: 0.9807598735933986 entropy 0.03264415264129639
epoch: 18, step: 16
	action: tensor([[ 0.2474,  0.2027,  0.0549, -0.4838,  0.6750, -0.0987,  0.1229]],
       dtype=torch.float64)
	q_value: tensor([[-4.9466]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.40666788303627777, distance: 0.8814661148648976 entropy 0.03264415264129639
epoch: 18, step: 17
	action: tensor([[ 0.2591,  0.0998, -0.3308,  0.4134, -0.1732, -0.1101,  0.1617]],
       dtype=torch.float64)
	q_value: tensor([[-4.4732]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.699147325696882, distance: 0.6276732659340081 entropy 0.03264415264129639
epoch: 18, step: 18
	action: tensor([[-0.1239,  0.1732, -0.4073, -0.2679,  0.4853,  0.0191,  0.0958]],
       dtype=torch.float64)
	q_value: tensor([[-4.3837]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2759201975458119, distance: 0.9737554692501992 entropy 0.03264415264129639
epoch: 18, step: 19
	action: tensor([[ 0.4548,  0.3819, -0.1831, -0.0868,  0.1790,  0.0196, -0.2609]],
       dtype=torch.float64)
	q_value: tensor([[-3.4341]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8346686684931836, distance: 0.46530130970032924 entropy 0.03264415264129639
epoch: 18, step: 20
	action: tensor([[ 0.1360,  0.0250,  0.1950, -0.0914,  0.3890, -0.4383, -0.4395]],
       dtype=torch.float64)
	q_value: tensor([[-4.1333]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27625741982488616, distance: 0.9735286915668656 entropy 0.03264415264129639
epoch: 18, step: 21
	action: tensor([[0.0969, 0.1384, 0.1936, 0.0640, 0.4849, 0.6542, 0.1933]],
       dtype=torch.float64)
	q_value: tensor([[-4.4980]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7173729716279498, distance: 0.6083640457551885 entropy 0.03264415264129639
epoch: 18, step: 22
	action: tensor([[ 0.2935,  0.1730, -0.1835, -0.4088, -0.0791, -0.2186, -0.0581]],
       dtype=torch.float64)
	q_value: tensor([[-4.5279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4739804168335292, distance: 0.8299609319218944 entropy 0.03264415264129639
epoch: 18, step: 23
	action: tensor([[ 0.4213,  0.1724, -0.0028, -0.3249,  0.2407, -0.3554, -0.3227]],
       dtype=torch.float64)
	q_value: tensor([[-4.0377]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5232374427036406, distance: 0.7901467599239638 entropy 0.03264415264129639
epoch: 18, step: 24
	action: tensor([[ 0.4164,  0.0439, -0.0411, -0.8601, -0.3175,  0.2333, -0.6294]],
       dtype=torch.float64)
	q_value: tensor([[-4.5571]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28574613430564544, distance: 0.967125853303911 entropy 0.03264415264129639
epoch: 18, step: 25
	action: tensor([[ 0.3674,  0.0395,  0.3597, -0.0749,  0.0667, -0.0633, -0.9871]],
       dtype=torch.float64)
	q_value: tensor([[-5.0166]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5694843994441607, distance: 0.7508464643318 entropy 0.03264415264129639
epoch: 18, step: 26
	action: tensor([[ 0.0748,  0.6195, -0.2965, -0.4763,  0.3709,  0.1429, -0.0235]],
       dtype=torch.float64)
	q_value: tensor([[-5.3325]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6853178086648838, distance: 0.6419375437472725 entropy 0.03264415264129639
epoch: 18, step: 27
	action: tensor([[ 0.4199,  0.5995, -0.4675, -0.1257,  0.0372,  0.1036, -0.0947]],
       dtype=torch.float64)
	q_value: tensor([[-3.8846]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8929918352070865, distance: 0.3743390649642811 entropy 0.03264415264129639
epoch: 18, step: 28
	action: tensor([[ 0.0797,  0.2582, -0.4089,  0.2637, -0.2648,  0.2297,  0.1667]],
       dtype=torch.float64)
	q_value: tensor([[-4.3169]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5763333609234544, distance: 0.7448500084790408 entropy 0.03264415264129639
epoch: 18, step: 29
	action: tensor([[ 0.5479,  0.5500, -0.3472, -0.2149,  0.0848, -0.0116, -0.0076]],
       dtype=torch.float64)
	q_value: tensor([[-3.9450]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9199394350375572, distance: 0.32379192854528366 entropy 0.03264415264129639
epoch: 18, step: 30
	action: tensor([[ 0.6055,  0.1610,  0.0943, -0.4202,  0.0991, -0.1632, -0.3350]],
       dtype=torch.float64)
	q_value: tensor([[-4.6561]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5914940346377581, distance: 0.7314015808760075 entropy 0.03264415264129639
epoch: 18, step: 31
	action: tensor([[-0.2698,  0.1968, -0.3814, -0.3517,  0.0727, -0.0892, -0.2178]],
       dtype=torch.float64)
	q_value: tensor([[-4.9192]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1151787113503141, distance: 1.0764267328341908 entropy 0.03264415264129639
epoch: 18, step: 32
	action: tensor([[ 0.1423,  0.6376, -0.4297, -0.3752,  0.2669, -0.4039,  0.0999]],
       dtype=torch.float64)
	q_value: tensor([[-3.0437]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7146927762196824, distance: 0.6112418439271388 entropy 0.03264415264129639
epoch: 18, step: 33
	action: tensor([[ 0.1345, -0.0225, -0.1214, -0.3763,  0.2209,  0.4961,  0.1379]],
       dtype=torch.float64)
	q_value: tensor([[-4.2140]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39163456048170187, distance: 0.8925631657800521 entropy 0.03264415264129639
epoch: 18, step: 34
	action: tensor([[ 0.1231,  0.1870, -0.1828, -0.4834,  0.0567,  0.1548,  0.0168]],
       dtype=torch.float64)
	q_value: tensor([[-4.0005]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41527082681296257, distance: 0.8750524282996817 entropy 0.03264415264129639
epoch: 18, step: 35
	action: tensor([[-0.0367,  0.1237, -0.1776,  0.0030,  0.0922, -0.4969, -0.2460]],
       dtype=torch.float64)
	q_value: tensor([[-3.6985]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29897687724854516, distance: 0.9581265280370493 entropy 0.03264415264129639
epoch: 18, step: 36
	action: tensor([[-0.4365, -0.0374, -0.0295, -0.1637,  0.5118,  0.0289, -0.1946]],
       dtype=torch.float64)
	q_value: tensor([[-4.0141]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2786779110014421, distance: 1.294008933800899 entropy 0.03264415264129639
epoch: 18, step: 37
	action: tensor([[ 0.4523, -0.1039, -0.3948, -0.6017,  0.3015, -0.0031, -0.3256]],
       dtype=torch.float64)
	q_value: tensor([[-3.5117]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2864330099361748, distance: 0.9666607140886939 entropy 0.03264415264129639
epoch: 18, step: 38
	action: tensor([[ 0.0908,  0.1503, -0.3187, -0.3143,  0.0838, -0.0119,  0.1484]],
       dtype=torch.float64)
	q_value: tensor([[-4.0058]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4215863375292982, distance: 0.8703139898105376 entropy 0.03264415264129639
epoch: 18, step: 39
	action: tensor([[-0.1785,  0.2836,  0.0520,  0.1891,  0.3235, -0.2100, -0.3501]],
       dtype=torch.float64)
	q_value: tensor([[-3.5555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34053937433930015, distance: 0.929289710358573 entropy 0.03264415264129639
epoch: 18, step: 40
	action: tensor([[ 0.4898,  0.2069, -0.0561,  0.3782, -0.3621,  0.2821, -0.0551]],
       dtype=torch.float64)
	q_value: tensor([[-3.9597]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8861232200163005, distance: 0.38616622124296546 entropy 0.03264415264129639
epoch: 18, step: 41
	action: tensor([[ 0.4744,  0.2382, -0.2886, -0.0916,  0.3373, -0.2847, -0.4490]],
       dtype=torch.float64)
	q_value: tensor([[-4.8641]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7426005161719742, distance: 0.5805779386839793 entropy 0.03264415264129639
epoch: 18, step: 42
	action: tensor([[0.4999, 0.1719, 0.1664, 0.0958, 0.4558, 0.0070, 0.0368]],
       dtype=torch.float64)
	q_value: tensor([[-4.5138]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8202339930919074, distance: 0.48518847820517064 entropy 0.03264415264129639
epoch: 18, step: 43
	action: tensor([[ 0.2082, -0.2955,  0.0120,  0.1400, -0.6585,  0.1310,  0.1168]],
       dtype=torch.float64)
	q_value: tensor([[-4.8657]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35979068527681723, distance: 0.9156250960245516 entropy 0.03264415264129639
epoch: 18, step: 44
	action: tensor([[-0.1549,  0.3727, -0.1645,  0.3714, -0.0174, -0.6378, -0.2792]],
       dtype=torch.float64)
	q_value: tensor([[-4.8246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.43782642959010787, distance: 0.8580091224761621 entropy 0.03264415264129639
epoch: 18, step: 45
	action: tensor([[-0.0031,  0.7898,  0.0776,  0.0483, -0.1613,  0.0083, -0.2442]],
       dtype=torch.float64)
	q_value: tensor([[-4.6039]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6480563625725415, distance: 0.6788803720072751 entropy 0.03264415264129639
epoch: 18, step: 46
	action: tensor([[ 0.0256,  0.5861, -0.2100,  0.0669, -0.0268, -0.0493, -0.0491]],
       dtype=torch.float64)
	q_value: tensor([[-4.5654]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6457964186922924, distance: 0.6810565393053254 entropy 0.03264415264129639
epoch: 18, step: 47
	action: tensor([[ 0.0818,  0.2707, -0.5785, -0.3303, -0.0267, -0.1128,  0.2585]],
       dtype=torch.float64)
	q_value: tensor([[-3.8179]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5555321915668116, distance: 0.7629162214045099 entropy 0.03264415264129639
epoch: 18, step: 48
	action: tensor([[ 0.6509,  0.0473, -0.4701, -0.3333,  0.0469,  0.2587, -0.5336]],
       dtype=torch.float64)
	q_value: tensor([[-3.9441]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6894220011217956, distance: 0.6377376204102092 entropy 0.03264415264129639
epoch: 18, step: 49
	action: tensor([[ 0.7191,  0.3397,  0.0218, -0.5837,  0.1744, -0.2739, -0.4023]],
       dtype=torch.float64)
	q_value: tensor([[-4.4217]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6719309197346122, distance: 0.6554496619535949 entropy 0.03264415264129639
epoch: 18, step: 50
	action: tensor([[-0.1013,  0.0948, -0.0886, -0.4484,  0.4333,  0.0699, -0.4652]],
       dtype=torch.float64)
	q_value: tensor([[-5.4276]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13537123372118454, distance: 1.06407326961333 entropy 0.03264415264129639
epoch: 18, step: 51
	action: tensor([[ 0.2996,  0.2144, -0.0464, -0.2605, -0.2195,  0.1142, -0.4292]],
       dtype=torch.float64)
	q_value: tensor([[-3.3043]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6145365622040813, distance: 0.7104741620218339 entropy 0.03264415264129639
epoch: 18, step: 52
	action: tensor([[ 0.5303,  0.4231, -0.3537, -0.1885,  0.1204,  0.3829, -0.8033]],
       dtype=torch.float64)
	q_value: tensor([[-4.2083]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9008533835384256, distance: 0.3603260344970476 entropy 0.03264415264129639
epoch: 18, step: 53
	action: tensor([[-0.1984,  0.6344,  0.1409, -0.5715,  0.2139, -0.3367, -0.5670]],
       dtype=torch.float64)
	q_value: tensor([[-4.6035]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20162528087664855, distance: 1.0224923830457873 entropy 0.03264415264129639
epoch: 18, step: 54
	action: tensor([[ 0.4085,  0.2449, -0.2047, -0.2561, -0.0082, -0.0889, -0.0766]],
       dtype=torch.float64)
	q_value: tensor([[-4.3337]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6706859433226486, distance: 0.6566921543323839 entropy 0.03264415264129639
epoch: 18, step: 55
	action: tensor([[ 0.5000, -0.2578, -0.0691, -0.3757,  0.2717, -0.1263, -0.7526]],
       dtype=torch.float64)
	q_value: tensor([[-4.1126]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18267782768903862, distance: 1.034554403474412 entropy 0.03264415264129639
epoch: 18, step: 56
	action: tensor([[ 0.3147,  0.2815, -0.2855,  0.1007, -0.0271, -0.0165, -0.2846]],
       dtype=torch.float64)
	q_value: tensor([[-4.5491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.74388805947744, distance: 0.5791240577984873 entropy 0.03264415264129639
epoch: 18, step: 57
	action: tensor([[-0.0962,  0.2437, -0.2707, -0.3604,  0.2179,  0.4529, -0.2035]],
       dtype=torch.float64)
	q_value: tensor([[-3.9259]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4040084005501894, distance: 0.8834393964448668 entropy 0.03264415264129639
epoch: 18, step: 58
	action: tensor([[ 0.2630,  0.2723, -0.1853,  0.0641,  0.0922,  0.0806, -0.5173]],
       dtype=torch.float64)
	q_value: tensor([[-3.3794]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7343645230998546, distance: 0.5897931600126687 entropy 0.03264415264129639
epoch: 18, step: 59
	action: tensor([[ 0.3872,  0.0882, -0.4506, -0.4807, -0.0856,  0.1979,  0.1188]],
       dtype=torch.float64)
	q_value: tensor([[-3.8799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5789661178149282, distance: 0.7425320716284469 entropy 0.03264415264129639
epoch: 18, step: 60
	action: tensor([[ 0.3885,  0.3327, -0.2229, -0.3688, -0.0573,  0.0460,  0.1267]],
       dtype=torch.float64)
	q_value: tensor([[-4.3443]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7349971518561633, distance: 0.5890904253191572 entropy 0.03264415264129639
epoch: 18, step: 61
	action: tensor([[ 0.2574,  0.0281,  0.0134,  0.1908,  0.2586, -0.1633, -0.0618]],
       dtype=torch.float64)
	q_value: tensor([[-4.3087]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6011167106135358, distance: 0.7227358781537231 entropy 0.03264415264129639
epoch: 18, step: 62
	action: tensor([[-0.1243,  0.1342, -0.0644,  0.0771,  0.4026, -0.1465, -0.3278]],
       dtype=torch.float64)
	q_value: tensor([[-4.3327]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27348330629514683, distance: 0.9753926803684649 entropy 0.03264415264129639
epoch: 18, step: 63
	action: tensor([[-0.0852,  0.3419,  0.1393, -0.7919,  0.3291,  0.2037, -0.1431]],
       dtype=torch.float64)
	q_value: tensor([[-3.6749]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15333064519684925, distance: 1.0529642191704072 entropy 0.03264415264129639
epoch: 18, step: 64
	action: tensor([[ 0.2277,  0.2574, -0.3284, -0.3932, -0.0567, -0.1614, -0.1725]],
       dtype=torch.float64)
	q_value: tensor([[-4.0057]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5376632345284339, distance: 0.778100883496936 entropy 0.03264415264129639
epoch: 18, step: 65
	action: tensor([[ 0.4760,  0.4222, -0.2293,  0.2601, -0.0198,  0.3341, -0.3242]],
       dtype=torch.float64)
	q_value: tensor([[-3.7468]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8833311626045596, distance: 0.3908716106109434 entropy 0.03264415264129639
epoch: 18, step: 66
	action: tensor([[ 0.1780,  0.4547, -0.3374, -0.3024,  0.0884,  0.6447, -0.1837]],
       dtype=torch.float64)
	q_value: tensor([[-4.4522]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7425929809221192, distance: 0.5805864366959917 entropy 0.03264415264129639
epoch: 18, step: 67
	action: tensor([[ 0.5628,  0.2878, -0.3912, -0.4556,  0.0256,  0.2393,  0.1064]],
       dtype=torch.float64)
	q_value: tensor([[-4.1957]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7845646305417739, distance: 0.5311475666540406 entropy 0.03264415264129639
epoch: 18, step: 68
	action: tensor([[ 0.0346,  0.1290,  0.0110, -0.4703, -0.3282,  0.3153, -0.3072]],
       dtype=torch.float64)
	q_value: tensor([[-4.6779]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28866166021913797, distance: 0.9651499705425062 entropy 0.03264415264129639
epoch: 18, step: 69
	action: tensor([[ 0.3982, -0.1126,  0.0879, -0.0643,  0.2551,  0.2564, -0.3072]],
       dtype=torch.float64)
	q_value: tensor([[-4.2261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5630103540328085, distance: 0.7564709706026764 entropy 0.03264415264129639
epoch: 18, step: 70
	action: tensor([[-0.1568, -0.0815,  0.5383, -0.3869,  0.3140, -0.1286,  0.0356]],
       dtype=torch.float64)
	q_value: tensor([[-4.2235]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.25814263079412547, distance: 1.2835761306472249 entropy 0.03264415264129639
epoch: 18, step: 71
	action: tensor([[ 0.6043,  0.0736, -0.2031, -0.6847,  0.4738,  0.1368, -0.1294]],
       dtype=torch.float64)
	q_value: tensor([[-4.6210]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.47630966153923615, distance: 0.8281213360399045 entropy 0.03264415264129639
epoch: 18, step: 72
	action: tensor([[ 0.4367,  0.3291, -0.1216, -0.0141,  0.0083, -0.0356, -0.2161]],
       dtype=torch.float64)
	q_value: tensor([[-4.7172]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8222567057644481, distance: 0.48245110535173796 entropy 0.03264415264129639
epoch: 18, step: 73
	action: tensor([[ 0.2318,  0.0267,  0.1632, -0.3871, -0.2329, -0.0210, -0.2897]],
       dtype=torch.float64)
	q_value: tensor([[-4.2487]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3272204029580249, distance: 0.9386271226723618 entropy 0.03264415264129639
epoch: 18, step: 74
	action: tensor([[-0.4997,  0.3487, -0.4503, -0.4377,  0.7875,  0.1028, -0.7748]],
       dtype=torch.float64)
	q_value: tensor([[-4.3647]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.008933500354910251, distance: 1.139221286925373 entropy 0.03264415264129639
epoch: 18, step: 75
	action: tensor([[ 0.3804,  0.1487, -0.2224, -0.4434,  0.6400,  0.1190, -0.7539]],
       dtype=torch.float64)
	q_value: tensor([[-3.7810]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5650745487501452, distance: 0.7546821962341912 entropy 0.03264415264129639
epoch: 18, step: 76
	action: tensor([[ 0.3435,  0.7555, -0.1957, -0.2479, -0.4972,  0.1643, -0.3970]],
       dtype=torch.float64)
	q_value: tensor([[-4.1297]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8475221441229903, distance: 0.4468482705831153 entropy 0.03264415264129639
epoch: 18, step: 77
	action: tensor([[ 0.2219,  0.4438, -0.1743, -0.4491,  0.0132, -0.1336, -0.1583]],
       dtype=torch.float64)
	q_value: tensor([[-5.1896]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6522264324468492, distance: 0.674846466884754 entropy 0.03264415264129639
epoch: 18, step: 78
	action: tensor([[ 0.2524,  0.3888, -0.1389,  0.1251,  0.1591,  0.0905, -0.1407]],
       dtype=torch.float64)
	q_value: tensor([[-3.9839]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7873640108114153, distance: 0.5276854011184026 entropy 0.03264415264129639
epoch: 18, step: 79
	action: tensor([[ 0.2793, -0.1126, -0.2879, -0.1712, -0.3126,  0.0795, -0.3101]],
       dtype=torch.float64)
	q_value: tensor([[-3.9112]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41630713206422376, distance: 0.874276664398601 entropy 0.03264415264129639
epoch: 18, step: 80
	action: tensor([[ 0.1167,  0.8313, -0.0143, -0.3188, -0.1918,  0.2586, -0.2577]],
       dtype=torch.float64)
	q_value: tensor([[-3.9024]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.715211675266414, distance: 0.6106857466247763 entropy 0.03264415264129639
epoch: 18, step: 81
	action: tensor([[ 0.0696,  0.0938, -0.0464, -0.2467, -0.1140,  0.1437,  0.0418]],
       dtype=torch.float64)
	q_value: tensor([[-4.8616]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3777328560557186, distance: 0.9027035046568526 entropy 0.03264415264129639
epoch: 18, step: 82
	action: tensor([[ 0.0908,  0.2234, -0.2979, -0.2944,  0.3223, -0.1164, -0.1855]],
       dtype=torch.float64)
	q_value: tensor([[-3.7687]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4619587937151244, distance: 0.8393912971696703 entropy 0.03264415264129639
epoch: 18, step: 83
	action: tensor([[ 0.2722,  0.2568, -0.3788, -0.3563,  0.1654,  0.1033, -0.1892]],
       dtype=torch.float64)
	q_value: tensor([[-3.3866]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6478190233681562, distance: 0.67910924071671 entropy 0.03264415264129639
epoch: 18, step: 84
	action: tensor([[-0.2248,  0.6811, -1.0689, -0.2206, -0.2063, -0.2222, -0.2401]],
       dtype=torch.float64)
	q_value: tensor([[-3.5852]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5966235519246659, distance: 0.7267950520494494 entropy 0.03264415264129639
epoch: 18, step: 85
	action: tensor([[ 0.4676,  0.2319, -0.3310,  0.1449,  0.4136,  0.2332, -0.5105]],
       dtype=torch.float64)
	q_value: tensor([[-4.1991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8643616832385101, distance: 0.4214517767758276 entropy 0.03264415264129639
epoch: 18, step: 86
	action: tensor([[ 0.0478, -0.1078,  0.0171, -0.0277,  0.0457, -0.0515, -0.5165]],
       dtype=torch.float64)
	q_value: tensor([[-4.2931]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25961415540728106, distance: 0.9846587549485442 entropy 0.03264415264129639
epoch: 18, step: 87
	action: tensor([[ 0.1351,  0.4489, -0.3465, -0.1016,  0.1428, -0.0825, -0.0038]],
       dtype=torch.float64)
	q_value: tensor([[-3.8378]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.672315759733378, distance: 0.6550651129684829 entropy 0.03264415264129639
epoch: 18, step: 88
	action: tensor([[ 0.2231,  0.2607, -0.6319, -0.0127,  0.2850,  0.0515, -0.7425]],
       dtype=torch.float64)
	q_value: tensor([[-3.6342]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.684400745289008, distance: 0.642872247311324 entropy 0.03264415264129639
epoch: 18, step: 89
	action: tensor([[ 0.0595, -0.0364, -0.3957, -0.2070,  0.3178,  0.0511, -0.0517]],
       dtype=torch.float64)
	q_value: tensor([[-3.8715]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.310572088701663, distance: 0.950169584175907 entropy 0.03264415264129639
epoch: 18, step: 90
	action: tensor([[ 0.1190,  0.4157, -0.2871, -0.3212,  0.5902, -0.2548,  0.0070]],
       dtype=torch.float64)
	q_value: tensor([[-3.3175]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5635808588700215, distance: 0.7559770100461893 entropy 0.03264415264129639
epoch: 18, step: 91
	action: tensor([[ 0.1775,  0.5127, -0.2228, -0.5252, -0.0058, -0.1109, -0.5280]],
       dtype=torch.float64)
	q_value: tensor([[-4.0317]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6487843367073156, distance: 0.6781778970610101 entropy 0.03264415264129639
epoch: 18, step: 92
	action: tensor([[ 0.3053,  0.4055, -0.4684, -0.1866,  0.0561,  0.4602,  0.0833]],
       dtype=torch.float64)
	q_value: tensor([[-4.1326]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7971481565632167, distance: 0.5154020868274513 entropy 0.03264415264129639
epoch: 18, step: 93
	action: tensor([[ 0.1807, -0.0215, -0.1054, -0.6241,  0.1246,  0.2740, -0.1840]],
       dtype=torch.float64)
	q_value: tensor([[-4.1990]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2592990022710383, distance: 0.9848682979060912 entropy 0.03264415264129639
epoch: 18, step: 94
	action: tensor([[ 0.4742, -0.2172, -0.0352, -0.8665, -0.2511,  0.3610, -0.5142]],
       dtype=torch.float64)
	q_value: tensor([[-3.8218]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08582879067268845, distance: 1.0941338720576468 entropy 0.03264415264129639
epoch: 18, step: 95
	action: tensor([[ 0.4969, -0.3334, -0.3738, -0.6334, -0.0815,  0.0385, -0.5525]],
       dtype=torch.float64)
	q_value: tensor([[-4.9606]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08158410684198525, distance: 1.096671073915674 entropy 0.03264415264129639
epoch: 18, step: 96
	action: tensor([[ 0.3719,  0.6092, -0.2894, -0.1473, -0.0702,  0.1584,  0.1257]],
       dtype=torch.float64)
	q_value: tensor([[-4.2719]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8622772697513092, distance: 0.4246777474261705 entropy 0.03264415264129639
epoch: 18, step: 97
	action: tensor([[ 0.1073,  0.2586,  0.1664, -0.0870, -0.3669, -0.0121,  0.2355]],
       dtype=torch.float64)
	q_value: tensor([[-4.4877]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5428395148276768, distance: 0.7737328497602152 entropy 0.03264415264129639
epoch: 18, step: 98
	action: tensor([[ 0.7873,  0.1255, -0.2353, -0.6999, -0.3220, -0.0460, -0.5620]],
       dtype=torch.float64)
	q_value: tensor([[-4.5479]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4920985359872396, distance: 0.8155421759707197 entropy 0.03264415264129639
epoch: 18, step: 99
	action: tensor([[ 0.5217,  0.4027,  0.2571, -0.3300,  0.3258, -0.1571, -0.3536]],
       dtype=torch.float64)
	q_value: tensor([[-5.3904]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7644803187795438, distance: 0.5553545005243371 entropy 0.03264415264129639
epoch: 18, step: 100
	action: tensor([[ 0.4669, -0.1334, -0.1478, -0.3953,  0.4601,  0.0802,  0.0797]],
       dtype=torch.float64)
	q_value: tensor([[-5.0565]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3587150573780259, distance: 0.9163939528242754 entropy 0.03264415264129639
epoch: 18, step: 101
	action: tensor([[ 0.4783,  0.3104, -0.0551, -0.4618,  0.3202,  0.3555, -0.6828]],
       dtype=torch.float64)
	q_value: tensor([[-4.3904]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7585752408715493, distance: 0.5622734754073642 entropy 0.03264415264129639
epoch: 18, step: 102
	action: tensor([[ 0.3333,  0.2971, -0.2975, -0.1650,  0.0628,  0.3723,  0.1402]],
       dtype=torch.float64)
	q_value: tensor([[-4.5049]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7753823063780313, distance: 0.5423487893727094 entropy 0.03264415264129639
epoch: 18, step: 103
	action: tensor([[ 0.2291,  0.5334, -0.4825, -0.1019,  0.3382, -0.0454, -0.2565]],
       dtype=torch.float64)
	q_value: tensor([[-4.1093]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7863767018966223, distance: 0.528909053663657 entropy 0.03264415264129639
epoch: 18, step: 104
	action: tensor([[ 0.2446, -0.2896,  0.3004,  0.0292,  0.1656, -0.0115, -0.4664]],
       dtype=torch.float64)
	q_value: tensor([[-3.8637]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2984958706163563, distance: 0.958455180667051 entropy 0.03264415264129639
epoch: 18, step: 105
	action: tensor([[-0.0173,  0.0753, -0.3244, -0.2841,  0.3392,  0.0035, -0.3646]],
       dtype=torch.float64)
	q_value: tensor([[-4.5549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29242588908892597, distance: 0.9625929141912658 entropy 0.03264415264129639
epoch: 18, step: 106
	action: tensor([[ 0.2293,  0.5168, -0.4815,  0.3691,  0.0783, -0.0197,  0.0381]],
       dtype=torch.float64)
	q_value: tensor([[-3.1874]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7264078656367383, distance: 0.5985611110727612 entropy 0.03264415264129639
epoch: 18, step: 107
	action: tensor([[ 0.1442,  0.5992,  0.0792, -0.3282,  0.2039,  0.5144, -0.5682]],
       dtype=torch.float64)
	q_value: tensor([[-4.2871]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7413555017205156, distance: 0.5819803423714475 entropy 0.03264415264129639
epoch: 18, step: 108
	action: tensor([[-0.2281,  0.2662, -0.1490, -0.1649, -0.0027, -0.3361, -0.0864]],
       dtype=torch.float64)
	q_value: tensor([[-4.4924]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1725957407738562, distance: 1.040915724846012 entropy 0.03264415264129639
epoch: 18, step: 109
	action: tensor([[ 0.4987,  0.0631, -0.2711, -0.1612, -0.0496, -0.5074,  0.0030]],
       dtype=torch.float64)
	q_value: tensor([[-3.5012]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5507538385862915, distance: 0.76700621099149 entropy 0.03264415264129639
epoch: 18, step: 110
	action: tensor([[ 0.3035,  0.4955, -0.1769, -0.0017, -0.3353, -0.3308, -0.3598]],
       dtype=torch.float64)
	q_value: tensor([[-4.8727]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.801424536142454, distance: 0.5099404773014463 entropy 0.03264415264129639
epoch: 18, step: 111
	action: tensor([[ 0.3424,  0.1576, -0.1313, -0.7841,  0.0823,  0.3216,  0.6212]],
       dtype=torch.float64)
	q_value: tensor([[-4.6258]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.448593279030052, distance: 0.8497530282174889 entropy 0.03264415264129639
epoch: 18, step: 112
	action: tensor([[ 0.3121,  0.7696, -0.0797, -0.0500, -0.2425, -0.2972, -0.2608]],
       dtype=torch.float64)
	q_value: tensor([[-5.5415]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8565607011247502, distance: 0.43340185932663816 entropy 0.03264415264129639
epoch: 18, step: 113
	action: tensor([[ 0.5794,  0.1467, -0.1621, -0.4061,  0.0733, -0.0667,  0.1024]],
       dtype=torch.float64)
	q_value: tensor([[-4.9923]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6190384588301298, distance: 0.7063130993060003 entropy 0.03264415264129639
epoch: 18, step: 114
	action: tensor([[ 0.0858, -0.1627,  0.2680, -0.0941, -0.1048, -0.0445, -0.2151]],
       dtype=torch.float64)
	q_value: tensor([[-4.6733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1950291429912674, distance: 1.0267075887265689 entropy 0.03264415264129639
epoch: 18, step: 115
	action: tensor([[-0.0615,  0.1804, -0.3373, -0.3057,  0.1996,  0.4246, -0.3105]],
       dtype=torch.float64)
	q_value: tensor([[-4.1873]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4073741552541511, distance: 0.8809413325508384 entropy 0.03264415264129639
epoch: 18, step: 116
	action: tensor([[ 0.2476,  0.2892, -0.5690, -0.0822, -0.2804,  0.3618, -0.0194]],
       dtype=torch.float64)
	q_value: tensor([[-3.2475]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6967480457906805, distance: 0.6301711218711215 entropy 0.03264415264129639
epoch: 18, step: 117
	action: tensor([[-0.0299, -0.0093, -0.1378,  0.3178,  0.0196,  0.0111, -0.1366]],
       dtype=torch.float64)
	q_value: tensor([[-3.9854]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41141238586516726, distance: 0.8779347768575206 entropy 0.03264415264129639
epoch: 18, step: 118
	action: tensor([[-0.1532,  0.2940, -0.1401,  0.0359, -0.0953,  0.4378, -0.4452]],
       dtype=torch.float64)
	q_value: tensor([[-3.7985]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42222733470390283, distance: 0.86983161578677 entropy 0.03264415264129639
epoch: 18, step: 119
	action: tensor([[ 0.4203,  0.4572,  0.3444, -0.1225, -0.0778,  0.1324, -0.0629]],
       dtype=torch.float64)
	q_value: tensor([[-3.6331]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8431078328557394, distance: 0.4532703635393617 entropy 0.03264415264129639
epoch: 18, step: 120
	action: tensor([[-0.0996,  0.3114, -0.1033, -0.1285, -0.1701, -0.0095,  0.1502]],
       dtype=torch.float64)
	q_value: tensor([[-5.1298]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38452119828717113, distance: 0.8977661843695687 entropy 0.03264415264129639
epoch: 18, step: 121
	action: tensor([[ 0.0134,  0.2196, -0.0922, -0.4172,  0.0730,  0.0590,  0.0523]],
       dtype=torch.float64)
	q_value: tensor([[-3.7226]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3383537405866124, distance: 0.9308283972535505 entropy 0.03264415264129639
epoch: 18, step: 122
	action: tensor([[ 0.4528,  0.1100, -0.6550,  0.2144,  0.0202, -0.0060, -0.1599]],
       dtype=torch.float64)
	q_value: tensor([[-3.5988]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7396099228917179, distance: 0.583940918140191 entropy 0.03264415264129639
epoch: 18, step: 123
	action: tensor([[ 0.0326,  0.1094, -0.5873, -0.7539, -0.0835, -0.2997, -0.2227]],
       dtype=torch.float64)
	q_value: tensor([[-4.2979]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35952735249807954, distance: 0.9158133854686688 entropy 0.03264415264129639
epoch: 18, step: 124
	action: tensor([[ 0.1025,  0.2647, -0.5204, -0.3997,  0.4791,  0.1298,  0.1078]],
       dtype=torch.float64)
	q_value: tensor([[-3.8093]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5756317572182761, distance: 0.745466499489712 entropy 0.03264415264129639
epoch: 18, step: 125
	action: tensor([[ 0.3276,  0.2101, -0.1133, -0.3830, -0.1417, -0.3138, -0.3441]],
       dtype=torch.float64)
	q_value: tensor([[-3.7795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5456127916804046, distance: 0.7713824283933454 entropy 0.03264415264129639
epoch: 18, step: 126
	action: tensor([[-0.4455, -0.4354, -0.0282,  0.1639, -0.1696,  0.2863, -0.1314]],
       dtype=torch.float64)
	q_value: tensor([[-4.3642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.37070089936495365, distance: 1.3397631923433366 entropy 0.03264415264129639
epoch: 18, step: 127
	action: tensor([[ 0.2275, -0.3848, -0.0313, -0.0401,  0.0408,  0.4699, -0.2634]],
       dtype=torch.float64)
	q_value: tensor([[-3.9475]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2721887265027716, distance: 0.9762613194881831 entropy 0.03264415264129639
LOSS epoch 18 actor 11.370385769351287 critic 73.13366105569784 
epoch: 19, step: 0
	action: tensor([[ 0.3846,  0.6824, -0.0420,  0.0868,  0.1251,  0.1336, -0.2733]],
       dtype=torch.float64)
	q_value: tensor([[-3.2667]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8608194562055373, distance: 0.4269194662366738 entropy 0.03264415264129639
epoch: 19, step: 1
	action: tensor([[ 0.5915, -0.1006, -0.4598, -0.4784,  0.1675,  0.1899, -0.0746]],
       dtype=torch.float64)
	q_value: tensor([[-3.6409]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.452361598544582, distance: 0.8468444396589581 entropy 0.03264415264129639
epoch: 19, step: 2
	action: tensor([[ 0.6068,  0.1073, -0.4750, -0.2946, -0.1353,  0.3678, -0.6667]],
       dtype=torch.float64)
	q_value: tensor([[-3.5115]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7329246378506311, distance: 0.5913894956456821 entropy 0.03264415264129639
epoch: 19, step: 3
	action: tensor([[ 0.6999,  0.9091, -0.5399, -0.4656, -0.0732, -0.0832, -0.5461]],
       dtype=torch.float64)
	q_value: tensor([[-3.5856]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.09858772602146768 entropy 0.03264415264129639
epoch: 19, step: 4
	action: tensor([[ 1.0755,  0.4565, -0.0871, -0.9055,  0.2130, -0.0059, -0.2777]],
       dtype=torch.float64)
	q_value: tensor([[-4.3638]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6628967660222608, distance: 0.6644130480343248 entropy 0.03264415264129639
epoch: 19, step: 5
	action: tensor([[ 0.5302,  0.3852, -0.5393, -0.1625, -0.1264, -0.1360, -0.1818]],
       dtype=torch.float64)
	q_value: tensor([[-5.3443]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8324110832970026, distance: 0.4684673630999226 entropy 0.03264415264129639
epoch: 19, step: 6
	action: tensor([[ 0.4992,  0.6213,  0.0081, -0.6092, -0.1607,  0.2477, -0.9085]],
       dtype=torch.float64)
	q_value: tensor([[-3.5987]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8196407010843005, distance: 0.4859884662937467 entropy 0.03264415264129639
epoch: 19, step: 7
	action: tensor([[ 0.7021,  0.8183, -0.5499, -0.6577, -0.0180,  0.3162, -0.0822]],
       dtype=torch.float64)
	q_value: tensor([[-4.4839]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9863613175397395, distance: 0.1336419802901459 entropy 0.03264415264129639
epoch: 19, step: 8
	action: tensor([[ 0.5859,  0.1105, -0.2455, -0.6454,  0.3473,  0.2796, -0.4360]],
       dtype=torch.float64)
	q_value: tensor([[-4.6651]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5691670652619943, distance: 0.7511231388092589 entropy 0.03264415264129639
epoch: 19, step: 9
	action: tensor([[ 0.5564,  0.3256,  0.2829, -0.3388, -0.0373,  0.1449, -0.2297]],
       dtype=torch.float64)
	q_value: tensor([[-3.4382]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7486924703626555, distance: 0.5736664397173329 entropy 0.03264415264129639
epoch: 19, step: 10
	action: tensor([[ 0.8906,  0.6020, -0.2622, -0.0850, -0.0827,  0.1175, -0.3259]],
       dtype=torch.float64)
	q_value: tensor([[-4.1635]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9849826700339497, distance: 0.14023391379208983 entropy 0.03264415264129639
epoch: 19, step: 11
	action: tensor([[ 0.6488,  0.2185, -0.4857, -0.3960, -0.1694, -0.0098, -0.0902]],
       dtype=torch.float64)
	q_value: tensor([[-4.5140]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.739872587294895, distance: 0.5836463232116511 entropy 0.03264415264129639
epoch: 19, step: 12
	action: tensor([[ 0.5458,  0.1469,  0.0590, -0.2838, -0.2987,  0.4866, -0.1680]],
       dtype=torch.float64)
	q_value: tensor([[-3.7736]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7261274628905077, distance: 0.5988677631585181 entropy 0.03264415264129639
epoch: 19, step: 13
	action: tensor([[ 0.5438,  0.0861, -0.5827, -0.4655,  0.2030, -0.1286, -0.4647]],
       dtype=torch.float64)
	q_value: tensor([[-4.1395]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5804222811758207, distance: 0.7412469203041049 entropy 0.03264415264129639
epoch: 19, step: 14
	action: tensor([[ 0.3482,  0.7183,  0.0380, -0.1800, -0.1404,  0.1393, -0.2008]],
       dtype=torch.float64)
	q_value: tensor([[-3.3746]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8126877752822362, distance: 0.4952674136170873 entropy 0.03264415264129639
epoch: 19, step: 15
	action: tensor([[ 0.9164,  0.4488, -0.1364, -0.1909, -0.0521, -0.1456, -0.5783]],
       dtype=torch.float64)
	q_value: tensor([[-3.9378]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.919873239369729, distance: 0.3239257596914395 entropy 0.03264415264129639
epoch: 19, step: 16
	action: tensor([[ 0.7108,  0.1392, -0.1436, -0.2850,  0.2705,  0.2650, -0.3844]],
       dtype=torch.float64)
	q_value: tensor([[-4.7607]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.763244692215264, distance: 0.5568093961235374 entropy 0.03264415264129639
epoch: 19, step: 17
	action: tensor([[ 0.8058,  0.5773, -0.5473, -0.2127,  0.1661, -0.2644, -0.0686]],
       dtype=torch.float64)
	q_value: tensor([[-3.6826]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9551081724296718, distance: 0.24246013147883297 entropy 0.03264415264129639
epoch: 19, step: 18
	action: tensor([[ 0.5455,  0.1897, -0.6809, -0.4401,  0.1747, -0.1947, -0.7988]],
       dtype=torch.float64)
	q_value: tensor([[-4.5785]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6856349370326484, distance: 0.6416139983946928 entropy 0.03264415264129639
epoch: 19, step: 19
	action: tensor([[ 0.5114,  0.2340, -0.0658, -0.7248,  0.0713,  0.2193, -0.7084]],
       dtype=torch.float64)
	q_value: tensor([[-3.6244]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.557688936343522, distance: 0.7610629752705246 entropy 0.03264415264129639
epoch: 19, step: 20
	action: tensor([[ 0.4825,  0.4025,  0.2837, -0.6227,  0.1470,  0.3195, -0.3004]],
       dtype=torch.float64)
	q_value: tensor([[-3.7210]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.675135980547183, distance: 0.6522401061199743 entropy 0.03264415264129639
epoch: 19, step: 21
	action: tensor([[ 0.3887, -0.1967, -0.4669, -0.7602,  0.2360,  0.0399, -0.1150]],
       dtype=torch.float64)
	q_value: tensor([[-4.0541]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1440474429171673, distance: 1.058721030732621 entropy 0.03264415264129639
epoch: 19, step: 22
	action: tensor([[ 0.3388,  0.5628, -0.7611, -0.5435, -0.0604, -0.1707, -0.1033]],
       dtype=torch.float64)
	q_value: tensor([[-3.2568]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8313210538942085, distance: 0.4699883931594109 entropy 0.03264415264129639
epoch: 19, step: 23
	action: tensor([[ 0.5182,  0.2126, -0.2736, -0.2471,  0.2374,  0.5387, -0.8070]],
       dtype=torch.float64)
	q_value: tensor([[-3.5755]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7939873588787377, distance: 0.5194020127139801 entropy 0.03264415264129639
epoch: 19, step: 24
	action: tensor([[ 0.7549,  0.2063, -0.6620, -0.5496,  0.0224, -0.1115, -0.1502]],
       dtype=torch.float64)
	q_value: tensor([[-3.6037]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6964285392560069, distance: 0.630503008908494 entropy 0.03264415264129639
epoch: 19, step: 25
	action: tensor([[ 0.9158,  0.3460, -0.1749, -0.7027, -0.2575,  0.5484, -0.4548]],
       dtype=torch.float64)
	q_value: tensor([[-4.0479]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8180671870716553, distance: 0.48810382438078853 entropy 0.03264415264129639
epoch: 19, step: 26
	action: tensor([[ 0.5325,  0.0065, -0.2975, -0.1452,  0.2782, -0.0413, -0.4121]],
       dtype=torch.float64)
	q_value: tensor([[-4.9479]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5932751482811709, distance: 0.7298053588812847 entropy 0.03264415264129639
epoch: 19, step: 27
	action: tensor([[ 0.2918,  0.6678, -0.0427, -0.3474,  0.2875,  0.0045, -0.6291]],
       dtype=torch.float64)
	q_value: tensor([[-3.3883]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7513654447984124, distance: 0.5706074486836724 entropy 0.03264415264129639
epoch: 19, step: 28
	action: tensor([[ 0.4649,  0.4523, -0.1228, -0.7750, -0.3660,  0.5590, -0.4270]],
       dtype=torch.float64)
	q_value: tensor([[-3.5623]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7127289510251154, distance: 0.6133418847128611 entropy 0.03264415264129639
epoch: 19, step: 29
	action: tensor([[ 0.7335,  0.7448, -0.1499,  0.3682,  0.4442,  0.2185, -0.5175]],
       dtype=torch.float64)
	q_value: tensor([[-4.5598]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 19, step: 30
	action: tensor([[ 0.9034,  0.8718, -0.7760, -1.2404,  0.1825,  0.4279, -0.4973]],
       dtype=torch.float64)
	q_value: tensor([[-4.3638]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8169582329841157, distance: 0.4895891595148602 entropy 0.03264415264129639
epoch: 19, step: 31
	action: tensor([[ 1.1024,  0.5194, -0.6871, -0.2174,  0.1191,  0.0838, -0.7135]],
       dtype=torch.float64)
	q_value: tensor([[-5.5252]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8965872716339922, distance: 0.3679965028200632 entropy 0.03264415264129639
epoch: 19, step: 32
	action: tensor([[ 0.5777,  0.4553, -0.2395, -0.7137,  0.2179,  0.2461, -0.5382]],
       dtype=torch.float64)
	q_value: tensor([[-5.1619]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8162913070819459, distance: 0.49048027527852367 entropy 0.03264415264129639
epoch: 19, step: 33
	action: tensor([[ 0.3779,  0.5104, -0.3659, -0.3228,  0.1815,  0.2782, -0.3771]],
       dtype=torch.float64)
	q_value: tensor([[-3.8036]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8723489937453313, distance: 0.40885453056945276 entropy 0.03264415264129639
epoch: 19, step: 34
	action: tensor([[ 0.7238,  0.4136, -0.4958, -0.6594, -0.4299,  0.0971, -0.1219]],
       dtype=torch.float64)
	q_value: tensor([[-3.1634]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8128718590629481, distance: 0.4950239882108131 entropy 0.03264415264129639
epoch: 19, step: 35
	action: tensor([[ 1.1056,  0.3962, -0.3565, -0.3774, -0.0971,  0.3301, -0.3129]],
       dtype=torch.float64)
	q_value: tensor([[-4.5215]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8503487996402108, distance: 0.442687027513817 entropy 0.03264415264129639
epoch: 19, step: 36
	action: tensor([[ 0.4441,  0.1049, -0.5794, -0.6719, -0.0613, -0.0783, -0.2519]],
       dtype=torch.float64)
	q_value: tensor([[-5.0004]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5434458826810403, distance: 0.7732195481748454 entropy 0.03264415264129639
epoch: 19, step: 37
	action: tensor([[ 0.8410,  0.3092, -0.0506, -0.3720,  0.5107,  0.1189, -0.4312]],
       dtype=torch.float64)
	q_value: tensor([[-3.3499]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8221628039294702, distance: 0.48257852805550727 entropy 0.03264415264129639
epoch: 19, step: 38
	action: tensor([[ 0.2234,  0.4255, -0.6665, -0.6195, -0.1005,  0.2991, -0.2963]],
       dtype=torch.float64)
	q_value: tensor([[-4.2865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7585294715672503, distance: 0.5623267707824942 entropy 0.03264415264129639
epoch: 19, step: 39
	action: tensor([[-0.0982,  0.2625, -0.4884, -0.6714,  0.1277,  0.0762, -0.4492]],
       dtype=torch.float64)
	q_value: tensor([[-3.2658]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3558765645845404, distance: 0.9184198117210142 entropy 0.03264415264129639
epoch: 19, step: 40
	action: tensor([[ 0.8452,  0.1545, -0.6307,  0.0423,  0.3750,  0.4485, -0.4609]],
       dtype=torch.float64)
	q_value: tensor([[-2.5802]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9217672856924429, distance: 0.32007436496453706 entropy 0.03264415264129639
epoch: 19, step: 41
	action: tensor([[ 0.6052,  0.3678, -0.2820, -0.7951,  0.2097, -0.0905, -0.7485]],
       dtype=torch.float64)
	q_value: tensor([[-4.2320]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6640678894300001, distance: 0.6632579320718875 entropy 0.03264415264129639
epoch: 19, step: 42
	action: tensor([[ 0.6453,  0.3124, -0.3700, -0.0826,  0.3709, -0.2649, -0.2020]],
       dtype=torch.float64)
	q_value: tensor([[-4.0476]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8537449404148468, distance: 0.4376350960020457 entropy 0.03264415264129639
epoch: 19, step: 43
	action: tensor([[ 0.6701,  0.5647, -0.2690,  0.1092, -0.0267, -0.3025, -0.4783]],
       dtype=torch.float64)
	q_value: tensor([[-4.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.973730229968353, distance: 0.1854747646785329 entropy 0.03264415264129639
epoch: 19, step: 44
	action: tensor([[ 0.2162,  0.2973, -0.5188, -0.1601, -0.4739, -0.1373, -0.1576]],
       dtype=torch.float64)
	q_value: tensor([[-4.4104]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6955936636900377, distance: 0.6313694113929511 entropy 0.03264415264129639
epoch: 19, step: 45
	action: tensor([[ 0.6567,  0.7770, -0.2853, -0.6430, -0.1578,  0.3479, -0.6774]],
       dtype=torch.float64)
	q_value: tensor([[-3.3653]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9745829743483587, distance: 0.18243957710921144 entropy 0.03264415264129639
epoch: 19, step: 46
	action: tensor([[ 0.1269,  0.1583, -0.5644, -0.8722,  0.2395,  0.0552, -0.3559]],
       dtype=torch.float64)
	q_value: tensor([[-4.6142]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.43662651169211697, distance: 0.8589243128843057 entropy 0.03264415264129639
epoch: 19, step: 47
	action: tensor([[ 0.3271,  0.3007, -0.5792, -0.2421, -0.0714,  0.2002,  0.0839]],
       dtype=torch.float64)
	q_value: tensor([[-2.9344]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7585268441437393, distance: 0.5623298300928146 entropy 0.03264415264129639
epoch: 19, step: 48
	action: tensor([[ 0.7706,  0.3418, -0.2802, -0.5247, -0.2017,  0.1803, -0.1860]],
       dtype=torch.float64)
	q_value: tensor([[-3.2477]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8094197990250652, distance: 0.4995691179255921 entropy 0.03264415264129639
epoch: 19, step: 49
	action: tensor([[ 0.5193,  0.7644, -0.4966, -0.3921,  0.2568,  0.0327,  0.2978]],
       dtype=torch.float64)
	q_value: tensor([[-4.1953]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9698297058997382, distance: 0.19876799873810488 entropy 0.03264415264129639
epoch: 19, step: 50
	action: tensor([[ 0.4612,  0.0817, -0.3642, -0.6640,  0.4519,  0.2156, -0.4399]],
       dtype=torch.float64)
	q_value: tensor([[-4.1728]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5377599869441658, distance: 0.7780194633236018 entropy 0.03264415264129639
epoch: 19, step: 51
	action: tensor([[ 0.2570,  0.4335, -0.5807, -0.2925, -0.0064,  0.0380, -0.1295]],
       dtype=torch.float64)
	q_value: tensor([[-3.1748]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7811406299999528, distance: 0.5353517980414464 entropy 0.03264415264129639
epoch: 19, step: 52
	action: tensor([[ 0.5432, -0.0964, -0.4365, -0.1069,  0.1666,  0.5336,  0.2708]],
       dtype=torch.float64)
	q_value: tensor([[-3.0118]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7013264767202119, distance: 0.6253959375477027 entropy 0.03264415264129639
epoch: 19, step: 53
	action: tensor([[ 0.5826,  0.6105, -0.6172, -0.6050,  0.2657, -0.1586, -0.4017]],
       dtype=torch.float64)
	q_value: tensor([[-3.8442]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9036109192392001, distance: 0.35527987922903054 entropy 0.03264415264129639
epoch: 19, step: 54
	action: tensor([[ 0.2021,  0.7060, -0.5537, -0.0723,  0.2274, -0.2424, -0.1471]],
       dtype=torch.float64)
	q_value: tensor([[-4.0153]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8155049209494715, distance: 0.49152893279183235 entropy 0.03264415264129639
epoch: 19, step: 55
	action: tensor([[ 0.4276,  0.0034, -0.6448, -0.4794,  0.1704, -0.0035, -0.1155]],
       dtype=torch.float64)
	q_value: tensor([[-3.3648]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5354947954273359, distance: 0.7799234625583482 entropy 0.03264415264129639
epoch: 19, step: 56
	action: tensor([[ 0.6058,  0.5824, -0.3823, -0.0506,  0.0567,  0.5344,  0.1469]],
       dtype=torch.float64)
	q_value: tensor([[-3.2271]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9526724488640612, distance: 0.24895090533496628 entropy 0.03264415264129639
epoch: 19, step: 57
	action: tensor([[ 0.3932,  0.0083, -0.5742, -0.1976,  0.0758,  0.2588, -0.4407]],
       dtype=torch.float64)
	q_value: tensor([[-4.1299]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6355929083231671, distance: 0.6907964599745167 entropy 0.03264415264129639
epoch: 19, step: 58
	action: tensor([[ 0.3020,  0.4391, -0.1399, -0.4845,  0.0503,  0.0815, -0.2268]],
       dtype=torch.float64)
	q_value: tensor([[-2.8810]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7142432428307619, distance: 0.6117231942851247 entropy 0.03264415264129639
epoch: 19, step: 59
	action: tensor([[ 0.2648,  0.3628, -0.5125, -0.8406,  0.0984,  0.1657, -0.4049]],
       dtype=torch.float64)
	q_value: tensor([[-3.1999]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6616951013605068, distance: 0.6655962040444612 entropy 0.03264415264129639
epoch: 19, step: 60
	action: tensor([[ 0.3600,  0.4520, -0.2845, -0.6900,  0.4164,  0.2170, -0.3498]],
       dtype=torch.float64)
	q_value: tensor([[-3.2321]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7558069077613603, distance: 0.5654879832165751 entropy 0.03264415264129639
epoch: 19, step: 61
	action: tensor([[ 0.5746,  0.3069, -0.0802, -0.4588,  0.1121, -0.0860,  0.1138]],
       dtype=torch.float64)
	q_value: tensor([[-3.2536]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7122735315715818, distance: 0.6138278668679066 entropy 0.03264415264129639
epoch: 19, step: 62
	action: tensor([[ 0.2667,  0.0836, -0.2481, -0.4477, -0.1155,  0.0722, -0.2671]],
       dtype=torch.float64)
	q_value: tensor([[-3.9018]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.46530028596397255, distance: 0.836780727688854 entropy 0.03264415264129639
epoch: 19, step: 63
	action: tensor([[ 0.2859,  0.2179,  0.1709, -0.6663, -0.1771,  0.1210, -0.2956]],
       dtype=torch.float64)
	q_value: tensor([[-2.9878]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4137999170254777, distance: 0.8761523518235013 entropy 0.03264415264129639
epoch: 19, step: 64
	action: tensor([[ 0.7838,  0.6913, -0.3658, -0.4673, -0.2273, -0.1342, -0.1807]],
       dtype=torch.float64)
	q_value: tensor([[-3.7509]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9478104215697118, distance: 0.261425885458046 entropy 0.03264415264129639
epoch: 19, step: 65
	action: tensor([[ 0.7905,  0.5388,  0.0308, -0.2848,  0.1424,  0.4880, -0.2242]],
       dtype=torch.float64)
	q_value: tensor([[-4.5821]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9810503541418301, distance: 0.1575278125153314 entropy 0.03264415264129639
epoch: 19, step: 66
	action: tensor([[ 0.8825,  0.2955, -0.2993, -1.0146, -0.0991,  0.0417, -0.4956]],
       dtype=torch.float64)
	q_value: tensor([[-4.4183]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5093437252092936, distance: 0.8015772299372989 entropy 0.03264415264129639
epoch: 19, step: 67
	action: tensor([[ 0.3655,  0.4106, -0.2871, -0.3732,  0.4960,  0.2021,  0.0215]],
       dtype=torch.float64)
	q_value: tensor([[-4.7008]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.804890974067444, distance: 0.5054699867694016 entropy 0.03264415264129639
epoch: 19, step: 68
	action: tensor([[ 0.3597,  0.1456, -0.5894, -0.5857,  0.1432,  0.2632,  0.0629]],
       dtype=torch.float64)
	q_value: tensor([[-3.3611]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6334819178348301, distance: 0.6927944437851355 entropy 0.03264415264129639
epoch: 19, step: 69
	action: tensor([[ 0.4611,  0.1642, -0.3014, -0.7757, -0.1775,  0.1889,  0.0379]],
       dtype=torch.float64)
	q_value: tensor([[-3.3996]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5242137432803868, distance: 0.7893373254856259 entropy 0.03264415264129639
epoch: 19, step: 70
	action: tensor([[ 0.4790,  0.1590, -0.6036, -0.0899,  0.0333,  0.4000, -0.7737]],
       dtype=torch.float64)
	q_value: tensor([[-3.8941]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.791298871746365, distance: 0.5227801532393758 entropy 0.03264415264129639
epoch: 19, step: 71
	action: tensor([[ 0.8229,  0.0894, -0.5041, -0.4829,  0.2968,  0.2864, -0.8711]],
       dtype=torch.float64)
	q_value: tensor([[-3.3801]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6934098747470728, distance: 0.6336300633459467 entropy 0.03264415264129639
epoch: 19, step: 72
	action: tensor([[ 0.5950,  0.3396, -0.2445, -0.2754,  0.1742,  0.1849, -0.2327]],
       dtype=torch.float64)
	q_value: tensor([[-4.0481]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8705940776332544, distance: 0.4116553552830406 entropy 0.03264415264129639
epoch: 19, step: 73
	action: tensor([[ 0.0016,  0.1538, -0.2023, -0.3328, -0.1619, -0.1286, -0.2053]],
       dtype=torch.float64)
	q_value: tensor([[-3.5116]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3192514728394922, distance: 0.9441696769337254 entropy 0.03264415264129639
epoch: 19, step: 74
	action: tensor([[-0.0369,  0.2460, -0.4640, -0.3947, -0.1500,  0.0410, -0.1263]],
       dtype=torch.float64)
	q_value: tensor([[-2.8126]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41376791471685037, distance: 0.8761762673056992 entropy 0.03264415264129639
epoch: 19, step: 75
	action: tensor([[ 0.1959,  0.2088, -0.2117, -0.3376,  0.0920,  0.0583, -0.2461]],
       dtype=torch.float64)
	q_value: tensor([[-2.6185]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5465719053348204, distance: 0.7705678871480633 entropy 0.03264415264129639
epoch: 19, step: 76
	action: tensor([[ 0.3398, -0.1461, -0.1684, -0.0453,  0.3381, -0.0069, -0.3056]],
       dtype=torch.float64)
	q_value: tensor([[-2.7235]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4549236541006366, distance: 0.844861191158832 entropy 0.03264415264129639
epoch: 19, step: 77
	action: tensor([[ 0.2178,  0.0114, -0.4161, -0.5467,  0.2567, -0.1152, -0.3117]],
       dtype=torch.float64)
	q_value: tensor([[-3.1315]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3429660396692302, distance: 0.9275783470105945 entropy 0.03264415264129639
epoch: 19, step: 78
	action: tensor([[ 0.6664,  0.2956,  0.2959, -0.6260, -0.0629, -0.2343, -0.6096]],
       dtype=torch.float64)
	q_value: tensor([[-2.7714]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6043804840522359, distance: 0.7197729924547974 entropy 0.03264415264129639
epoch: 19, step: 79
	action: tensor([[ 0.2678,  0.1950, -0.4201, -0.2404, -0.2047, -0.1575, -0.3126]],
       dtype=torch.float64)
	q_value: tensor([[-4.7985]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6200612070759657, distance: 0.7053643605335467 entropy 0.03264415264129639
epoch: 19, step: 80
	action: tensor([[ 0.4750,  0.2331, -0.5454, -0.3563,  0.1467, -0.0288, -0.5322]],
       dtype=torch.float64)
	q_value: tensor([[-3.0339]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7482212724631541, distance: 0.5742039959008602 entropy 0.03264415264129639
epoch: 19, step: 81
	action: tensor([[ 0.7658,  0.2558,  0.4538, -0.2261,  0.5011, -0.1989, -0.1819]],
       dtype=torch.float64)
	q_value: tensor([[-3.2150]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7862891184098417, distance: 0.5290174663518489 entropy 0.03264415264129639
epoch: 19, step: 82
	action: tensor([[ 0.7002,  0.5080, -0.2142, -0.1326, -0.0192, -0.0750, -0.1551]],
       dtype=torch.float64)
	q_value: tensor([[-4.8621]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9482912507366751, distance: 0.2602188239987187 entropy 0.03264415264129639
epoch: 19, step: 83
	action: tensor([[ 0.0459,  0.3880, -0.1926, -0.7077, -0.0773,  0.0165, -0.4851]],
       dtype=torch.float64)
	q_value: tensor([[-4.0534]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4335874478487174, distance: 0.8612378890935473 entropy 0.03264415264129639
epoch: 19, step: 84
	action: tensor([[ 0.3274,  0.0906, -0.2059, -0.5609, -0.1026,  0.2060, -0.2733]],
       dtype=torch.float64)
	q_value: tensor([[-3.1623]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4788395350864155, distance: 0.8261186462338107 entropy 0.03264415264129639
epoch: 19, step: 85
	action: tensor([[ 0.7638,  0.3141, -0.2035, -0.5290,  0.2815,  0.6590, -0.2794]],
       dtype=torch.float64)
	q_value: tensor([[-3.1904]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8759064138822085, distance: 0.4031172297634212 entropy 0.03264415264129639
epoch: 19, step: 86
	action: tensor([[ 0.1055,  0.1275, -0.0822, -0.6369,  0.2307, -0.1760, -0.0499]],
       dtype=torch.float64)
	q_value: tensor([[-4.1266]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20503375402843882, distance: 1.0203074030748327 entropy 0.03264415264129639
epoch: 19, step: 87
	action: tensor([[ 0.9504,  0.7833, -0.3319, -0.1083,  0.3549,  0.0878, -0.3883]],
       dtype=torch.float64)
	q_value: tensor([[-2.9960]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9744180587891629, distance: 0.18303048964823282 entropy 0.03264415264129639
epoch: 19, step: 88
	action: tensor([[ 0.8827,  0.0323, -0.4284, -0.2898,  0.2558,  0.0473, -0.3142]],
       dtype=torch.float64)
	q_value: tensor([[-4.9321]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6491808694197866, distance: 0.6777949476418891 entropy 0.03264415264129639
epoch: 19, step: 89
	action: tensor([[ 0.3865,  0.3330, -0.4479, -0.7412,  0.0948, -0.2908, -0.1532]],
       dtype=torch.float64)
	q_value: tensor([[-4.1882]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6153001846662207, distance: 0.7097700706101093 entropy 0.03264415264129639
epoch: 19, step: 90
	action: tensor([[ 0.9389,  0.4374,  0.3208, -0.4459, -0.1526,  0.2981, -0.1416]],
       dtype=torch.float64)
	q_value: tensor([[-3.5250]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.906437995236584, distance: 0.3500309548627421 entropy 0.03264415264129639
epoch: 19, step: 91
	action: tensor([[ 0.5468,  0.4120, -0.7167, -0.6348,  0.1900,  0.0329, -0.7248]],
       dtype=torch.float64)
	q_value: tensor([[-5.2474]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8449009961059075, distance: 0.45067264456647216 entropy 0.03264415264129639
epoch: 19, step: 92
	action: tensor([[ 0.0899,  0.2528, -0.8858, -0.5030,  0.0175,  0.4766,  0.0483]],
       dtype=torch.float64)
	q_value: tensor([[-3.6700]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6251331969544656, distance: 0.7006404140465853 entropy 0.03264415264129639
epoch: 19, step: 93
	action: tensor([[ 0.8291, -0.1245,  0.0508, -0.7222,  0.4222,  0.2293, -0.3629]],
       dtype=torch.float64)
	q_value: tensor([[-3.2888]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2756624673571223, distance: 0.973928753915521 entropy 0.03264415264129639
epoch: 19, step: 94
	action: tensor([[ 7.9008e-01,  8.1526e-01, -3.3901e-02, -1.8372e-01,  1.7465e-04,
          2.2672e-02, -2.9253e-01]], dtype=torch.float64)
	q_value: tensor([[-4.2762]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9841837343861172, distance: 0.14391586361590866 entropy 0.03264415264129639
epoch: 19, step: 95
	action: tensor([[ 0.5105,  0.5883, -0.5534, -0.0872,  0.0382,  0.0568, -0.5839]],
       dtype=torch.float64)
	q_value: tensor([[-4.7220]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9372473300409897, distance: 0.2866637630511312 entropy 0.03264415264129639
epoch: 19, step: 96
	action: tensor([[ 0.1679,  0.4352, -0.1444, -0.4276,  0.1834,  0.5698, -0.4487]],
       dtype=torch.float64)
	q_value: tensor([[-3.6120]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7261289846743231, distance: 0.5988660993397484 entropy 0.03264415264129639
epoch: 19, step: 97
	action: tensor([[ 0.8156,  0.4990, -0.0220, -0.5983, -0.0805,  0.0516, -0.4899]],
       dtype=torch.float64)
	q_value: tensor([[-3.1847]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8531139277887418, distance: 0.4385781609779761 entropy 0.03264415264129639
epoch: 19, step: 98
	action: tensor([[ 0.8378,  0.3526, -0.0220, -0.1825, -0.2890,  0.0467, -0.1894]],
       dtype=torch.float64)
	q_value: tensor([[-4.6016]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9051681187351602, distance: 0.35239835815141557 entropy 0.03264415264129639
epoch: 19, step: 99
	action: tensor([[ 0.4620,  0.3292, -0.0302, -0.6415,  0.2454,  0.2809, -0.2871]],
       dtype=torch.float64)
	q_value: tensor([[-4.4849]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.696359066865915, distance: 0.6305751501517991 entropy 0.03264415264129639
epoch: 19, step: 100
	action: tensor([[ 0.6119,  0.6593, -0.3437, -0.4926, -0.0397,  0.3829, -0.4841]],
       dtype=torch.float64)
	q_value: tensor([[-3.5019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9631185893072787, distance: 0.21976597560718472 entropy 0.03264415264129639
epoch: 19, step: 101
	action: tensor([[ 0.6407,  0.6831, -0.3089, -0.8449, -0.0036,  0.0354, -0.0269]],
       dtype=torch.float64)
	q_value: tensor([[-4.0927]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8791907502057102, distance: 0.3977468848933624 entropy 0.03264415264129639
epoch: 19, step: 102
	action: tensor([[ 0.2720,  0.4758, -0.5444, -0.7858, -0.0040,  0.3058, -0.3010]],
       dtype=torch.float64)
	q_value: tensor([[-4.5315]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7712446475116778, distance: 0.547321269437398 entropy 0.03264415264129639
epoch: 19, step: 103
	action: tensor([[ 0.7614,  0.4259, -0.0434, -0.4349, -0.0319,  0.2331, -0.4100]],
       dtype=torch.float64)
	q_value: tensor([[-3.4461]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.890370584659574, distance: 0.378896193517148 entropy 0.03264415264129639
epoch: 19, step: 104
	action: tensor([[ 0.8070,  0.0282, -0.6187, -0.2680, -0.1166,  0.3716, -0.8122]],
       dtype=torch.float64)
	q_value: tensor([[-4.2128]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.752588404676422, distance: 0.5692023941106066 entropy 0.03264415264129639
epoch: 19, step: 105
	action: tensor([[ 0.3115,  0.0680, -0.4143, -0.6533,  0.0094, -0.1875, -0.1689]],
       dtype=torch.float64)
	q_value: tensor([[-4.1188]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.40145225293312414, distance: 0.8853318605242312 entropy 0.03264415264129639
epoch: 19, step: 106
	action: tensor([[ 0.1119, -0.1799, -0.4817, -0.7030,  0.2380,  0.2022, -0.6923]],
       dtype=torch.float64)
	q_value: tensor([[-3.1695]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15765574477652688, distance: 1.0502713107282082 entropy 0.03264415264129639
epoch: 19, step: 107
	action: tensor([[ 0.2330,  0.4239, -0.1061, -0.3987, -0.1159,  0.2391, -0.6182]],
       dtype=torch.float64)
	q_value: tensor([[-2.7055]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.672866995980338, distance: 0.6545138999740305 entropy 0.03264415264129639
epoch: 19, step: 108
	action: tensor([[ 0.5879, -0.2460, -0.0387, -0.6739, -0.0220, -0.0612, -0.3926]],
       dtype=torch.float64)
	q_value: tensor([[-3.3725]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07182462791313593, distance: 1.1024825232264357 entropy 0.03264415264129639
epoch: 19, step: 109
	action: tensor([[ 0.2125,  0.4057, -0.3434, -0.4472,  0.0288,  0.1454, -0.2540]],
       dtype=torch.float64)
	q_value: tensor([[-3.7522]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6816125740808984, distance: 0.6457057401321422 entropy 0.03264415264129639
epoch: 19, step: 110
	action: tensor([[ 0.7298,  0.3794,  0.0041, -0.5142,  0.2386,  0.2670, -0.6776]],
       dtype=torch.float64)
	q_value: tensor([[-2.9045]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8338585637179482, distance: 0.4664398785154952 entropy 0.03264415264129639
epoch: 19, step: 111
	action: tensor([[ 0.4111,  0.0525, -0.5128, -0.4794, -0.2460, -0.0197, -0.3773]],
       dtype=torch.float64)
	q_value: tensor([[-4.2184]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5447657262247181, distance: 0.7721010964792048 entropy 0.03264415264129639
epoch: 19, step: 112
	action: tensor([[ 5.6163e-01,  3.1742e-01, -1.4447e-01, -6.5457e-01,  3.5171e-06,
         -3.8972e-01, -3.1924e-01]], dtype=torch.float64)
	q_value: tensor([[-3.2313]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5939923826345508, distance: 0.7291615914296615 entropy 0.03264415264129639
epoch: 19, step: 113
	action: tensor([[ 0.4675,  0.2031,  0.1622, -0.1404,  0.0696,  0.2344, -0.2125]],
       dtype=torch.float64)
	q_value: tensor([[-4.0727]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7877457123648215, distance: 0.5272115659729244 entropy 0.03264415264129639
epoch: 19, step: 114
	action: tensor([[ 0.4004,  0.2070, -0.2783, -0.0368,  0.0534,  0.4642, -0.2955]],
       dtype=torch.float64)
	q_value: tensor([[-3.6306]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8202835490668068, distance: 0.48512159780398423 entropy 0.03264415264129639
epoch: 19, step: 115
	action: tensor([[ 0.4380, -0.3540, -0.0648, -0.1076,  0.6680,  0.2413, -0.8234]],
       dtype=torch.float64)
	q_value: tensor([[-3.1528]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35853587573083634, distance: 0.9165219688579979 entropy 0.03264415264129639
epoch: 19, step: 116
	action: tensor([[ 0.6694,  0.4097, -0.6843,  0.1214, -0.0534,  0.5980, -0.6429]],
       dtype=torch.float64)
	q_value: tensor([[-3.8583]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9277173163800532, distance: 0.30766197763113795 entropy 0.03264415264129639
epoch: 19, step: 117
	action: tensor([[ 0.2712,  0.4064, -0.2839, -0.4461, -0.1523,  0.0797, -0.3950]],
       dtype=torch.float64)
	q_value: tensor([[-4.1868]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7098835947284274, distance: 0.6163719082278879 entropy 0.03264415264129639
epoch: 19, step: 118
	action: tensor([[ 0.3552,  0.1717,  0.1096, -0.8090, -0.3026,  0.0140, -0.5737]],
       dtype=torch.float64)
	q_value: tensor([[-3.1913]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3370729403952435, distance: 0.9317289001801766 entropy 0.03264415264129639
epoch: 19, step: 119
	action: tensor([[ 0.1503,  0.2687, -0.3945, -0.1850,  0.3397,  0.0441, -0.4055]],
       dtype=torch.float64)
	q_value: tensor([[-4.0804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5968621084126492, distance: 0.7265801072776925 entropy 0.03264415264129639
epoch: 19, step: 120
	action: tensor([[ 9.9829e-01,  8.5333e-04, -2.9720e-01, -2.6917e-01, -1.8354e-01,
          7.6948e-03, -3.4963e-01]], dtype=torch.float64)
	q_value: tensor([[-2.6098]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5826707095818656, distance: 0.7392581595178522 entropy 0.03264415264129639
epoch: 19, step: 121
	action: tensor([[ 0.1347,  0.2257, -0.2451, -0.4046,  0.0743,  0.0116,  0.0489]],
       dtype=torch.float64)
	q_value: tensor([[-4.6096]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.48609716530546554, distance: 0.8203462551614408 entropy 0.03264415264129639
epoch: 19, step: 122
	action: tensor([[ 0.6179,  0.4719, -0.6555, -0.1693, -0.1231, -0.0299, -0.3583]],
       dtype=torch.float64)
	q_value: tensor([[-2.8506]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9290698982397876, distance: 0.3047698383722718 entropy 0.03264415264129639
epoch: 19, step: 123
	action: tensor([[ 0.9463,  0.6374, -0.4873, -0.0486, -0.0083,  0.2563, -0.3271]],
       dtype=torch.float64)
	q_value: tensor([[-3.7916]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9862484267047833, distance: 0.13419393476550948 entropy 0.03264415264129639
epoch: 19, step: 124
	action: tensor([[ 0.5786,  0.5351,  0.2100, -0.2841, -0.2731,  0.1655, -0.4200]],
       dtype=torch.float64)
	q_value: tensor([[-4.6578]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9152091634613168, distance: 0.3332200715241833 entropy 0.03264415264129639
epoch: 19, step: 125
	action: tensor([[ 0.6486,  0.1766,  0.1874, -0.3473, -0.3509,  0.1160, -0.2204]],
       dtype=torch.float64)
	q_value: tensor([[-4.6183]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7227670361029979, distance: 0.6025306270320735 entropy 0.03264415264129639
epoch: 19, step: 126
	action: tensor([[ 0.3716,  0.0403, -0.3513, -0.4009,  0.3983,  0.4366, -0.3381]],
       dtype=torch.float64)
	q_value: tensor([[-4.3834]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6279880401974305, distance: 0.6979674094858133 entropy 0.03264415264129639
epoch: 19, step: 127
	action: tensor([[ 0.4905,  0.5500, -0.1395, -0.4650, -0.1135, -0.5192, -0.4889]],
       dtype=torch.float64)
	q_value: tensor([[-2.9561]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.805362908042855, distance: 0.5048582957278239 entropy 0.03264415264129639
LOSS epoch 19 actor 60.29454284697498 critic 476.5848871087845 
epoch: 20, step: 0
	action: tensor([[ 1.4701,  0.6491, -1.3501, -0.9598, -0.4124, -0.0492, -0.7546]],
       dtype=torch.float64)
	q_value: tensor([[-3.4624]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2152499265244333, distance: 1.013730179106056 entropy 0.03264415264129639
epoch: 20, step: 1
	action: tensor([[ 2.8294,  1.1018, -1.7420, -1.6020,  0.1256,  0.2128, -1.4004]],
       dtype=torch.float64)
	q_value: tensor([[-5.5176]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 2
	action: tensor([[ 2.7435,  1.6489, -2.1881, -1.2465, -0.3802,  0.5691, -1.1546]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 3
	action: tensor([[ 2.7089,  1.7305, -1.4449, -1.7544, -0.1839,  0.7259, -1.5855]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 4
	action: tensor([[ 2.6667,  1.5206, -2.1333, -2.1213, -0.4673,  0.7622, -1.4260]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 5
	action: tensor([[ 2.4561,  1.4645, -1.6206, -1.2310,  0.4746,  0.7399, -1.3896]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 6
	action: tensor([[ 3.2742,  1.4682, -1.8829, -1.8087, -0.1346,  0.7312, -1.4266]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 7
	action: tensor([[ 2.8220,  1.0783, -1.4554, -1.6621, -0.3233,  0.4820, -1.0431]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 8
	action: tensor([[ 2.7753,  1.8870, -1.4733, -1.8695, -0.0180,  0.4574, -1.2121]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 9
	action: tensor([[ 2.8655,  1.4553, -1.8318, -1.6688,  0.1865,  0.3035, -0.9263]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 10
	action: tensor([[ 3.0146,  1.4959, -1.7785, -1.6465, -0.3918,  0.5653, -0.7794]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 11
	action: tensor([[ 3.3163,  2.1012, -2.2090, -1.8890,  0.0118,  0.0699, -1.1440]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 12
	action: tensor([[ 2.9569,  1.3783, -1.8646, -1.3986, -0.1470,  0.6828, -0.9401]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 13
	action: tensor([[ 3.0690,  1.4829, -1.7120, -1.8945,  0.0107,  0.8889, -1.2137]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 14
	action: tensor([[ 3.0084,  1.4049, -1.8146, -1.7049, -0.4291,  0.8525, -1.1516]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 15
	action: tensor([[ 3.2452,  1.4135, -1.4685, -1.7809, -0.0803,  0.6174, -1.4901]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 16
	action: tensor([[ 2.6634,  1.6472, -1.4857, -1.5595, -0.0727,  0.1156, -1.1292]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 17
	action: tensor([[ 3.1681,  1.2570, -1.8532, -2.4084, -0.2235,  0.2742, -1.3086]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 18
	action: tensor([[ 2.9126,  1.4330, -1.8364, -1.5768, -0.4926,  0.4504, -1.1313]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 19
	action: tensor([[ 2.4671,  1.1417, -1.7626, -1.8027, -0.6678,  0.5430, -1.0652]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 20
	action: tensor([[ 2.8071,  1.0657, -1.5501, -2.2018, -0.1155,  0.7656, -1.3673]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 21
	action: tensor([[ 3.0654,  1.8153, -1.9139, -1.6468, -0.4221,  0.8097, -1.0899]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 22
	action: tensor([[ 2.8963,  1.8161, -1.4236, -1.3766, -0.4056,  0.7581, -1.4913]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 23
	action: tensor([[ 2.8790,  1.5525, -1.9978, -1.6734,  0.2096,  0.5655, -1.5392]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 24
	action: tensor([[ 2.8486,  1.3609, -1.6671, -2.0758,  0.4107,  0.5564, -1.5332]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 25
	action: tensor([[ 2.2409,  1.8621, -2.3311, -2.0625,  0.3853,  0.8907, -0.9738]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 26
	action: tensor([[ 2.3786,  1.8003, -2.2791, -2.0994, -0.1278,  0.4862, -0.6746]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 27
	action: tensor([[ 3.1059,  1.4755, -1.7263, -1.4251, -0.0209,  0.3642, -1.0763]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 28
	action: tensor([[ 3.2400,  1.3984, -1.9451, -2.0787, -0.1136,  0.5321, -1.4831]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 29
	action: tensor([[ 2.7438,  1.3826, -1.8142, -1.7805, -0.3748,  0.6283, -0.8882]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 30
	action: tensor([[ 2.8911,  1.2986, -1.8460, -1.4961, -0.1668,  0.4580, -1.6360]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 31
	action: tensor([[ 3.1862,  1.5911, -1.7228, -1.4256,  0.0919,  0.3260, -1.1756]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 32
	action: tensor([[ 2.5854,  1.3973, -1.8080, -1.8742,  0.6586,  0.4536, -0.8239]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 33
	action: tensor([[ 2.9245,  1.1027, -1.5837, -1.6041, -0.2395,  0.5538, -1.0852]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 34
	action: tensor([[ 2.8274,  1.6691, -1.7366, -1.6666,  0.3579,  0.6077, -1.5051]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 35
	action: tensor([[ 2.6728,  1.5256, -2.0072, -1.7764, -0.1952,  0.1382, -0.9707]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 36
	action: tensor([[ 2.9238,  1.8421, -2.1236, -1.8045,  0.0758,  0.2481, -1.0501]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 37
	action: tensor([[ 2.7483,  1.4805, -1.7513, -2.1213,  0.4076,  0.4649, -1.4668]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 38
	action: tensor([[ 2.9038,  0.8868, -1.2434, -2.3144, -0.2899,  0.4713, -1.7454]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 39
	action: tensor([[ 2.8155,  1.7367, -1.3478, -1.8682, -0.2013,  0.6650, -1.3999]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 40
	action: tensor([[ 3.1471,  1.4811, -1.9231, -2.2603, -0.6791,  0.1871, -1.3693]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 41
	action: tensor([[ 2.6476,  1.5732, -2.2255, -2.0407,  0.0039,  0.3295, -1.0218]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 42
	action: tensor([[ 2.9724,  1.5003, -1.7782, -2.0310, -0.0776,  0.5862, -1.3262]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 43
	action: tensor([[ 2.7363,  1.8334, -2.0181, -1.4157,  0.0292,  0.4913, -1.4772]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 44
	action: tensor([[ 2.5106,  1.4841, -1.6063, -1.8571,  0.1072, -0.0986, -1.6060]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 45
	action: tensor([[ 2.7368,  1.8823, -1.7302, -1.8722,  0.2736,  0.5233, -1.5288]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 46
	action: tensor([[ 3.3615,  1.3485, -1.4746, -1.6750, -0.0877,  0.2172, -1.3650]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 47
	action: tensor([[ 2.5298,  1.6036, -1.5296, -1.5827,  0.2077,  0.3465, -0.8644]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 48
	action: tensor([[ 3.2512,  1.7414, -1.9391, -1.8998, -0.1793,  0.6358, -1.4203]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 49
	action: tensor([[ 2.9410,  1.4646, -1.4124, -1.6643, -0.4553,  0.1032, -1.2030]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 50
	action: tensor([[ 2.3955,  1.5834, -1.5823, -1.9525,  0.0089,  0.7863, -1.4304]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17575507636087528, distance: 1.2408381127882058 entropy 0.03264415264129639
epoch: 20, step: 51
	action: tensor([[ 3.6944,  1.7671, -2.3078, -2.6050, -0.2374,  0.4994, -1.7484]],
       dtype=torch.float64)
	q_value: tensor([[-7.2213]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 52
	action: tensor([[ 2.7685,  1.5239, -1.6025, -1.6339, -0.2973,  0.2738, -1.0797]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 53
	action: tensor([[ 3.1493,  1.4424, -1.5324, -1.4186, -0.1251,  0.8910, -1.5248]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 54
	action: tensor([[ 2.6511,  1.2117, -1.3441, -1.3856,  0.0113,  0.1866, -1.2094]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 55
	action: tensor([[ 2.8297,  1.1865, -1.7731, -1.7591,  0.1563,  0.7309, -1.1512]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 56
	action: tensor([[ 2.8784,  1.8908, -1.7683, -1.7110,  0.4168,  0.4515, -1.3293]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 57
	action: tensor([[ 2.7161,  1.6138, -1.7993, -2.3185,  0.0733,  0.8375, -0.7940]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 58
	action: tensor([[ 2.4885,  1.3344, -1.7844, -1.9846, -0.0528,  0.8704, -1.3333]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 59
	action: tensor([[ 2.5611,  1.7445, -2.0948, -1.7354,  0.1730,  0.2014, -1.3055]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 60
	action: tensor([[ 2.4595,  1.3765, -1.7035, -1.8130, -0.6333,  0.1519, -1.0712]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 61
	action: tensor([[ 2.9417,  1.9279, -1.5192, -1.3123, -0.2145,  0.1708, -0.7979]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 62
	action: tensor([[ 2.6560,  1.3082, -2.1077, -1.5026, -0.3144,  0.1079, -1.1220]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 63
	action: tensor([[ 2.8424,  1.4077, -1.5844, -1.6411, -0.1903,  0.4928, -1.1298]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 64
	action: tensor([[ 2.3794,  1.8282, -1.2882, -1.5033,  0.4123,  0.4562, -0.6447]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 65
	action: tensor([[ 2.7693,  1.6647, -1.4407, -1.7996, -0.1333,  0.8587, -1.4058]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 66
	action: tensor([[ 2.7761,  1.1127, -1.8616, -1.6770,  0.3067,  0.3758, -0.8769]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 67
	action: tensor([[ 2.6202,  1.3493, -1.9465, -1.6310, -0.1717,  0.3956, -1.0980]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 68
	action: tensor([[ 3.0338,  2.0519, -1.8373, -1.9207, -0.1509,  0.8289, -1.1515]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 69
	action: tensor([[ 2.9664,  1.3921, -1.7005, -1.7105, -0.2968,  0.1406, -1.5724]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 70
	action: tensor([[ 2.4012,  1.5136, -1.6334, -1.8753, -0.2618, -0.1460, -1.2010]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 71
	action: tensor([[ 2.7275,  1.7012, -2.0748, -1.8248, -0.0663,  0.3485, -1.2456]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 72
	action: tensor([[ 2.2131,  1.8317, -2.0142, -1.4591,  0.2781,  0.2343, -1.3021]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 73
	action: tensor([[ 2.8562,  1.3568, -2.3016, -1.5482,  0.2577,  0.1364, -1.5669]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 74
	action: tensor([[ 2.6123,  1.2967, -1.8052, -2.0605,  0.2413,  0.1725, -1.3807]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 75
	action: tensor([[ 2.5766,  1.1966, -2.2896, -1.6233, -0.2636,  0.5597, -1.0155]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 76
	action: tensor([[ 2.8573,  1.3935, -1.1458, -1.1939,  0.3741,  0.3700, -1.5750]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 77
	action: tensor([[ 2.7442,  1.3922, -2.2437, -2.1409, -0.0812, -0.0448, -1.5258]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 78
	action: tensor([[ 2.9411,  1.3099, -2.0858, -1.7175, -0.2500,  0.5136, -1.1408]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 79
	action: tensor([[ 2.6943,  1.1717, -1.5663, -1.6940,  0.1961,  0.7206, -1.2641]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 80
	action: tensor([[ 2.8961,  1.6224, -1.8138, -1.4243, -0.0712,  0.7828, -1.1845]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 81
	action: tensor([[ 2.9735,  1.9851, -1.7891, -1.5763, -0.1583,  0.7206, -1.2041]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 82
	action: tensor([[ 2.8161,  1.2933, -1.5174, -1.7366, -0.9282,  0.2677, -1.2528]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 83
	action: tensor([[ 3.0559,  1.3444, -1.6826, -1.4481, -0.3735,  0.2368, -1.7407]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 84
	action: tensor([[ 2.6297,  1.5788, -1.8333, -1.9456,  0.1181,  0.2569, -0.9144]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 85
	action: tensor([[ 2.5591,  1.5233, -1.7507, -1.5981,  0.0519,  0.2551, -0.9627]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 86
	action: tensor([[ 2.7527,  1.5918, -1.7776, -1.9586,  0.1824,  0.3931, -0.9150]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 87
	action: tensor([[ 3.2283,  1.4373, -1.5641, -1.6662,  0.2640,  0.5984, -1.0333]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 88
	action: tensor([[ 2.7969,  0.8751, -1.7600, -2.1310, -0.2963,  0.5059, -1.0781]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 89
	action: tensor([[ 2.6350,  1.6905, -2.0994, -1.8806,  0.1800,  0.5480, -1.4588]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 90
	action: tensor([[ 2.7239,  1.1801, -1.8548, -1.7590,  0.3401,  0.3336, -1.3503]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 91
	action: tensor([[ 3.3341,  1.5113, -2.0660, -2.1580, -0.2607,  0.4277, -1.4991]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 92
	action: tensor([[ 2.4558,  1.6943, -1.7097, -2.0536, -0.1369, -0.0457, -1.3514]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 93
	action: tensor([[ 2.6795,  1.3667, -1.5559, -1.0645, -0.1810,  0.3389, -1.4966]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 94
	action: tensor([[ 2.2902,  1.9096, -2.4374, -1.8438, -0.2131,  0.4031, -1.3344]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 95
	action: tensor([[ 2.7616,  1.1960, -1.9496, -1.4030,  0.0625,  0.6870, -1.1296]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 96
	action: tensor([[ 3.1850,  1.1212, -2.0485, -1.7607,  0.0699,  0.3002, -1.5078]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 97
	action: tensor([[ 2.7032,  1.9361, -1.7570, -1.9143, -0.0380,  0.5367, -1.2552]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 98
	action: tensor([[ 2.3332,  1.4668, -1.7465, -1.5321, -0.2904,  0.5408, -1.1551]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16499283241183194, distance: 1.2351460674187849 entropy 0.03264415264129639
epoch: 20, step: 99
	action: tensor([[ 3.2838,  2.0058, -2.4653, -2.2084,  0.5640,  0.6284, -1.6182]],
       dtype=torch.float64)
	q_value: tensor([[-6.9633]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 100
	action: tensor([[ 2.5688,  1.4373, -1.6126, -2.0247, -0.6792,  0.5771, -1.1289]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 101
	action: tensor([[ 2.3053,  1.6159, -1.6153, -2.1184, -0.2457,  0.2536, -1.3182]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.26556275146467323, distance: 1.2873556259146073 entropy 0.03264415264129639
epoch: 20, step: 102
	action: tensor([[ 4.0158,  2.2237, -2.0423, -2.2328, -0.0250,  0.5555, -1.5580]],
       dtype=torch.float64)
	q_value: tensor([[-7.4119]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 103
	action: tensor([[ 2.6378,  1.6812, -1.9206, -1.9600, -0.0829,  0.4184, -1.2883]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 104
	action: tensor([[ 3.1510,  1.0184, -2.0435, -1.8640,  0.1936,  0.6455, -1.1654]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 105
	action: tensor([[ 2.9930,  1.8079, -1.7780, -1.9079, -0.0117,  0.5171, -1.0385]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 106
	action: tensor([[ 2.3907,  1.2003, -1.5342, -1.3772, -0.2341,  0.6662, -1.2917]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08060427821890725, distance: 1.0972559196638831 entropy 0.03264415264129639
epoch: 20, step: 107
	action: tensor([[ 3.5290,  2.1981, -2.3980, -2.4407, -0.0906,  0.2160, -1.6011]],
       dtype=torch.float64)
	q_value: tensor([[-6.5787]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 108
	action: tensor([[ 2.8457,  1.4576, -1.3335, -1.6927,  0.1974,  1.0175, -1.0588]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 109
	action: tensor([[ 2.5397,  1.1543, -1.7278, -1.8975,  0.2867,  0.6416, -1.2499]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 110
	action: tensor([[ 2.8319,  1.5218, -1.6290, -1.9145, -0.2536,  0.3387, -0.7408]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 111
	action: tensor([[ 2.4865,  0.9768, -1.6469, -1.7489, -0.1312, -0.0396, -0.8579]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 112
	action: tensor([[ 2.7901,  1.3859, -2.0519, -1.8342, -0.1741,  0.4105, -1.1597]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 113
	action: tensor([[ 2.9607,  1.5148, -1.8736, -1.9717,  0.0539,  0.7522, -0.6929]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 114
	action: tensor([[ 2.7725,  1.1920, -1.9538, -1.7730, -0.2521,  0.2066, -1.4153]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 115
	action: tensor([[ 2.7590,  1.5006, -2.0420, -1.9545, -0.2116,  0.6260, -1.0145]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 116
	action: tensor([[ 3.0490,  1.2030, -1.5713, -1.8914,  0.0372,  0.5509, -1.2788]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 117
	action: tensor([[ 2.5210,  1.3335, -1.8607, -2.2305,  0.0246,  0.1333, -1.0096]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 118
	action: tensor([[ 2.5826,  1.5292, -1.6525, -1.6229, -0.0897,  0.6203, -0.9189]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 119
	action: tensor([[ 2.4056,  1.7162, -1.2288, -1.8278, -0.2444,  0.2944, -1.0166]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 120
	action: tensor([[ 2.4535,  1.5414, -1.5181, -0.9952,  0.1116,  0.4407, -1.3244]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 121
	action: tensor([[ 2.7163,  1.7390, -1.6600, -1.6780, -0.0109,  0.0507, -1.3192]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 122
	action: tensor([[ 2.5621,  1.4469, -1.6261, -2.2726, -0.0252,  0.5263, -1.0215]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 123
	action: tensor([[ 2.9014,  1.4343, -1.7226, -1.8435,  0.0126,  0.1827, -1.2243]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 124
	action: tensor([[ 2.7935,  1.6686, -1.5542, -1.8518, -0.2513,  0.2793, -1.0494]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 125
	action: tensor([[ 2.7025,  1.8900, -1.8339, -2.5130, -0.1858,  0.3727, -1.2649]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 126
	action: tensor([[ 2.7853,  1.4064, -2.1317, -1.6867,  0.3574,  0.6592, -1.3331]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 20, step: 127
	action: tensor([[ 2.3350,  1.4088, -1.5896, -1.2744, -0.3082,  0.4732, -1.4841]],
       dtype=torch.float64)
	q_value: tensor([[-3.4208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.004669625645542519, distance: 1.1470129717882098 entropy 0.03264415264129639
LOSS epoch 20 actor 1025.1773825329344 critic 2131.889272485293 
epoch: 21, step: 0
	action: tensor([[ 2.7063,  1.4163, -1.7914, -1.3368, -0.8290,  0.5865, -1.7590]],
       dtype=torch.float64)
	q_value: tensor([[-7.8367]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 1
	action: tensor([[ 0.7824,  0.6083, -1.1692, -1.1864, -0.6402,  0.2773, -0.8521]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7805708948619343, distance: 0.5360481595649746 entropy 0.03264415264129639
epoch: 21, step: 2
	action: tensor([[ 2.0794,  0.7417, -1.3201, -0.8603, -0.3951,  0.8078, -1.0434]],
       dtype=torch.float64)
	q_value: tensor([[-5.1361]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 3
	action: tensor([[ 1.0688,  0.4139, -1.1292, -0.8806, -0.2623,  0.2169, -1.3419]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6603338553952735, distance: 0.6669339478456238 entropy 0.03264415264129639
epoch: 21, step: 4
	action: tensor([[ 2.0194,  1.0702, -1.7298, -0.7752, -0.7685,  0.6200, -0.9783]],
       dtype=torch.float64)
	q_value: tensor([[-4.8924]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 5
	action: tensor([[ 1.3748,  0.6384, -1.1061, -0.5743, -0.9351,  0.2996, -1.1543]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6592750982376068, distance: 0.6679725731310618 entropy 0.03264415264129639
epoch: 21, step: 6
	action: tensor([[ 2.4572,  0.6769, -1.0764, -1.0036, -0.5512,  0.9093, -1.5426]],
       dtype=torch.float64)
	q_value: tensor([[-6.2806]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 7
	action: tensor([[ 1.2917,  0.5569, -1.3561, -1.0396, -0.4131,  0.1976, -0.6154]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5345028268323713, distance: 0.7807557965874172 entropy 0.03264415264129639
epoch: 21, step: 8
	action: tensor([[ 1.5347,  0.9016, -1.4129, -0.8152, -1.0243,  0.6939, -1.2949]],
       dtype=torch.float64)
	q_value: tensor([[-5.8072]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.564421642190966, distance: 0.7552484451971959 entropy 0.03264415264129639
epoch: 21, step: 9
	action: tensor([[ 2.3339,  1.3363, -1.6857, -1.4134, -0.9740,  0.3225, -1.8108]],
       dtype=torch.float64)
	q_value: tensor([[-7.4185]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 10
	action: tensor([[ 1.6971,  0.7352, -1.1824, -0.8551, -0.8687,  0.2472, -1.0694]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2877581047262804, distance: 0.9657627519402029 entropy 0.03264415264129639
epoch: 21, step: 11
	action: tensor([[ 2.0840,  1.1763, -1.1836, -1.5243, -0.8049,  1.0153, -1.5731]],
       dtype=torch.float64)
	q_value: tensor([[-6.8327]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 12
	action: tensor([[ 1.1134,  0.7068, -1.4588, -1.2068, -0.1256,  0.5734, -1.2090]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6408318201451104, distance: 0.6858128531493441 entropy 0.03264415264129639
epoch: 21, step: 13
	action: tensor([[ 2.6351,  1.2251, -1.5486, -1.0956, -0.6334,  0.3991, -1.5891]],
       dtype=torch.float64)
	q_value: tensor([[-5.6410]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 14
	action: tensor([[ 1.3880,  0.6581, -0.9569, -1.1767, -0.5124,  0.2533, -1.0043]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34918023000843024, distance: 0.9231814338861495 entropy 0.03264415264129639
epoch: 21, step: 15
	action: tensor([[ 2.4545,  0.7430, -1.7901, -1.3977, -0.7867,  0.4807, -1.7967]],
       dtype=torch.float64)
	q_value: tensor([[-6.0862]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 16
	action: tensor([[ 1.1044,  0.7385, -0.5478, -0.9785, -0.4774,  0.0583, -1.3425]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7383617955140347, distance: 0.5853387466668754 entropy 0.03264415264129639
epoch: 21, step: 17
	action: tensor([[ 1.8902,  1.0460, -1.0183, -1.6915, -0.7315,  0.4599, -1.2797]],
       dtype=torch.float64)
	q_value: tensor([[-5.6034]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 18
	action: tensor([[ 1.3777,  0.3161, -1.4260, -0.8496, -0.6986, -0.0188, -1.2011]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3230159774805542, distance: 0.9415554527085113 entropy 0.03264415264129639
epoch: 21, step: 19
	action: tensor([[ 2.1203,  0.7619, -1.4523, -1.5217, -0.9302,  0.7867, -1.4988]],
       dtype=torch.float64)
	q_value: tensor([[-5.9728]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 20
	action: tensor([[ 2.0797,  0.6768, -0.6392, -1.0785, -0.4422,  0.4222, -1.1295]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26422004993403925, distance: 0.9815912261854501 entropy 0.03264415264129639
epoch: 21, step: 21
	action: tensor([[ 1.8327,  1.0439, -1.3237, -1.3797, -0.7358,  0.6411, -1.5394]],
       dtype=torch.float64)
	q_value: tensor([[-6.5422]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 22
	action: tensor([[ 1.9415,  0.4089, -0.9539, -1.1827, -0.7740,  0.7423, -1.1811]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14737422502882414, distance: 1.0566615923071994 entropy 0.03264415264129639
epoch: 21, step: 23
	action: tensor([[ 2.5894,  1.1478, -1.9439, -1.4146, -0.1763,  0.7464, -1.6910]],
       dtype=torch.float64)
	q_value: tensor([[-6.8266]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 24
	action: tensor([[ 1.3959,  0.6260, -1.0406, -1.1092, -0.3623,  0.1354, -1.1499]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31273490167849693, distance: 0.9486780189749265 entropy 0.03264415264129639
epoch: 21, step: 25
	action: tensor([[ 2.2604,  0.9437, -1.7203, -1.7217, -0.2162,  0.5603, -1.8742]],
       dtype=torch.float64)
	q_value: tensor([[-5.9181]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 26
	action: tensor([[ 1.3448,  1.0137, -0.5676, -0.8966, -0.1394, -0.1058, -1.4227]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.646460032736379, distance: 0.6804182473777476 entropy 0.03264415264129639
epoch: 21, step: 27
	action: tensor([[ 1.8444,  1.1044, -1.5104, -1.1938, -0.4168,  0.0046, -1.9024]],
       dtype=torch.float64)
	q_value: tensor([[-6.3508]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 28
	action: tensor([[ 1.2752,  0.5960, -1.0264, -0.4097, -0.2027,  0.2714, -0.9073]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7934144574542472, distance: 0.5201237149424389 entropy 0.03264415264129639
epoch: 21, step: 29
	action: tensor([[ 2.3701,  0.6319, -1.2186, -1.2376, -0.8457,  0.3936, -1.3526]],
       dtype=torch.float64)
	q_value: tensor([[-5.2259]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 30
	action: tensor([[ 1.2442,  0.6986, -1.3248, -0.3024, -0.7847,  0.3929, -1.2854]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8712300245489948, distance: 0.4106425987760263 entropy 0.03264415264129639
epoch: 21, step: 31
	action: tensor([[ 2.1941,  1.0921, -1.3278, -1.2303, -0.7723,  0.3301, -1.3509]],
       dtype=torch.float64)
	q_value: tensor([[-6.1355]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 32
	action: tensor([[ 1.1133,  0.6630, -1.0762, -0.9763, -0.7940,  0.3135, -1.2979]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.656550369627151, distance: 0.670638095653724 entropy 0.03264415264129639
epoch: 21, step: 33
	action: tensor([[ 1.9417,  0.9801, -1.4755, -1.3375, -0.7715,  0.6122, -1.8786]],
       dtype=torch.float64)
	q_value: tensor([[-5.8079]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 34
	action: tensor([[ 1.5663,  0.6700, -0.9607, -1.2504, -0.8998,  0.0866, -1.1311]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0848139844839424, distance: 1.0947409934827748 entropy 0.03264415264129639
epoch: 21, step: 35
	action: tensor([[ 2.4784,  1.0712, -1.4150, -0.8404, -0.9305,  0.0207, -1.1908]],
       dtype=torch.float64)
	q_value: tensor([[-6.8860]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 36
	action: tensor([[ 1.4500,  0.3848, -1.0694, -0.8227, -0.2534,  0.2429, -0.7369]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34123762142345826, distance: 0.9287976069215085 entropy 0.03264415264129639
epoch: 21, step: 37
	action: tensor([[ 1.9710,  1.3284, -1.8355, -1.2568, -0.4806,  0.8704, -1.4222]],
       dtype=torch.float64)
	q_value: tensor([[-5.5244]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 38
	action: tensor([[ 1.2585,  0.8263, -0.9199, -0.8944, -0.6558,  0.0893, -0.9888]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6209427430308749, distance: 0.7045455903079445 entropy 0.03264415264129639
epoch: 21, step: 39
	action: tensor([[ 1.8290,  0.8474, -1.0760, -1.1700, -0.2652,  0.7097, -1.4800]],
       dtype=torch.float64)
	q_value: tensor([[-5.8862]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 40
	action: tensor([[ 1.3649,  0.7168, -0.7211, -0.8555, -0.1324,  0.5240, -0.9071]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6461791795809135, distance: 0.6806884568200302 entropy 0.03264415264129639
epoch: 21, step: 41
	action: tensor([[ 2.3961,  0.5410, -1.4851, -0.8401, -0.6532,  0.0783, -1.4932]],
       dtype=torch.float64)
	q_value: tensor([[-5.6114]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 42
	action: tensor([[ 1.8900,  0.8044, -1.2346, -0.6769, -0.7150,  0.1403, -1.3892]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31279613259898476, distance: 0.948635757464503 entropy 0.03264415264129639
epoch: 21, step: 43
	action: tensor([[ 2.6462,  1.2799, -1.3015, -1.9869, -0.5819,  0.9964, -1.3616]],
       dtype=torch.float64)
	q_value: tensor([[-6.9375]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 44
	action: tensor([[ 1.5799,  0.5441, -0.7314, -1.1778, -0.3717,  0.6211, -1.0357]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2714666500717102, distance: 0.9767454838289453 entropy 0.03264415264129639
epoch: 21, step: 45
	action: tensor([[ 2.4143,  1.0447, -1.5340, -1.0075, -0.6755,  0.5047, -1.8319]],
       dtype=torch.float64)
	q_value: tensor([[-6.3682]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 46
	action: tensor([[ 1.3488,  0.6790, -1.1962, -0.7319, -0.6490,  0.3241, -1.1429]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6109051501372492, distance: 0.713812969430261 entropy 0.03264415264129639
epoch: 21, step: 47
	action: tensor([[ 2.0028,  1.0031, -1.6833, -1.4214, -0.3262,  0.1545, -1.9627]],
       dtype=torch.float64)
	q_value: tensor([[-5.9971]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 48
	action: tensor([[ 1.5031,  0.5615, -0.8671, -0.9844, -0.7134,  0.5734, -1.5094]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.40628980353332345, distance: 0.881746911393351 entropy 0.03264415264129639
epoch: 21, step: 49
	action: tensor([[ 2.2441,  1.1460, -1.3863, -1.4240, -0.1841,  0.5076, -1.8905]],
       dtype=torch.float64)
	q_value: tensor([[-6.6478]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 50
	action: tensor([[ 1.2589,  1.0215, -1.2210, -1.2530, -0.1164,  0.3205, -0.9850]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4412946297008098, distance: 0.8553583832303662 entropy 0.03264415264129639
epoch: 21, step: 51
	action: tensor([[ 2.0060,  1.1694, -1.1753, -2.0593, -0.5942,  0.8096, -1.7316]],
       dtype=torch.float64)
	q_value: tensor([[-6.1692]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 52
	action: tensor([[ 1.7670,  0.9499, -0.9430, -0.8013, -0.7199,  0.3351, -1.1060]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4399875410737534, distance: 0.8563583522050021 entropy 0.03264415264129639
epoch: 21, step: 53
	action: tensor([[ 2.5600,  1.1956, -1.5570, -1.5252, -1.0815,  0.4291, -1.7148]],
       dtype=torch.float64)
	q_value: tensor([[-6.9381]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 54
	action: tensor([[ 1.5216,  0.8819, -0.8895, -0.6794, -0.0313,  0.7870, -0.9756]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6518288336450538, distance: 0.6752321222730528 entropy 0.03264415264129639
epoch: 21, step: 55
	action: tensor([[ 2.1158,  1.3763, -1.6242, -1.6342, -0.3816,  0.5384, -1.7302]],
       dtype=torch.float64)
	q_value: tensor([[-6.2168]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 56
	action: tensor([[ 1.5157,  0.6882, -1.2182, -0.8027, -0.6430, -0.1450, -1.1249]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2901317078329795, distance: 0.964152168146916 entropy 0.03264415264129639
epoch: 21, step: 57
	action: tensor([[ 2.4155,  1.2623, -1.6046, -1.4946, -0.5979,  0.6665, -1.1405]],
       dtype=torch.float64)
	q_value: tensor([[-6.4288]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 58
	action: tensor([[ 1.6610,  0.8320, -1.0597, -1.2107, -0.3961,  0.7005, -0.9240]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3098918951428875, distance: 0.9506381900216109 entropy 0.03264415264129639
epoch: 21, step: 59
	action: tensor([[ 2.1454,  1.0771, -1.6867, -1.8321, -0.7355,  0.7982, -1.2736]],
       dtype=torch.float64)
	q_value: tensor([[-6.8781]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 60
	action: tensor([[ 1.6503,  0.5226, -1.2948, -0.6292, -0.4129,  0.0378, -1.3916]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2815275096060138, distance: 0.9699777345159576 entropy 0.03264415264129639
epoch: 21, step: 61
	action: tensor([[ 2.5480,  1.2796, -1.6043, -1.5503, -0.8930,  0.7407, -1.7090]],
       dtype=torch.float64)
	q_value: tensor([[-6.4381]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 62
	action: tensor([[ 1.5745,  1.1094, -1.1261, -1.2299, -0.8823,  0.5246, -0.9823]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42332296452936113, distance: 0.8690064939463011 entropy 0.03264415264129639
epoch: 21, step: 63
	action: tensor([[ 2.4641,  1.5355, -1.3138, -1.2650, -0.8760,  0.3665, -1.2766]],
       dtype=torch.float64)
	q_value: tensor([[-7.5567]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 64
	action: tensor([[ 0.9918,  0.2501, -0.7953, -0.6333, -0.8517, -0.0692, -0.8821]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6373172576463392, distance: 0.6891601212229125 entropy 0.03264415264129639
epoch: 21, step: 65
	action: tensor([[ 1.8639,  0.9821, -1.1392, -1.1438, -0.1395,  0.6417, -0.8747]],
       dtype=torch.float64)
	q_value: tensor([[-4.7177]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 66
	action: tensor([[ 1.4343,  0.5936, -1.0982, -0.6991, -0.3115,  0.6858, -1.1848]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6630366468442519, distance: 0.6642751847966811 entropy 0.03264415264129639
epoch: 21, step: 67
	action: tensor([[ 2.2512,  0.9002, -1.4534, -1.4044, -0.7395,  0.6144, -1.5629]],
       dtype=torch.float64)
	q_value: tensor([[-5.9618]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 68
	action: tensor([[ 1.3740,  0.7085, -1.4882, -0.5256, -0.0253,  0.8342, -0.6569]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.804144707282833, distance: 0.5064357428332931 entropy 0.03264415264129639
epoch: 21, step: 69
	action: tensor([[ 2.1743,  1.1703, -1.8269, -0.7930, -0.8650,  0.7644, -1.6617]],
       dtype=torch.float64)
	q_value: tensor([[-6.0236]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 70
	action: tensor([[ 1.5460,  0.8741, -1.0172, -0.8442, -0.3884,  0.2941, -0.7870]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4058343243362137, distance: 0.8820850733278662 entropy 0.03264415264129639
epoch: 21, step: 71
	action: tensor([[ 1.6423e+00,  1.3067e+00, -1.3946e+00, -7.2198e-01, -2.9233e-04,
          4.7020e-01, -1.3547e+00]], dtype=torch.float64)
	q_value: tensor([[-6.3783]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 72
	action: tensor([[ 1.1929,  0.7728, -1.0811, -1.1352, -0.3397,  0.4785, -0.7900]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6233111766786245, distance: 0.7023410629167541 entropy 0.03264415264129639
epoch: 21, step: 73
	action: tensor([[ 1.9986,  1.3183, -1.3249, -1.4037, -0.7944,  0.4230, -1.5687]],
       dtype=torch.float64)
	q_value: tensor([[-5.7350]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 74
	action: tensor([[ 1.4212,  0.7521, -0.7601, -0.8248, -0.5467,  0.3204, -1.4652]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5932752401271763, distance: 0.7298052764794959 entropy 0.03264415264129639
epoch: 21, step: 75
	action: tensor([[ 2.5576,  0.8872, -1.7215, -1.6096, -0.8956,  0.7054, -1.3494]],
       dtype=torch.float64)
	q_value: tensor([[-6.3693]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 76
	action: tensor([[ 1.1248,  0.5221, -1.0950, -0.5382, -0.8476,  0.4015, -0.8548]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8614517242637575, distance: 0.4259486595817672 entropy 0.03264415264129639
epoch: 21, step: 77
	action: tensor([[ 1.8713,  0.9818, -1.0805, -0.9238, -0.7437,  0.4964, -1.2387]],
       dtype=torch.float64)
	q_value: tensor([[-5.4978]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 78
	action: tensor([[ 1.5939,  0.8406, -0.9973, -0.9131, -0.2085,  0.0993, -0.7477]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2466942118139448, distance: 0.993212886022344 entropy 0.03264415264129639
epoch: 21, step: 79
	action: tensor([[ 2.1935,  0.6212, -1.7176, -1.2555, -0.6252,  0.2854, -1.3466]],
       dtype=torch.float64)
	q_value: tensor([[-6.3944]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 80
	action: tensor([[ 1.5061,  0.8980, -1.0810, -0.9139, -0.2126,  0.3064, -1.3124]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42343474515173474, distance: 0.8689222676043199 entropy 0.03264415264129639
epoch: 21, step: 81
	action: tensor([[ 2.4182,  1.2468, -1.3855, -1.6302, -0.5506,  0.3073, -2.0911]],
       dtype=torch.float64)
	q_value: tensor([[-6.4972]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 82
	action: tensor([[ 1.5635,  0.3599, -1.1355, -0.3613, -0.0820,  0.7953, -1.2282]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6854816592539443, distance: 0.6417703980637512 entropy 0.03264415264129639
epoch: 21, step: 83
	action: tensor([[ 2.8418,  1.0571, -2.0462, -1.5258, -0.1433,  0.2884, -1.5171]],
       dtype=torch.float64)
	q_value: tensor([[-6.1277]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 84
	action: tensor([[ 1.3075,  1.0546, -1.1489, -1.0294, -0.7034,  0.1797, -0.8667]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.49444158805196936, distance: 0.8136588710326023 entropy 0.03264415264129639
epoch: 21, step: 85
	action: tensor([[ 2.1384,  1.4427, -1.0226, -1.3686, -0.5161,  0.3432, -0.9904]],
       dtype=torch.float64)
	q_value: tensor([[-6.5181]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 86
	action: tensor([[ 1.3325,  0.4970, -1.1613, -1.2390, -0.7589, -0.1346, -1.2740]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18471966676616924, distance: 1.0332613304188267 entropy 0.03264415264129639
epoch: 21, step: 87
	action: tensor([[ 2.4569,  0.5793, -1.5703, -1.1683, -0.6896,  0.3169, -0.9168]],
       dtype=torch.float64)
	q_value: tensor([[-6.1564]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 88
	action: tensor([[ 1.4581,  0.8956, -0.9531, -0.9309, -0.5001,  0.2875, -1.5050]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4936579826058509, distance: 0.8142892043797126 entropy 0.03264415264129639
epoch: 21, step: 89
	action: tensor([[ 2.4900,  1.5755, -1.5033, -1.3893, -0.6711,  0.2236, -1.7469]],
       dtype=torch.float64)
	q_value: tensor([[-6.6693]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 90
	action: tensor([[ 1.6106,  0.6789, -1.1348, -1.0683, -0.0164,  0.5159, -1.2956]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24405603052248792, distance: 0.9949505505681941 entropy 0.03264415264129639
epoch: 21, step: 91
	action: tensor([[ 2.5689,  0.8615, -1.9164, -1.7437, -0.7919,  0.4578, -1.3537]],
       dtype=torch.float64)
	q_value: tensor([[-6.4166]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 92
	action: tensor([[ 1.7941,  0.9190, -1.1546, -0.9232, -0.0649,  0.1283, -1.3269]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22688410393289382, distance: 1.0061876783931887 entropy 0.03264415264129639
epoch: 21, step: 93
	action: tensor([[ 2.3652,  0.8204, -1.6034, -1.1191, -0.8413,  0.2416, -1.2984]],
       dtype=torch.float64)
	q_value: tensor([[-6.7092]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 94
	action: tensor([[ 1.6202,  0.6966, -1.0017, -0.8736, -0.4432,  0.4132, -1.0440]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3493107004200098, distance: 0.9230888937510975 entropy 0.03264415264129639
epoch: 21, step: 95
	action: tensor([[ 2.7931,  0.7997, -1.9373, -1.4995, -0.8037,  0.1776, -1.5141]],
       dtype=torch.float64)
	q_value: tensor([[-6.4115]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 96
	action: tensor([[ 1.5571,  0.8290, -0.8604, -0.9029, -0.4955,  0.6163, -0.9830]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5410740684823112, distance: 0.7752253973876986 entropy 0.03264415264129639
epoch: 21, step: 97
	action: tensor([[ 2.1079,  1.1873, -1.3934, -0.9362, -0.9446,  0.4123, -1.2570]],
       dtype=torch.float64)
	q_value: tensor([[-6.5908]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 98
	action: tensor([[ 1.1683,  0.7856, -1.1494, -0.7186, -0.4615,  0.7447, -0.8711]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8672093918631956, distance: 0.4170041462728006 entropy 0.03264415264129639
epoch: 21, step: 99
	action: tensor([[ 2.4140,  0.8492, -1.5053, -1.4811, -0.3601,  0.5052, -1.4075]],
       dtype=torch.float64)
	q_value: tensor([[-5.7325]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 100
	action: tensor([[ 1.7187,  0.6119, -0.9859, -0.3282, -0.3398,  0.1485, -1.0181]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.49556626168468154, distance: 0.8127533276170883 entropy 0.03264415264129639
epoch: 21, step: 101
	action: tensor([[ 2.3768,  0.8414, -1.4527, -1.3339, -0.7267,  0.3319, -1.5066]],
       dtype=torch.float64)
	q_value: tensor([[-6.2242]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 102
	action: tensor([[ 1.5156,  0.7772, -1.2679, -0.6753, -0.4327,  0.0070, -0.8516]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39734768292882794, distance: 0.8883622770313792 entropy 0.03264415264129639
epoch: 21, step: 103
	action: tensor([[ 2.6227,  1.3088, -1.7664, -1.5072, -0.5584,  1.0218, -1.4498]],
       dtype=torch.float64)
	q_value: tensor([[-6.3220]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 104
	action: tensor([[ 1.4824,  0.3735, -1.0307, -1.2119, -0.3825,  0.4886, -0.8249]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16860675669602943, distance: 1.0434218781041527 entropy 0.03264415264129639
epoch: 21, step: 105
	action: tensor([[ 2.2124,  1.0112, -1.7308, -0.8023, -0.9405,  0.8149, -1.7179]],
       dtype=torch.float64)
	q_value: tensor([[-5.9831]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 106
	action: tensor([[ 1.4597,  1.0986, -1.0777, -0.5294, -0.3131, -0.0274, -1.0127]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5149772307634863, distance: 0.7969622614107541 entropy 0.03264415264129639
epoch: 21, step: 107
	action: tensor([[ 2.1597,  1.1445, -1.1511, -1.1185, -0.2731,  0.5417, -1.7378]],
       dtype=torch.float64)
	q_value: tensor([[-6.5033]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 108
	action: tensor([[ 1.2319,  0.7164, -1.4083, -1.1247, -0.9577,  0.2930, -1.0301]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4782513764299602, distance: 0.8265846752736961 entropy 0.03264415264129639
epoch: 21, step: 109
	action: tensor([[ 2.0959,  1.1588, -1.6215, -1.0984, -0.6963,  0.5483, -1.9285]],
       dtype=torch.float64)
	q_value: tensor([[-6.4598]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 110
	action: tensor([[ 1.6288,  0.6744, -0.9220, -0.6996, -0.5066,  0.7632, -1.4156]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.576975876842956, distance: 0.7442849892755918 entropy 0.03264415264129639
epoch: 21, step: 111
	action: tensor([[ 2.6998,  1.2387, -1.5560, -1.3295, -1.1445,  0.5035, -1.3981]],
       dtype=torch.float64)
	q_value: tensor([[-6.7671]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 112
	action: tensor([[ 1.4425,  0.5632, -0.9333, -0.8569, -0.2545, -0.2360, -0.9205]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2781460240626523, distance: 0.9722576516331848 entropy 0.03264415264129639
epoch: 21, step: 113
	action: tensor([[ 1.8082,  0.8025, -0.9810, -1.1591, -0.7706,  0.6702, -1.2323]],
       dtype=torch.float64)
	q_value: tensor([[-5.7606]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 114
	action: tensor([[ 1.3374,  1.0692, -0.6743, -0.7760, -0.8637,  0.1407, -1.0073]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7035984798576216, distance: 0.6230127098089088 entropy 0.03264415264129639
epoch: 21, step: 115
	action: tensor([[ 2.1279,  0.8690, -1.7246, -1.0374, -0.6948,  0.5154, -1.7164]],
       dtype=torch.float64)
	q_value: tensor([[-6.5371]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 116
	action: tensor([[ 2.0723,  0.5859, -0.7087, -0.3037, -0.4084,  0.6747, -0.7618]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6779825123875953, distance: 0.6493762814575078 entropy 0.03264415264129639
epoch: 21, step: 117
	action: tensor([[ 2.2580,  0.7146, -1.4732, -0.9059, -0.8681,  0.6031, -1.2126]],
       dtype=torch.float64)
	q_value: tensor([[-6.1325]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 118
	action: tensor([[ 0.9793,  0.8915, -1.4556, -1.2059, -0.5395,  0.0816, -1.2626]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6071006487769229, distance: 0.7172942494524248 entropy 0.03264415264129639
epoch: 21, step: 119
	action: tensor([[ 2.3895,  0.9003, -1.6211, -1.5459, -0.4002,  0.4657, -1.4921]],
       dtype=torch.float64)
	q_value: tensor([[-5.9347]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 120
	action: tensor([[ 1.4881,  0.7160, -1.0009, -0.4954, -0.5447,  0.5691, -0.7076]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7076548232111337, distance: 0.6187349663110927 entropy 0.03264415264129639
epoch: 21, step: 121
	action: tensor([[ 2.2155,  0.9627, -1.4346, -1.2026, -0.7402,  0.8277, -1.4263]],
       dtype=torch.float64)
	q_value: tensor([[-6.0935]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 122
	action: tensor([[ 1.9195,  0.9447, -0.7154, -0.2798, -0.6213,  0.1561, -1.2675]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6223868534329874, distance: 0.7032022411240583 entropy 0.03264415264129639
epoch: 21, step: 123
	action: tensor([[ 2.5777,  1.0618, -1.2853, -1.1055, -0.3334,  0.2291, -1.3988]],
       dtype=torch.float64)
	q_value: tensor([[-6.8868]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 124
	action: tensor([[ 1.7215,  0.5496, -1.1796, -1.0053, -0.3288,  0.5470, -1.5365]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18980553127592326, distance: 1.0300334540508405 entropy 0.03264415264129639
epoch: 21, step: 125
	action: tensor([[ 2.4275,  1.5319, -1.5225, -1.3373, -0.4920,  0.6513, -1.4700]],
       dtype=torch.float64)
	q_value: tensor([[-6.5592]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 21, step: 126
	action: tensor([[ 0.7903,  0.6676, -1.2508, -1.1473, -0.4471,  0.4278, -1.0142]],
       dtype=torch.float64)
	q_value: tensor([[-5.0246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8410936721002426, distance: 0.4561705973124293 entropy 0.03264415264129639
epoch: 21, step: 127
	action: tensor([[ 2.0698,  1.0737, -1.0702, -1.1857, -0.2551,  0.6556, -1.4318]],
       dtype=torch.float64)
	q_value: tensor([[-5.0912]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
LOSS epoch 21 actor 485.0087898125499 critic 1829.377913687798 
epoch: 22, step: 0
	action: tensor([[ 0.5223,  0.4772, -0.9858, -0.1928,  0.1230,  0.1754, -0.4393]],
       dtype=torch.float64)
	q_value: tensor([[-8.9459]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9236891263822843, distance: 0.3161184952494169 entropy 0.03264415264129639
epoch: 22, step: 1
	action: tensor([[ 0.4355,  0.2885, -0.2188, -0.1964, -0.0281,  0.1764, -0.6094]],
       dtype=torch.float64)
	q_value: tensor([[-5.3312]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7989988277286564, distance: 0.5130456248925795 entropy 0.03264415264129639
epoch: 22, step: 2
	action: tensor([[ 0.0831, -0.0224, -0.4651,  0.0578, -0.5866,  0.0431, -0.7236]],
       dtype=torch.float64)
	q_value: tensor([[-4.3703]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37629491064938403, distance: 0.9037458940497116 entropy 0.03264415264129639
epoch: 22, step: 3
	action: tensor([[ 0.2320,  0.1262, -0.6228,  0.1049, -0.5931,  0.3724, -0.4634]],
       dtype=torch.float64)
	q_value: tensor([[-4.6893]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5575383042394899, distance: 0.7611925568794198 entropy 0.03264415264129639
epoch: 22, step: 4
	action: tensor([[ 0.7733,  0.2104, -0.5024, -0.2222, -0.1280, -0.2641, -0.3784]],
       dtype=torch.float64)
	q_value: tensor([[-4.9201]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7783730888182688, distance: 0.5387260051572419 entropy 0.03264415264129639
epoch: 22, step: 5
	action: tensor([[-0.3240,  0.4682, -0.2765, -0.4065, -0.3724,  0.5755, -0.4011]],
       dtype=torch.float64)
	q_value: tensor([[-5.3460]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18758881826987694, distance: 1.031441590610092 entropy 0.03264415264129639
epoch: 22, step: 6
	action: tensor([[ 0.0083, -0.1905, -0.2837, -0.2978, -0.3326,  0.6368, -0.2068]],
       dtype=torch.float64)
	q_value: tensor([[-5.3345]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14261593250939486, distance: 1.0596059729087526 entropy 0.03264415264129639
epoch: 22, step: 7
	action: tensor([[ 0.2134,  0.0303,  0.2577,  0.0306, -0.1439,  0.2238, -0.3258]],
       dtype=torch.float64)
	q_value: tensor([[-4.5398]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5722726988183575, distance: 0.7484110292389751 entropy 0.03264415264129639
epoch: 22, step: 8
	action: tensor([[ 0.1020, -0.0708, -0.5430, -0.1793, -0.3611, -0.0130, -0.1556]],
       dtype=torch.float64)
	q_value: tensor([[-4.3996]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3313208195715821, distance: 0.9357624070174131 entropy 0.03264415264129639
epoch: 22, step: 9
	action: tensor([[-0.0598, -0.1822, -0.2612, -0.0802,  0.1655, -0.1958, -0.9429]],
       dtype=torch.float64)
	q_value: tensor([[-4.1575]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04192415208139144, distance: 1.1200995928512014 entropy 0.03264415264129639
epoch: 22, step: 10
	action: tensor([[ 0.1722, -0.2775, -0.4540, -0.1331, -0.4246,  0.2971, -0.4163]],
       dtype=torch.float64)
	q_value: tensor([[-4.2324]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21144832835927663, distance: 1.0161826406694707 entropy 0.03264415264129639
epoch: 22, step: 11
	action: tensor([[ 0.5323,  0.4921, -0.7094, -0.5148, -0.0442,  0.0443, -0.3267]],
       dtype=torch.float64)
	q_value: tensor([[-4.2134]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8895760279886726, distance: 0.38026676992640657 entropy 0.03264415264129639
epoch: 22, step: 12
	action: tensor([[ 0.1678,  0.1249, -0.0635, -0.3992, -0.0885,  0.0925, -0.6884]],
       dtype=torch.float64)
	q_value: tensor([[-5.3312]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4137663684111387, distance: 0.876177422851009 entropy 0.03264415264129639
epoch: 22, step: 13
	action: tensor([[ 0.9788,  0.1966, -0.4632, -0.4751, -0.7633,  0.3743, -0.7074]],
       dtype=torch.float64)
	q_value: tensor([[-4.2419]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7329147066288869, distance: 0.5914004909796827 entropy 0.03264415264129639
epoch: 22, step: 14
	action: tensor([[ 0.6597,  0.3830, -0.7001, -0.2454, -0.3781, -0.0604, -0.5023]],
       dtype=torch.float64)
	q_value: tensor([[-7.0883]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9073432098440966, distance: 0.34833356042036606 entropy 0.03264415264129639
epoch: 22, step: 15
	action: tensor([[ 0.6841,  0.0349, -0.4684,  0.1202, -0.3069,  0.3694, -0.6468]],
       dtype=torch.float64)
	q_value: tensor([[-5.4517]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8412868632524448, distance: 0.45589321715298065 entropy 0.03264415264129639
epoch: 22, step: 16
	action: tensor([[ 0.1876,  0.1974, -0.0806, -0.3737, -0.1459, -0.1153, -0.2637]],
       dtype=torch.float64)
	q_value: tensor([[-5.2572]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.46278508422049647, distance: 0.8387465068213982 entropy 0.03264415264129639
epoch: 22, step: 17
	action: tensor([[ 0.1687,  0.3250, -0.1358, -0.1043,  0.3793,  0.0440, -0.0786]],
       dtype=torch.float64)
	q_value: tensor([[-4.1525]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6464521169007982, distance: 0.6804258646902477 entropy 0.03264415264129639
epoch: 22, step: 18
	action: tensor([[ 0.3514,  0.0843, -0.2675, -0.8586, -0.3296,  0.0130, -0.2526]],
       dtype=torch.float64)
	q_value: tensor([[-3.7244]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3350653603232213, distance: 0.9331386377483203 entropy 0.03264415264129639
epoch: 22, step: 19
	action: tensor([[ 0.1153,  0.0948, -0.8853, -0.1626, -0.3837,  0.5836, -0.3401]],
       dtype=torch.float64)
	q_value: tensor([[-5.3070]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4069464404470813, distance: 0.8812591753302753 entropy 0.03264415264129639
epoch: 22, step: 20
	action: tensor([[ 0.7454, -0.1218, -0.5539, -0.3685, -0.4265,  0.2553, -0.3948]],
       dtype=torch.float64)
	q_value: tensor([[-4.9407]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5309139212894816, distance: 0.7837597661449067 entropy 0.03264415264129639
epoch: 22, step: 21
	action: tensor([[ 0.9597, -0.3401, -0.6836, -0.3695, -0.1624,  0.1902, -0.1613]],
       dtype=torch.float64)
	q_value: tensor([[-5.3295]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2822509336187198, distance: 0.969489280247636 entropy 0.03264415264129639
epoch: 22, step: 22
	action: tensor([[ 0.3032,  0.4623, -0.6287, -0.0762, -0.4930,  0.0231, -0.7225]],
       dtype=torch.float64)
	q_value: tensor([[-5.6468]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.794961957719946, distance: 0.5181719699684036 entropy 0.03264415264129639
epoch: 22, step: 23
	action: tensor([[ 0.1790,  0.0254, -0.0271, -0.1858, -0.4076,  0.4682, -0.2517]],
       dtype=torch.float64)
	q_value: tensor([[-5.2467]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4818145792683547, distance: 0.8237573226243198 entropy 0.03264415264129639
epoch: 22, step: 24
	action: tensor([[ 0.7851,  0.0383, -0.3055, -0.2146, -0.3141,  0.0237,  0.0111]],
       dtype=torch.float64)
	q_value: tensor([[-4.7342]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6790695631312049, distance: 0.6482792886787452 entropy 0.03264415264129639
epoch: 22, step: 25
	action: tensor([[ 0.5239,  0.1172, -0.1992, -0.5867, -0.2965,  0.1191, -0.1162]],
       dtype=torch.float64)
	q_value: tensor([[-5.2932]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.547975753713837, distance: 0.7693740935849724 entropy 0.03264415264129639
epoch: 22, step: 26
	action: tensor([[ 0.6085, -0.1681,  0.1441, -0.2680, -0.3333,  0.1892, -0.5828]],
       dtype=torch.float64)
	q_value: tensor([[-5.0721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.47773085495572654, distance: 0.8269968927641091 entropy 0.03264415264129639
epoch: 22, step: 27
	action: tensor([[ 0.7949,  0.1084, -0.3295, -0.1715,  0.1342,  0.0425, -0.6353]],
       dtype=torch.float64)
	q_value: tensor([[-5.2181]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7637221851290692, distance: 0.5562476204803307 entropy 0.03264415264129639
epoch: 22, step: 28
	action: tensor([[ 0.2180,  0.3543, -0.3222, -0.4927, -0.4537,  0.1110, -0.7682]],
       dtype=torch.float64)
	q_value: tensor([[-4.9760]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6231121513279058, distance: 0.7025265810536749 entropy 0.03264415264129639
epoch: 22, step: 29
	action: tensor([[ 0.7422,  0.2296, -0.7104,  0.1642, -0.5047,  0.0391, -0.4358]],
       dtype=torch.float64)
	q_value: tensor([[-5.3232]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9118201304806616, distance: 0.3398141187733362 entropy 0.03264415264129639
epoch: 22, step: 30
	action: tensor([[ 0.3299, -0.4498, -0.5500, -0.3437, -0.2163,  0.0168, -0.6312]],
       dtype=torch.float64)
	q_value: tensor([[-5.6915]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.061663301229113676, distance: 1.1085008853691038 entropy 0.03264415264129639
epoch: 22, step: 31
	action: tensor([[ 0.1940,  0.2004, -0.4401, -0.1838, -0.1458,  0.3426, -0.4338]],
       dtype=torch.float64)
	q_value: tensor([[-4.2791]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5707269359171834, distance: 0.749762150252321 entropy 0.03264415264129639
epoch: 22, step: 32
	action: tensor([[ 0.5132, -0.4770, -0.5998,  0.2199,  0.0615, -0.1732, -0.5950]],
       dtype=torch.float64)
	q_value: tensor([[-4.0128]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3065457232105747, distance: 0.9529401135522624 entropy 0.03264415264129639
epoch: 22, step: 33
	action: tensor([[ 0.5909,  0.1309, -0.6015, -0.4017, -0.5034,  0.2537, -0.3668]],
       dtype=torch.float64)
	q_value: tensor([[-4.8425]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7067207894673895, distance: 0.6197225975603579 entropy 0.03264415264129639
epoch: 22, step: 34
	action: tensor([[ 0.4510,  0.1786, -0.3763, -0.3484, -0.2688,  0.1883, -0.4521]],
       dtype=torch.float64)
	q_value: tensor([[-5.4095]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.700282907289224, distance: 0.6264875558847353 entropy 0.03264415264129639
epoch: 22, step: 35
	action: tensor([[ 1.2357, -0.2599, -0.4375, -0.8222, -0.4249,  0.1605, -0.2949]],
       dtype=torch.float64)
	q_value: tensor([[-4.5200]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06209399854289965, distance: 1.1793376695553905 entropy 0.03264415264129639
epoch: 22, step: 36
	action: tensor([[ 1.1127,  0.3835, -0.6754, -0.3996, -0.1551,  0.2285, -0.3789]],
       dtype=torch.float64)
	q_value: tensor([[-7.0073]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.79815888804404, distance: 0.514116459768782 entropy 0.03264415264129639
epoch: 22, step: 37
	action: tensor([[ 0.1575,  0.3980, -0.2592,  0.0568, -0.3426,  0.4795, -0.4577]],
       dtype=torch.float64)
	q_value: tensor([[-6.6160]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6567101178354662, distance: 0.6704821110706715 entropy 0.03264415264129639
epoch: 22, step: 38
	action: tensor([[ 0.4402, -0.0604, -0.4191, -0.1687, -0.4707, -0.3060, -0.3712]],
       dtype=torch.float64)
	q_value: tensor([[-4.6973]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5359991338157435, distance: 0.7794999450547905 entropy 0.03264415264129639
epoch: 22, step: 39
	action: tensor([[ 0.4056,  0.2157, -0.3105, -0.5704, -0.1279, -0.1229, -0.0506]],
       dtype=torch.float64)
	q_value: tensor([[-4.8357]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5791299588496859, distance: 0.7423875831848511 entropy 0.03264415264129639
epoch: 22, step: 40
	action: tensor([[ 0.7951, -0.3773, -0.3307, -0.4929, -0.4195, -0.0120, -0.2928]],
       dtype=torch.float64)
	q_value: tensor([[-4.7284]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10080352579725438, distance: 1.085135548382729 entropy 0.03264415264129639
epoch: 22, step: 41
	action: tensor([[ 0.6175,  0.1469, -0.6872, -0.5125,  0.0623,  0.0220, -1.2528]],
       dtype=torch.float64)
	q_value: tensor([[-5.5342]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.671356486049282, distance: 0.6560232419063105 entropy 0.03264415264129639
epoch: 22, step: 42
	action: tensor([[ 0.5353,  0.4605, -0.3772,  0.1745, -0.0833,  0.1408, -0.6687]],
       dtype=torch.float64)
	q_value: tensor([[-5.6289]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9041273775995822, distance: 0.3543267956068243 entropy 0.03264415264129639
epoch: 22, step: 43
	action: tensor([[ 0.5127,  0.3826, -0.3370, -0.4002, -0.4748, -0.2050, -0.4697]],
       dtype=torch.float64)
	q_value: tensor([[-4.9654]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8061607872689215, distance: 0.5038224458594049 entropy 0.03264415264129639
epoch: 22, step: 44
	action: tensor([[ 0.3923,  0.4601, -0.8176, -0.1763, -0.6668, -0.3431, -0.0376]],
       dtype=torch.float64)
	q_value: tensor([[-5.3271]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9084350392651337, distance: 0.34627516909775646 entropy 0.03264415264129639
epoch: 22, step: 45
	action: tensor([[ 0.7293,  0.6398, -0.2594,  0.2021, -0.1256,  0.1128, -0.8963]],
       dtype=torch.float64)
	q_value: tensor([[-6.0814]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.958158576389795, distance: 0.2340776342536324 entropy 0.03264415264129639
epoch: 22, step: 46
	action: tensor([[ 0.5557,  0.1842, -1.0102, -0.3585, -0.2118,  0.6460, -0.5358]],
       dtype=torch.float64)
	q_value: tensor([[-6.1497]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.80729632074123, distance: 0.5023445519523886 entropy 0.03264415264129639
epoch: 22, step: 47
	action: tensor([[ 0.6586,  0.2125, -0.2201, -0.4762, -0.1229,  0.2556, -0.4783]],
       dtype=torch.float64)
	q_value: tensor([[-5.9408]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7460342529769765, distance: 0.5766924508910746 entropy 0.03264415264129639
epoch: 22, step: 48
	action: tensor([[ 0.4309,  0.2647, -0.8323, -0.5350,  0.0308, -0.0603, -0.3013]],
       dtype=torch.float64)
	q_value: tensor([[-4.9243]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7671757019472707, distance: 0.5521675044049204 entropy 0.03264415264129639
epoch: 22, step: 49
	action: tensor([[ 0.6580,  0.2818, -0.4401, -0.3430, -0.3405, -0.1656, -0.3362]],
       dtype=torch.float64)
	q_value: tensor([[-4.9589]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7969786979851287, distance: 0.5156173204445426 entropy 0.03264415264129639
epoch: 22, step: 50
	action: tensor([[ 0.8970,  0.2734, -0.3300, -0.1246,  0.1262,  0.1933, -0.3726]],
       dtype=torch.float64)
	q_value: tensor([[-5.1521]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9006060545879684, distance: 0.360775185228158 entropy 0.03264415264129639
epoch: 22, step: 51
	action: tensor([[ 0.9768,  0.1102, -0.3943, -0.1432, -0.1285,  0.2742, -0.4278]],
       dtype=torch.float64)
	q_value: tensor([[-5.2978]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8156779521262969, distance: 0.49129838523262614 entropy 0.03264415264129639
epoch: 22, step: 52
	action: tensor([[ 0.9964,  0.0691, -0.7883, -0.4321, -0.5256,  0.0885, -0.2624]],
       dtype=torch.float64)
	q_value: tensor([[-5.6661]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6171688536250822, distance: 0.7080441276579321 entropy 0.03264415264129639
epoch: 22, step: 53
	action: tensor([[ 0.6146,  0.2565, -0.4208, -0.3864, -0.2916, -0.3108, -0.9306]],
       dtype=torch.float64)
	q_value: tensor([[-6.4681]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7386846161907363, distance: 0.5849775269182771 entropy 0.03264415264129639
epoch: 22, step: 54
	action: tensor([[ 0.2649, -0.2450, -0.2852, -0.2222, -0.4745, -0.2139, -0.4928]],
       dtype=torch.float64)
	q_value: tensor([[-5.6994]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2314732499460972, distance: 1.0031969140474633 entropy 0.03264415264129639
epoch: 22, step: 55
	action: tensor([[ 0.4758,  0.4057, -0.1759, -0.1652,  0.1721,  0.1443, -0.5470]],
       dtype=torch.float64)
	q_value: tensor([[-4.5783]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8471509129777367, distance: 0.4473919008348411 entropy 0.03264415264129639
epoch: 22, step: 56
	action: tensor([[-0.0762, -0.1736,  0.0232, -0.6645, -0.1875, -0.4004, -0.5267]],
       dtype=torch.float64)
	q_value: tensor([[-4.3092]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2499892337696954, distance: 1.2794102609968412 entropy 0.03264415264129639
epoch: 22, step: 57
	action: tensor([[-0.1870, -0.2844, -0.6256, -0.0669, -0.0058,  0.2570, -0.1412]],
       dtype=torch.float64)
	q_value: tensor([[-4.6066]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1373111919100456, distance: 1.2203835633047408 entropy 0.03264415264129639
epoch: 22, step: 58
	action: tensor([[ 0.2890, -0.0724, -0.5473, -0.4301, -0.6471, -0.0135, -0.5897]],
       dtype=torch.float64)
	q_value: tensor([[-3.6710]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3921521704478955, distance: 0.8921833793296805 entropy 0.03264415264129639
epoch: 22, step: 59
	action: tensor([[ 0.7241, -0.0615, -0.5309, -0.3401, -0.8985, -0.0198, -0.5004]],
       dtype=torch.float64)
	q_value: tensor([[-5.1152]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5559213075325748, distance: 0.7625821950944268 entropy 0.03264415264129639
epoch: 22, step: 60
	action: tensor([[ 0.2317,  0.2886, -0.3941, -0.6755, -0.2103,  0.1205, -0.7136]],
       dtype=torch.float64)
	q_value: tensor([[-6.1935]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5708572772178865, distance: 0.7496483154915268 entropy 0.03264415264129639
epoch: 22, step: 61
	action: tensor([[ 0.6037,  0.5082, -0.5173,  0.0583, -0.8518,  0.0991, -0.5586]],
       dtype=torch.float64)
	q_value: tensor([[-4.8736]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9339627027945591, distance: 0.29407041759906305 entropy 0.03264415264129639
epoch: 22, step: 62
	action: tensor([[ 0.1165, -0.0509, -0.5093, -0.4415, -0.4694,  0.1231, -0.3170]],
       dtype=torch.float64)
	q_value: tensor([[-6.4441]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29241594388010017, distance: 0.9625996789624466 entropy 0.03264415264129639
epoch: 22, step: 63
	action: tensor([[ 0.3405,  0.1862, -0.6805, -0.3491, -0.1484, -0.3305, -0.1466]],
       dtype=torch.float64)
	q_value: tensor([[-4.5330]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6743281126743055, distance: 0.6530505939169579 entropy 0.03264415264129639
epoch: 22, step: 64
	action: tensor([[ 0.8571,  0.0147, -0.0925, -0.3702,  0.0837,  0.1195, -0.5743]],
       dtype=torch.float64)
	q_value: tensor([[-4.7157]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6057374061118411, distance: 0.7185375696329226 entropy 0.03264415264129639
epoch: 22, step: 65
	action: tensor([[ 0.5667,  0.2058, -0.6681, -0.0811, -0.1162,  0.1418, -0.5142]],
       dtype=torch.float64)
	q_value: tensor([[-5.1415]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8356555459260778, distance: 0.46391051868948724 entropy 0.03264415264129639
epoch: 22, step: 66
	action: tensor([[ 0.4399,  0.3487, -0.3209, -0.0090, -0.3092,  0.1420, -0.1665]],
       dtype=torch.float64)
	q_value: tensor([[-4.6676]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8500126205051503, distance: 0.4431839786114993 entropy 0.03264415264129639
epoch: 22, step: 67
	action: tensor([[ 0.0062, -0.0275, -0.6209, -0.2957, -0.2243,  0.0299, -0.2369]],
       dtype=torch.float64)
	q_value: tensor([[-4.5045]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2818965265055521, distance: 0.9697286058762105 entropy 0.03264415264129639
epoch: 22, step: 68
	action: tensor([[ 0.7334,  0.0994, -0.0103, -0.2921, -0.0545,  0.4917, -0.4338]],
       dtype=torch.float64)
	q_value: tensor([[-3.9279]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7773390122334937, distance: 0.5399813482909975 entropy 0.03264415264129639
epoch: 22, step: 69
	action: tensor([[ 0.5006, -0.1895, -0.4425, -0.6830, -0.2693,  0.5394, -0.5374]],
       dtype=torch.float64)
	q_value: tensor([[-5.1225]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33502678151867094, distance: 0.9331657072141218 entropy 0.03264415264129639
epoch: 22, step: 70
	action: tensor([[ 0.4707,  0.2079, -0.5887, -0.4161, -0.0198,  0.3186,  0.0015]],
       dtype=torch.float64)
	q_value: tensor([[-5.1931]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7471563617995476, distance: 0.5754170268050859 entropy 0.03264415264129639
epoch: 22, step: 71
	action: tensor([[ 0.5435,  0.1637, -0.7187, -0.3223, -0.1673,  0.3842, -0.6484]],
       dtype=torch.float64)
	q_value: tensor([[-4.8529]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7805361833161546, distance: 0.5360905566854783 entropy 0.03264415264129639
epoch: 22, step: 72
	action: tensor([[ 0.4107,  0.3671, -0.9420, -0.4249, -0.1866,  0.6750, -0.2750]],
       dtype=torch.float64)
	q_value: tensor([[-5.0131]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8389670807644786, distance: 0.4592128436348158 entropy 0.03264415264129639
epoch: 22, step: 73
	action: tensor([[ 0.5748,  0.2485, -0.7001,  0.2390, -0.5132, -0.0576, -0.3932]],
       dtype=torch.float64)
	q_value: tensor([[-5.8983]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8757392099289343, distance: 0.4033887188353975 entropy 0.03264415264129639
epoch: 22, step: 74
	action: tensor([[ 0.6182,  0.1644, -0.2523, -0.3973, -0.1867, -0.1098, -0.1496]],
       dtype=torch.float64)
	q_value: tensor([[-5.3159]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6629439754491824, distance: 0.6643665227353509 entropy 0.03264415264129639
epoch: 22, step: 75
	action: tensor([[ 0.4024,  0.3785, -0.7669, -0.0639, -0.3489,  0.2868, -0.5856]],
       dtype=torch.float64)
	q_value: tensor([[-4.8597]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8062817897403964, distance: 0.5036651678833136 entropy 0.03264415264129639
epoch: 22, step: 76
	action: tensor([[ 0.5143,  0.1564, -0.5982, -0.5002,  0.0229, -0.3298, -0.7171]],
       dtype=torch.float64)
	q_value: tensor([[-5.0706]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.62121632327577, distance: 0.7042912955505148 entropy 0.03264415264129639
epoch: 22, step: 77
	action: tensor([[ 0.3360,  0.4525, -0.3774, -0.6302, -0.3355,  0.1652, -0.0456]],
       dtype=torch.float64)
	q_value: tensor([[-5.0067]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7464340675804072, distance: 0.5762383327992888 entropy 0.03264415264129639
epoch: 22, step: 78
	action: tensor([[ 0.1563, -0.3609, -0.3835, -0.4006, -0.5067, -0.0276, -0.5072]],
       dtype=torch.float64)
	q_value: tensor([[-5.4973]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.001609556080042096, distance: 1.1434229399982216 entropy 0.03264415264129639
epoch: 22, step: 79
	action: tensor([[ 0.1337,  0.1591, -0.2405, -0.1223,  0.0461,  0.1025, -0.6446]],
       dtype=torch.float64)
	q_value: tensor([[-4.5569]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.495391749857631, distance: 0.8128939038636032 entropy 0.03264415264129639
epoch: 22, step: 80
	action: tensor([[ 0.0752,  0.6142, -0.2427,  0.0349,  0.0800, -0.3890,  0.1798]],
       dtype=torch.float64)
	q_value: tensor([[-3.7613]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6624226621189881, distance: 0.6648801010180975 entropy 0.03264415264129639
epoch: 22, step: 81
	action: tensor([[ 0.0738,  0.3484,  0.0486, -0.3153,  0.0991,  0.3408, -0.1498]],
       dtype=torch.float64)
	q_value: tensor([[-4.6852]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5407232458748343, distance: 0.7755216485044392 entropy 0.03264415264129639
epoch: 22, step: 82
	action: tensor([[ 0.0898, -0.0372, -0.4952, -0.0138, -0.5703,  0.1972, -0.9241]],
       dtype=torch.float64)
	q_value: tensor([[-4.0745]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33945375637218067, distance: 0.9300543039592943 entropy 0.03264415264129639
epoch: 22, step: 83
	action: tensor([[ 0.3568, -0.1662, -0.7532, -0.2591, -0.2602,  0.0142, -0.2366]],
       dtype=torch.float64)
	q_value: tensor([[-4.9852]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42928871727518925, distance: 0.8644998504825309 entropy 0.03264415264129639
epoch: 22, step: 84
	action: tensor([[ 0.1300,  0.0262, -0.8103, -0.2414,  0.2482, -0.0799, -0.2731]],
       dtype=torch.float64)
	q_value: tensor([[-4.3918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4604336709122684, distance: 0.8405801177337153 entropy 0.03264415264129639
epoch: 22, step: 85
	action: tensor([[ 0.3764,  0.0537, -0.1258, -0.4555, -0.2726,  0.4552, -0.7039]],
       dtype=torch.float64)
	q_value: tensor([[-4.0122]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5478590074562644, distance: 0.7694734419583577 entropy 0.03264415264129639
epoch: 22, step: 86
	action: tensor([[ 0.1407,  0.2409, -0.3122, -0.2510, -0.0958,  0.0914, -0.8289]],
       dtype=torch.float64)
	q_value: tensor([[-4.9838]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5579327744623914, distance: 0.7608531661986602 entropy 0.03264415264129639
epoch: 22, step: 87
	action: tensor([[ 0.0544,  0.7785, -0.6521,  0.4776, -0.1564, -0.3277, -0.4751]],
       dtype=torch.float64)
	q_value: tensor([[-4.3030]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 22, step: 88
	action: tensor([[ 0.6613,  0.0888, -0.1774, -0.3495, -0.3827,  0.1958, -0.0339]],
       dtype=torch.float64)
	q_value: tensor([[-8.9459]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6736195525511999, distance: 0.6537606248009666 entropy 0.03264415264129639
epoch: 22, step: 89
	action: tensor([[ 0.4662, -0.0069, -0.4603, -0.6188,  0.1344,  0.6831, -0.4843]],
       dtype=torch.float64)
	q_value: tensor([[-5.2568]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6156138776476399, distance: 0.7094806302625235 entropy 0.03264415264129639
epoch: 22, step: 90
	action: tensor([[ 0.5886,  0.2347, -0.5436, -0.1873, -0.1009,  0.0349, -0.4031]],
       dtype=torch.float64)
	q_value: tensor([[-4.8456]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8310232024635658, distance: 0.4704031603140178 entropy 0.03264415264129639
epoch: 22, step: 91
	action: tensor([[ 0.6065, -0.0736, -0.1595, -0.2620, -0.0177, -0.0214, -0.2394]],
       dtype=torch.float64)
	q_value: tensor([[-4.6153]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5252023966394737, distance: 0.7885168028197892 entropy 0.03264415264129639
epoch: 22, step: 92
	action: tensor([[ 0.4175, -0.1655,  0.0417, -0.2953, -0.5205, -0.3644, -0.2805]],
       dtype=torch.float64)
	q_value: tensor([[-4.4082]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28199493898915673, distance: 0.969662155375732 entropy 0.03264415264129639
epoch: 22, step: 93
	action: tensor([[ 0.0077, -0.3160, -0.5260,  0.0093,  0.0588,  0.4573, -0.4092]],
       dtype=torch.float64)
	q_value: tensor([[-5.2025]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1540279285399656, distance: 1.0525305400508576 entropy 0.03264415264129639
epoch: 22, step: 94
	action: tensor([[ 0.0947,  0.5026, -0.4134, -0.2074, -0.2903,  0.1150, -0.4929]],
       dtype=torch.float64)
	q_value: tensor([[-3.7115]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6613700639485119, distance: 0.6659158738580144 entropy 0.03264415264129639
epoch: 22, step: 95
	action: tensor([[ 0.8305,  0.3958, -0.5630, -0.6444, -0.0978,  0.1025, -0.1550]],
       dtype=torch.float64)
	q_value: tensor([[-4.5289]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7962655111981674, distance: 0.5165221739611686 entropy 0.03264415264129639
epoch: 22, step: 96
	action: tensor([[ 0.7127,  0.1658, -0.1952, -0.4637,  0.0403, -0.0584, -0.4814]],
       dtype=torch.float64)
	q_value: tensor([[-6.0175]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6521574188378072, distance: 0.6749134232095582 entropy 0.03264415264129639
epoch: 22, step: 97
	action: tensor([[ 0.8230,  0.1202, -0.1575, -0.3933, -0.3778, -0.0142, -0.7042]],
       dtype=torch.float64)
	q_value: tensor([[-4.8572]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6657820908041127, distance: 0.6615635245442868 entropy 0.03264415264129639
epoch: 22, step: 98
	action: tensor([[ 0.2009,  0.3349, -0.3597, -0.5629, -0.1148,  0.1546, -0.2559]],
       dtype=torch.float64)
	q_value: tensor([[-5.7442]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6168598215765401, distance: 0.7083298465578404 entropy 0.03264415264129639
epoch: 22, step: 99
	action: tensor([[ 0.5014,  0.3655, -0.2181, -0.3853, -0.7397, -0.2631, -0.4310]],
       dtype=torch.float64)
	q_value: tensor([[-4.4537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7932956258831323, distance: 0.5202732855074556 entropy 0.03264415264129639
epoch: 22, step: 100
	action: tensor([[ 0.5788,  0.4038, -0.7319, -0.3085,  0.0164, -0.5963, -0.6123]],
       dtype=torch.float64)
	q_value: tensor([[-5.8940]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8556995668749112, distance: 0.4347008712189846 entropy 0.03264415264129639
epoch: 22, step: 101
	action: tensor([[ 0.1114, -0.0288, -0.2703, -0.2296, -0.1026,  0.2476, -0.4562]],
       dtype=torch.float64)
	q_value: tensor([[-5.9021]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3694852767037856, distance: 0.9086660654716867 entropy 0.03264415264129639
epoch: 22, step: 102
	action: tensor([[-0.0292, -0.2105, -0.4402, -0.2503, -0.3655,  0.0981, -0.5003]],
       dtype=torch.float64)
	q_value: tensor([[-3.6908]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06279626956378115, distance: 1.1078314690860243 entropy 0.03264415264129639
epoch: 22, step: 103
	action: tensor([[ 0.5497, -0.0264, -0.3434, -0.5683, -0.3603,  0.4513, -0.6560]],
       dtype=torch.float64)
	q_value: tensor([[-4.0179]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5218609571163851, distance: 0.7912865743940057 entropy 0.03264415264129639
epoch: 22, step: 104
	action: tensor([[ 0.2367, -0.0062, -0.4549, -0.4059, -0.1118,  0.4929, -0.6446]],
       dtype=torch.float64)
	q_value: tensor([[-5.3096]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.48549273387898084, distance: 0.8208285421930218 entropy 0.03264415264129639
epoch: 22, step: 105
	action: tensor([[ 0.4688,  0.1117,  0.0601, -0.3590, -0.2591,  0.4672, -0.4336]],
       dtype=torch.float64)
	q_value: tensor([[-4.3595]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6828671602006362, distance: 0.6444323024897205 entropy 0.03264415264129639
epoch: 22, step: 106
	action: tensor([[ 0.8081,  0.4230, -0.3204, -0.1341, -0.3858,  0.0340, -0.3529]],
       dtype=torch.float64)
	q_value: tensor([[-5.0290]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9439709367773882, distance: 0.27087153817632637 entropy 0.03264415264129639
epoch: 22, step: 107
	action: tensor([[ 0.7450,  0.5494, -0.4554, -0.0733, -0.0176, -0.0065, -0.8078]],
       dtype=torch.float64)
	q_value: tensor([[-5.5652]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9801505562103183, distance: 0.16122443517306234 entropy 0.03264415264129639
epoch: 22, step: 108
	action: tensor([[ 0.4767, -0.1025, -0.1799, -0.5271, -0.4226,  0.1736, -0.8686]],
       dtype=torch.float64)
	q_value: tensor([[-5.7375]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3600711976780079, distance: 0.9154244801251298 entropy 0.03264415264129639
epoch: 22, step: 109
	action: tensor([[ 0.4209, -0.0033, -0.5140, -0.2076, -0.3045, -0.1539, -0.5666]],
       dtype=torch.float64)
	q_value: tensor([[-5.2897]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5788837311257076, distance: 0.7426047163362459 entropy 0.03264415264129639
epoch: 22, step: 110
	action: tensor([[ 0.4584,  0.2337, -0.3457, -0.4896, -0.2100, -0.3916, -0.1944]],
       dtype=torch.float64)
	q_value: tensor([[-4.4454]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6277324365779999, distance: 0.69820714957676 entropy 0.03264415264129639
epoch: 22, step: 111
	action: tensor([[ 0.7065,  0.1707,  0.0645, -0.5148, -0.2311,  0.4052, -0.6720]],
       dtype=torch.float64)
	q_value: tensor([[-4.9776]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7247274018623029, distance: 0.6003965447508004 entropy 0.03264415264129639
epoch: 22, step: 112
	action: tensor([[ 0.5785,  0.0736, -0.6561, -0.2304, -0.2402,  0.5270, -0.2375]],
       dtype=torch.float64)
	q_value: tensor([[-5.7052]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7605085023958194, distance: 0.5600176865276694 entropy 0.03264415264129639
epoch: 22, step: 113
	action: tensor([[ 0.5501,  0.2834, -0.8176, -0.0498, -0.3303,  0.4981, -1.1008]],
       dtype=torch.float64)
	q_value: tensor([[-5.1986]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8399112402449858, distance: 0.457864648641068 entropy 0.03264415264129639
epoch: 22, step: 114
	action: tensor([[ 0.5508,  0.4162, -0.2534, -0.3631, -0.1795,  0.0725, -0.7350]],
       dtype=torch.float64)
	q_value: tensor([[-6.0353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8591750172019761, distance: 0.4294341190660326 entropy 0.03264415264129639
epoch: 22, step: 115
	action: tensor([[ 0.4984,  0.2848, -0.7634, -0.3187, -0.4789,  0.2708, -0.2100]],
       dtype=torch.float64)
	q_value: tensor([[-5.1202]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8198769703478718, distance: 0.4856700414410881 entropy 0.03264415264129639
epoch: 22, step: 116
	action: tensor([[ 0.7758,  0.1553, -0.3520, -0.2520, -0.7907, -0.0161, -0.3905]],
       dtype=torch.float64)
	q_value: tensor([[-5.5910]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7682479914640175, distance: 0.5508945128683649 entropy 0.03264415264129639
epoch: 22, step: 117
	action: tensor([[ 0.7424, -0.0085, -0.8114, -0.0488, -0.1694, -0.2099, -0.5918]],
       dtype=torch.float64)
	q_value: tensor([[-6.0577]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7165493319893981, distance: 0.6092498566709625 entropy 0.03264415264129639
epoch: 22, step: 118
	action: tensor([[ 0.3320,  0.1822, -0.6606,  0.0451, -0.1766,  0.4073, -0.3295]],
       dtype=torch.float64)
	q_value: tensor([[-5.3938]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7003226179896662, distance: 0.626446051605572 entropy 0.03264415264129639
epoch: 22, step: 119
	action: tensor([[ 0.5723, -0.5140, -0.1295, -0.8911, -0.1884,  0.1770, -0.5120]],
       dtype=torch.float64)
	q_value: tensor([[-4.3663]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.27443261591736046, distance: 1.2918590503325862 entropy 0.03264415264129639
epoch: 22, step: 120
	action: tensor([[ 0.7844,  0.4330, -0.4128, -0.5712, -0.6470, -0.2125, -0.4813]],
       dtype=torch.float64)
	q_value: tensor([[-5.1564]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7827837200040366, distance: 0.5333384312487526 entropy 0.03264415264129639
epoch: 22, step: 121
	action: tensor([[ 0.3865,  0.0797, -0.3347,  0.0435, -0.1111,  0.2613, -0.5758]],
       dtype=torch.float64)
	q_value: tensor([[-6.4169]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6952672292151864, distance: 0.6317078496800477 entropy 0.03264415264129639
epoch: 22, step: 122
	action: tensor([[ 0.5774,  0.1183, -0.4876, -0.2423, -0.2016, -0.1338, -0.3708]],
       dtype=torch.float64)
	q_value: tensor([[-4.1293]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7113717942038832, distance: 0.6147889854758317 entropy 0.03264415264129639
epoch: 22, step: 123
	action: tensor([[ 0.1913,  0.3494,  0.0421,  0.3013, -0.2782,  0.3024, -0.5785]],
       dtype=torch.float64)
	q_value: tensor([[-4.6562]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7580697211441562, distance: 0.5628618402829788 entropy 0.03264415264129639
epoch: 22, step: 124
	action: tensor([[ 0.3052,  0.2498, -0.0937, -0.2248,  0.0919, -0.3221, -0.2046]],
       dtype=torch.float64)
	q_value: tensor([[-4.7962]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6021209175423134, distance: 0.721825544522865 entropy 0.03264415264129639
epoch: 22, step: 125
	action: tensor([[ 0.2507, -0.1443, -0.4462, -0.3856,  0.0672, -0.0350, -0.6072]],
       dtype=torch.float64)
	q_value: tensor([[-4.2530]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29753319394057165, distance: 0.9591126009697061 entropy 0.03264415264129639
epoch: 22, step: 126
	action: tensor([[ 0.0849,  0.3889, -0.1608, -0.2346, -0.1930,  0.0063, -0.4437]],
       dtype=torch.float64)
	q_value: tensor([[-3.7644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5762352119655345, distance: 0.7449362815199614 entropy 0.03264415264129639
epoch: 22, step: 127
	action: tensor([[ 0.3470,  0.3467, -0.1786, -0.1846,  0.2935,  0.4494, -0.3903]],
       dtype=torch.float64)
	q_value: tensor([[-4.2370]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8164688049838028, distance: 0.49024326891390807 entropy 0.03264415264129639
LOSS epoch 22 actor 24.620144109708896 critic 157.87564940707995 
epoch: 23, step: 0
	action: tensor([[ 0.2409, -0.2331, -0.2056,  0.1471,  0.0321, -0.0985, -0.4698]],
       dtype=torch.float64)
	q_value: tensor([[-5.5814]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39020390132220073, distance: 0.8936120450653194 entropy 0.03264415264129639
epoch: 23, step: 1
	action: tensor([[ 0.0745, -0.2614, -0.2319, -0.6001, -0.4911,  0.4794, -0.6133]],
       dtype=torch.float64)
	q_value: tensor([[-5.1508]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.030931665248570273, distance: 1.1619077082982692 entropy 0.03264415264129639
epoch: 23, step: 2
	action: tensor([[ 0.0566, -0.0485, -0.1875, -0.2603,  0.0584,  0.7751, -0.3493]],
       dtype=torch.float64)
	q_value: tensor([[-7.1058]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37957142124932564, distance: 0.9013689436035522 entropy 0.03264415264129639
epoch: 23, step: 3
	action: tensor([[ 0.1710,  0.1129, -0.4542, -0.1852, -0.3287,  0.2827,  0.1120]],
       dtype=torch.float64)
	q_value: tensor([[-5.9082]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5100147098091539, distance: 0.8010289540772021 entropy 0.03264415264129639
epoch: 23, step: 4
	action: tensor([[ 0.0102,  0.3135,  0.1989, -0.4321,  0.0616,  0.3087, -0.4213]],
       dtype=torch.float64)
	q_value: tensor([[-6.0805]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4135119373144519, distance: 0.876367536986165 entropy 0.03264415264129639
epoch: 23, step: 5
	action: tensor([[ 0.3411,  0.3595,  0.1360,  0.2941,  0.0480,  0.0284, -0.1866]],
       dtype=torch.float64)
	q_value: tensor([[-5.7847]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8577070651061831, distance: 0.43166651570758025 entropy 0.03264415264129639
epoch: 23, step: 6
	action: tensor([[ 0.5915, -0.0029, -0.3133,  0.4295,  0.3085, -0.3648,  0.2273]],
       dtype=torch.float64)
	q_value: tensor([[-5.8494]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7757093705100898, distance: 0.5419537905377676 entropy 0.03264415264129639
epoch: 23, step: 7
	action: tensor([[ 0.0328, -0.0638, -0.2001, -0.2035,  0.0996,  0.2744, -0.6257]],
       dtype=torch.float64)
	q_value: tensor([[-6.9346]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29104203053946887, distance: 0.9635337638704554 entropy 0.03264415264129639
epoch: 23, step: 8
	action: tensor([[ 0.4846,  0.4663, -0.2417, -0.1339,  0.1335,  0.3740, -0.3782]],
       dtype=torch.float64)
	q_value: tensor([[-4.9236]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8975497506877708, distance: 0.3662799979348398 entropy 0.03264415264129639
epoch: 23, step: 9
	action: tensor([[ 0.2873, -0.0516, -0.2808, -0.2545,  0.1047,  0.3150,  0.0126]],
       dtype=torch.float64)
	q_value: tensor([[-6.1111]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4943538152424124, distance: 0.8137294998896558 entropy 0.03264415264129639
epoch: 23, step: 10
	action: tensor([[ 0.3356,  0.3327, -0.4035, -0.1577, -0.4170, -0.1747,  0.3230]],
       dtype=torch.float64)
	q_value: tensor([[-5.1819]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.763883857445411, distance: 0.5560572826144449 entropy 0.03264415264129639
epoch: 23, step: 11
	action: tensor([[-0.2702, -0.0242, -0.0306, -0.4515,  0.0359,  0.2810, -0.3948]],
       dtype=torch.float64)
	q_value: tensor([[-7.1309]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11652592534103223, distance: 1.2091804004828661 entropy 0.03264415264129639
epoch: 23, step: 12
	action: tensor([[ 0.2818, -0.0162,  0.0653, -0.2967, -0.0321, -0.0273, -0.0290]],
       dtype=torch.float64)
	q_value: tensor([[-5.2166]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35736593962296415, distance: 0.917357388668084 entropy 0.03264415264129639
epoch: 23, step: 13
	action: tensor([[ 0.0519,  0.3809, -0.6006,  0.2028,  0.2642,  0.3790, -0.2350]],
       dtype=torch.float64)
	q_value: tensor([[-5.3292]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5703830199610945, distance: 0.7500624294490003 entropy 0.03264415264129639
epoch: 23, step: 14
	action: tensor([[-0.0475,  0.3843, -0.0921, -0.1972, -0.3153,  0.3369, -0.2516]],
       dtype=torch.float64)
	q_value: tensor([[-5.6265]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4767007200324438, distance: 0.8278120842386576 entropy 0.03264415264129639
epoch: 23, step: 15
	action: tensor([[ 0.3745, -0.0218, -0.8134, -0.3852, -0.1114, -0.1690,  0.1981]],
       dtype=torch.float64)
	q_value: tensor([[-6.2800]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5485567680346632, distance: 0.7688794728791369 entropy 0.03264415264129639
epoch: 23, step: 16
	action: tensor([[ 0.4126, -0.2494, -0.7955, -0.3984, -0.1747,  0.2501, -0.4309]],
       dtype=torch.float64)
	q_value: tensor([[-6.9912]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3700932464010498, distance: 0.9082278721602732 entropy 0.03264415264129639
epoch: 23, step: 17
	action: tensor([[ 0.2348, -0.0901, -0.1391, -0.1308,  0.1035, -0.0363, -0.0083]],
       dtype=torch.float64)
	q_value: tensor([[-6.2818]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38301301434625834, distance: 0.8988654651949645 entropy 0.03264415264129639
epoch: 23, step: 18
	action: tensor([[ 0.1290,  0.1346, -0.3424,  0.2408, -0.2894, -0.1161, -0.4786]],
       dtype=torch.float64)
	q_value: tensor([[-4.9375]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5795568341845474, distance: 0.7420109974040185 entropy 0.03264415264129639
epoch: 23, step: 19
	action: tensor([[ 0.1988, -0.2385, -0.1633, -0.0944,  0.2121, -0.0319, -0.1471]],
       dtype=torch.float64)
	q_value: tensor([[-5.6299]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24110210476817384, distance: 0.9968925890340211 entropy 0.03264415264129639
epoch: 23, step: 20
	action: tensor([[-0.4250,  0.1291, -0.0830,  0.2888, -0.1562,  0.0077,  0.0584]],
       dtype=torch.float64)
	q_value: tensor([[-4.7382]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04689641107930853, distance: 1.117189243838657 entropy 0.03264415264129639
epoch: 23, step: 21
	action: tensor([[ 0.5432,  0.5361, -0.3883, -0.0090, -0.0128,  0.3947, -0.1969]],
       dtype=torch.float64)
	q_value: tensor([[-5.4383]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.917452088310783, distance: 0.328783291330555 entropy 0.03264415264129639
epoch: 23, step: 22
	action: tensor([[-0.1140,  0.2595, -0.3018, -0.3217,  0.2396,  0.2509, -0.4633]],
       dtype=torch.float64)
	q_value: tensor([[-6.6206]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36800504020134417, distance: 0.9097320613386262 entropy 0.03264415264129639
epoch: 23, step: 23
	action: tensor([[ 0.0640,  0.1274, -0.2040, -0.2180, -0.3169, -0.1227,  0.0792]],
       dtype=torch.float64)
	q_value: tensor([[-5.0053]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3997080734533798, distance: 0.8866208590767919 entropy 0.03264415264129639
epoch: 23, step: 24
	action: tensor([[ 0.2016, -0.0907, -0.0750, -0.4908, -0.4217, -0.0095, -0.1588]],
       dtype=torch.float64)
	q_value: tensor([[-5.7421]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18956756428484456, distance: 1.0301847115423493 entropy 0.03264415264129639
epoch: 23, step: 25
	action: tensor([[ 0.0519,  0.0796, -0.3075, -0.5606, -0.3679, -0.3852, -0.2278]],
       dtype=torch.float64)
	q_value: tensor([[-6.3009]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2804045540346055, distance: 0.9707354647235208 entropy 0.03264415264129639
epoch: 23, step: 26
	action: tensor([[ 0.1165, -0.0958, -0.1665,  0.0097, -0.4179, -0.0452,  0.3071]],
       dtype=torch.float64)
	q_value: tensor([[-6.5207]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3582030534679378, distance: 0.9167597057377386 entropy 0.03264415264129639
epoch: 23, step: 27
	action: tensor([[ 0.0498,  0.0340, -0.4748, -0.3230,  0.1710,  0.1500, -0.4886]],
       dtype=torch.float64)
	q_value: tensor([[-5.9180]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3662218153267236, distance: 0.9110145985608364 entropy 0.03264415264129639
epoch: 23, step: 28
	action: tensor([[-0.0133, -0.0704, -0.2803,  0.1760, -0.3440, -0.0484,  0.0907]],
       dtype=torch.float64)
	q_value: tensor([[-4.8199]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3134989783767219, distance: 0.9481505192143765 entropy 0.03264415264129639
epoch: 23, step: 29
	action: tensor([[-0.0310, -0.0133, -0.2495, -0.2623, -0.0880,  0.3424, -0.0931]],
       dtype=torch.float64)
	q_value: tensor([[-5.3200]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2514922164428173, distance: 0.9900448150678721 entropy 0.03264415264129639
epoch: 23, step: 30
	action: tensor([[ 0.2900,  0.1831, -0.2267,  0.0013,  0.2374, -0.5084, -0.1337]],
       dtype=torch.float64)
	q_value: tensor([[-5.0862]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5896576753030456, distance: 0.7330436744901656 entropy 0.03264415264129639
epoch: 23, step: 31
	action: tensor([[ 0.1168,  0.0202, -0.1612,  0.2173,  0.1144, -0.0450,  0.0899]],
       dtype=torch.float64)
	q_value: tensor([[-5.9869]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5123294555014908, distance: 0.7991346386939732 entropy 0.03264415264129639
epoch: 23, step: 32
	action: tensor([[-0.4106,  0.1503,  0.0736, -0.1101,  0.0418, -0.2364, -0.7608]],
       dtype=torch.float64)
	q_value: tensor([[-5.1351]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.14972898065892482, distance: 1.2270278852342846 entropy 0.03264415264129639
epoch: 23, step: 33
	action: tensor([[ 0.3040,  0.3873, -0.7342, -0.2434, -0.0589,  0.3058, -0.3837]],
       dtype=torch.float64)
	q_value: tensor([[-5.6655]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7666876869133981, distance: 0.5527458911729176 entropy 0.03264415264129639
epoch: 23, step: 34
	action: tensor([[ 0.0880, -0.1506, -0.6016, -0.0208, -0.0818, -0.1772, -0.3876]],
       dtype=torch.float64)
	q_value: tensor([[-6.5999]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2809662958927537, distance: 0.9703564953949692 entropy 0.03264415264129639
epoch: 23, step: 35
	action: tensor([[ 0.5679,  0.0078, -0.2528, -0.4378, -0.1176, -0.1821, -0.2285]],
       dtype=torch.float64)
	q_value: tensor([[-5.2523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4804864463976378, distance: 0.8248123108582449 entropy 0.03264415264129639
epoch: 23, step: 36
	action: tensor([[-0.1009,  0.1872, -0.3526, -0.2044, -0.3514, -0.2330, -0.5805]],
       dtype=torch.float64)
	q_value: tensor([[-6.2482]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31858183762199566, distance: 0.9446339407817135 entropy 0.03264415264129639
epoch: 23, step: 37
	action: tensor([[ 0.2711,  0.1141, -0.0984, -0.1692,  0.2356, -0.1854,  0.0037]],
       dtype=torch.float64)
	q_value: tensor([[-5.9758]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.514044337292408, distance: 0.7977283323849977 entropy 0.03264415264129639
epoch: 23, step: 38
	action: tensor([[ 0.3540, -0.2270, -0.3196, -0.0573, -0.3875,  0.3578, -0.0230]],
       dtype=torch.float64)
	q_value: tensor([[-5.1768]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4441456214377181, distance: 0.8531732076842137 entropy 0.03264415264129639
epoch: 23, step: 39
	action: tensor([[ 0.1873, -0.0117, -0.5863, -0.3382, -0.1135,  0.3407, -0.2417]],
       dtype=torch.float64)
	q_value: tensor([[-6.0173]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.45436408610893864, distance: 0.8452947414442553 entropy 0.03264415264129639
epoch: 23, step: 40
	action: tensor([[ 0.2970,  0.3262, -0.5020, -0.0557, -0.1597, -0.2052, -0.3988]],
       dtype=torch.float64)
	q_value: tensor([[-5.6368]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7599779408707659, distance: 0.5606376656568752 entropy 0.03264415264129639
epoch: 23, step: 41
	action: tensor([[-0.3810,  0.5362,  0.0444,  0.3009, -0.0307,  0.2093,  0.0847]],
       dtype=torch.float64)
	q_value: tensor([[-5.9823]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3721940218692248, distance: 0.9067121110662835 entropy 0.03264415264129639
epoch: 23, step: 42
	action: tensor([[-0.2245,  0.2104, -0.0551, -0.0516, -0.5383, -0.1482, -0.0071]],
       dtype=torch.float64)
	q_value: tensor([[-6.0223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21254574719276131, distance: 1.0154752892848058 entropy 0.03264415264129639
epoch: 23, step: 43
	action: tensor([[ 0.1586, -0.0383, -0.0725, -0.1040,  0.2412, -0.0266, -0.0885]],
       dtype=torch.float64)
	q_value: tensor([[-6.2452]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3661708406816755, distance: 0.9110512341797273 entropy 0.03264415264129639
epoch: 23, step: 44
	action: tensor([[ 0.5987, -0.1429, -0.5331, -0.3066, -0.1670,  0.4190, -0.4661]],
       dtype=torch.float64)
	q_value: tensor([[-4.7361]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5382370303626285, distance: 0.7776178917783163 entropy 0.03264415264129639
epoch: 23, step: 45
	action: tensor([[ 0.1997, -0.5574, -0.3359, -0.0073, -0.0383,  0.1876, -0.1369]],
       dtype=torch.float64)
	q_value: tensor([[-6.5265]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03351887797104214, distance: 1.1250022240690334 entropy 0.03264415264129639
epoch: 23, step: 46
	action: tensor([[ 0.2492,  0.1178,  0.1226, -0.0834, -0.2774,  0.0693,  0.0154]],
       dtype=torch.float64)
	q_value: tensor([[-5.0427]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5823873351867557, distance: 0.7395091019861753 entropy 0.03264415264129639
epoch: 23, step: 47
	action: tensor([[ 0.3091, -0.4626, -0.4656, -0.0674, -0.5034,  0.0983, -0.2324]],
       dtype=torch.float64)
	q_value: tensor([[-5.8164]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1402927021882453, distance: 1.0610405944903791 entropy 0.03264415264129639
epoch: 23, step: 48
	action: tensor([[ 0.3039,  0.6097, -0.3264, -0.2695, -0.2249,  0.3849, -0.2402]],
       dtype=torch.float64)
	q_value: tensor([[-6.0280]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8050680389178606, distance: 0.5052405732855514 entropy 0.03264415264129639
epoch: 23, step: 49
	action: tensor([[-0.0084,  0.1722, -0.1142, -0.3603, -0.2304, -0.1969, -0.3575]],
       dtype=torch.float64)
	q_value: tensor([[-7.1099]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2896893787188376, distance: 0.9644525098895571 entropy 0.03264415264129639
epoch: 23, step: 50
	action: tensor([[-0.0256, -0.0476, -0.4373, -0.1969,  0.1181,  0.4272, -0.4837]],
       dtype=torch.float64)
	q_value: tensor([[-5.6469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.294705747023307, distance: 0.9610408889793126 entropy 0.03264415264129639
epoch: 23, step: 51
	action: tensor([[ 0.5452,  0.1405,  0.0116, -0.6640, -0.0280,  0.2834, -0.2690]],
       dtype=torch.float64)
	q_value: tensor([[-5.0218]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5525959031983952, distance: 0.7654321003858103 entropy 0.03264415264129639
epoch: 23, step: 52
	action: tensor([[ 0.1886,  0.3153, -0.5235, -0.0928, -0.1390, -0.0262, -0.1610]],
       dtype=torch.float64)
	q_value: tensor([[-6.5773]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6809079966395809, distance: 0.6464198045759274 entropy 0.03264415264129639
epoch: 23, step: 53
	action: tensor([[ 0.1443,  0.1382, -0.1388,  0.5777, -0.0205, -0.1531,  0.3137]],
       dtype=torch.float64)
	q_value: tensor([[-5.6141]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.683587201708737, distance: 0.6437003037289641 entropy 0.03264415264129639
epoch: 23, step: 54
	action: tensor([[ 0.2071, -0.1219, -0.0605, -0.3693, -0.3588,  0.1322, -0.0748]],
       dtype=torch.float64)
	q_value: tensor([[-6.1507]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2434817136514772, distance: 0.9953284280432304 entropy 0.03264415264129639
epoch: 23, step: 55
	action: tensor([[ 0.4303,  0.5548, -0.1455, -0.3391, -0.4325, -0.6093,  0.0294]],
       dtype=torch.float64)
	q_value: tensor([[-5.9384]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8282869902902112, distance: 0.47419644465003785 entropy 0.03264415264129639
epoch: 23, step: 56
	action: tensor([[ 0.2330,  0.4285,  0.1045, -0.1770,  0.4249,  0.0267, -0.6190]],
       dtype=torch.float64)
	q_value: tensor([[-7.9252]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7142662436141243, distance: 0.6116985747521537 entropy 0.03264415264129639
epoch: 23, step: 57
	action: tensor([[ 0.4463, -0.0280,  0.0426, -0.5482, -0.3067, -0.1988, -0.3152]],
       dtype=torch.float64)
	q_value: tensor([[-5.6493]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2995884718855527, distance: 0.9577084869690504 entropy 0.03264415264129639
epoch: 23, step: 58
	action: tensor([[ 0.4187, -0.2039, -0.3023,  0.1002, -0.0235,  0.2423, -0.5248]],
       dtype=torch.float64)
	q_value: tensor([[-6.6426]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5596405549162333, distance: 0.7593820918440459 entropy 0.03264415264129639
epoch: 23, step: 59
	action: tensor([[ 0.2941, -0.2197,  0.2024, -0.2133, -0.2450,  0.1779, -0.8131]],
       dtype=torch.float64)
	q_value: tensor([[-5.4097]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3067460648081072, distance: 0.952802449583636 entropy 0.03264415264129639
epoch: 23, step: 60
	action: tensor([[ 0.2838,  0.1002, -0.2152, -0.4125, -0.0606,  0.0148,  0.3174]],
       dtype=torch.float64)
	q_value: tensor([[-6.4575]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4753374437289717, distance: 0.8288896727178422 entropy 0.03264415264129639
epoch: 23, step: 61
	action: tensor([[ 0.5954, -0.0076, -0.1604, -0.6447,  0.2796, -0.0366, -0.3213]],
       dtype=torch.float64)
	q_value: tensor([[-6.1560]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3603531102812948, distance: 0.9152228185293614 entropy 0.03264415264129639
epoch: 23, step: 62
	action: tensor([[ 0.0613, -0.0228, -0.1913, -0.7636,  0.0888, -0.0687,  0.0090]],
       dtype=torch.float64)
	q_value: tensor([[-6.0787]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06327153652180362, distance: 1.1075505363268452 entropy 0.03264415264129639
epoch: 23, step: 63
	action: tensor([[ 0.1478,  0.1497,  0.1248, -0.7548, -0.2955,  0.2732, -0.2868]],
       dtype=torch.float64)
	q_value: tensor([[-5.8099]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2429697233495045, distance: 0.9956651762225603 entropy 0.03264415264129639
epoch: 23, step: 64
	action: tensor([[-0.2586, -0.2110, -0.3458, -0.0403, -0.5543, -0.1014, -0.3756]],
       dtype=torch.float64)
	q_value: tensor([[-6.9463]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12840372110614395, distance: 1.2155951204333104 entropy 0.03264415264129639
epoch: 23, step: 65
	action: tensor([[ 0.3319,  0.0365,  0.0385, -0.1051,  0.1667,  0.0654, -0.4790]],
       dtype=torch.float64)
	q_value: tensor([[-5.9906]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5654078431656564, distance: 0.7543929747504201 entropy 0.03264415264129639
epoch: 23, step: 66
	action: tensor([[-0.2874,  0.2296, -0.2940,  0.4656, -0.0712, -0.5435, -0.1655]],
       dtype=torch.float64)
	q_value: tensor([[-5.1690]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23965524438152708, distance: 0.997842438535035 entropy 0.03264415264129639
epoch: 23, step: 67
	action: tensor([[ 0.3135, -0.0375, -0.1503,  0.0537, -0.4042, -0.0752, -0.4951]],
       dtype=torch.float64)
	q_value: tensor([[-5.9813]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5712377070049385, distance: 0.7493159647770028 entropy 0.03264415264129639
epoch: 23, step: 68
	action: tensor([[ 0.0284,  0.1040, -0.3064, -0.4847, -0.2039,  0.1769, -0.1824]],
       dtype=torch.float64)
	q_value: tensor([[-5.9605]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3128084467444314, distance: 0.948627258028179 entropy 0.03264415264129639
epoch: 23, step: 69
	action: tensor([[ 0.3099,  0.3397, -0.0277, -0.2580, -0.4827,  0.1176,  0.0005]],
       dtype=torch.float64)
	q_value: tensor([[-5.7074]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6982199033953274, distance: 0.6286399687567757 entropy 0.03264415264129639
epoch: 23, step: 70
	action: tensor([[ 0.2110, -0.0557, -0.6730, -0.4796,  0.0095,  0.1421, -0.1296]],
       dtype=torch.float64)
	q_value: tensor([[-6.9442]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.416195388441996, distance: 0.8743603472491093 entropy 0.03264415264129639
epoch: 23, step: 71
	action: tensor([[-0.0020, -0.0306, -0.0903, -0.3887, -0.5174, -0.2085, -0.7315]],
       dtype=torch.float64)
	q_value: tensor([[-5.8746]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12060668480689762, distance: 1.0731199610885724 entropy 0.03264415264129639
epoch: 23, step: 72
	action: tensor([[ 0.1042, -0.1793, -0.2923,  0.3838,  0.2692,  0.2164, -0.1873]],
       dtype=torch.float64)
	q_value: tensor([[-6.6092]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4411947278971756, distance: 0.8554348528959742 entropy 0.03264415264129639
epoch: 23, step: 73
	action: tensor([[ 0.2629, -0.1018, -0.3697,  0.0277, -0.2719,  0.0969, -0.8269]],
       dtype=torch.float64)
	q_value: tensor([[-5.0833]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4875644622884323, distance: 0.8191742904157304 entropy 0.03264415264129639
epoch: 23, step: 74
	action: tensor([[ 0.3562,  0.0318, -0.4269, -0.3086, -0.0675, -0.1891,  0.1008]],
       dtype=torch.float64)
	q_value: tensor([[-5.9446]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5141175610405164, distance: 0.7976682293102509 entropy 0.03264415264129639
epoch: 23, step: 75
	action: tensor([[ 0.1976,  0.0434, -0.2962, -0.6706,  0.2004,  0.5654,  0.3100]],
       dtype=torch.float64)
	q_value: tensor([[-5.9749]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4374385801272366, distance: 0.858305046206633 entropy 0.03264415264129639
epoch: 23, step: 76
	action: tensor([[ 0.2005,  0.2223, -0.0318,  0.0329, -0.1427,  0.1063, -0.3872]],
       dtype=torch.float64)
	q_value: tensor([[-6.9538]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6719297813686067, distance: 0.6554507991241044 entropy 0.03264415264129639
epoch: 23, step: 77
	action: tensor([[-0.0838, -0.4039, -0.2300,  0.1816, -0.2450,  0.2521,  0.1310]],
       dtype=torch.float64)
	q_value: tensor([[-5.4312]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.01843882642382133, distance: 1.1337449846148198 entropy 0.03264415264129639
epoch: 23, step: 78
	action: tensor([[ 0.4210,  0.0188, -0.3788, -0.0579, -0.3887,  0.1742, -0.6302]],
       dtype=torch.float64)
	q_value: tensor([[-5.2571]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6443138882619879, distance: 0.6824803429925609 entropy 0.03264415264129639
epoch: 23, step: 79
	action: tensor([[-0.1146,  0.1082, -0.2237,  0.2721,  0.0984, -0.3379, -0.2932]],
       dtype=torch.float64)
	q_value: tensor([[-6.2541]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30813200672483254, distance: 0.9518495595662592 entropy 0.03264415264129639
epoch: 23, step: 80
	action: tensor([[ 0.3248, -0.0638, -0.6019, -0.3871,  0.1604,  0.1750, -0.2450]],
       dtype=torch.float64)
	q_value: tensor([[-5.2061]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4682600375977126, distance: 0.8344615755736674 entropy 0.03264415264129639
epoch: 23, step: 81
	action: tensor([[ 0.2035, -0.3598, -0.4510,  0.0931,  0.0789,  0.1339,  0.3957]],
       dtype=torch.float64)
	q_value: tensor([[-5.4295]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24610044136163278, distance: 0.9936042438934363 entropy 0.03264415264129639
epoch: 23, step: 82
	action: tensor([[ 0.4275,  0.2934, -0.1158,  0.1205,  0.4492, -0.1511, -0.2578]],
       dtype=torch.float64)
	q_value: tensor([[-5.6764]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8139245258086585, distance: 0.49362967574589 entropy 0.03264415264129639
epoch: 23, step: 83
	action: tensor([[-0.0737,  0.3458, -0.5386,  0.0022, -0.0849,  0.1705, -0.2046]],
       dtype=torch.float64)
	q_value: tensor([[-5.5889]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.45962320970818826, distance: 0.8412111818930963 entropy 0.03264415264129639
epoch: 23, step: 84
	action: tensor([[-0.1284, -0.2459, -0.1038, -0.0742, -0.2225, -0.3241, -0.0301]],
       dtype=torch.float64)
	q_value: tensor([[-5.2317]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07763383383849387, distance: 1.1879339723063926 entropy 0.03264415264129639
epoch: 23, step: 85
	action: tensor([[ 0.5714,  0.1717, -0.3270, -0.0276, -0.2667, -0.1934,  0.0023]],
       dtype=torch.float64)
	q_value: tensor([[-5.5098]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7700631923793986, distance: 0.5487328271203002 entropy 0.03264415264129639
epoch: 23, step: 86
	action: tensor([[ 0.2655, -0.0026, -0.0263, -0.1033, -0.2243,  0.3962,  0.1315]],
       dtype=torch.float64)
	q_value: tensor([[-6.4300]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5704873558507856, distance: 0.7499713446146782 entropy 0.03264415264129639
epoch: 23, step: 87
	action: tensor([[ 0.0031, -0.0684, -0.1511,  0.1267, -0.3626,  0.5430, -0.2725]],
       dtype=torch.float64)
	q_value: tensor([[-5.9282]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3837601588070817, distance: 0.898321056798651 entropy 0.03264415264129639
epoch: 23, step: 88
	action: tensor([[ 0.2318,  0.0179, -0.2673,  0.2682, -0.1435,  0.1220, -0.3535]],
       dtype=torch.float64)
	q_value: tensor([[-5.8402]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6348400339776155, distance: 0.6915096933275042 entropy 0.03264415264129639
epoch: 23, step: 89
	action: tensor([[ 0.0517,  0.1523, -0.2946, -0.2623,  0.2231,  0.3814, -0.2298]],
       dtype=torch.float64)
	q_value: tensor([[-5.1556]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.49345004497638967, distance: 0.8144563878060925 entropy 0.03264415264129639
epoch: 23, step: 90
	action: tensor([[-0.2063, -0.0315, -0.2220, -0.0608, -0.3651,  0.0764, -0.0822]],
       dtype=torch.float64)
	q_value: tensor([[-4.9149]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.061158933951212635, distance: 1.1087987616353778 entropy 0.03264415264129639
epoch: 23, step: 91
	action: tensor([[-0.0605,  0.5283, -0.1860, -0.3047, -0.0886,  0.0696,  0.5093]],
       dtype=torch.float64)
	q_value: tensor([[-5.2922]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4990247542174625, distance: 0.8099623407090326 entropy 0.03264415264129639
epoch: 23, step: 92
	action: tensor([[-0.0236,  0.4006, -0.1867, -0.3120, -0.1003,  0.4783, -0.0046]],
       dtype=torch.float64)
	q_value: tensor([[-6.9991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5111083399792036, distance: 0.8001345202932622 entropy 0.03264415264129639
epoch: 23, step: 93
	action: tensor([[ 0.4551,  0.6221, -0.4075, -0.0399, -0.1516, -0.0968, -0.3644]],
       dtype=torch.float64)
	q_value: tensor([[-6.3170]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9108251476902386, distance: 0.3417258973712022 entropy 0.03264415264129639
epoch: 23, step: 94
	action: tensor([[-0.0454, -0.0107,  0.3233, -0.2206, -0.4085,  0.3659, -0.0936]],
       dtype=torch.float64)
	q_value: tensor([[-6.7466]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21277852904846206, distance: 1.01532518425104 entropy 0.03264415264129639
epoch: 23, step: 95
	action: tensor([[ 0.2888,  0.0524, -0.3413, -0.2132, -0.5558,  0.0446, -0.0943]],
       dtype=torch.float64)
	q_value: tensor([[-6.5435]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5364366747997522, distance: 0.779132334087596 entropy 0.03264415264129639
epoch: 23, step: 96
	action: tensor([[ 0.1791, -0.0308, -0.6722, -0.1449, -0.2090,  0.2431, -0.0252]],
       dtype=torch.float64)
	q_value: tensor([[-6.4806]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4495909277406114, distance: 0.8489839599559854 entropy 0.03264415264129639
epoch: 23, step: 97
	action: tensor([[-0.0868, -0.1596,  0.3078, -0.1146,  0.0183,  0.0533, -0.4190]],
       dtype=torch.float64)
	q_value: tensor([[-5.6442]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.044894141396319864, distance: 1.1183621176953125 entropy 0.03264415264129639
epoch: 23, step: 98
	action: tensor([[-0.2036,  0.2158, -0.4639, -0.1412, -0.1554,  0.0689, -0.0994]],
       dtype=torch.float64)
	q_value: tensor([[-5.2211]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24339846871418858, distance: 0.9953831879709735 entropy 0.03264415264129639
epoch: 23, step: 99
	action: tensor([[ 0.0922,  0.3801,  0.0580, -0.1441, -0.1641,  0.0967, -0.3844]],
       dtype=torch.float64)
	q_value: tensor([[-5.2203]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5906337350417361, distance: 0.7321713291844099 entropy 0.03264415264129639
epoch: 23, step: 100
	action: tensor([[ 0.1886,  0.0907, -0.2993, -0.2395,  0.6126,  0.3228, -0.4770]],
       dtype=torch.float64)
	q_value: tensor([[-5.8598]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5645397746494223, distance: 0.7551460234353542 entropy 0.03264415264129639
epoch: 23, step: 101
	action: tensor([[ 0.4078, -0.2665, -0.3257, -0.3106,  0.0399, -0.3504,  0.1417]],
       dtype=torch.float64)
	q_value: tensor([[-4.9593]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16985202490454565, distance: 1.0426401619855477 entropy 0.03264415264129639
epoch: 23, step: 102
	action: tensor([[-0.1958,  0.3725, -0.1022, -0.2235, -0.2524, -0.2719, -0.6413]],
       dtype=torch.float64)
	q_value: tensor([[-6.0822]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2770889753841098, distance: 0.9729692551831387 entropy 0.03264415264129639
epoch: 23, step: 103
	action: tensor([[ 0.0375, -0.4081, -0.0119,  0.2428, -0.3247, -0.2723,  0.0011]],
       dtype=torch.float64)
	q_value: tensor([[-6.0989]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11088506527713038, distance: 1.0790352833712302 entropy 0.03264415264129639
epoch: 23, step: 104
	action: tensor([[-0.1066,  0.2518, -0.3444, -0.3236,  0.5213,  0.7992, -0.3561]],
       dtype=torch.float64)
	q_value: tensor([[-5.7803]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.513464853946862, distance: 0.7982038207788938 entropy 0.03264415264129639
epoch: 23, step: 105
	action: tensor([[-0.2964, -0.3208,  0.1139, -0.1676, -0.3844,  0.3636, -0.4124]],
       dtype=torch.float64)
	q_value: tensor([[-6.2474]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.28271613980417687, distance: 1.296050646143558 entropy 0.03264415264129639
epoch: 23, step: 106
	action: tensor([[ 0.1302,  0.3202, -0.2079, -0.0745,  0.0201, -0.3515, -0.4334]],
       dtype=torch.float64)
	q_value: tensor([[-5.9941]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.565225369728567, distance: 0.754551332684764 entropy 0.03264415264129639
epoch: 23, step: 107
	action: tensor([[-0.0530,  0.0568, -0.8682, -0.1848, -0.4562,  0.5131,  0.1734]],
       dtype=torch.float64)
	q_value: tensor([[-5.6426]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2410726427116736, distance: 0.9969119396086612 entropy 0.03264415264129639
epoch: 23, step: 108
	action: tensor([[ 0.1997,  0.4026, -0.5312,  0.0462, -0.0128,  0.2559, -0.1355]],
       dtype=torch.float64)
	q_value: tensor([[-7.2315]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6897589745595131, distance: 0.6373915576467626 entropy 0.03264415264129639
epoch: 23, step: 109
	action: tensor([[ 0.2039,  0.1023, -0.2600, -0.2411, -0.3124,  0.1494,  0.0618]],
       dtype=torch.float64)
	q_value: tensor([[-5.5652]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5140735759508381, distance: 0.7977043334290491 entropy 0.03264415264129639
epoch: 23, step: 110
	action: tensor([[ 8.0778e-02, -1.7030e-04, -4.0803e-01, -2.0284e-01, -1.8791e-01,
          2.6788e-01, -1.1420e-02]], dtype=torch.float64)
	q_value: tensor([[-5.8651]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3737012774027467, distance: 0.9056230261003522 entropy 0.03264415264129639
epoch: 23, step: 111
	action: tensor([[ 0.3771, -0.0649,  0.0720, -0.4770, -0.0223,  0.4640, -0.4107]],
       dtype=torch.float64)
	q_value: tensor([[-5.2987]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.43839941899659185, distance: 0.857571752685227 entropy 0.03264415264129639
epoch: 23, step: 112
	action: tensor([[ 0.2571,  0.2221, -0.1501, -0.1501,  0.1085, -0.4040, -0.3946]],
       dtype=torch.float64)
	q_value: tensor([[-6.0182]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5732713331462223, distance: 0.7475368441053621 entropy 0.03264415264129639
epoch: 23, step: 113
	action: tensor([[-0.0546,  0.0497, -0.3736,  0.3175,  0.3081,  0.3191,  0.1386]],
       dtype=torch.float64)
	q_value: tensor([[-5.7052]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42530616491950757, distance: 0.8675109442917565 entropy 0.03264415264129639
epoch: 23, step: 114
	action: tensor([[-0.0650,  0.1172, -0.3597,  0.1976, -0.2772, -0.2594, -0.3157]],
       dtype=torch.float64)
	q_value: tensor([[-5.2465]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3824757035401807, distance: 0.8992567741159861 entropy 0.03264415264129639
epoch: 23, step: 115
	action: tensor([[-0.1943,  0.2749, -0.0856, -0.5747, -0.0398,  0.0097, -0.0434]],
       dtype=torch.float64)
	q_value: tensor([[-5.5773]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11258121633838447, distance: 1.0780055624276657 entropy 0.03264415264129639
epoch: 23, step: 116
	action: tensor([[ 0.3289,  0.0116, -0.1881, -0.4318, -0.0220,  0.4417, -0.3590]],
       dtype=torch.float64)
	q_value: tensor([[-5.7176]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.49031812619593773, distance: 0.816970335894803 entropy 0.03264415264129639
epoch: 23, step: 117
	action: tensor([[ 0.3371,  0.3725, -0.1209, -0.4066, -0.3472,  0.1854, -0.3222]],
       dtype=torch.float64)
	q_value: tensor([[-5.7420]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7050657284441375, distance: 0.6214687761106413 entropy 0.03264415264129639
epoch: 23, step: 118
	action: tensor([[ 0.2275, -0.2823, -0.5488,  0.0485, -0.2000, -0.0903, -0.5450]],
       dtype=torch.float64)
	q_value: tensor([[-6.7862]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29499570799265495, distance: 0.9608433168357534 entropy 0.03264415264129639
epoch: 23, step: 119
	action: tensor([[ 0.1332,  0.1502, -0.2270,  0.0960, -0.2513,  0.0287, -0.3856]],
       dtype=torch.float64)
	q_value: tensor([[-5.4870]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5649562276508844, distance: 0.7547848445782636 entropy 0.03264415264129639
epoch: 23, step: 120
	action: tensor([[ 0.1416,  0.3387,  0.3158, -0.3820, -0.0779,  0.7343,  0.0290]],
       dtype=torch.float64)
	q_value: tensor([[-5.3124]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6397814771771497, distance: 0.6868149085669204 entropy 0.03264415264129639
epoch: 23, step: 121
	action: tensor([[-0.1176, -0.7575, -0.1724,  0.2722, -0.2790,  0.4123, -0.2982]],
       dtype=torch.float64)
	q_value: tensor([[-7.2513]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21784198913819708, distance: 1.2628511531077597 entropy 0.03264415264129639
epoch: 23, step: 122
	action: tensor([[-0.0380, -0.1231, -0.4065, -0.1987, -0.1188,  0.0662, -0.5094]],
       dtype=torch.float64)
	q_value: tensor([[-5.8448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.147549943108216, distance: 1.0565527027342798 entropy 0.03264415264129639
epoch: 23, step: 123
	action: tensor([[ 0.1680,  0.2811, -0.0658,  0.0566, -0.3726, -0.3765,  0.2333]],
       dtype=torch.float64)
	q_value: tensor([[-5.0092]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.639726698084209, distance: 0.6868671291845193 entropy 0.03264415264129639
epoch: 23, step: 124
	action: tensor([[ 0.1509,  0.2609, -0.1685,  0.0539, -0.3341,  0.1378, -0.3161]],
       dtype=torch.float64)
	q_value: tensor([[-6.4351]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6410951401468379, distance: 0.6855614091699062 entropy 0.03264415264129639
epoch: 23, step: 125
	action: tensor([[ 0.7948, -0.1489, -0.4873, -0.3579,  0.1507,  0.1065, -0.1606]],
       dtype=torch.float64)
	q_value: tensor([[-5.6861]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.47520980191425854, distance: 0.8289904942331466 entropy 0.03264415264129639
epoch: 23, step: 126
	action: tensor([[ 0.3149,  0.6153,  0.2568,  0.1854, -0.1645, -0.0480, -0.5347]],
       dtype=torch.float64)
	q_value: tensor([[-6.5102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8707001356410291, distance: 0.4114866292491775 entropy 0.03264415264129639
epoch: 23, step: 127
	action: tensor([[ 0.2061, -0.0723, -0.4465, -0.2192, -0.3489,  0.0343, -0.3462]],
       dtype=torch.float64)
	q_value: tensor([[-7.0407]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39365016712491585, distance: 0.8910833407115738 entropy 0.03264415264129639
LOSS epoch 23 actor 20.151021832779893 critic 50.983004757721716 
epoch: 24, step: 0
	action: tensor([[-0.4456,  0.1348,  0.3178, -0.0021, -0.4499, -0.2253, -0.6657]],
       dtype=torch.float64)
	q_value: tensor([[-6.4376]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13248005469122948, distance: 1.217788796365023 entropy 0.03264415264129639
epoch: 24, step: 1
	action: tensor([[ 0.0166,  0.4111,  0.0259, -0.3214, -0.4158, -0.3799, -0.0629]],
       dtype=torch.float64)
	q_value: tensor([[-7.2824]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4545579022611572, distance: 0.8451445989028433 entropy 0.03264415264129639
epoch: 24, step: 2
	action: tensor([[ 0.1921,  0.3317, -0.2863, -0.2484,  0.4512,  0.1325, -0.0396]],
       dtype=torch.float64)
	q_value: tensor([[-7.6856]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6551764022973129, distance: 0.6719781977281439 entropy 0.03264415264129639
epoch: 24, step: 3
	action: tensor([[ 0.1401,  0.1243,  0.1451, -0.0200, -0.0986,  0.0277, -0.0223]],
       dtype=torch.float64)
	q_value: tensor([[-5.9177]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.517371758688882, distance: 0.7949925501995142 entropy 0.03264415264129639
epoch: 24, step: 4
	action: tensor([[ 0.5159,  0.0039, -0.1731,  0.0293,  0.2086, -0.1804, -0.2293]],
       dtype=torch.float64)
	q_value: tensor([[-5.9231]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6390881662428252, distance: 0.6874755455864877 entropy 0.03264415264129639
epoch: 24, step: 5
	action: tensor([[ 0.0265,  0.1265,  0.3418, -0.0296, -0.1652,  0.0605, -0.2455]],
       dtype=torch.float64)
	q_value: tensor([[-6.0952]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.43032252440723706, distance: 0.8637165023719223 entropy 0.03264415264129639
epoch: 24, step: 6
	action: tensor([[-0.0006, -0.1280, -0.3559, -0.4678, -0.2470,  0.0161, -0.4507]],
       dtype=torch.float64)
	q_value: tensor([[-6.3738]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09141680528806073, distance: 1.0907847138682454 entropy 0.03264415264129639
epoch: 24, step: 7
	action: tensor([[ 0.2706, -0.1420,  0.2476,  0.2952,  0.2967,  0.1950, -0.2452]],
       dtype=torch.float64)
	q_value: tensor([[-6.3986]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6502935421279045, distance: 0.6767192325385103 entropy 0.03264415264129639
epoch: 24, step: 8
	action: tensor([[ 0.1724,  0.2084, -0.1842, -0.2677, -0.5004,  0.2328, -0.3352]],
       dtype=torch.float64)
	q_value: tensor([[-5.9861]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5426353586562043, distance: 0.7739055951198045 entropy 0.03264415264129639
epoch: 24, step: 9
	action: tensor([[-0.0109, -0.0212,  0.0863, -0.4436,  0.0387, -0.1006, -0.1474]],
       dtype=torch.float64)
	q_value: tensor([[-7.5379]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.031815029971386855, distance: 1.1259934429734129 entropy 0.03264415264129639
epoch: 24, step: 10
	action: tensor([[-0.0820, -0.3923, -0.1569, -0.0414,  0.1344,  0.4668,  0.0049]],
       dtype=torch.float64)
	q_value: tensor([[-5.7572]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03668223219496303, distance: 1.1231596131475958 entropy 0.03264415264129639
epoch: 24, step: 11
	action: tensor([[ 0.3511,  0.5692,  0.1373, -0.3662, -0.4880,  0.1991, -0.1277]],
       dtype=torch.float64)
	q_value: tensor([[-5.6036]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7797156626258779, distance: 0.5370917764371419 entropy 0.03264415264129639
epoch: 24, step: 12
	action: tensor([[-0.0664,  0.3209, -0.4583,  0.0215,  0.3153, -0.2601, -0.1625]],
       dtype=torch.float64)
	q_value: tensor([[-8.9311]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4332757233357052, distance: 0.8614748471489512 entropy 0.03264415264129639
epoch: 24, step: 13
	action: tensor([[ 0.1621, -0.0947, -0.6709,  0.1965,  0.2456, -0.4694,  0.0565]],
       dtype=torch.float64)
	q_value: tensor([[-6.0113]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3706898179183855, distance: 0.907797688466615 entropy 0.03264415264129639
epoch: 24, step: 14
	action: tensor([[-0.3462, -0.1458,  0.2335, -0.1050, -0.1137,  0.2132, -0.4335]],
       dtype=torch.float64)
	q_value: tensor([[-6.9686]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1803340261295785, distance: 1.2432519719345565 entropy 0.03264415264129639
epoch: 24, step: 15
	action: tensor([[ 0.2405, -0.3365,  0.1446, -0.1838, -0.1390,  0.1608, -0.1123]],
       dtype=torch.float64)
	q_value: tensor([[-6.1033]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13563981077041898, distance: 1.063907991827821 entropy 0.03264415264129639
epoch: 24, step: 16
	action: tensor([[ 0.4402,  0.1258, -0.4537,  0.1459,  0.0692,  0.2116,  0.1766]],
       dtype=torch.float64)
	q_value: tensor([[-5.9648]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7633470301962776, distance: 0.5566890420985284 entropy 0.03264415264129639
epoch: 24, step: 17
	action: tensor([[ 0.3375,  0.4066,  0.0614, -0.3057, -0.0011, -0.1150, -0.2731]],
       dtype=torch.float64)
	q_value: tensor([[-6.2875]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6899748893006068, distance: 0.6371697201478201 entropy 0.03264415264129639
epoch: 24, step: 18
	action: tensor([[-0.0524,  0.2927, -0.0767, -0.6027,  0.0259, -0.0627, -0.1551]],
       dtype=torch.float64)
	q_value: tensor([[-6.7473]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2482752269830628, distance: 0.9921700761530324 entropy 0.03264415264129639
epoch: 24, step: 19
	action: tensor([[-0.2304, -0.1689, -0.2468,  0.3006,  0.2346,  0.0920, -0.1798]],
       dtype=torch.float64)
	q_value: tensor([[-6.4523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07162549102076354, distance: 1.1026007838352487 entropy 0.03264415264129639
epoch: 24, step: 20
	action: tensor([[-0.0215,  0.0055, -0.1184,  0.2430, -0.2326,  0.3573, -0.3688]],
       dtype=torch.float64)
	q_value: tensor([[-5.4795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.43333151046375573, distance: 0.8614324452329236 entropy 0.03264415264129639
epoch: 24, step: 21
	action: tensor([[ 0.1252,  0.0034, -0.1826, -0.1626, -0.3266,  0.1262,  0.0164]],
       dtype=torch.float64)
	q_value: tensor([[-5.8709]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39382489959759903, distance: 0.8909549392461095 entropy 0.03264415264129639
epoch: 24, step: 22
	action: tensor([[ 0.3740, -0.2191, -0.3277,  0.0979, -0.3836,  0.0903, -0.3983]],
       dtype=torch.float64)
	q_value: tensor([[-6.2385]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4975419644437129, distance: 0.8111601209526752 entropy 0.03264415264129639
epoch: 24, step: 23
	action: tensor([[ 0.1964, -0.3221, -0.4714, -0.2511,  0.1138,  0.2276, -0.3182]],
       dtype=torch.float64)
	q_value: tensor([[-6.3643]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19372017485509097, distance: 1.0275420174569005 entropy 0.03264415264129639
epoch: 24, step: 24
	action: tensor([[ 0.0023,  0.0788, -0.1671, -0.3861, -0.0572,  0.2448, -0.2901]],
       dtype=torch.float64)
	q_value: tensor([[-5.5327]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2903711350246747, distance: 0.963989557901584 entropy 0.03264415264129639
epoch: 24, step: 25
	action: tensor([[-0.2928, -0.2448, -0.6085,  0.1964, -0.2966,  0.0682, -0.0166]],
       dtype=torch.float64)
	q_value: tensor([[-5.9081]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18791978918364993, distance: 1.2472406354688221 entropy 0.03264415264129639
epoch: 24, step: 26
	action: tensor([[-0.2480, -0.0181, -0.2746, -0.1619,  0.4865, -0.0576, -0.0731]],
       dtype=torch.float64)
	q_value: tensor([[-6.2411]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03263696814653083, distance: 1.1628682888812905 entropy 0.03264415264129639
epoch: 24, step: 27
	action: tensor([[ 0.4764,  0.1791, -0.2820, -0.3192,  0.3925,  0.0017, -0.1127]],
       dtype=torch.float64)
	q_value: tensor([[-5.5148]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6631607514840168, distance: 0.6641528462339968 entropy 0.03264415264129639
epoch: 24, step: 28
	action: tensor([[ 0.2192, -0.4166, -0.0941, -0.3094,  0.0400,  0.3169, -0.2182]],
       dtype=torch.float64)
	q_value: tensor([[-6.3074]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05006882588139239, distance: 1.1153284063233961 entropy 0.03264415264129639
epoch: 24, step: 29
	action: tensor([[-0.0909, -0.3379, -0.2419,  0.0659,  0.0075,  0.0903, -0.4791]],
       dtype=torch.float64)
	q_value: tensor([[-5.7518]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0020466690553310185, distance: 1.143172607212985 entropy 0.03264415264129639
epoch: 24, step: 30
	action: tensor([[ 0.2964,  0.0270, -0.2640, -0.1419, -0.2653,  0.6104, -0.8573]],
       dtype=torch.float64)
	q_value: tensor([[-5.3888]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5926775356970064, distance: 0.7303413241334828 entropy 0.03264415264129639
epoch: 24, step: 31
	action: tensor([[ 0.1779,  0.1874,  0.1188,  0.0276, -0.2456,  0.2111, -0.5451]],
       dtype=torch.float64)
	q_value: tensor([[-7.9932]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6479337180455993, distance: 0.6789986490562638 entropy 0.03264415264129639
epoch: 24, step: 32
	action: tensor([[-0.1771,  0.1623, -0.3471, -0.5599,  0.0730,  0.0950, -0.4290]],
       dtype=torch.float64)
	q_value: tensor([[-6.6910]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16301512914160288, distance: 1.0469248229511054 entropy 0.03264415264129639
epoch: 24, step: 33
	action: tensor([[ 0.5331,  0.2347, -0.0906, -0.1667, -0.1681,  0.3311, -0.2868]],
       dtype=torch.float64)
	q_value: tensor([[-6.2383]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8093607649675096, distance: 0.49964648511388626 entropy 0.03264415264129639
epoch: 24, step: 34
	action: tensor([[ 0.2770, -0.0109, -0.2237, -0.0865,  0.0724,  0.0643,  0.1021]],
       dtype=torch.float64)
	q_value: tensor([[-7.0310]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5174091479571433, distance: 0.7949617555185494 entropy 0.03264415264129639
epoch: 24, step: 35
	action: tensor([[ 0.0859,  0.4177, -0.0297,  0.1531, -0.2184, -0.3875, -0.3065]],
       dtype=torch.float64)
	q_value: tensor([[-5.6046]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6627842158786086, distance: 0.6645239540354271 entropy 0.03264415264129639
epoch: 24, step: 36
	action: tensor([[ 0.4595, -0.5699, -0.5280,  0.3228, -0.3304, -0.0051, -0.3422]],
       dtype=torch.float64)
	q_value: tensor([[-6.8361]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2847383155734837, distance: 0.9678079244954088 entropy 0.03264415264129639
epoch: 24, step: 37
	action: tensor([[ 0.0786,  0.2105,  0.0116, -0.2115, -0.2077,  0.0890,  0.0514]],
       dtype=torch.float64)
	q_value: tensor([[-6.8439]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.46537038105411255, distance: 0.8367258780775655 entropy 0.03264415264129639
epoch: 24, step: 38
	action: tensor([[-0.0775,  0.2148, -0.4318, -0.3054,  0.1821,  0.1353,  0.3759]],
       dtype=torch.float64)
	q_value: tensor([[-6.2831]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3652668769670101, distance: 0.9117006708747556 entropy 0.03264415264129639
epoch: 24, step: 39
	action: tensor([[ 0.0696,  0.1929, -0.2497, -0.1060, -0.0567, -0.2877, -0.2813]],
       dtype=torch.float64)
	q_value: tensor([[-6.8853]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.46113179622806977, distance: 0.8400361436465607 entropy 0.03264415264129639
epoch: 24, step: 40
	action: tensor([[-0.1302,  0.3289, -0.3909, -0.0763,  0.1034,  0.1055, -0.2873]],
       dtype=torch.float64)
	q_value: tensor([[-5.9937]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4035670783822427, distance: 0.8837664222297419 entropy 0.03264415264129639
epoch: 24, step: 41
	action: tensor([[ 0.0835,  0.2602, -0.2526, -0.1360, -0.2199, -0.3690, -0.0879]],
       dtype=torch.float64)
	q_value: tensor([[-5.5482]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5293077167880971, distance: 0.7851004610903204 entropy 0.03264415264129639
epoch: 24, step: 42
	action: tensor([[ 0.4429,  0.1848, -0.2366, -0.1663,  0.2329,  0.3217, -0.0645]],
       dtype=torch.float64)
	q_value: tensor([[-6.5401]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7767829591672815, distance: 0.5406551777340846 entropy 0.03264415264129639
epoch: 24, step: 43
	action: tensor([[-0.1159,  0.2562, -0.2436,  0.1577, -0.2400, -0.1610,  0.0064]],
       dtype=torch.float64)
	q_value: tensor([[-6.0724]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4063145726105739, distance: 0.8817285183412479 entropy 0.03264415264129639
epoch: 24, step: 44
	action: tensor([[-0.0372,  0.1425, -0.5189,  0.2477, -0.0341, -0.0783, -0.4963]],
       dtype=torch.float64)
	q_value: tensor([[-6.1027]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3965910335709517, distance: 0.8889197857924691 entropy 0.03264415264129639
epoch: 24, step: 45
	action: tensor([[ 0.1947, -0.1949, -0.1169,  0.1351,  0.2653, -0.0839,  0.0654]],
       dtype=torch.float64)
	q_value: tensor([[-5.9894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3661606382984015, distance: 0.9110585664839717 entropy 0.03264415264129639
epoch: 24, step: 46
	action: tensor([[ 0.7943, -0.1031, -0.3285, -0.2491,  0.0179,  0.4751,  0.0226]],
       dtype=torch.float64)
	q_value: tensor([[-5.6172]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6556161683974574, distance: 0.6715495618789828 entropy 0.03264415264129639
epoch: 24, step: 47
	action: tensor([[ 0.4900,  0.1642, -0.4101, -0.0993,  0.1419, -0.0388, -0.2512]],
       dtype=torch.float64)
	q_value: tensor([[-7.6607]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7464078496890456, distance: 0.5762681226124562 entropy 0.03264415264129639
epoch: 24, step: 48
	action: tensor([[ 0.3218,  0.2573, -0.4270, -0.0446, -0.3030, -0.2914, -0.1679]],
       dtype=torch.float64)
	q_value: tensor([[-6.3373]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7442360529265238, distance: 0.5787304801261356 entropy 0.03264415264129639
epoch: 24, step: 49
	action: tensor([[ 0.0495, -0.0348, -0.2449, -0.0749, -0.2706,  0.1754,  0.3321]],
       dtype=torch.float64)
	q_value: tensor([[-7.0034]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3327196574614495, distance: 0.9347831143733496 entropy 0.03264415264129639
epoch: 24, step: 50
	action: tensor([[ 0.3169,  0.1058, -0.2000, -0.4948,  0.0511,  0.1298, -0.1587]],
       dtype=torch.float64)
	q_value: tensor([[-6.3697]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4814312768714899, distance: 0.8240619334598688 entropy 0.03264415264129639
epoch: 24, step: 51
	action: tensor([[-0.1479,  0.1672,  0.0014,  0.1945,  0.4215,  0.0157,  0.0240]],
       dtype=torch.float64)
	q_value: tensor([[-6.2436]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3560050993235423, distance: 0.9183281718783627 entropy 0.03264415264129639
epoch: 24, step: 52
	action: tensor([[ 0.1634, -0.1668, -0.0267, -0.1790, -0.0907,  0.2978, -0.3160]],
       dtype=torch.float64)
	q_value: tensor([[-5.5997]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2923942485054287, distance: 0.9626144360788864 entropy 0.03264415264129639
epoch: 24, step: 53
	action: tensor([[ 0.4429, -0.1109,  0.1108, -0.0185, -0.1739,  0.1221, -0.3561]],
       dtype=torch.float64)
	q_value: tensor([[-5.6704]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5638864388139997, distance: 0.7557122967886613 entropy 0.03264415264129639
epoch: 24, step: 54
	action: tensor([[ 0.4151,  0.2665, -0.3360, -0.2030, -0.2351,  0.1399, -0.6101]],
       dtype=torch.float64)
	q_value: tensor([[-6.4042]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7557365577252584, distance: 0.5655694335861533 entropy 0.03264415264129639
epoch: 24, step: 55
	action: tensor([[-0.2645, -0.2016, -0.3779, -0.2732, -0.0532, -0.0360, -0.0398]],
       dtype=torch.float64)
	q_value: tensor([[-7.1886]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1786973171226276, distance: 1.242389695855236 entropy 0.03264415264129639
epoch: 24, step: 56
	action: tensor([[ 0.1088, -0.1551, -0.5034, -0.2520, -0.0097,  0.0668, -0.0046]],
       dtype=torch.float64)
	q_value: tensor([[-5.8350]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23450915533760908, distance: 1.0012134927554908 entropy 0.03264415264129639
epoch: 24, step: 57
	action: tensor([[ 0.6618, -0.1244, -0.2272, -0.2148,  0.2857,  0.1242, -0.6251]],
       dtype=torch.float64)
	q_value: tensor([[-5.7577]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5352075382169734, distance: 0.7801645836581884 entropy 0.03264415264129639
epoch: 24, step: 58
	action: tensor([[ 0.2392,  0.1789, -0.0225, -0.2213, -0.0298,  0.0363, -0.3786]],
       dtype=torch.float64)
	q_value: tensor([[-6.5539]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.572024312671148, distance: 0.7486283031323786 entropy 0.03264415264129639
epoch: 24, step: 59
	action: tensor([[ 0.2800, -0.5497, -0.1171, -0.7607, -0.1508, -0.1825,  0.0501]],
       dtype=torch.float64)
	q_value: tensor([[-5.9595]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3846286815500235, distance: 1.3465526998286772 entropy 0.03264415264129639
epoch: 24, step: 60
	action: tensor([[ 0.2621, -0.2239,  0.1867, -0.0344,  0.2313,  0.2766,  0.1351]],
       dtype=torch.float64)
	q_value: tensor([[-7.4564]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41720263748028097, distance: 0.873605746479477 entropy 0.03264415264129639
epoch: 24, step: 61
	action: tensor([[ 0.3162,  0.0241, -0.3160,  0.0560, -0.1228,  0.0338, -0.6653]],
       dtype=torch.float64)
	q_value: tensor([[-5.8990]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6272038924647737, distance: 0.6987026297365327 entropy 0.03264415264129639
epoch: 24, step: 62
	action: tensor([[-0.0465,  0.2524, -0.3766, -0.2428,  0.4601, -0.2761,  0.0325]],
       dtype=torch.float64)
	q_value: tensor([[-6.2493]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3473466211789078, distance: 0.9244809972207528 entropy 0.03264415264129639
epoch: 24, step: 63
	action: tensor([[-0.0833,  0.2296, -0.1669, -0.3957,  0.1981,  0.4445, -0.2384]],
       dtype=torch.float64)
	q_value: tensor([[-6.0587]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3799354327016432, distance: 0.9011044838657815 entropy 0.03264415264129639
epoch: 24, step: 64
	action: tensor([[ 0.0594,  0.2818, -0.0469,  0.0268, -0.2638, -0.6320, -0.4551]],
       dtype=torch.float64)
	q_value: tensor([[-6.0904]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5327386138705044, distance: 0.7822339117659708 entropy 0.03264415264129639
epoch: 24, step: 65
	action: tensor([[ 0.0977,  0.0466,  0.1763, -0.4890,  0.6139,  0.1018, -0.1010]],
       dtype=torch.float64)
	q_value: tensor([[-7.3194]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19008409534650272, distance: 1.0298563638641798 entropy 0.03264415264129639
epoch: 24, step: 66
	action: tensor([[ 0.0232, -0.2102, -0.4491,  0.1820, -0.0473, -0.0796, -0.2776]],
       dtype=torch.float64)
	q_value: tensor([[-5.5981]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20873287346327818, distance: 1.0179307991878264 entropy 0.03264415264129639
epoch: 24, step: 67
	action: tensor([[ 0.0446,  0.0475, -0.2110, -0.0284,  0.2244, -0.0680, -0.3576]],
       dtype=torch.float64)
	q_value: tensor([[-5.4836]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3764004438592947, distance: 0.9036694322398178 entropy 0.03264415264129639
epoch: 24, step: 68
	action: tensor([[ 0.5796,  0.2977, -0.2070,  0.3360,  0.2338,  0.5410,  0.1291]],
       dtype=torch.float64)
	q_value: tensor([[-5.1498]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9457778766205894, distance: 0.2664679333716767 entropy 0.03264415264129639
epoch: 24, step: 69
	action: tensor([[ 0.1619, -0.1284,  0.3017,  0.2048, -0.2393, -0.0520, -0.0174]],
       dtype=torch.float64)
	q_value: tensor([[-7.2231]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.46002772005042203, distance: 0.8408962698412579 entropy 0.03264415264129639
epoch: 24, step: 70
	action: tensor([[ 0.1550,  0.1204, -0.3028, -0.0984, -0.0271, -0.0491, -0.3666]],
       dtype=torch.float64)
	q_value: tensor([[-6.2878]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5004858569176016, distance: 0.8087803438809466 entropy 0.03264415264129639
epoch: 24, step: 71
	action: tensor([[-0.0966, -0.0129,  0.5369, -0.1097,  0.3824,  0.3776, -0.0122]],
       dtype=torch.float64)
	q_value: tensor([[-5.6147]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25879909139485124, distance: 0.9852005934816122 entropy 0.03264415264129639
epoch: 24, step: 72
	action: tensor([[ 0.3796, -0.0727,  0.2315,  0.2017, -0.2904, -0.2272, -0.1104]],
       dtype=torch.float64)
	q_value: tensor([[-6.2725]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5959298928525736, distance: 0.7274196936623976 entropy 0.03264415264129639
epoch: 24, step: 73
	action: tensor([[ 0.0579, -0.1519,  0.3340, -0.2329, -0.6247, -0.4428,  0.0268]],
       dtype=torch.float64)
	q_value: tensor([[-6.7711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.037728560144034806, distance: 1.1225494756506693 entropy 0.03264415264129639
epoch: 24, step: 74
	action: tensor([[ 4.0280e-04, -9.7043e-02, -1.7676e-01, -2.0161e-01, -3.3468e-01,
         -4.2274e-01, -4.8060e-01]], dtype=torch.float64)
	q_value: tensor([[-7.7580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13873971125903817, distance: 1.0619985036220487 entropy 0.03264415264129639
epoch: 24, step: 75
	action: tensor([[-0.1361,  0.5450,  0.0053, -0.2866, -0.0538,  0.0322, -0.3391]],
       dtype=torch.float64)
	q_value: tensor([[-6.6596]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4034094884338154, distance: 0.8838831692279242 entropy 0.03264415264129639
epoch: 24, step: 76
	action: tensor([[ 0.1765, -0.3245,  0.2045, -0.4554,  0.0959,  0.0631, -0.4812]],
       dtype=torch.float64)
	q_value: tensor([[-6.6824]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0828671389174136, distance: 1.1908149562231969 entropy 0.03264415264129639
epoch: 24, step: 77
	action: tensor([[ 0.5808,  0.2116, -0.0300, -0.0130,  0.1383,  0.2052, -0.8643]],
       dtype=torch.float64)
	q_value: tensor([[-5.9721]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8527466450641524, distance: 0.43912614217966406 entropy 0.03264415264129639
epoch: 24, step: 78
	action: tensor([[ 0.2550, -0.0552, -0.0897, -0.1754, -0.0031, -0.4832, -0.5049]],
       dtype=torch.float64)
	q_value: tensor([[-7.4340]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31816527313431553, distance: 0.9449226334339957 entropy 0.03264415264129639
epoch: 24, step: 79
	action: tensor([[-0.0364,  0.3382, -0.1552, -0.0949, -0.2178,  0.5269, -0.4749]],
       dtype=torch.float64)
	q_value: tensor([[-6.6075]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.47790259740010876, distance: 0.8268609071832876 entropy 0.03264415264129639
epoch: 24, step: 80
	action: tensor([[-0.4381,  0.3996,  0.0388, -0.1934, -0.2822,  0.2123, -0.5835]],
       dtype=torch.float64)
	q_value: tensor([[-7.1729]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03642794994654874, distance: 1.1233078408231947 entropy 0.03264415264129639
epoch: 24, step: 81
	action: tensor([[-0.1287,  0.1371, -0.1148, -0.1029,  0.1789, -0.0529, -0.3835]],
       dtype=torch.float64)
	q_value: tensor([[-7.3865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23678170974087576, distance: 0.9997262095504822 entropy 0.03264415264129639
epoch: 24, step: 82
	action: tensor([[-0.2633, -0.2238, -0.0237, -0.1686, -0.1880,  0.0254,  0.1986]],
       dtype=torch.float64)
	q_value: tensor([[-5.2744]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.22344566967880497, distance: 1.2657532096670066 entropy 0.03264415264129639
epoch: 24, step: 83
	action: tensor([[-0.2935,  0.0611, -0.2741,  0.1212,  0.1253, -0.0079, -0.2207]],
       dtype=torch.float64)
	q_value: tensor([[-6.1225]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07335760142334291, distance: 1.1015717174867838 entropy 0.03264415264129639
epoch: 24, step: 84
	action: tensor([[ 0.0814,  0.1918, -0.3386, -0.2012,  0.5264,  0.1862,  0.4644]],
       dtype=torch.float64)
	q_value: tensor([[-5.3783]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.506556119322549, distance: 0.8038510382705694 entropy 0.03264415264129639
epoch: 24, step: 85
	action: tensor([[-0.1843,  0.2446,  0.0119, -0.0738, -0.1265,  0.4043,  0.1776]],
       dtype=torch.float64)
	q_value: tensor([[-6.8788]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31819768551636674, distance: 0.9449001737702482 entropy 0.03264415264129639
epoch: 24, step: 86
	action: tensor([[ 0.2369, -0.1098, -0.4195, -0.2788, -0.0921, -0.2570, -0.0535]],
       dtype=torch.float64)
	q_value: tensor([[-6.4902]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2852465361947898, distance: 0.9674640312338548 entropy 0.03264415264129639
epoch: 24, step: 87
	action: tensor([[-0.1745,  0.3486, -0.1940, -0.1140, -0.2660,  0.0039,  0.0148]],
       dtype=torch.float64)
	q_value: tensor([[-6.2628]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3248185382830512, distance: 0.9403011083181771 entropy 0.03264415264129639
epoch: 24, step: 88
	action: tensor([[ 0.2750, -0.4166, -0.2210,  0.1604,  0.2776, -0.0459,  0.2970]],
       dtype=torch.float64)
	q_value: tensor([[-6.4177]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2301276585461104, distance: 1.00407476428779 entropy 0.03264415264129639
epoch: 24, step: 89
	action: tensor([[ 0.1724, -0.1335,  0.0895, -0.5747, -0.1208, -0.4272, -0.2916]],
       dtype=torch.float64)
	q_value: tensor([[-6.1218]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06713446687108426, distance: 1.1821327979243328 entropy 0.03264415264129639
epoch: 24, step: 90
	action: tensor([[-0.1723, -0.0744, -0.3456,  0.3066, -0.4196, -0.2039, -0.0663]],
       dtype=torch.float64)
	q_value: tensor([[-6.8737]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11386477115572846, distance: 1.077225671379699 entropy 0.03264415264129639
epoch: 24, step: 91
	action: tensor([[-0.3397, -0.0394, -0.2028,  0.0938, -0.0581,  0.5972, -0.2417]],
       dtype=torch.float64)
	q_value: tensor([[-6.3671]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.020394901091413176, distance: 1.1326147464442173 entropy 0.03264415264129639
epoch: 24, step: 92
	action: tensor([[ 0.0032, -0.2084, -0.8197, -0.4303,  0.1999, -0.2939, -0.0752]],
       dtype=torch.float64)
	q_value: tensor([[-6.1943]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15265602992732918, distance: 1.053383629825957 entropy 0.03264415264129639
epoch: 24, step: 93
	action: tensor([[ 0.7307,  0.0172, -0.2714, -0.4130, -0.7908, -0.1779,  0.0439]],
       dtype=torch.float64)
	q_value: tensor([[-6.8998]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5316566887254894, distance: 0.7831390038896348 entropy 0.03264415264129639
epoch: 24, step: 94
	action: tensor([[ 0.0547, -0.0885,  0.3918, -0.6919, -0.3064,  0.4163, -0.0795]],
       dtype=torch.float64)
	q_value: tensor([[-9.4971]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03293996185347603, distance: 1.1630388792932116 entropy 0.03264415264129639
epoch: 24, step: 95
	action: tensor([[ 0.3052,  0.1155, -0.2320,  0.0688, -0.2908, -0.2143, -0.1407]],
       dtype=torch.float64)
	q_value: tensor([[-7.9696]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6244020736406297, distance: 0.7013233299661829 entropy 0.03264415264129639
epoch: 24, step: 96
	action: tensor([[ 0.4011, -0.3228, -0.1521, -0.4551,  0.3064,  0.1247, -0.5461]],
       dtype=torch.float64)
	q_value: tensor([[-6.4132]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12462807682141974, distance: 1.0706635057050127 entropy 0.03264415264129639
epoch: 24, step: 97
	action: tensor([[-0.0021,  0.2448, -0.0312, -0.1628,  0.0492, -0.5080,  0.0665]],
       dtype=torch.float64)
	q_value: tensor([[-5.8256]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3356047732895008, distance: 0.932760067303862 entropy 0.03264415264129639
epoch: 24, step: 98
	action: tensor([[ 0.0434,  0.0784, -0.3977,  0.1198,  0.1261, -0.0283,  0.0843]],
       dtype=torch.float64)
	q_value: tensor([[-6.5137]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4368696095046336, distance: 0.8587389783277843 entropy 0.03264415264129639
epoch: 24, step: 99
	action: tensor([[ 0.3310, -0.4082, -0.0939, -0.4431,  0.0722,  0.4482, -0.4042]],
       dtype=torch.float64)
	q_value: tensor([[-5.5537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10848214443446458, distance: 1.0804923991981277 entropy 0.03264415264129639
epoch: 24, step: 100
	action: tensor([[ 0.3300,  0.0416, -0.1709,  0.0455,  0.1040, -0.3865, -0.1560]],
       dtype=torch.float64)
	q_value: tensor([[-6.2827]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5640814501331004, distance: 0.7555433167989032 entropy 0.03264415264129639
epoch: 24, step: 101
	action: tensor([[ 0.2413,  0.0636, -0.4114,  0.6450, -0.0869,  0.0230, -0.3793]],
       dtype=torch.float64)
	q_value: tensor([[-6.1692]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.670221658770234, distance: 0.6571549111281985 entropy 0.03264415264129639
epoch: 24, step: 102
	action: tensor([[ 0.3200,  0.1043, -0.0739, -0.2475, -0.2422,  0.0458, -0.1136]],
       dtype=torch.float64)
	q_value: tensor([[-6.5421]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5658924441554868, distance: 0.7539722566758233 entropy 0.03264415264129639
epoch: 24, step: 103
	action: tensor([[ 0.2888,  0.2824,  0.0638,  0.5453,  0.1849, -0.0041, -0.1709]],
       dtype=torch.float64)
	q_value: tensor([[-6.3814]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8553795371415989, distance: 0.4351826443942311 entropy 0.03264415264129639
epoch: 24, step: 104
	action: tensor([[-0.1808,  0.1688, -0.1384,  0.2422, -0.2155, -0.0323, -0.1970]],
       dtype=torch.float64)
	q_value: tensor([[-6.5614]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32511313426407595, distance: 0.9400959492861375 entropy 0.03264415264129639
epoch: 24, step: 105
	action: tensor([[ 0.5362,  0.1989,  0.2017,  0.0314, -0.4577, -0.0350, -0.3444]],
       dtype=torch.float64)
	q_value: tensor([[-5.8671]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8177526447351658, distance: 0.48852558176822103 entropy 0.03264415264129639
epoch: 24, step: 106
	action: tensor([[-0.3300,  0.0668, -0.4060, -0.3978, -0.3738, -0.0047,  0.0590]],
       dtype=torch.float64)
	q_value: tensor([[-7.8139]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04506027604210239, distance: 1.1698424143010193 entropy 0.03264415264129639
epoch: 24, step: 107
	action: tensor([[ 0.0454,  0.1740, -0.3693,  0.1092, -0.1248,  0.2016, -0.3002]],
       dtype=torch.float64)
	q_value: tensor([[-7.0987]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.48150666564304645, distance: 0.824002030816155 entropy 0.03264415264129639
epoch: 24, step: 108
	action: tensor([[ 0.6663, -0.1189, -0.1539,  0.0392, -0.0483,  0.1688, -0.0907]],
       dtype=torch.float64)
	q_value: tensor([[-5.6054]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.683173986558375, distance: 0.6441204825830109 entropy 0.03264415264129639
epoch: 24, step: 109
	action: tensor([[ 0.0361,  0.3394, -0.0776,  0.3601,  0.3097, -0.2945, -0.0291]],
       dtype=torch.float64)
	q_value: tensor([[-6.5124]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5945021816852962, distance: 0.7287036657246307 entropy 0.03264415264129639
epoch: 24, step: 110
	action: tensor([[ 0.5191,  0.0105, -0.0114, -0.0566, -0.0211,  0.2155, -0.0777]],
       dtype=torch.float64)
	q_value: tensor([[-6.1750]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6967023137311488, distance: 0.6302186367122061 entropy 0.03264415264129639
epoch: 24, step: 111
	action: tensor([[-0.0279, -0.2900,  0.1138, -0.4347, -0.3803, -0.0715, -0.5007]],
       dtype=torch.float64)
	q_value: tensor([[-6.1866]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21120529997373438, distance: 1.2594054679409636 entropy 0.03264415264129639
epoch: 24, step: 112
	action: tensor([[ 0.2188,  0.3448,  0.1850, -0.2956, -0.2616,  0.1205, -0.0508]],
       dtype=torch.float64)
	q_value: tensor([[-6.6850]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5891417631642213, distance: 0.7335043475807922 entropy 0.03264415264129639
epoch: 24, step: 113
	action: tensor([[ 0.3721, -0.3776, -0.1214, -0.2223,  0.3420,  0.3682,  0.2020]],
       dtype=torch.float64)
	q_value: tensor([[-7.1998]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26198154396192164, distance: 0.9830832681455589 entropy 0.03264415264129639
epoch: 24, step: 114
	action: tensor([[ 0.4108,  0.0203, -0.6081, -0.0661, -0.0781, -0.1671, -0.5209]],
       dtype=torch.float64)
	q_value: tensor([[-6.0986]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5921161523116215, distance: 0.7308444394159589 entropy 0.03264415264129639
epoch: 24, step: 115
	action: tensor([[ 0.6499,  0.2720, -0.1335, -0.2443,  0.2056,  0.0508, -0.2750]],
       dtype=torch.float64)
	q_value: tensor([[-6.6003]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8128039384678507, distance: 0.49511381776113345 entropy 0.03264415264129639
epoch: 24, step: 116
	action: tensor([[-0.3386, -0.1436, -0.0173,  0.2529, -0.3256,  0.3258, -0.5063]],
       dtype=torch.float64)
	q_value: tensor([[-6.7720]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.017975616190502253, distance: 1.1340124668411131 entropy 0.03264415264129639
epoch: 24, step: 117
	action: tensor([[-0.1657, -0.0291, -0.1324, -0.0589,  0.1693, -0.1622, -0.2442]],
       dtype=torch.float64)
	q_value: tensor([[-6.3502]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.062352053218461934, distance: 1.1080939832709387 entropy 0.03264415264129639
epoch: 24, step: 118
	action: tensor([[ 0.3507,  0.2639,  0.3024,  0.0959,  0.2879, -0.0203, -0.0220]],
       dtype=torch.float64)
	q_value: tensor([[-5.2548]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7820088704929424, distance: 0.534288841452935 entropy 0.03264415264129639
epoch: 24, step: 119
	action: tensor([[-0.0536,  0.3348, -0.0159, -0.3926, -0.1344,  0.2751,  0.4695]],
       dtype=torch.float64)
	q_value: tensor([[-6.2846]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3589220962516577, distance: 0.9162460119674307 entropy 0.03264415264129639
epoch: 24, step: 120
	action: tensor([[ 0.1654, -0.1128, -0.0168, -0.0872, -0.3107,  0.0813,  0.0544]],
       dtype=torch.float64)
	q_value: tensor([[-7.6302]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3052546838600507, distance: 0.9538267697735591 entropy 0.03264415264129639
epoch: 24, step: 121
	action: tensor([[ 0.1371, -0.2732, -0.3693, -0.0532, -0.0203,  0.9232,  0.3734]],
       dtype=torch.float64)
	q_value: tensor([[-6.0494]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38087511994577017, distance: 0.9004214280318432 entropy 0.03264415264129639
epoch: 24, step: 122
	action: tensor([[-0.0152,  0.4060, -0.2086,  0.0433, -0.2935,  0.0931, -0.3545]],
       dtype=torch.float64)
	q_value: tensor([[-7.8868]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5350508265349121, distance: 0.7802960946124946 entropy 0.03264415264129639
epoch: 24, step: 123
	action: tensor([[-0.1013,  0.0952,  0.0996, -0.0532, -0.2364,  0.2655,  0.1978]],
       dtype=torch.float64)
	q_value: tensor([[-6.5291]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30486939217537223, distance: 0.9540912196204159 entropy 0.03264415264129639
epoch: 24, step: 124
	action: tensor([[ 0.1984,  0.1962, -0.0930, -0.2285,  0.1804,  0.0485, -0.2198]],
       dtype=torch.float64)
	q_value: tensor([[-6.4024]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5341652846820615, distance: 0.781038816816822 entropy 0.03264415264129639
epoch: 24, step: 125
	action: tensor([[-0.1687, -0.0108, -0.2451, -0.1598, -0.2284,  0.3246, -0.0085]],
       dtype=torch.float64)
	q_value: tensor([[-5.4280]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1204188243295693, distance: 1.0732345776425378 entropy 0.03264415264129639
epoch: 24, step: 126
	action: tensor([[-0.0653, -0.0065, -0.0712, -0.0850, -0.1172,  0.3357,  0.2565]],
       dtype=torch.float64)
	q_value: tensor([[-6.0565]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26880757771985264, distance: 0.9785263709453973 entropy 0.03264415264129639
epoch: 24, step: 127
	action: tensor([[-0.2215,  0.0857, -0.3757, -0.3282, -0.2538,  0.2899, -0.0421]],
       dtype=torch.float64)
	q_value: tensor([[-6.1082]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0959287634110223, distance: 1.0880729642766906 entropy 0.03264415264129639
LOSS epoch 24 actor 23.31054682381194 critic 44.66025753101367 
epoch: 25, step: 0
	action: tensor([[-0.1397, -0.0577,  0.0029,  0.0130,  0.0208, -0.2873, -0.4465]],
       dtype=torch.float64)
	q_value: tensor([[-6.7807]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07612208024663669, distance: 1.0999273148855888 entropy 0.03264415264129639
epoch: 25, step: 1
	action: tensor([[ 0.1180,  0.0299,  0.0184, -0.0850, -0.0581, -0.2877,  0.0551]],
       dtype=torch.float64)
	q_value: tensor([[-5.6961]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3391167052688091, distance: 0.9302915589992945 entropy 0.03264415264129639
epoch: 25, step: 2
	action: tensor([[ 0.0720,  0.0041, -0.1446, -0.2331,  0.0077,  0.1758, -0.1448]],
       dtype=torch.float64)
	q_value: tensor([[-5.9231]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32754743150645016, distance: 0.9383989683274496 entropy 0.03264415264129639
epoch: 25, step: 3
	action: tensor([[ 0.0841,  0.1793, -0.0837,  0.4754, -0.0377,  0.0368,  0.0911]],
       dtype=torch.float64)
	q_value: tensor([[-5.3691]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.653729872338614, distance: 0.673386192009684 entropy 0.03264415264129639
epoch: 25, step: 4
	action: tensor([[-0.0567,  0.2307, -0.3449,  0.0084, -0.1051, -0.1422,  0.0096]],
       dtype=torch.float64)
	q_value: tensor([[-6.1969]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.420258914498742, distance: 0.871312075477841 entropy 0.03264415264129639
epoch: 25, step: 5
	action: tensor([[-0.1745, -0.1939, -0.4065, -0.6630,  0.0067,  0.5652, -0.1448]],
       dtype=torch.float64)
	q_value: tensor([[-5.9420]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05175527424000492, distance: 1.173583628715496 entropy 0.03264415264129639
epoch: 25, step: 6
	action: tensor([[-0.2448, -0.1114, -0.5478,  0.0708,  0.1817,  0.0626, -0.1183]],
       dtype=torch.float64)
	q_value: tensor([[-7.4472]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.012003690986965854, distance: 1.1511919433169877 entropy 0.03264415264129639
epoch: 25, step: 7
	action: tensor([[ 0.1795, -0.0449, -0.1631,  0.3329,  0.1649,  0.1570,  0.2304]],
       dtype=torch.float64)
	q_value: tensor([[-5.7017]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5850504868346361, distance: 0.737147374362791 entropy 0.03264415264129639
epoch: 25, step: 8
	action: tensor([[ 0.3290,  0.1208,  0.2067, -0.5349,  0.1076,  0.3743,  0.3652]],
       dtype=torch.float64)
	q_value: tensor([[-5.8344]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.49040467482946815, distance: 0.8169009684405167 entropy 0.03264415264129639
epoch: 25, step: 9
	action: tensor([[ 0.2348,  0.4692,  0.0655,  0.0442,  0.3542,  0.0196, -0.1718]],
       dtype=torch.float64)
	q_value: tensor([[-7.4876]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7644335609656723, distance: 0.5554096251609363 entropy 0.03264415264129639
epoch: 25, step: 10
	action: tensor([[ 0.1189, -0.0219, -0.2828, -0.3067, -0.0535,  0.2488, -0.0020]],
       dtype=torch.float64)
	q_value: tensor([[-5.9391]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3341558884756516, distance: 0.9337765751307117 entropy 0.03264415264129639
epoch: 25, step: 11
	action: tensor([[ 0.2567,  0.2422, -0.3126,  0.1147,  0.0111, -0.1349,  0.0932]],
       dtype=torch.float64)
	q_value: tensor([[-5.9148]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6844210918701992, distance: 0.6428515240950823 entropy 0.03264415264129639
epoch: 25, step: 12
	action: tensor([[ 0.1529, -0.1886,  0.0335, -0.0901, -0.2765, -0.1065,  0.0867]],
       dtype=torch.float64)
	q_value: tensor([[-6.1583]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2292481674902297, distance: 1.0046481208484002 entropy 0.03264415264129639
epoch: 25, step: 13
	action: tensor([[ 0.6754, -0.2670, -0.2752, -0.0201, -0.5447,  0.0725, -0.1647]],
       dtype=torch.float64)
	q_value: tensor([[-6.0227]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.49194232148263906, distance: 0.8156675838810251 entropy 0.03264415264129639
epoch: 25, step: 14
	action: tensor([[ 0.2296,  0.0636,  0.1008, -0.1307, -0.0887,  0.1031, -0.3126]],
       dtype=torch.float64)
	q_value: tensor([[-7.5451]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5301773766476074, distance: 0.7843748426262434 entropy 0.03264415264129639
epoch: 25, step: 15
	action: tensor([[-0.1792, -0.2274, -0.0021,  0.3099,  0.0309, -0.1204, -0.6271]],
       dtype=torch.float64)
	q_value: tensor([[-5.9205]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09381313793389445, distance: 1.0893453251582341 entropy 0.03264415264129639
epoch: 25, step: 16
	action: tensor([[ 0.3613,  0.4193, -0.3341, -0.2054,  0.0795,  0.0575, -0.3227]],
       dtype=torch.float64)
	q_value: tensor([[-6.0343]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7920658272415089, distance: 0.5218186870631283 entropy 0.03264415264129639
epoch: 25, step: 17
	action: tensor([[-0.2395,  0.4250, -0.4608, -0.1733, -0.0871, -0.2596, -0.0372]],
       dtype=torch.float64)
	q_value: tensor([[-6.6258]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34701946525967564, distance: 0.9247126757407148 entropy 0.03264415264129639
epoch: 25, step: 18
	action: tensor([[ 0.0777, -0.1583, -0.1480,  0.2187,  0.0277, -0.3796, -0.1453]],
       dtype=torch.float64)
	q_value: tensor([[-6.7391]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29384498998459363, distance: 0.9616271481681291 entropy 0.03264415264129639
epoch: 25, step: 19
	action: tensor([[ 0.2020,  0.3223, -0.4258, -0.1003,  0.2056, -0.0998, -0.1044]],
       dtype=torch.float64)
	q_value: tensor([[-5.8845]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6641350708838972, distance: 0.6631916078671257 entropy 0.03264415264129639
epoch: 25, step: 20
	action: tensor([[ 0.1566,  0.1513, -0.4747, -0.3985, -0.6741,  0.4078,  0.1278]],
       dtype=torch.float64)
	q_value: tensor([[-6.0800]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4670744995062954, distance: 0.8353912924024347 entropy 0.03264415264129639
epoch: 25, step: 21
	action: tensor([[-0.0624, -0.1573, -0.0262, -0.2969,  0.1123,  0.2656, -0.1023]],
       dtype=torch.float64)
	q_value: tensor([[-9.1966]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06342940635678751, distance: 1.107457202882415 entropy 0.03264415264129639
epoch: 25, step: 22
	action: tensor([[ 0.3925, -0.0874,  0.0229, -0.2994,  0.2796,  0.8560, -0.4998]],
       dtype=torch.float64)
	q_value: tensor([[-5.3536]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6403219236416424, distance: 0.6862994908472706 entropy 0.03264415264129639
epoch: 25, step: 23
	action: tensor([[ 0.0067,  0.3236,  0.0637,  0.1709, -0.3309, -0.2164, -0.0875]],
       dtype=torch.float64)
	q_value: tensor([[-7.2673]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.579896836638331, distance: 0.7417109133435028 entropy 0.03264415264129639
epoch: 25, step: 24
	action: tensor([[-0.0472, -0.0632, -0.2548,  0.1109, -0.0961,  0.5871, -0.3067]],
       dtype=torch.float64)
	q_value: tensor([[-6.7033]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3631416502394177, distance: 0.9132256830561846 entropy 0.03264415264129639
epoch: 25, step: 25
	action: tensor([[-0.2074, -0.1113, -0.0538,  0.2519, -0.1258,  0.4005,  0.0999]],
       dtype=torch.float64)
	q_value: tensor([[-5.9920]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.220633637937664, distance: 1.0102468896042187 entropy 0.03264415264129639
epoch: 25, step: 26
	action: tensor([[ 0.5540, -0.1361, -0.2125, -0.1261, -0.0626,  0.1146, -0.0128]],
       dtype=torch.float64)
	q_value: tensor([[-6.0182]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.529993247171497, distance: 0.7845285308202465 entropy 0.03264415264129639
epoch: 25, step: 27
	action: tensor([[ 0.1614, -0.3526, -0.1081, -0.3933,  0.1738,  0.2150, -0.0435]],
       dtype=torch.float64)
	q_value: tensor([[-6.2319]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.006145794277492178, distance: 1.1408223823017722 entropy 0.03264415264129639
epoch: 25, step: 28
	action: tensor([[ 0.0768, -0.0554, -0.3113, -0.3785, -0.4026,  0.0580, -0.2696]],
       dtype=torch.float64)
	q_value: tensor([[-5.5421]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2342125435449537, distance: 1.001407448678652 entropy 0.03264415264129639
epoch: 25, step: 29
	action: tensor([[ 0.2383, -0.0950, -0.0631,  0.0500, -0.4474, -0.3541,  0.1365]],
       dtype=torch.float64)
	q_value: tensor([[-6.8068]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44542039645902676, distance: 0.85219432884539 entropy 0.03264415264129639
epoch: 25, step: 30
	action: tensor([[ 0.0709, -0.2529, -0.2941, -0.3192,  0.1898, -0.1056, -0.0657]],
       dtype=torch.float64)
	q_value: tensor([[-6.8488]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0351475953934387, distance: 1.1240538954953725 entropy 0.03264415264129639
epoch: 25, step: 31
	action: tensor([[-0.2801, -0.0297, -0.4646,  0.0612, -0.1294,  0.0974, -0.1890]],
       dtype=torch.float64)
	q_value: tensor([[-5.5658]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.005453261986547719, distance: 1.1412197840286187 entropy 0.03264415264129639
epoch: 25, step: 32
	action: tensor([[ 0.4829,  0.3065, -0.1449, -0.0972,  0.1181,  0.1532, -0.0151]],
       dtype=torch.float64)
	q_value: tensor([[-5.7580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8293443425643318, distance: 0.4727342172730401 entropy 0.03264415264129639
epoch: 25, step: 33
	action: tensor([[-0.2628,  0.2034, -0.3889,  0.1553,  0.1755,  0.0789, -0.1229]],
       dtype=torch.float64)
	q_value: tensor([[-6.3021]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21107464360410488, distance: 1.0164233902601731 entropy 0.03264415264129639
epoch: 25, step: 34
	action: tensor([[ 0.3898,  0.0846,  0.4177, -0.2865,  0.0148, -0.0360, -0.1659]],
       dtype=torch.float64)
	q_value: tensor([[-5.6691]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4939694456622171, distance: 0.8140387214948925 entropy 0.03264415264129639
epoch: 25, step: 35
	action: tensor([[ 0.0407, -0.1188,  0.2665,  0.0742, -0.1408,  0.3382,  0.2843]],
       dtype=torch.float64)
	q_value: tensor([[-6.7003]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3920111134732619, distance: 0.8922868932207679 entropy 0.03264415264129639
epoch: 25, step: 36
	action: tensor([[-0.2315, -0.0056, -0.3765, -0.1081, -0.2713,  0.1724, -0.4772]],
       dtype=torch.float64)
	q_value: tensor([[-6.4433]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04764018550933369, distance: 1.1167532477015312 entropy 0.03264415264129639
epoch: 25, step: 37
	action: tensor([[ 0.3329,  0.3855, -0.1220, -0.2857,  0.3273,  0.0829, -0.3473]],
       dtype=torch.float64)
	q_value: tensor([[-6.2213]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7355075891556799, distance: 0.5885228113038994 entropy 0.03264415264129639
epoch: 25, step: 38
	action: tensor([[ 0.3155, -0.0270,  0.2235,  0.0414,  0.0626,  0.4052, -0.2417]],
       dtype=torch.float64)
	q_value: tensor([[-6.0262]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6732023926909882, distance: 0.654178290142436 entropy 0.03264415264129639
epoch: 25, step: 39
	action: tensor([[ 0.2847,  0.1050, -0.2548,  0.0245,  0.0475,  0.7481, -0.1426]],
       dtype=torch.float64)
	q_value: tensor([[-6.0157]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7429572351304673, distance: 0.5801755001123272 entropy 0.03264415264129639
epoch: 25, step: 40
	action: tensor([[ 0.2417, -0.0244,  0.0169, -0.0244,  0.2195,  0.4445, -0.2856]],
       dtype=torch.float64)
	q_value: tensor([[-6.9515]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6128029472074079, distance: 0.7120700421655737 entropy 0.03264415264129639
epoch: 25, step: 41
	action: tensor([[ 0.0216,  0.3210,  0.0218, -0.1450,  0.3364,  0.0719, -0.3812]],
       dtype=torch.float64)
	q_value: tensor([[-5.4407]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.512872419858508, distance: 0.7986896431204474 entropy 0.03264415264129639
epoch: 25, step: 42
	action: tensor([[ 0.3427, -0.2025, -0.4500, -0.5468, -0.2792,  0.1922, -0.3667]],
       dtype=torch.float64)
	q_value: tensor([[-5.5670]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25663331311641036, distance: 0.9866389144648057 entropy 0.03264415264129639
epoch: 25, step: 43
	action: tensor([[-0.1096,  0.2369,  0.1407, -0.4458,  0.1428,  0.0525, -0.0098]],
       dtype=torch.float64)
	q_value: tensor([[-7.2766]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16352884586278016, distance: 1.0466034877893926 entropy 0.03264415264129639
epoch: 25, step: 44
	action: tensor([[0.2737, 0.0209, 0.2565, 0.0470, 0.0305, 0.0045, 0.0659]],
       dtype=torch.float64)
	q_value: tensor([[-5.9896]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5727444169509146, distance: 0.7479982236465766 entropy 0.03264415264129639
epoch: 25, step: 45
	action: tensor([[ 0.2536, -0.1282, -0.3432, -0.0219,  0.0207,  0.1629, -0.3089]],
       dtype=torch.float64)
	q_value: tensor([[-5.9983]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4327036828197619, distance: 0.8619095155086123 entropy 0.03264415264129639
epoch: 25, step: 46
	action: tensor([[ 0.3576, -0.0535, -0.0444, -0.2183, -0.1996, -0.1785, -0.1855]],
       dtype=torch.float64)
	q_value: tensor([[-5.3034]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4167392084609969, distance: 0.8739530145260642 entropy 0.03264415264129639
epoch: 25, step: 47
	action: tensor([[ 0.2104,  0.1796, -0.3416,  0.2944,  0.1787,  0.1387, -0.1499]],
       dtype=torch.float64)
	q_value: tensor([[-6.2897]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6926242846920185, distance: 0.6344413337950673 entropy 0.03264415264129639
epoch: 25, step: 48
	action: tensor([[ 0.1306,  0.0092, -0.0525,  0.1062,  0.0561,  0.0388, -0.0181]],
       dtype=torch.float64)
	q_value: tensor([[-5.7176]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5034406621765909, distance: 0.8063846829652094 entropy 0.03264415264129639
epoch: 25, step: 49
	action: tensor([[ 0.0344, -0.3243, -0.1716, -0.8057, -0.0988,  0.2292, -0.2057]],
       dtype=torch.float64)
	q_value: tensor([[-5.4448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20435553580319987, distance: 1.255839242764773 entropy 0.03264415264129639
epoch: 25, step: 50
	action: tensor([[ 0.3624, -0.1810, -0.2119, -0.2266, -0.3126,  0.0979, -0.3750]],
       dtype=torch.float64)
	q_value: tensor([[-7.0662]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3471481319801749, distance: 0.9246215661266162 entropy 0.03264415264129639
epoch: 25, step: 51
	action: tensor([[ 0.1848, -0.1377, -0.0360, -0.2187, -0.0419, -0.3470, -0.1320]],
       dtype=torch.float64)
	q_value: tensor([[-6.4644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1863554444968032, distance: 1.0322242426505308 entropy 0.03264415264129639
epoch: 25, step: 52
	action: tensor([[-0.0845, -0.0598, -0.0854, -0.0167,  0.0952,  0.1695,  0.1082]],
       dtype=torch.float64)
	q_value: tensor([[-5.9966]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22203670702172484, distance: 1.0093371218954228 entropy 0.03264415264129639
epoch: 25, step: 53
	action: tensor([[-0.0859, -0.2707, -0.7206,  0.0891,  0.1797, -0.0029, -0.0279]],
       dtype=torch.float64)
	q_value: tensor([[-5.4268]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.02137784249990826, distance: 1.1320463677439732 entropy 0.03264415264129639
epoch: 25, step: 54
	action: tensor([[-0.1642,  0.2249, -0.0875, -0.0166,  0.2115, -0.1572,  0.0646]],
       dtype=torch.float64)
	q_value: tensor([[-5.9555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25713297116500367, distance: 0.9863072713125229 entropy 0.03264415264129639
epoch: 25, step: 55
	action: tensor([[ 0.0106,  0.1754, -0.0332,  0.3820, -0.0941,  0.3207,  0.0291]],
       dtype=torch.float64)
	q_value: tensor([[-5.7033]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6124827164951085, distance: 0.7123644395110232 entropy 0.03264415264129639
epoch: 25, step: 56
	action: tensor([[-0.1460,  0.1817,  0.0175,  0.1725, -0.0404, -0.4185,  0.2103]],
       dtype=torch.float64)
	q_value: tensor([[-6.1290]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28843181870253576, distance: 0.9653058834213671 entropy 0.03264415264129639
epoch: 25, step: 57
	action: tensor([[ 0.1207,  0.2541, -0.0035, -0.1328, -0.2112,  0.2676,  0.0049]],
       dtype=torch.float64)
	q_value: tensor([[-6.4051]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5637191592744939, distance: 0.7558572167090085 entropy 0.03264415264129639
epoch: 25, step: 58
	action: tensor([[ 2.2039e-01, -3.4131e-04,  2.7740e-02, -5.2964e-01,  3.4036e-02,
          1.7322e-01, -1.4654e-01]], dtype=torch.float64)
	q_value: tensor([[-6.5418]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2701715814450696, distance: 0.9776132483414701 entropy 0.03264415264129639
epoch: 25, step: 59
	action: tensor([[-0.3184, -0.3824, -0.5793,  0.2612, -0.2847,  0.0032, -0.1467]],
       dtype=torch.float64)
	q_value: tensor([[-6.0935]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3079833606344413, distance: 1.3087533395144477 entropy 0.03264415264129639
epoch: 25, step: 60
	action: tensor([[ 0.3338, -0.0596, -0.0556,  0.0178, -0.1495,  0.3053, -0.6875]],
       dtype=torch.float64)
	q_value: tensor([[-6.4035]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5859154669722377, distance: 0.7363786658219745 entropy 0.03264415264129639
epoch: 25, step: 61
	action: tensor([[ 0.4195, -0.1761,  0.0153, -0.2436,  0.2722,  0.0101,  0.2413]],
       dtype=torch.float64)
	q_value: tensor([[-6.5550]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34197368648943616, distance: 0.9282785680611896 entropy 0.03264415264129639
epoch: 25, step: 62
	action: tensor([[-0.5587,  0.4824, -0.3395, -0.0302,  0.2175, -0.1799,  0.0050]],
       dtype=torch.float64)
	q_value: tensor([[-6.1819]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.027060234627758195, distance: 1.1597240155203474 entropy 0.03264415264129639
epoch: 25, step: 63
	action: tensor([[-0.1986,  0.1909, -0.2328, -0.4385,  0.1867,  0.3244,  0.0426]],
       dtype=torch.float64)
	q_value: tensor([[-6.8073]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16563189701026682, distance: 1.0452869774288134 entropy 0.03264415264129639
epoch: 25, step: 64
	action: tensor([[-0.4032, -0.0323, -0.2788, -0.0493,  0.2501, -0.1669, -0.0718]],
       dtype=torch.float64)
	q_value: tensor([[-6.4212]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.21789491257620908, distance: 1.2628785925043442 entropy 0.03264415264129639
epoch: 25, step: 65
	action: tensor([[-0.1368,  0.4876,  0.0446, -0.2631, -0.1862, -0.2245, -0.4381]],
       dtype=torch.float64)
	q_value: tensor([[-5.7926]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3391913836653856, distance: 0.9302389970377458 entropy 0.03264415264129639
epoch: 25, step: 66
	action: tensor([[-0.1170, -0.3625, -0.1746,  0.1790,  0.1034, -0.1816,  0.0450]],
       dtype=torch.float64)
	q_value: tensor([[-7.0223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06388660165228632, distance: 1.1803324935022497 entropy 0.03264415264129639
epoch: 25, step: 67
	action: tensor([[-0.0570, -0.1581, -0.0363, -0.2259, -0.1672, -0.0986, -0.3320]],
       dtype=torch.float64)
	q_value: tensor([[-5.6051]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.012618364805424442, distance: 1.1371014567709425 entropy 0.03264415264129639
epoch: 25, step: 68
	action: tensor([[ 0.0875,  0.2885,  0.2555, -0.5907, -0.0008, -0.2070, -0.1964]],
       dtype=torch.float64)
	q_value: tensor([[-5.6836]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24633416402741748, distance: 0.9934502142011072 entropy 0.03264415264129639
epoch: 25, step: 69
	action: tensor([[ 0.1765,  0.5504, -0.1726, -0.0983, -0.1521,  0.0756, -0.4429]],
       dtype=torch.float64)
	q_value: tensor([[-6.9457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.71500128264562, distance: 0.6109112826499218 entropy 0.03264415264129639
epoch: 25, step: 70
	action: tensor([[-0.0607, -0.4121, -0.5007, -0.0086, -0.2813,  0.1574,  0.1850]],
       dtype=torch.float64)
	q_value: tensor([[-7.1312]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08036400379901232, distance: 1.1894378273048254 entropy 0.03264415264129639
epoch: 25, step: 71
	action: tensor([[ 0.0734, -0.0511, -0.2535, -0.3262, -0.1239,  0.5777, -0.2885]],
       dtype=torch.float64)
	q_value: tensor([[-6.1195]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32507124952186817, distance: 0.9401251208907223 entropy 0.03264415264129639
epoch: 25, step: 72
	action: tensor([[ 0.1387, -0.0030, -0.1352, -0.1465,  0.3555, -0.1878, -0.4395]],
       dtype=torch.float64)
	q_value: tensor([[-6.6831]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3347907216338476, distance: 0.9333313254954771 entropy 0.03264415264129639
epoch: 25, step: 73
	action: tensor([[ 0.7104, -0.1733, -0.1419,  0.0176, -0.1071,  0.1373, -0.2508]],
       dtype=torch.float64)
	q_value: tensor([[-5.3471]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6256799797625388, distance: 0.7001292485750484 entropy 0.03264415264129639
epoch: 25, step: 74
	action: tensor([[ 0.4644, -0.1171,  0.2077,  0.0625,  0.2356,  0.1142,  0.0421]],
       dtype=torch.float64)
	q_value: tensor([[-6.7734]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6288868507934311, distance: 0.6971237270834006 entropy 0.03264415264129639
epoch: 25, step: 75
	action: tensor([[-0.0442,  0.3256, -0.3004, -0.1740, -0.2494, -0.0930, -0.2122]],
       dtype=torch.float64)
	q_value: tensor([[-6.1257]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4608779987583572, distance: 0.8402339415022233 entropy 0.03264415264129639
epoch: 25, step: 76
	action: tensor([[ 0.1656, -0.4135, -0.2285, -0.1584, -0.3058,  0.0757, -0.1585]],
       dtype=torch.float64)
	q_value: tensor([[-6.5217]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04740763204757137, distance: 1.1168895874428582 entropy 0.03264415264129639
epoch: 25, step: 77
	action: tensor([[ 0.2599,  0.2374, -0.2652,  0.1518,  0.1541,  0.3415,  0.1701]],
       dtype=torch.float64)
	q_value: tensor([[-5.9810]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7483368625763578, distance: 0.5740721739533328 entropy 0.03264415264129639
epoch: 25, step: 78
	action: tensor([[-0.1394,  0.1961, -0.0705, -0.3831, -0.1296,  0.2730, -0.1353]],
       dtype=torch.float64)
	q_value: tensor([[-5.9644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21772034404978624, distance: 1.012133294361476 entropy 0.03264415264129639
epoch: 25, step: 79
	action: tensor([[-0.1182, -0.2144, -0.0325, -0.1490, -0.4720,  0.1064,  0.1341]],
       dtype=torch.float64)
	q_value: tensor([[-6.4298]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.034693284688387616, distance: 1.1640255378123479 entropy 0.03264415264129639
epoch: 25, step: 80
	action: tensor([[ 0.0569,  0.3756,  0.0196, -0.3374,  0.0357,  0.2493,  0.4526]],
       dtype=torch.float64)
	q_value: tensor([[-6.5680]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5209493238355188, distance: 0.7920405597370211 entropy 0.03264415264129639
epoch: 25, step: 81
	action: tensor([[ 0.1712,  0.2877, -0.1585,  0.2333, -0.1255,  0.0310, -0.2299]],
       dtype=torch.float64)
	q_value: tensor([[-7.3928]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.683758368407358, distance: 0.6435261720862636 entropy 0.03264415264129639
epoch: 25, step: 82
	action: tensor([[ 0.4485, -0.0410, -0.0610, -0.0926, -0.3941,  0.1377, -0.0267]],
       dtype=torch.float64)
	q_value: tensor([[-6.0150]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.605621924937914, distance: 0.7186427932682719 entropy 0.03264415264129639
epoch: 25, step: 83
	action: tensor([[ 0.1146, -0.2987, -0.5289,  0.2743,  0.1815, -0.0385, -0.4078]],
       dtype=torch.float64)
	q_value: tensor([[-6.8033]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24822999212772567, distance: 0.9921999275004437 entropy 0.03264415264129639
epoch: 25, step: 84
	action: tensor([[ 0.6736,  0.3686, -0.0744, -0.1392, -0.3309,  0.1044,  0.2965]],
       dtype=torch.float64)
	q_value: tensor([[-5.7127]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9001569547109887, distance: 0.36158932677993616 entropy 0.03264415264129639
epoch: 25, step: 85
	action: tensor([[ 0.1574, -0.5222,  0.1331, -0.2272,  0.0569,  0.1208,  0.0393]],
       dtype=torch.float64)
	q_value: tensor([[-8.3231]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1417541641103719, distance: 1.2227649893980204 entropy 0.03264415264129639
epoch: 25, step: 86
	action: tensor([[ 0.2139,  0.1103,  0.2193,  0.0806, -0.3500,  0.2895,  0.2906]],
       dtype=torch.float64)
	q_value: tensor([[-5.8003]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.667370658517575, distance: 0.6599894164483125 entropy 0.03264415264129639
epoch: 25, step: 87
	action: tensor([[-0.2431, -0.3359,  0.2396,  0.0063, -0.0018, -0.0825,  0.0032]],
       dtype=torch.float64)
	q_value: tensor([[-7.1002]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24174225771143676, distance: 1.2751827338665551 entropy 0.03264415264129639
epoch: 25, step: 88
	action: tensor([[ 0.5688,  0.3452, -0.1555, -0.0061,  0.1087, -0.2718,  0.0476]],
       dtype=torch.float64)
	q_value: tensor([[-5.7431]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8502600432810424, distance: 0.44281828427836645 entropy 0.03264415264129639
epoch: 25, step: 89
	action: tensor([[ 0.2998, -0.2055,  0.0842,  0.0135,  0.2714, -0.0593, -0.1635]],
       dtype=torch.float64)
	q_value: tensor([[-7.0691]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37004069888916336, distance: 0.9082657540525981 entropy 0.03264415264129639
epoch: 25, step: 90
	action: tensor([[ 0.0178, -0.2527, -0.0151, -0.1112,  0.1568, -0.0390,  0.3239]],
       dtype=torch.float64)
	q_value: tensor([[-5.5108]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.033857589030137136, distance: 1.1248050737690782 entropy 0.03264415264129639
epoch: 25, step: 91
	action: tensor([[-0.1162, -0.1942, -0.1512,  0.1512,  0.1565,  0.1989,  0.0859]],
       dtype=torch.float64)
	q_value: tensor([[-5.9167]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15512980784679276, distance: 1.051844855762463 entropy 0.03264415264129639
epoch: 25, step: 92
	action: tensor([[-0.2911,  0.1759, -0.4487, -0.1749,  0.1245, -0.0673,  0.0479]],
       dtype=torch.float64)
	q_value: tensor([[-5.4060]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10185898319014264, distance: 1.0844985070611768 entropy 0.03264415264129639
epoch: 25, step: 93
	action: tensor([[ 0.0425,  0.2541, -0.0398, -0.0459, -0.1705, -0.3694, -0.0509]],
       dtype=torch.float64)
	q_value: tensor([[-6.1215]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.45803847688600474, distance: 0.8424437653899302 entropy 0.03264415264129639
epoch: 25, step: 94
	action: tensor([[ 0.4613,  0.1126, -0.3599, -0.0228,  0.2198, -0.1779, -0.4484]],
       dtype=torch.float64)
	q_value: tensor([[-6.4036]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.682223534094806, distance: 0.6450859137958681 entropy 0.03264415264129639
epoch: 25, step: 95
	action: tensor([[ 0.6691, -0.2541, -0.1580, -0.3077,  0.3617,  0.2919, -0.0262]],
       dtype=torch.float64)
	q_value: tensor([[-6.3463]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41508212556221213, distance: 0.875193613463198 entropy 0.03264415264129639
epoch: 25, step: 96
	action: tensor([[ 0.2484, -0.6108, -0.3341, -0.1087,  0.2667,  0.1411, -0.1755]],
       dtype=torch.float64)
	q_value: tensor([[-6.4514]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.050395292471131725, distance: 1.172824626759544 entropy 0.03264415264129639
epoch: 25, step: 97
	action: tensor([[ 0.0511,  0.1859, -0.2777, -0.2257, -0.0974,  0.0540, -0.1191]],
       dtype=torch.float64)
	q_value: tensor([[-5.5128]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.45189825074556567, distance: 0.8472026144545672 entropy 0.03264415264129639
epoch: 25, step: 98
	action: tensor([[ 0.0170,  0.0143, -0.2520,  0.0572, -0.5798, -0.0591,  0.6435]],
       dtype=torch.float64)
	q_value: tensor([[-5.8280]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.372200406513321, distance: 0.9067075005265253 entropy 0.03264415264129639
epoch: 25, step: 99
	action: tensor([[-0.3392,  0.2858,  0.1254, -0.3680,  0.0620, -0.0974,  0.6232]],
       dtype=torch.float64)
	q_value: tensor([[-8.0364]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07876739566678537, distance: 1.1885586012984142 entropy 0.03264415264129639
epoch: 25, step: 100
	action: tensor([[ 0.5658, -0.2733,  0.0716, -0.1758,  0.3307,  0.3415, -0.1905]],
       dtype=torch.float64)
	q_value: tensor([[-7.8261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4485981958087599, distance: 0.849749239673796 entropy 0.03264415264129639
epoch: 25, step: 101
	action: tensor([[ 0.5789,  0.0320,  0.2883, -0.2826, -0.2881, -0.1114, -0.4967]],
       dtype=torch.float64)
	q_value: tensor([[-6.0395]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5336364770138742, distance: 0.78148200208754 entropy 0.03264415264129639
epoch: 25, step: 102
	action: tensor([[ 0.2758,  0.3791, -0.3549,  0.3454, -0.0070, -0.0388, -0.5140]],
       dtype=torch.float64)
	q_value: tensor([[-7.7898]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7367493048716555, distance: 0.5871397135072094 entropy 0.03264415264129639
epoch: 25, step: 103
	action: tensor([[ 0.6148, -0.1262, -0.2381, -0.2076,  0.0145,  0.2876,  0.2097]],
       dtype=torch.float64)
	q_value: tensor([[-6.7517]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5706120670374806, distance: 0.7498624577077978 entropy 0.03264415264129639
epoch: 25, step: 104
	action: tensor([[ 0.1174, -0.3628, -0.5112,  0.0995,  0.2409,  0.1666, -0.0838]],
       dtype=torch.float64)
	q_value: tensor([[-6.9079]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17350497588882774, distance: 1.040343636129242 entropy 0.03264415264129639
epoch: 25, step: 105
	action: tensor([[ 0.2951, -0.0367, -0.4381, -0.1144, -0.1164, -0.0373,  0.3223]],
       dtype=torch.float64)
	q_value: tensor([[-5.4178]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.476500418915078, distance: 0.8279704981979923 entropy 0.03264415264129639
epoch: 25, step: 106
	action: tensor([[-0.1169, -0.2670, -0.3670, -0.0600,  0.1862,  0.1171,  0.2866]],
       dtype=torch.float64)
	q_value: tensor([[-6.6726]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.032627669324242436, distance: 1.1628630530964794 entropy 0.03264415264129639
epoch: 25, step: 107
	action: tensor([[ 0.1355, -0.3514, -0.3065, -0.3813,  0.3787, -0.1833,  0.4026]],
       dtype=torch.float64)
	q_value: tensor([[-5.8773]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10868807420447801, distance: 1.2049287898169885 entropy 0.03264415264129639
epoch: 25, step: 108
	action: tensor([[-0.0353,  0.1293, -0.4697, -0.0570, -0.2744,  0.0795, -0.5649]],
       dtype=torch.float64)
	q_value: tensor([[-6.6598]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30166380448156394, distance: 0.9562885802120427 entropy 0.03264415264129639
epoch: 25, step: 109
	action: tensor([[-0.4802, -0.0787, -0.0866, -0.1802, -0.2468,  0.4383, -0.3485]],
       dtype=torch.float64)
	q_value: tensor([[-6.5066]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3226612381539724, distance: 1.3160761122441211 entropy 0.03264415264129639
epoch: 25, step: 110
	action: tensor([[ 0.0139,  0.2528, -0.0803,  0.0305, -0.0276, -0.2936, -0.1316]],
       dtype=torch.float64)
	q_value: tensor([[-6.8711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42364255488362546, distance: 0.8687656619334926 entropy 0.03264415264129639
epoch: 25, step: 111
	action: tensor([[ 0.1288,  0.1718, -0.1967, -0.0468,  0.0678, -0.1052,  0.0283]],
       dtype=torch.float64)
	q_value: tensor([[-5.9567]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4805064516620581, distance: 0.8247964298989183 entropy 0.03264415264129639
epoch: 25, step: 112
	action: tensor([[ 0.1034,  0.2375,  0.0613,  0.2290,  0.4122,  0.0032, -0.2690]],
       dtype=torch.float64)
	q_value: tensor([[-5.6264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6153629740112441, distance: 0.7097121451644774 entropy 0.03264415264129639
epoch: 25, step: 113
	action: tensor([[ 0.0963,  0.3066, -0.3334,  0.0647,  0.1246, -0.0141,  0.1616]],
       dtype=torch.float64)
	q_value: tensor([[-5.6046]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5666797386364941, distance: 0.7532882469537511 entropy 0.03264415264129639
epoch: 25, step: 114
	action: tensor([[-0.1393,  0.2992,  0.1249,  0.1473, -0.1359,  0.5543, -0.1175]],
       dtype=torch.float64)
	q_value: tensor([[-5.9923]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4956473326687505, distance: 0.8126880134293969 entropy 0.03264415264129639
epoch: 25, step: 115
	action: tensor([[-0.4318, -0.0035, -0.2117,  0.0437, -0.0645,  0.0583, -0.3259]],
       dtype=torch.float64)
	q_value: tensor([[-6.7665]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.17651582541923339, distance: 1.2412394777495541 entropy 0.03264415264129639
epoch: 25, step: 116
	action: tensor([[ 0.1982, -0.1664, -0.0138, -0.0305, -0.2822, -0.0574,  0.2264]],
       dtype=torch.float64)
	q_value: tensor([[-5.7149]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29049417371661646, distance: 0.9639059838238164 entropy 0.03264415264129639
epoch: 25, step: 117
	action: tensor([[-0.0823, -0.3501,  0.1175, -0.1406, -0.7424, -0.2792,  0.0596]],
       dtype=torch.float64)
	q_value: tensor([[-6.1531]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16118971207400867, distance: 1.2331283515397802 entropy 0.03264415264129639
epoch: 25, step: 118
	action: tensor([[-0.1396, -0.4752, -0.1989, -0.3569,  0.3118,  0.3980, -0.4374]],
       dtype=torch.float64)
	q_value: tensor([[-7.4657]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.25155850590599327, distance: 1.2802131131615142 entropy 0.03264415264129639
epoch: 25, step: 119
	action: tensor([[ 0.1152, -0.1808,  0.2192, -0.1454, -0.1782, -0.2018,  0.1142]],
       dtype=torch.float64)
	q_value: tensor([[-5.7689]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1071695030412797, distance: 1.0812875472477106 entropy 0.03264415264129639
epoch: 25, step: 120
	action: tensor([[ 0.2815,  0.5600, -0.0597, -0.4832, -0.0089,  0.1423, -0.3355]],
       dtype=torch.float64)
	q_value: tensor([[-6.1393]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7217174656546823, distance: 0.6036701035861953 entropy 0.03264415264129639
epoch: 25, step: 121
	action: tensor([[-0.1766,  0.2866, -0.0397,  0.4875, -0.1033,  0.6502, -0.3323]],
       dtype=torch.float64)
	q_value: tensor([[-7.6465]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4984487738102644, distance: 0.810427821216728 entropy 0.03264415264129639
epoch: 25, step: 122
	action: tensor([[ 0.2641,  0.0230,  0.2216,  0.0272,  0.5180, -0.2799, -0.0095]],
       dtype=torch.float64)
	q_value: tensor([[-7.0005]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4613501082463539, distance: 0.8398659642198881 entropy 0.03264415264129639
epoch: 25, step: 123
	action: tensor([[-0.0634,  0.5480, -0.0610, -0.1871, -0.1637, -0.0159, -0.1867]],
       dtype=torch.float64)
	q_value: tensor([[-6.1984]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4657520623168956, distance: 0.8364271482668769 entropy 0.03264415264129639
epoch: 25, step: 124
	action: tensor([[-0.1511, -0.1869,  0.1949, -0.0549,  0.0816,  0.0490, -0.3235]],
       dtype=torch.float64)
	q_value: tensor([[-6.8602]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.03593014907543246, distance: 1.1647210636012744 entropy 0.03264415264129639
epoch: 25, step: 125
	action: tensor([[-0.1021,  0.3040,  0.0787, -0.0274, -0.1732, -0.4405, -0.2777]],
       dtype=torch.float64)
	q_value: tensor([[-5.3321]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3189309295852111, distance: 0.9443919408023677 entropy 0.03264415264129639
epoch: 25, step: 126
	action: tensor([[ 0.1555,  0.3496,  0.1294, -0.1614, -0.1182, -0.3344,  0.1068]],
       dtype=torch.float64)
	q_value: tensor([[-6.5622]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5293861706971159, distance: 0.7850350289925881 entropy 0.03264415264129639
epoch: 25, step: 127
	action: tensor([[-0.1653,  0.2075,  0.0082, -0.4498, -0.4100, -0.1990, -0.1749]],
       dtype=torch.float64)
	q_value: tensor([[-6.8728]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08096847675802432, distance: 1.0970385711509214 entropy 0.03264415264129639
LOSS epoch 25 actor 22.438461744902842 critic 36.351776201445 
epoch: 26, step: 0
	action: tensor([[ 0.3423, -0.3250, -0.0339, -0.2996, -0.1195,  0.1522,  0.1881]],
       dtype=torch.float64)
	q_value: tensor([[-7.0046]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14450566972141377, distance: 1.0584376042179195 entropy 0.03264415264129639
epoch: 26, step: 1
	action: tensor([[ 0.1051, -0.4032, -0.2859,  0.2825, -0.2286, -0.2601, -0.2403]],
       dtype=torch.float64)
	q_value: tensor([[-6.0022]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1503107140014539, distance: 1.0548404232595097 entropy 0.03264415264129639
epoch: 26, step: 2
	action: tensor([[ 0.4910,  0.3379, -0.1460,  0.5255, -0.1652,  0.0563,  0.0751]],
       dtype=torch.float64)
	q_value: tensor([[-5.6962]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.885145492636648, distance: 0.3878204580407428 entropy 0.03264415264129639
epoch: 26, step: 3
	action: tensor([[-0.1052, -0.0383, -0.2730,  0.1209, -0.1054, -0.0885, -0.1135]],
       dtype=torch.float64)
	q_value: tensor([[-6.6704]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19433543788867658, distance: 1.0271498897780653 entropy 0.03264415264129639
epoch: 26, step: 4
	action: tensor([[-0.2806,  0.1615,  0.1481, -0.3723,  0.0236, -0.2676,  0.0100]],
       dtype=torch.float64)
	q_value: tensor([[-5.1039]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1474496360892512, distance: 1.2258109867531464 entropy 0.03264415264129639
epoch: 26, step: 5
	action: tensor([[ 0.6430, -0.1227,  0.0191,  0.0189,  0.1188, -0.3447,  0.2353]],
       dtype=torch.float64)
	q_value: tensor([[-5.9638]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5339791393832062, distance: 0.7811948508973005 entropy 0.03264415264129639
epoch: 26, step: 6
	action: tensor([[ 0.1516,  0.0985, -0.2305, -0.4671, -0.2023,  0.0796,  0.0284]],
       dtype=torch.float64)
	q_value: tensor([[-6.7313]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3467300210942973, distance: 0.9249175999464408 entropy 0.03264415264129639
epoch: 26, step: 7
	action: tensor([[ 0.5640,  0.2151,  0.0373, -0.4650, -0.0578,  0.0394,  0.0564]],
       dtype=torch.float64)
	q_value: tensor([[-6.4823]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6331452281637318, distance: 0.6931125770519042 entropy 0.03264415264129639
epoch: 26, step: 8
	action: tensor([[ 0.4814, -0.0648, -0.1119, -0.0189,  0.5671,  0.1415,  0.0243]],
       dtype=torch.float64)
	q_value: tensor([[-6.9523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6307151317841079, distance: 0.6954044243777006 entropy 0.03264415264129639
epoch: 26, step: 9
	action: tensor([[ 0.2243,  0.1075, -0.1707, -0.1654, -0.0734,  0.5449,  0.0107]],
       dtype=torch.float64)
	q_value: tensor([[-5.6672]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6039352305335512, distance: 0.7201779160280242 entropy 0.03264415264129639
epoch: 26, step: 10
	action: tensor([[ 0.4862,  0.5197,  0.2402, -0.2126, -0.1835,  0.2132, -0.5032]],
       dtype=torch.float64)
	q_value: tensor([[-6.3049]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.872254606455376, distance: 0.40900565957833246 entropy 0.03264415264129639
epoch: 26, step: 11
	action: tensor([[ 0.3591, -0.6621, -0.5615,  0.1066, -0.0050,  0.1597, -0.0074]],
       dtype=torch.float64)
	q_value: tensor([[-8.0131]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.042270015340124645, distance: 1.1198973978657023 entropy 0.03264415264129639
epoch: 26, step: 12
	action: tensor([[ 0.5126,  0.0685, -0.2585,  0.0344,  0.0176,  0.0370, -0.1374]],
       dtype=torch.float64)
	q_value: tensor([[-5.7556]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7284967537662852, distance: 0.5962717124152986 entropy 0.03264415264129639
epoch: 26, step: 13
	action: tensor([[ 0.1635,  0.0727, -0.1696, -0.3188,  0.1529,  0.0169, -0.1491]],
       dtype=torch.float64)
	q_value: tensor([[-5.7469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3900253756866594, distance: 0.8937428436913022 entropy 0.03264415264129639
epoch: 26, step: 14
	action: tensor([[ 0.5468,  0.3367,  0.1227, -0.3853, -0.2939, -0.3264, -0.6249]],
       dtype=torch.float64)
	q_value: tensor([[-5.1698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7008425541000127, distance: 0.6259023780718255 entropy 0.03264415264129639
epoch: 26, step: 15
	action: tensor([[ 0.1690,  0.2015, -0.4127, -0.1780, -0.0786,  0.3184, -0.2142]],
       dtype=torch.float64)
	q_value: tensor([[-8.1656]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5901456865913355, distance: 0.7326076482860306 entropy 0.03264415264129639
epoch: 26, step: 16
	action: tensor([[-0.0184,  0.0499, -0.4794,  0.0364, -0.3859, -0.0517, -0.2221]],
       dtype=torch.float64)
	q_value: tensor([[-5.9947]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3301890348672547, distance: 0.9365539927637615 entropy 0.03264415264129639
epoch: 26, step: 17
	action: tensor([[ 0.5053, -0.0292,  0.2295, -0.2280, -0.0439,  0.2152, -0.3834]],
       dtype=torch.float64)
	q_value: tensor([[-6.0265]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.577155522297413, distance: 0.7441269349162718 entropy 0.03264415264129639
epoch: 26, step: 18
	action: tensor([[ 0.5821, -0.0585,  0.2345,  0.0355,  0.0047, -0.0537, -0.2860]],
       dtype=torch.float64)
	q_value: tensor([[-6.2410]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6559676325519981, distance: 0.6712067962964882 entropy 0.03264415264129639
epoch: 26, step: 19
	action: tensor([[-0.2896,  0.0928,  0.2592, -0.3962, -0.0215, -0.1815, -0.6255]],
       dtype=torch.float64)
	q_value: tensor([[-6.3061]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23120295846406624, distance: 1.269759639400419 entropy 0.03264415264129639
epoch: 26, step: 20
	action: tensor([[-0.2708, -0.2317,  0.2735,  0.3588,  0.1976,  0.0136, -0.0362]],
       dtype=torch.float64)
	q_value: tensor([[-6.0589]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05422945068637941, distance: 1.1128832000101296 entropy 0.03264415264129639
epoch: 26, step: 21
	action: tensor([[-0.4566, -0.1866, -0.1499, -0.2700,  0.1353, -0.1734,  0.0160]],
       dtype=torch.float64)
	q_value: tensor([[-5.5001]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5195494689734359, distance: 1.4106332449784116 entropy 0.03264415264129639
epoch: 26, step: 22
	action: tensor([[-0.2584,  0.1844, -0.0718,  0.0879,  0.2294,  0.1499, -0.4150]],
       dtype=torch.float64)
	q_value: tensor([[-5.7157]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1626664118432889, distance: 1.0471428930680549 entropy 0.03264415264129639
epoch: 26, step: 23
	action: tensor([[-0.2340,  0.2867, -0.0182, -0.2818, -0.1721, -0.0597, -0.2285]],
       dtype=torch.float64)
	q_value: tensor([[-5.2376]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09820743897247142, distance: 1.0867008768627362 entropy 0.03264415264129639
epoch: 26, step: 24
	action: tensor([[ 0.1141, -0.0719, -0.2110,  0.1390, -0.1413, -0.2208,  0.0501]],
       dtype=torch.float64)
	q_value: tensor([[-5.9651]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3402396280207869, distance: 0.9295008825750957 entropy 0.03264415264129639
epoch: 26, step: 25
	action: tensor([[ 0.2292, -0.1185, -0.0018, -0.2820,  0.0161, -0.1437, -0.2568]],
       dtype=torch.float64)
	q_value: tensor([[-5.4403]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20025252650752978, distance: 1.023371060693856 entropy 0.03264415264129639
epoch: 26, step: 26
	action: tensor([[ 0.1583,  0.3975, -0.4386,  0.2425, -0.1284,  0.0116,  0.1596]],
       dtype=torch.float64)
	q_value: tensor([[-5.3135]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.64734873091409, distance: 0.6795625210200013 entropy 0.03264415264129639
epoch: 26, step: 27
	action: tensor([[-0.2682, -0.3492,  0.0906, -0.0172, -0.0293,  0.3063,  0.2579]],
       dtype=torch.float64)
	q_value: tensor([[-6.4079]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.19962007445659946, distance: 1.2533678647806614 entropy 0.03264415264129639
epoch: 26, step: 28
	action: tensor([[ 0.2209,  0.0584,  0.0449, -0.4220,  0.2508, -0.2172, -0.3510]],
       dtype=torch.float64)
	q_value: tensor([[-5.7640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23665697351740456, distance: 0.999807901105006 entropy 0.03264415264129639
epoch: 26, step: 29
	action: tensor([[-0.0676,  0.1390, -0.7773, -0.3212,  0.4136,  0.2712,  0.0130]],
       dtype=torch.float64)
	q_value: tensor([[-5.5725]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35169965507556034, distance: 0.9213928113762104 entropy 0.03264415264129639
epoch: 26, step: 30
	action: tensor([[ 0.2708, -0.1620, -0.4312, -0.1106,  0.2119,  0.0259,  0.0322]],
       dtype=torch.float64)
	q_value: tensor([[-6.9936]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33985464150937006, distance: 0.9297720364134167 entropy 0.03264415264129639
epoch: 26, step: 31
	action: tensor([[ 0.2854,  0.2073, -0.1499,  0.1319, -0.0054, -0.0104, -0.4664]],
       dtype=torch.float64)
	q_value: tensor([[-5.2952]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6811308548581496, distance: 0.6461940309327021 entropy 0.03264415264129639
epoch: 26, step: 32
	action: tensor([[-0.0443,  0.2636, -0.2289,  0.2534, -0.1062,  0.1866,  0.2514]],
       dtype=torch.float64)
	q_value: tensor([[-5.6756]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.48009987133504217, distance: 0.8251191292033648 entropy 0.03264415264129639
epoch: 26, step: 33
	action: tensor([[-0.0506,  0.1161, -0.3247,  0.2556,  0.1920,  0.0909, -0.3319]],
       dtype=torch.float64)
	q_value: tensor([[-5.9238]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.40238561236376724, distance: 0.8846413097393564 entropy 0.03264415264129639
epoch: 26, step: 34
	action: tensor([[ 0.0690,  0.1250, -0.1440,  0.0786,  0.5256,  0.0608, -0.2752]],
       dtype=torch.float64)
	q_value: tensor([[-5.2179]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4881843776668411, distance: 0.818678645228804 entropy 0.03264415264129639
epoch: 26, step: 35
	action: tensor([[-0.1093,  0.4394, -0.2394,  0.1416, -0.0866, -0.2567, -0.1592]],
       dtype=torch.float64)
	q_value: tensor([[-5.0405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4709781211495083, distance: 0.8323260935982034 entropy 0.03264415264129639
epoch: 26, step: 36
	action: tensor([[ 0.3417,  0.3520, -0.0061, -0.2923,  0.1308, -0.3361, -0.0936]],
       dtype=torch.float64)
	q_value: tensor([[-6.0461]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6301657202288147, distance: 0.695921533605739 entropy 0.03264415264129639
epoch: 26, step: 37
	action: tensor([[-0.0569,  0.2921,  0.0375,  0.3330, -0.1797, -0.0728, -0.0296]],
       dtype=torch.float64)
	q_value: tensor([[-6.4264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5437466317745339, distance: 0.7729648320541345 entropy 0.03264415264129639
epoch: 26, step: 38
	action: tensor([[-0.0696,  0.4854,  0.1758, -0.3658,  0.2968,  0.0038,  0.1802]],
       dtype=torch.float64)
	q_value: tensor([[-5.9610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3550448682441575, distance: 0.9190125552407865 entropy 0.03264415264129639
epoch: 26, step: 39
	action: tensor([[ 0.0934, -0.2402, -0.2005,  0.0118,  0.5104, -0.1529, -0.3114]],
       dtype=torch.float64)
	q_value: tensor([[-6.2685]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10400065261367442, distance: 1.0832047104757083 entropy 0.03264415264129639
epoch: 26, step: 40
	action: tensor([[ 0.5037, -0.0735,  0.0157, -0.1556, -0.0487, -0.1277,  0.2799]],
       dtype=torch.float64)
	q_value: tensor([[-4.9512]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4796064648365346, distance: 0.8255105722619231 entropy 0.03264415264129639
epoch: 26, step: 41
	action: tensor([[ 0.3712, -0.0252, -0.4110, -0.3230, -0.1674,  0.2565,  0.2624]],
       dtype=torch.float64)
	q_value: tensor([[-6.2188]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4860709579464024, distance: 0.8203671723789814 entropy 0.03264415264129639
epoch: 26, step: 42
	action: tensor([[ 0.1664,  0.0131,  0.1352, -0.2601, -0.2265, -0.2011,  0.0113]],
       dtype=torch.float64)
	q_value: tensor([[-6.8875]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27473204933054607, distance: 0.9745540632652582 entropy 0.03264415264129639
epoch: 26, step: 43
	action: tensor([[ 0.2158,  0.2239, -0.4150, -0.1276, -0.2796,  0.1139,  0.1099]],
       dtype=torch.float64)
	q_value: tensor([[-5.9761]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6079345666568008, distance: 0.7165326266194743 entropy 0.03264415264129639
epoch: 26, step: 44
	action: tensor([[ 0.2552,  0.1852,  0.2641, -0.2583, -0.1280, -0.0029, -0.2147]],
       dtype=torch.float64)
	q_value: tensor([[-6.5243]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5179090230317691, distance: 0.7945499320017904 entropy 0.03264415264129639
epoch: 26, step: 45
	action: tensor([[ 0.3092,  0.4235,  0.0499,  0.0421,  0.3185, -0.1575, -0.4667]],
       dtype=torch.float64)
	q_value: tensor([[-6.1537]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7749242372297326, distance: 0.5429015212474597 entropy 0.03264415264129639
epoch: 26, step: 46
	action: tensor([[-0.3549, -0.0558, -0.3436, -0.5882,  0.0587,  0.2168, -0.1300]],
       dtype=torch.float64)
	q_value: tensor([[-6.0075]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.20702865428192574, distance: 1.257232164661866 entropy 0.03264415264129639
epoch: 26, step: 47
	action: tensor([[ 0.2020,  0.2626, -0.3483, -0.1221, -0.3062,  0.2669, -0.1530]],
       dtype=torch.float64)
	q_value: tensor([[-6.3870]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5991411644239133, distance: 0.7245234118149405 entropy 0.03264415264129639
epoch: 26, step: 48
	action: tensor([[ 0.5252, -0.0555, -0.1268, -0.3861, -0.1085,  0.3780, -0.4423]],
       dtype=torch.float64)
	q_value: tensor([[-6.5302]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5327179573854708, distance: 0.782251201900568 entropy 0.03264415264129639
epoch: 26, step: 49
	action: tensor([[ 0.3946,  0.0536, -0.6842,  0.0578, -0.3189,  0.1317, -0.1585]],
       dtype=torch.float64)
	q_value: tensor([[-6.6591]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6651987327204913, distance: 0.6621406333139154 entropy 0.03264415264129639
epoch: 26, step: 50
	action: tensor([[ 0.1376,  0.1092, -0.0506, -0.2590,  0.1102,  0.0267, -0.2662]],
       dtype=torch.float64)
	q_value: tensor([[-6.7206]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.40735735966896724, distance: 0.8809538158237944 entropy 0.03264415264129639
epoch: 26, step: 51
	action: tensor([[-0.0752,  0.1089, -0.2670, -0.1654,  0.0106, -0.5304, -0.2089]],
       dtype=torch.float64)
	q_value: tensor([[-5.1249]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21330904069886314, distance: 1.0149830107726256 entropy 0.03264415264129639
epoch: 26, step: 52
	action: tensor([[-0.2697,  0.2298, -0.3798,  0.4424, -0.2623, -0.1188, -0.5184]],
       dtype=torch.float64)
	q_value: tensor([[-6.1577]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1994810348507089, distance: 1.0238645489174407 entropy 0.03264415264129639
epoch: 26, step: 53
	action: tensor([[ 0.0819,  0.5983, -0.3514,  0.0917, -0.3947,  0.1896,  0.0835]],
       dtype=torch.float64)
	q_value: tensor([[-6.3145]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6283206574109466, distance: 0.6976553121394355 entropy 0.03264415264129639
epoch: 26, step: 54
	action: tensor([[ 0.0380,  0.1791, -0.7775,  0.0679, -0.3903, -0.5495,  0.1640]],
       dtype=torch.float64)
	q_value: tensor([[-7.5280]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5623513291154132, distance: 0.7570411733584922 entropy 0.03264415264129639
epoch: 26, step: 55
	action: tensor([[ 0.4619,  0.2337, -0.0812,  0.1817,  0.4806, -0.0410, -0.5021]],
       dtype=torch.float64)
	q_value: tensor([[-7.9604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8350969600210862, distance: 0.46469823635543456 entropy 0.03264415264129639
epoch: 26, step: 56
	action: tensor([[ 0.1929, -0.0441,  0.5544,  0.0340,  0.0996,  0.2294, -0.2698]],
       dtype=torch.float64)
	q_value: tensor([[-5.9444]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5275113722392657, distance: 0.7865971585843832 entropy 0.03264415264129639
epoch: 26, step: 57
	action: tensor([[ 0.3086, -0.0176, -0.1378, -0.0901, -0.1804,  0.4269,  0.1066]],
       dtype=torch.float64)
	q_value: tensor([[-5.9298]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5873471017238348, distance: 0.7351046046231764 entropy 0.03264415264129639
epoch: 26, step: 58
	action: tensor([[ 0.4199, -0.1698, -0.3315, -0.6491,  0.2411,  0.3368, -0.3849]],
       dtype=torch.float64)
	q_value: tensor([[-6.1318]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30174045125984683, distance: 0.9562360994366305 entropy 0.03264415264129639
epoch: 26, step: 59
	action: tensor([[-0.1813, -0.0278,  0.1096,  0.3117,  0.0688,  0.6178,  0.1251]],
       dtype=torch.float64)
	q_value: tensor([[-6.3406]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4361188439150572, distance: 0.8593112231816501 entropy 0.03264415264129639
epoch: 26, step: 60
	action: tensor([[ 0.2994,  0.1687, -0.2602,  0.2461,  0.0756,  0.2218,  0.0735]],
       dtype=torch.float64)
	q_value: tensor([[-6.0261]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7382566138553832, distance: 0.5854563914006709 entropy 0.03264415264129639
epoch: 26, step: 61
	action: tensor([[ 0.0791, -0.2375, -0.0603, -0.1080, -0.2279, -0.2088, -0.0085]],
       dtype=torch.float64)
	q_value: tensor([[-5.4618]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09358961078975048, distance: 1.0894796700962195 entropy 0.03264415264129639
epoch: 26, step: 62
	action: tensor([[ 0.1485,  0.2345,  0.3585, -0.1121, -0.2738,  0.2805,  0.1673]],
       dtype=torch.float64)
	q_value: tensor([[-5.5639]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5878892839855787, distance: 0.7346215210418687 entropy 0.03264415264129639
epoch: 26, step: 63
	action: tensor([[-0.1464,  0.3037, -0.0644,  0.4813,  0.0205,  0.5212,  0.0772]],
       dtype=torch.float64)
	q_value: tensor([[-6.7943]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5530867268027803, distance: 0.7650121274355054 entropy 0.03264415264129639
epoch: 26, step: 64
	action: tensor([[-0.4197,  0.3963, -0.1208, -0.3116, -0.1115, -0.0574, -0.0318]],
       dtype=torch.float64)
	q_value: tensor([[-6.3722]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.00520931821112347, distance: 1.1473210090007235 entropy 0.03264415264129639
epoch: 26, step: 65
	action: tensor([[-0.0365,  0.1336,  0.0123, -0.1598, -0.1333, -0.0186, -0.5144]],
       dtype=torch.float64)
	q_value: tensor([[-6.4469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2767461251200364, distance: 0.9731999497033261 entropy 0.03264415264129639
epoch: 26, step: 66
	action: tensor([[ 0.0595,  0.1789, -0.3698,  0.1750,  0.0264, -0.6499,  0.1660]],
       dtype=torch.float64)
	q_value: tensor([[-5.6877]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.47090006019117914, distance: 0.8323874991643112 entropy 0.03264415264129639
epoch: 26, step: 67
	action: tensor([[-0.0538, -0.6066, -0.3297, -0.2386,  0.4135, -0.0771, -0.5168]],
       dtype=torch.float64)
	q_value: tensor([[-6.9865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3752627292983042, distance: 1.3419907735517462 entropy 0.03264415264129639
epoch: 26, step: 68
	action: tensor([[ 0.3961,  0.3097, -0.0507, -0.3971,  0.2391,  0.4150, -0.1820]],
       dtype=torch.float64)
	q_value: tensor([[-5.2756]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7373059385799954, distance: 0.5865186425887485 entropy 0.03264415264129639
epoch: 26, step: 69
	action: tensor([[-0.2744,  0.2004, -0.1254,  0.0076, -0.1861,  0.1424,  0.0197]],
       dtype=torch.float64)
	q_value: tensor([[-6.3237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15844556064233617, distance: 1.049778806933744 entropy 0.03264415264129639
epoch: 26, step: 70
	action: tensor([[ 0.4635, -0.4462,  0.0805, -0.2176, -0.2639, -0.0513, -0.4239]],
       dtype=torch.float64)
	q_value: tensor([[-5.7330]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07391120100822901, distance: 1.101242614935688 entropy 0.03264415264129639
epoch: 26, step: 71
	action: tensor([[ 0.0677, -0.2514, -0.0284, -0.1753,  0.0344,  0.0197, -0.1554]],
       dtype=torch.float64)
	q_value: tensor([[-6.2246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07018751234865073, distance: 1.1034543741217309 entropy 0.03264415264129639
epoch: 26, step: 72
	action: tensor([[-0.0358,  0.5819, -0.0623, -0.3002, -0.1393,  0.5561,  0.2071]],
       dtype=torch.float64)
	q_value: tensor([[-4.9644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.565535336624506, distance: 0.7542823109669833 entropy 0.03264415264129639
epoch: 26, step: 73
	action: tensor([[ 0.0446,  0.2274, -0.0477, -0.1462, -0.5277, -0.1275,  0.5629]],
       dtype=torch.float64)
	q_value: tensor([[-8.0505]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42449281268930683, distance: 0.8681246123375134 entropy 0.03264415264129639
epoch: 26, step: 74
	action: tensor([[ 0.0215, -0.3760, -0.4117,  0.0160, -0.2069, -0.0707, -0.0412]],
       dtype=torch.float64)
	q_value: tensor([[-7.7471]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.012179961691925478, distance: 1.1373538685428108 entropy 0.03264415264129639
epoch: 26, step: 75
	action: tensor([[ 0.2497,  0.3393, -0.1135, -0.1987,  0.3588, -0.0231, -0.4256]],
       dtype=torch.float64)
	q_value: tensor([[-5.5186]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.650662263948868, distance: 0.6763623804572721 entropy 0.03264415264129639
epoch: 26, step: 76
	action: tensor([[-0.1044,  0.2256, -0.0956, -0.2247,  0.1847,  0.8003, -0.1675]],
       dtype=torch.float64)
	q_value: tensor([[-5.4716]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4683507042666677, distance: 0.8343904307640359 entropy 0.03264415264129639
epoch: 26, step: 77
	action: tensor([[ 0.1251, -0.0549, -0.1713,  0.4135,  0.0016,  0.0876, -0.1527]],
       dtype=torch.float64)
	q_value: tensor([[-6.8545]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5506041060420798, distance: 0.7671340209021881 entropy 0.03264415264129639
epoch: 26, step: 78
	action: tensor([[-0.0483,  0.0942, -0.1313, -0.1574,  0.3870, -0.1107,  0.3579]],
       dtype=torch.float64)
	q_value: tensor([[-5.2904]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21195917695349753, distance: 1.0158534297919783 entropy 0.03264415264129639
epoch: 26, step: 79
	action: tensor([[ 0.2405, -0.2063, -0.5346,  0.0957,  0.2276, -0.6283,  0.0459]],
       dtype=torch.float64)
	q_value: tensor([[-5.8860]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23034482627583386, distance: 1.003933138199487 entropy 0.03264415264129639
epoch: 26, step: 80
	action: tensor([[-0.0511,  0.5392,  0.0035,  0.0024, -0.1282,  0.0900, -0.0292]],
       dtype=torch.float64)
	q_value: tensor([[-6.9445]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5311012335516163, distance: 0.78360326771111 entropy 0.03264415264129639
epoch: 26, step: 81
	action: tensor([[ 0.3420,  0.0361,  0.1369, -0.1070,  0.2598,  0.1370,  0.1518]],
       dtype=torch.float64)
	q_value: tensor([[-6.3684]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5760249596552598, distance: 0.7451210599111656 entropy 0.03264415264129639
epoch: 26, step: 82
	action: tensor([[ 0.6437,  0.2752, -0.3949, -0.2521,  0.1370,  0.0192,  0.0416]],
       dtype=torch.float64)
	q_value: tensor([[-5.5009]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8105259898251154, distance: 0.49811717540784906 entropy 0.03264415264129639
epoch: 26, step: 83
	action: tensor([[-0.0732, -0.0181, -0.0832, -0.4684, -0.0810,  0.3194,  0.4790]],
       dtype=torch.float64)
	q_value: tensor([[-7.1469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0685340596982682, distance: 1.104435055274585 entropy 0.03264415264129639
epoch: 26, step: 84
	action: tensor([[ 0.3201, -0.0119, -0.1005, -0.2963,  0.5519,  0.6278, -0.2358]],
       dtype=torch.float64)
	q_value: tensor([[-7.0634]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.601057589230482, distance: 0.7227894371303137 entropy 0.03264415264129639
epoch: 26, step: 85
	action: tensor([[ 0.1348, -0.2901, -0.2159, -0.0019, -0.0034,  0.4786,  0.0971]],
       dtype=torch.float64)
	q_value: tensor([[-5.8443]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.303846181322605, distance: 0.9547931578577084 entropy 0.03264415264129639
epoch: 26, step: 86
	action: tensor([[-0.2443,  0.4758,  0.0562,  0.3182, -0.1932, -0.0248, -0.5903]],
       dtype=torch.float64)
	q_value: tensor([[-5.3868]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4361361734579281, distance: 0.859298018639402 entropy 0.03264415264129639
epoch: 26, step: 87
	action: tensor([[-0.1969,  0.5070,  0.1516, -0.0905,  0.1484, -0.1404, -0.4565]],
       dtype=torch.float64)
	q_value: tensor([[-6.6242]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3302154989349846, distance: 0.9365354910711908 entropy 0.03264415264129639
epoch: 26, step: 88
	action: tensor([[ 0.3918,  0.3399,  0.0983, -0.1035,  0.0242, -0.0104,  0.2042]],
       dtype=torch.float64)
	q_value: tensor([[-6.0617]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7544672156713089, distance: 0.567037051523955 entropy 0.03264415264129639
epoch: 26, step: 89
	action: tensor([[ 0.1856, -0.0480,  0.2437, -0.4675,  0.4729,  0.5367, -0.3344]],
       dtype=torch.float64)
	q_value: tensor([[-6.1875]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3178243364914335, distance: 0.9451588479268034 entropy 0.03264415264129639
epoch: 26, step: 90
	action: tensor([[-0.3031,  0.4080, -0.4054, -0.1935, -0.1445, -0.0372, -0.1898]],
       dtype=torch.float64)
	q_value: tensor([[-5.5511]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22397742197724335, distance: 1.00807738456324 entropy 0.03264415264129639
epoch: 26, step: 91
	action: tensor([[-0.0985,  0.3382, -0.6235,  0.0466, -0.3944, -0.5079, -0.0625]],
       dtype=torch.float64)
	q_value: tensor([[-6.4768]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5048786070115205, distance: 0.8052162653456655 entropy 0.03264415264129639
epoch: 26, step: 92
	action: tensor([[ 0.1350, -0.1585, -0.1975, -0.0181,  0.2333,  0.3493, -0.2779]],
       dtype=torch.float64)
	q_value: tensor([[-7.5391]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39058440208909806, distance: 0.8933332033941392 entropy 0.03264415264129639
epoch: 26, step: 93
	action: tensor([[-0.1874, -0.1047, -0.3498, -0.2589, -0.4524,  0.1684,  0.0483]],
       dtype=torch.float64)
	q_value: tensor([[-4.7767]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04664658638459285, distance: 1.1707299370052702 entropy 0.03264415264129639
epoch: 26, step: 94
	action: tensor([[ 1.6683e-04,  4.0186e-01, -1.9225e-01, -1.5695e-01, -6.3710e-02,
          3.2536e-01,  1.0376e-01]], dtype=torch.float64)
	q_value: tensor([[-6.5519]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5164398034809223, distance: 0.7957597453827853 entropy 0.03264415264129639
epoch: 26, step: 95
	action: tensor([[ 0.4004,  0.2576,  0.0511, -0.0611, -0.0605,  0.2586,  0.1365]],
       dtype=torch.float64)
	q_value: tensor([[-6.4223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7743804018280569, distance: 0.5435570137153709 entropy 0.03264415264129639
epoch: 26, step: 96
	action: tensor([[ 0.7029, -0.1080, -0.4329, -0.2537, -0.2179,  0.3500, -0.2491]],
       dtype=torch.float64)
	q_value: tensor([[-6.1576]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5866629005795485, distance: 0.735713774100525 entropy 0.03264415264129639
epoch: 26, step: 97
	action: tensor([[ 0.4215,  0.0210,  0.0582, -0.0245, -0.0883, -0.0124, -0.0583]],
       dtype=torch.float64)
	q_value: tensor([[-7.2646]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6295957711312743, distance: 0.6964575671410504 entropy 0.03264415264129639
epoch: 26, step: 98
	action: tensor([[ 0.1018,  0.4461, -0.4668, -0.0257, -0.0009, -0.0798, -0.1242]],
       dtype=torch.float64)
	q_value: tensor([[-5.6851]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6543845311464025, distance: 0.6727493384003999 entropy 0.03264415264129639
epoch: 26, step: 99
	action: tensor([[ 0.4095,  0.4305, -0.5216, -0.4237,  0.3666,  0.3388, -0.3454]],
       dtype=torch.float64)
	q_value: tensor([[-6.2455]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8539999608930064, distance: 0.43725338406272957 entropy 0.03264415264129639
epoch: 26, step: 100
	action: tensor([[ 0.4623,  0.1929, -0.2341, -0.2321,  0.1077,  0.3987, -0.1329]],
       dtype=torch.float64)
	q_value: tensor([[-7.2761]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7824479393055621, distance: 0.5337504988790158 entropy 0.03264415264129639
epoch: 26, step: 101
	action: tensor([[ 0.0048,  0.0870,  0.1137, -0.1523,  0.2547, -0.2868, -0.0216]],
       dtype=torch.float64)
	q_value: tensor([[-6.1567]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19936373409145847, distance: 1.0239395600632817 entropy 0.03264415264129639
epoch: 26, step: 102
	action: tensor([[ 0.0622, -0.2513,  0.0587,  0.0704,  0.1615,  0.1298,  0.1773]],
       dtype=torch.float64)
	q_value: tensor([[-5.4137]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21999269225916251, distance: 1.0106622143710235 entropy 0.03264415264129639
epoch: 26, step: 103
	action: tensor([[ 0.2283,  0.0331,  0.1423, -0.3461, -0.2788, -0.3426, -0.2144]],
       dtype=torch.float64)
	q_value: tensor([[-5.2038]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2487222345373018, distance: 0.9918750390019527 entropy 0.03264415264129639
epoch: 26, step: 104
	action: tensor([[ 0.3085, -0.0932,  0.2054,  0.0321, -0.4244,  0.4288, -0.0355]],
       dtype=torch.float64)
	q_value: tensor([[-6.5208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5623672276601664, distance: 0.7570274226457807 entropy 0.03264415264129639
epoch: 26, step: 105
	action: tensor([[-0.1771,  0.6866,  0.3925,  0.0983,  0.0184,  0.2129, -0.1066]],
       dtype=torch.float64)
	q_value: tensor([[-6.7937]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5361818631845907, distance: 0.7793464415060448 entropy 0.03264415264129639
epoch: 26, step: 106
	action: tensor([[ 0.1428,  0.0740,  0.0713, -0.0244, -0.4801,  0.1624, -0.4573]],
       dtype=torch.float64)
	q_value: tensor([[-6.9532]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4818867088122518, distance: 0.8236999886031413 entropy 0.03264415264129639
epoch: 26, step: 107
	action: tensor([[-0.0291,  0.1363,  0.0848, -0.2792,  0.1466,  0.2412, -0.1287]],
       dtype=torch.float64)
	q_value: tensor([[-6.6755]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30007445542345923, distance: 0.9573761742476269 entropy 0.03264415264129639
epoch: 26, step: 108
	action: tensor([[ 0.1416,  0.3745, -0.3760,  0.0636,  0.4125, -0.2887, -0.4112]],
       dtype=torch.float64)
	q_value: tensor([[-5.2160]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6176746164727092, distance: 0.7075762702628536 entropy 0.03264415264129639
epoch: 26, step: 109
	action: tensor([[ 0.3444,  0.4871,  0.4066,  0.2918, -0.2104,  0.0513, -0.2115]],
       dtype=torch.float64)
	q_value: tensor([[-6.1622]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8835060768149907, distance: 0.39057849616310386 entropy 0.03264415264129639
epoch: 26, step: 110
	action: tensor([[ 0.1239,  0.3282,  0.2638, -0.0823,  0.2513,  0.2070, -0.0685]],
       dtype=torch.float64)
	q_value: tensor([[-7.2839]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6135253132399681, distance: 0.7114055028309821 entropy 0.03264415264129639
epoch: 26, step: 111
	action: tensor([[ 0.3186, -0.1867,  0.2277,  0.1956,  0.1482,  0.0094, -0.3394]],
       dtype=torch.float64)
	q_value: tensor([[-5.6429]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5100662499834747, distance: 0.8009868239781148 entropy 0.03264415264129639
epoch: 26, step: 112
	action: tensor([[-0.1302,  0.1816, -0.0522,  0.0284, -0.0623,  0.4214, -0.4818]],
       dtype=torch.float64)
	q_value: tensor([[-5.5145]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3639495874173688, distance: 0.9126462267784257 entropy 0.03264415264129639
epoch: 26, step: 113
	action: tensor([[ 0.3609,  0.1779,  0.0720, -0.3318,  0.2583, -0.0745,  0.0259]],
       dtype=torch.float64)
	q_value: tensor([[-5.8586]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5289701985024389, distance: 0.7853818957823474 entropy 0.03264415264129639
epoch: 26, step: 114
	action: tensor([[ 0.2369,  0.3814, -0.1143,  0.4355, -0.0642, -0.1675, -0.1908]],
       dtype=torch.float64)
	q_value: tensor([[-5.7555]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7582822213787127, distance: 0.5626145902286092 entropy 0.03264415264129639
epoch: 26, step: 115
	action: tensor([[ 0.5125, -0.1935, -0.3670, -0.1018,  0.2537, -0.0072,  0.1454]],
       dtype=torch.float64)
	q_value: tensor([[-6.2146]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4472082606356358, distance: 0.8508195598812188 entropy 0.03264415264129639
epoch: 26, step: 116
	action: tensor([[-0.0211,  0.0181, -0.3241, -0.0286,  0.2116,  0.1127, -0.1609]],
       dtype=torch.float64)
	q_value: tensor([[-5.9870]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3116923644820211, distance: 0.9493972882454526 entropy 0.03264415264129639
epoch: 26, step: 117
	action: tensor([[ 0.1865, -0.3441, -0.0458, -0.1740,  0.1415,  0.6379,  0.4647]],
       dtype=torch.float64)
	q_value: tensor([[-4.7855]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2807582767396706, distance: 0.9704968491350862 entropy 0.03264415264129639
epoch: 26, step: 118
	action: tensor([[-0.5586, -0.1697,  0.2764,  0.1239, -0.1926, -0.0183,  0.0388]],
       dtype=torch.float64)
	q_value: tensor([[-6.5118]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.41803877666895106, distance: 1.3627015400254199 entropy 0.03264415264129639
epoch: 26, step: 119
	action: tensor([[-0.0993,  0.0571, -0.7001,  0.1192, -0.0585,  0.0258,  0.3338]],
       dtype=torch.float64)
	q_value: tensor([[-6.0753]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20268707754563764, distance: 1.0218122261002456 entropy 0.03264415264129639
epoch: 26, step: 120
	action: tensor([[ 0.0979,  0.0307, -0.2298,  0.1508,  0.4697, -0.2170,  0.0662]],
       dtype=torch.float64)
	q_value: tensor([[-6.5636]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3786222636475449, distance: 0.9020581560717524 entropy 0.03264415264129639
epoch: 26, step: 121
	action: tensor([[ 0.4568, -0.0532, -0.0311,  0.2585,  0.0396, -0.1780, -0.0490]],
       dtype=torch.float64)
	q_value: tensor([[-5.5950]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6646332623369058, distance: 0.6626995663155586 entropy 0.03264415264129639
epoch: 26, step: 122
	action: tensor([[-0.0402,  0.7646, -0.3863, -0.3117, -0.0362, -0.0557,  0.0229]],
       dtype=torch.float64)
	q_value: tensor([[-5.7082]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6278939993405698, distance: 0.6980556234584058 entropy 0.03264415264129639
epoch: 26, step: 123
	action: tensor([[ 0.5971,  0.3334, -0.1077, -0.2019, -0.1404,  0.2910, -0.0433]],
       dtype=torch.float64)
	q_value: tensor([[-7.8010]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8629467116311204, distance: 0.42364435450458554 entropy 0.03264415264129639
epoch: 26, step: 124
	action: tensor([[ 0.4907, -0.5739,  0.1558, -0.2501,  0.5383, -0.0537, -0.0109]],
       dtype=torch.float64)
	q_value: tensor([[-7.0890]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1010865343870313, distance: 1.200790985741334 entropy 0.03264415264129639
epoch: 26, step: 125
	action: tensor([[-0.0214,  0.5999, -0.0887, -0.1436,  0.0935,  0.0109,  0.3001]],
       dtype=torch.float64)
	q_value: tensor([[-6.0549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.573269870838136, distance: 0.7475381249288928 entropy 0.03264415264129639
epoch: 26, step: 126
	action: tensor([[ 0.0452,  0.6261, -0.4625,  0.0908,  0.0103,  0.1297, -0.0327]],
       dtype=torch.float64)
	q_value: tensor([[-6.6417]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6380128131697846, distance: 0.6884989660061345 entropy 0.03264415264129639
epoch: 26, step: 127
	action: tensor([[ 0.2964,  0.0178,  0.0065, -0.2983, -0.3698,  0.4781, -0.0979]],
       dtype=torch.float64)
	q_value: tensor([[-6.8098]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5128586464614558, distance: 0.7987009344045641 entropy 0.03264415264129639
LOSS epoch 26 actor 22.093337465419953 critic 52.425975059117704 
epoch: 27, step: 0
	action: tensor([[ 0.4261, -0.0951, -0.3599,  0.3283, -0.2904, -0.2114, -0.6982]],
       dtype=torch.float64)
	q_value: tensor([[-6.4343]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.663399527402325, distance: 0.6639174048174923 entropy 0.03264415264129639
epoch: 27, step: 1
	action: tensor([[ 0.0414,  0.0434, -0.2583, -0.1150, -0.0626,  0.0831, -0.1547]],
       dtype=torch.float64)
	q_value: tensor([[-6.3109]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3541212469497895, distance: 0.9196703652565893 entropy 0.03264415264129639
epoch: 27, step: 2
	action: tensor([[-0.0450, -0.0379, -0.0257, -0.1922,  0.0237, -0.1991, -0.4222]],
       dtype=torch.float64)
	q_value: tensor([[-4.5724]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0944864832451432, distance: 1.0889405290246483 entropy 0.03264415264129639
epoch: 27, step: 3
	action: tensor([[-0.1326,  0.1818, -0.5342, -0.4745, -0.2469,  0.1379, -0.0474]],
       dtype=torch.float64)
	q_value: tensor([[-4.7165]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26002805031850673, distance: 0.984383491567995 entropy 0.03264415264129639
epoch: 27, step: 4
	action: tensor([[ 0.1879,  0.5056, -0.8171, -0.0852, -0.0390,  0.0811, -0.0501]],
       dtype=torch.float64)
	q_value: tensor([[-6.7191]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7242102091184728, distance: 0.6009603041310441 entropy 0.03264415264129639
epoch: 27, step: 5
	action: tensor([[ 0.1396,  0.0586, -0.3190,  0.0433,  0.3192,  0.0796,  0.0561]],
       dtype=torch.float64)
	q_value: tensor([[-7.2220]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4990693596659893, distance: 0.8099262815044733 entropy 0.03264415264129639
epoch: 27, step: 6
	action: tensor([[ 0.1049, -0.1638, -0.0588,  0.2862, -0.1722,  0.2574,  0.1464]],
       dtype=torch.float64)
	q_value: tensor([[-4.6813]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.46316461336199133, distance: 0.8384501775730454 entropy 0.03264415264129639
epoch: 27, step: 7
	action: tensor([[ 0.0717,  0.0400, -0.4081,  0.0901, -0.2223,  0.3304,  0.1547]],
       dtype=torch.float64)
	q_value: tensor([[-4.8836]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.43358478497936037, distance: 0.8612399135554698 entropy 0.03264415264129639
epoch: 27, step: 8
	action: tensor([[-0.1429,  0.0271, -0.0720, -0.2466, -0.2254,  0.3353, -0.7502]],
       dtype=torch.float64)
	q_value: tensor([[-5.2317]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12214814897611792, distance: 1.072179027454827 entropy 0.03264415264129639
epoch: 27, step: 9
	action: tensor([[-0.1023, -0.2220, -0.7177, -0.3933, -0.0891, -0.1398,  0.0699]],
       dtype=torch.float64)
	q_value: tensor([[-6.1313]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04644241275298988, distance: 1.1174552913425215 entropy 0.03264415264129639
epoch: 27, step: 10
	action: tensor([[ 0.3516,  0.0027, -0.2448,  0.0268,  0.1322, -0.1231, -0.1183]],
       dtype=torch.float64)
	q_value: tensor([[-6.1785]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.570035544448163, distance: 0.7503656945218051 entropy 0.03264415264129639
epoch: 27, step: 11
	action: tensor([[ 0.4413, -0.1959,  0.1047,  0.0664, -0.0959,  0.2078, -0.2803]],
       dtype=torch.float64)
	q_value: tensor([[-4.7672]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5708997256827593, distance: 0.7496112390069016 entropy 0.03264415264129639
epoch: 27, step: 12
	action: tensor([[ 0.3296,  0.2411, -0.1669, -0.3490,  0.0229, -0.1815, -0.1593]],
       dtype=torch.float64)
	q_value: tensor([[-5.2085]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5915340561308566, distance: 0.7313657521454863 entropy 0.03264415264129639
epoch: 27, step: 13
	action: tensor([[ 0.2557,  0.0141, -0.0064, -0.1219,  0.2977, -0.3493, -0.3293]],
       dtype=torch.float64)
	q_value: tensor([[-5.5962]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.38006300231736734, distance: 0.9010117842729272 entropy 0.03264415264129639
epoch: 27, step: 14
	action: tensor([[ 0.6532, -0.7438, -0.5141,  0.4006,  0.2072, -0.1473, -0.2655]],
       dtype=torch.float64)
	q_value: tensor([[-4.9365]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15227080388187897, distance: 1.053623051308811 entropy 0.03264415264129639
epoch: 27, step: 15
	action: tensor([[-0.2262, -0.1258, -0.2293,  0.1878,  0.3073,  0.2147,  0.2162]],
       dtype=torch.float64)
	q_value: tensor([[-6.2584]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07673162316085957, distance: 1.0995644078885956 entropy 0.03264415264129639
epoch: 27, step: 16
	action: tensor([[ 0.1662,  0.4697, -0.3775,  0.0899,  0.1680, -0.2637, -0.0479]],
       dtype=torch.float64)
	q_value: tensor([[-4.9268]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6699374067519538, distance: 0.6574380670412929 entropy 0.03264415264129639
epoch: 27, step: 17
	action: tensor([[-0.1096,  0.3591, -0.4464, -0.1663,  0.0360,  0.2060, -0.2224]],
       dtype=torch.float64)
	q_value: tensor([[-5.8017]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4127433340889394, distance: 0.8769415967314942 entropy 0.03264415264129639
epoch: 27, step: 18
	action: tensor([[ 0.4099,  0.0581, -0.1334, -0.0033, -0.0418,  0.3297, -0.0990]],
       dtype=torch.float64)
	q_value: tensor([[-5.5011]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7114756172622558, distance: 0.6146784020115044 entropy 0.03264415264129639
epoch: 27, step: 19
	action: tensor([[ 0.3005,  0.1380, -0.2186, -0.0664,  0.5045,  0.0816, -0.1710]],
       dtype=torch.float64)
	q_value: tensor([[-5.0593]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6368549210356749, distance: 0.6895992412018406 entropy 0.03264415264129639
epoch: 27, step: 20
	action: tensor([[-0.2931, -0.0425, -0.0287,  0.0325, -0.0033,  0.1680, -0.0928]],
       dtype=torch.float64)
	q_value: tensor([[-4.7336]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0013671760610109995, distance: 1.1451262468456298 entropy 0.03264415264129639
epoch: 27, step: 21
	action: tensor([[ 0.0665, -0.0421,  0.0910, -0.1354, -0.3850,  0.5148, -0.3141]],
       dtype=torch.float64)
	q_value: tensor([[-4.6203]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3497978264306236, distance: 0.9227433026990393 entropy 0.03264415264129639
epoch: 27, step: 22
	action: tensor([[-0.0135, -0.0878,  0.1182, -0.1549, -0.0317,  0.0757, -0.3408]],
       dtype=torch.float64)
	q_value: tensor([[-6.1401]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15426537290296372, distance: 1.0523828194635052 entropy 0.03264415264129639
epoch: 27, step: 23
	action: tensor([[-0.1176,  0.0862, -0.5168, -0.0142, -0.2098,  0.2203, -0.4428]],
       dtype=torch.float64)
	q_value: tensor([[-4.6014]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23395129604289355, distance: 1.001578248607328 entropy 0.03264415264129639
epoch: 27, step: 24
	action: tensor([[ 0.5109,  0.1505, -0.2726,  0.1058, -0.0797, -0.1595, -0.1817]],
       dtype=torch.float64)
	q_value: tensor([[-5.4355]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7822186881751332, distance: 0.5340316515512686 entropy 0.03264415264129639
epoch: 27, step: 25
	action: tensor([[ 0.0955, -0.0429,  0.0944, -0.0092, -0.1194, -0.0330, -0.4103]],
       dtype=torch.float64)
	q_value: tensor([[-5.6148]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3523503494780781, distance: 0.9209302976162594 entropy 0.03264415264129639
epoch: 27, step: 26
	action: tensor([[-0.1074, -0.5260, -0.4782, -0.4439,  0.2485,  0.0642, -0.0640]],
       dtype=torch.float64)
	q_value: tensor([[-4.8286]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.30865980963494755, distance: 1.3090917193751934 entropy 0.03264415264129639
epoch: 27, step: 27
	action: tensor([[ 0.1622,  0.0848, -0.1161, -0.2129,  0.0888,  0.1386,  0.0253]],
       dtype=torch.float64)
	q_value: tensor([[-5.1591]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44902538136419134, distance: 0.8494200142958994 entropy 0.03264415264129639
epoch: 27, step: 28
	action: tensor([[ 0.2232,  0.2918,  0.2095, -0.3474,  0.0612, -0.2432, -0.3462]],
       dtype=torch.float64)
	q_value: tensor([[-4.6936]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4700776805992436, distance: 0.8330341375898026 entropy 0.03264415264129639
epoch: 27, step: 29
	action: tensor([[-0.2804,  0.1205,  0.0915, -0.0669,  0.0167, -0.1791,  0.0500]],
       dtype=torch.float64)
	q_value: tensor([[-5.6855]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.005080157498250637, distance: 1.1472472962141016 entropy 0.03264415264129639
epoch: 27, step: 30
	action: tensor([[-0.1463,  0.3297, -0.4246,  0.2802,  0.1833, -0.2355, -0.1196]],
       dtype=torch.float64)
	q_value: tensor([[-4.9846]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3477384774436221, distance: 0.9242034241056236 entropy 0.03264415264129639
epoch: 27, step: 31
	action: tensor([[ 0.1476, -0.1322, -0.0356,  0.3706, -0.6358, -0.1837,  0.0040]],
       dtype=torch.float64)
	q_value: tensor([[-5.2507]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.49720365668303346, distance: 0.811433154288575 entropy 0.03264415264129639
epoch: 27, step: 32
	action: tensor([[ 0.1405,  0.4977, -0.2493,  0.5299, -0.1244,  0.2746, -0.1104]],
       dtype=torch.float64)
	q_value: tensor([[-5.7554]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6444416872552108, distance: 0.6823577234498219 entropy 0.03264415264129639
epoch: 27, step: 33
	action: tensor([[-0.0504, -0.3938, -0.0566, -0.1698, -0.0763,  0.7939, -0.3910]],
       dtype=torch.float64)
	q_value: tensor([[-5.9418]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0835913577975328, distance: 1.0954719994237074 entropy 0.03264415264129639
epoch: 27, step: 34
	action: tensor([[-0.1159, -0.2070,  0.2466, -0.0111, -0.1996, -0.1165,  0.0492]],
       dtype=torch.float64)
	q_value: tensor([[-5.7078]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.02782288264852628, distance: 1.1601545146605456 entropy 0.03264415264129639
epoch: 27, step: 35
	action: tensor([[ 0.0259,  0.2894, -0.1220, -0.1695,  0.2386,  0.5869, -0.0507]],
       dtype=torch.float64)
	q_value: tensor([[-4.9664]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5800376028149183, distance: 0.7415866384437936 entropy 0.03264415264129639
epoch: 27, step: 36
	action: tensor([[-0.1080,  0.2941, -0.1690,  0.0798,  0.1228,  0.2459, -0.5861]],
       dtype=torch.float64)
	q_value: tensor([[-5.5465]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4468394127551554, distance: 0.8511033653699692 entropy 0.03264415264129639
epoch: 27, step: 37
	action: tensor([[-0.1627,  0.1825,  0.1726,  0.1642,  0.1457,  0.2651, -0.3636]],
       dtype=torch.float64)
	q_value: tensor([[-5.2036]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41592560693490155, distance: 0.8745623489141744 entropy 0.03264415264129639
epoch: 27, step: 38
	action: tensor([[-0.1297, -0.3717, -0.7346,  0.0141, -0.0279,  0.4595, -0.0514]],
       dtype=torch.float64)
	q_value: tensor([[-4.9531]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.11365725524459247, distance: 1.207626038650856 entropy 0.03264415264129639
epoch: 27, step: 39
	action: tensor([[ 0.3539,  0.0923, -0.0805, -0.0089,  0.4410,  0.2013, -0.6632]],
       dtype=torch.float64)
	q_value: tensor([[-5.5386]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6708736599480858, distance: 0.656504962797968 entropy 0.03264415264129639
epoch: 27, step: 40
	action: tensor([[-0.0521, -0.2505, -0.1596, -0.2566, -0.0118, -0.2482, -0.5320]],
       dtype=torch.float64)
	q_value: tensor([[-5.0735]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10356756592691352, distance: 1.202143070194794 entropy 0.03264415264129639
epoch: 27, step: 41
	action: tensor([[-0.0250,  0.0245, -0.0854, -0.1069,  0.0849,  0.0211,  0.0278]],
       dtype=torch.float64)
	q_value: tensor([[-4.8743]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24509934192295335, distance: 0.9942637260786478 entropy 0.03264415264129639
epoch: 27, step: 42
	action: tensor([[ 0.3391, -0.1299, -0.5066,  0.0849,  0.1610,  0.4702, -0.3315]],
       dtype=torch.float64)
	q_value: tensor([[-4.5549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5772029120038176, distance: 0.7440852352525323 entropy 0.03264415264129639
epoch: 27, step: 43
	action: tensor([[ 0.2386,  0.1965,  0.3771, -0.7286,  0.2149,  0.0544,  0.1113]],
       dtype=torch.float64)
	q_value: tensor([[-5.1446]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23635618537993952, distance: 1.0000048641522912 entropy 0.03264415264129639
epoch: 27, step: 44
	action: tensor([[ 0.4205,  0.0099, -0.1758, -0.0024, -0.4690,  0.2747,  0.2105]],
       dtype=torch.float64)
	q_value: tensor([[-6.1871]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6475372111494088, distance: 0.6793808950996644 entropy 0.03264415264129639
epoch: 27, step: 45
	action: tensor([[ 0.7684, -0.0611, -0.2627, -0.0884,  0.0118, -0.6179, -0.1414]],
       dtype=torch.float64)
	q_value: tensor([[-6.1976]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5070123520846619, distance: 0.8034793364370202 entropy 0.03264415264129639
epoch: 27, step: 46
	action: tensor([[ 0.1111,  0.2929,  0.2004,  0.1837, -0.0354, -0.2368, -0.1125]],
       dtype=torch.float64)
	q_value: tensor([[-7.2943]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6223257627161076, distance: 0.7032591212831015 entropy 0.03264415264129639
epoch: 27, step: 47
	action: tensor([[ 0.3977, -0.3002, -0.2974,  0.2062, -0.4224,  0.3839,  0.4163]],
       dtype=torch.float64)
	q_value: tensor([[-5.3352]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.507947078560469, distance: 0.8027172587787074 entropy 0.03264415264129639
epoch: 27, step: 48
	action: tensor([[ 0.2531, -0.4499,  0.0861,  0.3967, -0.0923, -0.0549, -0.1844]],
       dtype=torch.float64)
	q_value: tensor([[-6.0746]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37306258587756524, distance: 0.9060846798937414 entropy 0.03264415264129639
epoch: 27, step: 49
	action: tensor([[-0.1998,  0.0860, -0.3191,  0.3348, -0.3267,  0.1350,  0.1524]],
       dtype=torch.float64)
	q_value: tensor([[-5.0034]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22320682861316932, distance: 1.0085777727059044 entropy 0.03264415264129639
epoch: 27, step: 50
	action: tensor([[ 0.1426, -0.2217, -0.4241,  0.0485, -0.3380, -0.0226,  0.2619]],
       dtype=torch.float64)
	q_value: tensor([[-5.4788]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2727219965008252, distance: 0.9759035988199052 entropy 0.03264415264129639
epoch: 27, step: 51
	action: tensor([[-0.1465,  0.7138, -0.3692, -0.3661, -0.3467,  0.0792,  0.0971]],
       dtype=torch.float64)
	q_value: tensor([[-5.4838]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5251070136400984, distance: 0.7885960021672928 entropy 0.03264415264129639
epoch: 27, step: 52
	action: tensor([[-0.3639,  0.3833, -0.6857, -0.2587,  0.0403,  0.0082, -0.3434]],
       dtype=torch.float64)
	q_value: tensor([[-8.0940]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18734881449474716, distance: 1.0315939343991403 entropy 0.03264415264129639
epoch: 27, step: 53
	action: tensor([[ 0.1532,  0.1507, -0.2166, -0.1800, -0.0011,  0.3743,  0.2679]],
       dtype=torch.float64)
	q_value: tensor([[-6.6099]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5383920985724251, distance: 0.7774873118622551 entropy 0.03264415264129639
epoch: 27, step: 54
	action: tensor([[ 0.3879, -0.1786, -0.5017,  0.1212, -0.0933, -0.2034,  0.3153]],
       dtype=torch.float64)
	q_value: tensor([[-5.6962]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.45428801433736643, distance: 0.8453536642769917 entropy 0.03264415264129639
epoch: 27, step: 55
	action: tensor([[ 0.2131, -0.2178, -0.3132, -0.3152, -0.0953,  0.4902, -0.0418]],
       dtype=torch.float64)
	q_value: tensor([[-5.9779]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28368687401421266, distance: 0.9685190067481848 entropy 0.03264415264129639
epoch: 27, step: 56
	action: tensor([[ 0.1154,  0.3912, -0.2907,  0.3559, -0.1339,  0.2284, -0.2556]],
       dtype=torch.float64)
	q_value: tensor([[-5.4711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6368535243455302, distance: 0.689600567332351 entropy 0.03264415264129639
epoch: 27, step: 57
	action: tensor([[-0.1233, -0.0158, -0.6291, -0.0938, -0.1698,  0.0651, -0.2294]],
       dtype=torch.float64)
	q_value: tensor([[-5.4576]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1571099201224727, distance: 1.0506115345351925 entropy 0.03264415264129639
epoch: 27, step: 58
	action: tensor([[ 0.0737, -0.0320, -0.4628, -0.2542,  0.3479,  0.0947,  0.2801]],
       dtype=torch.float64)
	q_value: tensor([[-5.2962]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30348049039404834, distance: 0.9550439022662367 entropy 0.03264415264129639
epoch: 27, step: 59
	action: tensor([[-0.0368,  0.0694, -0.2286, -0.1442,  0.5844,  0.0323, -0.0950]],
       dtype=torch.float64)
	q_value: tensor([[-5.5734]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2717132884542224, distance: 0.9765801356546918 entropy 0.03264415264129639
epoch: 27, step: 60
	action: tensor([[-0.3156,  0.4974, -0.1022,  0.5241, -0.3558,  0.0290, -0.2690]],
       dtype=torch.float64)
	q_value: tensor([[-4.7329]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.379369984753708, distance: 0.9015152568717157 entropy 0.03264415264129639
epoch: 27, step: 61
	action: tensor([[ 0.0181, -0.3472,  0.1472, -0.2542,  0.0946, -0.0940,  0.0903]],
       dtype=torch.float64)
	q_value: tensor([[-6.2258]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18319132255796888, distance: 1.2447558649449406 entropy 0.03264415264129639
epoch: 27, step: 62
	action: tensor([[0.4019, 0.0811, 0.1462, 0.2771, 0.1003, 0.1125, 0.0809]],
       dtype=torch.float64)
	q_value: tensor([[-4.7770]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8030617139326566, distance: 0.5078339956675983 entropy 0.03264415264129639
epoch: 27, step: 63
	action: tensor([[ 0.2163, -0.2704, -0.1012, -0.5199,  0.2258,  0.3962, -0.2142]],
       dtype=torch.float64)
	q_value: tensor([[-5.2453]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09592610258900092, distance: 1.0880745654593502 entropy 0.03264415264129639
epoch: 27, step: 64
	action: tensor([[-0.0765, -0.2061, -0.2802,  0.0892,  0.2379, -0.2043, -0.1274]],
       dtype=torch.float64)
	q_value: tensor([[-5.0707]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.041350350689312365, distance: 1.1204349621769967 entropy 0.03264415264129639
epoch: 27, step: 65
	action: tensor([[ 0.4345, -0.2302, -0.1471, -0.0342,  0.2961, -0.4250, -0.2269]],
       dtype=torch.float64)
	q_value: tensor([[-4.5838]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2878376150553932, distance: 0.9657088445121791 entropy 0.03264415264129639
epoch: 27, step: 66
	action: tensor([[-0.3317,  0.2195, -0.2444,  0.3978, -0.3512, -0.2120, -0.0887]],
       dtype=torch.float64)
	q_value: tensor([[-5.3398]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18646028735980302, distance: 1.0321577364405474 entropy 0.03264415264129639
epoch: 27, step: 67
	action: tensor([[ 0.0080,  0.1390, -0.2627,  0.1211, -0.5181,  0.0720, -0.5650]],
       dtype=torch.float64)
	q_value: tensor([[-5.6373]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4288924247713123, distance: 0.864799945616717 entropy 0.03264415264129639
epoch: 27, step: 68
	action: tensor([[ 0.3792, -0.2481, -0.8102,  0.0180,  0.1045, -0.1276, -0.5511]],
       dtype=torch.float64)
	q_value: tensor([[-6.0473]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3969023359408651, distance: 0.8886904566381663 entropy 0.03264415264129639
epoch: 27, step: 69
	action: tensor([[-0.0083,  0.0210, -0.2541, -0.1517, -0.1154, -0.0335, -0.3750]],
       dtype=torch.float64)
	q_value: tensor([[-5.7978]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2620639613225133, distance: 0.9830283742674268 entropy 0.03264415264129639
epoch: 27, step: 70
	action: tensor([[ 0.0263, -0.1058,  0.2166,  0.1065, -0.2009, -0.0297, -0.2881]],
       dtype=torch.float64)
	q_value: tensor([[-4.7787]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30654867240368666, distance: 0.9529380871696291 entropy 0.03264415264129639
epoch: 27, step: 71
	action: tensor([[ 0.0402, -0.0413,  0.1216, -0.0664,  0.0458,  0.3795, -0.0283]],
       dtype=torch.float64)
	q_value: tensor([[-4.8924]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37943328847090285, distance: 0.9014692788165186 entropy 0.03264415264129639
epoch: 27, step: 72
	action: tensor([[ 0.3506, -0.4760, -0.4644, -0.2018,  0.0427, -0.1974,  0.3963]],
       dtype=torch.float64)
	q_value: tensor([[-4.7180]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.00889755877049514, distance: 1.1494239150037495 entropy 0.03264415264129639
epoch: 27, step: 73
	action: tensor([[ 0.1419, -0.1878, -0.2469, -0.2632, -0.6614,  0.0186, -0.1681]],
       dtype=torch.float64)
	q_value: tensor([[-6.0558]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14259176198956758, distance: 1.0596209084829307 entropy 0.03264415264129639
epoch: 27, step: 74
	action: tensor([[-0.1458,  0.2363, -0.3233, -0.1473, -0.3764,  0.4356, -0.3637]],
       dtype=torch.float64)
	q_value: tensor([[-6.4296]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22617077790279994, distance: 1.0066517578306953 entropy 0.03264415264129639
epoch: 27, step: 75
	action: tensor([[ 0.3578, -0.1325, -0.4746, -0.3513,  0.4185,  0.1189,  0.1474]],
       dtype=torch.float64)
	q_value: tensor([[-6.4481]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3731944902881559, distance: 0.9059893570991263 entropy 0.03264415264129639
epoch: 27, step: 76
	action: tensor([[-0.1145,  0.3125, -0.3590,  0.0027,  0.0900, -0.1478, -0.0738]],
       dtype=torch.float64)
	q_value: tensor([[-5.6465]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3737571390426885, distance: 0.9055826374537728 entropy 0.03264415264129639
epoch: 27, step: 77
	action: tensor([[ 0.2606, -0.2058, -0.6040, -0.1548, -0.1126,  0.0788, -0.1040]],
       dtype=torch.float64)
	q_value: tensor([[-5.0292]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3385735965515553, distance: 0.9306737336581381 entropy 0.03264415264129639
epoch: 27, step: 78
	action: tensor([[ 0.1044,  0.1470, -0.1343, -0.1000,  0.3444,  0.3167, -0.2213]],
       dtype=torch.float64)
	q_value: tensor([[-5.1968]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5463431851307221, distance: 0.7707622092760885 entropy 0.03264415264129639
epoch: 27, step: 79
	action: tensor([[ 0.2296,  0.1998,  0.0130, -0.0571, -0.3189, -0.2945,  0.1082]],
       dtype=torch.float64)
	q_value: tensor([[-4.4535]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5783718455844877, distance: 0.7430559137840396 entropy 0.03264415264129639
epoch: 27, step: 80
	action: tensor([[ 0.0960,  0.0713, -0.0563, -0.2092,  0.0282,  0.0472,  0.1214]],
       dtype=torch.float64)
	q_value: tensor([[-5.7710]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3618346887842234, distance: 0.9141622642002123 entropy 0.03264415264129639
epoch: 27, step: 81
	action: tensor([[ 0.0518, -0.3142, -0.5869, -0.0268, -0.2216,  0.2632,  0.5272]],
       dtype=torch.float64)
	q_value: tensor([[-4.8551]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0922606796869393, distance: 1.0902780463822064 entropy 0.03264415264129639
epoch: 27, step: 82
	action: tensor([[-0.0460,  0.4627, -0.1863,  0.1785,  0.1705, -0.1134, -0.5128]],
       dtype=torch.float64)
	q_value: tensor([[-6.1437]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5240457333905764, distance: 0.7894766787875257 entropy 0.03264415264129639
epoch: 27, step: 83
	action: tensor([[-0.1249, -0.0120, -0.1525, -0.2279,  0.1563, -0.2230, -0.4824]],
       dtype=torch.float64)
	q_value: tensor([[-5.3945]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.031463931410248724, distance: 1.1261975872515122 entropy 0.03264415264129639
epoch: 27, step: 84
	action: tensor([[ 6.8862e-01,  1.9899e-01, -2.6802e-01,  2.3068e-02,  1.0574e-01,
         -3.9401e-04, -2.0368e-01]], dtype=torch.float64)
	q_value: tensor([[-4.7565]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8496475142123323, distance: 0.44372306032878955 entropy 0.03264415264129639
epoch: 27, step: 85
	action: tensor([[ 0.8751,  0.2878, -0.2158, -0.2560,  0.1658,  0.1689, -0.0859]],
       dtype=torch.float64)
	q_value: tensor([[-5.9495]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8615194580945096, distance: 0.42584452742519596 entropy 0.03264415264129639
epoch: 27, step: 86
	action: tensor([[ 0.1518, -0.0107, -0.2954, -0.2740, -0.7357, -0.1279, -0.2140]],
       dtype=torch.float64)
	q_value: tensor([[-7.1940]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35799496036839007, distance: 0.9169083165316594 entropy 0.03264415264129639
epoch: 27, step: 87
	action: tensor([[ 0.0150, -0.0092, -0.1943, -0.3884,  0.4321,  0.0042,  0.0318]],
       dtype=torch.float64)
	q_value: tensor([[-7.0165]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1602302891982007, distance: 1.0486650556862114 entropy 0.03264415264129639
epoch: 27, step: 88
	action: tensor([[-0.4063,  0.4714,  0.0730, -0.4007, -0.0150, -0.3385, -0.0069]],
       dtype=torch.float64)
	q_value: tensor([[-4.7354]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06419632281485277, distance: 1.1805042915814437 entropy 0.03264415264129639
epoch: 27, step: 89
	action: tensor([[-0.0550,  0.0299, -0.5403, -0.0869,  0.3513, -0.3774, -0.4944]],
       dtype=torch.float64)
	q_value: tensor([[-6.1496]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17755506058031367, distance: 1.0377915007620095 entropy 0.03264415264129639
epoch: 27, step: 90
	action: tensor([[ 0.0091, -0.0614,  0.0133, -0.1240,  0.0229, -0.0221, -0.3361]],
       dtype=torch.float64)
	q_value: tensor([[-5.5301]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17824292081474957, distance: 1.037357426215906 entropy 0.03264415264129639
epoch: 27, step: 91
	action: tensor([[ 0.3385, -0.0376, -0.4301,  0.0232,  0.0012, -0.0339, -0.2222]],
       dtype=torch.float64)
	q_value: tensor([[-4.4961]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5295058312758578, distance: 0.7849352192206693 entropy 0.03264415264129639
epoch: 27, step: 92
	action: tensor([[-0.3791, -0.0581, -0.5728, -0.1677, -0.1971,  0.2302,  0.0768]],
       dtype=torch.float64)
	q_value: tensor([[-4.8969]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.18796645711940618, distance: 1.2472651344184678 entropy 0.03264415264129639
epoch: 27, step: 93
	action: tensor([[ 0.1980, -0.1953, -0.6321, -0.5792, -0.1899, -0.2894,  0.1863]],
       dtype=torch.float64)
	q_value: tensor([[-6.0004]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2094718313413395, distance: 1.017455369550718 entropy 0.03264415264129639
epoch: 27, step: 94
	action: tensor([[ 0.1103,  0.2545, -0.0838,  0.2533,  0.0431,  0.0528, -0.0228]],
       dtype=torch.float64)
	q_value: tensor([[-7.0237]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6466293627670427, distance: 0.68025528271686 entropy 0.03264415264129639
epoch: 27, step: 95
	action: tensor([[-0.0116, -0.3085, -0.5077,  0.1097, -0.0054, -0.0833, -0.1733]],
       dtype=torch.float64)
	q_value: tensor([[-4.9756]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05690395502956358, distance: 1.111308548446367 entropy 0.03264415264129639
epoch: 27, step: 96
	action: tensor([[ 0.8517,  0.1357, -0.1329,  0.1196, -0.0783,  0.2665, -0.0423]],
       dtype=torch.float64)
	q_value: tensor([[-4.7589]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9151451714168988, distance: 0.33334578918886093 entropy 0.03264415264129639
epoch: 27, step: 97
	action: tensor([[-0.1100, -0.2289, -0.3877, -0.2651,  0.2050,  0.2812, -0.0842]],
       dtype=torch.float64)
	q_value: tensor([[-6.6190]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.002828332054853999, distance: 1.1427248153439344 entropy 0.03264415264129639
epoch: 27, step: 98
	action: tensor([[ 0.4661,  0.4648, -0.0594, -0.4861, -0.1283, -0.2520,  0.2248]],
       dtype=torch.float64)
	q_value: tensor([[-4.6600]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7212460168650259, distance: 0.6041812371804874 entropy 0.03264415264129639
epoch: 27, step: 99
	action: tensor([[-0.0407,  0.5580,  0.0349, -0.2382, -0.0271,  0.1734, -0.4728]],
       dtype=torch.float64)
	q_value: tensor([[-7.3098]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5262702785451034, distance: 0.7876295649532156 entropy 0.03264415264129639
epoch: 27, step: 100
	action: tensor([[-0.0061, -0.1593, -0.6912,  0.1617, -0.2036,  0.1290, -0.0934]],
       dtype=torch.float64)
	q_value: tensor([[-6.1960]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17479895604798712, distance: 1.0395289238189256 entropy 0.03264415264129639
epoch: 27, step: 101
	action: tensor([[ 0.1369,  0.0557, -0.2342,  0.3257, -0.0930,  0.0353, -0.4386]],
       dtype=torch.float64)
	q_value: tensor([[-5.0627]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5554275655100214, distance: 0.7630060099308767 entropy 0.03264415264129639
epoch: 27, step: 102
	action: tensor([[ 0.1085, -0.2894,  0.0465,  0.0969, -0.3652,  0.2298,  0.0790]],
       dtype=torch.float64)
	q_value: tensor([[-4.9171]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2716151367267694, distance: 0.9766459406436222 entropy 0.03264415264129639
epoch: 27, step: 103
	action: tensor([[-0.0646,  0.2543, -0.2759, -0.2951, -0.0839, -0.2091, -0.2275]],
       dtype=torch.float64)
	q_value: tensor([[-5.0754]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33226140386907643, distance: 0.9351040396074659 entropy 0.03264415264129639
epoch: 27, step: 104
	action: tensor([[-0.5117,  0.4155, -0.2105, -0.3408, -0.1221,  0.1431, -0.2329]],
       dtype=torch.float64)
	q_value: tensor([[-5.3834]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07543928348613282, distance: 1.1867237701778446 entropy 0.03264415264129639
epoch: 27, step: 105
	action: tensor([[ 0.0788,  0.0520, -0.0220,  0.4034,  0.0974, -0.0738,  0.2584]],
       dtype=torch.float64)
	q_value: tensor([[-6.4822]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5314070938158046, distance: 0.7833476558213267 entropy 0.03264415264129639
epoch: 27, step: 106
	action: tensor([[ 0.3812,  0.1402, -0.3158, -0.0602, -0.2216,  0.1867, -0.0819]],
       dtype=torch.float64)
	q_value: tensor([[-5.2317]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6706164387602782, distance: 0.6567614509381399 entropy 0.03264415264129639
epoch: 27, step: 107
	action: tensor([[-0.0208,  0.1031, -0.0406, -0.3522, -0.2352, -0.0103, -0.4216]],
       dtype=torch.float64)
	q_value: tensor([[-5.4964]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20846355816318296, distance: 1.0181040156696732 entropy 0.03264415264129639
epoch: 27, step: 108
	action: tensor([[-0.0662,  0.1934, -0.3880, -0.1269,  0.1206,  0.0762,  0.2892]],
       dtype=torch.float64)
	q_value: tensor([[-5.5376]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35453996672732324, distance: 0.9193722082099516 entropy 0.03264415264129639
epoch: 27, step: 109
	action: tensor([[ 0.2893, -0.0128, -0.4087, -0.2838, -0.7778, -0.1307,  0.0171]],
       dtype=torch.float64)
	q_value: tensor([[-5.5646]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4660159799609799, distance: 0.8362205258956339 entropy 0.03264415264129639
epoch: 27, step: 110
	action: tensor([[-0.3172,  0.0189, -0.5838,  0.0712, -0.0287,  0.3135,  0.0981]],
       dtype=torch.float64)
	q_value: tensor([[-7.4761]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.028491785864588115, distance: 1.1605319653286683 entropy 0.03264415264129639
epoch: 27, step: 111
	action: tensor([[ 0.2657,  0.4491, -0.2606, -0.0534,  0.0304,  0.0151, -0.3930]],
       dtype=torch.float64)
	q_value: tensor([[-5.5979]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.738392920307982, distance: 0.5853039293324336 entropy 0.03264415264129639
epoch: 27, step: 112
	action: tensor([[ 0.3573,  0.1396, -0.3074,  0.1756,  0.1480,  0.2629,  0.0118]],
       dtype=torch.float64)
	q_value: tensor([[-5.5459]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7445423434125504, distance: 0.5783838465539485 entropy 0.03264415264129639
epoch: 27, step: 113
	action: tensor([[-0.3135, -0.2301, -0.3704, -0.3457,  0.0301,  0.4086, -0.0196]],
       dtype=torch.float64)
	q_value: tensor([[-4.9603]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.23185370007210637, distance: 1.2700951552758637 entropy 0.03264415264129639
epoch: 27, step: 114
	action: tensor([[ 0.3822,  0.4534, -0.0445, -0.2965, -0.1764,  0.2148, -0.0643]],
       dtype=torch.float64)
	q_value: tensor([[-5.4820]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.769994958888063, distance: 0.548814239008396 entropy 0.03264415264129639
epoch: 27, step: 115
	action: tensor([[-0.0557, -0.0072, -0.3403,  0.0910, -0.1110,  0.3934, -0.7411]],
       dtype=torch.float64)
	q_value: tensor([[-6.5194]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30290563456452824, distance: 0.9554379323796793 entropy 0.03264415264129639
epoch: 27, step: 116
	action: tensor([[ 0.4618,  0.2508, -0.3211,  0.0671, -0.2064,  0.2124, -0.3067]],
       dtype=torch.float64)
	q_value: tensor([[-5.5728]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8301131152100913, distance: 0.47166822463054214 entropy 0.03264415264129639
epoch: 27, step: 117
	action: tensor([[ 0.2728, -0.1340,  0.3236,  0.0168,  0.0303, -0.2899, -0.1162]],
       dtype=torch.float64)
	q_value: tensor([[-5.8236]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.34461605983408017, distance: 0.9264128934839546 entropy 0.03264415264129639
epoch: 27, step: 118
	action: tensor([[ 0.4222,  0.1319, -0.3847, -0.1088, -0.2206, -0.1017,  0.0185]],
       dtype=torch.float64)
	q_value: tensor([[-5.1739]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6668254165948446, distance: 0.6605301182254346 entropy 0.03264415264129639
epoch: 27, step: 119
	action: tensor([[-0.0136, -0.0782, -0.5882,  0.2907, -0.1296,  0.0753, -0.2810]],
       dtype=torch.float64)
	q_value: tensor([[-5.8999]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25842002646108353, distance: 0.9854524869004561 entropy 0.03264415264129639
epoch: 27, step: 120
	action: tensor([[ 0.6679,  0.7841,  0.2198, -0.2683,  0.2390, -0.0573, -0.6181]],
       dtype=torch.float64)
	q_value: tensor([[-4.8962]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9509906602659425, distance: 0.25333553918715657 entropy 0.03264415264129639
epoch: 27, step: 121
	action: tensor([[ 0.2735,  0.1990, -0.4741,  0.0169,  0.0539, -0.3066,  0.1278]],
       dtype=torch.float64)
	q_value: tensor([[-7.9864]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6412640189524264, distance: 0.6854000984334474 entropy 0.03264415264129639
epoch: 27, step: 122
	action: tensor([[-0.0271,  0.3114, -0.1359, -0.5066,  0.4474,  0.6102,  0.1927]],
       dtype=torch.float64)
	q_value: tensor([[-5.8777]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.49282859349369956, distance: 0.8149558350779912 entropy 0.03264415264129639
epoch: 27, step: 123
	action: tensor([[ 0.1194, -0.0591, -0.1102,  0.0625,  0.0402, -0.1203, -0.2806]],
       dtype=torch.float64)
	q_value: tensor([[-6.6019]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3682347655057865, distance: 0.9095667060091339 entropy 0.03264415264129639
epoch: 27, step: 124
	action: tensor([[-0.2809,  0.3850,  0.0915, -0.1547,  0.2145,  0.0980,  0.1945]],
       dtype=torch.float64)
	q_value: tensor([[-4.5113]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19285928296355925, distance: 1.028090441564452 entropy 0.03264415264129639
epoch: 27, step: 125
	action: tensor([[ 0.2261, -0.0621, -0.7132, -0.2815, -0.0935, -0.1794, -0.0478]],
       dtype=torch.float64)
	q_value: tensor([[-5.4533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3769443149454459, distance: 0.9032752795513667 entropy 0.03264415264129639
epoch: 27, step: 126
	action: tensor([[-0.0029,  0.1099, -0.4740, -0.3685, -0.1250,  0.0350, -0.1763]],
       dtype=torch.float64)
	q_value: tensor([[-6.1026]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3168607164390169, distance: 0.9458261632984217 entropy 0.03264415264129639
epoch: 27, step: 127
	action: tensor([[-0.2717,  0.4845, -0.8084,  0.1223, -0.1933, -0.0996, -0.3605]],
       dtype=torch.float64)
	q_value: tensor([[-5.6084]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2688955193409479, distance: 0.9784675247509376 entropy 0.03264415264129639
LOSS epoch 27 actor 17.958774814121274 critic 39.45483629296897 
epoch: 28, step: 0
	action: tensor([[-0.0918, -0.1018, -0.1533,  0.0226,  0.0934, -0.1529, -0.1691]],
       dtype=torch.float64)
	q_value: tensor([[-6.2092]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10414453078644337, distance: 1.0831177373255867 entropy 0.03264415264129639
epoch: 28, step: 1
	action: tensor([[-0.1077,  0.6690, -0.1293, -0.2994,  0.1996,  0.4616, -0.0774]],
       dtype=torch.float64)
	q_value: tensor([[-3.9274]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5594412237135006, distance: 0.7595539417064806 entropy 0.03264415264129639
epoch: 28, step: 2
	action: tensor([[ 0.0377, -0.2404, -0.1708, -0.2464, -0.1610, -0.1375,  0.0550]],
       dtype=torch.float64)
	q_value: tensor([[-6.0775]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.009967594974075844, distance: 1.1386267909626742 entropy 0.03264415264129639
epoch: 28, step: 3
	action: tensor([[ 0.0059, -0.1107, -0.3523,  0.1130, -0.3202,  0.2496, -0.2955]],
       dtype=torch.float64)
	q_value: tensor([[-4.5101]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26149750395836413, distance: 0.9834055999427943 entropy 0.03264415264129639
epoch: 28, step: 4
	action: tensor([[-0.2631, -0.2825, -0.3838, -0.3317, -0.0290, -0.1917,  0.0574]],
       dtype=torch.float64)
	q_value: tensor([[-4.4386]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2720933865676274, distance: 1.290672897943303 entropy 0.03264415264129639
epoch: 28, step: 5
	action: tensor([[ 0.3624, -0.1026, -0.1487, -0.3617,  0.1248, -0.0469, -0.5364]],
       dtype=torch.float64)
	q_value: tensor([[-4.9507]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30901599621739073, distance: 0.9512412831622039 entropy 0.03264415264129639
epoch: 28, step: 6
	action: tensor([[ 0.4389,  0.3076, -0.0675, -0.2621, -0.2766, -0.0218, -0.0572]],
       dtype=torch.float64)
	q_value: tensor([[-4.4304]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7237394012854482, distance: 0.6014730427795644 entropy 0.03264415264129639
epoch: 28, step: 7
	action: tensor([[-0.0973,  0.0455,  0.4450, -0.1585,  0.0276,  0.1515,  0.3227]],
       dtype=torch.float64)
	q_value: tensor([[-5.5001]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15355874454199392, distance: 1.0528223712186104 entropy 0.03264415264129639
epoch: 28, step: 8
	action: tensor([[ 0.6213,  0.0462, -0.2177, -0.0195,  0.0063, -0.1266, -0.0581]],
       dtype=torch.float64)
	q_value: tensor([[-4.9337]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6951392269662522, distance: 0.6318405094129335 entropy 0.03264415264129639
epoch: 28, step: 9
	action: tensor([[ 0.2174, -0.4673,  0.2099,  0.0019, -0.0504, -0.1006, -0.0033]],
       dtype=torch.float64)
	q_value: tensor([[-5.0388]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.02988785834460206, distance: 1.1271135305178752 entropy 0.03264415264129639
epoch: 28, step: 10
	action: tensor([[-0.2037,  0.3339, -0.2992, -0.3470,  0.1907,  0.0708,  0.0477]],
       dtype=torch.float64)
	q_value: tensor([[-4.2713]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2565621724083075, distance: 0.9866861243499291 entropy 0.03264415264129639
epoch: 28, step: 11
	action: tensor([[-0.2312,  0.0725,  0.4766, -0.1628, -0.1003,  0.2277, -0.0247]],
       dtype=torch.float64)
	q_value: tensor([[-5.0637]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05040855085146456, distance: 1.1151289504033826 entropy 0.03264415264129639
epoch: 28, step: 12
	action: tensor([[ 0.2212,  0.4342, -0.8069, -0.2266,  0.0485,  0.3324, -0.1255]],
       dtype=torch.float64)
	q_value: tensor([[-4.8983]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7304833814665694, distance: 0.5940862048791458 entropy 0.03264415264129639
epoch: 28, step: 13
	action: tensor([[-0.0554,  0.4636, -0.3120, -0.4705, -0.0916, -0.0427, -0.8317]],
       dtype=torch.float64)
	q_value: tensor([[-6.6764]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4603026203700046, distance: 0.840682192109248 entropy 0.03264415264129639
epoch: 28, step: 14
	action: tensor([[-0.1998,  0.4217, -0.2644, -0.3235, -0.1733, -0.0073,  0.0855]],
       dtype=torch.float64)
	q_value: tensor([[-6.4342]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3051433908605061, distance: 0.9539031646688176 entropy 0.03264415264129639
epoch: 28, step: 15
	action: tensor([[ 0.1850,  0.2501, -0.3786,  0.2585, -0.2596, -0.2262,  0.2173]],
       dtype=torch.float64)
	q_value: tensor([[-5.6316]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6490909805049493, distance: 0.6778817763590194 entropy 0.03264415264129639
epoch: 28, step: 16
	action: tensor([[ 0.1601,  0.1100, -0.2902,  0.1104, -0.0669, -0.2007,  0.1169]],
       dtype=torch.float64)
	q_value: tensor([[-5.2680]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5368955586596735, distance: 0.7787466049349979 entropy 0.03264415264129639
epoch: 28, step: 17
	action: tensor([[ 0.4839,  0.2156, -0.1854,  0.0127, -0.1327, -0.1507,  0.0242]],
       dtype=torch.float64)
	q_value: tensor([[-4.5841]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.777711567324908, distance: 0.5395294123156795 entropy 0.03264415264129639
epoch: 28, step: 18
	action: tensor([[-0.1152,  0.1367, -0.0629, -0.0136, -0.1496,  0.0359, -0.3352]],
       dtype=torch.float64)
	q_value: tensor([[-5.0504]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2807679817100368, distance: 0.9704903014928755 entropy 0.03264415264129639
epoch: 28, step: 19
	action: tensor([[ 0.4242,  0.1024, -0.2438,  0.2438, -0.1553,  0.1428, -0.2456]],
       dtype=torch.float64)
	q_value: tensor([[-4.2945]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7703030170637802, distance: 0.548446587585526 entropy 0.03264415264129639
epoch: 28, step: 20
	action: tensor([[ 0.3752, -0.1250,  0.1959, -0.0740,  0.4233, -0.1153, -0.4960]],
       dtype=torch.float64)
	q_value: tensor([[-4.6272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41791187854029177, distance: 0.8730740130447365 entropy 0.03264415264129639
epoch: 28, step: 21
	action: tensor([[ 0.2222, -0.2414, -0.6822, -0.0054,  0.4004, -0.0352,  0.0207]],
       dtype=torch.float64)
	q_value: tensor([[-4.4117]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2926406853139577, distance: 0.962446797328814 entropy 0.03264415264129639
epoch: 28, step: 22
	action: tensor([[ 0.2658,  0.2094, -0.3434, -0.2935,  0.2019,  0.2626, -0.0632]],
       dtype=torch.float64)
	q_value: tensor([[-4.7645]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6429344767626224, distance: 0.6838024500691466 entropy 0.03264415264129639
epoch: 28, step: 23
	action: tensor([[ 0.1035,  0.0257,  0.0123,  0.4100, -0.5290,  0.0485, -0.2011]],
       dtype=torch.float64)
	q_value: tensor([[-4.7558]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6019044522799353, distance: 0.7220218716441725 entropy 0.03264415264129639
epoch: 28, step: 24
	action: tensor([[ 0.2704,  0.4058, -0.0072, -0.5721,  0.1300, -0.0234,  0.0829]],
       dtype=torch.float64)
	q_value: tensor([[-4.9307]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5729734713688492, distance: 0.7477976934794225 entropy 0.03264415264129639
epoch: 28, step: 25
	action: tensor([[-0.3002, -0.5451,  0.1266, -0.4111,  0.4097, -0.3330,  0.1964]],
       dtype=torch.float64)
	q_value: tensor([[-5.5896]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.7650009376587525, distance: 1.520299537877749 entropy 0.03264415264129639
epoch: 28, step: 26
	action: tensor([[ 0.2168, -0.2488, -0.3354,  0.0143, -0.0158,  0.0337, -0.4814]],
       dtype=torch.float64)
	q_value: tensor([[-5.3064]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2503731816929995, distance: 0.9907846074949792 entropy 0.03264415264129639
epoch: 28, step: 27
	action: tensor([[-0.1706, -0.3586,  0.3285, -0.1786, -0.1020, -0.2018, -0.8010]],
       dtype=torch.float64)
	q_value: tensor([[-4.0208]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3551596919385094, distance: 1.3321463249180765 entropy 0.03264415264129639
epoch: 28, step: 28
	action: tensor([[ 0.3312, -0.0523,  0.1243, -0.2253,  0.0102,  0.0820,  0.1022]],
       dtype=torch.float64)
	q_value: tensor([[-4.9792]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4066897507733097, distance: 0.8814498711404642 entropy 0.03264415264129639
epoch: 28, step: 29
	action: tensor([[-0.0895, -0.2960,  0.0333,  0.4457,  0.0730, -0.1717, -0.2328]],
       dtype=torch.float64)
	q_value: tensor([[-4.4253]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16363694556262087, distance: 1.0465358577355437 entropy 0.03264415264129639
epoch: 28, step: 30
	action: tensor([[ 0.3479, -0.0485,  0.0927, -0.1622, -0.4940,  0.2432, -0.6160]],
       dtype=torch.float64)
	q_value: tensor([[-4.2485]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.48623592143744443, distance: 0.8202354990514061 entropy 0.03264415264129639
epoch: 28, step: 31
	action: tensor([[-0.1018,  0.2270, -0.0803,  0.5858, -0.3852, -0.0960, -0.2140]],
       dtype=torch.float64)
	q_value: tensor([[-5.9127]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.49204777314604353, distance: 0.8155829301391242 entropy 0.03264415264129639
epoch: 28, step: 32
	action: tensor([[ 0.0498, -0.0716, -0.3382,  0.1003,  0.1182, -0.0884, -0.1742]],
       dtype=torch.float64)
	q_value: tensor([[-5.0510]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3123190009160641, distance: 0.9489650233919318 entropy 0.03264415264129639
epoch: 28, step: 33
	action: tensor([[ 0.1571,  0.3644, -0.3986,  0.2446, -0.1520,  0.0759, -0.2343]],
       dtype=torch.float64)
	q_value: tensor([[-3.9348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6526202916586668, distance: 0.6744642213388875 entropy 0.03264415264129639
epoch: 28, step: 34
	action: tensor([[-0.1229, -0.2263,  0.3086, -0.4318,  0.1128, -0.0138, -0.2121]],
       dtype=torch.float64)
	q_value: tensor([[-4.8582]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3050741987197174, distance: 1.3072970920977651 entropy 0.03264415264129639
epoch: 28, step: 35
	action: tensor([[ 0.0145, -0.0494, -0.0068,  0.0087,  0.6335,  0.1391,  0.2222]],
       dtype=torch.float64)
	q_value: tensor([[-4.3398]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3089532485111566, distance: 0.9512844729128124 entropy 0.03264415264129639
epoch: 28, step: 36
	action: tensor([[ 0.2386,  0.2403,  0.0256,  0.3411, -0.2738,  0.5285, -0.4839]],
       dtype=torch.float64)
	q_value: tensor([[-4.2948]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7851007039728978, distance: 0.5304863208991303 entropy 0.03264415264129639
epoch: 28, step: 37
	action: tensor([[ 0.0971, -0.2668, -0.3382,  0.0921, -0.1070,  0.4807, -0.0158]],
       dtype=torch.float64)
	q_value: tensor([[-5.5871]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29673885798207356, distance: 0.9596547208028597 entropy 0.03264415264129639
epoch: 28, step: 38
	action: tensor([[ 0.2557,  0.0325, -0.1319, -0.0716, -0.1429, -0.0218, -0.2593]],
       dtype=torch.float64)
	q_value: tensor([[-4.2369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5170044908190705, distance: 0.7952949772749814 entropy 0.03264415264129639
epoch: 28, step: 39
	action: tensor([[ 0.3797,  0.1948, -0.1544,  0.0148, -0.3945, -0.0656,  0.0701]],
       dtype=torch.float64)
	q_value: tensor([[-4.3460]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7345035844951533, distance: 0.5896387600852451 entropy 0.03264415264129639
epoch: 28, step: 40
	action: tensor([[ 0.3107,  0.1758,  0.0279,  0.0167,  0.1278, -0.6253, -0.0668]],
       dtype=torch.float64)
	q_value: tensor([[-5.2187]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5631630922911793, distance: 0.756338756764703 entropy 0.03264415264129639
epoch: 28, step: 41
	action: tensor([[-0.2474,  0.2190, -0.0637, -0.0973, -0.3848, -0.2460, -0.2099]],
       dtype=torch.float64)
	q_value: tensor([[-5.3838]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14470508832136875, distance: 1.0583142343488614 entropy 0.03264415264129639
epoch: 28, step: 42
	action: tensor([[ 0.1450,  0.0376,  0.0740, -0.4052,  0.3230,  0.1282, -0.2108]],
       dtype=torch.float64)
	q_value: tensor([[-5.1389]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27901704167188335, distance: 0.9716708922749773 entropy 0.03264415264129639
epoch: 28, step: 43
	action: tensor([[-0.1689, -0.1031, -0.4313,  0.2465,  0.2392, -0.0723, -0.1854]],
       dtype=torch.float64)
	q_value: tensor([[-4.0035]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07849199218999503, distance: 1.0985156539644056 entropy 0.03264415264129639
epoch: 28, step: 44
	action: tensor([[-0.0367, -0.2541, -0.3158, -0.5186, -0.0969,  0.1346,  0.2317]],
       dtype=torch.float64)
	q_value: tensor([[-4.1659]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08763353331926327, distance: 1.1934328496983804 entropy 0.03264415264129639
epoch: 28, step: 45
	action: tensor([[ 0.2509,  0.1119, -0.3520,  0.0386,  0.4554, -0.0748, -0.1087]],
       dtype=torch.float64)
	q_value: tensor([[-5.2152]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5798818594741196, distance: 0.7417241346521091 entropy 0.03264415264129639
epoch: 28, step: 46
	action: tensor([[ 0.2820, -0.0453, -0.5292,  0.1427,  0.1351, -0.1699, -0.2446]],
       dtype=torch.float64)
	q_value: tensor([[-4.3968]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.516545131382768, distance: 0.7956730754401515 entropy 0.03264415264129639
epoch: 28, step: 47
	action: tensor([[-0.0670, -0.5837,  0.1690,  0.1826, -0.4981,  0.3658,  0.0498]],
       dtype=torch.float64)
	q_value: tensor([[-4.4598]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05113761665712713, distance: 1.1732389766375413 entropy 0.03264415264129639
epoch: 28, step: 48
	action: tensor([[ 0.0966,  0.1810, -0.0328,  0.2622,  0.3743, -0.2320, -0.3423]],
       dtype=torch.float64)
	q_value: tensor([[-4.8891]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5404913683204747, distance: 0.775717394718329 entropy 0.03264415264129639
epoch: 28, step: 49
	action: tensor([[ 0.0957, -0.1498,  0.0788, -0.2177,  0.1097,  0.0881, -0.4450]],
       dtype=torch.float64)
	q_value: tensor([[-4.2477]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1665771505399919, distance: 1.0446947081395797 entropy 0.03264415264129639
epoch: 28, step: 50
	action: tensor([[ 0.3750,  0.1564, -0.2361, -0.4282,  0.1020,  0.0746, -0.4062]],
       dtype=torch.float64)
	q_value: tensor([[-3.9746]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.558330364656579, distance: 0.7605109379251934 entropy 0.03264415264129639
epoch: 28, step: 51
	action: tensor([[ 0.0556, -0.0056, -0.0614, -0.1743,  0.2638,  0.0089,  0.1917]],
       dtype=torch.float64)
	q_value: tensor([[-4.8947]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.263326780209289, distance: 0.9821868932455384 entropy 0.03264415264129639
epoch: 28, step: 52
	action: tensor([[ 0.2265, -0.1810, -0.6770, -0.0346, -0.0313, -0.4066, -0.5252]],
       dtype=torch.float64)
	q_value: tensor([[-4.2277]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3232902755544723, distance: 0.9413646852717668 entropy 0.03264415264129639
epoch: 28, step: 53
	action: tensor([[ 0.4255, -0.0041,  0.0527, -0.0774,  0.2891,  0.5457, -0.0205]],
       dtype=torch.float64)
	q_value: tensor([[-5.3640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7268650068415627, distance: 0.5980608383884822 entropy 0.03264415264129639
epoch: 28, step: 54
	action: tensor([[ 0.0196, -0.1518, -0.8105, -0.4396,  0.0096, -0.1597, -0.2499]],
       dtype=torch.float64)
	q_value: tensor([[-4.5100]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23113826672539994, distance: 1.0034155255152097 entropy 0.03264415264129639
epoch: 28, step: 55
	action: tensor([[ 0.3473, -0.1353, -0.1206, -0.1811, -0.4482,  0.3042, -0.1838]],
       dtype=torch.float64)
	q_value: tensor([[-5.6477]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4379402622532007, distance: 0.8579222503665481 entropy 0.03264415264129639
epoch: 28, step: 56
	action: tensor([[ 0.0267,  0.2650, -0.3199, -0.3330, -0.6894, -0.2713, -0.5001]],
       dtype=torch.float64)
	q_value: tensor([[-5.2995]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.46838474701247956, distance: 0.8343637163525869 entropy 0.03264415264129639
epoch: 28, step: 57
	action: tensor([[ 0.3453, -0.1700, -0.2090, -0.0191, -0.1009,  0.1692, -0.2910]],
       dtype=torch.float64)
	q_value: tensor([[-6.7891]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4832266808140482, distance: 0.8226341507833845 entropy 0.03264415264129639
epoch: 28, step: 58
	action: tensor([[ 0.1455,  0.2543, -0.1927,  0.2086, -0.2225, -0.2958, -0.0479]],
       dtype=torch.float64)
	q_value: tensor([[-4.1911]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6403071841051673, distance: 0.6863135529119473 entropy 0.03264415264129639
epoch: 28, step: 59
	action: tensor([[ 0.4323,  0.2136, -0.0905, -0.1588,  0.0435,  0.0843,  0.0286]],
       dtype=torch.float64)
	q_value: tensor([[-4.9240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7298635131792308, distance: 0.5947689896802592 entropy 0.03264415264129639
epoch: 28, step: 60
	action: tensor([[ 0.0125, -0.0374,  0.1653, -0.1623,  0.2076,  0.5340, -0.4423]],
       dtype=torch.float64)
	q_value: tensor([[-4.6119]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3815466113580134, distance: 0.899933005335455 entropy 0.03264415264129639
epoch: 28, step: 61
	action: tensor([[-0.1203,  0.0280, -0.8223, -0.1849,  0.3795,  0.2331, -0.4387]],
       dtype=torch.float64)
	q_value: tensor([[-4.2945]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24816309685026494, distance: 0.992244071329193 entropy 0.03264415264129639
epoch: 28, step: 62
	action: tensor([[-0.2025,  0.2126,  0.1056,  0.2568, -0.0307, -0.0762,  0.2388]],
       dtype=torch.float64)
	q_value: tensor([[-5.3880]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.325097339361043, distance: 0.9401069501206747 entropy 0.03264415264129639
epoch: 28, step: 63
	action: tensor([[-0.2397,  0.0804,  0.1094, -0.2427,  0.2274, -0.0074,  0.0226]],
       dtype=torch.float64)
	q_value: tensor([[-4.6964]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05988152819241943, distance: 1.178108677633047 entropy 0.03264415264129639
epoch: 28, step: 64
	action: tensor([[-0.0491,  0.0962,  0.0570,  0.4744, -0.5001, -0.1240, -0.2646]],
       dtype=torch.float64)
	q_value: tensor([[-4.2030]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4737358497712094, distance: 0.8301538501377518 entropy 0.03264415264129639
epoch: 28, step: 65
	action: tensor([[ 0.1596,  0.2653, -0.2102,  0.0829,  0.0201,  0.1908,  0.1804]],
       dtype=torch.float64)
	q_value: tensor([[-5.0872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6213410242124935, distance: 0.7041753546860572 entropy 0.03264415264129639
epoch: 28, step: 66
	action: tensor([[ 0.2917,  0.0521, -0.5466, -0.4887,  0.0022,  0.0994,  0.0289]],
       dtype=torch.float64)
	q_value: tensor([[-4.5335]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.456288558184749, distance: 0.8438027364154109 entropy 0.03264415264129639
epoch: 28, step: 67
	action: tensor([[ 0.0868, -0.2830, -0.4769, -0.0733, -0.0902,  0.1699, -0.3032]],
       dtype=torch.float64)
	q_value: tensor([[-5.7314]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1564429339551875, distance: 1.0510271313052877 entropy 0.03264415264129639
epoch: 28, step: 68
	action: tensor([[ 0.3190,  0.1969,  0.2877, -0.1319, -0.2319,  0.0139, -0.6519]],
       dtype=torch.float64)
	q_value: tensor([[-4.1059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6319327459437262, distance: 0.6942570264371458 entropy 0.03264415264129639
epoch: 28, step: 69
	action: tensor([[-0.1402, -0.3953, -0.0345,  0.1310,  0.1825,  0.2753, -0.2058]],
       dtype=torch.float64)
	q_value: tensor([[-5.5673]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0021527190426986476, distance: 1.1455753176561294 entropy 0.03264415264129639
epoch: 28, step: 70
	action: tensor([[ 0.1288,  0.0897, -0.0621,  0.0665,  0.0495,  0.0052, -0.1249]],
       dtype=torch.float64)
	q_value: tensor([[-3.8784]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4984066661337624, distance: 0.81046184019079 entropy 0.03264415264129639
epoch: 28, step: 71
	action: tensor([[ 0.2082, -0.1477,  0.1991, -0.2930, -0.2070,  0.0414, -0.1745]],
       dtype=torch.float64)
	q_value: tensor([[-4.0292]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19103763211301827, distance: 1.0292499458968376 entropy 0.03264415264129639
epoch: 28, step: 72
	action: tensor([[ 0.0542,  0.2691, -0.2397,  0.0272, -0.2837, -0.1723,  0.2588]],
       dtype=torch.float64)
	q_value: tensor([[-4.5962]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.526764949858793, distance: 0.7872182338741623 entropy 0.03264415264129639
epoch: 28, step: 73
	action: tensor([[ 0.3862,  0.1895, -0.0877, -0.0537, -0.4376,  0.1773, -0.1664]],
       dtype=torch.float64)
	q_value: tensor([[-5.2347]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7270719595236119, distance: 0.597834221968918 entropy 0.03264415264129639
epoch: 28, step: 74
	action: tensor([[ 1.4348e-01,  5.0996e-02, -5.0075e-02, -5.3342e-04, -6.2079e-01,
          1.4243e-01,  2.1937e-01]], dtype=torch.float64)
	q_value: tensor([[-5.5160]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4797401911949174, distance: 0.8254044990615254 entropy 0.03264415264129639
epoch: 28, step: 75
	action: tensor([[-0.2235, -0.0507, -0.1215,  0.0450,  0.1220,  0.0408, -0.2317]],
       dtype=torch.float64)
	q_value: tensor([[-5.6016]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.048394934394347056, distance: 1.1163106443304016 entropy 0.03264415264129639
epoch: 28, step: 76
	action: tensor([[ 0.0767,  0.3288, -0.0635, -0.2463,  0.0285, -0.1460, -0.5096]],
       dtype=torch.float64)
	q_value: tensor([[-3.8799]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.47185619405317847, distance: 0.8316350574283388 entropy 0.03264415264129639
epoch: 28, step: 77
	action: tensor([[-0.2588,  0.3974, -0.0384,  0.0374, -0.3591, -0.3817, -0.2164]],
       dtype=torch.float64)
	q_value: tensor([[-4.8010]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2881875278657732, distance: 0.9654715703871586 entropy 0.03264415264129639
epoch: 28, step: 78
	action: tensor([[ 0.1882,  0.0531, -0.3406,  0.1091, -0.1264,  0.2832, -0.3191]],
       dtype=torch.float64)
	q_value: tensor([[-5.4629]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5603844798075583, distance: 0.758740386485336 entropy 0.03264415264129639
epoch: 28, step: 79
	action: tensor([[-0.0350, -0.1008, -0.0558, -0.3630,  0.0914,  0.0362, -0.1231]],
       dtype=torch.float64)
	q_value: tensor([[-4.2728]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.02646487080847615, distance: 1.1291002587127201 entropy 0.03264415264129639
epoch: 28, step: 80
	action: tensor([[-0.0103, -0.2246, -0.1890,  0.1430,  0.0632, -0.0416, -0.4369]],
       dtype=torch.float64)
	q_value: tensor([[-4.0310]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1675009969353064, distance: 1.0441155273238742 entropy 0.03264415264129639
epoch: 28, step: 81
	action: tensor([[ 0.0764,  0.3570, -0.1831,  0.2384, -0.1792, -0.1080, -0.2726]],
       dtype=torch.float64)
	q_value: tensor([[-3.9131]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.611502555253977, distance: 0.7132647749084337 entropy 0.03264415264129639
epoch: 28, step: 82
	action: tensor([[ 0.1505,  0.1157, -0.3043, -0.4228,  0.4412, -0.0197, -0.2914]],
       dtype=torch.float64)
	q_value: tensor([[-4.8326]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39698696129380207, distance: 0.8886281048945033 entropy 0.03264415264129639
epoch: 28, step: 83
	action: tensor([[ 0.0014,  0.2328, -0.0039,  0.2689,  0.4190, -0.0350, -0.3528]],
       dtype=torch.float64)
	q_value: tensor([[-4.4195]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5404458748035743, distance: 0.775755793604166 entropy 0.03264415264129639
epoch: 28, step: 84
	action: tensor([[-0.2510,  0.2863, -0.0394, -0.0906, -0.0200,  0.5187, -0.0381]],
       dtype=torch.float64)
	q_value: tensor([[-4.2118]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2936879584402695, distance: 0.9617340633637566 entropy 0.03264415264129639
epoch: 28, step: 85
	action: tensor([[ 0.2443,  0.3358, -0.4728,  0.1560,  0.1940,  0.1879, -0.3860]],
       dtype=torch.float64)
	q_value: tensor([[-5.1625]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7221515802823604, distance: 0.6031990637751328 entropy 0.03264415264129639
epoch: 28, step: 86
	action: tensor([[ 0.3924,  0.0204, -0.0219, -0.0369,  0.1732, -0.5616, -0.0819]],
       dtype=torch.float64)
	q_value: tensor([[-4.7218]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.47891552213210586, distance: 0.8260584185228755 entropy 0.03264415264129639
epoch: 28, step: 87
	action: tensor([[-0.0586,  0.1087, -0.4885,  0.0687, -0.0354, -0.0599,  0.0729]],
       dtype=torch.float64)
	q_value: tensor([[-5.1825]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3267475099729584, distance: 0.9389569425812848 entropy 0.03264415264129639
epoch: 28, step: 88
	action: tensor([[ 0.0196,  0.2107, -0.4869, -0.1415,  0.2287,  0.0939, -0.3747]],
       dtype=torch.float64)
	q_value: tensor([[-4.4974]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4664396163989426, distance: 0.83588875213339 entropy 0.03264415264129639
epoch: 28, step: 89
	action: tensor([[ 0.4040,  0.2486, -0.0927,  0.3130,  0.2059,  0.1903, -0.0212]],
       dtype=torch.float64)
	q_value: tensor([[-4.3728]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8548205795538346, distance: 0.4360228230614589 entropy 0.03264415264129639
epoch: 28, step: 90
	action: tensor([[ 0.2540, -0.1624, -0.4170, -0.0385, -0.1028,  0.1547, -0.3258]],
       dtype=torch.float64)
	q_value: tensor([[-4.5734]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41022991655354557, distance: 0.878816217470237 entropy 0.03264415264129639
epoch: 28, step: 91
	action: tensor([[ 0.0177, -0.1146, -0.0649, -0.2803,  0.3015, -0.2673, -0.1488]],
       dtype=torch.float64)
	q_value: tensor([[-4.1413]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.016668053174442643, distance: 1.1347671830836243 entropy 0.03264415264129639
epoch: 28, step: 92
	action: tensor([[ 0.1295,  0.7105, -0.7857,  0.0238,  0.3651, -0.3715,  0.2386]],
       dtype=torch.float64)
	q_value: tensor([[-4.1695]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7330892563211429, distance: 0.5912072088190943 entropy 0.03264415264129639
epoch: 28, step: 93
	action: tensor([[ 0.3042,  0.1337,  0.2373,  0.4734, -0.1457, -0.2418, -0.0322]],
       dtype=torch.float64)
	q_value: tensor([[-7.2158]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7879291637821628, distance: 0.5269836821986708 entropy 0.03264415264129639
epoch: 28, step: 94
	action: tensor([[ 0.4270,  0.1687, -0.0815, -0.0739,  0.2128, -0.3744, -0.0389]],
       dtype=torch.float64)
	q_value: tensor([[-5.1110]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6216725092954243, distance: 0.70386706312427 entropy 0.03264415264129639
epoch: 28, step: 95
	action: tensor([[ 0.6207,  0.2791, -0.0676,  0.1660,  0.3368, -0.0230,  0.0790]],
       dtype=torch.float64)
	q_value: tensor([[-4.9746]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9060694525622149, distance: 0.3507196669066071 entropy 0.03264415264129639
epoch: 28, step: 96
	action: tensor([[-0.1942,  0.1448, -0.0207,  0.1552,  0.1390,  0.1954,  0.2072]],
       dtype=torch.float64)
	q_value: tensor([[-5.1329]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30003719018806385, distance: 0.9574016600824491 entropy 0.03264415264129639
epoch: 28, step: 97
	action: tensor([[ 0.0839,  0.0585, -0.5040, -0.2879, -0.1413, -0.4119,  0.1714]],
       dtype=torch.float64)
	q_value: tensor([[-4.4518]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3263743019922781, distance: 0.9392171552649947 entropy 0.03264415264129639
epoch: 28, step: 98
	action: tensor([[ 0.4739,  0.2283, -0.2933, -0.0955,  0.0892, -0.5322,  0.0076]],
       dtype=torch.float64)
	q_value: tensor([[-5.7192]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6798227166053894, distance: 0.6475181570826142 entropy 0.03264415264129639
epoch: 28, step: 99
	action: tensor([[-0.1568,  0.0971, -0.2871, -0.2994,  0.2247, -0.3090, -0.1023]],
       dtype=torch.float64)
	q_value: tensor([[-5.9102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08126880315461682, distance: 1.0968593082238303 entropy 0.03264415264129639
epoch: 28, step: 100
	action: tensor([[-0.1446, -0.0268, -0.1595, -0.3623, -0.2353,  0.4086, -0.3313]],
       dtype=torch.float64)
	q_value: tensor([[-4.5836]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05245440214121011, distance: 1.1139270555698613 entropy 0.03264415264129639
epoch: 28, step: 101
	action: tensor([[ 0.3282,  0.0654,  0.0058, -0.2960, -0.2028,  0.3380,  0.0396]],
       dtype=torch.float64)
	q_value: tensor([[-5.1694]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5328156486236494, distance: 0.7821694278583159 entropy 0.03264415264129639
epoch: 28, step: 102
	action: tensor([[ 0.0169, -0.1398, -0.1007,  0.4264,  0.0285, -0.0035, -0.2086]],
       dtype=torch.float64)
	q_value: tensor([[-5.1844]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4011118238790349, distance: 0.8855835946979773 entropy 0.03264415264129639
epoch: 28, step: 103
	action: tensor([[ 0.1373,  0.0594, -0.1062,  0.6845,  0.1392, -0.0988, -0.3100]],
       dtype=torch.float64)
	q_value: tensor([[-4.1131]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6670766235158294, distance: 0.6602810580716904 entropy 0.03264415264129639
epoch: 28, step: 104
	action: tensor([[-0.0093, -0.2650, -0.3760,  0.0283, -0.4547, -0.4299, -0.0956]],
       dtype=torch.float64)
	q_value: tensor([[-4.8317]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1183055705187036, distance: 1.0745230638513268 entropy 0.03264415264129639
epoch: 28, step: 105
	action: tensor([[ 0.2691,  0.5396,  0.2308, -0.0285,  0.3395, -0.0676, -0.5126]],
       dtype=torch.float64)
	q_value: tensor([[-5.3719]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7705476183648916, distance: 0.5481544930180022 entropy 0.03264415264129639
epoch: 28, step: 106
	action: tensor([[ 0.1495, -0.0218, -0.0867,  0.2641, -0.4895, -0.1460, -0.0057]],
       dtype=torch.float64)
	q_value: tensor([[-5.1422]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5310453784760254, distance: 0.7836499376104 entropy 0.03264415264129639
epoch: 28, step: 107
	action: tensor([[-0.0911,  0.1061, -0.0523,  0.0387, -0.2088, -0.2665,  0.2024]],
       dtype=torch.float64)
	q_value: tensor([[-4.8001]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26769974271445185, distance: 0.9792673766189813 entropy 0.03264415264129639
epoch: 28, step: 108
	action: tensor([[ 0.1286, -0.0230, -0.0828, -0.2391, -0.1651,  0.0279, -0.2082]],
       dtype=torch.float64)
	q_value: tensor([[-4.7231]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3010589061030914, distance: 0.9567026588496629 entropy 0.03264415264129639
epoch: 28, step: 109
	action: tensor([[ 0.1935, -0.0756, -0.0900, -0.6706, -0.0480,  0.2275,  0.1039]],
       dtype=torch.float64)
	q_value: tensor([[-4.3109]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14957940852035656, distance: 1.0552942624946589 entropy 0.03264415264129639
epoch: 28, step: 110
	action: tensor([[-0.2386,  0.0445, -0.3445, -0.4933, -0.1713,  0.1280,  0.0816]],
       dtype=torch.float64)
	q_value: tensor([[-5.4431]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.012778014913532587, distance: 1.151632270278473 entropy 0.03264415264129639
epoch: 28, step: 111
	action: tensor([[ 0.0793, -0.0170,  0.0797, -0.0755,  0.2533,  0.2925, -0.3343]],
       dtype=torch.float64)
	q_value: tensor([[-5.4914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39354022532767086, distance: 0.8911641215234657 entropy 0.03264415264129639
epoch: 28, step: 112
	action: tensor([[-0.0844, -0.0357,  0.0301,  0.3388, -0.2739,  0.0978,  0.3853]],
       dtype=torch.float64)
	q_value: tensor([[-3.8146]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3601833955458088, distance: 0.9153442265275663 entropy 0.03264415264129639
epoch: 28, step: 113
	action: tensor([[-0.0224,  0.4784, -0.5460, -0.1244,  0.3324, -0.2455, -0.0588]],
       dtype=torch.float64)
	q_value: tensor([[-4.9100]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5287716232499994, distance: 0.7855474277446317 entropy 0.03264415264129639
epoch: 28, step: 114
	action: tensor([[ 0.2454, -0.0359, -0.5735, -0.0238, -0.1887, -0.2040, -0.0272]],
       dtype=torch.float64)
	q_value: tensor([[-5.4926]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.47230129269260224, distance: 0.8312845490533167 entropy 0.03264415264129639
epoch: 28, step: 115
	action: tensor([[ 0.0871, -0.2933, -0.1450, -0.1309,  0.0705, -0.0087,  0.1321]],
       dtype=torch.float64)
	q_value: tensor([[-5.0240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06791005492597735, distance: 1.104804933190616 entropy 0.03264415264129639
epoch: 28, step: 116
	action: tensor([[ 0.2302,  0.3952, -0.0773,  0.2994, -0.1066,  0.1639, -0.4952]],
       dtype=torch.float64)
	q_value: tensor([[-4.0970]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7602247831776726, distance: 0.5603493073828536 entropy 0.03264415264129639
epoch: 28, step: 117
	action: tensor([[-0.3464, -0.1304,  0.1076,  0.0197, -0.1848,  0.1846, -0.1629]],
       dtype=torch.float64)
	q_value: tensor([[-5.0697]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.13403352543998315, distance: 1.2186237563030877 entropy 0.03264415264129639
epoch: 28, step: 118
	action: tensor([[ 0.1803,  0.1001, -0.1456, -0.3041, -0.0439, -0.0803, -0.4848]],
       dtype=torch.float64)
	q_value: tensor([[-4.3228]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39256828546729894, distance: 0.8918779455970833 entropy 0.03264415264129639
epoch: 28, step: 119
	action: tensor([[-0.1051, -0.2537, -0.0776, -0.2892,  0.5264, -0.2589,  0.0214]],
       dtype=torch.float64)
	q_value: tensor([[-4.6121]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24272878099903994, distance: 1.2756891786198536 entropy 0.03264415264129639
epoch: 28, step: 120
	action: tensor([[ 0.1550,  0.2711, -0.3276, -0.3380, -0.5720, -0.1728,  0.0845]],
       dtype=torch.float64)
	q_value: tensor([[-4.3344]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.51834228132651, distance: 0.7941928181542669 entropy 0.03264415264129639
epoch: 28, step: 121
	action: tensor([[-0.1384,  0.3080, -0.2507, -0.0349, -0.2460, -0.1728, -0.2712]],
       dtype=torch.float64)
	q_value: tensor([[-6.4430]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3402973537256484, distance: 0.9294602183660846 entropy 0.03264415264129639
epoch: 28, step: 122
	action: tensor([[ 0.6786,  0.0771,  0.0502, -0.4057,  0.1481, -0.1634,  0.2703]],
       dtype=torch.float64)
	q_value: tensor([[-4.9034]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5377920961934691, distance: 0.7779924405063925 entropy 0.03264415264129639
epoch: 28, step: 123
	action: tensor([[-0.0336, -0.1563,  0.1228, -0.1194, -0.2085,  0.5220,  0.1693]],
       dtype=torch.float64)
	q_value: tensor([[-5.8679]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20882983747072592, distance: 1.0178684272844003 entropy 0.03264415264129639
epoch: 28, step: 124
	action: tensor([[ 0.5743,  0.2440, -0.2175,  0.3958, -0.4295,  0.0612,  0.3077]],
       dtype=torch.float64)
	q_value: tensor([[-4.9652]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9025754556738216, distance: 0.35718308573972024 entropy 0.03264415264129639
epoch: 28, step: 125
	action: tensor([[ 0.0521,  0.3457, -0.2576,  0.2868,  0.2438,  0.1563,  0.5655]],
       dtype=torch.float64)
	q_value: tensor([[-5.9344]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6056453793416456, distance: 0.7186214234332201 entropy 0.03264415264129639
epoch: 28, step: 126
	action: tensor([[ 0.2023,  0.2107, -0.2023,  0.0171, -0.0804,  0.2195, -0.4222]],
       dtype=torch.float64)
	q_value: tensor([[-5.5192]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.637315663720053, distance: 0.6891616355894612 entropy 0.03264415264129639
epoch: 28, step: 127
	action: tensor([[ 0.2970, -0.2522, -0.3040,  0.2841, -0.0124, -0.3193, -0.3201]],
       dtype=torch.float64)
	q_value: tensor([[-4.4970]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41340717544831984, distance: 0.8764458043925254 entropy 0.03264415264129639
LOSS epoch 28 actor 14.162719633714246 critic 45.03933739632453 
epoch: 29, step: 0
	action: tensor([[ 0.2725, -0.0322, -0.4976, -0.0128, -0.0897,  0.0903,  0.0019]],
       dtype=torch.float64)
	q_value: tensor([[-3.9464]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5241682989044812, distance: 0.7893750210738478 entropy 0.03264415264129639
epoch: 29, step: 1
	action: tensor([[ 0.1661, -0.0820,  0.0470,  0.0479, -0.1684, -0.1034,  0.3579]],
       dtype=torch.float64)
	q_value: tensor([[-3.8458]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3860872567103317, distance: 0.8966232938444956 entropy 0.03264415264129639
epoch: 29, step: 2
	action: tensor([[ 0.1360,  0.1156,  0.2127, -0.1441,  0.0070,  0.1175, -0.1282]],
       dtype=torch.float64)
	q_value: tensor([[-4.0734]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.46067817421212875, distance: 0.8403896426335878 entropy 0.03264415264129639
epoch: 29, step: 3
	action: tensor([[ 0.0556,  0.6786, -0.0814,  0.1683,  0.1219, -0.0535, -0.0841]],
       dtype=torch.float64)
	q_value: tensor([[-3.7859]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6810466120152472, distance: 0.6462793851102069 entropy 0.03264415264129639
epoch: 29, step: 4
	action: tensor([[ 0.2533,  0.0980, -0.5463,  0.1306, -0.2661, -0.2005, -0.1344]],
       dtype=torch.float64)
	q_value: tensor([[-4.5240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6185890048180134, distance: 0.7067296264940246 entropy 0.03264415264129639
epoch: 29, step: 5
	action: tensor([[-0.2386,  0.0640,  0.1636, -0.3095, -0.0760,  0.0179, -0.0867]],
       dtype=torch.float64)
	q_value: tensor([[-4.4938]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.08259784808603476, distance: 1.1906668791982387 entropy 0.03264415264129639
epoch: 29, step: 6
	action: tensor([[-0.2920, -0.4529, -0.3148, -0.5005, -0.7057,  0.1957,  0.1011]],
       dtype=torch.float64)
	q_value: tensor([[-3.9119]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.5138340283115352, distance: 1.4079778572214963 entropy 0.03264415264129639
epoch: 29, step: 7
	action: tensor([[ 0.0091,  0.0225, -0.4041, -0.4199,  0.0342, -0.2756, -0.4223]],
       dtype=torch.float64)
	q_value: tensor([[-5.7402]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19217562912144615, distance: 1.0285257493101598 entropy 0.03264415264129639
epoch: 29, step: 8
	action: tensor([[ 0.1047,  0.3012, -1.0191,  0.1026,  0.2283,  0.3829, -0.3449]],
       dtype=torch.float64)
	q_value: tensor([[-4.3285]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4932902618620413, distance: 0.8145848313249503 entropy 0.03264415264129639
epoch: 29, step: 9
	action: tensor([[-0.0767,  0.1290,  0.0573,  0.1929, -0.1737, -0.1612, -0.6000]],
       dtype=torch.float64)
	q_value: tensor([[-5.6929]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37802811811839254, distance: 0.9024893155211436 entropy 0.03264415264129639
epoch: 29, step: 10
	action: tensor([[ 0.1376, -0.0190,  0.0869, -0.2142,  0.3347,  0.3525,  0.2010]],
       dtype=torch.float64)
	q_value: tensor([[-4.1406]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3990481283706092, distance: 0.887108088974477 entropy 0.03264415264129639
epoch: 29, step: 11
	action: tensor([[-0.1487, -0.0129, -0.2626,  0.0055, -0.0207,  0.3993, -0.5130]],
       dtype=torch.float64)
	q_value: tensor([[-3.7289]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20843086668953503, distance: 1.0181250399555974 entropy 0.03264415264129639
epoch: 29, step: 12
	action: tensor([[ 0.4739, -0.1721, -0.3422, -0.4989, -0.3239,  0.2607, -0.1614]],
       dtype=torch.float64)
	q_value: tensor([[-3.8638]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33844966232802753, distance: 0.9307609216834424 entropy 0.03264415264129639
epoch: 29, step: 13
	action: tensor([[ 0.5127,  0.3596, -0.3333,  0.1954, -0.3615,  0.2441,  0.1095]],
       dtype=torch.float64)
	q_value: tensor([[-5.2106]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8713272880178827, distance: 0.4104874847116602 entropy 0.03264415264129639
epoch: 29, step: 14
	action: tensor([[-0.0639,  0.1053, -0.5179, -0.4431,  0.1503,  0.0377, -0.5088]],
       dtype=torch.float64)
	q_value: tensor([[-5.3194]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27856278221395225, distance: 0.9719769474905843 entropy 0.03264415264129639
epoch: 29, step: 15
	action: tensor([[ 0.2044, -0.1126, -0.0926, -0.2345, -0.2274, -0.3605, -0.2220]],
       dtype=torch.float64)
	q_value: tensor([[-4.3970]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23207434938158733, distance: 1.0028045145263587 entropy 0.03264415264129639
epoch: 29, step: 16
	action: tensor([[-0.2909, -0.0806, -0.4594, -0.3686,  0.1291, -0.0732, -0.0399]],
       dtype=torch.float64)
	q_value: tensor([[-4.2153]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12305490562587762, distance: 1.2127106397651617 entropy 0.03264415264129639
epoch: 29, step: 17
	action: tensor([[ 0.2225, -0.1420, -0.3043, -0.2144,  0.1574, -0.1305, -0.0806]],
       dtype=torch.float64)
	q_value: tensor([[-4.3088]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.25690201485079567, distance: 0.9864605801691345 entropy 0.03264415264129639
epoch: 29, step: 18
	action: tensor([[-0.2935,  0.4147, -0.6070,  0.1019,  0.3127,  0.3887,  0.3884]],
       dtype=torch.float64)
	q_value: tensor([[-3.6673]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26716716337790947, distance: 0.9796234073416786 entropy 0.03264415264129639
epoch: 29, step: 19
	action: tensor([[ 0.1084, -0.2669, -0.3591, -0.3696, -0.1244, -0.1237,  0.0457]],
       dtype=torch.float64)
	q_value: tensor([[-5.7187]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.008698555876775482, distance: 1.1393563121179615 entropy 0.03264415264129639
epoch: 29, step: 20
	action: tensor([[-0.1487,  0.0065, -0.1508,  0.1153,  0.0200, -0.2497, -0.4451]],
       dtype=torch.float64)
	q_value: tensor([[-4.2444]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13783255137014117, distance: 1.0625576544991242 entropy 0.03264415264129639
epoch: 29, step: 21
	action: tensor([[ 0.2055, -0.5682,  0.2502, -0.0829, -0.0408, -0.4443,  0.1292]],
       dtype=torch.float64)
	q_value: tensor([[-3.7254]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.24524170616316598, distance: 1.276978314493488 entropy 0.03264415264129639
epoch: 29, step: 22
	action: tensor([[ 0.3422,  0.1786,  0.4600,  0.0135, -0.0636, -0.0710, -0.1036]],
       dtype=torch.float64)
	q_value: tensor([[-4.3900]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6856620197460885, distance: 0.641586360109825 entropy 0.03264415264129639
epoch: 29, step: 23
	action: tensor([[ 0.1410,  0.3173,  0.2337, -0.2189, -0.3279,  0.1340, -0.0106]],
       dtype=torch.float64)
	q_value: tensor([[-4.4199]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5533180985233445, distance: 0.7648140743705156 entropy 0.03264415264129639
epoch: 29, step: 24
	action: tensor([[ 0.4502, -0.0964,  0.2897,  0.0505, -0.5787,  0.0201, -0.4175]],
       dtype=torch.float64)
	q_value: tensor([[-4.8703]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6041504724841593, distance: 0.7199821985879148 entropy 0.03264415264129639
epoch: 29, step: 25
	action: tensor([[ 0.0271, -0.5386, -0.4358, -0.3015, -0.2054,  0.1106, -0.6032]],
       dtype=torch.float64)
	q_value: tensor([[-5.1560]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.22061079179999332, distance: 1.2642859042994443 entropy 0.03264415264129639
epoch: 29, step: 26
	action: tensor([[-0.0369,  0.4291, -0.2251, -0.1810, -0.2223,  0.1665, -0.2490]],
       dtype=torch.float64)
	q_value: tensor([[-4.2285]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44644018953438513, distance: 0.8514104361833286 entropy 0.03264415264129639
epoch: 29, step: 27
	action: tensor([[ 0.3082,  0.0314, -0.2437, -0.2080, -0.5284,  0.2267,  0.2345]],
       dtype=torch.float64)
	q_value: tensor([[-4.7465]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.501351586280773, distance: 0.808079174009133 entropy 0.03264415264129639
epoch: 29, step: 28
	action: tensor([[-0.0704,  0.0563, -0.3790,  0.3673, -0.1232,  0.4315, -0.0485]],
       dtype=torch.float64)
	q_value: tensor([[-5.2313]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3668714183670403, distance: 0.9105475979692236 entropy 0.03264415264129639
epoch: 29, step: 29
	action: tensor([[ 0.1189,  0.0835, -0.2734,  0.1085,  0.0652,  0.3306, -0.1615]],
       dtype=torch.float64)
	q_value: tensor([[-4.0322]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5494706868335412, distance: 0.7681008042063224 entropy 0.03264415264129639
epoch: 29, step: 30
	action: tensor([[-0.2024, -0.1099,  0.0421,  0.1992, -0.0726,  0.2661, -0.0525]],
       dtype=torch.float64)
	q_value: tensor([[-3.4908]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17414224018660207, distance: 1.0399424832594566 entropy 0.03264415264129639
epoch: 29, step: 31
	action: tensor([[-0.2878,  0.1276, -0.4514,  0.2095, -0.2276,  0.3957, -0.0606]],
       dtype=torch.float64)
	q_value: tensor([[-3.5527]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09633467164869469, distance: 1.0878286763564926 entropy 0.03264415264129639
epoch: 29, step: 32
	action: tensor([[ 0.1819,  0.4624, -0.3565,  0.0556, -0.1893, -0.1795,  0.0844]],
       dtype=torch.float64)
	q_value: tensor([[-4.5286]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7055211686524008, distance: 0.6209887518314374 entropy 0.03264415264129639
epoch: 29, step: 33
	action: tensor([[-0.2916,  0.0946, -0.0069,  0.3439, -0.0125, -0.1067, -0.2144]],
       dtype=torch.float64)
	q_value: tensor([[-4.9184]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18080903633433454, distance: 1.03573647249267 entropy 0.03264415264129639
epoch: 29, step: 34
	action: tensor([[ 0.3069,  0.2105,  0.0019, -0.4494,  0.3588, -0.0343, -0.5561]],
       dtype=torch.float64)
	q_value: tensor([[-3.7689]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4833306363041999, distance: 0.8225514049891055 entropy 0.03264415264129639
epoch: 29, step: 35
	action: tensor([[ 0.3310, -0.1549, -0.2010, -0.3370,  0.3890,  0.0573, -0.1143]],
       dtype=torch.float64)
	q_value: tensor([[-4.1375]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2831525308699473, distance: 0.9688801791151117 entropy 0.03264415264129639
epoch: 29, step: 36
	action: tensor([[ 0.2713,  0.0207, -0.3365,  0.2293,  0.0811, -0.1419, -0.1063]],
       dtype=torch.float64)
	q_value: tensor([[-3.6630]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5771236826480012, distance: 0.7441549502930633 entropy 0.03264415264129639
epoch: 29, step: 37
	action: tensor([[ 0.4138,  0.1940, -0.1824, -0.2008,  0.2951, -0.0765, -0.3204]],
       dtype=torch.float64)
	q_value: tensor([[-3.7474]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6569543958804827, distance: 0.6702435179536911 entropy 0.03264415264129639
epoch: 29, step: 38
	action: tensor([[-0.0208,  0.2270,  0.2245, -0.0283, -0.0832,  0.0485,  0.2574]],
       dtype=torch.float64)
	q_value: tensor([[-3.9889]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.419621042242442, distance: 0.8717912833679927 entropy 0.03264415264129639
epoch: 29, step: 39
	action: tensor([[ 0.5886,  0.3294, -0.2602,  0.1444, -0.0379,  0.0557,  0.1567]],
       dtype=torch.float64)
	q_value: tensor([[-4.1577]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9014582590594821, distance: 0.3592252110621514 entropy 0.03264415264129639
epoch: 29, step: 40
	action: tensor([[ 0.5064,  0.5810, -0.3065,  0.0432,  0.1176,  0.3845, -0.2520]],
       dtype=torch.float64)
	q_value: tensor([[-4.7696]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9015032521773052, distance: 0.359143192480794 entropy 0.03264415264129639
epoch: 29, step: 41
	action: tensor([[ 0.3179,  0.0865, -0.1258, -0.3204,  0.1805,  0.3918, -0.5251]],
       dtype=torch.float64)
	q_value: tensor([[-5.2460]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6062991094092776, distance: 0.71802553934618 entropy 0.03264415264129639
epoch: 29, step: 42
	action: tensor([[-0.1270, -0.2464, -0.7516, -0.1934, -0.2916,  0.0202, -0.4005]],
       dtype=torch.float64)
	q_value: tensor([[-4.1414]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.02622621058340746, distance: 1.1592530430680021 entropy 0.03264415264129639
epoch: 29, step: 43
	action: tensor([[ 0.1408, -0.0696, -0.3396,  0.1515, -0.0470,  0.1339,  0.0548]],
       dtype=torch.float64)
	q_value: tensor([[-4.6990]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42358132862690934, distance: 0.8688118050494935 entropy 0.03264415264129639
epoch: 29, step: 44
	action: tensor([[-0.0406,  0.2186, -0.1043,  0.0330, -0.2344,  0.0389,  0.1563]],
       dtype=torch.float64)
	q_value: tensor([[-3.5185]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4162906229410197, distance: 0.8742890282974097 entropy 0.03264415264129639
epoch: 29, step: 45
	action: tensor([[ 0.0719, -0.2594,  0.2391, -0.0673,  0.0287, -0.2698, -0.2203]],
       dtype=torch.float64)
	q_value: tensor([[-4.0975]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.02108615647266665, distance: 1.132215062834291 entropy 0.03264415264129639
epoch: 29, step: 46
	action: tensor([[ 0.4907,  0.2345,  0.2701,  0.1613, -0.1048,  0.3944, -0.2870]],
       dtype=torch.float64)
	q_value: tensor([[-3.6038]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9003856601867486, distance: 0.3611749520439513 entropy 0.03264415264129639
epoch: 29, step: 47
	action: tensor([[ 0.1810, -0.0638, -0.6066,  0.2634,  0.1102,  0.2838, -0.2813]],
       dtype=torch.float64)
	q_value: tensor([[-4.8286]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4852773492610212, distance: 0.8210003331034587 entropy 0.03264415264129639
epoch: 29, step: 48
	action: tensor([[-0.3701, -0.2696, -0.0214, -0.1404, -0.2477, -0.2126, -0.0894]],
       dtype=torch.float64)
	q_value: tensor([[-3.8243]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4111015549558461, distance: 1.359364200810454 entropy 0.03264415264129639
epoch: 29, step: 49
	action: tensor([[ 0.3341,  0.1295,  0.0632, -0.1627, -0.0963,  0.3357,  0.2702]],
       dtype=torch.float64)
	q_value: tensor([[-4.0061]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6399704632828722, distance: 0.6866347186327164 entropy 0.03264415264129639
epoch: 29, step: 50
	action: tensor([[-0.0867,  0.2591, -0.3013, -0.0986, -0.1643,  0.0226, -0.4535]],
       dtype=torch.float64)
	q_value: tensor([[-4.4778]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35747265998975775, distance: 0.9172812140705212 entropy 0.03264415264129639
epoch: 29, step: 51
	action: tensor([[-0.1387,  0.1607, -0.3235,  0.3117,  0.2245,  0.2698, -0.3404]],
       dtype=torch.float64)
	q_value: tensor([[-4.1929]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3667280810955631, distance: 0.9106506639386408 entropy 0.03264415264129639
epoch: 29, step: 52
	action: tensor([[ 0.2893, -0.0759, -0.3039, -0.3461, -0.0852, -0.0076, -0.0955]],
       dtype=torch.float64)
	q_value: tensor([[-3.8435]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3589537190240041, distance: 0.9162234136247053 entropy 0.03264415264129639
epoch: 29, step: 53
	action: tensor([[ 0.3144, -0.1448, -0.1522,  0.2300, -0.3295,  0.1922, -0.1792]],
       dtype=torch.float64)
	q_value: tensor([[-4.1317]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5868917051813719, distance: 0.7355101170861517 entropy 0.03264415264129639
epoch: 29, step: 54
	action: tensor([[ 0.5641, -0.1654,  0.0728, -0.1976, -0.0618,  0.2060,  0.1844]],
       dtype=torch.float64)
	q_value: tensor([[-3.8750]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4936127856009038, distance: 0.8143255460328699 entropy 0.03264415264129639
epoch: 29, step: 55
	action: tensor([[-0.0330,  0.1487, -0.2166, -0.3053,  0.5402,  0.0537, -0.1637]],
       dtype=torch.float64)
	q_value: tensor([[-4.3692]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2937920433868386, distance: 0.9616631982780446 entropy 0.03264415264129639
epoch: 29, step: 56
	action: tensor([[-0.0185,  0.1299, -0.0857, -0.0274, -0.3567, -0.1349, -0.2334]],
       dtype=torch.float64)
	q_value: tensor([[-3.7438]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35939944592373185, distance: 0.9159048278632691 entropy 0.03264415264129639
epoch: 29, step: 57
	action: tensor([[ 0.1698, -0.3750, -0.3185,  0.2030,  0.0532,  0.1135,  0.3895]],
       dtype=torch.float64)
	q_value: tensor([[-4.1434]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24070743478168277, distance: 0.9971517757093078 entropy 0.03264415264129639
epoch: 29, step: 58
	action: tensor([[-0.0442,  0.2296, -0.3715,  0.0629,  0.0068,  0.6454,  0.1176]],
       dtype=torch.float64)
	q_value: tensor([[-3.9199]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4775457777319696, distance: 0.8271434117887868 entropy 0.03264415264129639
epoch: 29, step: 59
	action: tensor([[-0.0367, -0.0166, -0.0522, -0.0398, -0.2800,  0.1045, -0.1503]],
       dtype=torch.float64)
	q_value: tensor([[-4.8679]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2518556514489665, distance: 0.9898044297045604 entropy 0.03264415264129639
epoch: 29, step: 60
	action: tensor([[-0.0056, -0.4539,  0.2650, -0.0515,  0.4981,  0.0779, -0.1949]],
       dtype=torch.float64)
	q_value: tensor([[-3.7327]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.12194136266298328, distance: 1.2121092709896042 entropy 0.03264415264129639
epoch: 29, step: 61
	action: tensor([[ 2.3840e-01,  2.7535e-01, -3.1276e-01, -1.4093e-01,  2.2845e-01,
          2.8469e-04,  3.6528e-02]], dtype=torch.float64)
	q_value: tensor([[-3.4392]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.634254686362939, distance: 0.6920637128054061 entropy 0.03264415264129639
epoch: 29, step: 62
	action: tensor([[ 0.1145,  0.2235, -0.0059, -0.5589, -0.3486,  0.1959, -0.2016]],
       dtype=torch.float64)
	q_value: tensor([[-4.0518]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3604214754447378, distance: 0.9151739079268739 entropy 0.03264415264129639
epoch: 29, step: 63
	action: tensor([[ 0.0303,  0.0625, -0.0804, -0.1806,  0.2644, -0.1682, -0.0510]],
       dtype=torch.float64)
	q_value: tensor([[-5.2350]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2528230873106573, distance: 0.9891642577470776 entropy 0.03264415264129639
epoch: 29, step: 64
	action: tensor([[ 0.4321, -0.2059, -0.3508, -0.4661,  0.1853, -0.0486, -0.4634]],
       dtype=torch.float64)
	q_value: tensor([[-3.5768]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2170165476678081, distance: 1.012588486823559 entropy 0.03264415264129639
epoch: 29, step: 65
	action: tensor([[ 0.1947,  0.3593, -0.1886, -0.0256,  0.2929,  0.0345,  0.1440]],
       dtype=torch.float64)
	q_value: tensor([[-4.1283]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6823584376552377, distance: 0.6449489722129828 entropy 0.03264415264129639
epoch: 29, step: 66
	action: tensor([[-0.0496,  0.4023, -0.1849,  0.1074,  0.1058, -0.1086,  0.1369]],
       dtype=torch.float64)
	q_value: tensor([[-4.1048]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5020918739796877, distance: 0.8074791186945234 entropy 0.03264415264129639
epoch: 29, step: 67
	action: tensor([[ 0.0877, -0.6011, -0.2316,  0.1736, -0.2053, -0.0620, -0.5691]],
       dtype=torch.float64)
	q_value: tensor([[-4.1118]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06956076234879771, distance: 1.1834759159076937 entropy 0.03264415264129639
epoch: 29, step: 68
	action: tensor([[-0.0386, -0.3272, -0.3585, -0.2292, -0.0613,  0.1100, -0.0372]],
       dtype=torch.float64)
	q_value: tensor([[-3.8170]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.05995743262991793, distance: 1.1781508625721775 entropy 0.03264415264129639
epoch: 29, step: 69
	action: tensor([[ 0.1123,  0.0709, -0.3365,  0.3517,  0.2536,  0.2675, -0.1991]],
       dtype=torch.float64)
	q_value: tensor([[-3.6956]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5611838577149628, distance: 0.7580502422080143 entropy 0.03264415264129639
epoch: 29, step: 70
	action: tensor([[ 0.1261, -0.0300, -0.1922, -0.8834, -0.2613, -0.1142, -0.3597]],
       dtype=torch.float64)
	q_value: tensor([[-3.6981]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.055276143793869026, distance: 1.1122672104125397 entropy 0.03264415264129639
epoch: 29, step: 71
	action: tensor([[ 0.0675,  0.2704, -0.2763,  0.1962, -0.2591, -0.1185, -0.1985]],
       dtype=torch.float64)
	q_value: tensor([[-5.6666]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5313439864473263, distance: 0.7834004023996651 entropy 0.03264415264129639
epoch: 29, step: 72
	action: tensor([[-0.0802,  0.2362, -0.0903,  0.1211,  0.0808,  0.1151, -0.1398]],
       dtype=torch.float64)
	q_value: tensor([[-4.1894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4151532322506075, distance: 0.875140414532429 entropy 0.03264415264129639
epoch: 29, step: 73
	action: tensor([[-0.4636,  0.0642, -0.3560,  0.1365, -0.1258,  0.0119, -0.0415]],
       dtype=torch.float64)
	q_value: tensor([[-3.6256]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.16361551353460446, distance: 1.234415722538076 entropy 0.03264415264129639
epoch: 29, step: 74
	action: tensor([[-0.1928, -0.0512,  0.0772,  0.1804,  0.0251, -0.2705,  0.2448]],
       dtype=torch.float64)
	q_value: tensor([[-4.1838]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05863767666776498, distance: 1.110286602707292 entropy 0.03264415264129639
epoch: 29, step: 75
	action: tensor([[-0.0972, -0.0040, -0.0592, -0.0220,  0.6241, -0.0461, -0.0890]],
       dtype=torch.float64)
	q_value: tensor([[-3.9169]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1357648908347383, distance: 1.0638310109023517 entropy 0.03264415264129639
epoch: 29, step: 76
	action: tensor([[-0.2282,  0.1034,  0.1712, -0.2200, -0.5924,  0.3465,  0.1397]],
       dtype=torch.float64)
	q_value: tensor([[-3.5467]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.015218516300992024, distance: 1.1356032594164343 entropy 0.03264415264129639
epoch: 29, step: 77
	action: tensor([[ 0.0795, -0.1869, -0.2298,  0.1773,  0.0071,  0.3003, -0.0804]],
       dtype=torch.float64)
	q_value: tensor([[-5.3597]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3233581442397254, distance: 0.9413174783356155 entropy 0.03264415264129639
epoch: 29, step: 78
	action: tensor([[-0.0890, -0.1258, -0.2982,  0.0795, -0.4500, -0.3938,  0.5087]],
       dtype=torch.float64)
	q_value: tensor([[-3.3408]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1214930754204524, distance: 1.0725789953692504 entropy 0.03264415264129639
epoch: 29, step: 79
	action: tensor([[ 0.2890, -0.3470, -0.0363,  0.2687, -0.0612, -0.0964, -0.0211]],
       dtype=torch.float64)
	q_value: tensor([[-5.2552]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3551458142654307, distance: 0.9189406321823449 entropy 0.03264415264129639
epoch: 29, step: 80
	action: tensor([[ 0.3276, -0.3635, -0.1858, -0.7899,  0.3617,  0.2191,  0.2514]],
       dtype=torch.float64)
	q_value: tensor([[-3.6343]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10573986531465329, distance: 1.2033256578768605 entropy 0.03264415264129639
epoch: 29, step: 81
	action: tensor([[0.4150, 0.3281, 0.2294, 0.2371, 0.0983, 0.2806, 0.2101]],
       dtype=torch.float64)
	q_value: tensor([[-4.9481]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9137446562857335, distance: 0.33608544006960317 entropy 0.03264415264129639
epoch: 29, step: 82
	action: tensor([[ 0.2295, -0.4140, -0.1933, -0.1255, -0.1374,  0.0372, -0.5872]],
       dtype=torch.float64)
	q_value: tensor([[-4.5108]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08816593538570017, distance: 1.092734360869262 entropy 0.03264415264129639
epoch: 29, step: 83
	action: tensor([[ 0.1490,  0.0311,  0.0634, -0.0627,  0.1503, -0.1376, -0.1566]],
       dtype=torch.float64)
	q_value: tensor([[-3.7714]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39046432552978194, distance: 0.8934212082748421 entropy 0.03264415264129639
epoch: 29, step: 84
	action: tensor([[ 0.1471, -0.0267, -0.6904, -0.3573,  0.0870, -0.1280, -0.2308]],
       dtype=torch.float64)
	q_value: tensor([[-3.4716]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37591257812017875, distance: 0.9040228506776733 entropy 0.03264415264129639
epoch: 29, step: 85
	action: tensor([[ 0.1899, -0.0109, -0.1713, -0.2629,  0.2309, -0.1228, -0.2186]],
       dtype=torch.float64)
	q_value: tensor([[-4.6124]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3267085414695584, distance: 0.938984116054762 entropy 0.03264415264129639
epoch: 29, step: 86
	action: tensor([[ 0.1851,  0.0554, -0.1630, -0.0267, -0.2486, -0.1108, -0.2132]],
       dtype=torch.float64)
	q_value: tensor([[-3.5831]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4918602993213599, distance: 0.8157334229785533 entropy 0.03264415264129639
epoch: 29, step: 87
	action: tensor([[ 0.2471, -0.0321, -0.2733, -0.0943,  0.2515,  0.6232, -0.4495]],
       dtype=torch.float64)
	q_value: tensor([[-3.8904]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6087152617120439, distance: 0.7158188781227708 entropy 0.03264415264129639
epoch: 29, step: 88
	action: tensor([[ 0.2313, -0.0200,  0.1734,  0.1071,  0.1724, -0.2310, -0.1428]],
       dtype=torch.float64)
	q_value: tensor([[-4.1001]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.47832344537961624, distance: 0.8265275853802753 entropy 0.03264415264129639
epoch: 29, step: 89
	action: tensor([[ 0.1341, -0.1333, -0.2034,  0.2992, -0.0913, -0.1783, -0.1067]],
       dtype=torch.float64)
	q_value: tensor([[-3.6973]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41979454109991066, distance: 0.8716609667056339 entropy 0.03264415264129639
epoch: 29, step: 90
	action: tensor([[ 0.4331,  0.0646,  0.0141, -0.4450, -0.0173, -0.0303,  0.2099]],
       dtype=torch.float64)
	q_value: tensor([[-3.6248]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44573149848367344, distance: 0.8519552679686636 entropy 0.03264415264129639
epoch: 29, step: 91
	action: tensor([[ 0.0550,  0.1286, -0.1069,  0.0751, -0.4131,  0.1900, -0.2559]],
       dtype=torch.float64)
	q_value: tensor([[-4.6862]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.484100156485048, distance: 0.8219386284145287 entropy 0.03264415264129639
epoch: 29, step: 92
	action: tensor([[ 0.2473, -0.2243,  0.2097,  0.1838,  0.2894,  0.3396,  0.2209]],
       dtype=torch.float64)
	q_value: tensor([[-4.2491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5508982249090829, distance: 0.7668829443777737 entropy 0.03264415264129639
epoch: 29, step: 93
	action: tensor([[ 0.0958,  0.0553, -0.1963, -0.0894, -0.1217,  0.1735, -0.2342]],
       dtype=torch.float64)
	q_value: tensor([[-3.8250]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4124800865514707, distance: 0.8771381265149347 entropy 0.03264415264129639
epoch: 29, step: 94
	action: tensor([[ 0.4830,  0.5397, -0.6276,  0.3079, -0.3107,  0.6015,  0.2358]],
       dtype=torch.float64)
	q_value: tensor([[-3.6579]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7715873802403073, distance: 0.5469111036750831 entropy 0.03264415264129639
epoch: 29, step: 95
	action: tensor([[-0.1819,  0.0894, -0.2170, -0.0292,  0.0603, -0.0975, -0.2977]],
       dtype=torch.float64)
	q_value: tensor([[-6.8178]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14815808361499327, distance: 1.056175761511774 entropy 0.03264415264129639
epoch: 29, step: 96
	action: tensor([[-0.2331,  0.2114, -0.0266, -0.3518, -0.1473, -0.0745, -0.1521]],
       dtype=torch.float64)
	q_value: tensor([[-3.5292]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.048014793983317317, distance: 1.1165335899784519 entropy 0.03264415264129639
epoch: 29, step: 97
	action: tensor([[ 0.2221, -0.1097, -0.2675, -0.0111, -0.2668, -0.4113, -0.3101]],
       dtype=torch.float64)
	q_value: tensor([[-4.1568]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3624426270584614, distance: 0.9137267292061891 entropy 0.03264415264129639
epoch: 29, step: 98
	action: tensor([[ 0.1342,  0.2344,  0.0940, -0.0622,  0.3212,  0.0377, -0.2230]],
       dtype=torch.float64)
	q_value: tensor([[-4.3433]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5596849527403029, distance: 0.7593438097511624 entropy 0.03264415264129639
epoch: 29, step: 99
	action: tensor([[ 0.1810, -0.3466, -0.4656,  0.0057, -0.2555, -0.0330, -0.7352]],
       dtype=torch.float64)
	q_value: tensor([[-3.5486]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17141766714084006, distance: 1.0416564988799821 entropy 0.03264415264129639
epoch: 29, step: 100
	action: tensor([[ 0.2015,  0.1205, -0.2652,  0.1186,  0.5324,  0.5462,  0.2553]],
       dtype=torch.float64)
	q_value: tensor([[-4.2519]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.705004329878246, distance: 0.62153346053293 entropy 0.03264415264129639
epoch: 29, step: 101
	action: tensor([[ 0.0221,  0.0046,  0.1614,  0.1444, -0.4740,  0.3211, -0.8172]],
       dtype=torch.float64)
	q_value: tensor([[-4.3936]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4510371474950442, distance: 0.8478678582863417 entropy 0.03264415264129639
epoch: 29, step: 102
	action: tensor([[ 0.0871,  0.3330,  0.0044,  0.1341, -0.0652, -0.3079,  0.2262]],
       dtype=torch.float64)
	q_value: tensor([[-5.0440]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5909880281119682, distance: 0.731854425471392 entropy 0.03264415264129639
epoch: 29, step: 103
	action: tensor([[ 0.3882, -0.0395, -0.1679, -0.3445,  0.0549, -0.3189, -0.1766]],
       dtype=torch.float64)
	q_value: tensor([[-4.3898]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33317271494577827, distance: 0.9344657197414362 entropy 0.03264415264129639
epoch: 29, step: 104
	action: tensor([[-0.0678, -0.0931, -0.3847,  0.2203, -0.1720,  0.1699,  0.0314]],
       dtype=torch.float64)
	q_value: tensor([[-4.3199]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2284133686717651, distance: 1.0051920391753792 entropy 0.03264415264129639
epoch: 29, step: 105
	action: tensor([[ 0.0662,  0.1362,  0.1523,  0.1509,  0.7835,  0.0591, -0.2980]],
       dtype=torch.float64)
	q_value: tensor([[-3.6673]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5269786921939643, distance: 0.7870404354764994 entropy 0.03264415264129639
epoch: 29, step: 106
	action: tensor([[ 0.0665, -0.0403, -0.0990, -0.1054,  0.0403,  0.3227, -0.6060]],
       dtype=torch.float64)
	q_value: tensor([[-3.7091]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.338004837935055, distance: 0.9310737894763359 entropy 0.03264415264129639
epoch: 29, step: 107
	action: tensor([[-0.1040,  0.0656, -0.0708,  0.0680, -0.2677,  0.1807, -0.4588]],
       dtype=torch.float64)
	q_value: tensor([[-3.7744]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.293281331676677, distance: 0.9620108606701282 entropy 0.03264415264129639
epoch: 29, step: 108
	action: tensor([[ 0.0007, -0.0145, -0.3866, -0.5264, -0.3944, -0.0581, -0.1716]],
       dtype=torch.float64)
	q_value: tensor([[-3.9496]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1751346550493742, distance: 1.0393174575815098 entropy 0.03264415264129639
epoch: 29, step: 109
	action: tensor([[ 0.2253, -0.1358, -0.4280, -0.2965,  0.1082,  0.0180, -0.1984]],
       dtype=torch.float64)
	q_value: tensor([[-5.1264]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30941101346662514, distance: 0.9509693445710997 entropy 0.03264415264129639
epoch: 29, step: 110
	action: tensor([[ 0.2706,  0.2379, -0.0252,  0.1045,  0.1417,  0.1557, -0.6312]],
       dtype=torch.float64)
	q_value: tensor([[-3.7745]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7344240289205713, distance: 0.5897270956221602 entropy 0.03264415264129639
epoch: 29, step: 111
	action: tensor([[ 0.6766, -0.2132,  0.1213, -0.1586,  0.2265, -0.0129, -0.1418]],
       dtype=torch.float64)
	q_value: tensor([[-4.0921]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4365329649277897, distance: 0.8589956210455472 entropy 0.03264415264129639
epoch: 29, step: 112
	action: tensor([[ 0.2306,  0.2980, -0.2309, -0.0342,  0.4167,  0.1339, -0.0450]],
       dtype=torch.float64)
	q_value: tensor([[-4.3099]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.705480633182322, distance: 0.6210314903954791 entropy 0.03264415264129639
epoch: 29, step: 113
	action: tensor([[-0.0557,  0.2242, -0.0540, -0.1069, -0.0030, -0.0006, -0.0445]],
       dtype=torch.float64)
	q_value: tensor([[-3.8924]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35710968898542084, distance: 0.9175402687867671 entropy 0.03264415264129639
epoch: 29, step: 114
	action: tensor([[ 0.2040,  0.1622, -0.1540, -0.3306, -0.1409,  0.0688,  0.1582]],
       dtype=torch.float64)
	q_value: tensor([[-3.7075]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.47581611388746425, distance: 0.8285114722120388 entropy 0.03264415264129639
epoch: 29, step: 115
	action: tensor([[ 0.1764, -0.3297, -0.7002, -0.2164,  0.0711,  0.2409, -0.4813]],
       dtype=torch.float64)
	q_value: tensor([[-4.4443]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1859507945569866, distance: 1.0324808888453354 entropy 0.03264415264129639
epoch: 29, step: 116
	action: tensor([[ 0.2023,  0.3985, -0.6013, -0.2892, -0.0392,  0.4204, -0.3670]],
       dtype=torch.float64)
	q_value: tensor([[-4.1058]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6930982084547095, distance: 0.6339520420303607 entropy 0.03264415264129639
epoch: 29, step: 117
	action: tensor([[ 0.4244,  0.0995, -0.1217,  0.1230, -0.2583, -0.0857, -0.1333]],
       dtype=torch.float64)
	q_value: tensor([[-5.6068]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7355511735597307, distance: 0.5884743194157814 entropy 0.03264415264129639
epoch: 29, step: 118
	action: tensor([[ 0.0088, -0.3369, -0.0741, -0.0569, -0.4456, -0.2677, -0.3323]],
       dtype=torch.float64)
	q_value: tensor([[-4.2002]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.016027377612689797, distance: 1.1534782198394775 entropy 0.03264415264129639
epoch: 29, step: 119
	action: tensor([[ 0.1497, -0.0755,  0.1453, -0.3414,  0.1212, -0.1381, -0.0107]],
       dtype=torch.float64)
	q_value: tensor([[-4.1996]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13259304617538548, distance: 1.0657814155533787 entropy 0.03264415264129639
epoch: 29, step: 120
	action: tensor([[ 0.1430, -0.2029, -0.5407, -0.1948,  0.0919, -0.0139,  0.0445]],
       dtype=torch.float64)
	q_value: tensor([[-3.7273]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20892320757844685, distance: 1.0178083635374564 entropy 0.03264415264129639
epoch: 29, step: 121
	action: tensor([[ 0.0318, -0.1663, -0.2464,  0.1742,  0.0644,  0.2066, -0.2246]],
       dtype=torch.float64)
	q_value: tensor([[-3.9356]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3159476499921796, distance: 0.9464580357295583 entropy 0.03264415264129639
epoch: 29, step: 122
	action: tensor([[ 0.0347,  0.2048, -0.1513, -0.2849,  0.2411, -0.0017,  0.0740]],
       dtype=torch.float64)
	q_value: tensor([[-3.2316]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3747899908976562, distance: 0.904835548152837 entropy 0.03264415264129639
epoch: 29, step: 123
	action: tensor([[-0.1400,  0.0704, -0.2753, -0.0363,  0.0019, -0.0012,  0.1260]],
       dtype=torch.float64)
	q_value: tensor([[-3.8153]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19434964588473613, distance: 1.0271408327792757 entropy 0.03264415264129639
epoch: 29, step: 124
	action: tensor([[ 0.1023,  0.0946, -0.2964,  0.1395, -0.2268, -0.1374,  0.2352]],
       dtype=torch.float64)
	q_value: tensor([[-3.8153]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.48079629214761066, distance: 0.8245663088970953 entropy 0.03264415264129639
epoch: 29, step: 125
	action: tensor([[ 0.2793,  0.3346, -0.6330,  0.0640,  0.2941,  0.2970,  0.1735]],
       dtype=torch.float64)
	q_value: tensor([[-4.1418]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7405763905948776, distance: 0.582856228796242 entropy 0.03264415264129639
epoch: 29, step: 126
	action: tensor([[ 0.5274,  0.4089, -0.2179,  0.1883, -0.1197, -0.3053,  0.0151]],
       dtype=torch.float64)
	q_value: tensor([[-5.0353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8948960702229236, distance: 0.3709933888066753 entropy 0.03264415264129639
epoch: 29, step: 127
	action: tensor([[ 0.2188, -0.1404, -0.2858, -0.0072,  0.3169,  0.2602,  0.0533]],
       dtype=torch.float64)
	q_value: tensor([[-5.0938]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.45031223243614915, distance: 0.8484274857741984 entropy 0.03264415264129639
LOSS epoch 29 actor 10.83202457109597 critic 37.668047260646716 
epoch: 30, step: 0
	action: tensor([[ 0.1925,  0.1815,  0.0125, -0.3539, -0.0797,  0.2476,  0.3038]],
       dtype=torch.float64)
	q_value: tensor([[-2.9762]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.49212174612524406, distance: 0.815523541388963 entropy 0.03264415264129639
epoch: 30, step: 1
	action: tensor([[ 0.0810,  0.1555, -0.0937, -0.4227, -0.3630,  0.3052,  0.2160]],
       dtype=torch.float64)
	q_value: tensor([[-4.0894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35808550481166557, distance: 0.9168436566962006 entropy 0.03264415264129639
epoch: 30, step: 2
	action: tensor([[ 0.1092, -0.0338, -0.2843,  0.1820,  0.1324,  0.1794,  0.0769]],
       dtype=torch.float64)
	q_value: tensor([[-4.5125]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4682628314878041, distance: 0.8344593833395352 entropy 0.03264415264129639
epoch: 30, step: 3
	action: tensor([[ 0.0121, -0.1532, -0.1480, -0.7510,  0.1551, -0.3278, -0.0685]],
       dtype=torch.float64)
	q_value: tensor([[-3.0036]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.15850529649574296, distance: 1.2317021658495428 entropy 0.03264415264129639
epoch: 30, step: 4
	action: tensor([[-0.4257,  0.4750, -0.0079, -0.2604,  0.1430,  0.2703, -0.2928]],
       dtype=torch.float64)
	q_value: tensor([[-4.1183]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10354131978085745, distance: 1.0834823266027196 entropy 0.03264415264129639
epoch: 30, step: 5
	action: tensor([[-0.0517,  0.0695,  0.1113,  0.0568,  0.2402,  0.3457, -0.0503]],
       dtype=torch.float64)
	q_value: tensor([[-4.2114]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.40294438592288984, distance: 0.8842276401689309 entropy 0.03264415264129639
epoch: 30, step: 6
	action: tensor([[ 0.1810, -0.2706, -0.7098,  0.0953, -0.4085,  0.0172, -0.0296]],
       dtype=torch.float64)
	q_value: tensor([[-3.0359]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24846352912770964, distance: 0.992045802308968 entropy 0.03264415264129639
epoch: 30, step: 7
	action: tensor([[ 0.2903, -0.3114, -0.1398,  0.3341,  0.2766,  0.0041, -0.1128]],
       dtype=torch.float64)
	q_value: tensor([[-3.8560]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.45442367212719925, distance: 0.8452485850954969 entropy 0.03264415264129639
epoch: 30, step: 8
	action: tensor([[ 0.3196,  0.5985, -0.6188,  0.1076,  0.1009, -0.2590, -0.5124]],
       dtype=torch.float64)
	q_value: tensor([[-3.1158]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8255165684085992, distance: 0.4780064868077024 entropy 0.03264415264129639
epoch: 30, step: 9
	action: tensor([[ 2.1944e-04,  3.0608e-01, -2.5250e-01, -4.5917e-01, -1.6637e-01,
          9.1687e-02,  5.2298e-03]], dtype=torch.float64)
	q_value: tensor([[-5.1665]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.40693854821860553, distance: 0.8812650391139984 entropy 0.03264415264129639
epoch: 30, step: 10
	action: tensor([[ 0.3887,  0.2775, -0.5912, -0.1584,  0.2890,  0.0973,  0.0850]],
       dtype=torch.float64)
	q_value: tensor([[-4.3107]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7639118784592407, distance: 0.5560242866681676 entropy 0.03264415264129639
epoch: 30, step: 11
	action: tensor([[ 0.3544,  0.2559, -0.2366,  0.0305,  0.3383,  0.3102, -0.3584]],
       dtype=torch.float64)
	q_value: tensor([[-4.4080]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7959441977778411, distance: 0.5169293218184119 entropy 0.03264415264129639
epoch: 30, step: 12
	action: tensor([[ 0.0635,  0.1425, -0.4522, -0.2855,  0.2770, -0.0101, -0.3991]],
       dtype=torch.float64)
	q_value: tensor([[-3.4141]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42094138445738005, distance: 0.8707990711060849 entropy 0.03264415264129639
epoch: 30, step: 13
	action: tensor([[ 0.2745, -0.3272, -0.6217, -0.0330, -0.2710,  0.0591, -0.2459]],
       dtype=torch.float64)
	q_value: tensor([[-3.4298]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2584675501687925, distance: 0.9854209103288457 entropy 0.03264415264129639
epoch: 30, step: 14
	action: tensor([[ 0.1874,  0.2794, -0.0541, -0.1629,  0.0276, -0.0157, -0.1659]],
       dtype=torch.float64)
	q_value: tensor([[-3.5268]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5927843745446195, distance: 0.7302455352320634 entropy 0.03264415264129639
epoch: 30, step: 15
	action: tensor([[ 0.3777,  0.2699, -0.5411, -0.2867,  0.2978, -0.3913, -0.4290]],
       dtype=torch.float64)
	q_value: tensor([[-3.3608]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6799370741299908, distance: 0.647402509923101 entropy 0.03264415264129639
epoch: 30, step: 16
	action: tensor([[ 0.4306, -0.1724, -0.4455,  0.1764,  0.2472, -0.0662,  0.0614]],
       dtype=torch.float64)
	q_value: tensor([[-4.7239]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5449168732327312, distance: 0.7719729092295801 entropy 0.03264415264129639
epoch: 30, step: 17
	action: tensor([[ 0.4268, -0.1047, -0.0649,  0.0029,  0.1054, -0.0073, -0.5438]],
       dtype=torch.float64)
	q_value: tensor([[-3.5503]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5575469807671218, distance: 0.7611850934755878 entropy 0.03264415264129639
epoch: 30, step: 18
	action: tensor([[-0.0299, -0.1079, -0.1237, -0.2048, -0.2875,  0.1838, -0.2082]],
       dtype=torch.float64)
	q_value: tensor([[-3.3910]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12678008491840287, distance: 1.0693466400979326 entropy 0.03264415264129639
epoch: 30, step: 19
	action: tensor([[-0.3068,  0.3117, -0.1450, -0.0707,  0.1374,  0.1273, -0.2333]],
       dtype=torch.float64)
	q_value: tensor([[-3.3934]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17774042301813908, distance: 1.037674545575422 entropy 0.03264415264129639
epoch: 30, step: 20
	action: tensor([[ 0.1860,  0.3360, -0.1306,  0.0425, -0.5924, -0.4547, -0.3678]],
       dtype=torch.float64)
	q_value: tensor([[-3.4727]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6929254897098552, distance: 0.6341304052769136 entropy 0.03264415264129639
epoch: 30, step: 21
	action: tensor([[-0.0268,  0.0744, -0.2655,  0.1556, -0.0043,  0.3295, -0.3275]],
       dtype=torch.float64)
	q_value: tensor([[-4.7466]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42080524484962634, distance: 0.8709014297220087 entropy 0.03264415264129639
epoch: 30, step: 22
	action: tensor([[ 0.2218,  0.3348, -0.2344,  0.2138,  0.0974, -0.0112, -0.4807]],
       dtype=torch.float64)
	q_value: tensor([[-3.0937]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7257606457871268, distance: 0.599268682201245 entropy 0.03264415264129639
epoch: 30, step: 23
	action: tensor([[-0.2312,  0.1362, -0.3452,  0.2119, -0.4586,  0.0850, -0.2757]],
       dtype=torch.float64)
	q_value: tensor([[-3.6028]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17724792941147638, distance: 1.0379852574196202 entropy 0.03264415264129639
epoch: 30, step: 24
	action: tensor([[ 0.0097, -0.0161, -0.3775,  0.3805,  0.2953, -0.0028, -0.4689]],
       dtype=torch.float64)
	q_value: tensor([[-3.8910]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3868760858861179, distance: 0.8960470637790626 entropy 0.03264415264129639
epoch: 30, step: 25
	action: tensor([[ 0.0836, -0.1935,  0.0374, -0.1007,  0.3636,  0.3893, -0.2451]],
       dtype=torch.float64)
	q_value: tensor([[-3.2932]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2938654103479307, distance: 0.9616132440554015 entropy 0.03264415264129639
epoch: 30, step: 26
	action: tensor([[-0.0920, -0.0862, -0.7343, -0.1666, -0.2364,  0.1588, -0.2343]],
       dtype=torch.float64)
	q_value: tensor([[-2.8125]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13187692988676414, distance: 1.0662212702071687 entropy 0.03264415264129639
epoch: 30, step: 27
	action: tensor([[ 0.5951, -0.0708, -0.1920,  0.3442, -0.1069,  0.1578, -0.3140]],
       dtype=torch.float64)
	q_value: tensor([[-4.0638]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7978852023017056, distance: 0.514464898890816 entropy 0.03264415264129639
epoch: 30, step: 28
	action: tensor([[-0.0591,  0.2183, -0.3315, -0.4552,  0.3038,  0.2245,  0.0535]],
       dtype=torch.float64)
	q_value: tensor([[-3.8602]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.347330427843332, distance: 0.9244924660483484 entropy 0.03264415264129639
epoch: 30, step: 29
	action: tensor([[ 0.2471, -0.0589, -0.1756, -0.1958,  0.0662,  0.3480, -0.0252]],
       dtype=torch.float64)
	q_value: tensor([[-4.0269]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.46495432632223244, distance: 0.8370513894656544 entropy 0.03264415264129639
epoch: 30, step: 30
	action: tensor([[ 0.0815,  0.2344, -0.0997, -0.0264, -0.1459,  0.1854, -0.0184]],
       dtype=torch.float64)
	q_value: tensor([[-3.2037]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5492842275263143, distance: 0.7682597336302748 entropy 0.03264415264129639
epoch: 30, step: 31
	action: tensor([[ 0.0705,  0.0692, -0.2931, -0.1370,  0.5049,  0.0680, -0.1433]],
       dtype=torch.float64)
	q_value: tensor([[-3.4800]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3943450369251654, distance: 0.8905726088152842 entropy 0.03264415264129639
epoch: 30, step: 32
	action: tensor([[ 0.1274,  0.0392, -0.0317,  0.1663, -0.4018, -0.0767,  0.0640]],
       dtype=torch.float64)
	q_value: tensor([[-3.1324]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5141618975381672, distance: 0.7976318350926856 entropy 0.03264415264129639
epoch: 30, step: 33
	action: tensor([[ 0.1643,  0.0231, -0.3463,  0.2852,  0.4424,  0.0470,  0.0418]],
       dtype=torch.float64)
	q_value: tensor([[-3.5143]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5467388643814146, distance: 0.770426006741349 entropy 0.03264415264129639
epoch: 30, step: 34
	action: tensor([[-0.2011,  0.1873, -0.1533,  0.2347, -0.3127, -0.2728, -0.0374]],
       dtype=torch.float64)
	q_value: tensor([[-3.2997]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2859509518931106, distance: 0.9669871781100191 entropy 0.03264415264129639
epoch: 30, step: 35
	action: tensor([[ 0.6376, -0.0321, -0.2973,  0.1517,  0.1222, -0.0861, -0.4337]],
       dtype=torch.float64)
	q_value: tensor([[-3.6612]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7268772453401178, distance: 0.5980474394303072 entropy 0.03264415264129639
epoch: 30, step: 36
	action: tensor([[-0.0838, -0.0069, -0.1701, -0.4323,  0.1112,  0.1093, -0.1891]],
       dtype=torch.float64)
	q_value: tensor([[-3.9202]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07934733992247178, distance: 1.0980057121054196 entropy 0.03264415264129639
epoch: 30, step: 37
	action: tensor([[-0.1456,  0.3739, -0.0442, -0.2402, -0.1399,  0.0685, -0.0178]],
       dtype=torch.float64)
	q_value: tensor([[-3.2320]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3188019905615811, distance: 0.9444813320383846 entropy 0.03264415264129639
epoch: 30, step: 38
	action: tensor([[ 0.0598, -0.1133, -0.2001, -0.3749,  0.2925,  0.1056,  0.1130]],
       dtype=torch.float64)
	q_value: tensor([[-3.8631]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.12695705955018743, distance: 1.0692382728846896 entropy 0.03264415264129639
epoch: 30, step: 39
	action: tensor([[ 0.1314,  0.3725, -0.1464,  0.0827,  0.2685,  0.3717, -0.1445]],
       dtype=torch.float64)
	q_value: tensor([[-3.2758]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7023905072172824, distance: 0.6242809507517285 entropy 0.03264415264129639
epoch: 30, step: 40
	action: tensor([[-0.0329, -0.5200, -0.3495, -0.3423, -0.2366, -0.2660, -0.1839]],
       dtype=torch.float64)
	q_value: tensor([[-3.4381]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2793908007611461, distance: 1.2943696021011337 entropy 0.03264415264129639
epoch: 30, step: 41
	action: tensor([[-0.1185, -0.1751, -0.0851,  0.2618, -0.3561,  0.3100, -0.2735]],
       dtype=torch.float64)
	q_value: tensor([[-3.8963]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1629693721090546, distance: 1.046953439668186 entropy 0.03264415264129639
epoch: 30, step: 42
	action: tensor([[ 0.3617,  0.0198, -0.3378,  0.0902, -0.3031,  0.0021, -0.2307]],
       dtype=torch.float64)
	q_value: tensor([[-3.2314]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6203906156891177, distance: 0.7050585172178165 entropy 0.03264415264129639
epoch: 30, step: 43
	action: tensor([[ 0.3028,  0.3634, -0.2775, -0.4331,  0.3164,  0.2336, -0.3333]],
       dtype=torch.float64)
	q_value: tensor([[-3.5781]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7135415222046343, distance: 0.6124738249650972 entropy 0.03264415264129639
epoch: 30, step: 44
	action: tensor([[-0.0855,  0.0335,  0.3291,  0.0213, -0.1834,  0.2572,  0.0436]],
       dtype=torch.float64)
	q_value: tensor([[-4.0126]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31734371178921494, distance: 0.9454917436377358 entropy 0.03264415264129639
epoch: 30, step: 45
	action: tensor([[ 0.5926,  0.2245, -0.6070, -0.0943, -0.1194,  0.3774, -0.1320]],
       dtype=torch.float64)
	q_value: tensor([[-3.5096]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8463399490238794, distance: 0.4485771834457812 entropy 0.03264415264129639
epoch: 30, step: 46
	action: tensor([[ 0.1478,  0.3475,  0.2483, -0.2117, -0.4567,  0.0943,  0.0431]],
       dtype=torch.float64)
	q_value: tensor([[-4.9773]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5693306304962886, distance: 0.7509805437507334 entropy 0.03264415264129639
epoch: 30, step: 47
	action: tensor([[-0.0144,  0.0248, -0.1570, -0.0510,  0.3753, -0.1927, -0.1227]],
       dtype=torch.float64)
	q_value: tensor([[-4.6297]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22842759315200778, distance: 1.0051827735922159 entropy 0.03264415264129639
epoch: 30, step: 48
	action: tensor([[ 0.0068,  0.3306, -0.5078,  0.1395,  0.1801,  0.1452,  0.0230]],
       dtype=torch.float64)
	q_value: tensor([[-3.0638]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5240774094974198, distance: 0.7894504073939521 entropy 0.03264415264129639
epoch: 30, step: 49
	action: tensor([[-0.0331,  0.3592, -0.2589, -0.3642, -0.0932,  0.5198,  0.0206]],
       dtype=torch.float64)
	q_value: tensor([[-3.7321]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4754117675980396, distance: 0.828830960247592 entropy 0.03264415264129639
epoch: 30, step: 50
	action: tensor([[ 0.1480, -0.1226, -0.1700,  0.2626, -0.0137,  0.2115, -0.1113]],
       dtype=torch.float64)
	q_value: tensor([[-4.7688]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4967532540028031, distance: 0.8117965119999522 entropy 0.03264415264129639
epoch: 30, step: 51
	action: tensor([[ 0.0747, -0.2735, -0.0753,  0.3611, -0.3237,  0.0823, -0.1088]],
       dtype=torch.float64)
	q_value: tensor([[-2.9004]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3571234873942053, distance: 0.9175304221119747 entropy 0.03264415264129639
epoch: 30, step: 52
	action: tensor([[-0.2642, -0.0411, -0.4399, -0.3811, -0.1488, -0.0120,  0.0069]],
       dtype=torch.float64)
	q_value: tensor([[-3.0319]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06366839670478242, distance: 1.1802114431962551 entropy 0.03264415264129639
epoch: 30, step: 53
	action: tensor([[-0.0705,  0.2822, -0.2858, -0.3794,  0.4950,  0.1933,  0.1115]],
       dtype=torch.float64)
	q_value: tensor([[-4.0203]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3719402528604584, distance: 0.9068953461694284 entropy 0.03264415264129639
epoch: 30, step: 54
	action: tensor([[ 0.3248, -0.1277, -0.2712, -0.5355, -0.2598, -0.0822,  0.1684]],
       dtype=torch.float64)
	q_value: tensor([[-3.9644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18493902415759955, distance: 1.0331223176470035 entropy 0.03264415264129639
epoch: 30, step: 55
	action: tensor([[ 0.4055, -0.0290, -0.1628,  0.0153, -0.2717,  0.1651, -0.2391]],
       dtype=torch.float64)
	q_value: tensor([[-4.5393]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5953025899240081, distance: 0.7279841198988385 entropy 0.03264415264129639
epoch: 30, step: 56
	action: tensor([[ 0.0260,  0.1911,  0.0930, -0.4235,  0.0359, -0.1942,  0.2081]],
       dtype=torch.float64)
	q_value: tensor([[-3.5569]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.20881302623064946, distance: 1.0178792413547542 entropy 0.03264415264129639
epoch: 30, step: 57
	action: tensor([[ 0.3706, -0.1824,  0.2769, -0.0529, -0.5450,  0.3424, -0.4717]],
       dtype=torch.float64)
	q_value: tensor([[-3.8794]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4740907584924232, distance: 0.829873878063006 entropy 0.03264415264129639
epoch: 30, step: 58
	action: tensor([[ 0.1541, -0.1452, -0.2192,  0.2612, -0.0311, -0.0485, -0.2672]],
       dtype=torch.float64)
	q_value: tensor([[-4.5892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.43192039323759335, distance: 0.8625043473816444 entropy 0.03264415264129639
epoch: 30, step: 59
	action: tensor([[ 0.1882, -0.0930, -0.3317, -0.1029, -0.0907, -0.1016,  0.6375]],
       dtype=torch.float64)
	q_value: tensor([[-2.9895]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33921413596154126, distance: 0.9302229823741808 entropy 0.03264415264129639
epoch: 30, step: 60
	action: tensor([[ 0.1663,  0.2980,  0.1113, -0.2233, -0.4009, -0.0108, -0.0840]],
       dtype=torch.float64)
	q_value: tensor([[-4.3775]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5532743952883483, distance: 0.7648514880552658 entropy 0.03264415264129639
epoch: 30, step: 61
	action: tensor([[ 0.0214, -0.3915, -0.2500, -0.0276,  0.1582, -0.1139, -0.2630]],
       dtype=torch.float64)
	q_value: tensor([[-4.3052]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.046838364657692644, distance: 1.170837189197489 entropy 0.03264415264129639
epoch: 30, step: 62
	action: tensor([[-0.0400,  0.1251,  0.1481, -0.1096, -0.0667,  0.0107, -0.1487]],
       dtype=torch.float64)
	q_value: tensor([[-2.8481]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29445583329843483, distance: 0.9612111413491484 entropy 0.03264415264129639
epoch: 30, step: 63
	action: tensor([[ 0.1634, -0.6325,  0.1880,  0.4033,  0.1982, -0.2565,  0.2484]],
       dtype=torch.float64)
	q_value: tensor([[-3.2533]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05661648957563126, distance: 1.1114779047265975 entropy 0.03264415264129639
epoch: 30, step: 64
	action: tensor([[-0.0119,  0.1979, -0.3838,  0.0738, -0.0542,  0.6708, -0.0270]],
       dtype=torch.float64)
	q_value: tensor([[-3.6841]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4854274819773843, distance: 0.8208805909479212 entropy 0.03264415264129639
epoch: 30, step: 65
	action: tensor([[ 0.3365,  0.5187, -0.2981, -0.0922,  0.4407, -0.2242,  0.5069]],
       dtype=torch.float64)
	q_value: tensor([[-4.2492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7844513848414492, distance: 0.5312871497590756 entropy 0.03264415264129639
epoch: 30, step: 66
	action: tensor([[ 0.6289, -0.0206, -0.4078,  0.3058,  0.0828,  0.0823,  0.0989]],
       dtype=torch.float64)
	q_value: tensor([[-5.1077]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7953759417248404, distance: 0.5176485956374642 entropy 0.03264415264129639
epoch: 30, step: 67
	action: tensor([[-0.0209, -0.4715, -0.5042, -0.5000,  0.0709,  0.0088, -0.0983]],
       dtype=torch.float64)
	q_value: tensor([[-3.8892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2046779802863843, distance: 1.2560073456726955 entropy 0.03264415264129639
epoch: 30, step: 68
	action: tensor([[ 0.2061, -0.3249, -0.1418, -0.0487, -0.4266,  0.3878, -0.1990]],
       dtype=torch.float64)
	q_value: tensor([[-3.7704]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23779939457512522, distance: 0.9990594634853643 entropy 0.03264415264129639
epoch: 30, step: 69
	action: tensor([[ 1.3382e-01, -2.3871e-01, -1.5918e-01,  2.2292e-04, -1.6533e-01,
          3.2327e-02, -2.9191e-01]], dtype=torch.float64)
	q_value: tensor([[-3.6674]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.243232223253239, distance: 0.995492538032457 entropy 0.03264415264129639
epoch: 30, step: 70
	action: tensor([[ 0.1454,  0.0624,  0.0249,  0.0930, -0.0596,  0.5530,  0.4263]],
       dtype=torch.float64)
	q_value: tensor([[-2.9833]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6262173107986297, distance: 0.6996265553459564 entropy 0.03264415264129639
epoch: 30, step: 71
	action: tensor([[ 0.4283,  0.0519,  0.1844,  0.0244,  0.1003, -0.1460, -0.0668]],
       dtype=torch.float64)
	q_value: tensor([[-3.9480]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6257275640920658, distance: 0.7000847462269034 entropy 0.03264415264129639
epoch: 30, step: 72
	action: tensor([[ 0.0976,  0.0698, -0.0596, -0.3171, -0.1380,  0.1315, -0.6908]],
       dtype=torch.float64)
	q_value: tensor([[-3.4414]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3174322437860503, distance: 0.9454304324130449 entropy 0.03264415264129639
epoch: 30, step: 73
	action: tensor([[-0.1781,  0.1575, -0.6231, -0.3276,  0.1619,  0.0344, -0.4926]],
       dtype=torch.float64)
	q_value: tensor([[-3.8978]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23633208356519297, distance: 1.0000206449006905 entropy 0.03264415264129639
epoch: 30, step: 74
	action: tensor([[-0.1895,  0.0126, -0.1159,  0.0148,  0.0437,  0.0246, -0.0599]],
       dtype=torch.float64)
	q_value: tensor([[-4.1315]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11734905615241897, distance: 1.0751057590065403 entropy 0.03264415264129639
epoch: 30, step: 75
	action: tensor([[-0.2043,  0.3747,  0.0932, -0.1846,  0.0236,  0.2015, -0.1929]],
       dtype=torch.float64)
	q_value: tensor([[-2.9663]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2930638874575735, distance: 0.9621588457324537 entropy 0.03264415264129639
epoch: 30, step: 76
	action: tensor([[ 0.5377, -0.1943, -0.2053, -0.7938,  0.2479, -0.0605,  0.3151]],
       dtype=torch.float64)
	q_value: tensor([[-3.6474]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06392007920805964, distance: 1.107167064382468 entropy 0.03264415264129639
epoch: 30, step: 77
	action: tensor([[ 0.0300, -0.0965, -0.5319,  0.2150,  0.4415, -0.1565, -0.2076]],
       dtype=torch.float64)
	q_value: tensor([[-4.9872]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2588254901886754, distance: 0.985183048752091 entropy 0.03264415264129639
epoch: 30, step: 78
	action: tensor([[-0.1316,  0.0991, -0.5078, -0.0244,  0.0171,  0.0189, -0.3566]],
       dtype=torch.float64)
	q_value: tensor([[-3.3826]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2357771920154088, distance: 1.0003838932643458 entropy 0.03264415264129639
epoch: 30, step: 79
	action: tensor([[ 0.3241,  0.1719, -0.3509,  0.0237, -0.4664,  0.2631,  0.1953]],
       dtype=torch.float64)
	q_value: tensor([[-3.3542]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6837112358305022, distance: 0.6435741257926797 entropy 0.03264415264129639
epoch: 30, step: 80
	action: tensor([[ 0.4623, -0.3647, -0.1759,  0.1937, -0.3650,  0.5465, -0.1335]],
       dtype=torch.float64)
	q_value: tensor([[-4.4249]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5607628909155994, distance: 0.7584137628909936 entropy 0.03264415264129639
epoch: 30, step: 81
	action: tensor([[ 0.2962, -0.1275,  0.1035, -0.4759, -0.2701,  0.3520,  0.2177]],
       dtype=torch.float64)
	q_value: tensor([[-4.0096]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2570075720794591, distance: 0.9863905142120031 entropy 0.03264415264129639
epoch: 30, step: 82
	action: tensor([[ 0.2516,  0.2007, -0.3677, -0.3031,  0.0798, -0.3747,  0.1265]],
       dtype=torch.float64)
	q_value: tensor([[-4.3211]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5226149782259687, distance: 0.790662402197591 entropy 0.03264415264129639
epoch: 30, step: 83
	action: tensor([[ 0.0658, -0.0791, -0.2026, -0.1982,  0.2618,  0.0068, -0.1071]],
       dtype=torch.float64)
	q_value: tensor([[-4.2861]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22676816105629938, distance: 1.0062631236924715 entropy 0.03264415264129639
epoch: 30, step: 84
	action: tensor([[ 0.3348, -0.1146, -0.5757, -0.1944,  0.3144, -0.0721,  0.2966]],
       dtype=torch.float64)
	q_value: tensor([[-2.8752]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4048283711407392, distance: 0.8828314653491238 entropy 0.03264415264129639
epoch: 30, step: 85
	action: tensor([[ 0.1291,  0.3702, -0.3980,  0.5431,  0.3146,  0.0662, -0.1692]],
       dtype=torch.float64)
	q_value: tensor([[-4.1447]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6404397394103301, distance: 0.686187079875051 entropy 0.03264415264129639
epoch: 30, step: 86
	action: tensor([[ 0.3226,  0.1261,  0.1205, -0.0361, -0.1806,  0.2383,  0.0991]],
       dtype=torch.float64)
	q_value: tensor([[-3.8813]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6743699424920597, distance: 0.6530086531391278 entropy 0.03264415264129639
epoch: 30, step: 87
	action: tensor([[ 0.3483, -0.0025, -0.2164,  0.3878,  0.0818, -0.3622, -0.2829]],
       dtype=torch.float64)
	q_value: tensor([[-3.6958]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6556463269191144, distance: 0.6715201566288429 entropy 0.03264415264129639
epoch: 30, step: 88
	action: tensor([[-0.1434, -0.2291, -0.7513, -0.1054,  0.0592, -0.0586, -0.4636]],
       dtype=torch.float64)
	q_value: tensor([[-3.6508]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.02460772093675989, distance: 1.1583385373870405 entropy 0.03264415264129639
epoch: 30, step: 89
	action: tensor([[-0.1007, -0.4068,  0.1466, -0.4881, -0.1893, -0.5196, -0.0819]],
       dtype=torch.float64)
	q_value: tensor([[-3.7601]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.4640929914508096, distance: 1.384653233206697 entropy 0.03264415264129639
epoch: 30, step: 90
	action: tensor([[ 0.0496,  0.0026, -0.0759, -0.4083, -0.2840,  0.1485,  0.1776]],
       dtype=torch.float64)
	q_value: tensor([[-4.1466]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17242126569824057, distance: 1.041025468223895 entropy 0.03264415264129639
epoch: 30, step: 91
	action: tensor([[ 0.1507, -0.3792, -0.3638,  0.0811, -0.0353,  0.4191, -0.0184]],
       dtype=torch.float64)
	q_value: tensor([[-4.0242]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2238385187466182, distance: 1.0081676003276077 entropy 0.03264415264129639
epoch: 30, step: 92
	action: tensor([[ 0.5744, -0.2617, -0.3420, -0.1903,  0.6090,  0.1600, -0.1254]],
       dtype=torch.float64)
	q_value: tensor([[-3.1173]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.411566642681106, distance: 0.8778197249142057 entropy 0.03264415264129639
epoch: 30, step: 93
	action: tensor([[ 0.0137,  0.1295,  0.0871, -0.4019, -0.0970,  0.0690,  0.1122]],
       dtype=torch.float64)
	q_value: tensor([[-3.7725]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.22299080886363454, distance: 1.0087180015215191 entropy 0.03264415264129639
epoch: 30, step: 94
	action: tensor([[ 0.3551, -0.0490, -0.4744,  0.1231,  0.3953,  0.0326,  0.1409]],
       dtype=torch.float64)
	q_value: tensor([[-3.7009]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5777771890549771, distance: 0.7435797252956825 entropy 0.03264415264129639
epoch: 30, step: 95
	action: tensor([[ 0.2854,  0.1943,  0.0489,  0.2583,  0.1626,  0.1137, -0.0587]],
       dtype=torch.float64)
	q_value: tensor([[-3.6894]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7646866587112282, distance: 0.555111172835723 entropy 0.03264415264129639
epoch: 30, step: 96
	action: tensor([[-0.0615,  0.0470, -0.1018,  0.2813, -0.2885,  0.3812, -0.0726]],
       dtype=torch.float64)
	q_value: tensor([[-3.3389]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4212721527546215, distance: 0.8705503278335321 entropy 0.03264415264129639
epoch: 30, step: 97
	action: tensor([[ 0.3892, -0.0236, -0.4095, -0.1786,  0.1622,  0.1100,  0.0053]],
       dtype=torch.float64)
	q_value: tensor([[-3.4727]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5481253756495033, distance: 0.7692467500082016 entropy 0.03264415264129639
epoch: 30, step: 98
	action: tensor([[-0.0589,  0.4422, -0.2329, -0.4161,  0.0115,  0.3024,  0.0838]],
       dtype=torch.float64)
	q_value: tensor([[-3.4840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.47119064588015536, distance: 0.8321588910163713 entropy 0.03264415264129639
epoch: 30, step: 99
	action: tensor([[ 0.1695,  0.1452,  0.2618,  0.2002, -0.0885,  0.3392,  0.0664]],
       dtype=torch.float64)
	q_value: tensor([[-4.5744]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7065758761938217, distance: 0.619875685362415 entropy 0.03264415264129639
epoch: 30, step: 100
	action: tensor([[-0.1560,  0.1650, -0.2861,  0.3345, -0.0649, -0.2345,  0.0413]],
       dtype=torch.float64)
	q_value: tensor([[-3.6218]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31062316089063524, distance: 0.9501343896773161 entropy 0.03264415264129639
epoch: 30, step: 101
	action: tensor([[-0.2271,  0.0555, -0.1577, -0.6761,  0.0741, -0.4487, -0.2012]],
       dtype=torch.float64)
	q_value: tensor([[-3.4003]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1472513508025164, distance: 1.225705068908574 entropy 0.03264415264129639
epoch: 30, step: 102
	action: tensor([[ 0.0653, -0.0374, -0.0495,  0.0332,  0.3420, -0.2013,  0.2344]],
       dtype=torch.float64)
	q_value: tensor([[-4.3086]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2772510974896907, distance: 0.9728601486238716 entropy 0.03264415264129639
epoch: 30, step: 103
	action: tensor([[ 0.0052,  0.2136,  0.1130, -0.2044,  0.0152, -0.0924, -0.0520]],
       dtype=torch.float64)
	q_value: tensor([[-3.4268]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.32714302226709535, distance: 0.9386810998792066 entropy 0.03264415264129639
epoch: 30, step: 104
	action: tensor([[-0.0145, -0.0037, -0.3010, -0.0055,  0.1178,  0.1104,  0.1229]],
       dtype=torch.float64)
	q_value: tensor([[-3.3231]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29528089895189613, distance: 0.9606489552210747 entropy 0.03264415264129639
epoch: 30, step: 105
	action: tensor([[-5.2943e-02,  3.4257e-01, -4.6529e-01, -1.3991e-01,  1.7736e-04,
          4.3988e-01, -2.3714e-01]], dtype=torch.float64)
	q_value: tensor([[-3.1329]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4781298489283762, distance: 0.8266809351554907 entropy 0.03264415264129639
epoch: 30, step: 106
	action: tensor([[ 0.0540, -0.1427, -0.2046,  0.0825,  0.1149, -0.1983, -0.0232]],
       dtype=torch.float64)
	q_value: tensor([[-4.2835]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23089454091830497, distance: 1.0035745520802744 entropy 0.03264415264129639
epoch: 30, step: 107
	action: tensor([[ 0.8216,  0.2460, -0.6244, -0.3780, -0.0614,  0.1972, -0.0854]],
       dtype=torch.float64)
	q_value: tensor([[-3.0540]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8044919095214437, distance: 0.5059866520315277 entropy 0.03264415264129639
epoch: 30, step: 108
	action: tensor([[ 0.2548, -0.3287, -0.4724,  0.2317,  0.2576,  0.2249, -0.0729]],
       dtype=torch.float64)
	q_value: tensor([[-5.8466]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3828622679178909, distance: 0.8989752669298529 entropy 0.03264415264129639
epoch: 30, step: 109
	action: tensor([[ 0.0371,  0.2159, -0.4013, -0.1200,  0.4213,  0.0324,  0.1617]],
       dtype=torch.float64)
	q_value: tensor([[-3.1409]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.46709890968273915, distance: 0.8353721600074344 entropy 0.03264415264129639
epoch: 30, step: 110
	action: tensor([[ 0.5156,  0.1289, -0.4962, -0.0624, -0.2006, -0.1596, -0.1013]],
       dtype=torch.float64)
	q_value: tensor([[-3.7618]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.720270542337283, distance: 0.6052374529419633 entropy 0.03264415264129639
epoch: 30, step: 111
	action: tensor([[-0.0446, -0.3023,  0.3003,  0.0960,  0.0450,  0.2147, -0.3065]],
       dtype=torch.float64)
	q_value: tensor([[-4.3398]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.156351056123336, distance: 1.0510843671869166 entropy 0.03264415264129639
epoch: 30, step: 112
	action: tensor([[-0.0751,  0.2043,  0.1966, -0.1452,  0.1397, -0.0388, -0.1996]],
       dtype=torch.float64)
	q_value: tensor([[-2.8765]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2733136066475268, distance: 0.9755065897465685 entropy 0.03264415264129639
epoch: 30, step: 113
	action: tensor([[ 0.1637, -0.3748, -0.0371, -0.2385, -0.0418,  0.0336, -0.2069]],
       dtype=torch.float64)
	q_value: tensor([[-3.2230]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.021138662858316115, distance: 1.1563759566754004 entropy 0.03264415264129639
epoch: 30, step: 114
	action: tensor([[ 0.3229, -0.1863, -0.1727, -0.0899, -0.2265,  0.0869, -0.3223]],
       dtype=torch.float64)
	q_value: tensor([[-3.0434]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3683724164766433, distance: 0.9094676110020503 entropy 0.03264415264129639
epoch: 30, step: 115
	action: tensor([[ 0.3227,  0.4246, -0.6212, -0.0101,  0.3100,  0.1929,  0.0182]],
       dtype=torch.float64)
	q_value: tensor([[-3.3061]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7846362726303399, distance: 0.5310592439120893 entropy 0.03264415264129639
epoch: 30, step: 116
	action: tensor([[-0.0578, -0.0101,  0.0953, -0.7067, -0.2625,  0.1827,  0.0156]],
       dtype=torch.float64)
	q_value: tensor([[-4.5315]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.09176068892981037, distance: 1.1956950176029673 entropy 0.03264415264129639
epoch: 30, step: 117
	action: tensor([[ 0.3252,  0.2491,  0.0192,  0.3637, -0.2147,  0.2813, -0.2505]],
       dtype=torch.float64)
	q_value: tensor([[-4.4308]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8098588396449925, distance: 0.4989933561245419 entropy 0.03264415264129639
epoch: 30, step: 118
	action: tensor([[ 0.1481,  0.2831, -0.0271,  0.2047,  0.3069, -0.0060, -0.4108]],
       dtype=torch.float64)
	q_value: tensor([[-3.8803]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6564513100705361, distance: 0.6707348031720517 entropy 0.03264415264129639
epoch: 30, step: 119
	action: tensor([[ 0.1563,  0.0832, -0.0283, -0.0844,  0.1947,  0.1819,  0.0749]],
       dtype=torch.float64)
	q_value: tensor([[-3.2638]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4933926582363548, distance: 0.814502521136922 entropy 0.03264415264129639
epoch: 30, step: 120
	action: tensor([[-0.0112,  0.5586, -0.1589,  0.1437, -0.1873,  0.2533, -0.3374]],
       dtype=torch.float64)
	q_value: tensor([[-3.0436]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5835442691262606, distance: 0.7384840414148257 entropy 0.03264415264129639
epoch: 30, step: 121
	action: tensor([[ 0.0899,  0.0540, -0.3228,  0.0818, -0.1008, -0.1845, -0.2914]],
       dtype=torch.float64)
	q_value: tensor([[-4.2389]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4374162297904547, distance: 0.8583220960927728 entropy 0.03264415264129639
epoch: 30, step: 122
	action: tensor([[ 0.0152, -0.0598,  0.0984, -0.4564,  0.2192,  0.1587,  0.1658]],
       dtype=torch.float64)
	q_value: tensor([[-3.3038]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05684252947629176, distance: 1.111344738626214 entropy 0.03264415264129639
epoch: 30, step: 123
	action: tensor([[ 0.5328, -0.2126, -0.1776,  0.3045, -0.4137,  0.0190,  0.0663]],
       dtype=torch.float64)
	q_value: tensor([[-3.3281]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6387847109478441, distance: 0.6877645001756443 entropy 0.03264415264129639
epoch: 30, step: 124
	action: tensor([[-0.1863, -0.3754, -0.4079, -0.3425, -0.1707, -0.0269, -0.0264]],
       dtype=torch.float64)
	q_value: tensor([[-3.8202]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.26405701660550385, distance: 1.286589566249504 entropy 0.03264415264129639
epoch: 30, step: 125
	action: tensor([[ 0.1242,  0.1996, -0.3076, -0.0029,  0.0584, -0.1175, -0.1141]],
       dtype=torch.float64)
	q_value: tensor([[-3.7482]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.510980412021994, distance: 0.8002391987811862 entropy 0.03264415264129639
epoch: 30, step: 126
	action: tensor([[ 0.5480,  0.3107, -0.4314, -0.4632,  0.1101,  0.1170, -0.1372]],
       dtype=torch.float64)
	q_value: tensor([[-3.2986]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7621814701791324, distance: 0.5580582569430619 entropy 0.03264415264129639
epoch: 30, step: 127
	action: tensor([[ 0.5782,  0.5240,  0.3423,  0.3992,  0.1281, -0.2908, -0.4244]],
       dtype=torch.float64)
	q_value: tensor([[-4.8675]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9555623835834284, distance: 0.24123041900542802 entropy 0.03264415264129639
LOSS epoch 30 actor 8.848219541955496 critic 41.61897407670193 
epoch: 31, step: 0
	action: tensor([[ 0.2786,  0.5141, -0.1018,  0.0099, -0.5286,  0.5144, -0.5095]],
       dtype=torch.float64)
	q_value: tensor([[-4.3679]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.756796922541521, distance: 0.5643405100410847 entropy 0.03264415264129639
epoch: 31, step: 1
	action: tensor([[ 0.2540,  0.0604, -0.0149, -0.1702,  0.2664,  0.2560, -0.0959]],
       dtype=torch.float64)
	q_value: tensor([[-5.1845]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5560938287716815, distance: 0.7624340519751908 entropy 0.03264415264129639
epoch: 31, step: 2
	action: tensor([[ 0.5224, -0.0821, -0.2397,  0.2004,  0.1681, -0.3630,  0.0275]],
       dtype=torch.float64)
	q_value: tensor([[-2.6069]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6111018945797557, distance: 0.7136324781023086 entropy 0.03264415264129639
epoch: 31, step: 3
	action: tensor([[ 0.1593,  0.2082, -0.4888, -0.2046,  0.3946, -0.1854, -0.5116]],
       dtype=torch.float64)
	q_value: tensor([[-3.4143]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5343087400067743, distance: 0.7809185458271165 entropy 0.03264415264129639
epoch: 31, step: 4
	action: tensor([[ 0.2629,  0.2106, -0.1985, -0.0713,  0.0208,  0.3454,  0.2066]],
       dtype=torch.float64)
	q_value: tensor([[-3.3921]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6954350264540555, distance: 0.6315339047701802 entropy 0.03264415264129639
epoch: 31, step: 5
	action: tensor([[-0.0466,  0.3939, -0.0339,  0.2342,  0.2539, -0.1796,  0.4284]],
       dtype=torch.float64)
	q_value: tensor([[-3.2757]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5118619663816933, distance: 0.7995175788410882 entropy 0.03264415264129639
epoch: 31, step: 6
	action: tensor([[-0.1538,  0.1662,  0.3982, -0.5701, -0.4859,  0.2039, -0.1977]],
       dtype=torch.float64)
	q_value: tensor([[-3.5444]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.06301240541633235, distance: 1.1798474538961206 entropy 0.03264415264129639
epoch: 31, step: 7
	action: tensor([[ 0.5014,  0.2265, -0.0438,  0.2592,  0.1721,  0.0657, -0.2241]],
       dtype=torch.float64)
	q_value: tensor([[-4.3355]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8682712114076279, distance: 0.4153335768681084 entropy 0.03264415264129639
epoch: 31, step: 8
	action: tensor([[ 0.3816,  0.1867, -0.3294, -0.3808,  0.1420, -0.0426, -0.2039]],
       dtype=torch.float64)
	q_value: tensor([[-3.0980]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5974037110760578, distance: 0.7260918748540942 entropy 0.03264415264129639
epoch: 31, step: 9
	action: tensor([[ 0.2129,  0.5027, -0.4424, -0.0974,  0.0137, -0.0470, -0.1273]],
       dtype=torch.float64)
	q_value: tensor([[-3.4131]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.742677999309511, distance: 0.5804905484812231 entropy 0.03264415264129639
epoch: 31, step: 10
	action: tensor([[-0.0864,  0.5480, -0.4073,  0.0586,  0.6018,  0.2203, -0.4641]],
       dtype=torch.float64)
	q_value: tensor([[-3.8295]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5589476217426289, distance: 0.759979324637485 entropy 0.03264415264129639
epoch: 31, step: 11
	action: tensor([[ 0.5323,  0.2410, -0.2337, -0.0348, -0.0429,  0.3816,  0.2322]],
       dtype=torch.float64)
	q_value: tensor([[-3.7407]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8591407346518176, distance: 0.42948638678356876 entropy 0.03264415264129639
epoch: 31, step: 12
	action: tensor([[ 0.3835,  0.2028, -0.1855, -0.4015, -0.3444,  0.2833, -0.2632]],
       dtype=torch.float64)
	q_value: tensor([[-3.7983]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6463204689096409, distance: 0.6805525354543787 entropy 0.03264415264129639
epoch: 31, step: 13
	action: tensor([[-0.0175, -0.0455, -0.1168, -0.4525, -0.0793, -0.2238,  0.1757]],
       dtype=torch.float64)
	q_value: tensor([[-4.1523]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.029523206968843807, distance: 1.1273253435993507 entropy 0.03264415264129639
epoch: 31, step: 14
	action: tensor([[ 0.2911, -0.2740, -0.2921,  0.0922,  0.3678, -0.1699, -0.2058]],
       dtype=torch.float64)
	q_value: tensor([[-3.4228]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.30084847265452797, distance: 0.9568466674725168 entropy 0.03264415264129639
epoch: 31, step: 15
	action: tensor([[ 0.3791,  0.6804, -0.2872, -0.0235, -0.1975, -0.2600, -0.1474]],
       dtype=torch.float64)
	q_value: tensor([[-2.6353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8729443241019985, distance: 0.4079000219577333 entropy 0.03264415264129639
epoch: 31, step: 16
	action: tensor([[-0.0973, -0.6141,  0.0518, -0.0652, -0.0753,  0.0542,  0.0099]],
       dtype=torch.float64)
	q_value: tensor([[-4.5733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3531235230837617, distance: 1.3311451534202894 entropy 0.03264415264129639
epoch: 31, step: 17
	action: tensor([[ 0.3061, -0.0404, -0.3533, -0.1525, -0.1959, -0.5732,  0.0018]],
       dtype=torch.float64)
	q_value: tensor([[-2.6959]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3920286121461426, distance: 0.8922740525676293 entropy 0.03264415264129639
epoch: 31, step: 18
	action: tensor([[ 0.0073, -0.1446, -0.4427,  0.4605,  0.1932,  0.6043,  0.2738]],
       dtype=torch.float64)
	q_value: tensor([[-3.9014]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4126983226155969, distance: 0.8769752035675366 entropy 0.03264415264129639
epoch: 31, step: 19
	action: tensor([[ 0.0612, -0.0172, -0.4390,  0.2103, -0.0092, -0.0980, -0.1212]],
       dtype=torch.float64)
	q_value: tensor([[-3.3391]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3835959656147415, distance: 0.8984407247984477 entropy 0.03264415264129639
epoch: 31, step: 20
	action: tensor([[-0.2074,  0.4526, -0.3205, -0.0674, -0.3048,  0.0764,  0.0461]],
       dtype=torch.float64)
	q_value: tensor([[-2.6991]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35220366326794217, distance: 0.9210345824922755 entropy 0.03264415264129639
epoch: 31, step: 21
	action: tensor([[ 0.2437, -0.1229, -0.2896,  0.1015, -0.0343, -0.0444,  0.0739]],
       dtype=torch.float64)
	q_value: tensor([[-3.9292]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4458416357342273, distance: 0.851870618857002 entropy 0.03264415264129639
epoch: 31, step: 22
	action: tensor([[ 0.2554, -0.0317, -0.2678,  0.1052,  0.0931,  0.1772, -0.3449]],
       dtype=torch.float64)
	q_value: tensor([[-2.7100]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5753479947396769, distance: 0.7457156935271487 entropy 0.03264415264129639
epoch: 31, step: 23
	action: tensor([[ 0.4867,  0.1011, -0.4333, -0.3695,  0.1986,  0.3831, -0.1903]],
       dtype=torch.float64)
	q_value: tensor([[-2.5527]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7003760572850644, distance: 0.6263901943234315 entropy 0.03264415264129639
epoch: 31, step: 24
	action: tensor([[-0.0983, -0.1795, -0.0271, -0.2706, -0.1844, -0.1740,  0.2223]],
       dtype=torch.float64)
	q_value: tensor([[-3.6921]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.10349932116033767, distance: 1.2021058992756182 entropy 0.03264415264129639
epoch: 31, step: 25
	action: tensor([[ 0.4950,  0.2803,  0.2655,  0.3835,  0.1962,  0.0316, -0.2484]],
       dtype=torch.float64)
	q_value: tensor([[-3.1961]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9281638251213896, distance: 0.3067102519787488 entropy 0.03264415264129639
epoch: 31, step: 26
	action: tensor([[ 0.3770, -0.1171,  0.0217,  0.1949,  0.2017, -0.2975, -0.2400]],
       dtype=torch.float64)
	q_value: tensor([[-3.4275]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5166397899803105, distance: 0.7955951767681319 entropy 0.03264415264129639
epoch: 31, step: 27
	action: tensor([[ 0.2802,  0.4175, -0.3919, -0.0532, -0.4037,  0.1234, -0.1977]],
       dtype=torch.float64)
	q_value: tensor([[-2.8882]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7352456526659025, distance: 0.5888141570012347 entropy 0.03264415264129639
epoch: 31, step: 28
	action: tensor([[ 0.3560, -0.0133,  0.0615, -0.1218,  0.0962, -0.5169, -0.2544]],
       dtype=torch.float64)
	q_value: tensor([[-4.0980]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39258722464835216, distance: 0.8918640415078236 entropy 0.03264415264129639
epoch: 31, step: 29
	action: tensor([[ 0.3403, -0.0844,  0.1681, -0.0802, -0.0066,  0.1274,  0.0471]],
       dtype=torch.float64)
	q_value: tensor([[-3.3312]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4986558162881567, distance: 0.8102605299257449 entropy 0.03264415264129639
epoch: 31, step: 30
	action: tensor([[-0.2734,  0.0717, -0.5002, -0.0354,  0.1874, -0.0792, -0.4835]],
       dtype=torch.float64)
	q_value: tensor([[-2.8014]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0420646332593797, distance: 1.1200174706063206 entropy 0.03264415264129639
epoch: 31, step: 31
	action: tensor([[ 0.2038,  0.1107, -0.4038, -0.1368,  0.2874,  0.0723, -0.1567]],
       dtype=torch.float64)
	q_value: tensor([[-3.1214]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5271983254334585, distance: 0.7868576949657878 entropy 0.03264415264129639
epoch: 31, step: 32
	action: tensor([[-0.0286, -0.1129,  0.1524, -0.0431,  0.4515,  0.3786, -0.3072]],
       dtype=torch.float64)
	q_value: tensor([[-2.8321]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28782467635026776, distance: 0.965717617065664 entropy 0.03264415264129639
epoch: 31, step: 33
	action: tensor([[ 0.0081,  0.2944, -0.2103, -0.1371, -0.1258, -0.2598, -0.2604]],
       dtype=torch.float64)
	q_value: tensor([[-2.4531]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44646594974351606, distance: 0.8513906255272404 entropy 0.03264415264129639
epoch: 31, step: 34
	action: tensor([[ 0.4568,  0.1651, -0.4721,  0.0037,  0.2178,  0.0946, -0.1976]],
       dtype=torch.float64)
	q_value: tensor([[-3.2926]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7610261875147106, distance: 0.5594120916233208 entropy 0.03264415264129639
epoch: 31, step: 35
	action: tensor([[-0.0084, -0.0468,  0.1514, -0.5483, -0.2166,  0.4758,  0.1894]],
       dtype=torch.float64)
	q_value: tensor([[-3.2862]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06267117499319452, distance: 1.1079054012916392 entropy 0.03264415264129639
epoch: 31, step: 36
	action: tensor([[-0.2227,  0.2271, -0.0126, -0.2985, -0.2024,  0.3604,  0.0691]],
       dtype=torch.float64)
	q_value: tensor([[-3.8765]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15199666185377747, distance: 1.0537933999375304 entropy 0.03264415264129639
epoch: 31, step: 37
	action: tensor([[ 0.1534, -0.5801,  0.2198, -0.1442, -0.1316,  0.3930, -0.1788]],
       dtype=torch.float64)
	q_value: tensor([[-3.6438]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07237428071376129, distance: 1.1850314817398246 entropy 0.03264415264129639
epoch: 31, step: 38
	action: tensor([[ 0.2201, -0.0286, -0.2089, -0.4487,  0.0643,  0.2715, -0.4111]],
       dtype=torch.float64)
	q_value: tensor([[-2.9068]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.329775772913257, distance: 0.9368428671417783 entropy 0.03264415264129639
epoch: 31, step: 39
	action: tensor([[-0.1704,  0.3305, -0.3322, -0.4413, -0.0468, -0.1594, -0.5942]],
       dtype=torch.float64)
	q_value: tensor([[-3.0878]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26591172407464536, distance: 0.9804621603219701 entropy 0.03264415264129639
epoch: 31, step: 40
	action: tensor([[ 0.0459, -0.4105, -0.4163, -0.2440,  0.1999,  0.1539, -0.1740]],
       dtype=torch.float64)
	q_value: tensor([[-3.8859]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.040610595391295856, distance: 1.1673492672328616 entropy 0.03264415264129639
epoch: 31, step: 41
	action: tensor([[ 0.3356,  0.0614, -0.2220, -0.1140,  0.1865,  0.2213, -0.2056]],
       dtype=torch.float64)
	q_value: tensor([[-2.5976]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.615480178477493, distance: 0.7096040071471316 entropy 0.03264415264129639
epoch: 31, step: 42
	action: tensor([[ 0.3148,  0.1784, -0.3121, -0.0068,  0.1426,  0.1681,  0.2168]],
       dtype=torch.float64)
	q_value: tensor([[-2.6865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6959972021746955, distance: 0.6309507828699806 entropy 0.03264415264129639
epoch: 31, step: 43
	action: tensor([[ 0.3030, -0.0857, -0.3178,  0.2729,  0.3373,  0.1046, -0.2283]],
       dtype=torch.float64)
	q_value: tensor([[-3.1664]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6033726127905894, distance: 0.7206892479031692 entropy 0.03264415264129639
epoch: 31, step: 44
	action: tensor([[ 0.2615, -0.0042, -0.1195, -0.0723, -0.0230, -0.2449, -0.8128]],
       dtype=torch.float64)
	q_value: tensor([[-2.6783]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4520477897926314, distance: 0.847087035080652 entropy 0.03264415264129639
epoch: 31, step: 45
	action: tensor([[-0.0640,  0.3680, -0.4701,  0.0136,  0.0063,  0.2978,  0.2584]],
       dtype=torch.float64)
	q_value: tensor([[-3.4711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.47267815606658903, distance: 0.8309876593234532 entropy 0.03264415264129639
epoch: 31, step: 46
	action: tensor([[-0.0532, -0.0554, -0.4133,  0.1275, -0.0359,  0.2400,  0.2282]],
       dtype=torch.float64)
	q_value: tensor([[-3.8925]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.258175309740578, distance: 0.9856150700456638 entropy 0.03264415264129639
epoch: 31, step: 47
	action: tensor([[-0.1967, -0.0755,  0.3196,  0.3172,  0.3417,  0.5409, -0.0957]],
       dtype=torch.float64)
	q_value: tensor([[-2.9322]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42997517278688846, distance: 0.8639797808245047 entropy 0.03264415264129639
epoch: 31, step: 48
	action: tensor([[ 0.3809,  0.6312, -0.0552, -0.0528, -0.0465,  0.2290, -0.1880]],
       dtype=torch.float64)
	q_value: tensor([[-2.9453]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8512580982652465, distance: 0.4413400686137555 entropy 0.03264415264129639
epoch: 31, step: 49
	action: tensor([[-0.2082,  0.1007, -0.0617,  0.1092, -0.1698,  0.0637, -0.4405]],
       dtype=torch.float64)
	q_value: tensor([[-4.0356]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2057397320024491, distance: 1.0198542551904524 entropy 0.03264415264129639
epoch: 31, step: 50
	action: tensor([[ 0.1409, -0.0861, -0.1643, -0.1924, -0.0384, -0.1075, -0.5280]],
       dtype=torch.float64)
	q_value: tensor([[-2.8744]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26768235671673724, distance: 0.9792790012491451 entropy 0.03264415264129639
epoch: 31, step: 51
	action: tensor([[-0.3059, -0.1437,  0.1326,  0.0359,  0.2785,  0.2116, -0.0272]],
       dtype=torch.float64)
	q_value: tensor([[-2.8105]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.061311333840176285, distance: 1.1789030582702218 entropy 0.03264415264129639
epoch: 31, step: 52
	action: tensor([[ 0.2883, -0.0716,  0.1075,  0.1075,  0.1047,  0.2680, -0.1626]],
       dtype=torch.float64)
	q_value: tensor([[-2.6531]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5933997123097436, distance: 0.7296935948021818 entropy 0.03264415264129639
epoch: 31, step: 53
	action: tensor([[ 0.1061, -0.0950, -0.2614, -0.0314,  0.1549, -0.1476, -0.0927]],
       dtype=torch.float64)
	q_value: tensor([[-2.6282]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2610074882286624, distance: 0.9837318035142405 entropy 0.03264415264129639
epoch: 31, step: 54
	action: tensor([[ 0.3802,  0.1842, -0.0725, -0.2759,  0.5925,  0.1715, -0.0157]],
       dtype=torch.float64)
	q_value: tensor([[-2.6151]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.653129109421503, distance: 0.673970086064948 entropy 0.03264415264129639
epoch: 31, step: 55
	action: tensor([[-0.0127,  0.1677, -0.1622, -0.2922,  0.0775, -0.2282, -0.3231]],
       dtype=torch.float64)
	q_value: tensor([[-3.1655]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2554029541163766, distance: 0.987455078418514 entropy 0.03264415264129639
epoch: 31, step: 56
	action: tensor([[ 0.3047, -0.0490, -0.0769, -0.4290, -0.5036, -0.4213, -0.3310]],
       dtype=torch.float64)
	q_value: tensor([[-2.9776]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.27796422164774826, distance: 0.9723800778210844 entropy 0.03264415264129639
epoch: 31, step: 57
	action: tensor([[ 0.0579, -0.3063, -0.0750, -0.0567,  0.0166, -0.3998, -0.1409]],
       dtype=torch.float64)
	q_value: tensor([[-4.1298]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.017036097231192038, distance: 1.1540506687158159 entropy 0.03264415264129639
epoch: 31, step: 58
	action: tensor([[ 0.0032,  0.0710, -0.3203, -0.4954, -0.1498,  0.2291, -0.1099]],
       dtype=torch.float64)
	q_value: tensor([[-2.8392]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24727469376813194, distance: 0.9928301375576812 entropy 0.03264415264129639
epoch: 31, step: 59
	action: tensor([[ 0.2864,  0.3145, -0.1999,  0.1456,  0.2747,  0.1016, -0.0984]],
       dtype=torch.float64)
	q_value: tensor([[-3.5856]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7558654959497905, distance: 0.5654201416087523 entropy 0.03264415264129639
epoch: 31, step: 60
	action: tensor([[ 0.1628,  0.0014, -0.5946,  0.0727,  0.1160, -0.2611, -0.0411]],
       dtype=torch.float64)
	q_value: tensor([[-2.9191]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4399846042564507, distance: 0.8563605976592672 entropy 0.03264415264129639
epoch: 31, step: 61
	action: tensor([[ 4.2466e-01, -4.5079e-01, -3.3466e-01,  1.2039e-01, -1.1353e-01,
         -9.0497e-06,  6.7597e-02]], dtype=torch.float64)
	q_value: tensor([[-3.2624]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28817640992482507, distance: 0.9654791103042104 entropy 0.03264415264129639
epoch: 31, step: 62
	action: tensor([[ 0.1102,  0.0117, -0.5486,  0.2426, -0.1543,  0.5058, -0.3719]],
       dtype=torch.float64)
	q_value: tensor([[-2.8782]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.45445471169933116, distance: 0.8452245403136202 entropy 0.03264415264129639
epoch: 31, step: 63
	action: tensor([[ 0.5473,  0.3486, -0.3754,  0.3110, -0.1818,  0.1932, -0.3156]],
       dtype=torch.float64)
	q_value: tensor([[-3.3311]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8963745690308561, distance: 0.3683747618973078 entropy 0.03264415264129639
epoch: 31, step: 64
	action: tensor([[-0.3274,  0.0399, -0.3224,  0.0720,  0.4028,  0.5271, -0.2593]],
       dtype=torch.float64)
	q_value: tensor([[-3.9365]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14054967041224253, distance: 1.0608820090653865 entropy 0.03264415264129639
epoch: 31, step: 65
	action: tensor([[-0.0493, -0.1257, -0.3964,  0.0857, -0.0568,  0.1042,  0.0173]],
       dtype=torch.float64)
	q_value: tensor([[-3.1735]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18963035634205294, distance: 1.0301448015711225 entropy 0.03264415264129639
epoch: 31, step: 66
	action: tensor([[ 0.4248,  0.3713, -0.2576,  0.1525, -0.5159, -0.1537, -0.2207]],
       dtype=torch.float64)
	q_value: tensor([[-2.6574]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8443923619401054, distance: 0.4514110112654937 entropy 0.03264415264129639
epoch: 31, step: 67
	action: tensor([[-0.4076, -0.1559, -0.2280,  0.1368,  0.1185, -0.3758, -0.1842]],
       dtype=torch.float64)
	q_value: tensor([[-4.1137]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3033995904128244, distance: 1.3064580925621654 entropy 0.03264415264129639
epoch: 31, step: 68
	action: tensor([[ 0.0266,  0.2110,  0.0825, -0.5011,  0.3512, -0.1162, -0.0906]],
       dtype=torch.float64)
	q_value: tensor([[-2.9530]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.18613782193977302, distance: 1.0323622760577524 entropy 0.03264415264129639
epoch: 31, step: 69
	action: tensor([[ 0.0242, -0.0282, -0.7500, -0.3563, -0.0976, -0.5041, -0.1552]],
       dtype=torch.float64)
	q_value: tensor([[-3.0828]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2951657342927403, distance: 0.9607274462750414 entropy 0.03264415264129639
epoch: 31, step: 70
	action: tensor([[ 0.2622, -0.1109,  0.1218,  0.1637,  0.0862, -0.1877, -0.5574]],
       dtype=torch.float64)
	q_value: tensor([[-4.3918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.46368803668386493, distance: 0.838041326540425 entropy 0.03264415264129639
epoch: 31, step: 71
	action: tensor([[ 0.3083,  0.0589, -0.2367,  0.0213,  0.2374,  0.3497, -0.3865]],
       dtype=torch.float64)
	q_value: tensor([[-2.9589]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6609777729370359, distance: 0.6663014824094424 entropy 0.03264415264129639
epoch: 31, step: 72
	action: tensor([[ 0.4124,  0.2434, -0.6759,  0.0047, -0.0416,  0.1787, -0.0897]],
       dtype=torch.float64)
	q_value: tensor([[-2.7610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7750984662386963, distance: 0.5426913531132173 entropy 0.03264415264129639
epoch: 31, step: 73
	action: tensor([[ 0.3493, -0.3407,  0.2246, -0.2605,  0.4358, -0.2761, -0.0297]],
       dtype=torch.float64)
	q_value: tensor([[-3.8963]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.0015958846519816827, distance: 1.145257010697345 entropy 0.03264415264129639
epoch: 31, step: 74
	action: tensor([[ 0.4789,  0.0274, -0.0609, -0.1203, -0.2188,  0.5825, -0.2471]],
       dtype=torch.float64)
	q_value: tensor([[-3.0486]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7318358314940403, distance: 0.5925937506983577 entropy 0.03264415264129639
epoch: 31, step: 75
	action: tensor([[-0.2054,  0.0464, -0.1336, -0.3545,  0.1734,  0.1147,  0.0439]],
       dtype=torch.float64)
	q_value: tensor([[-3.7924]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.009585239359997666, distance: 1.1388466415035088 entropy 0.03264415264129639
epoch: 31, step: 76
	action: tensor([[ 0.3932, -0.1757, -0.3974,  0.0431,  0.3170,  0.1720, -0.0858]],
       dtype=torch.float64)
	q_value: tensor([[-2.9418]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5052236909381853, distance: 0.8049356113205794 entropy 0.03264415264129639
epoch: 31, step: 77
	action: tensor([[ 0.4264,  0.1129,  0.1272,  0.0487,  0.2583, -0.3743, -0.2928]],
       dtype=torch.float64)
	q_value: tensor([[-2.7680]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6281843148371806, distance: 0.6977832603307488 entropy 0.03264415264129639
epoch: 31, step: 78
	action: tensor([[-0.4032,  0.2829,  0.0701, -0.2352,  0.0763,  0.0403,  0.3604]],
       dtype=torch.float64)
	q_value: tensor([[-3.2389]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07596255305741995, distance: 1.1870124433415623 entropy 0.03264415264129639
epoch: 31, step: 79
	action: tensor([[-0.1306, -0.1364, -0.2039, -0.1655, -0.0190, -0.2230, -0.3077]],
       dtype=torch.float64)
	q_value: tensor([[-3.7269]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07546730445039618, distance: 1.1867392303382935 entropy 0.03264415264129639
epoch: 31, step: 80
	action: tensor([[ 0.2372,  0.7244, -0.4286, -0.6245,  0.4019, -0.1348, -0.1039]],
       dtype=torch.float64)
	q_value: tensor([[-2.7321]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7855985055284671, distance: 0.5298715445185808 entropy 0.03264415264129639
epoch: 31, step: 81
	action: tensor([[ 0.4354,  0.3072,  0.0328, -0.2082,  0.0888,  0.2779, -0.1987]],
       dtype=torch.float64)
	q_value: tensor([[-5.1756]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.778664699739531, distance: 0.5383714676681997 entropy 0.03264415264129639
epoch: 31, step: 82
	action: tensor([[ 0.2916,  0.2289, -0.2701, -0.1567, -0.1459,  0.0831, -0.2720]],
       dtype=torch.float64)
	q_value: tensor([[-3.2852]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6551253340896319, distance: 0.6720279557086141 entropy 0.03264415264129639
epoch: 31, step: 83
	action: tensor([[ 0.6130,  0.3869, -0.3206,  0.0104,  0.2135, -0.1565, -0.2144]],
       dtype=torch.float64)
	q_value: tensor([[-3.2496]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8987387331092351, distance: 0.3641483710551916 entropy 0.03264415264129639
epoch: 31, step: 84
	action: tensor([[ 0.2587,  0.1735, -0.4870,  0.3442,  0.1365, -0.2212, -0.2747]],
       dtype=torch.float64)
	q_value: tensor([[-3.9740]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6653164966663547, distance: 0.6620241714611331 entropy 0.03264415264129639
epoch: 31, step: 85
	action: tensor([[ 0.1828,  0.3327, -0.3642,  0.0824, -0.1870, -0.0159, -0.2503]],
       dtype=torch.float64)
	q_value: tensor([[-3.2725]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6788011072369067, distance: 0.6485503723729148 entropy 0.03264415264129639
epoch: 31, step: 86
	action: tensor([[ 0.0793, -0.0858, -0.1528, -0.4364,  0.1445, -0.4000, -0.1747]],
       dtype=torch.float64)
	q_value: tensor([[-3.3198]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04793603007021807, distance: 1.1165797780574043 entropy 0.03264415264129639
epoch: 31, step: 87
	action: tensor([[ 0.1868,  0.2009,  0.1108, -0.0757, -0.1433,  0.1553, -0.4773]],
       dtype=torch.float64)
	q_value: tensor([[-3.2339]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5791304234206618, distance: 0.7423871734481324 entropy 0.03264415264129639
epoch: 31, step: 88
	action: tensor([[ 2.3712e-01,  4.7542e-02, -8.1953e-02, -1.8517e-02,  5.2453e-01,
          3.9533e-04, -3.6837e-01]], dtype=torch.float64)
	q_value: tensor([[-3.2140]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5245007354111311, distance: 0.7890992272124661 entropy 0.03264415264129639
epoch: 31, step: 89
	action: tensor([[ 0.1555,  0.0833, -0.3301,  0.0459, -0.3866, -0.0008,  0.1057]],
       dtype=torch.float64)
	q_value: tensor([[-2.5457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5159124089219492, distance: 0.7961935745054064 entropy 0.03264415264129639
epoch: 31, step: 90
	action: tensor([[-0.0588,  0.0796,  0.2123,  0.0589,  0.0087,  0.1110, -0.1834]],
       dtype=torch.float64)
	q_value: tensor([[-3.2937]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36077393969772253, distance: 0.9149217023506445 entropy 0.03264415264129639
epoch: 31, step: 91
	action: tensor([[ 0.1279,  0.3259, -0.1610, -0.0648,  0.0049,  0.4935,  0.1290]],
       dtype=torch.float64)
	q_value: tensor([[-2.6927]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.662187159663699, distance: 0.6651119791383414 entropy 0.03264415264129639
epoch: 31, step: 92
	action: tensor([[-0.2482, -0.1447, -0.0946,  0.0757, -0.4249,  0.5815,  0.0448]],
       dtype=torch.float64)
	q_value: tensor([[-3.5457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.02002003487287196, distance: 1.132831434983719 entropy 0.03264415264129639
epoch: 31, step: 93
	action: tensor([[ 0.3273,  0.2927, -0.0804, -0.1657,  0.2950, -0.0999, -0.3808]],
       dtype=torch.float64)
	q_value: tensor([[-3.5329]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6531259250973469, distance: 0.6739731796285164 entropy 0.03264415264129639
epoch: 31, step: 94
	action: tensor([[ 0.2605, -0.0723, -0.3205, -0.4132,  0.0966,  0.2302, -0.2257]],
       dtype=torch.float64)
	q_value: tensor([[-3.0353]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3450257605537661, distance: 0.9261232835760134 entropy 0.03264415264129639
epoch: 31, step: 95
	action: tensor([[ 0.3703,  0.5164,  0.1786,  0.4784, -0.0981,  0.3064,  0.2276]],
       dtype=torch.float64)
	q_value: tensor([[-3.0557]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.86955526119783, distance: 0.41330435075110034 entropy 0.03264415264129639
epoch: 31, step: 96
	action: tensor([[-0.0643, -0.0385, -0.4039, -0.3133, -0.0722,  0.1328,  0.4768]],
       dtype=torch.float64)
	q_value: tensor([[-3.9973]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11590587827646481, distance: 1.0759843255291077 entropy 0.03264415264129639
epoch: 31, step: 97
	action: tensor([[ 0.0400,  0.1707,  0.2839, -0.4564, -0.1573,  0.3473, -0.1423]],
       dtype=torch.float64)
	q_value: tensor([[-3.9318]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2644548488535331, distance: 0.9814345931123569 entropy 0.03264415264129639
epoch: 31, step: 98
	action: tensor([[ 0.2473,  0.1843, -0.1010, -0.0884,  0.2665,  0.1909, -0.7332]],
       dtype=torch.float64)
	q_value: tensor([[-3.6250]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.614361419674573, distance: 0.710635552309654 entropy 0.03264415264129639
epoch: 31, step: 99
	action: tensor([[ 0.3653,  0.1636, -0.3234, -0.5416, -0.2277,  0.5300,  0.0382]],
       dtype=torch.float64)
	q_value: tensor([[-3.1444]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6017810391761922, distance: 0.7221337795212465 entropy 0.03264415264129639
epoch: 31, step: 100
	action: tensor([[-0.2032,  0.1072, -0.2387,  0.1532,  0.0976, -0.0576, -0.3222]],
       dtype=torch.float64)
	q_value: tensor([[-4.6265]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.19409096838150364, distance: 1.0273057162803005 entropy 0.03264415264129639
epoch: 31, step: 101
	action: tensor([[ 0.1637, -0.1977, -0.0028,  0.5397,  0.0194,  0.1369, -0.2179]],
       dtype=torch.float64)
	q_value: tensor([[-2.7327]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5806532136779765, distance: 0.7410429037910674 entropy 0.03264415264129639
epoch: 31, step: 102
	action: tensor([[ 0.5832,  0.1652, -0.5597, -0.3948, -0.0028, -0.0868,  0.0110]],
       dtype=torch.float64)
	q_value: tensor([[-2.7642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6703521396660057, distance: 0.6570248925156105 entropy 0.03264415264129639
epoch: 31, step: 103
	action: tensor([[ 0.3838, -0.1628,  0.0444,  0.0400, -0.3411, -0.2571,  0.0379]],
       dtype=torch.float64)
	q_value: tensor([[-4.4556]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4500172277856953, distance: 0.8486551208940225 entropy 0.03264415264129639
epoch: 31, step: 104
	action: tensor([[ 0.2521,  0.7626, -0.3527, -0.0481,  0.1865, -0.2855, -0.2741]],
       dtype=torch.float64)
	q_value: tensor([[-3.1690]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8240677604278014, distance: 0.47998692682371225 entropy 0.03264415264129639
epoch: 31, step: 105
	action: tensor([[0.3814, 0.1238, 0.0967, 0.3546, 0.1750, 0.5879, 0.0670]],
       dtype=torch.float64)
	q_value: tensor([[-4.5168]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9015014415875754, distance: 0.3591464933917474 entropy 0.03264415264129639
epoch: 31, step: 106
	action: tensor([[ 0.2973, -0.0615, -0.4174, -0.4980,  0.0299,  0.3124, -0.4969]],
       dtype=torch.float64)
	q_value: tensor([[-3.3081]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4142341246208585, distance: 0.8758278018182939 entropy 0.03264415264129639
epoch: 31, step: 107
	action: tensor([[ 0.2394, -0.0010, -0.2658,  0.1142, -0.3013, -0.0519,  0.0504]],
       dtype=torch.float64)
	q_value: tensor([[-3.5362]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5517849626999989, distance: 0.7661254767732811 entropy 0.03264415264129639
epoch: 31, step: 108
	action: tensor([[ 0.2921,  0.0683, -0.5204, -0.0964, -0.1444,  0.0543, -0.5821]],
       dtype=torch.float64)
	q_value: tensor([[-3.0059]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5896334211166566, distance: 0.7330653382500653 entropy 0.03264415264129639
epoch: 31, step: 109
	action: tensor([[-0.1374, -0.0351, -0.5124,  0.0826,  0.0048,  0.0613, -0.5854]],
       dtype=torch.float64)
	q_value: tensor([[-3.3689]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14412114127202325, distance: 1.0586754512918144 entropy 0.03264415264129639
epoch: 31, step: 110
	action: tensor([[-0.2630,  0.2092, -0.6345, -0.0388,  0.1007, -0.1816, -0.1529]],
       dtype=torch.float64)
	q_value: tensor([[-3.0073]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16395340079475818, distance: 1.046337849777556 entropy 0.03264415264129639
epoch: 31, step: 111
	action: tensor([[ 0.1411,  0.0621,  0.0266, -0.0577,  0.1387,  0.0573, -0.2830]],
       dtype=torch.float64)
	q_value: tensor([[-3.5779]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.45315684131647516, distance: 0.8462293516932968 entropy 0.03264415264129639
epoch: 31, step: 112
	action: tensor([[ 0.4973,  0.1405, -0.3943, -0.1981, -0.2637,  0.0277, -0.4456]],
       dtype=torch.float64)
	q_value: tensor([[-2.5439]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7000436930853908, distance: 0.6267375163215205 entropy 0.03264415264129639
epoch: 31, step: 113
	action: tensor([[ 0.2807,  0.7668, -0.2556,  0.1373,  0.2793,  0.0446, -0.4519]],
       dtype=torch.float64)
	q_value: tensor([[-3.7795]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7790392018812478, distance: 0.5379158091579895 entropy 0.03264415264129639
epoch: 31, step: 114
	action: tensor([[ 0.3344,  0.2359,  0.1266, -0.2501, -0.0324,  0.2702, -0.1735]],
       dtype=torch.float64)
	q_value: tensor([[-4.1112]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6682857815795676, distance: 0.6590809164435789 entropy 0.03264415264129639
epoch: 31, step: 115
	action: tensor([[-0.0433,  0.3609, -0.0988,  0.0503,  0.1167, -0.3084,  0.2902]],
       dtype=torch.float64)
	q_value: tensor([[-3.2614]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4452045492621187, distance: 0.8523601534279055 entropy 0.03264415264129639
epoch: 31, step: 116
	action: tensor([[ 0.5848, -0.1874, -0.7874,  0.1633, -0.0140,  0.0750, -0.2239]],
       dtype=torch.float64)
	q_value: tensor([[-3.4337]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5845528861010209, distance: 0.7375892295305178 entropy 0.03264415264129639
epoch: 31, step: 117
	action: tensor([[ 0.1339, -0.5670, -0.3569, -0.2431,  0.2853, -0.1690, -0.1482]],
       dtype=torch.float64)
	q_value: tensor([[-3.6272]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.22667380903408008, distance: 1.2674219948408272 entropy 0.03264415264129639
epoch: 31, step: 118
	action: tensor([[ 0.1464, -0.6144, -0.2982, -0.2188,  0.0745, -0.1021, -0.2810]],
       dtype=torch.float64)
	q_value: tensor([[-2.7510]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2439360288543393, distance: 1.2763086618105357 entropy 0.03264415264129639
epoch: 31, step: 119
	action: tensor([[-0.4861, -0.0936,  0.2299,  0.2159,  0.1113, -0.3688, -0.5253]],
       dtype=torch.float64)
	q_value: tensor([[-2.7379]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.31744845479093997, distance: 1.3134801366938067 entropy 0.03264415264129639
epoch: 31, step: 120
	action: tensor([[ 0.3760,  0.5557, -0.0380,  0.0129, -0.1013, -0.0010, -0.3197]],
       dtype=torch.float64)
	q_value: tensor([[-3.0116]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8372650174694085, distance: 0.4616333264669511 entropy 0.03264415264129639
epoch: 31, step: 121
	action: tensor([[ 0.4385,  0.1459,  0.1159, -0.0957, -0.1764, -0.1673,  0.0551]],
       dtype=torch.float64)
	q_value: tensor([[-3.8192]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6480551703917481, distance: 0.6788815218326577 entropy 0.03264415264129639
epoch: 31, step: 122
	action: tensor([[-0.0402, -0.1175, -0.5745, -0.2960,  0.1114,  0.6205, -0.4419]],
       dtype=torch.float64)
	q_value: tensor([[-3.3380]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23119091556304494, distance: 1.003381169815598 entropy 0.03264415264129639
epoch: 31, step: 123
	action: tensor([[-0.1714,  0.6926,  0.0374, -0.1852,  0.1814,  0.3900, -0.1052]],
       dtype=torch.float64)
	q_value: tensor([[-3.8286]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5003897810213321, distance: 0.8088581200174916 entropy 0.03264415264129639
epoch: 31, step: 124
	action: tensor([[ 0.1122,  0.0622, -0.4129,  0.1354, -0.2196,  0.0657,  0.0627]],
       dtype=torch.float64)
	q_value: tensor([[-4.0356]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4758823715183551, distance: 0.828459108009716 entropy 0.03264415264129639
epoch: 31, step: 125
	action: tensor([[ 0.2591, -0.3212,  0.1272,  0.1571,  0.0124,  0.0620, -0.0297]],
       dtype=torch.float64)
	q_value: tensor([[-2.9086]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3624400738914373, distance: 0.9137285587628043 entropy 0.03264415264129639
epoch: 31, step: 126
	action: tensor([[ 0.5361, -0.2455, -0.4434, -0.1456, -0.4013, -0.2699,  0.0994]],
       dtype=torch.float64)
	q_value: tensor([[-2.5933]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36112202953529726, distance: 0.9146725586324447 entropy 0.03264415264129639
epoch: 31, step: 127
	action: tensor([[ 0.5745,  0.1035,  0.2267, -0.3028,  0.1680,  0.1177, -0.0629]],
       dtype=torch.float64)
	q_value: tensor([[-3.9322]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6458178624980973, distance: 0.6810359231032457 entropy 0.03264415264129639
LOSS epoch 31 actor 7.3930032181791505 critic 48.87856133736132 
epoch: 32, step: 0
	action: tensor([[ 0.0683,  0.6823, -0.2248,  0.6346,  0.2978,  0.3131, -0.1898]],
       dtype=torch.float64)
	q_value: tensor([[-2.8704]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 32, step: 1
	action: tensor([[ 0.1352, -0.2511, -0.4212, -0.3221,  0.3666,  0.1081, -0.5426]],
       dtype=torch.float64)
	q_value: tensor([[-10.7313]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.17014541328854904, distance: 1.042455902339311 entropy 0.03264415264129639
epoch: 32, step: 2
	action: tensor([[ 0.3009,  0.1490, -0.4908,  0.0491, -0.3759,  0.3010, -0.5539]],
       dtype=torch.float64)
	q_value: tensor([[-2.3998]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6550382120636448, distance: 0.6721128339928978 entropy 0.03264415264129639
epoch: 32, step: 3
	action: tensor([[-0.1005,  0.3042, -0.8369, -0.2917, -0.2239,  0.0351,  0.3524]],
       dtype=torch.float64)
	q_value: tensor([[-3.5260]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.47170502343356724, distance: 0.8317540683782642 entropy 0.03264415264129639
epoch: 32, step: 4
	action: tensor([[ 0.4837,  0.0916, -0.5523, -0.3263, -0.0646,  0.4296, -0.0875]],
       dtype=torch.float64)
	q_value: tensor([[-4.8312]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7130649417952961, distance: 0.6129830989860271 entropy 0.03264415264129639
epoch: 32, step: 5
	action: tensor([[-0.0535, -0.1489, -0.0083, -0.2480, -0.7289,  0.7248, -0.3486]],
       dtype=torch.float64)
	q_value: tensor([[-3.7147]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07664999101677117, distance: 1.0996130166238545 entropy 0.03264415264129639
epoch: 32, step: 6
	action: tensor([[ 0.2637,  0.4020, -0.3751, -0.1150,  0.0829,  0.1359,  0.2652]],
       dtype=torch.float64)
	q_value: tensor([[-4.3449]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7375307520144729, distance: 0.5862676177033066 entropy 0.03264415264129639
epoch: 32, step: 7
	action: tensor([[ 0.0271,  0.4784, -0.4087, -0.3552,  0.0713, -0.4718, -0.3309]],
       dtype=torch.float64)
	q_value: tensor([[-3.4121]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5579200929353483, distance: 0.7608640793700607 entropy 0.03264415264129639
epoch: 32, step: 8
	action: tensor([[ 0.0024,  0.0505, -0.1540,  0.1054, -0.2576, -0.2340,  0.1189]],
       dtype=torch.float64)
	q_value: tensor([[-3.8313]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3787158710903875, distance: 0.901990208241732 entropy 0.03264415264129639
epoch: 32, step: 9
	action: tensor([[ 0.3309, -0.0085, -0.2654, -0.3259, -0.2515, -0.3249,  0.0227]],
       dtype=torch.float64)
	q_value: tensor([[-2.6227]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41248972173276244, distance: 0.877130934061263 entropy 0.03264415264129639
epoch: 32, step: 10
	action: tensor([[ 0.2421, -0.0836, -0.4251,  0.0314, -0.1818,  0.0703, -0.3409]],
       dtype=torch.float64)
	q_value: tensor([[-3.2632]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.48672790326339577, distance: 0.8198426751565204 entropy 0.03264415264129639
epoch: 32, step: 11
	action: tensor([[ 0.0063,  0.2390, -0.0611,  0.4234,  0.2672, -0.0765, -0.5061]],
       dtype=torch.float64)
	q_value: tensor([[-2.4423]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5937281627098892, distance: 0.7293988131938027 entropy 0.03264415264129639
epoch: 32, step: 12
	action: tensor([[-0.1583, -0.0870, -0.1433,  0.1031, -0.1085, -0.1397, -0.5187]],
       dtype=torch.float64)
	q_value: tensor([[-2.6246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11141026438257773, distance: 1.0787165438828643 entropy 0.03264415264129639
epoch: 32, step: 13
	action: tensor([[ 0.7922,  0.0553,  0.2562,  0.4113, -0.1291,  0.5460, -0.4286]],
       dtype=torch.float64)
	q_value: tensor([[-2.4196]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.983137647779576, distance: 0.14859896055759095 entropy 0.03264415264129639
epoch: 32, step: 14
	action: tensor([[ 0.3673,  0.2745, -0.4216, -0.5233, -0.4071, -0.0973, -0.4281]],
       dtype=torch.float64)
	q_value: tensor([[-4.0885]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6695374445055182, distance: 0.6578362805930261 entropy 0.03264415264129639
epoch: 32, step: 15
	action: tensor([[ 0.1268, -0.0368, -0.1032, -0.1731,  0.2482,  0.1436,  0.0124]],
       dtype=torch.float64)
	q_value: tensor([[-4.2428]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36839422059901195, distance: 0.9094519132121361 entropy 0.03264415264129639
epoch: 32, step: 16
	action: tensor([[ 0.4271, -0.2383, -0.5774, -0.0042,  0.1665,  0.4488, -0.0720]],
       dtype=torch.float64)
	q_value: tensor([[-2.1871]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5442741451914271, distance: 0.7725178575497031 entropy 0.03264415264129639
epoch: 32, step: 17
	action: tensor([[ 0.3635,  0.5559, -0.2858, -0.2355, -0.1157,  0.3081,  0.4881]],
       dtype=torch.float64)
	q_value: tensor([[-2.7905]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8575293123613988, distance: 0.43193605108614863 entropy 0.03264415264129639
epoch: 32, step: 18
	action: tensor([[ 0.3863,  0.5325, -0.1203,  0.1513, -0.1841,  0.3577,  0.0270]],
       dtype=torch.float64)
	q_value: tensor([[-4.4337]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8682134616527282, distance: 0.41542460778209295 entropy 0.03264415264129639
epoch: 32, step: 19
	action: tensor([[ 0.4192, -0.1109, -0.3261, -0.5497, -0.2540,  0.1721,  0.2540]],
       dtype=torch.float64)
	q_value: tensor([[-3.5437]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3420244122747348, distance: 0.9282427879062932 entropy 0.03264415264129639
epoch: 32, step: 20
	action: tensor([[ 0.4080,  0.3466, -0.0593,  0.1803, -0.1973, -0.2272, -0.6922]],
       dtype=torch.float64)
	q_value: tensor([[-3.8369]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8482330006278982, distance: 0.4458054434747327 entropy 0.03264415264129639
epoch: 32, step: 21
	action: tensor([[ 0.4044, -0.0604, -0.1122,  0.1493, -0.0624, -0.4098,  0.1829]],
       dtype=torch.float64)
	q_value: tensor([[-3.6918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5747562690255837, distance: 0.7462350664915082 entropy 0.03264415264129639
epoch: 32, step: 22
	action: tensor([[ 0.5510, -0.3343, -0.3107,  0.1452,  0.1542,  0.1227, -0.1099]],
       dtype=torch.float64)
	q_value: tensor([[-2.9386]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.52032820391692, distance: 0.7925538590953881 entropy 0.03264415264129639
epoch: 32, step: 23
	action: tensor([[ 0.3600, -0.0832, -0.5116, -0.1885, -0.1892,  0.0889, -0.3738]],
       dtype=torch.float64)
	q_value: tensor([[-2.5350]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5065627358873637, distance: 0.8038456488529131 entropy 0.03264415264129639
epoch: 32, step: 24
	action: tensor([[ 0.0239,  0.1029,  0.1155, -0.1088, -0.2331,  0.1899, -0.1443]],
       dtype=torch.float64)
	q_value: tensor([[-2.8660]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.401941483169453, distance: 0.8849699682107907 entropy 0.03264415264129639
epoch: 32, step: 25
	action: tensor([[ 0.2401,  0.0671, -0.4087,  0.4486, -0.2933,  0.5464, -0.5480]],
       dtype=torch.float64)
	q_value: tensor([[-2.6591]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6355976852566187, distance: 0.6907919322092676 entropy 0.03264415264129639
epoch: 32, step: 26
	action: tensor([[-0.1205, -0.1080, -0.4127, -0.1356,  0.5450,  0.3381, -0.4688]],
       dtype=torch.float64)
	q_value: tensor([[-3.3668]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1935472536603462, distance: 1.0276521989732421 entropy 0.03264415264129639
epoch: 32, step: 27
	action: tensor([[ 0.1999,  0.3428, -0.3636, -0.2133, -0.0789, -0.3298, -0.1099]],
       dtype=torch.float64)
	q_value: tensor([[-2.5712]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6492577115430574, distance: 0.6777207127289109 entropy 0.03264415264129639
epoch: 32, step: 28
	action: tensor([[-0.4402, -0.1346, -0.3545, -0.3387, -0.1302, -0.1975,  0.0677]],
       dtype=torch.float64)
	q_value: tensor([[-3.3572]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.3298045886231531, distance: 1.3196252197000145 entropy 0.03264415264129639
epoch: 32, step: 29
	action: tensor([[ 0.1203, -0.0344, -0.3300,  0.0357, -0.3739, -0.4459, -0.3971]],
       dtype=torch.float64)
	q_value: tensor([[-3.2608]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3917830870558632, distance: 0.892454203770547 entropy 0.03264415264129639
epoch: 32, step: 30
	action: tensor([[-0.3494,  0.2434, -0.2287, -0.0290,  0.2850, -0.0816,  0.0927]],
       dtype=torch.float64)
	q_value: tensor([[-3.1091]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06855997889164267, distance: 1.104419689031081 entropy 0.03264415264129639
epoch: 32, step: 31
	action: tensor([[ 0.3180,  0.0326, -0.5291, -0.0117,  0.2798,  0.2548, -0.2342]],
       dtype=torch.float64)
	q_value: tensor([[-2.8009]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6113358943362655, distance: 0.713417749680178 entropy 0.03264415264129639
epoch: 32, step: 32
	action: tensor([[ 0.1840,  0.0872, -0.4501,  0.1015,  0.0630, -0.3168, -0.2273]],
       dtype=torch.float64)
	q_value: tensor([[-2.5636]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5283397187764444, distance: 0.7859073421284568 entropy 0.03264415264129639
epoch: 32, step: 33
	action: tensor([[-0.0602,  0.3930, -0.0871, -0.2153, -0.0934,  0.0391,  0.0556]],
       dtype=torch.float64)
	q_value: tensor([[-2.7789]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4488462165728698, distance: 0.8495581093955413 entropy 0.03264415264129639
epoch: 32, step: 34
	action: tensor([[ 0.0551,  0.3236, -0.0562,  0.1480, -0.1050,  0.0902, -0.4312]],
       dtype=torch.float64)
	q_value: tensor([[-2.9980]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.618033376132122, distance: 0.7072442109602425 entropy 0.03264415264129639
epoch: 32, step: 35
	action: tensor([[ 0.0841,  0.0027, -0.9502,  0.4571,  0.0142, -0.2864, -0.2527]],
       dtype=torch.float64)
	q_value: tensor([[-2.7105]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35409485817922215, distance: 0.9196891526236195 entropy 0.03264415264129639
epoch: 32, step: 36
	action: tensor([[ 0.1939, -0.0217, -0.1406,  0.1050,  0.4710, -0.0393,  0.2554]],
       dtype=torch.float64)
	q_value: tensor([[-3.4563]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4911689941773931, distance: 0.8162881218563729 entropy 0.03264415264129639
epoch: 32, step: 37
	action: tensor([[ 0.3172,  0.1183, -0.0868, -0.0282, -0.0566,  0.0376, -0.2859]],
       dtype=torch.float64)
	q_value: tensor([[-2.5940]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6361395865201718, distance: 0.6902781041137669 entropy 0.03264415264129639
epoch: 32, step: 38
	action: tensor([[ 0.0799, -0.1796, -0.0618,  0.1721, -0.1148,  0.3938,  0.4468]],
       dtype=torch.float64)
	q_value: tensor([[-2.4951]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.41851418479838276, distance: 0.8726221965382552 entropy 0.03264415264129639
epoch: 32, step: 39
	action: tensor([[ 0.2331,  0.0202, -0.2166,  0.1718, -0.1282,  0.2402, -0.1018]],
       dtype=torch.float64)
	q_value: tensor([[-2.7027]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6209873385028808, distance: 0.7045041447654299 entropy 0.03264415264129639
epoch: 32, step: 40
	action: tensor([[ 0.2692,  0.2521, -0.0213, -0.0848,  0.0361, -0.2354, -0.0870]],
       dtype=torch.float64)
	q_value: tensor([[-2.3324]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.637585672325973, distance: 0.6889050567968968 entropy 0.03264415264129639
epoch: 32, step: 41
	action: tensor([[-0.0251,  0.2386, -0.4501, -0.0677,  0.2084, -0.2736,  0.0068]],
       dtype=torch.float64)
	q_value: tensor([[-2.7334]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4235982447414953, distance: 0.8687990564792211 entropy 0.03264415264129639
epoch: 32, step: 42
	action: tensor([[ 0.0541,  0.4397, -0.1130,  0.3033, -0.1196, -0.0029, -0.4675]],
       dtype=torch.float64)
	q_value: tensor([[-2.9340]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.65302086867143, distance: 0.6740752337429974 entropy 0.03264415264129639
epoch: 32, step: 43
	action: tensor([[ 0.7261,  0.1399, -0.2473,  0.0834,  0.0254,  0.2860, -0.1644]],
       dtype=torch.float64)
	q_value: tensor([[-3.0050]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9018030283556315, distance: 0.3585962474337957 entropy 0.03264415264129639
epoch: 32, step: 44
	action: tensor([[ 0.5346, -0.5455, -0.5274, -0.3161, -0.3493,  0.1616, -0.0786]],
       dtype=torch.float64)
	q_value: tensor([[-3.2985]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04586911025583651, distance: 1.117791161799813 entropy 0.03264415264129639
epoch: 32, step: 45
	action: tensor([[ 0.0355,  0.4795, -0.1998, -0.2947,  0.1380,  0.0185, -0.5438]],
       dtype=torch.float64)
	q_value: tensor([[-3.4191]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5755524415195554, distance: 0.745536161198359 entropy 0.03264415264129639
epoch: 32, step: 46
	action: tensor([[ 0.1458,  0.0829, -0.1186, -0.1994, -0.0830,  0.0584, -0.1706]],
       dtype=torch.float64)
	q_value: tensor([[-3.1716]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44958109790256884, distance: 0.8489915409879739 entropy 0.03264415264129639
epoch: 32, step: 47
	action: tensor([[-0.0183,  0.0185, -0.2294,  0.1032,  0.3186, -0.2653, -0.0425]],
       dtype=torch.float64)
	q_value: tensor([[-2.4259]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28403503262096064, distance: 0.9682836074521993 entropy 0.03264415264129639
epoch: 32, step: 48
	action: tensor([[ 0.3419, -0.1036, -0.2866,  0.2473,  0.5746,  0.2989, -0.2231]],
       dtype=torch.float64)
	q_value: tensor([[-2.4454]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6715450390913269, distance: 0.6558350244005642 entropy 0.03264415264129639
epoch: 32, step: 49
	action: tensor([[ 0.0754,  0.2308, -0.1742, -0.0160, -0.0649,  0.0769,  0.1040]],
       dtype=torch.float64)
	q_value: tensor([[-2.5035]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5407001594023215, distance: 0.7755411398395331 entropy 0.03264415264129639
epoch: 32, step: 50
	action: tensor([[ 0.1881,  0.1587, -0.0348,  0.3157, -0.3104,  0.4412,  0.2051]],
       dtype=torch.float64)
	q_value: tensor([[-2.5663]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7217377258771454, distance: 0.603648128234349 entropy 0.03264415264129639
epoch: 32, step: 51
	action: tensor([[ 0.2124,  0.0665, -0.4883, -0.4574,  0.0233,  0.0964, -0.2820]],
       dtype=torch.float64)
	q_value: tensor([[-3.0724]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.47491407636116134, distance: 0.8292240343662173 entropy 0.03264415264129639
epoch: 32, step: 52
	action: tensor([[ 0.2738, -0.0246,  0.0479, -0.0616,  0.0796, -0.0041, -0.6593]],
       dtype=torch.float64)
	q_value: tensor([[-3.1563]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5076158532401285, distance: 0.8029873878037965 entropy 0.03264415264129639
epoch: 32, step: 53
	action: tensor([[ 0.1304,  0.1378, -0.0417, -0.0457, -0.0657,  0.1405, -0.3239]],
       dtype=torch.float64)
	q_value: tensor([[-2.5583]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5481020353535506, distance: 0.7692666163759925 entropy 0.03264415264129639
epoch: 32, step: 54
	action: tensor([[ 0.0096,  0.2582, -0.5408,  0.1071, -0.2902,  0.1146, -0.3223]],
       dtype=torch.float64)
	q_value: tensor([[-2.4156]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4873127936124415, distance: 0.8193754232291254 entropy 0.03264415264129639
epoch: 32, step: 55
	action: tensor([[-0.4351,  0.3822,  0.0185, -0.0104, -0.0752, -0.0206,  0.2805]],
       dtype=torch.float64)
	q_value: tensor([[-3.0912]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.06953621961504064, distance: 1.1038407670879684 entropy 0.03264415264129639
epoch: 32, step: 56
	action: tensor([[-0.1622, -0.1315,  0.1202,  0.0066, -0.0583,  0.3847,  0.1254]],
       dtype=torch.float64)
	q_value: tensor([[-3.2147]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13178484981261407, distance: 1.066277814684421 entropy 0.03264415264129639
epoch: 32, step: 57
	action: tensor([[-0.0336,  0.4970,  0.0590, -0.8016,  0.0837,  0.3322, -0.4350]],
       dtype=torch.float64)
	q_value: tensor([[-2.4950]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33353980047005927, distance: 0.9342084746734003 entropy 0.03264415264129639
epoch: 32, step: 58
	action: tensor([[ 0.2108, -0.0882,  0.1096, -0.0278, -0.1343,  0.6833, -0.4255]],
       dtype=torch.float64)
	q_value: tensor([[-4.0491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5570429156073856, distance: 0.7616185605922303 entropy 0.03264415264129639
epoch: 32, step: 59
	action: tensor([[ 0.7415,  0.4314, -1.1274,  0.4122, -0.0550, -0.1889, -0.0872]],
       dtype=torch.float64)
	q_value: tensor([[-2.9829]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9345098780774062, distance: 0.2928495713958497 entropy 0.03264415264129639
epoch: 32, step: 60
	action: tensor([[ 0.2097, -0.0153, -0.6328, -0.1640, -0.0860,  0.0918, -0.8421]],
       dtype=torch.float64)
	q_value: tensor([[-5.3918]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4776518774780022, distance: 0.8270594195860401 entropy 0.03264415264129639
epoch: 32, step: 61
	action: tensor([[ 0.2758,  0.2860, -0.2369, -0.3899,  0.1812,  0.2255, -0.1368]],
       dtype=torch.float64)
	q_value: tensor([[-3.3147]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6730001166196616, distance: 0.6543807153465954 entropy 0.03264415264129639
epoch: 32, step: 62
	action: tensor([[ 0.1120,  0.1790, -0.0468, -0.2426, -0.3540,  0.4077, -0.1160]],
       dtype=torch.float64)
	q_value: tensor([[-2.9671]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.502859053658326, distance: 0.8068567946676602 entropy 0.03264415264129639
epoch: 32, step: 63
	action: tensor([[ 0.0910,  0.1982, -0.0173, -0.4838,  0.2541,  0.0999, -0.5003]],
       dtype=torch.float64)
	q_value: tensor([[-3.3258]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37360543917492817, distance: 0.9056923141115428 entropy 0.03264415264129639
epoch: 32, step: 64
	action: tensor([[ 0.0194, -0.2898, -0.0758,  0.6369, -0.1550,  0.0035,  0.2499]],
       dtype=torch.float64)
	q_value: tensor([[-2.6710]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42930473471710395, distance: 0.8644877189777594 entropy 0.03264415264129639
epoch: 32, step: 65
	action: tensor([[-0.0649, -0.4598,  0.0360,  0.0200,  0.0103,  0.2403, -0.1498]],
       dtype=torch.float64)
	q_value: tensor([[-2.5724]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04403719764745184, distance: 1.1692696561613016 entropy 0.03264415264129639
epoch: 32, step: 66
	action: tensor([[ 0.6012,  0.2211, -0.3087,  0.0035,  0.3975, -0.5238,  0.1826]],
       dtype=torch.float64)
	q_value: tensor([[-2.1003]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7559304173850341, distance: 0.5653449569723954 entropy 0.03264415264129639
epoch: 32, step: 67
	action: tensor([[ 0.2351, -0.0391, -0.0941, -0.2285,  0.5578, -0.2491,  0.0150]],
       dtype=torch.float64)
	q_value: tensor([[-3.9749]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28942206232723555, distance: 0.9646339725520888 entropy 0.03264415264129639
epoch: 32, step: 68
	action: tensor([[-0.1297, -0.3744,  0.0136,  0.0953, -0.1413,  0.0694, -0.3957]],
       dtype=torch.float64)
	q_value: tensor([[-2.5911]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.07331684153889984, distance: 1.185552157655344 entropy 0.03264415264129639
epoch: 32, step: 69
	action: tensor([[ 0.1033, -0.0969, -0.4923,  0.1600,  0.1060,  0.0460, -0.0201]],
       dtype=torch.float64)
	q_value: tensor([[-2.1230]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.35697719525467175, distance: 0.9176348121858199 entropy 0.03264415264129639
epoch: 32, step: 70
	action: tensor([[ 0.1487,  0.1152, -0.0957, -0.0535,  0.2747,  0.1585,  0.0333]],
       dtype=torch.float64)
	q_value: tensor([[-2.2905]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5302826412572219, distance: 0.7842869674018171 entropy 0.03264415264129639
epoch: 32, step: 71
	action: tensor([[ 0.1942,  0.5192, -0.0217,  0.1964,  0.3259,  0.5429, -0.5816]],
       dtype=torch.float64)
	q_value: tensor([[-2.2658]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7997960147102201, distance: 0.5120272238061171 entropy 0.03264415264129639
epoch: 32, step: 72
	action: tensor([[ 0.4007,  0.3341, -0.2943, -0.1988,  0.1242,  0.1981, -0.0304]],
       dtype=torch.float64)
	q_value: tensor([[-3.3597]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7981269905925245, distance: 0.5141570817135562 entropy 0.03264415264129639
epoch: 32, step: 73
	action: tensor([[ 4.4929e-01,  3.7010e-01, -4.7983e-01, -8.5761e-02,  5.5377e-02,
          2.4179e-01,  3.3408e-04]], dtype=torch.float64)
	q_value: tensor([[-3.0774]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.859185399689731, distance: 0.4294182885060853 entropy 0.03264415264129639
epoch: 32, step: 74
	action: tensor([[ 0.2521, -0.4295, -0.4921, -0.2667,  0.3856,  0.0837, -0.1121]],
       dtype=torch.float64)
	q_value: tensor([[-3.4874]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.08097267110443329, distance: 1.0970360677737656 entropy 0.03264415264129639
epoch: 32, step: 75
	action: tensor([[-0.1044, -0.1691, -0.0863, -0.0210,  0.1524,  0.1509, -0.3500]],
       dtype=torch.float64)
	q_value: tensor([[-2.4744]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11679315331961837, distance: 1.0754442621648581 entropy 0.03264415264129639
epoch: 32, step: 76
	action: tensor([[ 0.0957, -0.2829, -0.6723, -0.0711,  0.4071,  0.3747,  0.0948]],
       dtype=torch.float64)
	q_value: tensor([[-2.0262]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24813377640514256, distance: 0.9922634191119897 entropy 0.03264415264129639
epoch: 32, step: 77
	action: tensor([[-0.0037,  0.2028, -0.3239,  0.2959, -0.2542, -0.0924, -0.3286]],
       dtype=torch.float64)
	q_value: tensor([[-2.9580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5005420606100914, distance: 0.8087348419456227 entropy 0.03264415264129639
epoch: 32, step: 78
	action: tensor([[-0.2906, -0.0015,  0.2954,  0.2129, -0.4606,  0.3146, -0.0179]],
       dtype=torch.float64)
	q_value: tensor([[-2.7492]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2041811752371231, distance: 1.0208543818604847 entropy 0.03264415264129639
epoch: 32, step: 79
	action: tensor([[ 0.6458, -0.3376, -0.4383, -0.2790,  0.0521,  0.0051, -0.7095]],
       dtype=torch.float64)
	q_value: tensor([[-2.9418]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2898856584573912, distance: 0.9643192473702548 entropy 0.03264415264129639
epoch: 32, step: 80
	action: tensor([[ 0.5431,  0.0096, -0.5555,  0.3238, -0.2804, -0.2480, -0.4557]],
       dtype=torch.float64)
	q_value: tensor([[-3.2392]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7632537608087195, distance: 0.5567987321041175 entropy 0.03264415264129639
epoch: 32, step: 81
	action: tensor([[ 0.1692,  0.5533, -0.4545, -0.0844,  0.2011, -0.0730,  0.2230]],
       dtype=torch.float64)
	q_value: tensor([[-3.5210]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7530659583349726, distance: 0.5686527917746732 entropy 0.03264415264129639
epoch: 32, step: 82
	action: tensor([[ 0.0795,  0.1643,  0.0469, -0.1137, -0.1147,  0.0279,  0.1535]],
       dtype=torch.float64)
	q_value: tensor([[-3.6692]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4615688633381253, distance: 0.8396954048118612 entropy 0.03264415264129639
epoch: 32, step: 83
	action: tensor([[ 0.0333,  0.2190, -0.2772, -0.0948, -0.3999,  0.1444, -0.0062]],
       dtype=torch.float64)
	q_value: tensor([[-2.6094]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4802440118480428, distance: 0.8250047405615831 entropy 0.03264415264129639
epoch: 32, step: 84
	action: tensor([[-0.2017, -0.1653, -0.3026,  0.0604, -0.1058,  0.1538, -0.2988]],
       dtype=torch.float64)
	q_value: tensor([[-3.1181]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.01787272241515192, distance: 1.1340718746163507 entropy 0.03264415264129639
epoch: 32, step: 85
	action: tensor([[ 0.2604, -0.1757, -0.3889, -0.3349, -0.3042, -0.3670, -0.2340]],
       dtype=torch.float64)
	q_value: tensor([[-2.3092]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26275923648135, distance: 0.9825651659134457 entropy 0.03264415264129639
epoch: 32, step: 86
	action: tensor([[-0.1789, -0.1139, -0.0171, -0.0894, -0.2070,  0.2327, -0.1020]],
       dtype=torch.float64)
	q_value: tensor([[-3.2391]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.045960933918203994, distance: 1.1177373735046638 entropy 0.03264415264129639
epoch: 32, step: 87
	action: tensor([[ 0.5785,  0.0161, -0.1758, -0.1076,  0.0694,  0.7179,  0.1788]],
       dtype=torch.float64)
	q_value: tensor([[-2.3686]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.815946372905801, distance: 0.4909405258690885 entropy 0.03264415264129639
epoch: 32, step: 88
	action: tensor([[ 0.1822,  0.1483, -0.4038,  0.1032,  0.0723,  0.3653, -0.4105]],
       dtype=torch.float64)
	q_value: tensor([[-3.5465]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.637609624644272, distance: 0.6888822912153691 entropy 0.03264415264129639
epoch: 32, step: 89
	action: tensor([[ 0.1431,  0.2943, -0.0692, -0.1259,  0.5626, -0.1019,  0.0458]],
       dtype=torch.float64)
	q_value: tensor([[-2.5688]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.559328585618206, distance: 0.7596510334523575 entropy 0.03264415264129639
epoch: 32, step: 90
	action: tensor([[-0.0550, -0.0061, -0.3077,  0.0111, -0.1069,  0.2237, -0.2938]],
       dtype=torch.float64)
	q_value: tensor([[-2.7002]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29405920008933273, distance: 0.9614812836814259 entropy 0.03264415264129639
epoch: 32, step: 91
	action: tensor([[ 0.6689,  0.0354, -0.4565,  0.2766,  0.5360, -0.2591, -0.1448]],
       dtype=torch.float64)
	q_value: tensor([[-2.3368]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7842203800633021, distance: 0.531571765378367 entropy 0.03264415264129639
epoch: 32, step: 92
	action: tensor([[ 0.2966,  0.1845, -0.2518, -0.1178, -0.0030, -0.2927, -0.3792]],
       dtype=torch.float64)
	q_value: tensor([[-3.5543]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6170011338426509, distance: 0.7081992090880601 entropy 0.03264415264129639
epoch: 32, step: 93
	action: tensor([[ 0.3372,  0.3281, -0.5629,  0.0190, -0.0106,  0.3658, -0.2686]],
       dtype=torch.float64)
	q_value: tensor([[-2.8987]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7801046045736006, distance: 0.536617412678777 entropy 0.03264415264129639
epoch: 32, step: 94
	action: tensor([[ 0.1758, -0.4148,  0.3566,  0.0196,  0.2363, -0.2084, -0.0138]],
       dtype=torch.float64)
	q_value: tensor([[-3.3171]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03026573776412722, distance: 1.126893991728494 entropy 0.03264415264129639
epoch: 32, step: 95
	action: tensor([[ 0.3750, -0.3458, -0.2041,  0.1362, -0.0796,  0.1172, -0.2771]],
       dtype=torch.float64)
	q_value: tensor([[-2.3845]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.39972296691244447, distance: 0.8866098603170286 entropy 0.03264415264129639
epoch: 32, step: 96
	action: tensor([[ 0.5398, -0.3636,  0.2272, -0.3626,  0.1419, -0.2203, -0.0949]],
       dtype=torch.float64)
	q_value: tensor([[-2.3016]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.032142847280278275, distance: 1.125802802030784 entropy 0.03264415264129639
epoch: 32, step: 97
	action: tensor([[ 0.3251, -0.3940,  0.1174, -0.2573, -0.0298, -0.0323, -0.0420]],
       dtype=torch.float64)
	q_value: tensor([[-2.8965]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.03483594878744045, distance: 1.1242354151232004 entropy 0.03264415264129639
epoch: 32, step: 98
	action: tensor([[ 0.0058,  0.4189, -0.3040,  0.3526, -0.1664,  0.0491, -0.1652]],
       dtype=torch.float64)
	q_value: tensor([[-2.4243]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5784430182592317, distance: 0.7429931955946595 entropy 0.03264415264129639
epoch: 32, step: 99
	action: tensor([[ 0.3306,  0.2137, -0.1927, -0.0062, -0.2633, -0.2021, -0.1133]],
       dtype=torch.float64)
	q_value: tensor([[-2.9352]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7168431276889563, distance: 0.6089340320601275 entropy 0.03264415264129639
epoch: 32, step: 100
	action: tensor([[ 0.0683, -0.2670, -0.4330,  0.1864,  0.5495,  0.0806,  0.0250]],
       dtype=torch.float64)
	q_value: tensor([[-2.9590]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23970750641720673, distance: 0.9978081447670525 entropy 0.03264415264129639
epoch: 32, step: 101
	action: tensor([[-0.1436,  0.1896, -0.0324, -0.1284, -0.2307,  0.0986, -0.3552]],
       dtype=torch.float64)
	q_value: tensor([[-2.4370]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2669270315765504, distance: 0.9797838937816504 entropy 0.03264415264129639
epoch: 32, step: 102
	action: tensor([[ 0.1062,  0.8059,  0.2490, -0.1666,  0.1018, -0.1833, -0.5715]],
       dtype=torch.float64)
	q_value: tensor([[-2.7717]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6811772823655855, distance: 0.6461469861327278 entropy 0.03264415264129639
epoch: 32, step: 103
	action: tensor([[-0.0376, -0.2749,  0.1194,  0.1043, -0.1762,  0.1777,  0.1423]],
       dtype=torch.float64)
	q_value: tensor([[-3.8904]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.16695222822819078, distance: 1.0444596019310295 entropy 0.03264415264129639
epoch: 32, step: 104
	action: tensor([[-0.1075, -0.1521, -0.1242, -0.0657, -0.0894,  0.0559, -0.1900]],
       dtype=torch.float64)
	q_value: tensor([[-2.3038]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0757313985932132, distance: 1.1001598542683384 entropy 0.03264415264129639
epoch: 32, step: 105
	action: tensor([[-0.1243,  0.4355, -0.0966, -0.0278, -0.3746,  0.2138, -0.0536]],
       dtype=torch.float64)
	q_value: tensor([[-2.1735]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44339284789574274, distance: 0.8537507230790882 entropy 0.03264415264129639
epoch: 32, step: 106
	action: tensor([[ 0.1500, -0.1118, -0.0686, -0.1099, -0.1796,  0.1323, -0.3333]],
       dtype=torch.float64)
	q_value: tensor([[-3.4190]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3488532139589211, distance: 0.9234133392812963 entropy 0.03264415264129639
epoch: 32, step: 107
	action: tensor([[-0.3505,  0.5326, -0.0473,  0.3322, -0.2764,  0.1040, -0.1921]],
       dtype=torch.float64)
	q_value: tensor([[-2.3633]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3576736457796921, distance: 0.9171377377564418 entropy 0.03264415264129639
epoch: 32, step: 108
	action: tensor([[ 0.0459,  0.1256, -0.0175,  0.3114,  0.5970,  0.3388,  0.2252]],
       dtype=torch.float64)
	q_value: tensor([[-3.3199]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6424300219678827, distance: 0.6842853105730158 entropy 0.03264415264129639
epoch: 32, step: 109
	action: tensor([[ 0.1325,  0.0585, -0.5931, -0.1729, -0.3506,  0.1561, -0.0108]],
       dtype=torch.float64)
	q_value: tensor([[-2.6734]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4342955729885033, distance: 0.8606993638352752 entropy 0.03264415264129639
epoch: 32, step: 110
	action: tensor([[ 0.4996,  0.0677, -0.3810, -0.5351, -0.2368,  0.0347,  0.0981]],
       dtype=torch.float64)
	q_value: tensor([[-3.3285]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.528362146231826, distance: 0.7858886569517689 entropy 0.03264415264129639
epoch: 32, step: 111
	action: tensor([[ 0.3687,  0.5265, -0.3676, -0.3007,  0.0516,  0.0288, -0.3118]],
       dtype=torch.float64)
	q_value: tensor([[-4.0093]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8483240491751607, distance: 0.4456716988935774 entropy 0.03264415264129639
epoch: 32, step: 112
	action: tensor([[ 0.2894, -0.1168, -0.2366, -0.2568,  0.0326,  0.0643, -0.1875]],
       dtype=torch.float64)
	q_value: tensor([[-3.7366]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3771639758915889, distance: 0.9031160387336278 entropy 0.03264415264129639
epoch: 32, step: 113
	action: tensor([[ 0.3053,  0.2286,  0.2624, -0.1112, -0.0137,  0.1716, -0.0491]],
       dtype=torch.float64)
	q_value: tensor([[-2.3850]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.705004911206309, distance: 0.6215328481256012 entropy 0.03264415264129639
epoch: 32, step: 114
	action: tensor([[-0.1650,  0.0494, -0.0191, -0.1058,  0.2153, -0.0707, -0.4400]],
       dtype=torch.float64)
	q_value: tensor([[-2.7365]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11629569285788555, distance: 1.0757470879920696 entropy 0.03264415264129639
epoch: 32, step: 115
	action: tensor([[-0.3019,  0.0668, -0.2991, -0.3093, -0.2083,  0.6243, -0.1801]],
       dtype=torch.float64)
	q_value: tensor([[-2.2454]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.021503220739486073, distance: 1.131973848168387 entropy 0.03264415264129639
epoch: 32, step: 116
	action: tensor([[ 0.5351, -0.2003, -0.3719,  0.0805,  0.8490,  0.1937, -0.4104]],
       dtype=torch.float64)
	q_value: tensor([[-3.7278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6034715719360393, distance: 0.7205993357545133 entropy 0.03264415264129639
epoch: 32, step: 117
	action: tensor([[ 0.3111,  0.5110,  0.0647, -0.0865,  0.0048,  0.2402, -0.4275]],
       dtype=torch.float64)
	q_value: tensor([[-2.9889]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8178956308279095, distance: 0.48833390252419895 entropy 0.03264415264129639
epoch: 32, step: 118
	action: tensor([[-0.0835,  0.0996, -0.3450, -0.1095, -0.3664,  0.1635, -0.6359]],
       dtype=torch.float64)
	q_value: tensor([[-3.2989]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2781160419781299, distance: 0.9722778427032478 entropy 0.03264415264129639
epoch: 32, step: 119
	action: tensor([[ 0.2527,  0.2295, -0.2864, -0.1106, -0.3641,  0.0926,  0.0644]],
       dtype=torch.float64)
	q_value: tensor([[-3.2520]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6634578451791174, distance: 0.6638598887601009 entropy 0.03264415264129639
epoch: 32, step: 120
	action: tensor([[ 0.0735, -0.0533, -0.3353, -0.1355, -0.1511, -0.0111, -0.4685]],
       dtype=torch.float64)
	q_value: tensor([[-3.1891]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3133219131581503, distance: 0.9482727868073332 entropy 0.03264415264129639
epoch: 32, step: 121
	action: tensor([[ 0.2945,  0.1558, -0.6388, -0.1920, -0.5018,  0.1222,  0.1137]],
       dtype=torch.float64)
	q_value: tensor([[-2.5347]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6467053153444704, distance: 0.680182172626227 entropy 0.03264415264129639
epoch: 32, step: 122
	action: tensor([[ 0.1856,  0.1403,  0.1178,  0.3518,  0.1513,  0.2451, -0.3904]],
       dtype=torch.float64)
	q_value: tensor([[-4.1046]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7559383229139109, distance: 0.5653358010033751 entropy 0.03264415264129639
epoch: 32, step: 123
	action: tensor([[-0.1155, -0.3377, -0.4139,  0.2568, -0.3486, -0.0948, -0.0406]],
       dtype=torch.float64)
	q_value: tensor([[-2.5508]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0029784457218640092, distance: 1.1426387995282796 entropy 0.03264415264129639
epoch: 32, step: 124
	action: tensor([[-0.3631,  0.3552, -0.3933, -0.3428, -0.4591,  0.2231,  0.3101]],
       dtype=torch.float64)
	q_value: tensor([[-2.5932]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.09910425666720146, distance: 1.08616038948405 entropy 0.03264415264129639
epoch: 32, step: 125
	action: tensor([[-0.0289,  0.3902, -0.1376, -0.1741, -0.2535,  0.1800, -0.2464]],
       dtype=torch.float64)
	q_value: tensor([[-4.6376]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.48346344497114113, distance: 0.8224456807249395 entropy 0.03264415264129639
epoch: 32, step: 126
	action: tensor([[ 0.0341,  0.6295,  0.2823, -0.2372, -0.0298,  0.0148, -0.0027]],
       dtype=torch.float64)
	q_value: tensor([[-3.2203]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.582437971214334, distance: 0.7394642674521756 entropy 0.03264415264129639
epoch: 32, step: 127
	action: tensor([[ 0.0362,  0.5930, -0.6393, -0.5051, -0.3522, -0.1527, -0.2093]],
       dtype=torch.float64)
	q_value: tensor([[-3.3928]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7161837887577146, distance: 0.6096425800459219 entropy 0.03264415264129639
LOSS epoch 32 actor 15.387248838583972 critic 73.47175329995619 
epoch: 33, step: 0
	action: tensor([[-0.0445,  0.2149, -0.6579,  0.2230,  0.0085, -0.0659, -0.2009]],
       dtype=torch.float64)
	q_value: tensor([[-4.5469]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4091136967814175, distance: 0.8796474637118535 entropy 0.03264415264129639
epoch: 33, step: 1
	action: tensor([[ 0.2591,  0.3337,  0.1318,  0.4667,  0.4940, -0.2727, -0.3645]],
       dtype=torch.float64)
	q_value: tensor([[-2.5282]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7984433812459171, distance: 0.5137540107818633 entropy 0.03264415264129639
epoch: 33, step: 2
	action: tensor([[ 0.0816, -0.1107, -0.2869, -0.3913, -0.1733,  0.1966, -0.0432]],
       dtype=torch.float64)
	q_value: tensor([[-2.5733]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1987628749838697, distance: 1.0243237095505475 entropy 0.03264415264129639
epoch: 33, step: 3
	action: tensor([[ 0.2778, -0.3578, -0.1401, -0.3137, -0.0178,  0.2477, -0.0883]],
       dtype=torch.float64)
	q_value: tensor([[-2.4792]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.1360485443488919, distance: 1.0636564147965355 entropy 0.03264415264129639
epoch: 33, step: 4
	action: tensor([[ 0.2511,  0.1623, -0.5922,  0.1587,  0.6952, -0.2435,  0.2740]],
       dtype=torch.float64)
	q_value: tensor([[-2.1100]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6028144841391763, distance: 0.7211961416840132 entropy 0.03264415264129639
epoch: 33, step: 5
	action: tensor([[ 0.2755,  0.4374, -0.3965, -0.0381, -0.3133, -0.4528, -0.2974]],
       dtype=torch.float64)
	q_value: tensor([[-3.2698]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7875054290260672, distance: 0.5275098975750658 entropy 0.03264415264129639
epoch: 33, step: 6
	action: tensor([[ 7.6307e-02, -5.3023e-02,  1.1107e-01,  1.5915e-02,  4.3478e-05,
          3.2210e-01, -5.7119e-01]], dtype=torch.float64)
	q_value: tensor([[-3.4941]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.45880379564907114, distance: 0.8418487362532706 entropy 0.03264415264129639
epoch: 33, step: 7
	action: tensor([[ 0.0061, -0.0372, -0.1594,  0.1218,  0.1406, -0.0788, -0.7766]],
       dtype=torch.float64)
	q_value: tensor([[-2.0890]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33286452191646354, distance: 0.9346816396346025 entropy 0.03264415264129639
epoch: 33, step: 8
	action: tensor([[ 0.5764, -0.1605, -0.2809,  0.3517, -0.0924, -0.1479,  0.0587]],
       dtype=torch.float64)
	q_value: tensor([[-2.2343]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6982975202266626, distance: 0.6285591215095089 entropy 0.03264415264129639
epoch: 33, step: 9
	action: tensor([[-0.0581,  0.2516, -0.1778, -0.0817,  0.5919,  0.0460, -0.0314]],
       dtype=torch.float64)
	q_value: tensor([[-2.5024]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4116605597369909, distance: 0.8777496697984032 entropy 0.03264415264129639
epoch: 33, step: 10
	action: tensor([[ 0.4861, -0.0359,  0.0379, -0.4042, -0.0534,  0.3341, -0.4754]],
       dtype=torch.float64)
	q_value: tensor([[-2.3091]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.49412457881748917, distance: 0.8139139325127918 entropy 0.03264415264129639
epoch: 33, step: 11
	action: tensor([[ 0.5701,  0.4711, -0.1831, -0.2159, -0.1585,  0.0505, -0.4381]],
       dtype=torch.float64)
	q_value: tensor([[-2.7591]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8901788322879065, distance: 0.3792274116552697 entropy 0.03264415264129639
epoch: 33, step: 12
	action: tensor([[ 0.1282,  0.5164, -0.5035, -0.0510,  0.2039,  0.1920,  0.1597]],
       dtype=torch.float64)
	q_value: tensor([[-3.4246]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7113116862881763, distance: 0.614852998209235 entropy 0.03264415264129639
epoch: 33, step: 13
	action: tensor([[ 0.6144,  0.3088,  0.1168,  0.2819, -0.3748, -0.3958, -0.1468]],
       dtype=torch.float64)
	q_value: tensor([[-3.2430]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9242420302074494, distance: 0.31497120874178086 entropy 0.03264415264129639
epoch: 33, step: 14
	action: tensor([[ 0.6906,  0.3751, -0.1274,  0.0216,  0.3402, -0.0690, -0.2707]],
       dtype=torch.float64)
	q_value: tensor([[-3.4016]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9302796169178568, distance: 0.3021597243437725 entropy 0.03264415264129639
epoch: 33, step: 15
	action: tensor([[-0.2627,  0.0583,  0.0082,  0.2617,  0.0148,  0.2927, -0.6049]],
       dtype=torch.float64)
	q_value: tensor([[-3.0149]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2775446250758198, distance: 0.9726625763484409 entropy 0.03264415264129639
epoch: 33, step: 16
	action: tensor([[ 0.3671,  0.1294, -0.5152, -0.0772,  0.1871,  0.2361, -0.1076]],
       dtype=torch.float64)
	q_value: tensor([[-2.2242]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7135624953713862, distance: 0.6124514033022929 entropy 0.03264415264129639
epoch: 33, step: 17
	action: tensor([[ 0.5057,  0.5473, -0.3785, -0.2162,  0.4499, -0.1389, -0.0602]],
       dtype=torch.float64)
	q_value: tensor([[-2.4816]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8988916572705212, distance: 0.36387329981421246 entropy 0.03264415264129639
epoch: 33, step: 18
	action: tensor([[ 0.0305,  0.2751, -0.2919,  0.2430, -0.0351,  0.1958, -0.3846]],
       dtype=torch.float64)
	q_value: tensor([[-3.5229]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.574864473692837, distance: 0.7461401194665869 entropy 0.03264415264129639
epoch: 33, step: 19
	action: tensor([[ 0.2955,  0.0485, -0.0207, -0.3047, -0.4018, -0.2774, -0.0560]],
       dtype=torch.float64)
	q_value: tensor([[-2.2840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4295784193476546, distance: 0.8642804057328413 entropy 0.03264415264129639
epoch: 33, step: 20
	action: tensor([[-0.1383, -0.1073, -0.5366,  0.0348,  0.0477,  0.1476,  0.0059]],
       dtype=torch.float64)
	q_value: tensor([[-2.8707]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11407782052020421, distance: 1.0770961674205906 entropy 0.03264415264129639
epoch: 33, step: 21
	action: tensor([[ 0.2540,  0.2767,  0.1006, -0.1080, -0.1249,  0.2951, -0.0054]],
       dtype=torch.float64)
	q_value: tensor([[-2.2267]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6928150005890745, distance: 0.6342444789092256 entropy 0.03264415264129639
epoch: 33, step: 22
	action: tensor([[ 0.2499,  0.3776, -0.1177,  0.3634, -0.5921,  0.0045,  0.0477]],
       dtype=torch.float64)
	q_value: tensor([[-2.5454]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7754127235075996, distance: 0.5423120664154615 entropy 0.03264415264129639
epoch: 33, step: 23
	action: tensor([[ 0.1531, -0.2602, -0.4416, -0.3821,  0.0093,  0.1403, -0.4143]],
       dtype=torch.float64)
	q_value: tensor([[-3.1240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.15389879188077282, distance: 1.0526108707707498 entropy 0.03264415264129639
epoch: 33, step: 24
	action: tensor([[-0.0223,  0.5038, -0.5347,  0.0267,  0.2232,  0.0371,  0.2857]],
       dtype=torch.float64)
	q_value: tensor([[-2.2883]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5773627226397045, distance: 0.7439445957154054 entropy 0.03264415264129639
epoch: 33, step: 25
	action: tensor([[ 0.3019,  0.5271, -0.3847, -0.2485, -0.1182,  0.2880, -0.3738]],
       dtype=torch.float64)
	q_value: tensor([[-3.2569]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8182445610654905, distance: 0.4878658298279862 entropy 0.03264415264129639
epoch: 33, step: 26
	action: tensor([[ 0.4689,  0.6227, -0.3633,  0.2138,  0.0130,  0.0689, -0.2830]],
       dtype=torch.float64)
	q_value: tensor([[-3.5036]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.885637714795119, distance: 0.3869885412482835 entropy 0.03264415264129639
epoch: 33, step: 27
	action: tensor([[ 0.4447,  0.2426, -0.3756,  0.2464, -0.2100, -0.1092, -0.1601]],
       dtype=torch.float64)
	q_value: tensor([[-3.2823]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.844238863462788, distance: 0.45163360259916085 entropy 0.03264415264129639
epoch: 33, step: 28
	action: tensor([[ 0.1439,  0.0557, -0.0064,  0.0990,  0.0309,  0.0603, -0.2835]],
       dtype=torch.float64)
	q_value: tensor([[-2.7583]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5519014226615004, distance: 0.7660259389035426 entropy 0.03264415264129639
epoch: 33, step: 29
	action: tensor([[ 0.0944,  0.3115, -0.7013, -0.1807,  0.0052, -0.0133, -0.1636]],
       dtype=torch.float64)
	q_value: tensor([[-1.9171]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6193162214245512, distance: 0.7060555626372291 entropy 0.03264415264129639
epoch: 33, step: 30
	action: tensor([[ 0.2363,  0.1901, -0.4036,  0.0821, -0.1998, -0.0506,  0.3725]],
       dtype=torch.float64)
	q_value: tensor([[-3.1292]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6659968338059802, distance: 0.6613509551983087 entropy 0.03264415264129639
epoch: 33, step: 31
	action: tensor([[ 0.6841,  0.2023, -0.6695, -0.0608,  0.2512,  0.2113, -0.2120]],
       dtype=torch.float64)
	q_value: tensor([[-2.8259]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8763816453896255, distance: 0.4023445960568243 entropy 0.03264415264129639
epoch: 33, step: 32
	action: tensor([[ 0.6704, -0.0338, -0.1320,  0.0891,  0.1806, -0.3234, -0.1155]],
       dtype=torch.float64)
	q_value: tensor([[-3.3859]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6735342445135488, distance: 0.653846057909175 entropy 0.03264415264129639
epoch: 33, step: 33
	action: tensor([[ 0.5128,  0.0835, -0.1788,  0.4221, -0.1046,  0.0032, -0.1831]],
       dtype=torch.float64)
	q_value: tensor([[-2.7687]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.873346124147269, distance: 0.40725454104574754 entropy 0.03264415264129639
epoch: 33, step: 34
	action: tensor([[ 0.1479,  0.5932, -0.3324, -0.4423,  0.0472,  0.4886, -0.4343]],
       dtype=torch.float64)
	q_value: tensor([[-2.4979]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7582754493913237, distance: 0.5626224713043135 entropy 0.03264415264129639
epoch: 33, step: 35
	action: tensor([[ 0.2772,  0.3344, -0.2651, -0.0091, -0.5265,  0.0519, -0.0359]],
       dtype=torch.float64)
	q_value: tensor([[-3.8786]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7514099341081238, distance: 0.5705563957091689 entropy 0.03264415264129639
epoch: 33, step: 36
	action: tensor([[-0.0677,  0.3233, -0.6451,  0.2833,  0.0437,  0.1534,  0.0780]],
       dtype=torch.float64)
	q_value: tensor([[-3.1333]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42127272027814766, distance: 0.8705499009853036 entropy 0.03264415264129639
epoch: 33, step: 37
	action: tensor([[ 0.2646,  0.3806, -0.2455, -0.5741, -0.2129,  0.2089, -0.3171]],
       dtype=torch.float64)
	q_value: tensor([[-2.8142]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6607609618997761, distance: 0.666514504469536 entropy 0.03264415264129639
epoch: 33, step: 38
	action: tensor([[ 0.3132,  0.0934, -0.2751,  0.2575,  0.2230,  0.0749, -0.5315]],
       dtype=torch.float64)
	q_value: tensor([[-3.6027]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7296655737839477, distance: 0.5949868548369351 entropy 0.03264415264129639
epoch: 33, step: 39
	action: tensor([[-0.0993,  0.6159, -0.5044, -0.0374, -0.1525,  0.1430, -0.4531]],
       dtype=torch.float64)
	q_value: tensor([[-2.1921]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5330128348555224, distance: 0.7820043438614154 entropy 0.03264415264129639
epoch: 33, step: 40
	action: tensor([[ 0.4005, -0.0552, -0.3527,  0.2506, -0.3198,  0.4497, -0.4653]],
       dtype=torch.float64)
	q_value: tensor([[-3.4422]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7082387783566308, distance: 0.6181167004630888 entropy 0.03264415264129639
epoch: 33, step: 41
	action: tensor([[ 0.1908,  0.0174, -0.1004, -0.4386, -0.6123,  0.3380, -0.2753]],
       dtype=torch.float64)
	q_value: tensor([[-2.7585]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.33779747204950084, distance: 0.9312196046080587 entropy 0.03264415264129639
epoch: 33, step: 42
	action: tensor([[ 0.2778,  0.1731, -0.2858, -0.2473,  0.0376, -0.0793, -0.2448]],
       dtype=torch.float64)
	q_value: tensor([[-3.5312]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5902643363789093, distance: 0.7325015983667598 entropy 0.03264415264129639
epoch: 33, step: 43
	action: tensor([[ 0.2616,  0.2151, -0.3348,  0.6253, -0.3795,  0.3090, -0.5423]],
       dtype=torch.float64)
	q_value: tensor([[-2.3839]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6906006517660641, distance: 0.6365263556596229 entropy 0.03264415264129639
epoch: 33, step: 44
	action: tensor([[ 0.1773,  0.4289, -0.4638, -0.2030,  0.3001,  0.2812,  0.1226]],
       dtype=torch.float64)
	q_value: tensor([[-3.0793]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.728621859375963, distance: 0.5961343189725207 entropy 0.03264415264129639
epoch: 33, step: 45
	action: tensor([[-0.3132,  0.2499, -0.2544, -0.1795, -0.2694,  0.0296, -0.0470]],
       dtype=torch.float64)
	q_value: tensor([[-3.1670]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10930262619068609, distance: 1.0799950853411038 entropy 0.03264415264129639
epoch: 33, step: 46
	action: tensor([[-0.0668, -0.0334, -0.1350,  0.0469, -0.1826,  0.1614, -0.0835]],
       dtype=torch.float64)
	q_value: tensor([[-2.7448]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2700353976291897, distance: 0.9777044539516896 entropy 0.03264415264129639
epoch: 33, step: 47
	action: tensor([[ 0.5188, -0.1211, -0.0034, -0.0600,  0.2942,  0.4250, -0.3434]],
       dtype=torch.float64)
	q_value: tensor([[-1.9654]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6622883427641693, distance: 0.665012363131668 entropy 0.03264415264129639
epoch: 33, step: 48
	action: tensor([[-0.0772, -0.4253, -0.3413, -0.2988,  0.2746,  0.0818, -0.1724]],
       dtype=torch.float64)
	q_value: tensor([[-2.2473]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.1867918513396536, distance: 1.2466483631501706 entropy 0.03264415264129639
epoch: 33, step: 49
	action: tensor([[ 0.3415,  0.1025, -0.5777,  0.0865, -0.4648, -0.1569,  0.1559]],
       dtype=torch.float64)
	q_value: tensor([[-1.9644]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6580637252166508, distance: 0.6691589349494069 entropy 0.03264415264129639
epoch: 33, step: 50
	action: tensor([[ 0.4667, -0.0100, -0.5166, -0.2576,  0.0556,  0.3427, -0.2238]],
       dtype=torch.float64)
	q_value: tensor([[-3.1818]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6343332613958068, distance: 0.6919893689274454 entropy 0.03264415264129639
epoch: 33, step: 51
	action: tensor([[ 0.7178,  0.1936, -0.1141, -0.3499, -0.0827, -0.1483, -0.4231]],
       dtype=torch.float64)
	q_value: tensor([[-2.7073]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7035451400847814, distance: 0.6230687652931649 entropy 0.03264415264129639
epoch: 33, step: 52
	action: tensor([[ 0.7234,  0.1419, -0.4907, -0.0950,  0.1268,  0.3962, -0.2111]],
       dtype=torch.float64)
	q_value: tensor([[-3.3439]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8762159870468701, distance: 0.40261409254265 entropy 0.03264415264129639
epoch: 33, step: 53
	action: tensor([[ 0.3033,  0.6047, -0.3208, -0.0967, -0.2286,  0.2521, -0.3635]],
       dtype=torch.float64)
	q_value: tensor([[-3.2925]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8300965460617818, distance: 0.4716912250823785 entropy 0.03264415264129639
epoch: 33, step: 54
	action: tensor([[-0.0005,  0.2276, -0.2874, -0.1345, -0.1875,  0.2928, -0.0550]],
       dtype=torch.float64)
	q_value: tensor([[-3.5544]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4644432476342689, distance: 0.8374510721812117 entropy 0.03264415264129639
epoch: 33, step: 55
	action: tensor([[ 0.0540,  0.0797,  0.1113, -0.1352,  0.1824, -0.0757, -0.4613]],
       dtype=torch.float64)
	q_value: tensor([[-2.5701]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3399913096173731, distance: 0.9296757873366895 entropy 0.03264415264129639
epoch: 33, step: 56
	action: tensor([[ 0.0480,  0.3627, -0.5899, -0.1718,  0.0511,  0.2125, -0.0744]],
       dtype=torch.float64)
	q_value: tensor([[-1.9744]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5903498351359799, distance: 0.7324251695258882 entropy 0.03264415264129639
epoch: 33, step: 57
	action: tensor([[ 0.3824,  0.1211, -0.7828,  0.0688, -0.2760, -0.1923, -0.2615]],
       dtype=torch.float64)
	q_value: tensor([[-3.0333]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7231249490682361, distance: 0.6021415622388957 entropy 0.03264415264129639
epoch: 33, step: 58
	action: tensor([[ 0.1247, -0.2178, -0.2987,  0.8788, -0.0334,  0.2839, -0.4885]],
       dtype=torch.float64)
	q_value: tensor([[-3.1989]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5676477575514407, distance: 0.7524463696623925 entropy 0.03264415264129639
epoch: 33, step: 59
	action: tensor([[0.1626, 0.2842, 0.1060, 0.3192, 0.5074, 0.1088, 0.0414]],
       dtype=torch.float64)
	q_value: tensor([[-2.6899]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7448229991497162, distance: 0.5780660417245329 entropy 0.03264415264129639
epoch: 33, step: 60
	action: tensor([[ 0.1856,  0.0010, -0.2062, -0.0240, -0.0480,  0.1379,  0.1546]],
       dtype=torch.float64)
	q_value: tensor([[-2.2276]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.46302033849583024, distance: 0.8385628370248487 entropy 0.03264415264129639
epoch: 33, step: 61
	action: tensor([[ 0.0985, -0.1338,  0.0969,  0.1134, -0.1075,  0.2886, -0.1487]],
       dtype=torch.float64)
	q_value: tensor([[-2.0806]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4100052685085296, distance: 0.8789835755256755 entropy 0.03264415264129639
epoch: 33, step: 62
	action: tensor([[ 0.4042, -0.0399, -0.5124,  0.4752,  0.1069,  0.0300, -0.2245]],
       dtype=torch.float64)
	q_value: tensor([[-1.8607]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6812509677670822, distance: 0.6460723139893267 entropy 0.03264415264129639
epoch: 33, step: 63
	action: tensor([[ 0.4889, -0.0770, -0.3858, -0.0269, -0.1500, -0.1726, -0.3415]],
       dtype=torch.float64)
	q_value: tensor([[-2.3591]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5859916493596278, distance: 0.7363109240195218 entropy 0.03264415264129639
epoch: 33, step: 64
	action: tensor([[-0.0944,  0.3420, -0.5067, -0.5312,  0.3104,  0.2158,  0.0560]],
       dtype=torch.float64)
	q_value: tensor([[-2.5220]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4567214348958778, distance: 0.843466772101138 entropy 0.03264415264129639
epoch: 33, step: 65
	action: tensor([[ 0.4493,  0.0178, -0.4047,  0.2110,  0.5170, -0.3626, -0.1659]],
       dtype=torch.float64)
	q_value: tensor([[-3.5224]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6462852711661318, distance: 0.6805863984794418 entropy 0.03264415264129639
epoch: 33, step: 66
	action: tensor([[-0.1527,  0.0355, -0.3510,  0.4744,  0.2664,  0.0523, -0.8348]],
       dtype=torch.float64)
	q_value: tensor([[-2.7521]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2981701670148972, distance: 0.9586776569609498 entropy 0.03264415264129639
epoch: 33, step: 67
	action: tensor([[ 0.2609, -0.0964, -0.2435,  0.2146,  0.4882,  0.3650, -0.4236]],
       dtype=torch.float64)
	q_value: tensor([[-2.5977]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6359130491253743, distance: 0.6904929522613207 entropy 0.03264415264129639
epoch: 33, step: 68
	action: tensor([[ 0.1003, -0.2533, -0.5145,  0.0045,  0.2058, -0.0290,  0.6077]],
       dtype=torch.float64)
	q_value: tensor([[-2.0508]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.182231212836084, distance: 1.0348370241323115 entropy 0.03264415264129639
epoch: 33, step: 69
	action: tensor([[ 0.0101, -0.0428, -0.4043,  0.1754, -0.0754,  0.3827,  0.2791]],
       dtype=torch.float64)
	q_value: tensor([[-2.9335]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.37620770356671873, distance: 0.9038090731783567 entropy 0.03264415264129639
epoch: 33, step: 70
	action: tensor([[ 0.7231,  0.0494, -0.3980, -0.0117,  0.4520,  0.3919, -0.1738]],
       dtype=torch.float64)
	q_value: tensor([[-2.3245]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8461456675938128, distance: 0.44886067510889716 entropy 0.03264415264129639
epoch: 33, step: 71
	action: tensor([[ 0.1155,  0.2520,  0.1142, -0.2831,  0.2949,  0.1879, -0.5988]],
       dtype=torch.float64)
	q_value: tensor([[-2.9604]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5247688168255553, distance: 0.7888767529864101 entropy 0.03264415264129639
epoch: 33, step: 72
	action: tensor([[ 0.3014,  0.0197, -0.1289,  0.0474,  0.2752,  0.1601, -0.5927]],
       dtype=torch.float64)
	q_value: tensor([[-2.3356]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6450563758944189, distance: 0.6817676388377565 entropy 0.03264415264129639
epoch: 33, step: 73
	action: tensor([[ 0.0072,  0.0548, -0.2861, -0.0098,  0.4998,  0.2222, -0.2187]],
       dtype=torch.float64)
	q_value: tensor([[-2.0640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.42725922942674166, distance: 0.8660355963043096 entropy 0.03264415264129639
epoch: 33, step: 74
	action: tensor([[ 0.4461,  0.2543, -0.5128, -0.2001,  0.1331,  0.3066, -0.9145]],
       dtype=torch.float64)
	q_value: tensor([[-2.0229]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8118693887324937, distance: 0.49634817178292895 entropy 0.03264415264129639
epoch: 33, step: 75
	action: tensor([[ 0.3521,  0.5292,  0.2610, -0.1056,  0.0084,  0.2079, -0.0551]],
       dtype=torch.float64)
	q_value: tensor([[-3.4586]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8548193954442269, distance: 0.4360246011983313 entropy 0.03264415264129639
epoch: 33, step: 76
	action: tensor([[ 0.3519, -0.0255,  0.1876, -0.1119, -0.0787,  0.0948, -0.1415]],
       dtype=torch.float64)
	q_value: tensor([[-2.9146]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5393963915806393, distance: 0.7766410847610524 entropy 0.03264415264129639
epoch: 33, step: 77
	action: tensor([[-0.0249, -0.0453, -0.5451,  0.0864,  0.0947,  0.2002, -0.1451]],
       dtype=torch.float64)
	q_value: tensor([[-2.1810]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.29786913833870077, distance: 0.9588832328096929 entropy 0.03264415264129639
epoch: 33, step: 78
	action: tensor([[ 0.2078, -0.4229,  0.0691,  0.3938, -0.2592,  0.3131, -0.3228]],
       dtype=torch.float64)
	q_value: tensor([[-2.0891]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4659781287562944, distance: 0.8362501629186642 entropy 0.03264415264129639
epoch: 33, step: 79
	action: tensor([[ 0.1954,  0.1892, -0.2438, -0.3486, -0.0390,  0.3792, -0.3890]],
       dtype=torch.float64)
	q_value: tensor([[-2.1322]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5852291343595621, distance: 0.7369886758623347 entropy 0.03264415264129639
epoch: 33, step: 80
	action: tensor([[-0.0033,  0.0829, -0.4419,  0.0152,  0.0650, -0.2513, -0.3726]],
       dtype=torch.float64)
	q_value: tensor([[-2.7486]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3621763103389124, distance: 0.9139175475615299 entropy 0.03264415264129639
epoch: 33, step: 81
	action: tensor([[ 0.3090, -0.1347, -0.4586, -0.6186, -0.0963, -0.0517, -0.1928]],
       dtype=torch.float64)
	q_value: tensor([[-2.2865]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.257402660406835, distance: 0.9861282213641462 entropy 0.03264415264129639
epoch: 33, step: 82
	action: tensor([[-0.1805,  0.2838, -0.0857,  0.1543,  0.0201, -0.0410, -0.4320]],
       dtype=torch.float64)
	q_value: tensor([[-3.1058]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.36301543234466593, distance: 0.9133161739107775 entropy 0.03264415264129639
epoch: 33, step: 83
	action: tensor([[ 0.3254,  0.0048, -0.3872,  0.0201, -0.1836,  0.1906, -0.5441]],
       dtype=torch.float64)
	q_value: tensor([[-2.2503]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6227224035629062, distance: 0.7028897360540876 entropy 0.03264415264129639
epoch: 33, step: 84
	action: tensor([[ 0.2921,  0.1181, -0.4976, -0.2348, -0.3056,  0.0510, -0.4249]],
       dtype=torch.float64)
	q_value: tensor([[-2.4025]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6073814228079042, distance: 0.717037906968301 entropy 0.03264415264129639
epoch: 33, step: 85
	action: tensor([[ 0.1185,  0.1294, -0.5360,  0.3076,  0.1042, -0.0398,  0.1456]],
       dtype=torch.float64)
	q_value: tensor([[-2.9388]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5418093573780955, distance: 0.7746041172539984 entropy 0.03264415264129639
epoch: 33, step: 86
	action: tensor([[-0.0408,  0.4835, -0.3657,  0.0923, -0.0853,  0.1294, -0.0808]],
       dtype=torch.float64)
	q_value: tensor([[-2.4230]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5563387032677541, distance: 0.7622237300410074 entropy 0.03264415264129639
epoch: 33, step: 87
	action: tensor([[ 0.3690, -0.2500, -0.0598, -0.3162,  0.1178, -0.3966, -0.4376]],
       dtype=torch.float64)
	q_value: tensor([[-2.7171]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.11219486121253763, distance: 1.078240202295661 entropy 0.03264415264129639
epoch: 33, step: 88
	action: tensor([[-0.1269, -0.1477, -0.4469,  0.0854,  0.4529,  0.1480, -0.3934]],
       dtype=torch.float64)
	q_value: tensor([[-2.4804]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.13439335001456043, distance: 1.0646748259752121 entropy 0.03264415264129639
epoch: 33, step: 89
	action: tensor([[ 0.1563, -0.2135, -0.1342, -0.1925,  0.1342, -0.3473, -0.2425]],
       dtype=torch.float64)
	q_value: tensor([[-2.0542]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.10724878478666244, distance: 1.0812395379853312 entropy 0.03264415264129639
epoch: 33, step: 90
	action: tensor([[-0.0328,  0.1828, -0.7280,  0.3609, -0.3923, -0.1092,  0.1734]],
       dtype=torch.float64)
	q_value: tensor([[-2.1078]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3768783380962222, distance: 0.9033231032698308 entropy 0.03264415264129639
epoch: 33, step: 91
	action: tensor([[-0.0773,  0.5844, -0.0614,  0.0105,  0.2749,  0.2293, -0.1235]],
       dtype=torch.float64)
	q_value: tensor([[-3.0057]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5730339547578633, distance: 0.7477447331392174 entropy 0.03264415264129639
epoch: 33, step: 92
	action: tensor([[ 0.3022,  0.0864, -0.1575,  0.2867,  0.1757,  0.5754, -0.3033]],
       dtype=torch.float64)
	q_value: tensor([[-2.6360]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7856538544645598, distance: 0.5298031454634415 entropy 0.03264415264129639
epoch: 33, step: 93
	action: tensor([[ 0.4592, -0.5500, -0.3005, -0.1592, -0.2571,  0.0039, -0.1827]],
       dtype=torch.float64)
	q_value: tensor([[-2.3459]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.05028559681889755, distance: 1.1152011420619232 entropy 0.03264415264129639
epoch: 33, step: 94
	action: tensor([[ 0.1051,  0.1993, -0.4331, -0.2289, -0.0856, -0.0785, -0.0381]],
       dtype=torch.float64)
	q_value: tensor([[-2.3586]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5173226844533769, distance: 0.7950329670841941 entropy 0.03264415264129639
epoch: 33, step: 95
	action: tensor([[ 0.2594,  0.4359, -0.2001, -0.1500,  0.4792,  0.1541, -0.2889]],
       dtype=torch.float64)
	q_value: tensor([[-2.6340]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7754181099182691, distance: 0.5423055630795173 entropy 0.03264415264129639
epoch: 33, step: 96
	action: tensor([[ 0.1107,  0.3894, -0.6069,  0.1859, -0.4606,  0.1939, -0.5915]],
       dtype=torch.float64)
	q_value: tensor([[-2.3955]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5861288447725654, distance: 0.7361889133747377 entropy 0.03264415264129639
epoch: 33, step: 97
	action: tensor([[ 0.2844,  0.0423, -0.3113,  0.1146, -0.4651, -0.5662, -0.0469]],
       dtype=torch.float64)
	q_value: tensor([[-3.5196]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6313675721850432, distance: 0.694789843930856 entropy 0.03264415264129639
epoch: 33, step: 98
	action: tensor([[ 0.0147, -0.3783,  0.0610,  0.2866, -0.3226,  0.4988, -0.0728]],
       dtype=torch.float64)
	q_value: tensor([[-3.0968]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31965972394259223, distance: 0.9438865208894649 entropy 0.03264415264129639
epoch: 33, step: 99
	action: tensor([[ 0.4079,  0.5239, -0.2824, -0.2433, -0.0187,  0.4991, -0.3360]],
       dtype=torch.float64)
	q_value: tensor([[-2.1421]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8740017839272614, distance: 0.4061990388151593 entropy 0.03264415264129639
epoch: 33, step: 100
	action: tensor([[-0.2542,  0.1508, -0.1795, -0.0544, -0.3535,  0.2110,  0.3394]],
       dtype=torch.float64)
	q_value: tensor([[-3.5507]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.14027196842873457, distance: 1.0610533890938063 entropy 0.03264415264129639
epoch: 33, step: 101
	action: tensor([[-0.1483, -0.1746, -0.2466, -0.3369, -0.2281,  0.3806,  0.1929]],
       dtype=torch.float64)
	q_value: tensor([[-2.8611]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.04259825898723846, distance: 1.168463608521059 entropy 0.03264415264129639
epoch: 33, step: 102
	action: tensor([[ 0.6099,  0.2268, -0.6532, -0.4061, -0.2160, -0.0452, -0.2312]],
       dtype=torch.float64)
	q_value: tensor([[-2.6921]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7411372875114866, distance: 0.5822257943451367 entropy 0.03264415264129639
epoch: 33, step: 103
	action: tensor([[ 0.6659,  0.1748, -0.4637, -0.0626,  0.1799,  0.0185, -0.5006]],
       dtype=torch.float64)
	q_value: tensor([[-3.9451]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8265118376745068, distance: 0.47664124104753125 entropy 0.03264415264129639
epoch: 33, step: 104
	action: tensor([[ 0.1547,  0.1413,  0.2924,  0.0335,  0.1062,  0.2194, -0.0525]],
       dtype=torch.float64)
	q_value: tensor([[-3.0801]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.628197151223937, distance: 0.6977712152591348 entropy 0.03264415264129639
epoch: 33, step: 105
	action: tensor([[ 0.4695,  0.2163, -0.3938,  0.0106, -0.0939,  0.0167, -0.1789]],
       dtype=torch.float64)
	q_value: tensor([[-2.1449]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8039703837700911, distance: 0.5066610725159421 entropy 0.03264415264129639
epoch: 33, step: 106
	action: tensor([[ 0.5780,  0.1337, -0.1834, -0.1179,  0.3698,  0.4698,  0.0619]],
       dtype=torch.float64)
	q_value: tensor([[-2.6605]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8484727003835216, distance: 0.4454532533315391 entropy 0.03264415264129639
epoch: 33, step: 107
	action: tensor([[ 0.6651,  0.0465, -0.3311, -0.1891,  0.0527, -0.1797, -0.1947]],
       dtype=torch.float64)
	q_value: tensor([[-2.7424]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.643393154379505, distance: 0.68336311100156 entropy 0.03264415264129639
epoch: 33, step: 108
	action: tensor([[ 0.0069,  0.4642, -0.3438,  0.0447,  0.4020,  0.0697, -0.2306]],
       dtype=torch.float64)
	q_value: tensor([[-2.9624]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6050776190178805, distance: 0.719138544272079 entropy 0.03264415264129639
epoch: 33, step: 109
	action: tensor([[-0.2047, -0.3451, -0.4431, -0.3654, -0.6967,  0.1035, -0.0392]],
       dtype=torch.float64)
	q_value: tensor([[-2.4993]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: -0.2586485519313546, distance: 1.2838341789043823 entropy 0.03264415264129639
epoch: 33, step: 110
	action: tensor([[ 0.2518,  0.0012, -0.1373, -0.3467, -0.3510,  0.0388, -0.1481]],
       dtype=torch.float64)
	q_value: tensor([[-3.3609]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3763947247225977, distance: 0.9036735760826985 entropy 0.03264415264129639
epoch: 33, step: 111
	action: tensor([[ 0.1643,  0.2283, -0.0592,  0.1603, -0.1348,  0.1381, -0.2484]],
       dtype=torch.float64)
	q_value: tensor([[-2.6711]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6612491758209741, distance: 0.6660347265216774 entropy 0.03264415264129639
epoch: 33, step: 112
	action: tensor([[ 0.2427,  0.2050, -0.1720,  0.0657,  0.1523,  0.0589, -0.0449]],
       dtype=torch.float64)
	q_value: tensor([[-2.2052]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6787736745041699, distance: 0.6485780672522675 entropy 0.03264415264129639
epoch: 33, step: 113
	action: tensor([[ 0.4496,  0.1187, -0.2805,  0.0075,  0.2768, -0.2248, -0.1774]],
       dtype=torch.float64)
	q_value: tensor([[-2.0600]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6821747492061374, distance: 0.6451354285336215 entropy 0.03264415264129639
epoch: 33, step: 114
	action: tensor([[ 0.1440,  0.3583, -0.6292,  0.0064, -0.4009,  0.1317, -0.2412]],
       dtype=torch.float64)
	q_value: tensor([[-2.4303]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6328746907077281, distance: 0.6933680982690897 entropy 0.03264415264129639
epoch: 33, step: 115
	action: tensor([[ 0.4251,  0.3698, -0.3182,  0.0442, -0.1969, -0.1821, -0.0696]],
       dtype=torch.float64)
	q_value: tensor([[-3.3102]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.854797168321323, distance: 0.43605797756259673 entropy 0.03264415264129639
epoch: 33, step: 116
	action: tensor([[ 0.2322, -0.2057, -0.3516, -0.1086,  0.0607, -0.4386, -0.6493]],
       dtype=torch.float64)
	q_value: tensor([[-2.9582]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2398818644997387, distance: 0.9976937244012362 entropy 0.03264415264129639
epoch: 33, step: 117
	action: tensor([[ 0.0696,  0.0335, -0.3171,  0.1039,  0.0195,  0.4093, -0.3592]],
       dtype=torch.float64)
	q_value: tensor([[-2.5999]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.47023198215269824, distance: 0.8329128482847007 entropy 0.03264415264129639
epoch: 33, step: 118
	action: tensor([[ 0.5350,  0.2678,  0.1280,  0.3520,  0.0345,  0.1460, -0.0628]],
       dtype=torch.float64)
	q_value: tensor([[-2.0727]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9376165327468656, distance: 0.2858192319049373 entropy 0.03264415264129639
epoch: 33, step: 119
	action: tensor([[ 0.2796, -0.1647, -0.2063,  0.2643,  0.1695,  0.0954,  0.2024]],
       dtype=torch.float64)
	q_value: tensor([[-2.5933]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5430154381772794, distance: 0.7735839624693422 entropy 0.03264415264129639
epoch: 33, step: 120
	action: tensor([[ 0.1077,  0.5492, -0.1660, -0.0384, -0.3263, -0.1251, -0.6637]],
       dtype=torch.float64)
	q_value: tensor([[-2.0365]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.684457200165386, distance: 0.6428147457553138 entropy 0.03264415264129639
epoch: 33, step: 121
	action: tensor([[ 0.3358,  0.2114, -0.5879, -0.1239, -0.1965, -0.0461, -0.2917]],
       dtype=torch.float64)
	q_value: tensor([[-3.4374]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7218272080638839, distance: 0.6035510613611401 entropy 0.03264415264129639
epoch: 33, step: 122
	action: tensor([[ 0.5124,  0.2191, -0.5147,  0.1435,  0.5904,  0.3540, -0.4779]],
       dtype=torch.float64)
	q_value: tensor([[-2.9840]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8846266342164097, distance: 0.3886954657702619 entropy 0.03264415264129639
epoch: 33, step: 123
	action: tensor([[ 0.2455,  0.1750, -0.4050, -0.1512,  0.2968,  0.0382,  0.2754]],
       dtype=torch.float64)
	q_value: tensor([[-2.9107]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6215499133760779, distance: 0.7039810969208463 entropy 0.03264415264129639
epoch: 33, step: 124
	action: tensor([[ 0.3054,  0.1130, -0.1759, -0.3613,  0.0470,  0.5522, -0.0995]],
       dtype=torch.float64)
	q_value: tensor([[-2.7451]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.640080438277947, distance: 0.6865298405470011 entropy 0.03264415264129639
epoch: 33, step: 125
	action: tensor([[ 0.5044,  0.3255, -0.2817,  0.0284,  0.3860,  0.3861, -0.5767]],
       dtype=torch.float64)
	q_value: tensor([[-2.7525]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9111716526286738, distance: 0.3410613324319745 entropy 0.03264415264129639
epoch: 33, step: 126
	action: tensor([[ 0.3967,  0.0732, -0.2441,  0.2849,  0.1949,  0.2938, -0.1518]],
       dtype=torch.float64)
	q_value: tensor([[-2.8100]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8132363718485185, distance: 0.494541616799193 entropy 0.03264415264129639
epoch: 33, step: 127
	action: tensor([[ 0.1454,  0.1038, -0.0417,  0.2874, -0.0059,  0.1178, -0.1461]],
       dtype=torch.float64)
	q_value: tensor([[-2.1348]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6539769230290854, distance: 0.673145931308602 entropy 0.03264415264129639
LOSS epoch 33 actor 5.681170791687566 critic 81.93327221345857 
epoch: 34, step: 0
	action: tensor([[ 0.3486,  0.1115, -0.3855, -0.0492, -0.2427,  0.2006, -0.5989]],
       dtype=torch.float64)
	q_value: tensor([[-1.7300]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6856510036109745, distance: 0.6415976023722172 entropy 0.03264415264129639
epoch: 34, step: 1
	action: tensor([[ 0.4738,  0.3195, -0.4736,  0.0044,  0.1938, -0.1264, -0.3264]],
       dtype=torch.float64)
	q_value: tensor([[-2.4236]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8475074100160738, distance: 0.4468698597873686 entropy 0.03264415264129639
epoch: 34, step: 2
	action: tensor([[ 0.2668,  0.0011, -0.3233,  0.0325, -0.4239,  0.0396, -0.2315]],
       dtype=torch.float64)
	q_value: tensor([[-2.5788]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5670272350833988, distance: 0.7529861407362459 entropy 0.03264415264129639
epoch: 34, step: 3
	action: tensor([[ 0.0953,  0.0962, -0.3526, -0.3341, -0.2749,  0.0310, -0.2091]],
       dtype=torch.float64)
	q_value: tensor([[-2.1476]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4020932756917591, distance: 0.8848576544951665 entropy 0.03264415264129639
epoch: 34, step: 4
	action: tensor([[ 0.4895,  0.1050, -0.2285,  0.1218, -0.1239, -0.1825, -0.0230]],
       dtype=torch.float64)
	q_value: tensor([[-2.3792]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7554394110212981, distance: 0.565913336898389 entropy 0.03264415264129639
epoch: 34, step: 5
	action: tensor([[ 0.3936,  0.1473, -0.1736,  0.1209, -0.4618, -0.2035, -0.4094]],
       dtype=torch.float64)
	q_value: tensor([[-2.1932]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7612402411716074, distance: 0.5591614971564666 entropy 0.03264415264129639
epoch: 34, step: 6
	action: tensor([[ 0.2066,  0.0567,  0.1395, -0.2645,  0.1048,  0.0565, -0.4271]],
       dtype=torch.float64)
	q_value: tensor([[-2.5385]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4223199537083926, distance: 0.8697618944482082 entropy 0.03264415264129639
epoch: 34, step: 7
	action: tensor([[ 0.3628,  0.2474, -0.2217,  0.2315,  0.2546, -0.1569, -0.6287]],
       dtype=torch.float64)
	q_value: tensor([[-1.8220]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7845656271971869, distance: 0.5311463380449432 entropy 0.03264415264129639
epoch: 34, step: 8
	action: tensor([[ 0.2591,  0.4362, -0.6898, -0.4444,  0.2649,  0.3949, -0.2916]],
       dtype=torch.float64)
	q_value: tensor([[-2.2767]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8191969194110448, distance: 0.4865859965320671 entropy 0.03264415264129639
epoch: 34, step: 9
	action: tensor([[ 0.3054, -0.4246, -0.4695, -0.2481,  0.0953,  0.8620, -0.2835]],
       dtype=torch.float64)
	q_value: tensor([[-3.4689]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.343852222680636, distance: 0.9269525943236943 entropy 0.03264415264129639
epoch: 34, step: 10
	action: tensor([[ 0.4732,  0.1199, -0.7276, -0.4370, -0.0641,  0.4447, -0.5320]],
       dtype=torch.float64)
	q_value: tensor([[-2.7184]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7273959328884001, distance: 0.5974792934776952 entropy 0.03264415264129639
epoch: 34, step: 11
	action: tensor([[ 0.6386,  0.0180, -0.6901,  0.0913,  0.0202, -0.2469, -0.0400]],
       dtype=torch.float64)
	q_value: tensor([[-3.4944]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7425567482656861, distance: 0.5806272969816858 entropy 0.03264415264129639
epoch: 34, step: 12
	action: tensor([[ 0.2370,  0.2630, -0.3857,  0.0707, -0.3586,  0.3578, -0.4189]],
       dtype=torch.float64)
	q_value: tensor([[-2.8683]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6882391609031373, distance: 0.6389508822052664 entropy 0.03264415264129639
epoch: 34, step: 13
	action: tensor([[ 0.4076,  0.2189, -0.0935,  0.4924, -0.0354,  0.0558,  0.2813]],
       dtype=torch.float64)
	q_value: tensor([[-2.6549]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8719315311400516, distance: 0.40952253219152857 entropy 0.03264415264129639
epoch: 34, step: 14
	action: tensor([[ 0.3578, -0.4056, -0.8512,  0.1541,  0.0196, -0.1702, -0.4834]],
       dtype=torch.float64)
	q_value: tensor([[-2.2168]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.26490603426257686, distance: 0.9811335396056173 entropy 0.03264415264129639
epoch: 34, step: 15
	action: tensor([[ 0.1111,  0.2779, -0.1925,  0.1906,  0.0816, -0.0660, -0.4199]],
       dtype=torch.float64)
	q_value: tensor([[-2.3203]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6162917565643231, distance: 0.7088547567464026 entropy 0.03264415264129639
epoch: 34, step: 16
	action: tensor([[ 0.3204,  0.0196, -0.1832, -0.1774,  0.0442,  0.0310, -0.5002]],
       dtype=torch.float64)
	q_value: tensor([[-1.9010]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.540072003845794, distance: 0.7760712881025668 entropy 0.03264415264129639
epoch: 34, step: 17
	action: tensor([[ 0.2476, -0.1569, -0.1011,  0.1343, -0.2889,  0.1797, -0.2050]],
       dtype=torch.float64)
	q_value: tensor([[-1.8610]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5119018690974082, distance: 0.7994848999956992 entropy 0.03264415264129639
epoch: 34, step: 18
	action: tensor([[ 0.4218,  0.3360, -0.5865, -0.0078, -0.0312,  0.2435, -0.5968]],
       dtype=torch.float64)
	q_value: tensor([[-1.7522]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8345120429100477, distance: 0.4655216576414128 entropy 0.03264415264129639
epoch: 34, step: 19
	action: tensor([[ 0.3320, -0.2735, -0.1311,  0.2035, -0.0465,  0.3527, -0.4487]],
       dtype=torch.float64)
	q_value: tensor([[-2.8529]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5672721662855517, distance: 0.7527731297736553 entropy 0.03264415264129639
epoch: 34, step: 20
	action: tensor([[ 0.4080,  0.1315, -0.6523,  0.4531, -0.2545,  0.0394, -0.7809]],
       dtype=torch.float64)
	q_value: tensor([[-1.8104]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7383074830870047, distance: 0.5853994975773732 entropy 0.03264415264129639
epoch: 34, step: 21
	action: tensor([[ 0.4935, -0.1503, -0.1918,  0.4221, -0.1858, -0.1425, -0.3052]],
       dtype=torch.float64)
	q_value: tensor([[-3.0130]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7306526293325213, distance: 0.5938996419631345 entropy 0.03264415264129639
epoch: 34, step: 22
	action: tensor([[ 0.2497,  0.2408, -0.4344,  0.3149,  0.1584, -0.1964, -0.9593]],
       dtype=torch.float64)
	q_value: tensor([[-2.0992]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7007332349691214, distance: 0.6260167273137084 entropy 0.03264415264129639
epoch: 34, step: 23
	action: tensor([[ 0.2622, -0.1347,  0.2009,  0.1489,  0.1525,  0.0809, -0.6662]],
       dtype=torch.float64)
	q_value: tensor([[-2.9029]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5560179769761794, distance: 0.7624991890753989 entropy 0.03264415264129639
epoch: 34, step: 24
	action: tensor([[ 0.1106, -0.2845, -0.1075,  0.1175,  0.1633,  0.2304, -0.1963]],
       dtype=torch.float64)
	q_value: tensor([[-1.8260]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3208224638523858, distance: 0.9430795983185527 entropy 0.03264415264129639
epoch: 34, step: 25
	action: tensor([[ 0.6328,  0.0535, -0.5964, -0.0627, -0.0693,  0.0672, -0.5901]],
       dtype=torch.float64)
	q_value: tensor([[-1.4074]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7551309658387395, distance: 0.5662700956065796 entropy 0.03264415264129639
epoch: 34, step: 26
	action: tensor([[ 0.1113,  0.2506,  0.0619,  0.1270,  0.4734,  0.2933, -0.6394]],
       dtype=torch.float64)
	q_value: tensor([[-2.7817]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7007299541882243, distance: 0.6260201587306438 entropy 0.03264415264129639
epoch: 34, step: 27
	action: tensor([[ 0.5421, -0.0381,  0.2057, -0.0344,  0.1010,  0.0303, -0.7862]],
       dtype=torch.float64)
	q_value: tensor([[-2.0154]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6597164258172985, distance: 0.6675398336418847 entropy 0.03264415264129639
epoch: 34, step: 28
	action: tensor([[ 0.4110,  0.0793, -0.6587,  0.2201, -0.0340,  0.4935, -0.3871]],
       dtype=torch.float64)
	q_value: tensor([[-2.4773]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7374777048286326, distance: 0.5863268594550212 entropy 0.03264415264129639
epoch: 34, step: 29
	action: tensor([[ 0.5742,  0.3608, -0.3775,  0.1361,  0.0616,  0.2952, -0.5406]],
       dtype=torch.float64)
	q_value: tensor([[-2.5834]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9301543031709695, distance: 0.302431149761346 entropy 0.03264415264129639
epoch: 34, step: 30
	action: tensor([[ 0.2892,  0.0846, -0.7606, -0.0515,  0.0101,  0.4556, -0.5185]],
       dtype=torch.float64)
	q_value: tensor([[-2.7495]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6405904814977886, distance: 0.6860432262186783 entropy 0.03264415264129639
epoch: 34, step: 31
	action: tensor([[ 0.2657,  0.4591, -0.6748,  0.2595, -0.1138,  0.0705, -0.5080]],
       dtype=torch.float64)
	q_value: tensor([[-2.7470]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7312894372558445, distance: 0.5931971591388182 entropy 0.03264415264129639
epoch: 34, step: 32
	action: tensor([[ 0.2205,  0.3963, -0.8107,  0.4202,  0.0156,  0.1864, -0.6074]],
       dtype=torch.float64)
	q_value: tensor([[-2.9040]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6134320118111114, distance: 0.7114913702197803 entropy 0.03264415264129639
epoch: 34, step: 33
	action: tensor([[ 0.3000,  0.2267, -0.6181,  0.0650,  0.1885, -0.1281, -0.3299]],
       dtype=torch.float64)
	q_value: tensor([[-2.9985]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.714683841990418, distance: 0.6112514141930895 entropy 0.03264415264129639
epoch: 34, step: 34
	action: tensor([[ 0.5889,  0.1717, -0.2700,  0.0183, -0.0611,  0.2066, -0.3911]],
       dtype=torch.float64)
	q_value: tensor([[-2.3943]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.859639707956551, distance: 0.4287250156317353 entropy 0.03264415264129639
epoch: 34, step: 35
	action: tensor([[ 0.3911,  0.5443, -0.3767,  0.0649, -0.2107,  0.0440, -0.0998]],
       dtype=torch.float64)
	q_value: tensor([[-2.3915]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8747096537841799, distance: 0.405056399442696 entropy 0.03264415264129639
epoch: 34, step: 36
	action: tensor([[ 0.3748,  0.0300, -0.5996,  0.0740,  0.0063,  0.3871, -0.4692]],
       dtype=torch.float64)
	q_value: tensor([[-2.8652]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6925612603654402, distance: 0.6345063733970774 entropy 0.03264415264129639
epoch: 34, step: 37
	action: tensor([[ 0.5647, -0.1993, -0.0180,  0.2209, -0.1452,  0.2575, -0.6018]],
       dtype=torch.float64)
	q_value: tensor([[-2.2522]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7344369276052338, distance: 0.5897127743035246 entropy 0.03264415264129639
epoch: 34, step: 38
	action: tensor([[ 0.3026, -0.0978, -0.5118,  0.3007, -0.2527,  0.1274,  0.1233]],
       dtype=torch.float64)
	q_value: tensor([[-2.3812]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5784020887805004, distance: 0.7430292637745329 entropy 0.03264415264129639
epoch: 34, step: 39
	action: tensor([[ 0.4277,  0.0436, -0.5779,  0.0859, -0.0108,  0.4917, -0.1581]],
       dtype=torch.float64)
	q_value: tensor([[-1.9355]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7440404428237941, distance: 0.5789517464405212 entropy 0.03264415264129639
epoch: 34, step: 40
	action: tensor([[ 8.5015e-01,  2.5890e-02, -3.5078e-01,  1.9378e-01, -8.4754e-04,
          5.5843e-01, -4.6535e-01]], dtype=torch.float64)
	q_value: tensor([[-2.4066]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9320137097004156, distance: 0.2983783894561984 entropy 0.03264415264129639
epoch: 34, step: 41
	action: tensor([[ 0.3514,  0.2577, -0.8412, -0.1022, -0.1363,  0.2665, -0.4642]],
       dtype=torch.float64)
	q_value: tensor([[-3.1924]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7564710687797547, distance: 0.5647184471225908 entropy 0.03264415264129639
epoch: 34, step: 42
	action: tensor([[ 0.6800,  0.2231, -0.4651,  0.0326, -0.0796,  0.4964, -0.3109]],
       dtype=torch.float64)
	q_value: tensor([[-3.2305]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9245880367431814, distance: 0.3142511074802551 entropy 0.03264415264129639
epoch: 34, step: 43
	action: tensor([[ 0.2858, -0.1865, -0.4007,  0.0587, -0.4812,  0.5838, -0.1987]],
       dtype=torch.float64)
	q_value: tensor([[-3.1218]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.46598145735451835, distance: 0.8362475567097292 entropy 0.03264415264129639
epoch: 34, step: 44
	action: tensor([[ 0.5358,  0.0821, -0.0220, -0.0827,  0.3815,  0.6352, -0.4808]],
       dtype=torch.float64)
	q_value: tensor([[-2.5914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8432572146512947, distance: 0.4530545259135725 entropy 0.03264415264129639
epoch: 34, step: 45
	action: tensor([[ 0.6214,  0.1739, -0.2418, -0.0147, -0.0928,  0.2386, -0.4597]],
       dtype=torch.float64)
	q_value: tensor([[-2.3736]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8629484592983672, distance: 0.4236416533956527 entropy 0.03264415264129639
epoch: 34, step: 46
	action: tensor([[ 0.2738,  0.3430, -0.1966, -0.2493, -0.0059,  0.1854, -0.2235]],
       dtype=torch.float64)
	q_value: tensor([[-2.5687]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.725278827772525, distance: 0.5997948862827636 entropy 0.03264415264129639
epoch: 34, step: 47
	action: tensor([[ 0.6071,  0.0488, -0.4704, -0.0360,  0.2583, -0.0617, -0.3657]],
       dtype=torch.float64)
	q_value: tensor([[-2.2470]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7363285481091613, distance: 0.5876087424127571 entropy 0.03264415264129639
epoch: 34, step: 48
	action: tensor([[ 0.2536,  0.2198, -0.2120, -0.1710, -0.3713,  0.0830, -0.2100]],
       dtype=torch.float64)
	q_value: tensor([[-2.4063]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6428816535888672, distance: 0.6838530280128142 entropy 0.03264415264129639
epoch: 34, step: 49
	action: tensor([[ 0.4007,  0.0193, -0.0011,  0.2709,  0.2407, -0.0205, -0.1071]],
       dtype=torch.float64)
	q_value: tensor([[-2.4482]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7502705962306844, distance: 0.5718623879213429 entropy 0.03264415264129639
epoch: 34, step: 50
	action: tensor([[ 0.7076,  0.3471,  0.0392,  0.0227, -0.0913,  0.3076, -0.4182]],
       dtype=torch.float64)
	q_value: tensor([[-1.7852]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9588805788001624, distance: 0.23204926101628343 entropy 0.03264415264129639
epoch: 34, step: 51
	action: tensor([[ 0.6104,  0.1840, -0.3440,  0.4389,  0.0366,  0.3837,  0.1675]],
       dtype=torch.float64)
	q_value: tensor([[-2.8931]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9309192272536405, distance: 0.3007705339227807 entropy 0.03264415264129639
epoch: 34, step: 52
	action: tensor([[ 0.0220, -0.4703, -0.3775,  0.1920, -0.0507,  0.1666, -0.3311]],
       dtype=torch.float64)
	q_value: tensor([[-2.5602]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.04630025772965063, distance: 1.1175385825696238 entropy 0.03264415264129639
epoch: 34, step: 53
	action: tensor([[ 5.7568e-01,  3.2528e-01, -3.1072e-01,  4.1122e-04, -1.5169e-01,
          3.9271e-01, -4.7474e-01]], dtype=torch.float64)
	q_value: tensor([[-1.5451]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8962794137632496, distance: 0.36854385531145106 entropy 0.03264415264129639
epoch: 34, step: 54
	action: tensor([[ 0.6448,  0.4470, -0.1956, -0.2263,  0.3417, -0.0183, -0.1813]],
       dtype=torch.float64)
	q_value: tensor([[-2.9414]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9025067180403512, distance: 0.35730906831983095 entropy 0.03264415264129639
epoch: 34, step: 55
	action: tensor([[ 0.5030, -0.1926, -0.1518,  0.3455, -0.0064,  0.2719,  0.1095]],
       dtype=torch.float64)
	q_value: tensor([[-2.8682]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7482981177197452, distance: 0.5741163629598703 entropy 0.03264415264129639
epoch: 34, step: 56
	action: tensor([[ 0.5365,  0.0007, -0.6098,  0.3770,  0.0060, -0.1255, -0.4602]],
       dtype=torch.float64)
	q_value: tensor([[-1.9186]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7704556483539333, distance: 0.5482643387376687 entropy 0.03264415264129639
epoch: 34, step: 57
	action: tensor([[ 0.4823,  0.0991, -0.3360, -0.0814, -0.1674, -0.3071, -0.3485]],
       dtype=torch.float64)
	q_value: tensor([[-2.5155]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6864667175014096, distance: 0.6407646109449066 entropy 0.03264415264129639
epoch: 34, step: 58
	action: tensor([[ 0.0817, -0.2581, -0.1971,  0.1774, -0.1900,  0.1048, -0.0685]],
       dtype=torch.float64)
	q_value: tensor([[-2.5097]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.2892748973619198, distance: 0.9647338581298269 entropy 0.03264415264129639
epoch: 34, step: 59
	action: tensor([[ 0.3289, -0.1169, -0.1553,  0.2242,  0.0728,  0.2616, -0.2725]],
       dtype=torch.float64)
	q_value: tensor([[-1.5318]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6436509633485259, distance: 0.6831160476114212 entropy 0.03264415264129639
epoch: 34, step: 60
	action: tensor([[ 0.5487,  0.2572, -0.2732,  0.3084, -0.3893,  0.0284,  0.1703]],
       dtype=torch.float64)
	q_value: tensor([[-1.6477]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.898333929566135, distance: 0.3648755075429049 entropy 0.03264415264129639
epoch: 34, step: 61
	action: tensor([[ 0.3917,  0.1592, -0.5796, -0.3306,  0.2128,  0.3212, -0.6181]],
       dtype=torch.float64)
	q_value: tensor([[-2.6383]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7318602988996161, distance: 0.592566715836719 entropy 0.03264415264129639
epoch: 34, step: 62
	action: tensor([[ 0.5956, -0.1099, -0.1792, -0.2086, -0.5105, -0.0297, -0.4438]],
       dtype=torch.float64)
	q_value: tensor([[-2.6249]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5268847891107044, distance: 0.7871185523126737 entropy 0.03264415264129639
epoch: 34, step: 63
	action: tensor([[ 0.3958,  0.1609, -0.2204,  0.3742, -0.0311, -0.3030,  0.1836]],
       dtype=torch.float64)
	q_value: tensor([[-2.7408]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8003697135693985, distance: 0.5112930721462298 entropy 0.03264415264129639
epoch: 34, step: 64
	action: tensor([[ 0.1759,  0.1129, -0.4370, -0.0303, -0.5284,  0.1983, -0.7699]],
       dtype=torch.float64)
	q_value: tensor([[-2.3431]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5334695336892643, distance: 0.7816218624365923 entropy 0.03264415264129639
epoch: 34, step: 65
	action: tensor([[ 0.3877,  0.2231, -0.2616, -0.3306, -0.0971,  0.6698,  0.1050]],
       dtype=torch.float64)
	q_value: tensor([[-3.0703]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7600653440802896, distance: 0.5605355792209799 entropy 0.03264415264129639
epoch: 34, step: 66
	action: tensor([[ 0.2123,  0.2524, -0.1681,  0.0299, -0.0850,  0.1597, -0.1719]],
       dtype=torch.float64)
	q_value: tensor([[-3.0641]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6946951647754178, distance: 0.6323005134666503 entropy 0.03264415264129639
epoch: 34, step: 67
	action: tensor([[ 0.1343,  0.3565, -0.2439,  0.1477,  0.2010,  0.2471, -0.1828]],
       dtype=torch.float64)
	q_value: tensor([[-1.8942]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7046641073100094, distance: 0.6218917688012934 entropy 0.03264415264129639
epoch: 34, step: 68
	action: tensor([[ 0.7786,  0.1197, -0.5150,  0.1988, -0.0101,  0.0947, -0.1325]],
       dtype=torch.float64)
	q_value: tensor([[-1.9640]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8991701269409793, distance: 0.363371869665955 entropy 0.03264415264129639
epoch: 34, step: 69
	action: tensor([[ 0.7183,  0.1709, -0.2253, -0.2473,  0.2449, -0.0957, -0.3478]],
       dtype=torch.float64)
	q_value: tensor([[-2.8372]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7430136241936127, distance: 0.5801118582755727 entropy 0.03264415264129639
epoch: 34, step: 70
	action: tensor([[ 0.1449, -0.1144, -0.3982, -0.1136, -0.3393,  0.3165, -0.3219]],
       dtype=torch.float64)
	q_value: tensor([[-2.7350]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3532336254988332, distance: 0.9203020928594337 entropy 0.03264415264129639
epoch: 34, step: 71
	action: tensor([[ 0.0953,  0.2244, -0.3187, -0.3194,  0.1037,  0.4249, -0.1411]],
       dtype=torch.float64)
	q_value: tensor([[-2.1749]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5516998830708639, distance: 0.7661981858226524 entropy 0.03264415264129639
epoch: 34, step: 72
	action: tensor([[ 0.2964,  0.2021, -0.1836, -0.0868, -0.1349, -0.1788, -0.4438]],
       dtype=torch.float64)
	q_value: tensor([[-2.3509]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6593074726608747, distance: 0.6679408382323007 entropy 0.03264415264129639
epoch: 34, step: 73
	action: tensor([[ 0.1685,  0.3868, -0.3662, -0.5468, -0.3804,  0.3263, -0.4309]],
       dtype=torch.float64)
	q_value: tensor([[-2.2032]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6215191238628506, distance: 0.7040097331865286 entropy 0.03264415264129639
epoch: 34, step: 74
	action: tensor([[ 0.5594,  0.3773, -0.3869, -0.1165,  0.0455,  0.1642, -0.7760]],
       dtype=torch.float64)
	q_value: tensor([[-3.6457]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9022360742166378, distance: 0.35780467412317607 entropy 0.03264415264129639
epoch: 34, step: 75
	action: tensor([[ 0.3841,  0.2007, -0.1155, -0.1809, -0.3564, -0.0561, -0.2691]],
       dtype=torch.float64)
	q_value: tensor([[-3.1171]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.692103581610297, distance: 0.6349784871089387 entropy 0.03264415264129639
epoch: 34, step: 76
	action: tensor([[ 0.0648,  0.0556, -0.2109, -0.1172,  0.0258,  0.5789, -0.4108]],
       dtype=torch.float64)
	q_value: tensor([[-2.4548]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5028254298550315, distance: 0.8068840798225472 entropy 0.03264415264129639
epoch: 34, step: 77
	action: tensor([[ 0.2969, -0.1087,  0.1840,  0.2660,  0.0719, -0.3014, -0.2696]],
       dtype=torch.float64)
	q_value: tensor([[-2.1229]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5514323735102662, distance: 0.7664267545478001 entropy 0.03264415264129639
epoch: 34, step: 78
	action: tensor([[ 0.6452,  0.5494,  0.3269,  0.0074, -0.2495,  0.5473, -0.5051]],
       dtype=torch.float64)
	q_value: tensor([[-1.8754]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9669778479227252, distance: 0.20795021792884957 entropy 0.03264415264129639
epoch: 34, step: 79
	action: tensor([[ 0.3862,  0.0789, -0.1312, -0.2126, -0.3396,  0.2613, -0.4597]],
       dtype=torch.float64)
	q_value: tensor([[-3.6445]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6386253498875999, distance: 0.6879161975113836 entropy 0.03264415264129639
epoch: 34, step: 80
	action: tensor([[ 0.6409, -0.2364, -0.0715, -0.0303, -0.2088, -0.2660, -0.2594]],
       dtype=torch.float64)
	q_value: tensor([[-2.5853]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4453536755461659, distance: 0.8522455906237961 entropy 0.03264415264129639
epoch: 34, step: 81
	action: tensor([[ 0.2153,  0.0617, -0.4470,  0.5052,  0.1756, -0.1694, -0.4335]],
       dtype=torch.float64)
	q_value: tensor([[-2.3765]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6177989387512414, distance: 0.7074612182021044 entropy 0.03264415264129639
epoch: 34, step: 82
	action: tensor([[ 0.5414,  0.6211, -0.0608, -0.1743, -0.0133,  0.0653, -0.1634]],
       dtype=torch.float64)
	q_value: tensor([[-2.0642]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9310981137292988, distance: 0.30038085488907423 entropy 0.03264415264129639
epoch: 34, step: 83
	action: tensor([[ 0.0524,  0.4374, -0.5268,  0.0938, -0.1396,  0.2406, -0.3849]],
       dtype=torch.float64)
	q_value: tensor([[-3.0034]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5946057377789172, distance: 0.7286106115575158 entropy 0.03264415264129639
epoch: 34, step: 84
	action: tensor([[ 0.4722,  0.0016, -0.7615, -0.3745,  0.2232,  0.0390, -0.9532]],
       dtype=torch.float64)
	q_value: tensor([[-2.5904]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6103193536798712, distance: 0.7143501030399163 entropy 0.03264415264129639
epoch: 34, step: 85
	action: tensor([[ 0.9678,  0.2494, -0.3318, -0.0930, -0.4573,  0.0648, -0.3928]],
       dtype=torch.float64)
	q_value: tensor([[-3.1276]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8706734976261947, distance: 0.4115290137578463 entropy 0.03264415264129639
epoch: 34, step: 86
	action: tensor([[ 0.2578,  0.6920, -0.2512, -0.3218,  0.3068, -0.0771, -0.7640]],
       dtype=torch.float64)
	q_value: tensor([[-3.9306]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8230625303727364, distance: 0.4813562325084918 entropy 0.03264415264129639
epoch: 34, step: 87
	action: tensor([[ 0.0998, -0.0709, -0.1444, -0.2548, -0.4249, -0.1036, -0.4245]],
       dtype=torch.float64)
	q_value: tensor([[-3.2574]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.24622626053922003, distance: 0.9935213285476391 entropy 0.03264415264129639
epoch: 34, step: 88
	action: tensor([[ 0.0154,  0.0989, -0.1214,  0.0143, -0.3408,  0.5354, -0.2034]],
       dtype=torch.float64)
	q_value: tensor([[-2.2384]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44079184727977927, distance: 0.8557431678341229 entropy 0.03264415264129639
epoch: 34, step: 89
	action: tensor([[ 0.5915, -0.0661, -0.7540, -0.4133, -0.5739,  0.1342, -0.0133]],
       dtype=torch.float64)
	q_value: tensor([[-2.3790]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5555044063133738, distance: 0.7629400673264011 entropy 0.03264415264129639
epoch: 34, step: 90
	action: tensor([[ 0.2307,  0.2551, -0.2011, -0.1450,  0.2740,  0.1835, -0.5051]],
       dtype=torch.float64)
	q_value: tensor([[-3.8573]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6856765168121775, distance: 0.6415715651634751 entropy 0.03264415264129639
epoch: 34, step: 91
	action: tensor([[ 0.3961, -0.0210, -0.2271, -0.4363,  0.0367, -0.2078, -0.3123]],
       dtype=torch.float64)
	q_value: tensor([[-1.9584]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3802668051640664, distance: 0.9008636693341555 entropy 0.03264415264129639
epoch: 34, step: 92
	action: tensor([[ 0.4575, -0.0452,  0.0217,  0.0528, -0.0563, -0.0179, -0.2462]],
       dtype=torch.float64)
	q_value: tensor([[-2.2496]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6484484953389811, distance: 0.6785020650797134 entropy 0.03264415264129639
epoch: 34, step: 93
	action: tensor([[ 0.1707,  0.3471, -0.7343,  0.0038, -0.2156, -0.0574, -0.3834]],
       dtype=torch.float64)
	q_value: tensor([[-1.8914]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6834811724355874, distance: 0.6438081460143559 entropy 0.03264415264129639
epoch: 34, step: 94
	action: tensor([[ 0.5893,  0.0926, -0.1821,  0.2238, -0.4586,  0.1684, -0.2589]],
       dtype=torch.float64)
	q_value: tensor([[-2.9127]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8698186716897248, distance: 0.41288684169993467 entropy 0.03264415264129639
epoch: 34, step: 95
	action: tensor([[-0.1394,  0.3056, -0.1694,  0.2812,  0.2761,  0.3718, -0.4054]],
       dtype=torch.float64)
	q_value: tensor([[-2.6285]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5146261005849188, distance: 0.7972506879449686 entropy 0.03264415264129639
epoch: 34, step: 96
	action: tensor([[ 0.0855,  0.4011, -0.3660,  0.0682, -0.1986, -0.1252, -0.6505]],
       dtype=torch.float64)
	q_value: tensor([[-2.0456]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6393827377315964, distance: 0.6871949340707039 entropy 0.03264415264129639
epoch: 34, step: 97
	action: tensor([[ 0.5917,  0.4508, -0.4663, -0.1518, -0.1271, -0.0135, -0.0621]],
       dtype=torch.float64)
	q_value: tensor([[-2.6564]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9255301778989413, distance: 0.3122819276116331 entropy 0.03264415264129639
epoch: 34, step: 98
	action: tensor([[ 0.7755,  0.0203, -0.4167, -0.0810,  0.0150, -0.2265, -0.4872]],
       dtype=torch.float64)
	q_value: tensor([[-3.2331]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6927441533562955, distance: 0.6343176137884836 entropy 0.03264415264129639
epoch: 34, step: 99
	action: tensor([[-0.0865,  0.0328, -0.4182, -0.1682,  0.3368,  0.3533, -0.4742]],
       dtype=torch.float64)
	q_value: tensor([[-2.9956]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.31149643752525336, distance: 0.9495324017300387 entropy 0.03264415264129639
epoch: 34, step: 100
	action: tensor([[-0.1355,  0.6089, -0.2163,  0.5667,  0.2152,  0.0653, -0.4135]],
       dtype=torch.float64)
	q_value: tensor([[-2.0167]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 34, step: 101
	action: tensor([[ 0.4503,  0.1379, -0.2110,  0.0308, -0.0574,  0.0365, -0.0478]],
       dtype=torch.float64)
	q_value: tensor([[-9.1491]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.743613783230823, distance: 0.5794340735458462 entropy 0.03264415264129639
epoch: 34, step: 102
	action: tensor([[ 0.5698,  0.4937, -0.5334,  0.0621, -0.1311, -0.3001,  0.4376]],
       dtype=torch.float64)
	q_value: tensor([[-1.9877]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9290478770811286, distance: 0.3048171445509655 entropy 0.03264415264129639
epoch: 34, step: 103
	action: tensor([[ 0.4939,  0.1019, -0.2902, -0.1679,  0.1045,  0.4116, -0.1762]],
       dtype=torch.float64)
	q_value: tensor([[-3.6603]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.762133159785693, distance: 0.5581149359683504 entropy 0.03264415264129639
epoch: 34, step: 104
	action: tensor([[ 0.6548,  0.6157, -0.6966,  0.0235,  0.1611,  0.5783, -0.5764]],
       dtype=torch.float64)
	q_value: tensor([[-2.2411]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.934776814506684, distance: 0.29225213748931 entropy 0.03264415264129639
epoch: 34, step: 105
	action: tensor([[ 0.5508, -0.0772, -0.4179,  0.2649, -0.2780,  0.1093, -0.1388]],
       dtype=torch.float64)
	q_value: tensor([[-4.0934]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7522459198052899, distance: 0.5695962232506714 entropy 0.03264415264129639
epoch: 34, step: 106
	action: tensor([[ 0.4366, -0.3899, -0.9164, -0.2674,  0.3210, -0.1260,  0.0505]],
       dtype=torch.float64)
	q_value: tensor([[-2.2202]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.23814103010866505, distance: 0.9988355378776073 entropy 0.03264415264129639
epoch: 34, step: 107
	action: tensor([[ 0.1575,  0.1543, -0.5277, -0.0286, -0.1896,  0.4017, -0.3671]],
       dtype=torch.float64)
	q_value: tensor([[-2.8559]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5635830141177609, distance: 0.7559751433547454 entropy 0.03264415264129639
epoch: 34, step: 108
	action: tensor([[ 0.0942, -0.0192, -0.4696,  0.2574,  0.2723,  0.0703, -0.5703]],
       dtype=torch.float64)
	q_value: tensor([[-2.4316]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4579209707365044, distance: 0.8425350882547569 entropy 0.03264415264129639
epoch: 34, step: 109
	action: tensor([[ 0.1937, -0.3425, -1.0726,  0.1410, -0.1134,  0.0657, -0.6337]],
       dtype=torch.float64)
	q_value: tensor([[-1.8621]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.173222572392295, distance: 1.0405213574581365 entropy 0.03264415264129639
epoch: 34, step: 110
	action: tensor([[ 0.5118, -0.1980,  0.0095,  0.0039, -0.2241, -0.3624, -0.5072]],
       dtype=torch.float64)
	q_value: tensor([[-2.6580]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.44367010750716185, distance: 0.8535380595532871 entropy 0.03264415264129639
epoch: 34, step: 111
	action: tensor([[ 0.7661, -0.0874, -0.2747,  0.2518,  0.1159,  0.1406, -0.9159]],
       dtype=torch.float64)
	q_value: tensor([[-2.4416]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8081242043259825, distance: 0.5012643171440027 entropy 0.03264415264129639
epoch: 34, step: 112
	action: tensor([[ 0.1271,  0.2020, -0.6721, -0.5530,  0.0302,  0.2150, -0.4313]],
       dtype=torch.float64)
	q_value: tensor([[-3.0295]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5675232735646685, distance: 0.7525546850282597 entropy 0.03264415264129639
epoch: 34, step: 113
	action: tensor([[ 0.4257,  0.1319, -0.5552,  0.2244,  0.2069,  0.0427, -0.1930]],
       dtype=torch.float64)
	q_value: tensor([[-3.1888]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7714531142588652, distance: 0.5470718232714252 entropy 0.03264415264129639
epoch: 34, step: 114
	action: tensor([[ 0.0955,  0.1656, -0.2852, -0.0437, -0.0704,  0.2707, -0.2230]],
       dtype=torch.float64)
	q_value: tensor([[-2.1707]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5449190936747887, distance: 0.7719710259217787 entropy 0.03264415264129639
epoch: 34, step: 115
	action: tensor([[ 0.1114,  0.3366, -0.3242,  0.1343, -0.3463, -0.1734,  0.0311]],
       dtype=torch.float64)
	q_value: tensor([[-1.8989]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6549668108613269, distance: 0.672182388336473 entropy 0.03264415264129639
epoch: 34, step: 116
	action: tensor([[ 0.4356, -0.0968,  0.0774,  0.4187, -0.1078, -0.1161, -0.7695]],
       dtype=torch.float64)
	q_value: tensor([[-2.4573]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7649669862704489, distance: 0.554780423775389 entropy 0.03264415264129639
epoch: 34, step: 117
	action: tensor([[ 0.4618,  0.0724,  0.1820, -0.0884, -0.5618,  0.0468, -0.1812]],
       dtype=torch.float64)
	q_value: tensor([[-2.4188]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6848726649175343, distance: 0.6423914199234316 entropy 0.03264415264129639
epoch: 34, step: 118
	action: tensor([[ 0.7960, -0.3148, -0.7987, -0.0359, -0.2930,  0.2605, -0.4904]],
       dtype=torch.float64)
	q_value: tensor([[-2.7135]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.511609997279947, distance: 0.7997239013586983 entropy 0.03264415264129639
epoch: 34, step: 119
	action: tensor([[ 0.7399,  0.0134, -0.1790,  0.1427, -0.0246,  0.4642, -0.3928]],
       dtype=torch.float64)
	q_value: tensor([[-3.1942]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8849796560912165, distance: 0.38810034085708345 entropy 0.03264415264129639
epoch: 34, step: 120
	action: tensor([[-0.0078,  0.1155, -0.5699, -0.0540,  0.1917, -0.1778, -0.3223]],
       dtype=torch.float64)
	q_value: tensor([[-2.7001]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.3795377453059494, distance: 0.9013934057565366 entropy 0.03264415264129639
epoch: 34, step: 121
	action: tensor([[ 0.4565, -0.1606, -0.3248, -0.0641,  0.3721,  0.3147,  0.1123]],
       dtype=torch.float64)
	q_value: tensor([[-2.1028]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5783336373399537, distance: 0.7430895811501631 entropy 0.03264415264129639
epoch: 34, step: 122
	action: tensor([[ 0.4126, -0.0331, -0.4257, -0.1534,  0.0613,  0.2140, -0.3969]],
       dtype=torch.float64)
	q_value: tensor([[-2.0030]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5978716472836222, distance: 0.72566978519276 entropy 0.03264415264129639
epoch: 34, step: 123
	action: tensor([[ 0.6533,  0.0284, -0.2491,  0.7037, -0.2037, -0.2175, -0.4151]],
       dtype=torch.float64)
	q_value: tensor([[-1.9482]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9226466450078926, distance: 0.31827041518186583 entropy 0.03264415264129639
epoch: 34, step: 124
	action: tensor([[ 0.4193,  0.0767, -0.4244, -0.3703,  0.0853,  0.2198, -0.5681]],
       dtype=torch.float64)
	q_value: tensor([[-2.9396]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6331702223916165, distance: 0.6930889653803202 entropy 0.03264415264129639
epoch: 34, step: 125
	action: tensor([[ 0.7604, -0.1198, -0.2655,  0.0577, -0.0893,  0.5031, -0.6025]],
       dtype=torch.float64)
	q_value: tensor([[-2.4125]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7968677824733892, distance: 0.5157581484006618 entropy 0.03264415264129639
epoch: 34, step: 126
	action: tensor([[ 0.6310, -0.0331, -0.8825,  0.0813, -0.1402,  0.4019, -0.2938]],
       dtype=torch.float64)
	q_value: tensor([[-2.9349]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7454118982714645, distance: 0.5773986241722685 entropy 0.03264415264129639
epoch: 34, step: 127
	action: tensor([[ 0.4326, -0.0317, -0.4403,  0.0404, -0.0245,  0.3384, -0.5260]],
       dtype=torch.float64)
	q_value: tensor([[-3.2054]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.696069772839789, distance: 0.6308754690057112 entropy 0.03264415264129639
LOSS epoch 34 actor 14.620943986583272 critic 177.44536470910165 
epoch: 35, step: 0
	action: tensor([[ 1.1278,  0.0055, -0.4085,  0.1133, -0.0782,  0.3182, -0.7438]],
       dtype=torch.float64)
	q_value: tensor([[-1.8615]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8344781310507842, distance: 0.4655693526415323 entropy 0.03264415264129639
epoch: 35, step: 1
	action: tensor([[ 0.9439,  0.5919, -0.2203, -0.0485, -0.1609,  0.2474, -0.6713]],
       dtype=torch.float64)
	q_value: tensor([[-3.6968]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.984932880890721, distance: 0.14046619039497008 entropy 0.03264415264129639
epoch: 35, step: 2
	action: tensor([[ 0.3116, -0.0769, -0.4844,  0.0554, -0.4984, -0.2485, -0.5923]],
       dtype=torch.float64)
	q_value: tensor([[-3.9385]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5429859297226032, distance: 0.7736089380382263 entropy 0.03264415264129639
epoch: 35, step: 3
	action: tensor([[ 0.5474,  0.1137,  0.0942,  0.2801, -0.3925, -0.1441, -0.7383]],
       dtype=torch.float64)
	q_value: tensor([[-2.2701]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8621504802888493, distance: 0.424873184578709 entropy 0.03264415264129639
epoch: 35, step: 4
	action: tensor([[ 0.5411,  0.5324, -0.3160, -0.2735, -0.1093,  0.5224, -0.7020]],
       dtype=torch.float64)
	q_value: tensor([[-2.6843]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9323765762783959, distance: 0.2975810492700566 entropy 0.03264415264129639
epoch: 35, step: 5
	action: tensor([[ 1.1956,  0.4591, -0.2848, -0.0908,  0.0515, -0.0834, -0.8902]],
       dtype=torch.float64)
	q_value: tensor([[-3.4513]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8630747823521123, distance: 0.42344636903628524 entropy 0.03264415264129639
epoch: 35, step: 6
	action: tensor([[ 0.5961,  0.0189, -0.6639,  0.5937,  0.0269,  0.3466, -0.6344]],
       dtype=torch.float64)
	q_value: tensor([[-4.4543]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.826617129653227, distance: 0.4764965795451541 entropy 0.03264415264129639
epoch: 35, step: 7
	action: tensor([[ 1.3952,  0.0733, -0.7305, -0.0641,  0.0243,  0.5303, -0.2428]],
       dtype=torch.float64)
	q_value: tensor([[-2.6178]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7181284744511107, distance: 0.6075503791959425 entropy 0.03264415264129639
epoch: 35, step: 8
	action: tensor([[ 0.9087,  0.2689, -0.5880,  0.3264, -0.4564,  0.2740, -0.0181]],
       dtype=torch.float64)
	q_value: tensor([[-4.5962]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9775627384918698, distance: 0.17141215626644332 entropy 0.03264415264129639
epoch: 35, step: 9
	action: tensor([[ 0.5679,  0.0444, -0.8695, -0.2745, -0.3302,  0.2103, -0.6579]],
       dtype=torch.float64)
	q_value: tensor([[-3.4336]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7035344087087005, distance: 0.6230800424311707 entropy 0.03264415264129639
epoch: 35, step: 10
	action: tensor([[ 0.4159,  0.1741,  0.0306,  0.2413,  0.0056,  0.4042, -0.4462]],
       dtype=torch.float64)
	q_value: tensor([[-3.3445]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.860282183124687, distance: 0.42774268260609033 entropy 0.03264415264129639
epoch: 35, step: 11
	action: tensor([[ 0.8038,  0.3517, -0.5696, -0.0061, -0.4074,  0.1694, -0.5619]],
       dtype=torch.float64)
	q_value: tensor([[-1.9327]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9536465572454053, distance: 0.24637559778832277 entropy 0.03264415264129639
epoch: 35, step: 12
	action: tensor([[ 0.6456,  0.0717, -0.5702,  0.4354,  0.0832,  0.0683, -0.8117]],
       dtype=torch.float64)
	q_value: tensor([[-3.5472]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8761416566594207, distance: 0.4027349561724975 entropy 0.03264415264129639
epoch: 35, step: 13
	action: tensor([[ 0.8204,  0.3794, -0.5267,  0.0482, -0.1950,  0.3463, -0.4478]],
       dtype=torch.float64)
	q_value: tensor([[-2.7333]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9776857935115219, distance: 0.17094146320438125 entropy 0.03264415264129639
epoch: 35, step: 14
	action: tensor([[ 0.9730,  0.0889, -0.5455, -0.0522,  0.0205,  0.7739, -0.4704]],
       dtype=torch.float64)
	q_value: tensor([[-3.3822]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.926891111919749, distance: 0.3094152983627748 entropy 0.03264415264129639
epoch: 35, step: 15
	action: tensor([[ 0.2281,  0.3677, -0.5116, -0.0638, -0.1635,  0.7868, -0.2312]],
       dtype=torch.float64)
	q_value: tensor([[-3.6124]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7121598770623344, distance: 0.6139490886264273 entropy 0.03264415264129639
epoch: 35, step: 16
	action: tensor([[ 0.2836,  0.3969, -0.1573, -0.1202, -0.1015,  0.5233, -0.6280]],
       dtype=torch.float64)
	q_value: tensor([[-3.1274]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7974881413883723, distance: 0.5149699922282324 entropy 0.03264415264129639
epoch: 35, step: 17
	action: tensor([[ 0.4336, -0.0503, -0.6667,  0.4467,  0.2324,  0.1217, -0.4270]],
       dtype=torch.float64)
	q_value: tensor([[-2.5798]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6987816299285923, distance: 0.6280546282530174 entropy 0.03264415264129639
epoch: 35, step: 18
	action: tensor([[ 0.9530,  0.1573, -0.8519,  0.2567,  0.2292,  0.1562, -0.6459]],
       dtype=torch.float64)
	q_value: tensor([[-1.9855]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9361070684320232, distance: 0.28925648260582026 entropy 0.03264415264129639
epoch: 35, step: 19
	action: tensor([[ 0.4319,  0.1993, -1.3120,  0.1590,  0.2219,  0.1536, -0.4187]],
       dtype=torch.float64)
	q_value: tensor([[-3.5026]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7292009358712794, distance: 0.5954979528265426 entropy 0.03264415264129639
epoch: 35, step: 20
	action: tensor([[ 0.6570, -0.0934, -0.5883, -0.0645, -0.0237,  0.3095, -0.3577]],
       dtype=torch.float64)
	q_value: tensor([[-3.4471]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.70312755558931, distance: 0.6235074362888117 entropy 0.03264415264129639
epoch: 35, step: 21
	action: tensor([[ 0.8411,  0.6573, -0.1187,  0.1599,  0.1575,  0.0491, -0.3419]],
       dtype=torch.float64)
	q_value: tensor([[-2.2614]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9755414123480862, distance: 0.1789667621416887 entropy 0.03264415264129639
epoch: 35, step: 22
	action: tensor([[ 0.1741, -0.1672, -0.6639,  0.1704,  0.3517,  0.2022, -0.4366]],
       dtype=torch.float64)
	q_value: tensor([[-3.1278]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4113207032685704, distance: 0.8780031508801982 entropy 0.03264415264129639
epoch: 35, step: 23
	action: tensor([[-0.2257, -0.0832, -0.3593,  0.2285, -0.1861,  0.0595, -0.3640]],
       dtype=torch.float64)
	q_value: tensor([[-1.6623]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.07298586743728597, distance: 1.1017926498476287 entropy 0.03264415264129639
epoch: 35, step: 24
	action: tensor([[ 0.4511,  0.7927,  0.0875,  0.0376, -0.1879, -0.4416, -0.3921]],
       dtype=torch.float64)
	q_value: tensor([[-1.6063]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.908520175927285, distance: 0.3461141491628922 entropy 0.03264415264129639
epoch: 35, step: 25
	action: tensor([[ 0.3638,  0.0080, -0.0769,  0.2217, -0.3194,  0.2486, -0.4631]],
       dtype=torch.float64)
	q_value: tensor([[-3.2030]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.729357100420496, distance: 0.5953262220254648 entropy 0.03264415264129639
epoch: 35, step: 26
	action: tensor([[ 0.9106,  0.0648, -0.1798,  0.2172, -0.0899,  0.1325, -0.7709]],
       dtype=torch.float64)
	q_value: tensor([[-1.9035]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9026299663146578, distance: 0.35708314684197134 entropy 0.03264415264129639
epoch: 35, step: 27
	action: tensor([[ 0.8982,  0.5568, -0.7482,  0.3132,  0.2270,  0.4657, -0.5306]],
       dtype=torch.float64)
	q_value: tensor([[-3.1020]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.957627734878809, distance: 0.23555782397406955 entropy 0.03264415264129639
epoch: 35, step: 28
	action: tensor([[ 0.9073,  0.3011, -0.4555,  0.1641, -0.4280, -0.2335, -0.6654]],
       dtype=torch.float64)
	q_value: tensor([[-3.9206]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9455021197100838, distance: 0.26714466070816345 entropy 0.03264415264129639
epoch: 35, step: 29
	action: tensor([[ 0.6645,  0.5902, -0.2266, -0.3805, -0.0126,  0.6015, -0.8027]],
       dtype=torch.float64)
	q_value: tensor([[-3.7138]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9749630999728693, distance: 0.18107019565426274 entropy 0.03264415264129639
epoch: 35, step: 30
	action: tensor([[ 0.6190, -0.1380, -0.5150,  0.6444, -0.0959,  0.6408, -0.3332]],
       dtype=torch.float64)
	q_value: tensor([[-3.8559]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8435419704370133, distance: 0.45264280506847215 entropy 0.03264415264129639
epoch: 35, step: 31
	action: tensor([[ 1.0266,  0.2772, -1.0186,  0.2726, -0.2411,  0.2495, -1.0112]],
       dtype=torch.float64)
	q_value: tensor([[-2.5160]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9620873639530028, distance: 0.2228171854905799 entropy 0.03264415264129639
epoch: 35, step: 32
	action: tensor([[ 0.6556,  0.1811, -0.7961,  0.6894,  0.1317,  0.1716, -0.8924]],
       dtype=torch.float64)
	q_value: tensor([[-4.8298]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.838277393293116, distance: 0.46019517370126883 entropy 0.03264415264129639
epoch: 35, step: 33
	action: tensor([[ 0.5260,  0.5310, -0.5919,  0.0008, -0.4965,  0.3306, -0.7457]],
       dtype=torch.float64)
	q_value: tensor([[-3.3513]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8943083983381012, distance: 0.3720291182708334 entropy 0.03264415264129639
epoch: 35, step: 34
	action: tensor([[ 0.4740,  0.1172, -0.3934,  0.0697, -0.3128,  0.5046,  0.1175]],
       dtype=torch.float64)
	q_value: tensor([[-3.8526]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7959053626141472, distance: 0.5169785095394941 entropy 0.03264415264129639
epoch: 35, step: 35
	action: tensor([[ 0.4808,  0.2205, -0.1490,  0.3824, -0.4005,  0.5689, -0.7578]],
       dtype=torch.float64)
	q_value: tensor([[-2.4006]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8726855375883082, distance: 0.4083152152499127 entropy 0.03264415264129639
epoch: 35, step: 36
	action: tensor([[ 1.0994,  0.4390, -0.2807,  0.0057, -0.0801, -0.1415, -0.6005]],
       dtype=torch.float64)
	q_value: tensor([[-2.9390]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9109763039014956, distance: 0.34143615257069326 entropy 0.03264415264129639
epoch: 35, step: 37
	action: tensor([[ 0.4534,  0.0816,  0.0113, -0.0020, -0.1624,  0.1837, -0.8657]],
       dtype=torch.float64)
	q_value: tensor([[-3.9605]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7571092205146127, distance: 0.563978057776919 entropy 0.03264415264129639
epoch: 35, step: 38
	action: tensor([[ 0.5179,  0.0207, -0.6136,  0.3354, -0.4473, -0.2566, -0.5031]],
       dtype=torch.float64)
	q_value: tensor([[-2.3736]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7789611084924024, distance: 0.538010857596013 entropy 0.03264415264129639
epoch: 35, step: 39
	action: tensor([[ 0.4166,  0.5231, -0.4517,  0.1168,  0.1072,  0.5086, -0.1540]],
       dtype=torch.float64)
	q_value: tensor([[-2.5643]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8482640997988653, distance: 0.445759765263805 entropy 0.03264415264129639
epoch: 35, step: 40
	action: tensor([[-0.1193,  0.0348, -0.5951,  0.2223,  0.2182, -0.2035, -0.5188]],
       dtype=torch.float64)
	q_value: tensor([[-2.5672]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.21291159739852472, distance: 1.015239367640334 entropy 0.03264415264129639
epoch: 35, step: 41
	action: tensor([[ 0.3967,  0.2263, -0.4246, -0.2346,  0.3843,  0.5433, -0.1690]],
       dtype=torch.float64)
	q_value: tensor([[-1.8116]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7952735697793915, distance: 0.5177780673817265 entropy 0.03264415264129639
epoch: 35, step: 42
	action: tensor([[ 1.0913,  0.3247, -0.3360,  0.0996, -0.0603,  0.1865, -0.2918]],
       dtype=torch.float64)
	q_value: tensor([[-2.2265]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9408239032967397, distance: 0.27837475702214265 entropy 0.03264415264129639
epoch: 35, step: 43
	action: tensor([[ 0.5429,  0.1623, -0.2935,  0.0927, -0.0755,  0.1184, -0.2085]],
       dtype=torch.float64)
	q_value: tensor([[-3.5519]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.84779657042479, distance: 0.446445975631257 entropy 0.03264415264129639
epoch: 35, step: 44
	action: tensor([[ 1.0682,  0.2939, -0.7796, -0.2124,  0.0131,  0.1584, -0.6774]],
       dtype=torch.float64)
	q_value: tensor([[-1.8958]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8504239302920494, distance: 0.4425758906158985 entropy 0.03264415264129639
epoch: 35, step: 45
	action: tensor([[ 0.7677,  0.1898, -0.4509,  0.1670,  0.2710,  0.4142, -0.5673]],
       dtype=torch.float64)
	q_value: tensor([[-4.2525]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9606353810919767, distance: 0.22704383766687894 entropy 0.03264415264129639
epoch: 35, step: 46
	action: tensor([[ 0.6107, -0.0137, -0.3568, -0.1085, -0.3068,  0.0576, -0.8264]],
       dtype=torch.float64)
	q_value: tensor([[-2.6848]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6850097525208554, distance: 0.6422516772386767 entropy 0.03264415264129639
epoch: 35, step: 47
	action: tensor([[ 0.8792,  0.2696, -0.5390,  0.3908,  0.0145,  0.3663,  0.1448]],
       dtype=torch.float64)
	q_value: tensor([[-2.7013]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9802576912390047, distance: 0.1607887515656191 entropy 0.03264415264129639
epoch: 35, step: 48
	action: tensor([[ 0.6880,  0.5450, -0.3276,  0.4350, -0.2855,  0.2157, -1.2524]],
       dtype=torch.float64)
	q_value: tensor([[-3.0202]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9188937938870894, distance: 0.32589953196382554 entropy 0.03264415264129639
epoch: 35, step: 49
	action: tensor([[ 0.7551, -0.2007, -0.5191,  0.0142, -0.1523,  0.1654, -0.5081]],
       dtype=torch.float64)
	q_value: tensor([[-4.1746]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6359853716093519, distance: 0.6904243688562565 entropy 0.03264415264129639
epoch: 35, step: 50
	action: tensor([[ 0.8997,  0.1249, -0.5613, -0.1020, -0.2381,  0.5801, -0.7642]],
       dtype=torch.float64)
	q_value: tensor([[-2.4156]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8862188969373662, distance: 0.38600396270833254 entropy 0.03264415264129639
epoch: 35, step: 51
	action: tensor([[ 0.5706,  0.3336, -0.5606,  0.4901,  0.1001,  0.4666, -0.3440]],
       dtype=torch.float64)
	q_value: tensor([[-3.8393]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8587479112093388, distance: 0.43008483822194615 entropy 0.03264415264129639
epoch: 35, step: 52
	action: tensor([[ 0.4607,  0.8863, -0.4037,  0.1133,  0.0250,  0.5126, -0.3532]],
       dtype=torch.float64)
	q_value: tensor([[-2.6723]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 35, step: 53
	action: tensor([[ 0.9470,  0.4531, -0.4723,  0.2572, -0.1136,  0.3703, -0.4732]],
       dtype=torch.float64)
	q_value: tensor([[-8.5223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9904997976556359, distance: 0.11153806886239141 entropy 0.03264415264129639
epoch: 35, step: 54
	action: tensor([[ 0.7023,  0.8255, -0.5897, -0.1747, -0.5500,  0.3030, -0.6861]],
       dtype=torch.float64)
	q_value: tensor([[-3.6876]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9604843519905268, distance: 0.22747896698610529 entropy 0.03264415264129639
epoch: 35, step: 55
	action: tensor([[ 0.5662,  0.0713, -0.1862,  0.2688,  0.3604, -0.1846, -0.7954]],
       dtype=torch.float64)
	q_value: tensor([[-4.8896]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8191648909655905, distance: 0.486629092874436 entropy 0.03264415264129639
epoch: 35, step: 56
	action: tensor([[ 0.6767,  0.5685, -0.7617,  0.1085, -0.1303,  0.3916, -0.6604]],
       dtype=torch.float64)
	q_value: tensor([[-2.2779]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9361950703017462, distance: 0.28905721265718004 entropy 0.03264415264129639
epoch: 35, step: 57
	action: tensor([[ 0.4293,  0.2243, -0.4054,  0.4255,  0.0621,  0.2963, -0.5882]],
       dtype=torch.float64)
	q_value: tensor([[-3.8988]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8357369468193969, distance: 0.46379561548802195 entropy 0.03264415264129639
epoch: 35, step: 58
	action: tensor([[ 1.1767,  0.3189, -0.3345,  0.5500, -0.1335,  0.3296, -0.5051]],
       dtype=torch.float64)
	q_value: tensor([[-2.1940]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9762541986020188, distance: 0.1763397045700161 entropy 0.03264415264129639
epoch: 35, step: 59
	action: tensor([[ 0.8115, -0.0398, -0.5343,  0.1071,  0.1531,  0.1480, -0.5580]],
       dtype=torch.float64)
	q_value: tensor([[-4.0437]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8047100282501153, distance: 0.5057043210946827 entropy 0.03264415264129639
epoch: 35, step: 60
	action: tensor([[ 0.5929,  0.2099, -0.2954,  0.3584, -0.4177,  0.1387, -0.5892]],
       dtype=torch.float64)
	q_value: tensor([[-2.5722]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.913894649500638, distance: 0.3357930960372374 entropy 0.03264415264129639
epoch: 35, step: 61
	action: tensor([[ 0.3004,  0.6568, -1.0771,  0.2212,  0.1808, -0.0552, -0.3861]],
       dtype=torch.float64)
	q_value: tensor([[-2.6788]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7641982452827213, distance: 0.5556869651180251 entropy 0.03264415264129639
epoch: 35, step: 62
	action: tensor([[ 0.3586, -0.0735, -0.6842,  0.1841, -0.3303,  0.3849, -0.5075]],
       dtype=torch.float64)
	q_value: tensor([[-3.6405]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5758917729148778, distance: 0.7452380862018121 entropy 0.03264415264129639
epoch: 35, step: 63
	action: tensor([[ 0.5798,  0.0764, -0.6920, -0.2494, -0.6890,  0.1325, -0.4474]],
       dtype=torch.float64)
	q_value: tensor([[-2.3220]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7082758237177684, distance: 0.6180774576137981 entropy 0.03264415264129639
epoch: 35, step: 64
	action: tensor([[ 0.1960,  0.1334,  0.0467, -0.2087,  0.1450,  0.2839, -0.5817]],
       dtype=torch.float64)
	q_value: tensor([[-3.4443]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5737156970389251, distance: 0.7471475277441808 entropy 0.03264415264129639
epoch: 35, step: 65
	action: tensor([[ 0.9948,  0.8560, -0.4620,  0.4303,  0.3477,  0.0897, -0.5435]],
       dtype=torch.float64)
	q_value: tensor([[-1.7418]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 35, step: 66
	action: tensor([[ 0.3860,  0.2305, -0.8800,  0.3383, -0.1277,  0.3303, -1.1306]],
       dtype=torch.float64)
	q_value: tensor([[-8.5223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6855491352094648, distance: 0.6417015524861507 entropy 0.03264415264129639
epoch: 35, step: 67
	action: tensor([[ 0.5752,  0.3111, -0.6091,  0.3393,  0.0965,  0.2450, -0.4537]],
       dtype=torch.float64)
	q_value: tensor([[-3.4847]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.896783554024572, distance: 0.36764709931043094 entropy 0.03264415264129639
epoch: 35, step: 68
	action: tensor([[ 0.5346,  0.5712, -0.9393,  0.6396, -0.0249,  0.3026, -0.7939]],
       dtype=torch.float64)
	q_value: tensor([[-2.5477]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6933125669294828, distance: 0.6337306084418609 entropy 0.03264415264129639
epoch: 35, step: 69
	action: tensor([[ 0.7363,  0.1712, -0.2470,  0.2043, -0.0178,  0.2816, -0.1401]],
       dtype=torch.float64)
	q_value: tensor([[-3.8197]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.947412460242288, distance: 0.26242071832457775 entropy 0.03264415264129639
epoch: 35, step: 70
	action: tensor([[ 0.9247,  0.0450, -0.5152,  0.1378,  0.1415,  0.0270, -0.2655]],
       dtype=torch.float64)
	q_value: tensor([[-2.2703]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.841939550294657, distance: 0.45495485078067 entropy 0.03264415264129639
epoch: 35, step: 71
	action: tensor([[ 0.9737,  0.0081, -0.5486,  0.3686, -0.2058,  0.4583, -0.3739]],
       dtype=torch.float64)
	q_value: tensor([[-2.7905]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9446328389081099, distance: 0.2692668074964211 entropy 0.03264415264129639
epoch: 35, step: 72
	action: tensor([[ 0.4937,  0.3703, -0.4129,  0.2275, -0.0103,  0.2728, -0.4376]],
       dtype=torch.float64)
	q_value: tensor([[-3.2719]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8924973710485434, distance: 0.3752029425899798 entropy 0.03264415264129639
epoch: 35, step: 73
	action: tensor([[ 0.9856,  0.3157, -0.3397, -0.0735, -0.4773,  0.1755, -0.5148]],
       dtype=torch.float64)
	q_value: tensor([[-2.2941]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9245229121167347, distance: 0.31438676944015864 entropy 0.03264415264129639
epoch: 35, step: 74
	action: tensor([[ 0.5038,  0.1943, -0.3682,  0.1590, -0.2528,  0.7297, -0.9217]],
       dtype=torch.float64)
	q_value: tensor([[-3.8022]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8447194892658536, distance: 0.45093627053310736 entropy 0.03264415264129639
epoch: 35, step: 75
	action: tensor([[ 0.8843,  0.3027, -0.6768,  0.0856, -0.4618, -0.4894, -0.6121]],
       dtype=torch.float64)
	q_value: tensor([[-3.2961]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9161561708567905, distance: 0.33135402120733115 entropy 0.03264415264129639
epoch: 35, step: 76
	action: tensor([[ 0.3049,  0.5261, -0.8110,  0.0257, -0.1057,  0.0829, -0.2192]],
       dtype=torch.float64)
	q_value: tensor([[-3.9506]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8120492504729361, distance: 0.49611084891134366 entropy 0.03264415264129639
epoch: 35, step: 77
	action: tensor([[ 0.2061,  0.3631,  0.1611,  0.0913, -0.0555,  0.2529, -0.3932]],
       dtype=torch.float64)
	q_value: tensor([[-3.0569]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7771261725971116, distance: 0.5402393683040491 entropy 0.03264415264129639
epoch: 35, step: 78
	action: tensor([[ 0.1763,  0.1062, -0.4763, -0.0066, -0.6885,  0.0077, -0.4669]],
       dtype=torch.float64)
	q_value: tensor([[-1.9737]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5468662717482546, distance: 0.7703177194429967 entropy 0.03264415264129639
epoch: 35, step: 79
	action: tensor([[ 0.0752,  0.2647, -0.4756,  0.1305,  0.1274,  0.2267, -0.3209]],
       dtype=torch.float64)
	q_value: tensor([[-2.6515]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5771944084046663, distance: 0.7440927180036435 entropy 0.03264415264129639
epoch: 35, step: 80
	action: tensor([[ 0.6108,  0.5382, -0.4548,  0.4383, -0.0303,  0.3283, -0.6848]],
       dtype=torch.float64)
	q_value: tensor([[-1.7712]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8725373498089302, distance: 0.40855277556803854 entropy 0.03264415264129639
epoch: 35, step: 81
	action: tensor([[ 0.6468,  0.4206, -0.5212,  0.3593, -0.2312,  0.2531, -0.4884]],
       dtype=torch.float64)
	q_value: tensor([[-3.1349]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9208710510088934, distance: 0.3219025314137006 entropy 0.03264415264129639
epoch: 35, step: 82
	action: tensor([[ 0.5834,  0.7272,  0.0934,  0.0667,  0.0628, -0.3243, -0.8012]],
       dtype=torch.float64)
	q_value: tensor([[-3.0112]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9545759821853637, distance: 0.2438930731544084 entropy 0.03264415264129639
epoch: 35, step: 83
	action: tensor([[ 0.7616,  0.1243, -0.2507,  0.1490, -0.1399,  0.3392, -0.4049]],
       dtype=torch.float64)
	q_value: tensor([[-3.4240]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9299960242179217, distance: 0.3027736289664931 entropy 0.03264415264129639
epoch: 35, step: 84
	action: tensor([[ 0.5489,  0.3810, -0.3057,  0.2508,  0.1134,  0.3479, -0.5296]],
       dtype=torch.float64)
	q_value: tensor([[-2.5521]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9309185169039864, distance: 0.30077208031323793 entropy 0.03264415264129639
epoch: 35, step: 85
	action: tensor([[ 0.5225,  0.4897, -0.7427,  0.3475, -0.2418,  0.1932, -0.2673]],
       dtype=torch.float64)
	q_value: tensor([[-2.3420]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8450354344230356, distance: 0.45047728287721256 entropy 0.03264415264129639
epoch: 35, step: 86
	action: tensor([[-0.0970, -0.1868, -0.2364,  0.0584,  0.0809, -0.1714, -0.4599]],
       dtype=torch.float64)
	q_value: tensor([[-3.0646]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.0642481449650909, distance: 1.1069730342450945 entropy 0.03264415264129639
epoch: 35, step: 87
	action: tensor([[ 0.3114,  0.6234, -0.4357,  0.2916,  0.2941, -0.2375, -0.1291]],
       dtype=torch.float64)
	q_value: tensor([[-1.4122]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7846465709253364, distance: 0.5310465466250339 entropy 0.03264415264129639
epoch: 35, step: 88
	action: tensor([[ 0.5776,  0.3112, -0.8946, -0.3289, -0.2282,  0.1862, -0.6439]],
       dtype=torch.float64)
	q_value: tensor([[-2.5919]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8678326389805296, distance: 0.4160243996365332 entropy 0.03264415264129639
epoch: 35, step: 89
	action: tensor([[ 0.3420,  0.4345, -0.6302,  0.1056, -0.6489, -0.0321, -0.1994]],
       dtype=torch.float64)
	q_value: tensor([[-3.7699]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8153128468140369, distance: 0.4917847267023963 entropy 0.03264415264129639
epoch: 35, step: 90
	action: tensor([[ 0.4585,  0.4332, -0.5103, -0.2424, -0.2671,  0.2255, -0.2396]],
       dtype=torch.float64)
	q_value: tensor([[-3.0785]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8744991451073598, distance: 0.405396537782352 entropy 0.03264415264129639
epoch: 35, step: 91
	action: tensor([[ 0.4990,  0.5218, -0.4249,  0.2617,  0.0051,  0.0434, -0.6242]],
       dtype=torch.float64)
	q_value: tensor([[-3.0212]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8969841233122677, distance: 0.3672897213117606 entropy 0.03264415264129639
epoch: 35, step: 92
	action: tensor([[ 0.4470,  0.0041, -0.5807,  0.1169,  0.2541, -0.0963, -0.5354]],
       dtype=torch.float64)
	q_value: tensor([[-2.7387]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6779381057920015, distance: 0.6494210547958306 entropy 0.03264415264129639
epoch: 35, step: 93
	action: tensor([[ 0.7799,  0.6656, -0.4836, -0.3021, -0.0790,  0.5056, -0.5510]],
       dtype=torch.float64)
	q_value: tensor([[-1.9622]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9891046904130015, distance: 0.11944733945886055 entropy 0.03264415264129639
epoch: 35, step: 94
	action: tensor([[ 0.7015,  0.7483,  0.0334,  0.2621,  0.0008,  0.1037, -0.2208]],
       dtype=torch.float64)
	q_value: tensor([[-4.1447]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 35, step: 95
	action: tensor([[ 0.0648,  0.1042, -0.3774, -0.0476, -0.2457,  0.4842, -0.3831]],
       dtype=torch.float64)
	q_value: tensor([[-8.5223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.45970618228870763, distance: 0.8411465971965849 entropy 0.03264415264129639
epoch: 35, step: 96
	action: tensor([[ 0.4486, -0.1075, -0.2189, -0.1306, -0.2558,  0.3363, -0.7981]],
       dtype=torch.float64)
	q_value: tensor([[-2.1173]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.5856512144128692, distance: 0.7366135923847503 entropy 0.03264415264129639
epoch: 35, step: 97
	action: tensor([[ 0.6131, -0.2439, -0.6073,  0.4047,  0.1975, -0.1197, -0.7652]],
       dtype=torch.float64)
	q_value: tensor([[-2.3693]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.6485671268682209, distance: 0.6783875747018273 entropy 0.03264415264129639
epoch: 35, step: 98
	action: tensor([[ 0.7079,  0.4268, -0.6620,  0.0113, -0.5144,  0.3607, -0.7326]],
       dtype=torch.float64)
	q_value: tensor([[-2.4220]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9425268363488025, distance: 0.2743400700479432 entropy 0.03264415264129639
epoch: 35, step: 99
	action: tensor([[ 0.5113,  0.2898, -0.8113,  0.1722, -0.2971,  0.0616, -0.3833]],
       dtype=torch.float64)
	q_value: tensor([[-4.0661]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8488981066472364, distance: 0.44482751858299374 entropy 0.03264415264129639
epoch: 35, step: 100
	action: tensor([[ 0.7005,  0.2107, -0.4229, -0.2133,  0.0622,  0.2825, -0.2413]],
       dtype=torch.float64)
	q_value: tensor([[-2.8713]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8615214168438299, distance: 0.42584151571821977 entropy 0.03264415264129639
epoch: 35, step: 101
	action: tensor([[ 0.6962,  0.1535, -0.7427, -0.4012, -0.1897,  0.3596, -1.1577]],
       dtype=torch.float64)
	q_value: tensor([[-2.5763]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7880096585593042, distance: 0.5268836602889752 entropy 0.03264415264129639
epoch: 35, step: 102
	action: tensor([[ 0.7147,  0.3519, -0.4419,  0.3935, -0.1017,  0.0274,  0.1137]],
       dtype=torch.float64)
	q_value: tensor([[-4.1039]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9587268930258773, distance: 0.23248250410083773 entropy 0.03264415264129639
epoch: 35, step: 103
	action: tensor([[ 0.6473,  0.3178, -0.1994,  0.1692,  0.2608,  0.2656, -0.5391]],
       dtype=torch.float64)
	q_value: tensor([[-2.6408]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9591979614955062, distance: 0.23115198323600025 entropy 0.03264415264129639
epoch: 35, step: 104
	action: tensor([[ 0.9854,  0.4031, -0.4905,  0.2581, -0.2703,  0.3644, -0.0847]],
       dtype=torch.float64)
	q_value: tensor([[-2.2955]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 100, distance: 0.09105313473771022 entropy 0.03264415264129639
epoch: 35, step: 105
	action: tensor([[ 0.7599,  0.2893, -0.6268, -0.1139,  0.0595,  0.0632, -0.8280]],
       dtype=torch.float64)
	q_value: tensor([[-8.5223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9077300496768278, distance: 0.3476056578341917 entropy 0.03264415264129639
epoch: 35, step: 106
	action: tensor([[ 0.3520,  0.5567, -0.2905,  0.5699,  0.1047, -0.0687, -0.3299]],
       dtype=torch.float64)
	q_value: tensor([[-3.3218]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7953748140204786, distance: 0.5176500220429902 entropy 0.03264415264129639
epoch: 35, step: 107
	action: tensor([[ 0.4410,  0.3312, -0.5208,  0.0225, -0.4672, -0.0635, -0.0139]],
       dtype=torch.float64)
	q_value: tensor([[-2.3938]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8523550614190945, distance: 0.43970962786524503 entropy 0.03264415264129639
epoch: 35, step: 108
	action: tensor([[ 0.2804,  0.2706, -0.4737,  0.2563,  0.0047,  0.6063, -0.4794]],
       dtype=torch.float64)
	q_value: tensor([[-2.7065]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7340264954749662, distance: 0.5901683037981619 entropy 0.03264415264129639
epoch: 35, step: 109
	action: tensor([[ 0.7351,  0.4018, -0.0811,  0.2114,  0.3255,  0.1414, -0.8065]],
       dtype=torch.float64)
	q_value: tensor([[-2.2845]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9840553014999149, distance: 0.1444990024804939 entropy 0.03264415264129639
epoch: 35, step: 110
	action: tensor([[ 0.7153,  0.1932, -0.8631, -0.0866, -0.3200,  0.2403, -0.4806]],
       dtype=torch.float64)
	q_value: tensor([[-2.8338]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8673297778722523, distance: 0.4168150785037315 entropy 0.03264415264129639
epoch: 35, step: 111
	action: tensor([[ 0.4611,  0.2628, -0.1726,  0.0902, -0.3527,  0.2052, -0.6042]],
       dtype=torch.float64)
	q_value: tensor([[-3.5351]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8556767808802204, distance: 0.43473519093840673 entropy 0.03264415264129639
epoch: 35, step: 112
	action: tensor([[ 0.7591,  0.4802, -0.5921,  0.4532,  0.1871,  0.0089, -0.0336]],
       dtype=torch.float64)
	q_value: tensor([[-2.4958]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9555655864728576, distance: 0.24122172537665174 entropy 0.03264415264129639
epoch: 35, step: 113
	action: tensor([[ 0.5894,  0.5561, -0.7509, -0.1383,  0.2313,  0.3013, -0.8579]],
       dtype=torch.float64)
	q_value: tensor([[-3.0727]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9602025566665664, distance: 0.2282886289251725 entropy 0.03264415264129639
epoch: 35, step: 114
	action: tensor([[ 1.1200,  0.3064, -0.3701,  0.1605, -0.2165, -0.2872, -0.3086]],
       dtype=torch.float64)
	q_value: tensor([[-3.6324]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8804416995117336, distance: 0.39568224194293966 entropy 0.03264415264129639
epoch: 35, step: 115
	action: tensor([[ 0.4254,  0.1157, -0.6950, -0.1005,  0.1573,  0.0245, -0.6409]],
       dtype=torch.float64)
	q_value: tensor([[-3.7658]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7219494430443598, distance: 0.6034184402696828 entropy 0.03264415264129639
epoch: 35, step: 116
	action: tensor([[ 0.4657,  0.3287, -0.4156,  0.0332,  0.4100,  0.1896, -0.5653]],
       dtype=torch.float64)
	q_value: tensor([[-2.2863]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8767958598171841, distance: 0.401669951996257 entropy 0.03264415264129639
epoch: 35, step: 117
	action: tensor([[ 0.6817, -0.0046, -0.7259, -0.1943,  0.3057,  0.1534, -0.1006]],
       dtype=torch.float64)
	q_value: tensor([[-2.0986]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7122805772784138, distance: 0.6138203512616862 entropy 0.03264415264129639
epoch: 35, step: 118
	action: tensor([[ 0.5285,  0.8067, -0.5179,  0.2395,  0.0210,  0.3905, -0.5341]],
       dtype=torch.float64)
	q_value: tensor([[-2.6274]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 35, step: 119
	action: tensor([[ 0.6649,  0.5055, -0.6289,  0.0044,  0.1924,  0.4208, -0.1284]],
       dtype=torch.float64)
	q_value: tensor([[-8.5223]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.954942430066579, distance: 0.2429073052033937 entropy 0.03264415264129639
epoch: 35, step: 120
	action: tensor([[ 0.3348,  0.4385, -0.4816,  0.2724,  0.0309,  0.6508, -0.5689]],
       dtype=torch.float64)
	q_value: tensor([[-3.1116]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7588724210368704, distance: 0.5619273054904176 entropy 0.03264415264129639
epoch: 35, step: 121
	action: tensor([[ 0.1334,  0.3449, -0.7881,  0.4054, -0.2904,  0.3525, -0.7601]],
       dtype=torch.float64)
	q_value: tensor([[-2.7095]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.4899467793118939, distance: 0.8172678981241437 entropy 0.03264415264129639
epoch: 35, step: 122
	action: tensor([[ 0.7208,  0.0853, -0.7244,  0.2246,  0.1177,  0.4341, -0.7051]],
       dtype=torch.float64)
	q_value: tensor([[-2.9110]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8965048782680182, distance: 0.368143072947417 entropy 0.03264415264129639
epoch: 35, step: 123
	action: tensor([[-0.0072,  0.0417, -0.4506, -0.2367,  0.2549, -0.1219, -0.2827]],
       dtype=torch.float64)
	q_value: tensor([[-2.9124]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.28726037978027374, distance: 0.9661001375031779 entropy 0.03264415264129639
epoch: 35, step: 124
	action: tensor([[ 0.5546,  0.1065, -0.6927, -0.1299, -0.4586,  0.1634, -0.3443]],
       dtype=torch.float64)
	q_value: tensor([[-1.7370]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7435306728611135, distance: 0.5795279808522962 entropy 0.03264415264129639
epoch: 35, step: 125
	action: tensor([[ 0.7938,  0.4784, -0.6784, -0.0505, -0.1087,  0.3913, -0.7520]],
       dtype=torch.float64)
	q_value: tensor([[-2.9649]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9755602287999329, distance: 0.17889790764703814 entropy 0.03264415264129639
epoch: 35, step: 126
	action: tensor([[ 0.5734,  0.1994, -0.4356,  0.3362, -0.1508, -0.5251, -0.8269]],
       dtype=torch.float64)
	q_value: tensor([[-3.9837]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8748676964982939, distance: 0.4048008473785727 entropy 0.03264415264129639
epoch: 35, step: 127
	action: tensor([[ 0.6499, -0.2125, -0.6569,  0.4782, -0.1504,  0.2601, -0.1585]],
       dtype=torch.float64)
	q_value: tensor([[-3.1892]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7422411862351113, distance: 0.5809830410439969 entropy 0.03264415264129639
LOSS epoch 35 actor 84.37610715989827 critic 968.4387463268167 
epoch: 36, step: 0
	action: tensor([[ 1.6729,  0.5128, -0.5433,  0.4237, -0.1568,  0.6230, -1.0697]],
       dtype=torch.float64)
	q_value: tensor([[-2.0066]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.8861367900528442, distance: 0.3861432119610994 entropy 0.03264415264129639
epoch: 36, step: 1
	action: tensor([[ 1.8650,  0.4028, -1.3881,  0.5033, -0.1074,  0.3407, -1.0971]],
       dtype=torch.float64)
	q_value: tensor([[-5.9559]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 36, step: 2
	action: tensor([[ 1.1960, -0.1521, -0.7745,  0.3223,  0.1277,  0.2700,  0.0208]],
       dtype=torch.float64)
	q_value: tensor([[-8.1238]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7783780371992854, distance: 0.5387199909137474 entropy 0.03264415264129639
epoch: 36, step: 3
	action: tensor([[ 1.5476,  0.1499, -1.5718,  0.4065, -0.3856,  0.6259, -1.6357]],
       dtype=torch.float64)
	q_value: tensor([[-3.3185]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.7566108059946963, distance: 0.5645564057875866 entropy 0.03264415264129639
epoch: 36, step: 4
	action: tensor([[ 1.9681,  0.4689, -1.1722,  0.2722,  0.0290,  0.7218, -0.8873]],
       dtype=torch.float64)
	q_value: tensor([[-7.4020]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 36, step: 5
	action: tensor([[ 1.1872,  0.1054, -0.7758,  0.2860,  0.0708,  0.4005, -0.2812]],
       dtype=torch.float64)
	q_value: tensor([[-8.1238]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9247182419750839, distance: 0.3139796995128756 entropy 0.03264415264129639
epoch: 36, step: 6
	action: tensor([[ 1.1750,  0.6705, -0.6910,  0.6578, -0.6334,  0.9315, -1.0276]],
       dtype=torch.float64)
	q_value: tensor([[-3.6734]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9081767388357858, distance: 0.3467632380834821 entropy 0.03264415264129639
epoch: 36, step: 7
	action: tensor([[ 2.1394,  1.0813, -1.3616,  0.8523, -0.6584,  0.9801, -1.2344]],
       dtype=torch.float64)
	q_value: tensor([[-5.9188]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 36, step: 8
	action: tensor([[ 0.9548,  0.3354, -0.0810,  0.4408, -0.3821,  0.5771, -0.7897]],
       dtype=torch.float64)
	q_value: tensor([[-8.1238]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9892179688783049, distance: 0.1188247704002329 entropy 0.03264415264129639
epoch: 36, step: 9
	action: tensor([[ 1.7208,  0.9449, -0.7488,  0.8901, -0.0292,  0.4657, -1.1262]],
       dtype=torch.float64)
	q_value: tensor([[-3.6214]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 36, step: 10
	action: tensor([[ 0.6258,  0.2932, -0.4565,  0.2983, -0.4696,  0.3562,  0.2314]],
       dtype=torch.float64)
	q_value: tensor([[-8.1238]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9023730495703924, distance: 0.3575539292785334 entropy 0.03264415264129639
epoch: 36, step: 11
	action: tensor([[ 1.3977,  0.7722, -0.6859,  0.1648, -0.0253,  0.9875, -1.1166]],
       dtype=torch.float64)
	q_value: tensor([[-2.7446]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9035606517728884, distance: 0.35537250741590987 entropy 0.03264415264129639
epoch: 36, step: 12
	action: tensor([[ 2.0033,  0.9473, -1.0784,  0.6374, -0.7810,  0.4578, -0.9192]],
       dtype=torch.float64)
	q_value: tensor([[-6.2995]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 36, step: 13
	action: tensor([[ 0.9042,  0.5112, -0.4240,  0.2222, -0.1284,  0.4321, -0.7911]],
       dtype=torch.float64)
	q_value: tensor([[-8.1238]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9766610257151633, distance: 0.17482260046178447 entropy 0.03264415264129639
epoch: 36, step: 14
	action: tensor([[ 1.7186,  0.7437, -1.6722,  0.4585, -0.0328,  0.5982, -1.0646]],
       dtype=torch.float64)
	q_value: tensor([[-3.7173]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.9790825193810867, distance: 0.16550509996300192 entropy 0.03264415264129639
epoch: 36, step: 15
	action: tensor([[ 1.9455,  0.7591, -1.0796,  0.5927, -0.5232,  0.8195, -1.3328]],
       dtype=torch.float64)
	q_value: tensor([[-7.5351]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	COLLISION
	reward: -50, distance: 0 entropy 0.03264415264129639
epoch: 36, step: 16
	action: tensor([[ 1.0140,  0.5526, -0.9682,  0.8176, -0.2939,  0.2318, -0.2333]],
       dtype=torch.float64)
	q_value: tensor([[-8.1238]], dtype=torch.float64, grad_fn=<AddmmBackward0>)
	reward: 0.894513210113234, distance: 0.37166847990261537 entropy 0.03264415264129639
